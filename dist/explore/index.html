<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="Search and browse our entire collection of open-source AI models and tools."><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.15.3"><title>Explore All AI Models | Free AI Tools</title><link rel="stylesheet" href="/_astro/about.CDWXvzd1.css"></head> <body class="bg-gray-50 dark:bg-gray-900 text-gray-900 dark:text-gray-100 flex flex-col min-h-screen"> <header class="bg-white dark:bg-gray-800 shadow-md sticky top-0 z-50"> <nav class="container mx-auto px-4 py-4 flex justify-between items-center"> <a href="/" class="text-2xl font-bold text-gray-900 dark:text-white">Free AI Tools</a> <div class="space-x-4"> <a href="/#explore" class="text-gray-600 dark:text-gray-300 hover:text-blue-500">Explore</a> <a href="/about" class="text-gray-600 dark:text-gray-300 hover:text-blue-500">About</a> </div> </nav> </header> <main class="flex-grow">  <section id="explore" class="py-20"> <div class="container max-w-7xl mx-auto px-4"> <div class="text-center mb-12"> <h1 class="text-4xl font-bold text-gray-900 dark:text-white">Explore All Models</h1> <p class="mt-4 text-lg text-gray-600 dark:text-gray-400">Search our comprehensive index of open-source AI.</p> </div> <!-- Search Bar --> <div class="max-w-2xl mx-auto mb-16"> <input type="search" id="search-input" placeholder="Search for models, tasks, or tags (e.g., 'Llama 3', 'text-to-image')..." class="w-full px-5 py-3 text-lg bg-white dark:bg-gray-800 border-2 border-gray-300 dark:border-gray-700 rounded-full focus:ring-4 focus:ring-blue-500/50 focus:border-blue-500 outline-none transition-all"> </div> <!-- Ad Slot --> <div class="ad-container my-4 my-16 text-center">  <ins class="adsbygoogle" style="display:block;" data-ad-client="ca-pub-YOUR_ID" data-ad-slot="YOUR_EXPLORE_PAGE_SLOT" data-ad-format="auto" data-full-width-responsive="true"></ins> <script>
        (window.adsbygoogle = window.adsbygoogle || []).push({});
      </script>    </div> <div id="models-grid" class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8"> <div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">DeepSeek-R1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 12,833</span> <span class="flex items-center gap-1">↓ 407,998</span> </div> <a href="/model/deepseek-ai--DeepSeek-R1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">FLUX.1-dev</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 11,764</span> <span class="flex items-center gap-1">↓ 1,570,740</span> </div> <a href="/model/black-forest-labs--FLUX.1-dev" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-xl-base-1.0</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 7,112</span> <span class="flex items-center gap-1">↓ 2,670,555</span> </div> <a href="/model/stabilityai--stable-diffusion-xl-base-1.0" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-v1-4</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 6,933</span> <span class="flex items-center gap-1">↓ 541,993</span> </div> <a href="/model/CompVis--stable-diffusion-v1-4" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Meta-Llama-3-8B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 6,364</span> <span class="flex items-center gap-1">↓ 1,832,499</span> </div> <a href="/model/meta-llama--Meta-Llama-3-8B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Kokoro-82M</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-speech</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-speech.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 5,243</span> <span class="flex items-center gap-1">↓ 4,368,458</span> </div> <a href="/model/hexgrad--Kokoro-82M" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">whisper-large-v3</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: automatic-speech-recognition</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for automatic-speech-recognition.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 5,077</span> <span class="flex items-center gap-1">↓ 3,994,171</span> </div> <a href="/model/openai--whisper-large-v3" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">bloom</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,959</span> <span class="flex items-center gap-1">↓ 3,685</span> </div> <a href="/model/bigscience--bloom" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.1-8B-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,894</span> <span class="flex items-center gap-1">↓ 4,932,517</span> </div> <a href="/model/meta-llama--Llama-3.1-8B-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-3-medium</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,865</span> <span class="flex items-center gap-1">↓ 12,029</span> </div> <a href="/model/stabilityai--stable-diffusion-3-medium" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-2-7b-chat-hf</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,640</span> <span class="flex items-center gap-1">↓ 366,593</span> </div> <a href="/model/meta-llama--Llama-2-7b-chat-hf" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mixtral-8x7B-Instruct-v0.1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,593</span> <span class="flex items-center gap-1">↓ 568,687</span> </div> <a href="/model/mistralai--Mixtral-8x7B-Instruct-v0.1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-2-7b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,421</span> <span class="flex items-center gap-1">↓ 530</span> </div> <a href="/model/meta-llama--Llama-2-7b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">FLUX.1-schnell</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,379</span> <span class="flex items-center gap-1">↓ 908,020</span> </div> <a href="/model/black-forest-labs--FLUX.1-schnell" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Meta-Llama-3-8B-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,270</span> <span class="flex items-center gap-1">↓ 1,051,969</span> </div> <a href="/model/meta-llama--Meta-Llama-3-8B-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">gpt-oss-120b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,124</span> <span class="flex items-center gap-1">↓ 3,786,599</span> </div> <a href="/model/openai--gpt-oss-120b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">all-MiniLM-L6-v2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: sentence-similarity</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for sentence-similarity.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,094</span> <span class="flex items-center gap-1">↓ 138,081,386</span> </div> <a href="/model/sentence-transformers--all-MiniLM-L6-v2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-2-1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,036</span> <span class="flex items-center gap-1">↓ 656,721</span> </div> <a href="/model/stabilityai--stable-diffusion-2-1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mistral-7B-v0.1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4,000</span> <span class="flex items-center gap-1">↓ 575,523</span> </div> <a href="/model/mistralai--Mistral-7B-v0.1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">DeepSeek-V3</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,993</span> <span class="flex items-center gap-1">↓ 156,315</span> </div> <a href="/model/deepseek-ai--DeepSeek-V3" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ControlNet-v1-1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,941</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/lllyasviel--ControlNet-v1-1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">gpt-oss-20b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,887</span> <span class="flex items-center gap-1">↓ 4,487,346</span> </div> <a href="/model/openai--gpt-oss-20b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">OrangeMixs</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,884</span> <span class="flex items-center gap-1">↓ 2,127</span> </div> <a href="/model/WarriorMama777--OrangeMixs" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ControlNet</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,763</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/lllyasviel--ControlNet" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Janus-Pro-7B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: any-to-any</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for any-to-any.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,521</span> <span class="flex items-center gap-1">↓ 76,128</span> </div> <a href="/model/deepseek-ai--Janus-Pro-7B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">phi-2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,406</span> <span class="flex items-center gap-1">↓ 776,584</span> </div> <a href="/model/microsoft--phi-2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">gemma-7b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,228</span> <span class="flex items-center gap-1">↓ 27,694</span> </div> <a href="/model/google--gemma-7b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-3.5-large</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,203</span> <span class="flex items-center gap-1">↓ 215,966</span> </div> <a href="/model/stabilityai--stable-diffusion-3.5-large" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-video-diffusion-img2vid-xt</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-to-video</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-to-video.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,185</span> <span class="flex items-center gap-1">↓ 107,322</span> </div> <a href="/model/stabilityai--stable-video-diffusion-img2vid-xt" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">openjourney</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,166</span> <span class="flex items-center gap-1">↓ 14,584</span> </div> <a href="/model/prompthero--openjourney" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">XTTS-v2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-speech</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-speech.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,156</span> <span class="flex items-center gap-1">↓ 5,080,268</span> </div> <a href="/model/coqui--XTTS-v2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">DeepSeek-V3-0324</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,076</span> <span class="flex items-center gap-1">↓ 239,294</span> </div> <a href="/model/deepseek-ai--DeepSeek-V3-0324" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mistral-7B-Instruct-v0.2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,012</span> <span class="flex items-center gap-1">↓ 3,466,767</span> </div> <a href="/model/mistralai--Mistral-7B-Instruct-v0.2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">gpt2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3,011</span> <span class="flex items-center gap-1">↓ 11,477,729</span> </div> <a href="/model/openai-community--gpt2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">starcoder</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,905</span> <span class="flex items-center gap-1">↓ 11,339</span> </div> <a href="/model/bigcode--starcoder" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">chatglm-6b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,867</span> <span class="flex items-center gap-1">↓ 2,250</span> </div> <a href="/model/zai-org--chatglm-6b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">QwQ-32B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,863</span> <span class="flex items-center gap-1">↓ 53,346</span> </div> <a href="/model/Qwen--QwQ-32B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-v-1-4-original</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,819</span> <span class="flex items-center gap-1">↓ 5</span> </div> <a href="/model/CompVis--stable-diffusion-v-1-4-original" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Dia-1.6B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-speech</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-speech.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,801</span> <span class="flex items-center gap-1">↓ 200,028</span> </div> <a href="/model/nari-labs--Dia-1.6B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">whisper-large-v3-turbo</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: automatic-speech-recognition</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for automatic-speech-recognition.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,675</span> <span class="flex items-center gap-1">↓ 4,015,779</span> </div> <a href="/model/openai--whisper-large-v3-turbo" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">DeepSeek-OCR</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,578</span> <span class="flex items-center gap-1">↓ 3,167,241</span> </div> <a href="/model/deepseek-ai--DeepSeek-OCR" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.3-70B-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,555</span> <span class="flex items-center gap-1">↓ 667,534</span> </div> <a href="/model/meta-llama--Llama-3.3-70B-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">sdxl-turbo</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,478</span> <span class="flex items-center gap-1">↓ 402,235</span> </div> <a href="/model/stabilityai--sdxl-turbo" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">bge-m3</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: sentence-similarity</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for sentence-similarity.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,474</span> <span class="flex items-center gap-1">↓ 7,010,285</span> </div> <a href="/model/BAAI--bge-m3" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">bert-base-uncased</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: fill-mask</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for fill-mask.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,462</span> <span class="flex items-center gap-1">↓ 55,476,414</span> </div> <a href="/model/google-bert--bert-base-uncased" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">waifu-diffusion</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,458</span> <span class="flex items-center gap-1">↓ 2,719</span> </div> <a href="/model/hakurei--waifu-diffusion" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">falcon-40b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,433</span> <span class="flex items-center gap-1">↓ 8,889</span> </div> <a href="/model/tiiuae--falcon-40b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">FLUX.1-Kontext-dev</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,413</span> <span class="flex items-center gap-1">↓ 253,049</span> </div> <a href="/model/black-forest-labs--FLUX.1-Kontext-dev" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">DeepSeek-R1-0528</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,386</span> <span class="flex items-center gap-1">↓ 556,260</span> </div> <a href="/model/deepseek-ai--DeepSeek-R1-0528" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">grok-1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,368</span> <span class="flex items-center gap-1">↓ 1,120</span> </div> <a href="/model/xai-org--grok-1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">r1-1776</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,324</span> <span class="flex items-center gap-1">↓ 856</span> </div> <a href="/model/perplexity-ai--r1-1776" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">csm-1b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-speech</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-speech.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,257</span> <span class="flex items-center gap-1">↓ 19,562</span> </div> <a href="/model/sesame--csm-1b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mistral-7B-Instruct-v0.3</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,237</span> <span class="flex items-center gap-1">↓ 1,209,683</span> </div> <a href="/model/mistralai--Mistral-7B-Instruct-v0.3" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Kimi-K2-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,231</span> <span class="flex items-center gap-1">↓ 84,333</span> </div> <a href="/model/moonshotai--Kimi-K2-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-2-70b-chat-hf</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,199</span> <span class="flex items-center gap-1">↓ 21,799</span> </div> <a href="/model/meta-llama--Llama-2-70b-chat-hf" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-2-7b-hf</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,192</span> <span class="flex items-center gap-1">↓ 584,686</span> </div> <a href="/model/meta-llama--Llama-2-7b-hf" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Qwen-Image</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,188</span> <span class="flex items-center gap-1">↓ 193,015</span> </div> <a href="/model/Qwen--Qwen-Image" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">phi-4</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,187</span> <span class="flex items-center gap-1">↓ 509,344</span> </div> <a href="/model/microsoft--phi-4" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.2-1B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,154</span> <span class="flex items-center gap-1">↓ 1,734,119</span> </div> <a href="/model/meta-llama--Llama-3.2-1B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">SDXL-Lightning</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,103</span> <span class="flex items-center gap-1">↓ 110,539</span> </div> <a href="/model/ByteDance--SDXL-Lightning" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Qwen-Image-Edit</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,099</span> <span class="flex items-center gap-1">↓ 190,983</span> </div> <a href="/model/Qwen--Qwen-Image-Edit" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">HunyuanVideo</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-video</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-video.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,076</span> <span class="flex items-center gap-1">↓ 1,481</span> </div> <a href="/model/tencent--HunyuanVideo" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">sd_control_collection</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,068</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/lllyasviel--sd_control_collection" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.1-Nemotron-70B-Instruct-HF</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,056</span> <span class="flex items-center gap-1">↓ 24,853</span> </div> <a href="/model/nvidia--Llama-3.1-Nemotron-70B-Instruct-HF" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">chatglm2-6b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,054</span> <span class="flex items-center gap-1">↓ 637,017</span> </div> <a href="/model/zai-org--chatglm2-6b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">LTX-Video</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-to-video</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-to-video.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2,039</span> <span class="flex items-center gap-1">↓ 242,182</span> </div> <a href="/model/Lightricks--LTX-Video" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-xl-refiner-1.0</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,990</span> <span class="flex items-center gap-1">↓ 429,292</span> </div> <a href="/model/stabilityai--stable-diffusion-xl-refiner-1.0" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">dolly-v2-12b</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,955</span> <span class="flex items-center gap-1">↓ 1,722</span> </div> <a href="/model/databricks--dolly-v2-12b" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">VibeVoice-1.5B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-speech</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-speech.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,954</span> <span class="flex items-center gap-1">↓ 182,036</span> </div> <a href="/model/microsoft--VibeVoice-1.5B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Qwen2.5-Coder-32B-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,947</span> <span class="flex items-center gap-1">↓ 110,135</span> </div> <a href="/model/Qwen--Qwen2.5-Coder-32B-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">stable-diffusion-2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,921</span> <span class="flex items-center gap-1">↓ 226,966</span> </div> <a href="/model/stabilityai--stable-diffusion-2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.1-8B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,904</span> <span class="flex items-center gap-1">↓ 563,164</span> </div> <a href="/model/meta-llama--Llama-3.1-8B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">clip-vit-large-patch14</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: zero-shot-image-classification</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for zero-shot-image-classification.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,894</span> <span class="flex items-center gap-1">↓ 12,221,474</span> </div> <a href="/model/openai--clip-vit-large-patch14" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">RMBG-1.4</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-segmentation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-segmentation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,878</span> <span class="flex items-center gap-1">↓ 212,734</span> </div> <a href="/model/briaai--RMBG-1.4" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Qwen2.5-Omni-7B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: any-to-any</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for any-to-any.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,814</span> <span class="flex items-center gap-1">↓ 182,389</span> </div> <a href="/model/Qwen--Qwen2.5-Omni-7B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mistral-7B-Instruct-v0.1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,806</span> <span class="flex items-center gap-1">↓ 564,713</span> </div> <a href="/model/mistralai--Mistral-7B-Instruct-v0.1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">zephyr-7b-beta</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,806</span> <span class="flex items-center gap-1">↓ 251,327</span> </div> <a href="/model/HuggingFaceH4--zephyr-7b-beta" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.2-3B-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,795</span> <span class="flex items-center gap-1">↓ 1,803,532</span> </div> <a href="/model/meta-llama--Llama-3.2-3B-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">IP-Adapter-FaceID</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,776</span> <span class="flex items-center gap-1">↓ 242,156</span> </div> <a href="/model/h94--IP-Adapter-FaceID" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">whisper-large-v2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: automatic-speech-recognition</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for automatic-speech-recognition.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,773</span> <span class="flex items-center gap-1">↓ 328,759</span> </div> <a href="/model/openai--whisper-large-v2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mixtral-8x7B-v0.1</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,768</span> <span class="flex items-center gap-1">↓ 79,368</span> </div> <a href="/model/mistralai--Mixtral-8x7B-v0.1" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">c4ai-command-r-plus</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,759</span> <span class="flex items-center gap-1">↓ 2,728</span> </div> <a href="/model/CohereLabs--c4ai-command-r-plus" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">QwQ-32B-Preview</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,740</span> <span class="flex items-center gap-1">↓ 12,028</span> </div> <a href="/model/Qwen--QwQ-32B-Preview" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">dreamlike-photoreal-2.0</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,719</span> <span class="flex items-center gap-1">↓ 8,454</span> </div> <a href="/model/dreamlike-art--dreamlike-photoreal-2.0" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Reflection-Llama-3.1-70B</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,710</span> <span class="flex items-center gap-1">↓ 97</span> </div> <a href="/model/mattshumer--Reflection-Llama-3.1-70B" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Florence-2-large</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,699</span> <span class="flex items-center gap-1">↓ 944,154</span> </div> <a href="/model/microsoft--Florence-2-large" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">WanVideo_comfy</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,696</span> <span class="flex items-center gap-1">↓ 6,336,215</span> </div> <a href="/model/Kijai--WanVideo_comfy" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">OmniParser</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,695</span> <span class="flex items-center gap-1">↓ 284</span> </div> <a href="/model/microsoft--OmniParser" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Phi-3-mini-128k-instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-generation.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,682</span> <span class="flex items-center gap-1">↓ 292,634</span> </div> <a href="/model/microsoft--Phi-3-mini-128k-instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">DeepSeek-V3-Base</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,681</span> <span class="flex items-center gap-1">↓ 4,856</span> </div> <a href="/model/deepseek-ai--DeepSeek-V3-Base" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">gemma-3-27b-it</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,676</span> <span class="flex items-center gap-1">↓ 835,178</span> </div> <a href="/model/google--gemma-3-27b-it" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Hunyuan3D-2</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-to-3d</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-to-3d.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,648</span> <span class="flex items-center gap-1">↓ 112,412</span> </div> <a href="/model/tencent--Hunyuan3D-2" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Mistral-Nemo-Instruct-2407</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: N/A</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for various tasks.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,617</span> <span class="flex items-center gap-1">↓ 113,328</span> </div> <a href="/model/mistralai--Mistral-Nemo-Instruct-2407" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ChatTTS</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-audio</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-audio.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,614</span> <span class="flex items-center gap-1">↓ 1,175</span> </div> <a href="/model/2Noise--ChatTTS" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">controlnet-union-sdxl-1.0</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,605</span> <span class="flex items-center gap-1">↓ 169,093</span> </div> <a href="/model/xinsir--controlnet-union-sdxl-1.0" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">SmolDocling-256M-preview</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,591</span> <span class="flex items-center gap-1">↓ 364,646</span> </div> <a href="/model/docling-project--SmolDocling-256M-preview" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Counterfeit-V2.5</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: text-to-image</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for text-to-image.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,578</span> <span class="flex items-center gap-1">↓ 4,680</span> </div> <a href="/model/gsdf--Counterfeit-V2.5" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Nanonets-OCR-s</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,551</span> <span class="flex items-center gap-1">↓ 166,325</span> </div> <a href="/model/nanonets--Nanonets-OCR-s" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Llama-3.2-11B-Vision-Instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-text-to-text</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for image-text-to-text.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,541</span> <span class="flex items-center gap-1">↓ 262,133</span> </div> <a href="/model/meta-llama--Llama-3.2-11B-Vision-Instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Phi-4-multimodal-instruct</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Hugging Face</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: automatic-speech-recognition</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A model for automatic-speech-recognition.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1,527</span> <span class="flex items-center gap-1">↓ 380,689</span> </div> <a href="/model/microsoft--Phi-4-multimodal-instruct" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">joinly</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Make your meetings accessible to AI Agents</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 392</span> <span class="flex items-center gap-1">↓ 392</span> </div> <a href="/model/github-joinly-ai-joinly" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">auto-md</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub, GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Convert Files /  Folders / GitHub Repos Into AI / LLM-ready Files</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 199</span> <span class="flex items-center gap-1">↓ 199</span> </div> <a href="/model/github-tegridydev-auto-md" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">polyfire-js</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">🔥 React library of AI components 🔥</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 141</span> <span class="flex items-center gap-1">↓ 141</span> </div> <a href="/model/github-polyfact-polyfire-js" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">robin</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">AI-Powered Dark Web OSINT Tool</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 132</span> <span class="flex items-center gap-1">↓ 132</span> </div> <a href="/model/github-apurvsinghgautam-robin" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">diny</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">generate git commit messages</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 98</span> <span class="flex items-center gap-1">↓ 98</span> </div> <a href="/model/github-dinoDanic-diny" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">sketch2app</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">The ultimate sketch to code app made using GPT4o serving 30k+ users. Choose your desired framework (React, Next, React Native, Flutter) for your app. It will instantly generate code and preview (sandbox) from a simple hand drawn sketch on paper captured from webcam</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 84</span> <span class="flex items-center gap-1">↓ 84</span> </div> <a href="/model/github-cameronking4-sketch2app" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">json-repair</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Repair JSON! A Java library for fixing JSON anomalies generated by LLMs.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 67</span> <span class="flex items-center gap-1">↓ 67</span> </div> <a href="/model/github-HAibiiin-json-repair" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">phantom-lens</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">The open-source, privacy-focused alternative to Cluely that helps you see beyond and know more. This undetectable AI assistant operates like a ghost across your screen, providing real-time information during meetings, interviews, and presentations without leaving a trace.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 55</span> <span class="flex items-center gap-1">↓ 55</span> </div> <a href="/model/github-inulute-phantom-lens" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">mocxykit</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">This is an Frontend development service middleware that can be used with webpack and vite. Its main function is to visualize the configuration, manage http(s)-proxy, and mock data.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 39</span> <span class="flex items-center gap-1">↓ 39</span> </div> <a href="/model/github-shunseven-mocxykit" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">AIForge</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">🚀 智能意图自适应执行引擎，只需一句话，让AI帮你搞定想做的事（数据分析与处理、高时效性内容创作、最新信息获取、数据可视化、系统交互、自动化工作流、代码开发等)</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 27</span> <span class="flex items-center gap-1">↓ 27</span> </div> <a href="/model/github-iniwap-AIForge" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">commander</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Commander, your AI coding commander centre for all you ai coding cli agents</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 19</span> <span class="flex items-center gap-1">↓ 19</span> </div> <a href="/model/github-autohandai-commander" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">AI-HR-Agent</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">AI HR Agent for HRMS</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 18</span> <span class="flex items-center gap-1">↓ 18</span> </div> <a href="/model/github-eVolpe-AI-AI-HR-Agent" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">promptdown</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A Python package that enables the creation and parsing of structured prompts for language models in markdown format</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 14</span> <span class="flex items-center gap-1">↓ 14</span> </div> <a href="/model/github-btfranklin-promptdown" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">videocutterAI</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub, GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">AI-powered web tool that automatically finds and generates highlight clips from your videos.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 13</span> <span class="flex items-center gap-1">↓ 13</span> </div> <a href="/model/github-Crezy-haker-videocutterAI" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">market-maven</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Maven is a cutting-edge web application that leverages the power of AI to revolutionize electronic categorized product research and data-driven decision-making.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 9</span> <span class="flex items-center gap-1">↓ 9</span> </div> <a href="/model/github-rizzzky78-market-maven" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">message-mcp</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Desktop notifications, custom sounds, ntfy mobile notifications, email notifications, and API pushes reduce anxiety while waiting for AI tasks, allowing you to comfortably enjoy a cup of coffee.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 8</span> <span class="flex items-center gap-1">↓ 8</span> </div> <a href="/model/github-gimjin-message-mcp" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">npm-helper-mcp</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A Model Context Protocol (MCP) server providing tools for NPM package management and dependency updates. Helps LLMs like Claude interact with npm packages, search npm registry, and keep dependencies up-to-date.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 6</span> <span class="flex items-center gap-1">↓ 6</span> </div> <a href="/model/github-pinkpixel-dev-npm-helper-mcp" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">AI-HR-MintHCM-Package</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">AI package for MintHCM system</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 5</span> <span class="flex items-center gap-1">↓ 5</span> </div> <a href="/model/github-eVolpe-AI-AI-HR-MintHCM-Package" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">CommiZard</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Use LLMs to write good commit messages with full Control</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4</span> <span class="flex items-center gap-1">↓ 4</span> </div> <a href="/model/github-Chungzter-CommiZard" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">n8n-nodes-bookstack</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Community n8n node for the BookStack API</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4</span> <span class="flex items-center gap-1">↓ 4</span> </div> <a href="/model/github-lucaguindani-n8n-nodes-bookstack" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">MockGen</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Instantly generate mock REST APIs powered by LLMs (GPT/Gemini). Just describe your endpoint—MockGen does the rest. Docker-ready, fast, and open source.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 4</span> <span class="flex items-center gap-1">↓ 4</span> </div> <a href="/model/github-Mo-Ko-MockGen" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Camouflage-AI</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">🎥 Camouflage-AI – A fast and flexible AI tool for removing video backgrounds using YOLOv8 segmentation. Customize with solid colors, blur, or images. Built with Python &amp; CustomTkinter for a stunning desktop experience.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3</span> <span class="flex items-center gap-1">↓ 3</span> </div> <a href="/model/github-Vishnu-tppr-Camouflage-AI" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">aidy</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">AI-assisted CLI for GitHub workflows — generate commits, issues, PRs, and releases with one command</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3</span> <span class="flex items-center gap-1">↓ 3</span> </div> <a href="/model/github-volodya-lombrozo-aidy" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">clipboardAI</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">ClipboardAI is an AI-powered clipboard assistant that works with multiple LLM providers to help you process text from your clipboard quickly and efficiently.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3</span> <span class="flex items-center gap-1">↓ 3</span> </div> <a href="/model/github-lokeshch185-clipboardAI" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Diagrammer-Bot</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Diagrammer Bot Telegram</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3</span> <span class="flex items-center gap-1">↓ 3</span> </div> <a href="/model/github-Lixher-Diagrammer-Bot" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">RAG-Scraper-AI-GUI</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">This python powered AI based RAG Scraper allows you to ask question based on PDF/URL provided to the software using local Ollama powered LLMs</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3</span> <span class="flex items-center gap-1">↓ 3</span> </div> <a href="/model/github-TufayelLUS-RAG-Scraper-AI-GUI" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ai-council-mcp</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Multi-AI consensus MCP server that queries multiple AI models (OpenAI, Claude, Gemini, custom APIs) in parallel and synthesizes responses to reduce bias and improve accuracy. A Python implementation of the wisdom-of-crowds approach for AI decision making.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 3</span> <span class="flex items-center gap-1">↓ 3</span> </div> <a href="/model/github-0xAkuti-ai-council-mcp" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">AI-Agent</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Agentic code editor using Python and Google Gemini — supports function-calling, file editing, and debugging via LLM.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2</span> <span class="flex items-center gap-1">↓ 2</span> </div> <a href="/model/github-Ocidemus-AI-Agent" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">PLC-Data-Lab</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A portable, zero-dependency, browser-based tool for analyzing and converting PLC raw data formats.  一个可在任意浏览器运行的、零依赖的 PLC 原始数据解析与转换工具。</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2</span> <span class="flex items-center gap-1">↓ 2</span> </div> <a href="/model/github-duyl328-PLC-Data-Lab" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Contextinator</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Turning messy repos into weapons of mass structured context.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2</span> <span class="flex items-center gap-1">↓ 2</span> </div> <a href="/model/github-starthackHQ-Contextinator" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">CodeCharm</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">VS Code extension that adds AI-powered inline comments to selected code using Google Gemini. Simple, fast, and emoji-rich 💬✨</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2</span> <span class="flex items-center gap-1">↓ 2</span> </div> <a href="/model/github-DeveloperPuneet-CodeCharm" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">jobpeap4u-easy-seo-site</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">jobleap4u是一个开源的AI导航站，你可以基于这个模版再开发出自己的AI导航站点</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2</span> <span class="flex items-center gap-1">↓ 2</span> </div> <a href="/model/github-XiaomingX-jobpeap4u-easy-seo-site" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ocr-ai-shell</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">AI OCR Tool | Webcam &amp; Image Text Recognition with Astra | Offline Summarization</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 2</span> <span class="flex items-center gap-1">↓ 2</span> </div> <a href="/model/github-Motaz432-ocr-ai-shell" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">REFRAME_Feedback-rewriter-gpt</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">REFRAME helps you rewrite workplace feedback and everyday messages with the right tone — empathetic, constructive, or persuasive — powered by free LLMs via OpenRouter.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-dmmudhan-REFRAME_Feedback-rewriter-gpt" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">pickled_pipeline</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A Python package for caching repeat runs of pipelines that have expensive operations along the way</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-btfranklin-pickled_pipeline" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ResxMcp</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A lightweight MCP server for managing .resx localization files—works with any MCP-compatible client.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-miaofalianhua-ResxMcp" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">AI-Research-Lab-Simulator</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">🧠 A multi-agent Gen AI platform powered by Google Gemini 2.5 Pro that autonomously generates, reviews, and composes full-length academic research papers — complete with chat assistant, dark UI, and editable .docx export.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-Pranav-Sharma-Official-AI-Research-Lab-Simulator" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">now-ai-landing-page</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Powerful, HIPAA-compliant AI tools that automate your patient communication, reduce call wait times, and grow your practice effortlessly.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-fabiconcept-now-ai-landing-page" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">MindTrial</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">MindTrial: Evaluate and compare AI language models (LLMs) on text-based tasks with optional file/image attachments and tool use. Supports multiple providers (OpenAI, Google, Anthropic, DeepSeek, Mistral AI, xAI, Alibaba), custom tasks in YAML, and HTML/CSV reports.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-petmal-MindTrial" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">repo2file</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A utility for merging repository files into a single text file for interacting with it using large-context neural networks, e.g. qwen.ai</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-prokhororlov-repo2file" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">screenshot_based_ai_desktop_assistant</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A lightweight Python-based desktop assistant that lets users capture a region of their screen, extract text using PaddleOCR, and instantly query selected large language models (LLMs) for responses, all without interrupting workflow. Designed with a minimal popup UI and global hotkey support for distraction-free productivity.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-KatavinaNguyen-screenshot_based_ai_desktop_assistant" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Tweets</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">In a nutshell: An all-powerful AI docking station disguised as a tweet tool!</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-chaolunner-Tweets" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">reqsync</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Synchronize requirements.txt to match installed versions, safely and atomically.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-ImYourBoyRoy-reqsync" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">autocorrect-tool</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">A user-friendly text correction tool powered by AI (T5 transformer) that fixes grammar and spelling mistakes in real-time. Features an easy-to-use GUI interface with instant corrections and clipboard support.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-iamrealvinnu-autocorrect-tool" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">El-Moufid</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">El Moufid, AI-Powered Tools to Enhance Your Learning and Productivity. 🎥 YouTube Summarizer (Main Tool). El Moufid allows 2 free summaries per day for all users.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 1</span> <span class="flex items-center gap-1">↓ 1</span> </div> <a href="/model/github-fenneccyber-El-Moufid" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">EasyNegative</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Civitai</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/datasets/gsdf/EasyNegative&quot;&gt;&lt;strong&gt;Original Hugging Face Repository&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Counterfeit-V3 (which has 2.5 and 2.5 as well) on Civitai - &lt;/strong&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://civitai.com/models/4468/counterfeit-v25&quot;&gt;&lt;strong&gt;https://civitai.com/models/4468/counterfeit-v25&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;If you like this embedding, please consider taking the time to give the repository a like and browsing their other work on HuggingFace.&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;This embedding should be used in your NEGATIVE prompt. Adjust the strength as desired (seems to scale well without any distortions), the strength required may vary based on positive and negative prompts. Use the EasyNegative_pt (PickleTensors) version if you are unable to use SafeTensors embeddings.&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;Samples are, in order:&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;sample01 - Counterfeit-V2.0.safetensors&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;sample02 - AbyssOrangeMix2_sfw.safetensors&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;sample03 - anything-v4.0-pruned.safetensors&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strength comparison using AbyssOrangeMix2_sfw.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br /&gt;&lt;strong&gt;From Author&lt;/strong&gt;&lt;br /&gt;&quot;This is a Negative Embedding trained with Counterfeit. Please use it in the &quot;\stable-diffusion-webui\embeddings&quot; folder. It can be used with other models, but the effectiveness is not certain.&quot;&lt;/p&gt;</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/civitai-easynegative" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Counterfeit-V3.0</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Civitai</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">&lt;p&gt;high quality anime style model.&lt;/p&gt;&lt;p&gt;Support☕ &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://ko-fi.com/sfa837348&quot;&gt;https://ko-fi.com/sfa837348&lt;/a&gt;&lt;/p&gt;&lt;p&gt;more info. &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/gsdf/Counterfeit-V2.0&quot;&gt;https://huggingface.co/gsdf/Counterfeit-V2.0&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Verson2.5 &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/gsdf/Counterfeit-V2.5&quot;&gt;https://huggingface.co/gsdf/Counterfeit-V2.5&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Verson3.0 &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/gsdf/Counterfeit-V3.0&quot;&gt;https://huggingface.co/gsdf/Counterfeit-V3.0&lt;/a&gt;&lt;/p&gt;&lt;p&gt;EasyNegative &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/datasets/gsdf/EasyNegative&quot;&gt;https://huggingface.co/datasets/gsdf/EasyNegative&lt;/a&gt;&lt;/p&gt;&lt;p&gt;(Use clip: openai/clip-vit-large-patch14-336)&lt;br /&gt;EasyNegative(Negative Embedding) &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/datasets/gsdf/EasyNegative&quot;&gt;https://huggingface.co/datasets/gsdf/EasyNegative&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color:rgb(209, 213, 219)&quot;&gt;Official hosting for online AI image generator. &lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://rendernet.ai/&quot;&gt;https://rendernet.ai/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/civitai-counterfeit-v3.0" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ReV Animated</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Civitai</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">&lt;p&gt;&lt;em&gt;April 28, 2024: added V2 Rebirth pruned&lt;/em&gt;&lt;/p&gt;&lt;h1 id=&quot;heading-46&quot;&gt;&lt;span style=&quot;color:rgb(64, 192, 87)&quot;&gt;v2:REBIRTH&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span style=&quot;color:rgb(230, 73, 128)&quot;&gt;Thanks to &lt;/span&gt;&lt;span style=&quot;color:rgb(250, 82, 82)&quot;&gt;S6yx&lt;/span&gt;&lt;span style=&quot;color:rgb(230, 73, 128)&quot;&gt; for the creation of this beautiful model. Enjoyed by millions. With their permission, I, &lt;/span&gt;&lt;span style=&quot;color:rgb(250, 82, 82)&quot;&gt;Zovya&lt;/span&gt;&lt;span style=&quot;color:rgb(230, 73, 128)&quot;&gt;, will be maintaining it moving forward.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;April 4, 2024: fp16 and +VAE added&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;April 2, 2024: Rebirth&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Update 3: Disclaimer/Permissions updated&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Update 2: I am no longer maintaining/updating this model&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Update 1: I&#39;ve been a bit burnt out on SD model development (SD in general tbh) and that is the reason there have not been an update. Looking to come back around and develop again by next month or so.Thank you everyone who sends reviews and enjoy my model&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Pay attention to the &lt;em&gt;&lt;u&gt;About this version&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;section &lt;/strong&gt;of model page&lt;strong&gt; for specific version information. ➡️➡️➡️➡️➡️&lt;/strong&gt;&lt;/p&gt;&lt;h3 id=&quot;heading-416&quot;&gt;&lt;br /&gt;&lt;u&gt;Model Overview:&lt;/u&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;u&gt;rev&lt;/u&gt; or &lt;u&gt;revision&lt;/u&gt;: The concept of how the model generates images is likely to change as I see fit.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;u&gt;Animated&lt;/u&gt;: The model has the ability to create 2.5D like image generations. This model is a checkpoint merge, meaning it is a product of other models to create a product that derives from the originals.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Kind of generations:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Fantasy&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Anime&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;semi-realistic&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;decent Landscape&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LoRA friendly&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It works &lt;strong&gt;&lt;em&gt;&lt;u&gt;best on these resolution dimensions:&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;512x512&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;512x768&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;768x512&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;h3 id=&quot;heading-417&quot;&gt;&lt;u&gt;VAE&lt;/u&gt;:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/VAEs/orangemix.vae.pt&quot;&gt;&lt;u&gt;orangemix.vae.pt&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main/vae&quot;&gt;kl-f8-anime2.ckpt&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/NoCrypt/blessed_vae/blob/main/blessed2.vae.pt&quot;&gt;Blessed2.vae.pt&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;heading-418&quot;&gt;&lt;u&gt;Prompting&lt;/u&gt;:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Order matters&lt;/strong&gt; - words near the front of your prompt are weighted more heavily than the things in the back of your prompt.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prompt order&lt;/strong&gt; - content type &amp;gt; description &amp;gt; style &amp;gt; composition&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;This model likes&lt;/strong&gt;: ((best quality)), ((masterpiece)), (detailed) in beginning of prompt if you want anime-2.5D type&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;This model does great on&lt;strong&gt; &lt;u&gt;PORTRAITS&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;Negative Prompt Embeddings:&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/embed/EasyNegative/tree/main&quot;&gt;EasyNegative&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://civitai.com/models/4629/deep-negative-v1x&quot;&gt;Deep Negative&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/embed/bad_prompt/blob/main/bad_prompt_version2.pt&quot;&gt;bad_prompt_version2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/nick-x-hacker/bad-artist/blob/main/bad-artist.pt&quot;&gt;bad-artist&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/nick-x-hacker/bad-artist/blob/main/bad-artist-anime.pt&quot;&gt;bad-artist-anime&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://huggingface.co/p1atdev/badquality/tree/main&quot;&gt;bad-quality&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Make use of weights in negative prompts (i.e (worst quality, low quality:1.4))&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;h3 id=&quot;heading-419&quot;&gt;&lt;u&gt;Video Features&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://youtu.be/Nl43zR5dVuM?t=192&quot;&gt;Olivio Sarikas - Why Is EVERYONE Using This Model?! - Rev Animated for Stable Diffusion / A1111&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://www.youtube.com/watch?v=A6dQPMy_tHY&quot;&gt;Olivio Sarikas - ULTRA SHARP Upscale! - Don&#39;t miss this Method!!! / A1111 - NEW Model&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://www.youtube.com/watch?v=ezNDCWhv4pQ&quot;&gt;AMAZING SD Models - And how to get the MOST out of them!&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2 id=&quot;heading-420&quot;&gt;&lt;strong&gt;&lt;u&gt;Disclaimer (Updated 10/31/2023):&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/h2&gt;&lt;p&gt;The license type is &lt;a target=&quot;_blank&quot; rel=&quot;ugc&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0&quot;&gt;CC BY-NC-ND 4.0&lt;/a&gt; &lt;br /&gt;&lt;strong&gt;Do not sell&lt;/strong&gt; this model on any website without permissions from creator (me)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Credit&lt;/strong&gt; me if you use my model in your own merges&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;You can use derivative models which uses ReV Animated for Buzz points and site-based currency that does not convert over to real world currency.&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Do not use this model to &lt;strong&gt;&lt;u&gt;monetize&lt;/u&gt;&lt;/strong&gt; on other platforms without expressed written consent. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/civitai-rev-animated" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">Detail Tweaker XL</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): Civitai</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: image-generation</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">&lt;p&gt;Detail tweaker for SDXL.&lt;/p&gt;&lt;p&gt;Works with weights [-3, 3]&lt;/p&gt;&lt;p&gt;Use positive weight to increase details and negative weight to reduce details.&lt;/p&gt;&lt;p&gt;Good weight depends on your prompt and number of sampling steps, I recommend starting at 1.5 and then adjusting it.&lt;/p&gt;</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/civitai-detail-tweaker-xl" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">tagit-video</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">TAGiT - AI-powered Chrome extension and web app for tagging and organizing YouTube video moments</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/github-adampao-tagit-video" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">ai-subtitle-translator</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">An AI-powered subtitle translation tool using GPT &amp; Whisper</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/github-yoshi08010801-ai-subtitle-translator" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div><div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col"> <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">sales-meeting-insights-ai-extension</h3> <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): GitHub</p> <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: tool</p> <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">Most AI tools help you after the call. PitchPulse helps you during it — guiding discovery and building your pitch in real time, so you can close while others guess.</p> <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3"> <span class="flex items-center gap-1">❤️ 0</span> <span class="flex items-center gap-1">↓ 0</span> </div> <a href="/model/github-HappyRIO-sales-meeting-insights-ai-extension" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a> </div> </div> <p id="no-results" class="text-center text-xl text-gray-500 mt-12 hidden">No models found matching your search.</p> </div> </section> <script>(function(){const allModels = [{"id":"deepseek-ai/DeepSeek-R1","name":"DeepSeek-R1","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2501.12948","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","fp8","region:us"],"likes":12833,"downloads":407998,"readme":"---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-R1"}]},{"id":"black-forest-labs/FLUX.1-dev","name":"FLUX.1-dev","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","text-to-image","image-generation","flux","en","license:other","endpoints_compatible","diffusers:FluxPipeline","region:us"],"likes":11764,"downloads":1570740,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/black-forest-labs/FLUX.1-dev"}]},{"id":"stabilityai/stable-diffusion-xl-base-1.0","name":"stable-diffusion-xl-base-1.0","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","onnx","safetensors","text-to-image","stable-diffusion","arxiv:2307.01952","arxiv:2211.01324","arxiv:2108.01073","arxiv:2112.10752","license:openrail++","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionXLPipeline","region:us"],"likes":7112,"downloads":2670555,"readme":"---\nlicense: openrail++\ntags:\n- text-to-image\n- stable-diffusion\n---\n# SD-XL 1.0-base Model Card\n![row01](01.png)\n\n## Model\n\n![pipeline](pipeline.png)\n\n[SDXL](https://arxiv.org/abs/2307.01952) consists of an [ensemble of experts](https://arxiv.org/abs/2211.01324) pipeline for latent diffusion: \nIn a first step, the base model is used to generate (noisy) latents, \nwhich are then further processed with a refinement model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/) specialized for the final denoising steps.\nNote that the base model can be used as a standalone module.\n\nAlternatively, we can use a two-stage pipeline as follows: \nFirst, the base model is used to generate latents of the desired output size. \nIn the second step, we use a specialized high-resolution model and apply a technique called SDEdit (https://arxiv.org/abs/2108.01073, also known as \"img2img\") \nto the latents generated in the first step, using the same prompt. This technique is slightly slower than the first one, as it requires more function evaluations.\n\nSource code is available at https://github.com/Stability-AI/generative-models .\n\n### Model Description\n\n- **Developed by:** Stability AI\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [CreativeML Open RAIL++-M License](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/LICENSE.md)\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses two fixed, pretrained text encoders ([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip) and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main)).\n- **Resources for more information:** Check out our [GitHub Repository](https://github.com/Stability-AI/generative-models) and the [SDXL report on arXiv](https://arxiv.org/abs/2307.01952).\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), which implements the most popular diffusion frameworks (both training and inference) and for which new functionalities like distillation will be added over time.\n[Clipdrop](https://clipdrop.co/stable-diffusion) provides free SDXL inference.\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Demo:** https://clipdrop.co/stable-diffusion\n\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SDXL (with and without refinement) over SDXL 0.9 and Stable Diffusion 1.5 and 2.1. \nThe SDXL base model performs significantly better than the previous variants, and the model combined with the refinement module achieves the best overall performance.\n\n\n### 🧨 Diffusers \n\nMake sure to upgrade diffusers to >= 0.19.0:\n```\npip install diffusers --upgrade\n```\n\nIn addition make sure to install `transformers`, `safetensors`, `accelerate` as well as the invisible watermark:\n```\npip install invisible_watermark transformers accelerate safetensors\n```\n\nTo just use the base model, you can run:\n\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = \"An astronaut riding a green horse\"\n\nimages = pipe(prompt=prompt).images[0]\n```\n\nTo use the whole base + refiner pipeline as an ensemble of experts you can run:\n\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load both base & refiner\nbase = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nbase.to(\"cuda\")\nrefiner = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner.to(\"cuda\")\n\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = \"A majestic lion jumping from a big stone at night\"\n\n# run both experts\nimage = base(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type=\"latent\",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n```\n\nWhen using `torch >= 2.0`, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline:\n```py\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\nIf you are limited by GPU VRAM, you can enable *cpu offloading* by calling `pipe.enable_model_cpu_offload`\ninstead of `.to(\"cuda\")`:\n\n```diff\n- pipe.to(\"cuda\")\n+ pipe.enable_model_cpu_offload()\n```\n\nFor more information on how to use Stable Diffusion XL with `diffusers`, please have a look at [the Stable Diffusion XL Docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl).\n\n### Optimum\n[Optimum](https://github.com/huggingface/optimum) provides a Stable Diffusion pipeline compatible with both [OpenVINO](https://docs.openvino.ai/latest/index.html) and [ONNX Runtime](https://onnxruntime.ai/).\n\n#### OpenVINO\n\nTo install Optimum with the dependencies required for OpenVINO :\n\n```bash\npip install optimum[openvino]\n```\n\nTo load an OpenVINO model and run inference with OpenVINO Runtime, you need to replace `StableDiffusionXLPipeline` with Optimum `OVStableDiffusionXLPipeline`. In case you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True`.\n\n```diff\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n```\n\nYou can find more examples (such as static reshaping and model compilation) in optimum [documentation](https://huggingface.co/docs/optimum/main/en/intel/inference#stable-diffusion-xl).\n\n\n#### ONNX\n\nTo install Optimum with the dependencies required for ONNX Runtime inference :\n\n```bash\npip install optimum[onnxruntime]\n```\n\nTo load an ONNX model and run inference with ONNX Runtime, you need to replace `StableDiffusionXLPipeline` with Optimum `ORTStableDiffusionXLPipeline`. In case you want to load a PyTorch model and convert it to the ONNX format on-the-fly, you can set `export=True`.\n\n```diff\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\n```\n\nYou can find more examples in optimum [documentation](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models#stable-diffusion-xl).\n\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"}]},{"id":"CompVis/stable-diffusion-v1-4","name":"stable-diffusion-v1-4","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","arxiv:2207.12598","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:1910.09700","license:creativeml-openrail-m","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":6933,"downloads":541993,"readme":"---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\nwidget:\n- text: \"A high tech solarpunk utopia in the Amazon rainforest\"\n  example_title: Amazon rainforest\n- text: \"A pikachu fine dining with a view to the Eiffel Tower\"\n  example_title: Pikachu in Paris\n- text: \"A mecha robot in a favela in expressionist style\"\n  example_title: Expressionist robot\n- text: \"an insect robot preparing a delicious meal\"\n  example_title: Insect robot\n- text: \"A small cabin on top of a snowy mountain in the style of Disney, artstation\"\n  example_title: Snowy disney cabin\nextra_gated_prompt: |-\n  This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n  The CreativeML OpenRAIL License specifies: \n\n  1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n  2. The authors claim no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n  3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n  Please read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n      \nextra_gated_heading: Please read the LICENSE to access this model\n---\n\n# Stable Diffusion v1-4 Model Card\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [🤗's Stable Diffusion with 🧨Diffusers blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-4** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nThis weights here are intended to be used with the 🧨 Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, [come here](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n## Examples\n\nWe recommend using [🤗's Diffusers library](https://github.com/huggingface/diffusers) to run Stable Diffusion.\n\n### PyTorch\n\n```bash\npip install --upgrade diffusers transformers scipy\n```\n\nRunning the pipeline with the default PNDM scheduler:\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\n**Note**:\nIf you are limited by GPU memory and have less than 4GB of GPU RAM available, please make sure to load the StableDiffusionPipeline in float16 precision instead of the default float32 precision as done above. You can do so by telling diffusers to expect the weights to be in float16 precision:\n\n\n```py\nimport torch\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\npipe.enable_attention_slicing()\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\nTo swap out the noise scheduler, pass it to `from_pretrained`:\n\n```python\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\n### JAX/Flax\n\nTo use StableDiffusion on TPUs and GPUs for faster inference you can leverage JAX/Flax.\n\nRunning the pipeline with default PNDMScheduler\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", revision=\"flax\", dtype=jax.numpy.bfloat16\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n**Note**:\nIf you are limited by TPU memory, please make sure to load the `FlaxStableDiffusionPipeline` in `bfloat16` precision instead of the default `float32` precision as done above. You can do so by telling diffusers to load the weights from \"bf16\" branch.\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", revision=\"bf16\", dtype=jax.numpy.bfloat16\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n\n### Bias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n### Safety Module\n\nThe intended use of this model is with the [Safety Checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) in Diffusers. \nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the `CLIPTextModel` *after generation* of the images. \nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1-4 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nWe currently provide four checkpoints, which were trained as follows.\n- [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1): 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2): Resumed from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3): Resumed from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2`.225,000 steps at resolution `512x512` on \"laion-aesthetics v2 5+\"  and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-variants-scores.jpg)\n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/CompVis/stable-diffusion-v1-4"}]},{"id":"meta-llama/Meta-Llama-3-8B","name":"Meta-Llama-3-8B","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","license:llama3","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":6364,"downloads":1832499,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Meta-Llama-3-8B"}]},{"id":"hexgrad/Kokoro-82M","name":"Kokoro-82M","description":"A model for text-to-speech.","task":"text-to-speech","tags":["text-to-speech","en","arxiv:2306.07691","arxiv:2203.02395","base_model:yl4579/StyleTTS2-LJSpeech","base_model:finetune:yl4579/StyleTTS2-LJSpeech","doi:10.57967/hf/4329","license:apache-2.0","region:us"],"likes":5243,"downloads":4368458,"readme":"---\nlicense: apache-2.0\nlanguage:\n- en\nbase_model:\n- yl4579/StyleTTS2-LJSpeech\npipeline_tag: text-to-speech\n---\n**Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.\n\n<audio controls><source src=\"https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/samples/HEARME.wav\" type=\"audio/wav\"></audio>\n\n🐈 **GitHub**: https://github.com/hexgrad/kokoro\n\n🚀 **Demo**: https://hf.co/spaces/hexgrad/Kokoro-TTS\n\n> [!NOTE]\n> As of April 2025, the market rate of Kokoro served over API is **under $1 per million characters of text input**, or under $0.06 per hour of audio output. (On average, 1000 characters of input is about 1 minute of output.) Sources: [ArtificialAnalysis/Replicate at 65 cents per M chars](https://artificialanalysis.ai/text-to-speech/model-family/kokoro#price) and [DeepInfra at 80 cents per M chars](https://deepinfra.com/hexgrad/Kokoro-82M).\n>\n> This is an Apache-licensed model, and Kokoro has been deployed in numerous projects and commercial APIs. We welcome the deployment of the model in real use cases.\n\n> [!CAUTION]\n> Fake websites like kokorottsai_com (snapshot: https://archive.ph/nRRnk) and kokorotts_net (snapshot: https://archive.ph/60opa) are likely scams masquerading under the banner of a popular model.\n>\n> Any website containing \"kokoro\" in its root domain (e.g. kokorottsai_com, kokorotts_net) is **NOT owned by and NOT affiliated with this model page or its author**, and attempts to imply otherwise are red flags.\n\n- [Releases](#releases)\n- [Usage](#usage)\n- [EVAL.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/EVAL.md) ↗️\n- [SAMPLES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md) ↗️\n- [VOICES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) ↗️\n- [Model Facts](#model-facts)\n- [Training Details](#training-details)\n- [Creative Commons Attribution](#creative-commons-attribution)\n- [Acknowledgements](#acknowledgements)\n\n### Releases\n\n| Model | Published | Training Data | Langs & Voices | SHA256 |\n| ----- | --------- | ------------- | -------------- | ------ |\n| **v1.0** | **2025 Jan 27** | **Few hundred hrs** | [**8 & 54**](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) | `496dba11` |\n| [v0.19](https://huggingface.co/hexgrad/kLegacy/tree/main/v0.19) | 2024 Dec 25 | <100 hrs | 1 & 10 | `3b0c392f` |\n\n| Training Costs | v0.19 | v1.0 | **Total** |\n| -------------- | ----- | ---- | ----- |\n| in A100 80GB GPU hours | 500 | 500 | **1000** |\n| average hourly rate | $0.80/h | $1.20/h | **$1/h** |\n| in USD | $400 | $600 | **$1000** |\n\n### Usage\nYou can run this basic cell on [Google Colab](https://colab.research.google.com/). [Listen to samples](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md). For more languages and details, see [Advanced Usage](https://github.com/hexgrad/kokoro?tab=readme-ov-file#advanced-usage).\n```py\n!pip install -q kokoro>=0.9.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code='a')\ntext = '''\n[Kokoro](/kˈOkəɹO/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/kˈOkəɹO/) can be deployed anywhere from production environments to personal projects.\n'''\ngenerator = pipeline(text, voice='af_heart')\nfor i, (gs, ps, audio) in enumerate(generator):\n    print(i, gs, ps)\n    display(Audio(data=audio, rate=24000, autoplay=i==0))\n    sf.write(f'{i}.wav', audio, 24000)\n```\nUnder the hood, `kokoro` uses [`misaki`](https://pypi.org/project/misaki/), a G2P library at https://github.com/hexgrad/misaki\n\n### Model Facts\n\n**Architecture:**\n- StyleTTS 2: https://arxiv.org/abs/2306.07691\n- ISTFTNet: https://arxiv.org/abs/2203.02395\n- Decoder only: no diffusion, no encoder release\n\n**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2\n\n**Trained by**: `@rzvzn` on Discord\n\n**Languages:** Multiple\n\n**Model SHA256 Hash:** `496dba118d1a58f5f3db2efc88dbdc216e0483fc89fe6e47ee1f2c53f18ad1e4`\n\n### Training Details\n\n**Data:** Kokoro was trained exclusively on **permissive/non-copyrighted audio data** and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\n- Public domain audio\n- Audio licensed under Apache, MIT, etc\n- Synthetic audio<sup>[1]</sup> generated by closed<sup>[2]</sup> TTS models from large providers<br/>\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf<br/>\n[2] No synthetic audio from open TTS models or \"custom voice clones\"\n\n**Total Dataset Size:** A few hundred hours of audio\n\n**Total Training Cost:** About $1000 for 1000 hours of A100 80GB vRAM\n\n### Creative Commons Attribution\n\nThe following CC BY audio was part of the dataset used to train Kokoro v1.0.\n\n| Audio Data | Duration Used | License | Added to Training Set After |\n| ---------- | ------------- | ------- | --------------------------- |\n| [Koniwa](https://github.com/koniwa/koniwa) `tnc` | <1h | [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/deed.ja) | v0.19 / 22 Nov 2024 |\n| [SIWIS](https://datashare.ed.ac.uk/handle/10283/2353) | <11h | [CC BY 4.0](https://datashare.ed.ac.uk/bitstream/handle/10283/2353/license_text) | v0.19 / 22 Nov 2024 |\n\n### Acknowledgements\n\n- 🛠️ [@yl4579](https://huggingface.co/yl4579) for architecting StyleTTS 2.\n- 🏆 [@Pendrokar](https://huggingface.co/Pendrokar) for adding Kokoro as a contender in the TTS Spaces Arena.\n- 📊 Thank you to everyone who contributed synthetic training data.\n- ❤️ Special thanks to all compute sponsors.\n- 👾 Discord server: https://discord.gg/QuGxSWBfQy\n- 🪽 Kokoro is a Japanese word that translates to \"heart\" or \"spirit\". It is also the name of an [AI in the Terminator franchise](https://terminator.fandom.com/wiki/Kokoro).\n\n<img src=\"https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg\" width=\"400\" alt=\"kokoro\" />\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/hexgrad/Kokoro-82M"}]},{"id":"openai/whisper-large-v3","name":"whisper-large-v3","description":"A model for automatic-speech-recognition.","task":"automatic-speech-recognition","tags":["transformers","pytorch","jax","safetensors","whisper","automatic-speech-recognition","audio","hf-asr-leaderboard","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","license:apache-2.0","endpoints_compatible","region:us"],"likes":5077,"downloads":3994171,"readme":"---\nlanguage: \n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- no\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3 has the same architecture as the previous [large](https://huggingface.co/openai/whisper-large) and [large-v2](https://huggingface.co/openai/whisper-large-v2) \nmodels, except for the following minor differences:\n\n1. The spectrogram input uses 128 Mel frequency bins instead of 80\n2. A new language token for Cantonese\n\nThe Whisper large-v3 model was trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled \naudio collected using Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . The model was trained for 2.0 epochs over this mixture dataset.\n\nThe large-v3 model shows improved performance over a wide variety of languages, showing 10% to 20% reduction of errors \ncompared to Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . For more details on the different checkpoints available, refer to the section [Model details](#model-details).\n\n**Disclaimer**: Content for this model card has partly been written by the 🤗 Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3 is supported in Hugging Face 🤗 Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install 🤗 Datasets to load toy audio dataset from the Hugging Face Hub, and \n🤗 Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 ⚠️\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"high\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result[\"text\"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation=\"flash_attention_2\"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation=\"sdpa\"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v3) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models’ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box – their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai/whisper-large-v3"}]},{"id":"bigscience/bloom","name":"bloom","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","tensorboard","safetensors","bloom","text-generation","ak","ar","as","bm","bn","ca","code","en","es","eu","fon","fr","gu","hi","id","ig","ki","kn","lg","ln","ml","mr","ne","nso","ny","or","pa","pt","rn","rw","sn","st","sw","ta","te","tn","ts","tum","tw","ur","vi","wo","xh","yo","zh","zu","arxiv:2211.05100","arxiv:1909.08053","arxiv:2110.02861","arxiv:2108.12409","doi:10.57967/hf/0003","license:bigscience-bloom-rail-1.0","model-index","co2_eq_emissions","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":4959,"downloads":3685,"readme":"---\nlicense: bigscience-bloom-rail-1.0\nlanguage:\n- ak\n- ar\n- as\n- bm\n- bn\n- ca\n- code\n- en\n- es\n- eu\n- fon\n- fr\n- gu\n- hi\n- id\n- ig\n- ki\n- kn\n- lg\n- ln\n- ml\n- mr\n- ne\n- nso\n- ny\n- or\n- pa\n- pt\n- rn\n- rw\n- sn\n- st\n- sw\n- ta\n- te\n- tn\n- ts\n- tum\n- tw\n- ur\n- vi\n- wo\n- xh\n- yo\n- zh\n- zu\nprogramming_language: \n- C\n- C++\n- C#\n- Go\n- Java\n- JavaScript\n- Lua\n- PHP\n- Python\n- Ruby\n- Rust\n- Scala\n- TypeScript\npipeline_tag: text-generation\nwidget:\n- text: 'A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus. | To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:'\n  example_title: Imaginary word\n  group: English\n- text: 'Un \"whatpu\" est un petit animal à fourrure originaire de Tanzanie. Un exemple de phrase qui utilise le mot whatpu est: Nous étions en Afrique et nous avons vu des whatpus trop mignons. Faire un \"farduddle\" veut dire sauter sur place vraiment vite. Un exemple de phrase qui utilise le mot farduddle est:'\n  example_title: Imaginary word\n  group: French\n- text: 'Un \"whatpu\" es un pequeño animal peludo nativo de Tanzania. Un ejemplo de una oración que usa la palabra whatpu es: Estábamos viajando por África y vimos estos whatpus muy bonitos. Hacer un \"farduddle\" significa saltar arriba y abajo muy rápido. Un ejemplo de una oración que usa la palabra farduddle es:'\n  example_title: Imaginary word\n  group: Spanish\n- text: ' ال\"واتبو\" هو حيوان صغير مكسو بالفراء يعيش في تنزانيا. مثال على جملة تستخدم كلمة واتبو هي: كنا نسافر في افريقيا و رأينا هؤلاء الواتبو اللطفاء. للقيام ب\"فاردادل\" يعني ان تقفز للأعلى و الأسفل بسرعة كبيرة. مثال على جملة تستخدم كلمة فاردادل هي:'\n  example_title: Imaginary word\n  group: Arabic\n- text: 'Um \"whatpu\" é um pequeno animal peludo nativo da Tanzânia. Um exemplo de uma frase que usa a palavra whatpu é: Estávamos a viajar por África e vimos uns whatpus muito queridos. Fazer um \"farduddle\" significa saltar para cima e para baixo muito rápido. Um exemplo de uma frase que usa a palavra farduddle é:'\n  example : Imaginary word\n  group: Portuguese\n- text: Pour déguster un ortolan, il faut tout d'abord\n  example_title: Recipe\n  group: French\n- text: |-\n    34+10=44 \n    54+20=\n  example_title: Addition\n  group: Math\n- text: |-\n    This tool converts irregular verbs to past tense.\n    Arise - Arose\n    Become - Became\n    Forget - Forgot\n    Freeze -\n  example_title: Irregular verbs\n  group: English\n- text: |-\n    Please unscramble the letters into a word, and write that word:\n    r e!c.i p r o.c a/l = reciprocal\n    d.o m i!n a n.t =\n  example_title: Word unscrambling\n  group: English\n- text: |-\n    Estos ejemplos quitan vocales de las palabras\n    Ejemplos:\n    hola - hl\n    manzana - mnzn\n    papas - pps\n    alacran - lcrn\n    papa -\n  example_title: Vowel removal\n  group: Spanish\n- text: |-\n    Traduce español de España a español de Argentina\n    El coche es rojo - el auto es rojo\n    El ordenador es nuevo - la computadora es nueva\n    el boligrafo es negro - lapicera es negra\n    la nevera\n  example_title: Spanish to Argentinian Spanish\n  group: Spanish\n- text: To say \"I love you\" in Hindi, you would say\n  example_title: Translation to Hindi\n  group: English\n- text: To say \"I love you\" in Hindi, you would say\n  example_title: Translation from English\n  group: Hindi\n- text: 'Poor English: She no went to the market. Corrected English:'\n  example_title: Grammar exercise 1 \n  group: English\n- text: 'استخراج العدد العاملي في لغة بايثون:'\n  example_title: Code generation\n  group: Arabic\n- text: 'Regexp. Here is a regular expression to match a word starting with a number and then having only vowels:'\n  example_title: Regular expressions\n  group: English\n- text: |-\n    Do a hello world in different languages:\n    Python: print(\"hello world\")\n    R:\n  example_title: Code generation\n  group: English\n- text: |-\n    Which is the correct preposition? I'm born X July. X is the preposition in\n    He sat X a chair. X is the preposition on\n    She drove X the bridge. X is the preposition\n  example_title: Grammar exercise 2\n  group: English\n- text: |-\n    Traduction en français: Dans cet essai je vais m'interroger sur la conscience des modèles d'intelligence artificielle récents comme les modèles de langue. Pour commencer, je m'intéresserai à la notion de conscience et à ce qui la caractérise. Ensuite, j'aborderai la question de l'intelligence et de son lien avec le langage. Enfin, dans une dernière partie je me pencherai sur le cas de l'IA et sur sa conscience.\n    Traduction en espagnol:\n  example_title: Translation to Spanish\n  group: French\n- text: |-\n    Traducción al francés: Dans cet essai je vais m'interroger sur la conscience des modèles d'intelligence artificielle récents comme les modèles de langue. Pour commencer, je m'intéresserai à la notion de conscience et à ce qui la caractérise. Ensuite, j'aborderai la question de l'intelligence et de son lien avec le langage. Enfin, dans une dernière partie je me pencherai sur le cas de l'IA et sur sa conscience.\n    Traducción al español:\n  example_title: Translation from French\n  group: Spanish\n- text: ذات مرة ، عاش شبل الدب في الغابة\n  example_title: Fairy tale\n  group: Arabic\n- text: एक बार की बात है, जंगल में एक भालू का शावक रहता था\n  example_title: Fairy tale\n  group: Hindi\n- text: Il était une fois une licorne qui vivait\n  example_title: Fairy tale\n  group: French\n- text: |-\n    Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n    A: Let's think step by step.\n  example_title: Mathematical reasoning\n  group: English\n\nco2_eq_emissions:\n  emissions: 24_700_000\n  source: \"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. https://arxiv.org/abs/2211.02001\"\n  training_type: \"pre-training\"\n  geographical_location: \"Orsay, France\"\n  hardware_used: \"384 A100 80GB GPUs\"\n\nmodel-index:\n- name: bloom\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: humaneval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.15542682926829265\n      verified: false\n    - name: pass@10\n      type: pass@10\n      value: 0.3278356276947017\n      verified: false\n    - name: pass@100\n      type: pass@100\n      value: 0.5719815685597749\n      verified: false\n---\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/1657124309515-5f17f0a0925b9863e28ad517.png\" alt=\"BigScience Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\nBigScience Large Open-science Open-access Multilingual Language Model  \nVersion 1.3 / 6 July 2022\n\nCurrent Checkpoint: **Training Iteration  95000**\n\nLink to paper: [here](https://arxiv.org/abs/2211.05100)\n\nTotal seen tokens: **366B**\n\n---\n\n# Model Details  \n\nBLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.\n\n## Basics\n*This section provides information about the model type, version, license, funders, release date, developers, and contact information.*\n*It is useful for anyone who wants to reference the model.*\n\n<details>\n<summary>Click to expand</summary>\n  \n**Developed by:** BigScience ([website](https://bigscience.huggingface.co))\n\n*All collaborators are either volunteers or have an agreement with their employer. (Further breakdown of participants forthcoming.)*\n    \n**Model Type:** Transformer-based Language Model\n\n**Checkpoints format:** `transformers` (Megatron-DeepSpeed format available [here](https://huggingface.co/bigscience/bloom-optimizer-states))\n\n**Version:** 1.0.0\n\n**Languages:** Multiple; see [training data](#training-data)\n\n**License:** RAIL License v1.0 ([link](https://huggingface.co/spaces/bigscience/license) / [article and FAQ](https://bigscience.huggingface.co/blog/the-bigscience-rail-license))\n\n**Release Date Estimate:** Monday, 11.July.2022\n\n**Send Questions to:** bigscience-contact@googlegroups.com\n\n**Cite as:** BigScience, _BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model_. International, May 2021-May 2022\n\n**Funded by:** \n    \n* The French government.\n\n* Hugging Face ([website](https://huggingface.co)).\n\n* Organizations of contributors.  *(Further breakdown of organizations forthcoming.)*\n\n</details>\n\n\n## Technical Specifications\n*This section includes details about the model objective and architecture, and the compute infrastructure.*\n*It is useful for people interested in model development.*\n\n<details>\n<summary>Click to expand</summary>\n\nPlease see [the BLOOM training README](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) for full details on replicating training.\n\n### Model Architecture and Objective\n\n* Modified from Megatron-LM GPT2 (see [paper](https://arxiv.org/abs/1909.08053), [BLOOM Megatron code](https://github.com/bigscience-workshop/Megatron-DeepSpeed)):\n\n* Decoder-only architecture\n\n* Layer normalization applied to word embeddings layer (`StableEmbedding`; see [code](https://github.com/facebookresearch/bitsandbytes), [paper](https://arxiv.org/pdf/2110.02861.pdf))\n\n* ALiBI positional encodings (see [paper](https://arxiv.org/pdf/2108.12409.pdf)), with GeLU activation functions\n\n* 176,247,271,424 parameters:\n\n    * 3,596,615,680 embedding parameters\n\n    * 70 layers, 112 attention heads\n\n    * Hidden layers are 14336-dimensional\n\n    * Sequence length of 2048 tokens used (see [BLOOM tokenizer](https://huggingface.co/bigscience/tokenizer), [tokenizer description](#tokenization))\n\n**Objective Function:** Cross Entropy with mean reduction (see [API documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)).\n    \n### Compute infrastructure\nJean Zay Public Supercomputer, provided by the French government (see [announcement](https://www.enseignementsup-recherche.gouv.fr/fr/signature-du-marche-d-acquisition-de-l-un-des-supercalculateurs-les-plus-puissants-d-europe-46733)).\n\n#### Hardware\n\n* 384 A100 80GB GPUs (48 nodes)\n    \n* Additional 32 A100 80GB GPUs (4 nodes) in reserve\n\n* 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links\n\n* CPU: AMD\n\n* CPU memory: 512GB per node\n\n* GPU memory: 640GB per node\n\n* Inter-node connect: Omni-Path Architecture (OPA)\n\n* NCCL-communications network: a fully dedicated subnet\n\n* Disc IO network: shared network with other types of nodes\n\n#### Software\n\n* Megatron-DeepSpeed ([Github link](https://github.com/bigscience-workshop/Megatron-DeepSpeed))\n\n* DeepSpeed ([Github link](https://github.com/microsoft/DeepSpeed))\n\n* PyTorch (pytorch-1.11 w/ CUDA-11.5; see [Github link](https://github.com/pytorch/pytorch))\n\n* apex ([Github link](https://github.com/NVIDIA/apex))\n    \n</details>\n\n---\n\n# Training\n*This section provides information about the training data, the speed and size of training elements, and the environmental impact of training.*\n*It is useful for people who want to learn more about the model inputs and training footprint.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Training Data\n*This section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.*\n\nDetails for each dataset are provided in individual [Data Cards](https://huggingface.co/spaces/bigscience/BigScienceCorpus), and the sizes of each of their contributions to the aggregated training data are presented in an [Interactive Corpus Map](https://huggingface.co/spaces/bigscience-catalogue-lm-data/corpus-map).\n\nTraining data includes:\n\n-   46 natural languages\n    \n-   13 programming languages\n\n-   In 1.6TB of pre-processed text, converted into 350B unique tokens (see [the tokenizer section](#tokenization) for more.)\n\n### Languages\n    \nThe pie chart shows the distribution of languages in training data.\n   \n![pie chart showing the distribution of languages in training data](https://github.com/bigscience-workshop/model_card/blob/main/assets/data/pie_v2.svg?raw=true)\n\n\nThe following tables shows the further distribution of Niger-Congo & Indic languages and programming languages in the training data.\n\nDistribution of Niger Congo and Indic languages.\n    \n| Niger Congo    | Percentage |         | Indic     | Percentage |\n|----------------|------------| ------  |-----------|------------|\n| Chi Tumbuka    | 0.00002    |         | Assamese  | 0.01       |\n| Kikuyu         | 0.00004    |         | Odia      | 0.04       |\n| Bambara        | 0.00004    |         | Gujarati  | 0.04       |\n| Akan           | 0.00007    |         | Marathi   | 0.05       |\n| Xitsonga       | 0.00007    |         | Punjabi   | 0.05       |\n| Sesotho        | 0.00007    |         | Kannada   | 0.06       |\n| Chi Chewa      | 0.0001     |         | Nepali    | 0.07       |\n| Setswana       | 0.0002     |         | Telugu    | 0.09       |\n| Lingala        | 0.0002     |         | Malayalam | 0.10       |\n| Northern Sotho | 0.0002     |         | Urdu      | 0.10       |\n| Fon            | 0.0002     |         | Tamil     | 0.20       |\n| Kirundi        | 0.0003     |         | Bengali   | 0.50       |\n| Wolof          | 0.0004     |         | Hindi     | 0.70       |\n| Luganda        | 0.0004     |\n| Chi Shona      | 0.001      |\n| Isi Zulu       | 0.001      |\n| Igbo           | 0.001      |\n| Xhosa          | 0.001      |\n| Kinyarwanda    | 0.003      |\n| Yoruba         | 0.006      |\n| Swahili        | 0.02       |\n\nDistribution of programming languages.\n    \n| Extension      | Language   | Number of files |\n|----------------|------------|-----------------|\n| java           | Java       | 5,407,724       |\n| php            | PHP        | 4,942,186       |\n| cpp            | C++        | 2,503,930       |\n| py             | Python     | 2,435,072       |\n| js             | JavaScript | 1,905,518       |\n| cs             | C#         | 1,577,347       |\n| rb             | Ruby       | 6,78,413        |\n| cc             | C++        | 443,054         |\n| hpp            | C++        | 391,048         |\n| lua            | Lua        | 352,317         |\n| go             | GO         | 227,763         |\n| ts             | TypeScript | 195,254         |\n| C              | C          | 134,537         |\n| scala          | Scala      | 92,052          |\n| hh             | C++        | 67,161          |\n| H              | C++        | 55,899          |\n| tsx            | TypeScript | 33,107          |\n| rs             | Rust       | 29,693          |\n| phpt           | PHP        | 9,702           |\n| c++            | C++        | 1,342           |\n| h++            | C++        | 791             |\n| php3           | PHP        | 540             |\n| phps           | PHP        | 270             |\n| php5           | PHP        | 166             |\n| php4           | PHP        | 29              |\n    \n### Preprocessing\n\n**Tokenization:** The BLOOM tokenizer ([link](https://huggingface.co/bigscience/tokenizer)), a learned subword tokenizer trained using:\n    \n- A byte-level Byte Pair Encoding (BPE) algorithm \n\n- A simple pre-tokenization rule, no normalization\n\n- A vocabulary size of 250,680\n\nIt was trained on a subset of a preliminary version of the corpus using alpha-weighting per language.  \n\n## Speeds, Sizes, Times\n\nTraining logs: [Tensorboard link](https://huggingface.co/tensorboard/bigscience/tr11-176B-ml-logs/)\n\n- Dates:\n    \n    - Started 11th March, 2022 11:42am PST\n\n    - Estimated end: 5th July, 2022\n\n- Checkpoint size:\n    \n    - Bf16 weights: 329GB\n    \n    - Full checkpoint with optimizer states: 2.3TB\n\n- Training throughput: About 150 TFLOP per GPU per second\n\n- Number of epochs: 1\n\n- Estimated cost of training: Equivalent of $2-5M in cloud computing (including preliminary experiments)\n\n- Server training location: Île-de-France, France\n\n\n## Environmental Impact\n\nThe training supercomputer, Jean Zay ([website](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html)), uses mostly nuclear energy. The heat generated by it is reused for heating campus housing.\n    \n**Estimated carbon emissions:**  *(Forthcoming.)*\n    \n**Estimated electricity usage:** *(Forthcoming.)*\n\n</details>\n\n---\n\n# Uses\n\n*This section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model.*\n*It is useful for anyone considering using the model or who is affected by the model.*\n\n<details>\n<summary>Click to expand</summary>\n    \n## How to use\n\nThis model can be easily used and deployed using HuggingFace's ecosystem. This needs `transformers` and `accelerate` installed. The model can be downloaded as follows:\n\n <img src=\"https://s3.amazonaws.com/moonup/production/uploads/1657271608456-62441d1d9fdefb55a0b7d12c.png\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n## Intended Use\n\nThis model is being created in order to enable public research on large language models (LLMs). LLMs are intended to be used for language generation or as a pretrained base model that can be further fine-tuned for specific tasks. Use cases below are not exhaustive.\n\n### Direct Use\n\n-   Text generation\n\n-   Exploring characteristics of language generated by a language model\n\n    -   Examples: Cloze tests, counterfactuals, generations with reframings\n\n### Downstream Use\n\n-   Tasks that leverage language models include: Information Extraction, Question Answering, Summarization\n\n### Misuse and Out-of-scope Use\n*This section addresses what users ought not do with the model.*\n\nSee the [BLOOM License](https://huggingface.co/spaces/bigscience/license), Attachment A, for detailed usage restrictions. The below list is non-exhaustive, but lists some easily foreseeable problematic use cases.\n\n#### Out-of-scope Uses\n\nUsing the model in [high-stakes](#high-stakes) settings is out of scope for this model.  The model is not designed for [critical decisions](#critical-decisions) nor uses with any material consequences on an individual's livelihood or wellbeing. The model outputs content that appears factual but may not be correct.  \n\nOut-of-scope Uses Include:\n\n-   Usage in biomedical domains, political and legal domains, or finance domains\n\n-   Usage for evaluating or scoring individuals, such as for employment, education, or credit\n\n-   Applying the model for critical automatic decisions, generating factual content, creating reliable summaries, or generating predictions that must be correct\n\n#### Misuse\n\nIntentionally using the model for harm, violating [human rights](#human-rights), or other kinds of malicious activities, is a misuse of this model. This includes:\n\n-   Spam generation\n\n-   Disinformation and influence operations\n\n-   Disparagement and defamation\n\n-   Harassment and abuse\n  \n-   [Deception](#deception)\n\n-   Unconsented impersonation and imitation\n\n-   Unconsented surveillance \n\n-   Generating content without attribution to the model, as specified in the [RAIL License, Use Restrictions](https://huggingface.co/spaces/bigscience/license)\n\n## Intended Users\n\n### Direct Users\n\n-   General Public\n\n-   Researchers\n\n-   Students\n\n-   Educators\n\n-   Engineers/developers\n\n-   Non-commercial entities\n\n-   Community advocates, including human and civil rights groups\n\n### Indirect Users\n\n-   Users of derivatives created by Direct Users, such as those using software with an [intended use](#intended-use)\n\n-   Users of [Derivatives of the Model, as described in the License](https://huggingface.co/spaces/bigscience/license)\n\n### Others Affected (Parties Prenantes)\n\n-   People and groups referred to by the LLM\n\n-   People and groups exposed to outputs of, or decisions based on, the LLM\n\n-   People and groups whose original work is included in the LLM\n\n</details>\n\n---\n\n# Risks and Limitations\n*This section identifies foreseeable harms and misunderstandings.*\n    \n<details>\n<summary>Click to expand</summary>\n\nModel may:\n\n-   Overrepresent some viewpoints and underrepresent others\n\n-   Contain stereotypes\n  \n-   Contain [personal information](#personal-data-and-information)\n\n-   Generate:\n\n    -   Hateful, abusive, or violent language\n\n    -   Discriminatory or prejudicial language\n\n    -   Content that may not be appropriate for all settings, including sexual content\n\n-   Make errors, including producing incorrect information as if it were factual\n\n-   Generate irrelevant or repetitive outputs\n\n-   Induce users into attributing human traits to it, such as sentience or consciousness\n\n</details>\n\n---\n\n# Evaluation\n*This section describes the evaluation protocols and provides the results.*\n\n\n<details>\n<summary>Click to expand</summary>\n\n## Metrics \n*This section describes the different ways performance is calculated and why.*\n\nIncludes:\n\n| Metric             | Why chosen                                                         |\n|--------------------|--------------------------------------------------------------------|\n| [Perplexity](#perplexity)         | Standard metric for quantifying model improvements during training |\n| Cross Entropy [Loss](#loss) | Standard objective for language models.                            |\n\nAnd multiple different metrics for specific tasks. _(More evaluation metrics forthcoming upon completion of evaluation protocol.)_\n\n## Factors \n*This section lists some different aspects of BLOOM models. Its focus is on aspects that are likely to give rise to high variance in model behavior.*\n\n- Language, such as English or Yoruba\n\n- Domain, such as newswire or stories\n\n- Demographic characteristics, such as gender or nationality\n\n##  Results\n*Results are based on the [Factors](#factors) and [Metrics](#metrics).*\n\n**Zero-shot evaluations:**\n\n<span style=\"color:red\"><b>WARNING:</b> This section used to contain much more results, however they were not correct and we released without the approval of the evaluation working group. We are currently in the process of fixing the evaluations.</span>\n\nSee this repository for JSON files: https://github.com/bigscience-workshop/evaluation-results\n\n| Task | Language | Metric | BLOOM-176B | OPT-175B* |\n|:--------|:-----------------|:------------------------|-------------:|------------:|\n| humaneval | python | pass@1 ↑ | 0.155 | 0.0 |\n| humaneval | python | pass@10 ↑ | 0.328 | 0.0 |\n| humaneval | python | pass@100 ↑ | 0.572 | 0.003 |\n\n\n**Train-time Evaluation:**\n\nFinal checkpoint after 95K steps:\n\n- Training Loss: 1.939\n\n- Validation Loss: 2.061\n\n- Perplexity: 7.045\n\nFor more see: https://huggingface.co/bigscience/tr11-176B-ml-logs\n\n</details>\n\n---\n\n# Recommendations\n\n*This section provides information on warnings and potential mitigations.*\n\n<details>\n<summary>Click to expand</summary>\n\n-   Indirect users should be made aware when the content they're working with is created by the LLM.\n\n-   Users should be aware of [Risks and Limitations](#risks-and-limitations), and include an appropriate age disclaimer or blocking interface as necessary.\n\n-   Models trained or finetuned downstream of BLOOM LM should include an updated Model Card.\n\n-   Users of the model should provide mechanisms for those affected to provide feedback, such as an email address for comments.\n\n</details>\n\n---\n\n# Glossary and Calculations\n\n*This section defines common terms and how metrics are calculated.*\n<details>\n<summary>Click to expand</summary>\n\n-   <a name=\"loss\">**Loss:**</a> A calculation of the difference between what the model has learned and what the data shows (\"groundtruth\"). The lower the loss, the better. The training process aims to minimize the loss. \n\n-   <a name=\"perplexity\">**Perplexity:**</a> This is based on what the model estimates the probability of new data is. The lower the perplexity, the better.  If the model is 100% correct at predicting the next token it will see, then the perplexity is 1. Mathematically this is calculated using entropy. \n\n-   <a name=\"high-stakes\">**High-stakes settings:**</a> Such as those identified as \"high-risk AI systems\" and \"unacceptable risk AI systems\" in the European Union's proposed [Artificial Intelligence (AI) Act](https://artificialintelligenceact.eu/annexes/).\n\n-   <a name=\"critical-decisions\">**Critical decisions:**</a> Such as those defined in [the United States' proposed Algorithmic Accountability Act](https://www.congress.gov/117/bills/s3572/BILLS-117s3572is.pdf).\n\n-   <a name=\"human-rights\">**Human rights:**</a> Includes those rights defined in the [Universal Declaration of Human Rights](https://www.un.org/sites/un2.un.org/files/2021/03/udhr.pdf).\n\n-  <a name=\"personal-data-and-information\">**Personal Data and Personal Information:**</a> Personal data and information is defined in multiple data protection regulations, such as \"[personal data](https://gdpr-info.eu/issues/personal-data/)\" in the [European Union's General Data Protection Regulation](https://gdpr-info.eu); and \"personal information\" in the Republic of South Africa's [Protection of Personal Information Act](https://www.gov.za/sites/default/files/gcis_document/201409/3706726-11act4of2013popi.pdf), The People's Republic of China's [Personal information protection law](http://en.npc.gov.cn.cdurl.cn/2021-12/29/c_694559.htm).\n  \n- <a name=\"sensitive-characteristics\">**Sensitive characteristics:**</a> This includes specifically protected categories in human rights (see [UHDR, Article 2](https://www.un.org/sites/un2.un.org/files/2021/03/udhr.pdf)) and personal information regulation (see GDPR, [Article 9; Protection of Personal Information Act, Chapter 1](https://www.gov.za/sites/default/files/gcis_document/201409/3706726-11act4of2013popi.pdf))\n\n- <a name=\"deception\">**Deception:**</a> Doing something to intentionally mislead individuals to believe something that is false, such as by creating deadbots or chatbots on social media posing as real people, or generating text documents without making consumers aware that the text is machine generated.\n\n</details>\n\n---\n\n# More Information\n*This section provides links to writing on dataset creation, technical specifications, lessons learned, and initial results.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Intermediate checkpoints\n\nFor academic (or any) usage, we published the intermediate checkpoints, corresponding to the model state at each 5000 steps. Please follow [this link](https://huggingface.co/bigscience/bloom-176-intermediate) to get these checkpoints.\n\n    \n## Dataset Creation\n\nBlog post detailing the design choices during the dataset creation: https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling\n\n## Technical Specifications\n\nBlog post summarizing how the architecture, size, shape, and pre-training duration where selected: https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours\n\nMore details on the architecture/optimizer: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\n\nBlog post on the hardware/engineering side: https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model\n\nDetails on the distributed setup used for the training: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\n\nTensorboard updated during the training: https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss\n\n## Lessons\n\nInsights on how to approach training, negative results: https://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.md\n\nDetails on the obstacles overcome during the preparation on the engineering side (instabilities, optimization of training throughput, so many technical tricks and questions): https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md\n\n## Initial Results\n\nInitial prompting experiments using interim checkpoints: https://huggingface.co/spaces/bigscience/bloom-book\n\n</details>\n\n\n## Original checkpoints\n\nThe checkpoints in this repo correspond to the HuggingFace Transformers format. If you want to use our fork of [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) that the model was trained with, you'd want to use [this repo instead](https://huggingface.co/bigscience/bloom-optimizer-states).\n\nMany intermediate checkpoints are available at https://huggingface.co/bigscience/bloom-intermediate/\n\n---\n    \n# Model Card Authors\n*Ordered roughly chronologically and by amount of time spent on creating this model card.*\n\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/bigscience/bloom"}]},{"id":"meta-llama/Llama-3.1-8B-Instruct","name":"Llama-3.1-8B-Instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","base_model:meta-llama/Llama-3.1-8B","base_model:finetune:meta-llama/Llama-3.1-8B","license:llama3.1","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":4894,"downloads":4932517,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"}]},{"id":"stabilityai/stable-diffusion-3-medium","name":"stable-diffusion-3-medium","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusion-single-file","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","region:us"],"likes":4865,"downloads":12029,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-diffusion-3-medium"}]},{"id":"meta-llama/Llama-2-7b-chat-hf","name":"Llama-2-7b-chat-hf","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","conversational","en","arxiv:2307.09288","license:llama2","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":4640,"downloads":366593,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"}]},{"id":"mistralai/Mixtral-8x7B-Instruct-v0.1","name":"Mixtral-8x7B-Instruct-v0.1","description":"A model for various tasks.","task":"N/A","tags":["vllm","safetensors","mixtral","fr","it","de","es","en","base_model:mistralai/Mixtral-8x7B-v0.1","base_model:finetune:mistralai/Mixtral-8x7B-v0.1","license:apache-2.0","region:us"],"likes":4593,"downloads":568687,"readme":"---\nlibrary_name: vllm\nlanguage:\n- fr\n- it\n- de\n- es\n- en\nlicense: apache-2.0\nbase_model: mistralai/Mixtral-8x7B-v0.1\ninference: false\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- vllm\n---\n# Model Card for Mixtral-8x7B\n\n### Tokenization with `mistral-common`\n\n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mistral-common reference implementation are very welcome!\n     \n        \n---\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\n\nFor full details of this model please read our [release blog post](https://mistral.ai/news/mixtral-of-experts/).\n\n## Warning\nThis repo contains weights that are compatible with [vLLM](https://github.com/vllm-project/vllm) serving of the model as well as Hugging Face [transformers](https://github.com/huggingface/transformers) library. It is based on the original Mixtral [torrent release](magnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&dn=mixtral-8x7b-32kseqlen&tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce), but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.\n\n## Instruction format\n\nThis format must be strictly respected, otherwise the model will generate sub-optimal outputs.\n\nThe template used to build a prompt for the Instruct model is defined as follows:\n```\n<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n```\nNote that `<s>` and `</s>` are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.\n\nAs reference, here is the pseudo-code used to tokenize instructions during fine-tuning:\n```python\ndef tokenize(text):\n    return tok.encode(text, add_special_tokens=False)\n\n[BOS_ID] + \ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_1) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\n…\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_N) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\n```\n\nIn the pseudo-code above, note that the `tokenize` method should not add a BOS or EOS token automatically, but should add a prefix space. \n\nIn the Transformers library, one can use [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating) which make sure the right format is applied.\n\n## Run the model\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\n\n### In half-precision\n\nNote `float16` precision only works on GPU devices\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Lower precision using (8-bit & 4-bit) using `bitsandbytes`\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n\ntext = \"Hello my name is\"\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Load the model with Flash Attention 2\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, device_map=\"auto\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n## Limitations\n\nThe Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n# The Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"}]},{"id":"meta-llama/Llama-2-7b","name":"Llama-2-7b","description":"A model for text-generation.","task":"text-generation","tags":["facebook","meta","pytorch","llama","llama-2","text-generation","en","arxiv:2307.09288","license:llama2","region:us"],"likes":4421,"downloads":530,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-2-7b"}]},{"id":"black-forest-labs/FLUX.1-schnell","name":"FLUX.1-schnell","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","text-to-image","image-generation","flux","en","license:apache-2.0","endpoints_compatible","diffusers:FluxPipeline","region:us"],"likes":4379,"downloads":908020,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/black-forest-labs/FLUX.1-schnell"}]},{"id":"meta-llama/Meta-Llama-3-8B-Instruct","name":"Meta-Llama-3-8B-Instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","license:llama3","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":4270,"downloads":1051969,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"}]},{"id":"openai/gpt-oss-120b","name":"gpt-oss-120b","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","gpt_oss","text-generation","vllm","conversational","arxiv:2508.10925","license:apache-2.0","autotrain_compatible","endpoints_compatible","8-bit","mxfp4","region:us"],"likes":4124,"downloads":3786599,"readme":"---\nlicense: apache-2.0\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- vllm\n---\n\n<p align=\"center\">\n  <img alt=\"gpt-oss-120b\" src=\"https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://gpt-oss.com\"><strong>Try gpt-oss</strong></a> ·\n  <a href=\"https://cookbook.openai.com/topic/gpt-oss\"><strong>Guides</strong></a> ·\n  <a href=\"https://arxiv.org/abs/2508.10925\"><strong>Model card</strong></a> ·\n  <a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI’s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe’re releasing two flavors of these open models:\n- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the larger `gpt-oss-120b` model. Check out [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) for the smaller model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs. It’s not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models’ native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-120b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-120b\nhuggingface-cli download openai/gpt-oss-120b --include \"original/*\" --local-dir gpt-oss-120b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) can even be fine-tuned on consumer hardware.\n\n# Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai/gpt-oss-120b"}]},{"id":"sentence-transformers/all-MiniLM-L6-v2","name":"all-MiniLM-L6-v2","description":"A model for sentence-similarity.","task":"sentence-similarity","tags":["sentence-transformers","pytorch","tf","rust","onnx","safetensors","openvino","bert","feature-extraction","sentence-similarity","transformers","en","dataset:s2orc","dataset:flax-sentence-embeddings/stackexchange_xml","dataset:ms_marco","dataset:gooaq","dataset:yahoo_answers_topics","dataset:code_search_net","dataset:search_qa","dataset:eli5","dataset:snli","dataset:multi_nli","dataset:wikihow","dataset:natural_questions","dataset:trivia_qa","dataset:embedding-data/sentence-compression","dataset:embedding-data/flickr30k-captions","dataset:embedding-data/altlex","dataset:embedding-data/simple-wiki","dataset:embedding-data/QQP","dataset:embedding-data/SPECTER","dataset:embedding-data/PAQ_pairs","dataset:embedding-data/WikiAnswers","arxiv:1904.06472","arxiv:2102.07033","arxiv:2104.08727","arxiv:1704.05179","arxiv:1810.09305","license:apache-2.0","autotrain_compatible","text-embeddings-inference","endpoints_compatible","region:us"],"likes":4094,"downloads":138081386,"readme":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"}]},{"id":"stabilityai/stable-diffusion-2-1","name":"stable-diffusion-2-1","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","text-to-image","arxiv:2112.10752","arxiv:2202.00512","arxiv:1910.09700","license:openrail++","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":4036,"downloads":656721,"readme":"---\nlicense: openrail++\ntags:\n- stable-diffusion\n- text-to-image\npinned: true\n---\n\n# Stable Diffusion v2-1 Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available [here](https://github.com/Stability-AI/stablediffusion).\n\nThis `stable-diffusion-2-1` model is fine-tuned from [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) (`768-v-ema.ckpt`) with an additional 55k steps on the same dataset (with `punsafe=0.1`), and then fine-tuned for another 155k extra steps with `punsafe=0.98`.\n\n- Use it with the [`stablediffusion`](https://github.com/Stability-AI/stablediffusion) repository: download the `v2-1_768-ema-pruned.ckpt` [here](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.ckpt).\n- Use it with 🧨 [`diffusers`](#examples)\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [CreativeML Open RAIL++-M License](https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/LICENSE-MODEL)\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([OpenCLIP-ViT/H](https://github.com/mlfoundations/open_clip)).\n- **Resources for more information:** [GitHub Repository](https://github.com/Stability-AI/).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n\n## Examples\n\nUsing the [🤗's Diffusers library](https://github.com/huggingface/diffusers) to run Stable Diffusion 2 in a simple and efficient manner.\n\n```bash\npip install diffusers transformers accelerate scipy safetensors\n```\nRunning the pipeline (if you don't swap the scheduler it will run with the default DDIM, in this example we are swapping it to DPMSolverMultistepScheduler):\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n\nmodel_id = \"stabilityai/stable-diffusion-2-1\"\n\n# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\n**Notes**:\n- Despite not being a dependency, we highly recommend you to install [xformers](https://github.com/facebookresearch/xformers) for memory efficient attention (better performance)\n- If you have low GPU RAM available, make sure to add a `pipe.enable_attention_slicing()` after sending it to `cuda` for less VRAM usage (to the cost of speed)\n\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is originally taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2_.\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a subset of the large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\n\n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's [NeurIPS 2022](https://openreview.net/forum?id=M3Y74vmsMcY) paper and reviewer discussions on the topic.\n\n**Training Procedure**\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through the OpenCLIP-ViT/H text-encoder.\n- The output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called _v-objective_, see https://arxiv.org/abs/2202.00512.\n\nWe currently provide the following checkpoints:\n\n- `512-base-ema.ckpt`: 550k steps at resolution `256x256` on a subset of [LAION-5B](https://laion.ai/blog/laion-5b/) filtered for explicit pornographic material, using the [LAION-NSFW classifier](https://github.com/LAION-AI/CLIP-based-NSFW-Detector) with `punsafe=0.1` and an [aesthetic score](https://github.com/christophschuhmann/improved-aesthetic-predictor) >= `4.5`.\n  850k steps at resolution `512x512` on the same dataset with resolution `>= 512x512`.\n- `768-v-ema.ckpt`: Resumed from `512-base-ema.ckpt` and trained for 150k steps using a [v-objective](https://arxiv.org/abs/2202.00512) on the same dataset. Resumed for another 140k steps on a `768x768` subset of our dataset.\n- `512-depth-ema.ckpt`: Resumed from `512-base-ema.ckpt` and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by [MiDaS](https://github.com/isl-org/MiDaS) (`dpt_hybrid`) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n- `512-inpainting-ema.ckpt`: Resumed from `512-base-ema.ckpt` and trained for another 200k steps. Follows the mask-generation strategy presented in [LAMA](https://github.com/saic-mdal/lama) which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the [1.5-inpainting checkpoint](https://huggingface.co/runwayml/stable-diffusion-inpainting).\n- `x4-upscaling-ema.ckpt`: Trained for 1.25M steps on a 10M subset of LAION containing images `>2048x2048`. The model was trained on crops of size `512x512` and is a text-guided [latent upscaling diffusion model](https://arxiv.org/abs/2112.10752).\nIn addition to the textual input, it receives a `noise_level` as an input parameter, which can be used to add noise to the low-resolution input according to a [predefined diffusion schedule](configs/stable-diffusion/x4-upscaling.yaml). \n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 1\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\n\n![pareto](model-variants.jpg) \n\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 200000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 15000 kg CO2 eq.\n\n## Citation\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n\n*This model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md) and [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-diffusion-2-1"}]},{"id":"mistralai/Mistral-7B-v0.1","name":"Mistral-7B-v0.1","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","mistral","text-generation","pretrained","mistral-common","en","arxiv:2310.06825","license:apache-2.0","autotrain_compatible","text-generation-inference","region:us"],"likes":4000,"downloads":575523,"readme":"---\nlibrary_name: transformers\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- pretrained\n- mistral-common\ninference: false\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-v0.1\n\nThe Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. \nMistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n\n## Model Architecture\n\nMistral-7B-v0.1 is a transformer model, with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## Troubleshooting\n\n- If you see the following error:\n```\nKeyError: 'mistral'\n```\n- Or:\n```\nNotImplementedError: Cannot copy out of meta tensor; no data!\n```\n\nEnsure you are utilizing a stable version of Transformers, 4.34.0 or newer.\n\n## Notice\n\nMistral 7B is a pretrained base model and therefore does not have any moderation mechanisms.\n\n## The Mistral AI Team\n \nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mistral-7B-v0.1"}]},{"id":"deepseek-ai/DeepSeek-V3","name":"DeepSeek-V3","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","autotrain_compatible","text-generation-inference","endpoints_compatible","fp8","region:us"],"likes":3993,"downloads":156315,"readme":"---\nlibrary_name: transformers\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.png\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align=\"center\">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/niah.png\">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align=\"center\">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align=\"center\">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface's Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n```\n\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-V3"}]},{"id":"lllyasviel/ControlNet-v1-1","name":"ControlNet-v1-1","description":"A model for various tasks.","task":"N/A","tags":["license:openrail","region:us"],"likes":3941,"downloads":0,"readme":"---\nlicense: openrail\n---\n\nThis is the model files for [ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly).\nThis model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/lllyasviel/ControlNet-v1-1"}]},{"id":"openai/gpt-oss-20b","name":"gpt-oss-20b","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","gpt_oss","text-generation","vllm","conversational","arxiv:2508.10925","license:apache-2.0","autotrain_compatible","endpoints_compatible","8-bit","mxfp4","region:us"],"likes":3887,"downloads":4487346,"readme":"---\nlicense: apache-2.0\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- vllm\n---\n\n<p align=\"center\">\n  <img alt=\"gpt-oss-20b\" src=\"https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-20b.svg\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://gpt-oss.com\"><strong>Try gpt-oss</strong></a> ·\n  <a href=\"https://cookbook.openai.com/topic/gpt-oss\"><strong>Guides</strong></a> ·\n  <a href=\"https://arxiv.org/abs/2508.10925\"><strong>Model card</strong></a> ·\n  <a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI’s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe’re releasing two flavors of these open models:\n- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the smaller `gpt-oss-20b` model. Check out [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) for the larger model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs. It’s not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models’ native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-20b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-20b\nollama pull gpt-oss:20b\nollama run gpt-oss:20b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-20b\nlms get openai/gpt-oss-20b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-20b\nhuggingface-cli download openai/gpt-oss-20b --include \"original/*\" --local-dir gpt-oss-20b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis smaller model `gpt-oss-20b` can be fine-tuned on consumer hardware, whereas the larger [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) can be fine-tuned on a single H100 node.\n\n# Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai/gpt-oss-20b"}]},{"id":"WarriorMama777/OrangeMixs","name":"OrangeMixs","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","stable-diffusion","text-to-image","dataset:Nerfgun3/bad_prompt","license:creativeml-openrail-m","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":3884,"downloads":2127,"readme":"---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\ndatasets: Nerfgun3/bad_prompt\n---\n\n\n----\n\n# OrangeMixs\n\n\"OrangeMixs\" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.\n&nbsp;\n<img src=\"https://i.imgur.com/VZg0LqQ.png\"  width=\"1000\" height=\"\">\n\nMaintain a repository for the following purposes.\n\n1. to provide easy access to models commonly used in the Japanese community.The Wisdom of the Anons💎\n2. As a place to upload my merge models when I feel like it.\n\n\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/img_general/img_orangemixs_infograph_4_comp001.webp \"image_orangemixs_infographics_03\")\n<span style=\"font-size: 60%;\">Hero image prompts(AOM3B2):https://majinai.art/ja/i/jhw20Z_</span>\n\n----\n\n# UPDATE NOTE / How to read this README\n\n## How to read this README\n\n1. Read the ToC as release notes.  \nSections are in descending order. The order within the section is ascending. It is written like SNS.\n2. UPDATE NOTE\n3. View the repository history when you need to check the full history.\n\n## UPDATE NOTE\n- 2023-02-27: Add AOM3A1B\n- 2023-03-10: Model name fix\nI found that I abbreviated the model name too much, so that when users see illustrations using OrangeMixs models on the web, they cannot reach them in their searches.\nTo make the specification more search engine friendly, I renamed it to \"ModelName + (orangemixs)\".\n- 2023-03-11: Change model name : () to _\nChanged to _ because an error occurs when using () in the Cloud environment(e.g.:paperspace).\n\"ModelName + _orangemixs\"\n- 2023-04-01: Added description of AOM3A1 cursed by Dreamlike\n- 2023-06-27: Added AOM3B2. Removed Terms of Service.\n- 2023-11-25: Add VividOrangeMix (nonlabel, NSFW, Hard)\n- 2023-06-27: Added AOM3B2. Removed Terms of Service.\n- 2023-11-25: Add VividOrangeMix (nonlabel, NSFW, Hard)\n- 2024-01-07: Fix repo & Done upload VividOrangeMixs\n\n----\n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run OrangeMixs:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/webui-orangemixs)\n\n----\n\n# Table of Contents\n\n- [OrangeMixs](#orangemixs)\n- [UPDATE NOTE / How to read this README](#update-note--how-to-read-this-readme)\n  - [How to read this README](#how-to-read-this-readme)\n  - [UPDATE NOTE](#update-note)\n- [Gradio](#gradio)\n- [Table of Contents](#table-of-contents)\n- [Reference](#reference)\n- [Licence](#licence)\n- [~~Terms of use~~](#terms-of-use)\n- [Disclaimer](#disclaimer)\n- [How to download](#how-to-download)\n  - [Batch Download](#batch-download)\n  - [Batch Download (Advanced)](#batch-download-advanced)\n  - [Select and download](#select-and-download)\n- [Model Detail \\& Merge Recipes](#model-detail--merge-recipes)\n  - [VividOrangeMix (VOM)](#vividorangemix-vom)\n    - [VividOrangeMix](#vividorangemix)\n    - [VividOrangeMix\\_NSFW / Hard](#vividorangemix_nsfw--hard)\n    - [Instructions](#instructions)\n  - [AbyssOrangeMix3 (AOM3)](#abyssorangemix3-aom3)\n    - [About](#about)\n    - [More feature](#more-feature)\n    - [Variations / Sample Gallery](#variations--sample-gallery)\n      - [AOM3](#aom3)\n      - [AOM3A1](#aom3a1)\n      - [AOM3A2](#aom3a2)\n      - [AOM3A3](#aom3a3)\n      - [AOM3A1B](#aom3a1b)\n      - [AOM3B2](#aom3b2)\n      - [AOM3B3](#aom3b3)\n      - [AOM3B4](#aom3b4)\n      - [AOM3B3](#aom3b3-1)\n      - [AOM3B4](#aom3b4-1)\n    - [Description for enthusiast](#description-for-enthusiast)\n  - [AbyssOrangeMix2 (AOM2)](#abyssorangemix2-aom2)\n    - [AbyssOrangeMix2\\_sfw (AOM2s)](#abyssorangemix2_sfw-aom2s)\n    - [AbyssOrangeMix2\\_nsfw (AOM2n)](#abyssorangemix2_nsfw-aom2n)\n    - [AbyssOrangeMix2\\_hard (AOM2h)](#abyssorangemix2_hard-aom2h)\n  - [EerieOrangeMix (EOM)](#eerieorangemix-eom)\n    - [EerieOrangeMix (EOM1)](#eerieorangemix-eom1)\n      - [EerieOrangeMix\\_base (EOM1b)](#eerieorangemix_base-eom1b)\n      - [EerieOrangeMix\\_Night (EOM1n)](#eerieorangemix_night-eom1n)\n      - [EerieOrangeMix\\_half (EOM1h)](#eerieorangemix_half-eom1h)\n      - [EerieOrangeMix (EOM1)](#eerieorangemix-eom1-1)\n    - [EerieOrangeMix2 (EOM2)](#eerieorangemix2-eom2)\n      - [EerieOrangeMix2\\_base (EOM2b)](#eerieorangemix2_base-eom2b)\n      - [EerieOrangeMix2\\_night (EOM2n)](#eerieorangemix2_night-eom2n)\n      - [EerieOrangeMix2\\_half (EOM2h)](#eerieorangemix2_half-eom2h)\n      - [EerieOrangeMix2 (EOM2)](#eerieorangemix2-eom2-1)\n    - [Models Comparison](#models-comparison)\n  - [AbyssOrangeMix (AOM)](#abyssorangemix-aom)\n    - [AbyssOrangeMix\\_base (AOMb)](#abyssorangemix_base-aomb)\n    - [AbyssOrangeMix\\_Night (AOMn)](#abyssorangemix_night-aomn)\n    - [AbyssOrangeMix\\_half (AOMh)](#abyssorangemix_half-aomh)\n    - [AbyssOrangeMix (AOM)](#abyssorangemix-aom-1)\n  - [ElyOrangeMix (ELOM)](#elyorangemix-elom)\n    - [ElyOrangeMix (ELOM)](#elyorangemix-elom-1)\n    - [ElyOrangeMix\\_half (ELOMh)](#elyorangemix_half-elomh)\n    - [ElyNightOrangeMix (ELOMn)](#elynightorangemix-elomn)\n  - [BloodOrangeMix (BOM)](#bloodorangemix-bom)\n    - [BloodOrangeMix (BOM)](#bloodorangemix-bom-1)\n    - [BloodOrangeMix\\_half (BOMh)](#bloodorangemix_half-bomh)\n    - [BloodNightOrangeMix (BOMn)](#bloodnightorangemix-bomn)\n  - [ElderOrangeMix](#elderorangemix)\n  - [Troubleshooting](#troubleshooting)\n  - [FAQ and Tips (🐈MEME ZONE🦐)](#faq-and-tips-meme-zone)\n\n\n\n----\n\n# Reference\n\n+/hdg/ Stable Diffusion Models Cookbook - <https://rentry.org/hdgrecipes#g-anons-unnamed-mix-e93c3bf7>\nModel names are named after Cookbook precedents🍊\n\n# Licence\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: \n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here ：https://huggingface.co/spaces/CompVis/stable-diffusion-license\n\n# ~~Terms of use~~\n\n~~- **Clearly indicate where modifications have been made.**  \nIf you used it for merging, please state what steps you took to do so.~~\n\nRemoved terms of use. 2023-06-28  \nFreedom. If you share your recipes, Marge swamp will be fun.\n\n# Disclaimer\n\n<details><summary>READ MORE: Disclaimer</summary>\nThe user has complete control over whether or not to generate NSFW content, and the user's decision to enjoy either SFW or NSFW is entirely up to the user.The learning model does not contain any obscene visual content that can be viewed with a single click.The posting of the Learning Model is not intended to display obscene material in a public place.  \nIn publishing examples of the generation of copyrighted characters, I consider the following cases to be exceptional cases in which unauthorised use is permitted. \n\"when the use is for private use or research purposes; when the work is used as material for merchandising (however, this does not apply when the main use of the work is to be merchandised); when the work is used in criticism, commentary or news reporting; when the work is used as a parody or derivative work to demonstrate originality.\"\nIn these cases, use against the will of the copyright holder or use for unjustified gain should still be avoided, and if a complaint is lodged by the copyright holder, it is guaranteed that the publication will be stopped as soon as possible.  \nI would also like to note that I am aware of the fact that many of the merged models use NAI, which is learned from Danbooru and other sites that could be interpreted as illegal, and whose model data itself is also a leak, and that this should be watched carefully. I believe that the best we can do is to expand the possibilities of GenerativeAI while protecting the works of illustrators and artists.  \n</details>\n\n\n----\n\n# How to download\n\n## Batch Download\n\n⚠Deprecated: Orange has grown too huge. Doing this will kill your storage.\n\n1. install Git\n2. create a folder of your choice and right click → \"Git bash here\" and open a gitbash on the folder's directory.\n3. run the following commands in order.\n\n```\ngit lfs install\ngit clone https://huggingface.co/WarriorMama777/OrangeMixs\n```\n\n4. complete\n\n\n## Batch Download (Advanced)\n\nAdvanced: (When you want to download only selected directories, not the entire repository.)\n&nbsp;\n<details>\n<summary>Toggle: How to Batch Download (Advanced)</summary>\n\n1. Run the command `git clone --filter=tree:0 --no-checkout https://huggingface.co/WarriorMama777/OrangeMixs` to clone the huggingface repository. By adding the `--filter=tree:0` and `--no-checkout` options, you can download only the file names without their contents.\n```\ngit clone --filter=tree:0 --no-checkout https://huggingface.co/WarriorMama777/OrangeMixs\n```\n\n2. Move to the cloned directory with the command `cd OrangeMixs`.\n```\ncd OrangeMixs\n```\n\n3. Enable sparse-checkout mode with the command `git sparse-checkout init --cone`. By adding the `--cone` option, you can achieve faster performance.\n```\ngit sparse-checkout init --cone\n```\n\n4. Specify the directory you want to get with the command `git sparse-checkout add <directory name>`. For example, if you want to get only the `Models/AbyssOrangeMix3` directory, enter `git sparse-checkout add Models/AbyssOrangeMix3`.\n```\ngit sparse-checkout add Models/AbyssOrangeMix3\n```\n\n5. Download the contents of the specified directory with the command `git checkout main`.\n```\ngit checkout main\n```\n\nThis completes how to clone only a specific directory. If you want to add other directories, run `git sparse-checkout add <directory name>` again.\n\n\n</details>\n\n\n\n## Select and download\n\n1. Go to the Files and vaersions tab.\n2. select the model you want to download\n3. download\n4. complete\n\n----\n\n\n\n----\n\n# Model Detail & Merge Recipes\n\n<a name=\"VOM\"></a>\n\n## VividOrangeMix (VOM)\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/VOM_heroimage_02_comp002.webp \"VividOrangeMix\")\nPrompt: https://majinai.art/ja/i/VZ9dNoI\n\nCivitai: https://civitai.com/models/196585?modelVersionId=221033\n\n2023-11-25\n\n### VividOrangeMix\n\n▼About\n\"VividOrangeMix is a StableDiffusion model created for fans seeking vivid, flat, anime-style illustrations. With rich, bold colors and flat shading, it embodies the style seen in anime and manga.”\nOne of the versions of OrangeMixs, AbyssOrangeMix1~3 (AOM), has improved the anatomical accuracy of the human body by merging photorealistic models, but I was dissatisfied with the too-realistic shapes and shadows.  \nVividOrangeMix is a model that has been adjusted to solve this problem.  \n\n▼Sample Gallery\nDefault\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/2023-11-14_VividOrangeMixSample_default_big_v2.1.webp \"VividOrangeMixSampleGallery_default\")\nLoRA\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/2023-11-14_VividOrangeMixSample_LoRA_med_v2.webp \"VividOrangeMixSampleGallery_LoRA\")\n\n\n### VividOrangeMix_NSFW / Hard\n\n▼About\nVividOrangeMix NSFW/Hard is, as before, a model that Merges elements of NAI and Gape by U-Net Blocks Weight method.\nAs of AOM3, elements of these models should be included, but when I simply merged other models, the elements of the old merge seem to gradually fade away. Also, by merging U-Net Blocks Weight, it is now possible to merge without affecting the design to some extent, but some changes are unavoidable, so I decided to upload it separately as before. .\n\n▼Sample Gallery\n\n←NSFW | Hard→\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/2023-11-27_VividOrangeMixSample_NSFWandHard.webp \"VividOrangeMixSampleGallery_LoRA\")\n\n\n___\n### Instructions\n\n▼Tool\n- https://github.com/hako-mikan/sd-webui-supermerger/  \n\n___\n\n▼VividOrangeMix\n\nSTEP: 1 | Base model create\n\n[GO TO AOM3B4 Instructions↓](#AOM3B4)\n\nSTEP: 2 | Model merge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3B4 | Animelike_2D_Pruend_fp16 |  | sum @ 0.3 |  | VividOrangeMix |\n\n___\n\n▼VividOrangeMix_NSFW\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| VividOrangeMix | NAI full | NAI sfw | Add Difference @ 1.0 | 0,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.2,0.25,0.25,0.25,0.25,0,0 | VividOrangeMix_NSFW |\n\n___\n\n▼VividOrangeMix_Hard\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| VividOrangeMix_NSFW | gape60 | NAI full | Add Difference @ 1.0 | 0.0,0.25,0.25,0.25,0.25,0.25,0.0,0.0,0.0,0.0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.0,0.0 | VividOrangeMix_Hard |\n\n____\n\n## AbyssOrangeMix3 (AOM3)\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/AOM3_G_Top_comp001.webp \"\")\n\n――Everyone has different “ABYSS”!\n\n▼About\n\nThe main model, \"AOM3 (AbyssOrangeMix3)\", is a purely upgraded model that improves on the problems of the previous version, \"AOM2\". \"AOM3\" can generate illustrations with very realistic textures and can generate a wide variety of content. There are also three variant models based on the AOM3 that have been adjusted to a unique illustration style. These models will help you to express your ideas more clearly.\n\n▼Links\n\n- [⚠NSFW] Civitai: AbyssOrangeMix3 (AOM3) | Stable Diffusion Checkpoint | https://civitai.com/models/9942/abyssorangemix3-aom3\n\n\n### About\n\nFeatures: high-quality, realistic textured illustrations can be generated.  \nThere are two major changes from AOM2.\n\n1: Models for NSFW such as _nsfw and _hard have been improved: the models after nsfw in AOM2 generated creepy realistic faces, muscles and ribs when using Hires.fix, even though they were animated characters. These have all been improved in AOM3.\n\ne.g.: explanatory diagram by MEME : [GO TO MEME ZONE↓](#MEME_realface)\n\n2: sfw/nsfw merged into one model. Originally, nsfw models were separated because adding NSFW content (models like NAI and gape) would change the face and cause the aforementioned problems. Now that those have been improved, the models can be packed into one.  \nIn addition, thanks to excellent extensions such as [ModelToolkit](https://github.com/arenatemp/stable-diffusion-webui-model-toolkit\n), the model file size could be reduced (1.98 GB per model).\n\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/AOM3_G_Full_2_comp002.webp \"\")\n\n\n### More feature\nIn addition, these U-Net Blocks Weight Merge models take numerous steps but are carefully merged to ensure that mutual content is not overwritten.  \n\n(Of course, all models allow full control over adult content.)\n- 🔐 When generating illustrations for the general public: write \"nsfw\" in the negative prompt field\n- 🔞 ~~When generating adult illustrations: \"nsfw\" in the positive prompt field~~ -> It can be generated without putting it in. If you include it, the atmosphere will be more NSFW.\n\n### Variations / Sample Gallery\n🚧Editing🚧\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/AOM3_G_Art_comp003.webp \"\")\n\n\n#### AOM3 \n\n\n\n\n\n▼AOM3\n![](https://github.com/WarriorMama777/imgup/raw/2c840982550fab41f45ba4b5aedbd3d84ddf2390/img/AOM3/img_sanmples_AOM3_01_comp001.webp \"OrangeMixs_img_sanmples_AOM3_01_comp001\")\n\n<span style=\"font-size: 60%;\">(Actually, this gallery doesn't make much sense since AOM3 is mainly an improvement of the NSFW part 😂  ...But we can confirm that the picture is not much different from AOM2sfw.)</span>\n\n#### AOM3A1\n\n⛔Only this model (AOM3A1) includes ChilloutMix. The curse of the DreamLike license. In other words, only AOM3A1 is not available for commercial use. I recommend AOM3A1B instead.⛔\n[GO TO MEME ZONE↓](#MEME_AOM3A1)\n\nFeatures: Anime like illustrations with flat paint. Cute enough as it is, but I really like to apply LoRA of anime characters to this model to generate high quality anime illustrations like a frame from a theatre version.\n\n▼A1\n\n![](https://github.com/WarriorMama777/imgup/raw/33d21cd31e35ae6b7593e7f6dd913f5f71ddef4e/img/AOM3/img_sanmples_AOMA1_3.0_comp001.webp \"OrangeMixs_img_sanmples_AOMA1_3.0_comp001\")\n\n\n<details>\n<summary>©</summary>\n(1)©Yurucamp: Inuyama Aoi, (2)©The Quintessential Quintuplets: Nakano Yotsuba, (3)©Sailor Moon: Mizuno Ami/SailorMercury\n</details>\n\n#### AOM3A2\n🚧Editing🚧\nFeatures: Oil paintings like style artistic illustrations and stylish background depictions. In fact, this is mostly due to the work of Counterfeit 2.5, but the textures are more realistic thanks to the U-Net Blocks Weight Merge.\n\n#### AOM3A3\n🚧Editing🚧\nFeatures: Midpoint of artistic and kawaii. the model has been tuned to combine realistic textures, a artistic style that also feels like an oil colour style, and a cute anime-style face. Can be used to create a wide range of illustrations.\n\n#### AOM3A1B\n\nAOM3A1B added. This model is my latest favorite. I recommend it for its moderate realism, moderate brush touch, and moderate LoRA conformity.  \nThe model was merged by mistakenly selecting 'Add sum' when 'Add differences' should have been selected in the ~~AOM3A3~~AOM3A2 recipe. It was an unintended merge, but we share it because the illustrations produced are consistently good results.  \nThe model was merged by mistakenly selecting 'Add sum' when 'Add differences' should have been selected in the ~~AOM3A3~~AOM3A2 recipe. It was an unintended merge, but we share it because the illustrations produced are consistently good results.  \nIn my review, this is an illustration style somewhere between AOM3A1 and A3.\n\n▼A1B\n\n![](https://github.com/WarriorMama777/imgup/raw/c66097319405d5373fab1cebec03c5c71427879c/img/AOM3/img_AOM3A1B_01_comp001.webp \"orangemix_img_AOM3A1B_01_comp001.webp\")  \n![](https://github.com/WarriorMama777/imgup/raw/3e060893c0fb2c80c6f3aedf63bf8d576c9a37fc/img/AOM3/img_samples_AOM3A1B_01_comp001.webp \"orangemix_img_samples_AOM3A1B_01_comp001.webp\")  \n- Meisho Doto (umamusume): https://civitai.com/models/11980/meisho-doto-umamusume\n- Train and Girl: [JR East E235 series / train interior](https://civitai.com/models/9517/jr-east-e235-series-train-interior) \n\n<details>\n<summary>©</summary>\n©umamusume: Meisho Doto, ©Girls und Panzer: Nishizumi Miho,©IDOLM@STER: Sagisawa Fumika\n</details>\n\n#### AOM3B2\nmy newest toy. \nJust AOM3A1B + BreakdomainM21: 0.4  \nSo this model is somewhat of a troll model.\nI would like to create an improved DiffLoRAKit_v2 based on this.  \nUpload for access for research etc. 2023-06-27  \n\n![AOM3B2_orangemixs_sampleGallery](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/img_sanmples_AOM3B2_02_comp001.webp \"AOM3B2_orangemixs_sampleGallery\")\n\n<details><summary>Sample image prompts</summary>\n\n1. [Maid](https://majinai.art/ja/i/jhw20Z_)\n2. Yotsuba: https://majinai.art/ja/i/f-O4wau\n3. Inuko in cafe: https://majinai.art/ja/i/Cj-Ar9C\n4. bathroom: https://majinai.art/ja/i/XiSj5K6\n\n</details>\n\n&nbsp;\n\n\n#### AOM3B3\n\n2023-09-25\n\nThis is a derivative model of AOM3B2.\nI merged some nice models and also merged some LoRAs to further adjust the color and painting style.\n\n\n\n◆**Instructions:**\n\n▼Tool\nSupermerger\n\n▼Model Merge\nAOM3B2+Mixprov4+BreakdomainAnime\n triple sum : 0.3, 0.3 | mode:normal\n\n＋\n\n▼LoRA Merge\nloraH(DiffLoRA)_FaceShadowTweaker_v1_dim4:-2,nijipretty_20230624235607:0.1,MatureFemale_epoch8:0.1,colorful_V1_lbw:0.5\n\n\n#### AOM3B4\n<a name=\"AOM3B4\"></a>\n▼About\nFix AOM3B3\n\n▼**Instructions:**\n\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \n\nSTEP: 1 | Model merge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3B2 | Mixprov4 | BreakdomainAnime | triple sum @ 0.3, 0.3, mode:normal |  | temp01 |\n\nSTEP: 2 | LoRA Merge\n\nColor fix\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | colorful_V1_lbw |  | sum @ 0.45 |  | AOM3B4 |\n\n\n⚓[GO TO VividOrangeMix Instructions↑](#VOM)\n\n\n#### AOM3B3\n\n2023-09-25\n\nThis is a derivative model of AOM3B2.\nI merged some nice models and also merged some LoRAs to further adjust the color and painting style.\n\n\n\n◆**Instructions:**\n\n▼Tool\nSupermerger\n\n▼Model Merge\nAOM3B2+Mixprov4+BreakdomainAnime\n triple sum : 0.3, 0.3 | mode:normal\n\n＋\n\n▼LoRA Merge\nloraH(DiffLoRA)_FaceShadowTweaker_v1_dim4:-2,nijipretty_20230624235607:0.1,MatureFemale_epoch8:0.1,colorful_V1_lbw:0.5\n\n\n#### AOM3B4\n<a name=\"AOM3B4\"></a>\n▼About\nFix AOM3B3\n\n▼**Instructions:**\n\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \n\nSTEP: 1 | Model merge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3B2 | Mixprov4 | BreakdomainAnime | triple sum @ 0.3, 0.3, mode:normal |  | temp01 |\n\nSTEP: 2 | LoRA Merge\n\nColor fix\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | colorful_V1_lbw |  | sum @ 0.45 |  | AOM3B4 |\n\n\n⚓[GO TO VividOrangeMix Instructions↑](#VOM)\n____\n### Description for enthusiast\n\nAOM3 was created with a focus on improving the nsfw version of AOM2, as mentioned above.The AOM3 is a merge of the following two models into AOM2sfw using U-Net Blocks Weight Merge, while extracting only the NSFW content part.  \n(1) NAI: trained in Danbooru  \n(2)gape: Finetune model of NAI trained on Danbooru's very hardcore NSFW content.  \nIn other words, if you are looking for something like AOM3sfw, it is AOM2sfw.The AOM3 was merged with the NSFW model while removing only the layers that have a negative impact on the face and body.   However, the faces and compositions are not an exact match to AOM2sfw.AOM2sfw is sometimes superior when generating SFW content. I recommend choosing according to the intended use of the illustration.See below for a comparison between AOM2sfw and AOM3.\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/img_modelComparison_AOM_comp001.webp \"modelComparison_AOM\")\n\n\n▼A summary of the AOM3 work is as follows\n\n1. investigated the impact of the NAI and gape layers as AOM2 _nsfw onwards is crap.  \n2. cut face layer: OUT04 because I want realistic faces to stop → Failed. No change.  \n3. gapeNAI layer investigation｜  \n  a. (IN05-08 (especially IN07) | Change the illustration   significantly. Noise is applied, natural colours are lost, shadows die, and we can see that the IN deep layer is a layer of light and shade.  \n  b. OUT03-05(?) | likely to be sexual section/NSFW layer.Cutting here will kill the NSFW.  \n  c. OUT03,OUT04｜NSFW effects are in(?). e.g.: spoken hearts, trembling, motion lines, etc...  \n  d. OUT05｜This is really an NSFW switch. All the \"NSFW atmosphere\" is in here. Facial expressions, Heavy breaths, etc...  \n  e. OUT10-11｜Paint layer. Does not affect detail, but does have an extensive impact.  \n1. (mass production of rubbish from here...)   \n2. cut IN05-08 and merge NAIgape with flat parameters → avoided creepy muscles and real faces. Also, merging NSFW models stronger has less impact.  \n3. so, cut IN05-08, OUT10-11 and merge NAI+gape with all others 0.5.  \n4. → AOM3  \nAOM3 roughly looks like this  \n\n\n\n----\n\n▼How to use\n\n- Prompts\n    - Negative prompts is As simple as possible is good.  \n    (worst quality, low quality:1.4)\n    - Using \"3D\" as a negative will result in a rough sketch style at the \"sketch\" level. Use with caution as it is a very strong prompt.\n    - How to avoid Real Face  \n    (realistic, lip, nose, tooth, rouge, lipstick, eyeshadow:1.0), (abs, muscular, rib:1.0),\n    - How to avoid Bokeh  \n    (depth of field, bokeh, blurry:1.4)\n    - How to remove mosaic: `(censored, mosaic censoring, bar censor, convenient censoring, pointless censoring:1.0),`\n    - How to remove blush: `(blush, embarrassed, nose blush, light blush, full-face blush:1.4), `\n    - How to remove NSFW effects: `(trembling, motion lines, motion blur, emphasis lines:1.2),`\n    - 🔰Basic negative prompts sample for Anime girl ↓  \n      - v1  \n    `nsfw, (worst quality, low quality:1.4), (realistic, lip, nose, tooth, rouge, lipstick, eyeshadow:1.0), (dusty sunbeams:1.0),, (abs, muscular, rib:1.0), (depth of field, bokeh, blurry:1.4),(motion lines, motion blur:1.4), (greyscale, monochrome:1.0), text, title, logo, signature`\n      - v2  \n    `nsfw, (worst quality, low quality:1.4), (lip, nose, tooth, rouge, lipstick, eyeshadow:1.4), (blush:1.2), (jpeg artifacts:1.4), (depth of field, bokeh, blurry, film grain, chromatic aberration, lens flare:1.0), (1boy, abs, muscular, rib:1.0), greyscale, monochrome, dusty sunbeams,  trembling, motion lines, motion blur, emphasis lines, text, title, logo, signature, `\n- Sampler: ~~“DPM++ SDE Karras” is good~~ Take your pick  \n- Steps: \n  - DPM++ SDE Karras: Test: 12～ ,illustration: 20～  \n  - DPM++ 2M Karras: Test: 20～ ,illustration: 28～  \n- Clipskip: 1 or 2  \n- CFG: 8 (6～12)\n- Upscaler :  \n    - Detailed illust → Latenet (nearest-exact)  \n    Denoise strength: 0.5 (0.5~0.6)  \n    - Simple upscale: Swin IR, ESRGAN, Remacri etc…  \n    Denoise strength: Can be set low. (0.35~0.6)  \n\n\n\n---\n\n👩‍🍳Model details / Recipe\n\n▼Hash(SHA256)\n▼Hash(SHA256)\n\n- AOM3.safetensors  \nD124FC18F0232D7F0A2A70358CDB1288AF9E1EE8596200F50F0936BE59514F6D\n- AOM3A1.safetensors  \nF303D108122DDD43A34C160BD46DBB08CB0E088E979ACDA0BF168A7A1F5820E0\n- AOM3A2.safetensors  \n553398964F9277A104DA840A930794AC5634FC442E6791E5D7E72B82B3BB88C3\n- AOM3A3.safetensors  \nEB4099BA9CD5E69AB526FCA22A2E967F286F8512D9509B735C892FA6468767CF\n- AOM3A1B.safetensors\n5493A0EC491F5961DBDC1C861404088A6AE9BD4007F6A3A7C5DEE8789CDC1361\n- AOM3B2.safetensors\nF553E7BDE46CFE9B3EF1F31998703A640AF7C047B65883996E44AC7156F8C1DB\n\n- AOM3A1B.safetensors\n5493A0EC491F5961DBDC1C861404088A6AE9BD4007F6A3A7C5DEE8789CDC1361\n- AOM3B2.safetensors\nF553E7BDE46CFE9B3EF1F31998703A640AF7C047B65883996E44AC7156F8C1DB\n\n\n▼Use Models\n\n1. AOM2sfw  \n「038ba203d8ba3c8af24f14e01fbb870c85bbb8d4b6d9520804828f4193d12ce9」\n1. AnythingV3.0 huggingface pruned  \n[2700c435]「543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e」\n1. NovelAI animefull-final-pruned  \n[925997e9]「89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8」\n1. NovelAI sfw  \n[1d4a34af]「22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca」\n1. Gape60  \n[25396b85]「893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5」\n1. BasilMix  \n「bbf07e3a1c3482c138d096f7dcdb4581a2aa573b74a68ba0906c7b657942f1c2」\n1. chilloutmix_fp16.safetensors  \n「4b3bf0860b7f372481d0b6ac306fed43b0635caf8aa788e28b32377675ce7630」\n1. Counterfeit-V2.5_fp16.safetensors  \n「71e703a0fca0e284dd9868bca3ce63c64084db1f0d68835f0a31e1f4e5b7cca6」\n1. kenshi_01_fp16.safetensors  \n「3b3982f3aaeaa8af3639a19001067905e146179b6cddf2e3b34a474a0acae7fa」\n\n----\n\n▼AOM3\n\n◆**Instructions:**\n◆**Instructions:**\n\nTool: SuperMerger\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \nTool: SuperMerger\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \n\n(This extension is really great. It turns a month's work into an hour. Thank you)\n\nSTEP: 1 | BWM : NAI - NAIsfw & gape - NAI\n\nCUT: IN05-IN08, OUT10-11\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM2sfw | NAI full | NAI sfw | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | temp01 |\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM2sfw | NAI full | NAI sfw | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | temp01 |\n\nCUT: IN05-IN08, OUT10-11\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | gape60 | NAI full | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | AOM3 |\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | gape60 | NAI full | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | AOM3 |\n\n▼AOM3A1\n\n◆**Instructions:**\n\nTool: SuperMerger\n◆**Instructions:**\n\nTool: SuperMerger\n\nSTEP: 1 | Change the base photorealistic model of AOM3 from BasilMix to Chilloutmix.\n\nChange the photorealistic model from BasilMix to Chilloutmix and proceed to gapeNAI merge.\n\nSTEP: 2 | \n\n| Step | Interpolation Method | Primary Model | Secondary Model | Tertiary Model | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| 1 | SUM @ 0.5 | Counterfeit2.5 | Kenshi |  | Counterfeit+Kenshi |\n| Step | Interpolation Method | Primary Model | Secondary Model | Tertiary Model | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| 1 | SUM @ 0.5 | Counterfeit2.5 | Kenshi |  | Counterfeit+Kenshi |\n\nSTEP: 3 | \n\nCUT: BASE0, IN00-IN08：0, IN10：0.1, OUT03-04-05：0, OUT08：0.2\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit+Kenshi |  | Add SUM @ 1.0 | 0,0,0,0,0,0,0,0,0,0.3,0.1,0.3,0.3,0.3,0.2,0.1,0,0,0,0.3,0.3,0.2,0.3,0.4,0.5 | AOM3A1 |\n\n▼AOM3A1\n⛔Only this model (AOM3A1) includes ChilloutMix (=The curse of DreamLike).Commercial use is not available.\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit+Kenshi |  | Add SUM @ 1.0 | 0,0,0,0,0,0,0,0,0,0.3,0.1,0.3,0.3,0.3,0.2,0.1,0,0,0,0.3,0.3,0.2,0.3,0.4,0.5 | AOM3A1 |\n\n▼AOM3A1\n⛔Only this model (AOM3A1) includes ChilloutMix (=The curse of DreamLike).Commercial use is not available.\n\n▼AOM3A2\n\n◆?\n◆?\n\nCUT: BASE0, IN05:0.3、IN06-IN08：0, IN10：0.1, OUT03：0, OUT04：0.3, OUT05：0, OUT08：0.2\n\n◆**Instructions:**\n◆**Instructions:**\n\nTool: SuperMerger\nTool: SuperMerger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A2 |\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A2 |\n\n◆AOM3A3\n◆AOM3A3\n\nCUT : BASE0, IN05-IN08：0, IN10：0.1, OUT03：0.5, OUT04-05：0.1, OUT08：0.2\n\nTool: SuperMerger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,0.6,0.6,0.6,0.6,0.6,0,0,0,0,0.6,0.1,0.6,0.6,0.6,0.6,0.6,0.5,0.1,0.1,0.6,0.6,0.2,0.6,0.6,0.6 | AOM3A3 |\n\n▼AOM3A1B\n\n◆**Instructions:**\n\nTool: SuperMerge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 |  | Add Sum @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A1B |\n\n▼AOM3B2\n\n◆**Instructions:**\n\nTool: Checkpoint Merger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3A1B | Breakdomain m21_fp16 |  | Add Sum | 0.4 | AOM3B2 |\n\nTool: SuperMerger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,0.6,0.6,0.6,0.6,0.6,0,0,0,0,0.6,0.1,0.6,0.6,0.6,0.6,0.6,0.5,0.1,0.1,0.6,0.6,0.2,0.6,0.6,0.6 | AOM3A3 |\n\n▼AOM3A1B\n\n◆**Instructions:**\n\nTool: SuperMerge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 |  | Add Sum @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A1B |\n\n▼AOM3B2\n\n◆**Instructions:**\n\nTool: Checkpoint Merger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3A1B | Breakdomain m21_fp16 |  | Add Sum | 0.4 | AOM3B2 |\n\n\n----\n\n&nbsp;\n\n## AbyssOrangeMix2 (AOM2)\n\n――Creating the next generation of illustration with “Abyss”!\n\n<img src=\"https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/HeroImage_AbyssOrangeMix2_Designed_01_comp001.webp\"  width=\"\" height=\"\" alt=”HeroImage_AbyssOrangeMix2_Designed_01_comp001”>\n\nPrompt: [https://majinai.art/ja/i/nxpKRpw](https://majinai.art/ja/i/nxpKRpw)\n\n▼About\n\nAbyssOrangeMix2 (AOM2) is an AI model capable of generating high-quality, highly realistic illustrations.\nIt can generate elaborate and detailed illustrations that cannot be drawn by hand. It can also be used for a variety of purposes, making it extremely useful for design and artwork.\nFurthermore, it provides an unparalleled new means of expression.\nIt can generate illustrations in a variety of genres to meet a wide range of needs. I encourage you to use \"Abyss\" to make your designs and artwork richer and of higher quality.\n\n<img src=\"https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/UBM_ON_OFF_4_comp001.webp\"  width=\"\" height=\"\" alt=”UBM_ON_OFF_4_comp001.webp”>\n※nvidia joke.\n\n▼Description for engineers/enthusiasts\n\nThe merged model was formulated using an extension such as sdweb-merge-block-weighted-gui, which merges models at separate rates for each of the 25 U-Net blocks (input, intermediate, and output).\nThe validation of many Anons has shown that such a recipe can generate a painting style that is anatomically realistic enough to feel the finger skeleton, but still maintains an anime-style face.\n\nThe changes from AbyssOrangeMix are as follows.\n\n1. the model used for U-Net Blocks Weight Merge was changed from Instagram+F222 to BasilMix. (<https://huggingface.co/nuigurumi>)\n\nThis is an excellent merge model that can generate decent human bodies while maintaining the facial layers of the Instagram model. Thanks!!!\nThis has improved the dullness of the color and given a more Japanese skin tone (or more precisely, the moisturized white skin that the Japanese would ideally like).\nAlso, the unnatural bokeh that sometimes occurred in the previous version may have been eliminated (needs to be verified).\n\n2.Added IN deep layers (IN06-11) to the layer merging from the realistic model (BasilMix).\n\nIt is said that the IN deep layer (IN06-11) is the layer that determines composition, etc., but perhaps light, reflections, skin texture, etc., may also be involved.\nIt is like \"Global Illumination\", \"Ray tracing\" and \"Ambient Occlusion\" in 3DCG.\n\n<img src=\"https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/AbyssOrangeMix2_comparison_comp001.webp\"  width=\"\" height=\"\" alt=”AbyssOrangeMix2_comparison_comp001”>\n\n※This does not fundamentally improve the fingers. Therefore, More research needs to be done to improve the fingers (e.g. '[bad_prompt](https://huggingface.co/datasets/Nerfgun3/bad_prompt)').\nAbout 30-50% chance of generating correct fingers(?). Abyss is deep.\n\n▼Sample Gallery\n\nThe prompts for generating these images were all generated using ChatGPT. I simply asked \"Pirates sailing the oceans\" to tell me what the prompts were.  \nHowever, to make sure the AI understood the specifications, I used the template for AI questions (Question template for AI prompt generation(v1.2) ).\nPlease review the following.\n\n```jsx\nhttps://seesaawiki.jp/nai_ch/d/AI%a4%f2%b3%e8%cd%d1%a4%b7%a4%bf%a5%d7%a5%ed%a5%f3%a5%d7%a5%c8%c0%b8%c0%ae\n```\n\nThe images thus generated, strangely enough, look like MidJourney or Nijijourney illustrations. Perhaps they are passing user prompts through GPT or something else before passing them on to the image AI🤔\n\n<img src=\"https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/SampleGallerBoardDesign_AbyssOrangeMix2_ReadMore_comp001.webp\"  width=\"\" height=\"\" alt=”SampleGallerBoardDesign_AbyssOrangeMix2_03_comp001”>\n\n<details>\n<summary>▼READ MORE🖼</summary>\n\n<img src=\"https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/SampleGallerBoardDesign_AbyssOrangeMix2_03_comp001.webp\"  width=\"\" height=\"\" alt=”SampleGallerBoardDesign_AbyssOrangeMix2_03_comp001”>\n\n▼All prompts to generate sample images\n\n1. [Gaming Girl](https://majinai.art/ja/i/GbTbLyk)\n2. [Fantasy](https://majinai.art/ja/i/ax45Pof)\n3. [Rainy Day](https://majinai.art/ja/i/1P9DUul)\n4. [Kemomimi Girl](https://majinai.art/ja/i/hrUSb31)\n5. [Supermarket](https://majinai.art/ja/i/6Mf4bVK)\n6. [Lunch Time](https://majinai.art/ja/i/YAgQ4On)\n7. [Womens in the Garden](https://majinai.art/ja/i/oHZYum_)\n8. [Pirate](https://majinai.art/ja/i/yEA3EZk)\n9. [Japanese Girl](https://majinai.art/ja/i/x4G_B_e)\n10. [Sweets Time](https://majinai.art/ja/i/vK_mkac)\n11. [Glasses Girl](https://majinai.art/ja/i/Z87IHOC)\n\n</details>\n\n\n\n▼How to use\n\n- VAE: orangemix.vae.pt\n- ~~Prompts can be long or short~~  \nAs simple as possible is good. Do not add excessive detail prompts. Start with just this negative propmt.  \n(worst quality, low quality:1.4)  \n- Sampler: “DPM++ SDE Karras” is good\n- Steps: forTest: 12～ ,illustration: 20～\n- Clipskip: 1 or 2\n- Upscaler : Latenet (nearest-exact)\n- CFG Scale : 5 or 6 (4～8)\n- Denoise strength: 0.5 (0.45~0.6)  \nIf you use 0.7～, the picture will change too much.  \nIf below 0.45, Block noise occurs.  \n\n🗒Model List\n\n- AbyssOrangeMix2_sfw｜BasilMix U-Net Blocks Weight Merge\n  - AbyssOrangeMix2_nsfw｜+ NAI-NAISFW 0.3 Merge\n    - AbyssOrangeMix2_hard｜+ Gape 0.3 Merge\n\n※Changed suffix of models.  \n_base →_sfw: _base was changed to_sfw.\n_night →_nsfw: Merged models up to NAI-NAI SFW were changed from _night to_nsfw.\n_half and non suffix →_hard: Gape merged models were given the suffix _hard.gape was reduced to 0.3 because it affects character modeling.  \n\n▼How to choice models\n\n- _sfw : SFW😉\n- _nsfw : SFW ～ Soft NSFW🥰\n- _hard : SFW ～ hard NSFW👄\n\n▼Hash\n\n- AbyssOrangeMix2_sfw.ckpt  \n「f75b19923f2a4a0e70f564476178eedd94e76e2c94f8fd8f80c548742b5b51b9」  \n- AbyssOrangeMix2_sfw.safetensors  \n「038ba203d8ba3c8af24f14e01fbb870c85bbb8d4b6d9520804828f4193d12ce9」  \n- AbyssOrangeMix2_nsfw.safetensors  \n「0873291ac5419eaa7a18726e8841ce0f15f701ace29e0183c47efad2018900a4」  \n- AbyssOrangeMix_hard.safetensors  \n「0fc198c4908e98d7aae2a76bd78fa004e9c21cb0be7582e36008b4941169f18e」  \n\n▼Use Models\n\n1. AnythingV3.0 huggingface pruned  \n[2700c435]「543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e」  \n1. NovelAI animefull-final-pruned  \n[925997e9]「89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8」  \n1. NovelAI sfw  \n[1d4a34af]「22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca」  \n1. Gape60  \n[25396b85]「893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5」  \n1. BasilMix  \n「bbf07e3a1c3482c138d096f7dcdb4581a2aa573b74a68ba0906c7b657942f1c2」  \n\n### AbyssOrangeMix2_sfw (AOM2s)\n\n▼**Instructions:**\n\nSTEP: 1｜Block Merge\n\n| Model: A     | Model: B | Weight                                                                | Base alpha | Merge Name          |\n| ------------ | -------- | --------------------------------------------------------------------- | ---------- | ------------------- |\n| AnythingV3.0 | BasilMix | 1,0.9,0.7,0.5,0.3,0.1,1,1,1,1,1,1,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | AbyssOrangeMix2_sfw |\n\n### AbyssOrangeMix2_nsfw (AOM2n)\n\n▼?\n\nJUST AbyssOrangeMix2_sfw+ (NAI-NAISFW) 0.3.\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | AbyssOrangeMix_base | NovelAI animefull | NovelAI sfw    | AbyssOrangeMix2_nsfw |\n\n### AbyssOrangeMix2_hard (AOM2h)\n\n▼?\n+Gape0.3 version AbyssOrangeMix2_nsfw.\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name           |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | AbyssOrangeMix2_nsfw | Gape60          | NovelAI animefull | AbyssOrangeMix2_hard |\n\n----\n\n## EerieOrangeMix (EOM)\n\nEerieOrangeMix is the generic name for a U-Net Blocks Weight Merge Models based on Elysium(Anime V2).  \nSince there are infinite possibilities for U-Net Blocks Weight Merging, I plan to treat all Elysium-based models as a lineage of this model.\n\n※This does not fundamentally improve the fingers. Therefore, More research needs to be done to improve the fingers (e.g. '[bad_prompt](https://huggingface.co/datasets/Nerfgun3/bad_prompt)').\n\n<img src=\"https://files.catbox.moe/yjnqna.webp\"  width=\"1000\" height=\"\" alt=”HeroImage_EerieOrangeMix_Designed_comp001” >\n\n\n&nbsp;\n\n### EerieOrangeMix (EOM1)\n\n▼?  \n\nThis merge model is simply a U-Net Blocks Weight Merge of ElysiumAnime V2 with the AbyssOrangeMix method.\n\nThe AnythingModel is good at cute girls anyway, and no matter how hard I try, it doesn't seem to be good at women in their late 20s and beyond. Therefore, I created a U-Net Blocks Weight Merge model based on my personal favorite ElysiumAnime V2 model. ElyOrangeMix was originally my favorite, so this is an enhanced version of that.\n\n🗒Model List  \n\n- EerieOrangeMix_base｜Instagram+F222 U-Net Blocks Weight Merge\n  - EerieOrangeMix_night｜+ NAI-NAISFW Merge\n    - EerieOrangeMix_half｜+ Gape0.5 Merge\n    - EerieOrangeMix｜+ Gape1.0 Merge\n\n▼ How to choice models\n\n- _base : SFW😉\n- _Night : SFW ～ Soft NSFW🥰\n- _half : SFW ～ NSFW👄\n- unlabeled : SFW ～ HARDCORE ～🤯  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n▼Hash  \n\n- EerieOrangeMix.safetensors\n- EerieOrangeMix_half.safetensors\n- EerieOrangeMix_night.safetensors\n- EerieOrangeMix_base.ckpt\n\n▼Use Models  \n\n[] = WebUI Hash,「」= SHA256\n\n1. Elysium Anime V2\n[]「5c4787ce1386500ee05dbb9d27c17273c7a78493535f2603321f40f6e0796851」\n2. NovelAI animefull-final-pruned\n[925997e9]「89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8」\n3. NovelAI sfw\n[1d4a34af]「22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca」\n4. Gape60\n[25396b85]「893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5」\n5. instagram-latest-plus-clip-v6e1_50000.safetensors\n[] 「8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5」\n6. f222\n[] 「9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f」\n7. sd1.5_pruned\n[] 「e1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053」\n\n▼ Sample Gallery  \n\n<img src=\"https://files.catbox.moe/oqbvti.webp\"  width=\"1000\" height=\"\" alt=”2022-12-30_MotorbikeGIrlAsa3_comp001”>\n<details>\n  <summary>More🖼</summary>\n  <img src=\"https://files.catbox.moe/nmmswd.webp\"  width=\"\" height=\"600\" alt=”2022-12-30_SampleGallery5”>\n</details>\n\n▼ How to use  \n\n- VAE: orangemix.vae.pt\n- As simple as possible is good. Do not add excessive detail prompts. Start with just this.\n(worst quality, low quality:1.4)\n- Sampler: “DPM++ SDE Karras” is good\n- Steps: forTest: 20～24 ,illustration: 24～50\n- Clipskip: 1\n- USE “upscale latent space”\n- Denoise strength: 0.45 (0.4~0.5)  \nIf you use 0.7～, the picture will change too much.\n\n▼Prompts\n\n🖌When generating cute girls, try this negative prompt first. It avoids low quality, prevents blurring, avoids dull colors, and dictates Anime-like cute face modeling.\n\n```jsx\nnsfw, (worst quality, low quality:1.3), (depth of field, blurry:1.2), (greyscale, monochrome:1.1), 3D face, nose, cropped, lowres, text, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, (tan, muscular, loli, petite, child, infant, toddlers, chibi, sd character:1.1), multiple view, Reference sheet,\n```\n\n---\n\n#### EerieOrangeMix_base (EOM1b)\n\n▼?  \nDetails are omitted since it is the same as AbyssOrangeMix.\n\n▼**Instructions:**\n\nSTEP: 1｜Creation of photorealistic model for Merge\n\n| Step | Interpolation Method | Primary Model                         | Secondary Model | Tertiary Model | Merge Name |\n| ---- | -------------------- | ------------------------------------- | --------------- | -------------- | ---------- |\n| 1    | Add Difference @ 1.0 | instagram-latest-plus-clip-v6e1_50000 | f222            | sd1.5_pruned   | Insta_F222 |\n\nSTEP: 2｜Block Merge\n\nMerge InstaF222\n\n| Model: A         | Model: B   | Weight                                                                | Base alpha | Merge Name |\n| ---------------- | ---------- | --------------------------------------------------------------------- | ---------- | ---------- |\n| Elysium Anime V2 | Insta_F222 | 1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | Temp1      |\n\n#### EerieOrangeMix_Night (EOM1n)\n\n▼?\n\nJUST EerieOrangeMix_base+ (NAI-NAISFW) 0.3.\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | EerieOrangeMix_base | NovelAI animefull | NovelAI sfw    | EerieOrangeMix_Night |\n\n#### EerieOrangeMix_half (EOM1h)\n\n▼?\n+Gape0.5 version EerieOrangeMix.\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model   | Tertiary Model | Merge Name          |\n| ---- | -------------------- | -------------------- | ----------------- | -------------- | ------------------- |\n| 1    | Add Difference @ 0.5 | EerieOrangeMix_Night | NovelAI animefull | NovelAI sfw    | EerieOrangeMix_half |\n\n#### EerieOrangeMix (EOM1)\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name     |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | -------------- |\n| 1    | Add Difference @ 1.0 | EerieOrangeMix_Night | Gape60          | NovelAI animefull | EerieOrangeMix |\n\n----\n\n### EerieOrangeMix2 (EOM2)\n\n▼?\n\nThe model was created by adding the hierarchy responsible for detailing and painting ElysiumV1 to EerieOrangeMix_base, then merging NAI and Gape.\n\n🗒Model List\n\n- EerieOrangeMix2_base｜Instagram+F222+ElysiumV1 U-Net Blocks Weight Merge\n  - EerieOrangeMix2_night｜+ NAI-NAISFW Merge\n    - EerieOrangeMix2_half｜+ Gape0.5 Merge\n    - EerieOrangeMix2｜+ Gape1.0 Merge\n\n▼ How to choice models\n\n- _base : SFW😉\n- _Night : SFW ～ Soft NSFW🥰\n- _half : SFW ～ NSFW👄\n- unlabeled : SFW ～ HARDCORE ～🤯  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n▼Hash\n\n- EerieOrangeMix2.safetensors\n- EerieOrangeMix2_half.safetensors\n- EerieOrangeMix2_night.safetensors\n- EerieOrangeMix2_base.ckpt\n\n▼Use Models\n\n[] = webuHash,「」= SHA256\n\n1. Elysium Anime V2\n[]「5c4787ce1386500ee05dbb9d27c17273c7a78493535f2603321f40f6e0796851」\n2. NovelAI animefull-final-pruned\n[925997e9]「89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8」\n3. NovelAI sfw\n[1d4a34af]「22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca」\n4. Gape60\n[25396b85]「893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5」\n5. instagram-latest-plus-clip-v6e1_50000.safetensors\n[] 「8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5」\n6. f222\n[] 「9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f」\n7. sd1.5_pruned\n[] 「e1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053」\n8. ElysiumV1\n「abbb28cb5e70d3e0a635f241b8d61cefe42eb8f1be91fd1168bc3e52b0f09ae4」\n\n#### EerieOrangeMix2_base (EOM2b)\n\n▼?\n\n▼Instructions\n\nSTEP: 1｜Block Merge\n\nMerge ElysiumV1\n\nThe generated results do not change much with or without this process, but I wanted to incorporate Elysium's depiction, so I merged it.\n\n| Model: A            | Model: B  | Weight                                                                | Base alpha | Merge Name           |\n| ------------------- | --------- | --------------------------------------------------------------------- | ---------- | -------------------- |\n| EerieOrangeMix_base | ElysiumV1 | 1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | EerieOrangeMix2_base |\n\n#### EerieOrangeMix2_night (EOM2n)\n\n▼?\n\nJUST EerieOrangeMix2_base+ (NAI-NAISFW) 0.3.\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name            |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | --------------------- |\n| 1    | Add Difference @ 0.3 | EerieOrangeMix_base | NovelAI animefull | NovelAI sfw    | EerieOrangeMix2_Night |\n\n#### EerieOrangeMix2_half (EOM2h)\n\n▼?\n+Gape0.5 version EerieOrangeMix2.\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model        | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | -------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.5 | EerieOrangeMix_Night | NovelAI animefull | NovelAI sfw    | EerieOrangeMix2_half |\n\n#### EerieOrangeMix2 (EOM2)\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name      |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | --------------- |\n| 1    | Add Difference @ 1.0 | EerieOrangeMix_Night | Gape60          | NovelAI animefull | EerieOrangeMix2 |\n\n### Models Comparison\n\n<img src=\"https://files.catbox.moe/mp2fr4.webp\"  width=\"1000\" height=\"\" alt=\"MotorbikeGIrlAsa_Eerie_Abyss_Comparison_comp001\">  \n<img src=\"https://files.catbox.moe/9xqths.webp\"  width=\"1000\" height=\"\" alt=”Eerie_Abyss_Comparison_02_comp001”>\n<img src=\"https://files.catbox.moe/cm6c7m.webp\"  width=\"1000\" height=\"\" alt=”Eerie_Comparison_01_comp001”>  \n※The difference is slight but probably looks like this.\n← warm color, ↑ natural color, → animated color\n\n----\n\n## AbyssOrangeMix (AOM)\n\n――How can you guys take on such a deep swamp and get results?  \nIs it something like \"Made in Abyss\"?  \nBy Anon, 115th thread\n\n<img src=\"https://files.catbox.moe/wst1bp.webp\"  width=\"1000\" height=\"\">\n\n\n▼?\n\nThe merged model was formulated using an extension such as sdweb-merge-block-weighted-gui, which merges models at separate rates for each of the 25 U-Net blocks (input, intermediate, and output).\nThe validation of many Anons has shown that such a recipe can generate a painting style that is anatomically realistic enough to feel the finger skeleton, but still maintains an anime-style face.\n\n※This model is the result of a great deal of testing and experimentation by many Anons🤗\n※This model can be very difficult to handle. I am not 100% confident in my ability to use this model. It is peaky and for experts.  \n※This does not fundamentally improve the fingers, and I recommend using bad_prompt, etc. (Embedding) in combination.  \n\n▼Sample Gallery\n\n(1)\n<img src=\"https://files.catbox.moe/8mke0t.webp\" width=\"1000\" height=\"\">\n\n```jsx\n((masterpiece)), best quality, perfect anatomy, (1girl, solo focus:1.4), pov, looking at viewer, flower trim,(perspective, sideway, From directly above ,lying on water, open hand, palm, :1.3),(Accurate five-fingered hands, Reach out, hand focus, foot focus, Sole, heel, ball of the thumb:1.2), (outdoor, sunlight:1.2),(shiny skin:1.3),,(masterpiece, white border, outside border, frame:1.3),\n, (motherhood, aged up, mature female, medium breasts:1.2), (curvy:1.1), (single side braid:1.2), (long hair with queue and braid, disheveled hair, hair scrunchie, tareme:1.2), (light Ivory hair:1.2), looking at viewer,, Calm, Slight smile,\n,(anemic, dark, lake, river,puddle, Meadow, rock, stone, moss, cliff, white flower, stalactite, Godray, ruins, ancient, eternal, deep ,mystic background,sunlight,plant,lily,white flowers, Abyss, :1.2), (orange fruits, citrus fruit, citrus fruit bearing tree:1.4), volumetric lighting,good lighting,, masterpiece, best quality, highly detailed,extremely detailed cg unity 8k wallpaper,illustration,((beautiful detailed face)), best quality, (((hyper-detailed ))), high resolution illustration ,high quality, highres, sidelighting, ((illustrationbest)),highres,illustration, absurdres, hyper-detailed, intricate detail, perfect, high detailed eyes,perfect lighting, (extremely detailed CG:1.2),\n\nNegative prompt: (bad_prompt_version2:1), distant view, lip, Pregnant, maternity, pointy ears, realistic, tan, muscular, greyscale, monochrome, lineart, 2koma, 3koma, 4koma, manga, 3D, 3Dcubism, pablo picasso, disney, marvel, mutanted breasts, mutanted nipple, cropped, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, trademark, watermark, title, text, deformed, bad anatomy, disfigured, mutated, extra limbs, ugly, missing limb, floating limbs, disconnected limbs, out of frame, mutated hands and fingers, poorly drawn hands, malformed hands, poorly drawn face, poorly drawn asymmetrical eyes, (blurry:1.4), duplicate (loli, petite, child, infant, toddlers, chibi, sd character, teen age:1.4), tsurime, helmet hair, evil smile, smug_face, naughty smile, multiple view, Reference sheet, (worst quality, low quality:1.4),\nSteps: 24, Sampler: DPM++ SDE Karras, CFG scale: 10, Seed: 1159970659, Size: 1536x768, Model hash: cc44dbff, Model: AbyssOrangeMix, Variation seed: 93902374, Variation seed strength: 0.45, Denoising strength: 0.45, ENSD: 31337\n```\n\n(2)\n<img src=\"https://files.catbox.moe/6cbrqh.webp\" width=\"\" height=\"600\">\n\n```jsx\nstreet, 130mm f1.4 lens, ,(shiny skin:1.3),, (teen age, school uniform:1.2), (glasses, black hair, medium hair with queue and braid, disheveled hair, hair scrunchie, tareme:1.2), looking at viewer,, Calm, Slight smile,\n\nNegative prompt: (bad_prompt_version2:1), distant view, lip, Pregnant, maternity, pointy ears, realistic, tan, muscular, greyscale, monochrome, lineart, 2koma, 3koma, 4koma, manga, 3D, 3Dcubism, pablo picasso, disney, marvel, mutanted breasts, mutanted nipple, cropped, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, trademark, watermark, title, text, deformed, bad anatomy, disfigured, mutated, extra limbs, ugly, missing limb, floating limbs, disconnected limbs, out of frame, mutated hands and fingers, poorly drawn hands, malformed hands, poorly drawn face, poorly drawn asymmetrical eyes, (blurry:1.4), duplicate (loli, petite, child, infant, toddlers, chibi, sd character, teen age:1.4), tsurime, helmet hair, evil smile, smug_face, naughty smile, multiple view, Reference sheet, (worst quality, low quality:1.4),\nSteps: 24, Sampler: DPM++ SDE Karras, CFG scale: 10, Seed: 1140782193, Size: 1024x1536, Model hash: cc44dbff, Model: AbyssOrangeMix, Denoising strength: 0.45, ENSD: 31337, First pass size: 512x768, Model sha256: 6bb3a5a3b1eadd32, VAE sha256: f921fb3f29891d2a, Options: xformers medvram gtx_16x0\n\nUsed embeddings: bad_prompt_version2 [afea]\n```\n\n----\n\n▼How to use\n\n- VAE: orangemix.vae.pt\n- ~~Prompts can be long or short~~  \nAs simple as possible is good. Do not add excessive detail prompts. Start with just this.\n(worst quality, low quality:1.4)\n- Sampler: “DPM++ SDE Karras” is good\n- Steps: forTest: 20～24 ,illustration: 24～50\n- Clipskip: 1\n- USE “upscale latent space”\n- Denoise strength: 0.45 (0.4~0.5)\nIf you use 0.7～, the picture will change too much.\n\n▼Prompts\n\n🖌When generating cute girls, try this negative prompt first. It avoids low quality, prevents blurring, avoids dull colors, and dictates Anime-like cute face modeling.\n\n```jsx\nnsfw, (worst quality, low quality:1.3), (depth of field, blurry:1.2), (greyscale, monochrome:1.1), 3D face, nose, cropped, lowres, text, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, (tan, muscular, loli, petite, child, infant, toddlers, chibi, sd character:1.1), multiple view, Reference sheet,\n```\n\n🗒Model List\n\n- AbyssOrangeMix_base｜Instagram Merge\n  - AbyssOrangeMix_Night｜+ NAI-NAISFW Merge\n    - AbyssOrangeMix_half｜+ Gape0.5 Merge\n    - AbyssOrangeMix｜+ Gape1.0 Merge\n\n▼ How to choice models\n\n- _base : SFW😉\n- _Night : SFW ～ Soft NSFW🥰\n- _half : SFW ～ NSFW👄\n- unlabeled : SFW ～ HARDCORE ～🤯  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n▼Hash (SHA256)\n\n- AbyssOrangeMix.safetensors  \n6bb3a5a3b1eadd32dfbc8f0987559c48cb4177aee7582baa6d6a25181929b345\n- AbyssOrangeMix_half.safetensors  \n468d1b5038c4fbd354113842e606fe0557b4e0e16cbaca67706b29bcf51dc402\n- AbyssOrangeMix_Night.safetensors  \n167cd104699dd98df22f4dfd3c7a2c7171df550852181e454e71e5bff61d56a6\n- AbyssOrangeMix_base.ckpt  \nbbd2621f3ec4fad707f75fc032a2c2602c296180a53ed3d9897d8ca7a01dd6ed\n\n▼Use Models\n\n1. AnythingV3.0 huggingface pruned\n[2700c435]「543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e」\n1. NovelAI animefull-final-pruned\n[925997e9]「89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8」\n1. NovelAI sfw\n[1d4a34af]「22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca」\n1. Gape60\n[25396b85]「893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5」\n1. instagram-latest-plus-clip-v6e1_50000.safetensors\n[] 「8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5」\n1. f222\n[] 「9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f」\n1. sd1.5_pruned\n[] 「e1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053」\n\n### AbyssOrangeMix_base (AOMb)\n\n▼?\n\nThe basic trick for this merged model is to incorporate a model that has learned more than 1m Instagram photos (mostly Japanese) or a photorealistic model like f222. The choice of base model here depends on the person. I chose AnythingV3 for versatility.\n\n▼**Instructions:**\n\nSTEP: 1｜Creation of photorealistic model for Merge\n\n| Step | Interpolation Method | Primary Model                         | Secondary Model | Tertiary Model | Merge Name |\n| ---- | -------------------- | ------------------------------------- | --------------- | -------------- | ---------- |\n| 1    | Add Difference @ 1.0 | instagram-latest-plus-clip-v6e1_50000 | f222            | sd1.5_pruned   | Insta_F222 |\n\nSTEP: 2｜Block Merge\n\n| Model: A     | Model: B   | Weight                                                                | Base alpha | Merge Name          |\n| ------------ | ---------- | --------------------------------------------------------------------- | ---------- | ------------------- |\n| AnythingV3.0 | Insta_F222 | 1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | AbyssOrangeMix_base |\n\n### AbyssOrangeMix_Night (AOMn)\n\n▼?\n\nJUST AbyssOrangeMix_base+ (NAI-NAISFW) 0.3.\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | AbyssOrangeMix_base | NovelAI animefull | NovelAI sfw    | AbyssOrangeMix_Night |\n\n### AbyssOrangeMix_half (AOMh)\n\n▼?\n+Gape0.5 version AbyssOrangeMix.\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name          |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | ------------------- |\n| 1    | Add Difference @ 0.5 | AbyssOrangeMix_Night | Gape60          | NovelAI animefull | AbyssOrangeMix_half |\n\n### AbyssOrangeMix (AOM)\n\n▼**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name     |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | -------------- |\n| 1    | Add Difference @ 1.0 | AbyssOrangeMix_Night | Gape60          | NovelAI animefull | AbyssOrangeMix |\n\n----\n\n## ElyOrangeMix (ELOM)\n\n<img src=\"https://i.imgur.com/AInEXA5.jpg\"  width=\"1000\" height=\"\">\n\n▼?  \nElysium_Anime_V2 + NAI + Gape.  \nThis is a merge model that improves on the Elysium_Anime_V2, where NSFW representation is not good.  \nIt can produce SFW, NSFW, and any other type of artwork, while retaining the Elysium's three-dimensional, thickly painted style.\n\n▼ How to choice models\n\n- _base : SFW😉\n- _Night : SFW ～ Soft NSFW🥰\n- _half : SFW ～ NSFW👄\n- unlabeled : SFW ～ HARDCORE ～🤯  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n▼How to use\n- VAE: orangemix.vae.pt\n\n▼Hash (SHA256)\n\n- ElyOrangeMix [6b508e59]\n- ElyOrangeMix_half [6b508e59]\n- ElyNightOrangeMix[6b508e59]\n\n\n### ElyOrangeMix (ELOM)\n\n▼Use Models\n\n1. Elysium_Anime_V2 [6b508e59]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model    | Secondary Model   | Tertiary Model    | Merge Name               |\n| ---- | -------------------- | ---------------- | ----------------- | ----------------- | ------------------------ |\n| 1    | Add Difference @ 0.3 | Elysium_Anime_V2 | NovelAI animefull | NovelAI sfw       | tempmix-part1 []         |\n| 2    | Add Difference @ 1.0 | tempmix-part1    | Gape60            | NovelAI animefull | ElyOrangeMix  [6b508e59] |\n\n---\n\n### ElyOrangeMix_half (ELOMh)\n\n▼?\n\n+Gape0.5 version ElyOrangeMix.\n\n▼Use Models\n\n1. Elysium_Anime_V2 [6b508e59]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model    | Secondary Model   | Tertiary Model    | Merge Name                    |\n| ---- | -------------------- | ---------------- | ----------------- | ----------------- | ----------------------------- |\n| 1    | Add Difference @ 0.3 | Elysium_Anime_V2 | NovelAI animefull | NovelAI sfw       | tempmix-part1 []              |\n| 2    | Add Difference @ 0.5 | tempmix-part1    | Gape60            | NovelAI animefull | ElyOrangeMix_half  [6b508e59] |\n\n----\n\n### ElyNightOrangeMix (ELOMn)\n\n▼?\n\nIt is a merged model that just did Elysium_Anime_V2+ (NAI-NAISFW) 0.3.\n\n▼Use Models\n\n1. Elysium_Anime_V2 [6b508e59]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model    | Secondary Model   | Tertiary Model | Merge Name        |\n| ---- | -------------------- | ---------------- | ----------------- | -------------- | ----------------- |\n| 1    | Add Difference @ 0.3 | Elysium_Anime_V2 | NovelAI animefull | NovelAI sfw    | ElyNightOrangeMix |\n\n----\n\n## BloodOrangeMix (BOM)\n\n<img src=\"https://i.imgur.com/soAnnFk.jpg\"  width=\"1000\" height=\"\">\n\n▼?\nAnything+NAI+Gape.  \nThis is a merge model that improves on the AnythingV3, where NSFW representation is not good.  \nIt can produce SFW, NSFW, and any other type of artwork, while retaining the flat, beautifully painted style of AnythingV3.  \nStable. Popular in the Japanese community.  \n\n▼ModelList & [] = WebUI Hash,「」= SHA256\n\n- BloodNightOrangeMix.ckpt  \n  [ffa7b160]「f8aff727ba3da0358815b1766ed232fd1ef9682ad165067cac76e576d19689e0」\n- BloodOrangeMix_half.ckpt  \n [ffa7b160]「b2168aaa59fa91229b8add21f140ac9271773fe88a387276f3f0c7d70f726a83」\n- BloodOrangeMix.ckpt  \n[ffa7b160] 「25cece3fe303ea8e3ad40c3dca788406dbd921bcf3aa8e3d1c7c5ac81f208a4f」\n- BloodOrangeMix.safetensors  \n「79a1edf6af43c75ee1e00a884a09213a28ee743b2e913de978cb1f6faa1b320d」\n\n▼ How to choice models\n\n- _base : SFW😉\n- _Night : SFW ～ Soft NSFW🥰\n- _half : SFW ～ NSFW👄\n- unlabeled : SFW ～ HARDCORE ～🤯  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n▼How to use\n- VAE: orangemix.vae.pt\n\n### BloodOrangeMix (BOM)\n\n▼Use Models\n\n1. AnythingV3.0 huggingface pruned [2700c435]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model | Secondary Model   | Tertiary Model    | Merge Name                |\n| ---- | -------------------- | ------------- | ----------------- | ----------------- | ------------------------- |\n| 1    | Add Difference @ 0.3 | AnythingV3.0  | NovelAI animefull | NovelAI sfw       | tempmix-part1 []          |\n| 2    | Add Difference @ 1.0 | tempmix-part1 | Gape60            | NovelAI animefull | BloodOrangeMix [ffa7b160] |\n\n----\n\n### BloodOrangeMix_half (BOMh)\n\n▼?\nAnything+Nai+Gape0.5\n+Gape0.5 version BloodOrangeMix.\nNSFW expression will be softer and have less impact on the Anything style painting style.\n\n▼Use Models\n\n1. AnythingV3.0 huggingface pruned [2700c435]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model | Secondary Model   | Tertiary Model    | Merge Name                     |\n| ---- | -------------------- | ------------- | ----------------- | ----------------- | ------------------------------ |\n| 1    | Add Difference @ 0.3 | AnythingV3.0  | NovelAI animefull | NovelAI sfw       | tempmix-part1 []               |\n| 2    | Add Difference @ 0.5 | tempmix-part1 | Gape60            | NovelAI animefull | BloodOrangeMix_half [ffa7b160] |\n\n----\n\n### BloodNightOrangeMix (BOMn)\n\n▼?\n\nIt is a merged model that just did AnythingV3+ (NAI-NAISFW) 0.3.\n\n▼Use Models\n\n1. AnythingV3.0 huggingface pruned [2700c435]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n\n▼Instructions\n\n| Step | Interpolation Method | Primary Model | Secondary Model   | Tertiary Model | Merge Name          |\n| ---- | -------------------- | ------------- | ----------------- | -------------- | ------------------- |\n| 1    | Add Difference @ 0.3 | AnythingV3.0  | NovelAI animefull | NovelAI sfw    | BloodNightOrangeMix |\n\n----\n\n## ElderOrangeMix \n\n※I found this model to be very prone to body collapse. Not recommended.\n\n▼?  \nanything and everything mix ver.1.5+Gape+Nai(AnEve.G.N0.3)  \nThis is a merged model with improved NSFW representation of anything and everything mix ver.1.5.\n\n▼Hash\n[3a46a1e0]\n\n▼Use Models\n\n1. anything and everything mix ver.1.5 [5265dcf6]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n▼Instructions:**\n\n| Step | Interpolation Method | Primary Model                       | Secondary Model | Tertiary Model | Merge Name                 |\n| ---- | -------------------- | ----------------------------------- | --------------- | -------------- | -------------------------- |\n| 1    | Add Difference @ 0.5 | anything and everything mix ver.1.5 | Gape60          | NovelAI full   | tempmix-part1 []           |\n| 2    | Add Difference @ 0.3 | tempmix-part1                       | NovelAI full    | NovelAI sfw    | ElderOrangeMix  [3a46a1e0] |\n\n----\n\n## Troubleshooting\n\n1. blurred Images & clearly low quality output  \nIf the generated images are blurred or only clearly low quality output is produced, it is possible that the vae, etc. are not loaded properly. Try reloading the model/vae or restarting the WebUI/OS.\n\n## FAQ and Tips (🐈MEME ZONE🦐)\n\n\nTrash zone.\n\n----\n\n<a name=\"MEME_AOM3A1\"></a>\n\n\n▼Noooo, not work. This guy is Scammer  \nSTEP1: BUY HUGE PC  \n\n\n▼Noooo, can't generate image like samples.This models is hype. \n\n❌  \n<img src=\"https://files.catbox.moe/nte6ud.webp\"  width=\"500\" height=\"\" alt=\"keyboard guy\">  \n\n🟢  \n<img src=\"https://files.catbox.moe/lta462.webp\"  width=\"500\" height=\"\" alt=\"clever guy\">  \n\n\n▼Noooo, This models have troy virus. don't download.  \n\nAll models in this repository are secure. It is most likely that anti-virus software has detected them erroneously.  \nHowever, the models with the .ckpt extension have the potential danger of executing arbitrary code.  \nA safe model that is free from these dangers is the model with the .safetensors extension.  \n\n<a name=\"MEME_realface\"></a>\n▼AOM2?  \n(only NSFW models) \n![](https://github.com/WarriorMama777/imgup/raw/main/img/img_general/img_Neko.webp \"\")\n\n\n▼AOM3A1?  \nR.I.P.  \n\n▼Noooo^()&*%#NG0u!!!!!!!!縺ゅ♀繧?縺医?縺､繝ｼ縺ｨ縺医?縺吶ｊ繝ｼ縺ｯ驕主ｭｦ鄙偵?繧ｴ繝溘〒縺? (「AOM3A2 and A3 are overlearning and Trash. delete!」)\n\n<img src=\"https://github.com/WarriorMama777/imgup/raw/main/img/img_general/img_meme_tension_comp001.webp\"  width=\"300\" height=\"\" alt=”getting_excited”>\n\n\n▼Noooo, Too many models. Tell me which one to choose.  \n\n→ [全部同じじゃないですか](https://github.com/WarriorMama777/imgup/blob/main/img/img_general/img_MEME_whichModel_comp001.webp?raw=true \"全部同じじゃないですか\")\n\n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/WarriorMama777/OrangeMixs"}]},{"id":"lllyasviel/ControlNet","name":"ControlNet","description":"A model for various tasks.","task":"N/A","tags":["license:openrail","region:us"],"likes":3763,"downloads":0,"readme":"---\nlicense: openrail\n---\n\nThis is the pretrained weights and some other detector weights of ControlNet.\n\nSee also: https://github.com/lllyasviel/ControlNet\n\n# Description of Files\n\nControlNet/models/control_sd15_canny.pth\n\n- The ControlNet+SD1.5 model to control SD using canny edge detection.\n\nControlNet/models/control_sd15_depth.pth\n\n- The ControlNet+SD1.5 model to control SD using Midas depth estimation.\n\nControlNet/models/control_sd15_hed.pth\n\n- The ControlNet+SD1.5 model to control SD using HED edge detection (soft edge).\n\nControlNet/models/control_sd15_mlsd.pth\n\n- The ControlNet+SD1.5 model to control SD using M-LSD line detection (will also work with traditional Hough transform).\n\nControlNet/models/control_sd15_normal.pth\n\n- The ControlNet+SD1.5 model to control SD using normal map. Best to use the normal map generated by that Gradio app. Other normal maps may also work as long as the direction is correct (left looks red, right looks blue, up looks green, down looks purple). \n\nControlNet/models/control_sd15_openpose.pth\n\n- The ControlNet+SD1.5 model to control SD using OpenPose pose detection. Directly manipulating pose skeleton should also work.\n\nControlNet/models/control_sd15_scribble.pth\n\n- The ControlNet+SD1.5 model to control SD using human scribbles. The model is trained with boundary edges with very strong data augmentation to simulate boundary lines similar to that drawn by human.\n\nControlNet/models/control_sd15_seg.pth\n\n- The ControlNet+SD1.5 model to control SD using semantic segmentation. The protocol is ADE20k.\n\nControlNet/annotator/ckpts/body_pose_model.pth\n\n- Third-party model: Openpose’s pose detection model.\n\nControlNet/annotator/ckpts/hand_pose_model.pth\n\n- Third-party model: Openpose’s hand detection model.\n\nControlNet/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\n\n- Third-party model: Midas depth estimation model.\n\nControlNet/annotator/ckpts/mlsd_large_512_fp32.pth\n\n- Third-party model: M-LSD detection model.\n\nControlNet/annotator/ckpts/mlsd_tiny_512_fp32.pth\n\n- Third-party model: M-LSD’s another smaller detection model (we do not use this one).\n\nControlNet/annotator/ckpts/network-bsds500.pth\n\n- Third-party model: HED boundary detection.\n\nControlNet/annotator/ckpts/upernet_global_small.pth\n\n- Third-party model: Uniformer semantic segmentation.\n\nControlNet/training/fill50k.zip\n\n- The data for our training tutorial.\n\n# Related Resources\n\nSpecial Thank to the great project - [Mikubill' A1111 Webui Plugin](https://github.com/Mikubill/sd-webui-controlnet) !\n\nWe also thank Hysts for making [Gradio](https://github.com/gradio-app/gradio) demo in [Hugging Face Space](https://huggingface.co/spaces/hysts/ControlNet) as well as more than 65 models in that amazing [Colab list](https://github.com/camenduru/controlnet-colab)! \n\nThank haofanwang for making [ControlNet-for-Diffusers](https://github.com/haofanwang/ControlNet-for-Diffusers)!\n\nWe also thank all authors for making Controlnet DEMOs, including but not limited to [fffiloni](https://huggingface.co/spaces/fffiloni/ControlNet-Video), [other-model](https://huggingface.co/spaces/hysts/ControlNet-with-other-models), [ThereforeGames](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/7784), [RamAnanth1](https://huggingface.co/spaces/RamAnanth1/ControlNet), etc!\n\n# Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/lllyasviel/ControlNet"}]},{"id":"deepseek-ai/Janus-Pro-7B","name":"Janus-Pro-7B","description":"A model for any-to-any.","task":"any-to-any","tags":["transformers","pytorch","multi_modality","muiltimodal","text-to-image","unified-model","any-to-any","arxiv:2501.17811","license:mit","endpoints_compatible","region:us"],"likes":3521,"downloads":76128,"readme":"---\nlicense: mit\nlicense_name: deepseek\nlicense_link: LICENSE\npipeline_tag: any-to-any\nlibrary_name: transformers\ntags:\n- muiltimodal\n- text-to-image\n- unified-model\n---\n\n## 1. Introduction\n\nJanus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation. \nIt addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder’s roles in understanding and generation, but also enhances the framework’s flexibility. \nJanus-Pro surpasses previous unified model and matches or exceeds the performance of task-specific models. \nThe simplicity, high flexibility, and effectiveness of Janus-Pro make it a strong candidate for next-generation unified multimodal models.\n\n[**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n<div align=\"center\">\n<img alt=\"image\" src=\"janus_pro_teaser1.png\" style=\"width:90%;\">\n</div>\n\n<div align=\"center\">\n<img alt=\"image\" src=\"janus_pro_teaser2.png\" style=\"width:90%;\">\n</div>\n\n\n### 2. Model Summary\n\nJanus-Pro is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. \nJanus-Pro is constructed based on the DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base.\n\nFor multimodal understanding, it uses the [SigLIP-L](https://huggingface.co/timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from [here](https://github.com/FoundationVision/LlamaGen) with a downsample rate of 16.\n\n\n\n## 3. Quick Start\n\nPlease refer to [**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n\n## 4. License\n\nThis code repository is licensed under [the MIT License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-CODE). The use of Janus-Pro models is subject to [DeepSeek Model License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-MODEL).\n## 5. Citation\n\n```\n@article{chen2025janus,\n  title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},\n  author={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},\n  journal={arXiv preprint arXiv:2501.17811},\n  year={2025}\n}\n```\n\n## 6. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/Janus-Pro-7B"}]},{"id":"microsoft/phi-2","name":"phi-2","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","phi","text-generation","nlp","code","en","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":3406,"downloads":776584,"readme":"---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n---\n\n## Model Summary\n\nPhi-2 is a Transformer with **2.7 billion** parameters. It was trained using the same data sources as [Phi-1.5](https://huggingface.co/microsoft/phi-1.5), augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n\nOur model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n\n## How to Use\n\nPhi-2 has been integrated in the `transformers` version 4.37.0, please ensure that you are using a version equal or higher than it.\n\nPhi-2 is known for having an attention overflow issue (with FP16). If you are facing this issue, please enable/disable autocast on the [PhiAttention.forward()](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/modeling_phi.py#L306) function.\n\n## Intended Uses\n\nGiven the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n\n### QA Format:\n\nYou can provide the prompt as a standalone question as follows:\n\n```markdown\nWrite a detailed analogy between mathematics and a lighthouse.\n```\nwhere the model generates the text after \".\" . \nTo encourage the model to write more concise answers, you can also try the following QA format using \"Instruct: \\<prompt\\>\\nOutput:\"\n```markdown\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n```\n\nwhere the model generates the text after \"Output:\".\n\n### Chat Format:\n\n```markdown\nAlice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\nBob: Well, have you tried creating a study schedule and sticking to it?\nAlice: Yes, I have, but it doesn't seem to help much.\nBob: Hmm, maybe you should try studying in a quiet environment, like the library.\nAlice: ...\n```\n\nwhere the model generates the text after the first \"Bob:\".\n\n### Code Format:\n\n```python\ndef print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n```\n\nwhere the model generates the text after the comments.\n\n**Notes:**\n\n* Phi-2 is intended for QA, chat, and code purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\n\n* Direct adoption for production tasks without evaluation is out of scope of this project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\n\n* If you are using `transformers<4.37.0`, always load the model with `trust_remote_code=True` to prevent side-effects.\n\n## Sample Code\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n```\n\n## Limitations of Phi-2\n\n* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\n\n## Training\n\n### Model\n\n* Architecture: a Transformer-based model with next-word prediction objective\n\n* Context length: 2048 tokens\n\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n\n* Training tokens: 1.4T tokens\n\n* GPUs: 96xA100-80G\n\n* Training time: 14 days\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/phi-2"}]},{"id":"google/gemma-7b","name":"gemma-7b","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","gguf","gemma","text-generation","arxiv:2305.14314","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","license:gemma","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":3228,"downloads":27694,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/google/gemma-7b"}]},{"id":"stabilityai/stable-diffusion-3.5-large","name":"stable-diffusion-3.5-large","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","diffusers:StableDiffusion3Pipeline","region:us"],"likes":3203,"downloads":215966,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-diffusion-3.5-large"}]},{"id":"stabilityai/stable-video-diffusion-img2vid-xt","name":"stable-video-diffusion-img2vid-xt","description":"A model for image-to-video.","task":"image-to-video","tags":["diffusers","safetensors","image-to-video","license:other","diffusers:StableVideoDiffusionPipeline","region:us"],"likes":3185,"downloads":107322,"readme":"---\npipeline_tag: image-to-video\nlicense: other\nlicense_name: stable-video-diffusion-community\nlicense_link: LICENSE.md\n---\n\n# Stable Video Diffusion Image-to-Video Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.gif)\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. \n\nPlease note: For commercial use, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\n\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. \nThis model was trained to generate 25 frames at resolution 576x1024 given a context frame of the same size, finetuned from [SVD Image-to-Video [14 frames]](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid).\nWe also finetune the widely used [f8-decoder](https://huggingface.co/docs/diffusers/api/models/autoencoderkl#loading-from-the-original-format) for temporal consistency. \nFor convenience, we additionally provide the model with the \nstandard frame-wise decoder [here](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/svd_xt_image_decoder.safetensors).\n\n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative image-to-video model\n- **Finetuned from model:** SVD Image-to-Video [14 frames]\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SVD-Image-to-Video over [GEN-2](https://research.runwayml.com/gen2) and [PikaLabs](https://www.pika.art/).\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\n\n## Uses\n\n### Direct Use\n\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under this [license](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/LICENSE.md). Possible research areas and tasks include\n\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nFor commercial use, please refer to https://stability.ai/license.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\n- The model may generate videos without motion, or very slow camera pans.\n- The model cannot be controlled through text.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for both non-commercial and commercial usage.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models\n\n# Appendix: \n\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters.\nNo explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation.\nNo other third party was involved in the development of this model; the model was fully developed in-house at Stability AI.\nTraining the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh.\nThe released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos.\nWith the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards.\nThe information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. \nThe released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.  \nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AI’s future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. \nFor usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt"}]},{"id":"prompthero/openjourney","name":"openjourney","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","text-to-image","en","license:creativeml-openrail-m","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":3166,"downloads":14584,"readme":"---\ninference: true\nlanguage:\n  - en\ntags:\n  - stable-diffusion\n  - text-to-image\nlicense: creativeml-openrail-m\n---\n# Openjourney is an open source Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\n\nInclude **'mdjrny-v4 style'** in prompt. Here you'll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\n\n# Openjourney Links\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\n- [Openjourney v4](https://huggingface.co/prompthero/openjourney-v2)\n\n# Want to learn AI art generation?:\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\n\n# Use it for free:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\n\n### Stable Diffusion v1.5 vs Openjourney \n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\n\n### 🧨 Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"prompthero/openjourney\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\nimage = pipe(prompt).images[0]\nimage.save(\"./retro_cars.png\")\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/prompthero/openjourney"}]},{"id":"coqui/XTTS-v2","name":"XTTS-v2","description":"A model for text-to-speech.","task":"text-to-speech","tags":["coqui","text-to-speech","license:other","region:us"],"likes":3156,"downloads":5080268,"readme":"---\nlicense: other\nlicense_name: coqui-public-model-license\nlicense_link: https://coqui.ai/cpml\nlibrary_name: coqui\npipeline_tag: text-to-speech\nwidget:\n  - text: \"Once when I was six years old I saw a magnificent picture\"\n---\n\n# ⓍTTS\nⓍTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. There is no need for an excessive amount of training data that spans countless hours.\n\nThis is the same or similar model to what powers [Coqui Studio](https://coqui.ai/) and [Coqui API](https://docs.coqui.ai/docs).\n\n### Features\n- Supports 17 languages. \n- Voice cloning with just a 6-second audio clip.\n- Emotion and style transfer by cloning. \n- Cross-language voice cloning.\n- Multi-lingual speech generation.\n- 24khz sampling rate.\n\n### Updates over XTTS-v1\n- 2 new languages; Hungarian and Korean\n- Architectural improvements for speaker conditioning.\n- Enables the use of multiple speaker references and interpolation between speakers.\n- Stability improvements.\n- Better prosody and audio quality across the board.\n\n### Languages\nXTTS-v2 supports 17 languages: **English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt),\nPolish (pl), Turkish (tr), Russian (ru), Dutch (nl), Czech (cs), Arabic (ar), Chinese (zh-cn), Japanese (ja), Hungarian (hu), Korean (ko)\nHindi (hi)**.\n\nStay tuned as we continue to add support for more languages. If you have any language requests, feel free to reach out!\n\n### Code\nThe [code-base](https://github.com/coqui-ai/TTS) supports inference and [fine-tuning](https://tts.readthedocs.io/en/latest/models/xtts.html#training).\n\n### Demo Spaces\n- [XTTS Space](https://huggingface.co/spaces/coqui/xtts)  :  You can see how model performs on supported languages, and try with your own reference or microphone input\n- [XTTS Voice Chat with Mistral or Zephyr](https://huggingface.co/spaces/coqui/voice-chat-with-mistral) : You can experience streaming voice chat with Mistral 7B Instruct or Zephyr 7B Beta\n\n|                                 |                                         |\n| ------------------------------- | --------------------------------------- |\n| 🐸💬 **CoquiTTS**               | [coqui/TTS on Github](https://github.com/coqui-ai/TTS)|\n| 💼 **Documentation**            | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| 👩‍💻 **Questions**                | [GitHub Discussions](https://github.com/coqui-ai/TTS/discussions) |\n| 🗯 **Community**         | [Discord](https://discord.gg/5eXr5seRrv)  |\n\n\n### License\nThis model is licensed under [Coqui Public Model License](https://coqui.ai/cpml). There's a lot that goes into a license for generative models, and you can read more of [the origin story of CPML here](https://coqui.ai/blog/tts/cpml).\n\n### Contact\nCome and join in our 🐸Community. We're active on [Discord](https://discord.gg/fBC58unbKE) and [Twitter](https://twitter.com/coqui_ai).\nYou can also mail us at info@coqui.ai.\n\nUsing 🐸TTS API:\n\n```python\nfrom TTS.api import TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n\n# generate speech by cloning a voice using default settings\ntts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n                file_path=\"output.wav\",\n                speaker_wav=\"/path/to/target/speaker.wav\",\n                language=\"en\")\n\n```\n\nUsing 🐸TTS Command line:\n\n```console\n tts --model_name tts_models/multilingual/multi-dataset/xtts_v2 \\\n     --text \"Bugün okula gitmek istemiyorum.\" \\\n     --speaker_wav /path/to/target/speaker.wav \\\n     --language_idx tr \\\n     --use_cuda true\n```\n\nUsing the model directly:\n\n```python\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\n\nconfig = XttsConfig()\nconfig.load_json(\"/path/to/xtts/config.json\")\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(config, checkpoint_dir=\"/path/to/xtts/\", eval=True)\nmodel.cuda()\n\noutputs = model.synthesize(\n    \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n    config,\n    speaker_wav=\"/data/TTS-public/_refclips/3.wav\",\n    gpt_cond_len=3,\n    language=\"en\",\n)\n```\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/coqui/XTTS-v2"}]},{"id":"deepseek-ai/DeepSeek-V3-0324","name":"DeepSeek-V3-0324","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","fp8","region:us"],"likes":3076,"downloads":239294,"readme":"---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-V3-0324\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Features\n\nDeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.\n\n![Model Performance](figures/0324_comparison.png)\n\n### Reasoning Capabilities\n\n- Significant improvements in benchmark performance:\n  - MMLU-Pro: 75.9 → 81.2 (+5.3)\n  - GPQA: 59.1 → 68.4 (+9.3)\n  - AIME: 39.6 → 59.4 (+19.8)\n  - LiveCodeBench: 39.2 → 49.2 (+10.0)\n\n### Front-End Web Development\n\n- Improved the executability of the code\n- More aesthetically pleasing web pages and game front-ends\n\n### Chinese Writing Proficiency\n\n- Enhanced style and content quality:\n  - Aligned with the R1 writing style\n  - Better quality in medium-to-long-form writing\n\n- Feature Enhancements\n  - Improved multi-turn interactive rewriting\n  - Optimized translation quality and letter writing\n\n### Chinese Search Capabilities\n\n- Enhanced report analysis requests with more detailed outputs\n\n### Function Calling Improvements\n\n- Increased accuracy in Function Calling, fixing issues from previous V3 versions\n\n---\n\n## Usage Recommendations\n\n### System Prompt\n\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\n\n```\n该助手为DeepSeek Chat，由深度求索公司创造。\n今天是{current date}。\n```\n\nFor example,\n\n```\n该助手为DeepSeek Chat，由深度求索公司创造。\n今天是3月24日，星期一。\n```\n\n### Temperature\n\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3.\n\n$$\nT_{model} = T_{api} \\times 0.3 \\quad (0 \\leq T_{api} \\leq 1)\n$$\n\n$$\nT_{model} = T_{api} - 0.7 \\quad (1 < T_{api} \\leq 2)\n$$\n\nThus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3.\n\n### Prompts for File Uploading and Web Search\n\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\n\n```\nfile_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"\n```\n\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\n\nFor Chinese query, we use the prompt:\n\n```\nsearch_answer_zh_template = \\\n'''# 以下内容是基于用户发送的消息的搜索结果:\n{search_results}\n在我给你的搜索结果中，每个结果都是[webpage X begin]...[webpage X end]格式的，X代表每篇文章的数字索引。请在适当的情况下在句子末尾引用上下文。请按照引用编号[citation:X]的格式在答案中对应部分引用上下文。如果一句话源自多个上下文，请列出所有相关的引用编号，例如[citation:3][citation:5]，切记不要将引用集中在最后返回引用编号，而是在答案对应部分列出。\n在回答时，请注意以下几点：\n- 今天是{cur_date}。\n- 并非搜索结果的所有内容都与用户的问题密切相关，你需要结合问题，对搜索结果进行甄别、筛选。\n- 对于列举类的问题（如列举所有航班信息），尽量将答案控制在10个要点以内，并告诉用户可以查看搜索来源、获得完整信息。优先提供信息完整、最相关的列举项；如非必要，不要主动告诉用户搜索结果未提供的内容。\n- 对于创作类的问题（如写论文），请务必在正文的段落中引用对应的参考编号，例如[citation:3][citation:5]，不能只在文章末尾引用。你需要解读并概括用户的题目要求，选择合适的格式，充分利用搜索结果并抽取重要信息，生成符合用户要求、极具思想深度、富有创造力与专业性的答案。你的创作篇幅需要尽可能延长，对于每一个要点的论述要推测用户的意图，给出尽可能多角度的回答要点，且务必信息量大、论述详尽。\n- 如果回答很长，请尽量结构化、分段落总结。如果需要分点作答，尽量控制在5个点以内，并合并相关的内容。\n- 对于客观类的问答，如果问题的答案非常简短，可以适当补充一到两句相关信息，以丰富内容。\n- 你需要根据用户要求和回答内容选择合适、美观的回答格式，确保可读性强。\n- 你的回答应该综合多个相关网页来回答，不能重复引用一个网页。\n- 除非用户要求，否则你回答的语言需要和用户提问的语言保持一致。\n\n# 用户消息为：\n{question}'''\n```\n\nFor English query, we use the prompt:\n\n```\nsearch_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n\n# The user's message is:\n{question}'''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**This model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer to [DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5#function-calling) repo.**\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-V3-0324"}]},{"id":"mistralai/Mistral-7B-Instruct-v0.2","name":"Mistral-7B-Instruct-v0.2","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","mistral","text-generation","finetuned","mistral-common","conversational","arxiv:2310.06825","license:apache-2.0","autotrain_compatible","text-generation-inference","region:us"],"likes":3012,"downloads":3466767,"readme":"---\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- finetuned\n- mistral-common\nnew_version: mistralai/Mistral-7B-Instruct-v0.3\ninference: false\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-Instruct-v0.2\n\n\n## Encode and Decode with `mistral_common`\n            \n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the `transformers` tokenizer so that it gives 1-to-1 the same results as the `mistral_common` reference implementation are very welcome!\n            \n---\n\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Troubleshooting\n- If you see the following error:\n```\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\n```\n\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\n\nThis should not be required after transformers-v4.33.4.\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"}]},{"id":"openai-community/gpt2","name":"gpt2","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","tf","jax","tflite","rust","onnx","safetensors","gpt2","text-generation","exbert","en","doi:10.57967/hf/0039","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":3011,"downloads":11477729,"readme":"---\nlanguage: en\ntags:\n- exbert\n\nlicense: mit\n---\n\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=gpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai-community/gpt2"}]},{"id":"bigcode/starcoder","name":"starcoder","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","gpt_bigcode","text-generation","code","dataset:bigcode/the-stack-dedup","arxiv:1911.02150","arxiv:2205.14135","arxiv:2207.14255","arxiv:2305.06161","license:bigcode-openrail-m","model-index","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2905,"downloads":11339,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/bigcode/starcoder"}]},{"id":"zai-org/chatglm-6b","name":"chatglm-6b","description":"A model for various tasks.","task":"N/A","tags":["transformers","pytorch","chatglm","glm","thudm","custom_code","zh","en","arxiv:2103.10360","arxiv:2210.02414","arxiv:2406.12793","endpoints_compatible","region:us"],"likes":2867,"downloads":2250,"readme":"---\nlanguage:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm\n---\n# ChatGLM-6B\n<p align=\"center\">\n   🌐 <a href=\"https://chatglm.cn/blog\" target=\"_blank\">Blog</a> • 💻 <a href=\"https://github.com/THUDM/ChatGLM-6B\" target=\"_blank\">Github Repo</a> • 🐦 <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> • 📃 <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> • 📃 <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GLM-130B\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n\n<p align=\"center\">\n    👋 Join our <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-1y7pqoloy-9b1g6T6JjA8J0KxvUjbwJw\" target=\"_blank\">Slack</a> and <a href=\"https://github.com/THUDM/ChatGLM-6B/blob/main/resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n\n<p align=\"center\">\n📍Experience the larger-scale ChatGLM model at <a href=\"https://www.chatglm.cn\">chatglm.cn</a>\n</p>\n\n**我们发布了 [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)，ChatGLM-6B 的升级版本，在保留了了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了更强大的性能、更长的上下文、更高效的推理等升级。**\n## 介绍\nChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 [General Language Model (GLM)](https://github.com/THUDM/GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 [ChatGLM](https://chatglm.cn) 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。 ChatGLM-6B 权重对学术研究**完全开放**，在填写[问卷](https://open.bigmodel.cn/mla/form)进行登记后**亦允许免费商业使用**。\n\nChatGLM-6B is an open bilingual language model based on [General Language Model (GLM)](https://github.com/THUDM/GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference. ChatGLM-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\n## 软件依赖\n\n```shell\npip install protobuf==3.20.0 transformers==4.27.1 icetk cpm_kernels\n```\n\n## 代码调用 \n\n可以通过如下代码调用 ChatGLM-6B 模型来生成对话：\n\n```ipython\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n>>> response, history = model.chat(tokenizer, \"你好\", history=[])\n>>> print(response)\n你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\n>>> response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n>>> print(response)\n晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:\n\n1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。\n2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。\n3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。\n4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。\n5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。\n6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。\n\n如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。\n```\n\n关于更多的使用说明，包括如何运行命令行和网页版本的 DEMO，以及使用模型量化以节省显存，请参考我们的 [Github Repo](https://github.com/THUDM/ChatGLM-6B)。\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM-6B).\n\n## Change Log\n* v1.1.0 ([942945d](https://huggingface.co/THUDM/chatglm-6b/commit/942945df047dee66f653c68ae0e56655045f1741)): 更新 v1.1 版本 checkpoint\n* v0.1.0 ([f831824](https://huggingface.co/THUDM/chatglm-6b/commit/f83182484538e663a03d3f73647f10f89878f438))\n\n## 协议\n\n本仓库的代码依照 [Apache-2.0](LICENSE) 协议开源，ChatGLM-6B 模型的权重的使用则需要遵循 [Model License](MODEL_LICENSE)。\n\n## 引用\n\n如果你觉得我们的工作有帮助的话，请考虑引用下列论文。\n\nIf you find our work helpful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/zai-org/chatglm-6b"}]},{"id":"Qwen/QwQ-32B","name":"QwQ-32B","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","arxiv:2412.15115","base_model:Qwen/Qwen2.5-32B","base_model:finetune:Qwen/Qwen2.5-32B","license:apache-2.0","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2863,"downloads":53346,"readme":"---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/QWQ-32B/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-32B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# QwQ-32B\n\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figures/benchmark.jpg\">\n</p>\n\n\n**This repo contains the QwQ 32B model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training (Supervised Finetuning and Reinforcement Learning)\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 32.5B\n- Number of Paramaters (Non-Embedding): 31.0B\n- Number of Layers: 64\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n- Context Length: Full 131,072 tokens\n    - For prompts exceeding 8,192 tokens in length, you must enable YaRN as outlined in [this section](#usage-guidelines).\n\n**Note:** For the best experience, please review the [usage guidelines](#usage-guidelines) before deploying QwQ models.\n\nYou can try our [demo](https://huggingface.co/spaces/Qwen/QwQ-32B-Demo) or access QwQ models via [QwenChat](https://chat.qwen.ai).\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwq-32b/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nQwQ is based on Qwen2.5, whose code has been in the latest Hugging face `transformers`. We advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/QwQ-32B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"How many r's are in the word \\\"strawberry\\\"\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n```\n\n### Usage Guidelines\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Enforce Thoughtful Output**: Ensure the model starts with \"\\<think\\>\\n\" to prevent generating empty thinking content, which can degrade output quality. If you use `apply_chat_template` and set `add_generation_prompt=True`, this is already automatically implemented, but it may cause the response to lack the \\<think\\> tag at the beginning. This is normal behavior.\n\n2. **Sampling Parameters**:\n   - Use Temperature=0.6, TopP=0.95, MinP=0 instead of Greedy decoding to avoid endless repetitions.\n   - Use TopK between 20 and 40 to filter out rare token occurrences while maintaining the diversity of the generated output.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may result in occasional language mixing and a slight decrease in performance.\n\n3. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. This feature is already implemented in `apply_chat_template`.\n\n4. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g.,`\\\"answer\\\": \\\"C\\\"`.\" in the prompt.\n\n5. **Handle Long Inputs**: For inputs exceeding 8,192 tokens, enable [YaRN](https://arxiv.org/abs/2309.00071) to improve the model's ability to capture long-sequence information effectively.\n\n    For supported frameworks, you could add the following to `config.json` to enable YaRN:\n    ```json\n    {\n    ...,\n    \"rope_scaling\": {\n        \"factor\": 4.0,\n        \"original_max_position_embeddings\": 32768,\n        \"type\": \"yarn\"\n    }\n    }\n    ```\n\n    For deployment, we recommend using vLLM. Please refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\n    Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \n    We advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwq-32b/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwq32b,\n    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},\n    url = {https://qwenlm.github.io/blog/qwq-32b/},\n    author = {Qwen Team},\n    month = {March},\n    year = {2025}\n}\n\n@article{qwen2.5,\n      title={Qwen2.5 Technical Report}, \n      author={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},\n      journal={arXiv preprint arXiv:2412.15115},\n      year={2024}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Qwen/QwQ-32B"}]},{"id":"CompVis/stable-diffusion-v-1-4-original","name":"stable-diffusion-v-1-4-original","description":"A model for text-to-image.","task":"text-to-image","tags":["stable-diffusion","text-to-image","arxiv:2207.12598","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:1910.09700","license:creativeml-openrail-m","region:us"],"likes":2819,"downloads":5,"readme":"---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\nlibrary_name: \"stable-diffusion\"\ninference: false\nextra_gated_prompt: |-\n  One more step before getting this model.\n  This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n  The CreativeML OpenRAIL License specifies: \n\n  1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n  2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n  3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n  Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n  \n  By clicking on \"Access repository\" below, you accept that your *contact information* (email address and username) can be shared with the model authors as well.\n    \nextra_gated_fields:\n I have read the License and agree with its terms: checkbox\n---\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n\nThe **Stable-Diffusion-v-1-4** checkpoint was initialized with the weights of the [Stable-Diffusion-v-1-2](https://steps/huggingface.co/CompVis/stable-diffusion-v-1-2-original) \ncheckpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n#### Download the weights\n- [sd-v1-4.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt)\n- [sd-v1-4-full-ema.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt)\n\nThese weights are intended to be used with the original [CompVis Stable Diffusion codebase](https://github.com/CompVis/stable-diffusion). If you are looking for the model to use with the D🧨iffusers library, [come here](https://huggingface.co/CompVis/stable-diffusion-v1-4).\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n  \n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nWe currently provide three checkpoints, `sd-v1-1.ckpt`, `sd-v1-2.ckpt` and `sd-v1-3.ckpt`,\nwhich were trained as follows,\n\n- `sd-v1-1.ckpt`: 237k steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194k steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- `sd-v1-2.ckpt`: Resumed from `sd-v1-1.ckpt`.\n  515k steps at resolution `512x512` on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- `sd-v1-3.ckpt`: Resumed from `sd-v1-2.ckpt`. 195k steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10\\% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-variants-scores.jpg) \n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original"}]},{"id":"nari-labs/Dia-1.6B","name":"Dia-1.6B","description":"A model for text-to-speech.","task":"text-to-speech","tags":["safetensors","model_hub_mixin","pytorch_model_hub_mixin","text-to-speech","en","arxiv:2305.09636","license:apache-2.0","region:us"],"likes":2801,"downloads":200028,"readme":"---\nlicense: apache-2.0\npipeline_tag: text-to-speech\nlanguage:\n- en\ntags:\n- model_hub_mixin\n- pytorch_model_hub_mixin\nwidget:\n- text: \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\n  example_title: \"Dia intro\"\n- text: \"[S1] Oh fire! Oh my goodness! What's the procedure? What to we do people? The smoke could be coming through an air duct! [S2] Oh my god! Okay.. it's happening. Everybody stay calm! [S1] What's the procedure... [S2] Everybody stay fucking calm!!!... Everybody fucking calm down!!!!! [S1] No! No! If you touch the handle, if its hot there might be a fire down the hallway!\"\n  example_title: \"Panic protocol\"\n---\n\n<center>\n<a href=\"https://github.com/nari-labs/dia\">\n<img src=\"https://github.com/nari-labs/dia/raw/main/dia/static/images/banner.png\">\n</a>\n</center>\n\nDia is a 1.6B parameter text to speech model created by Nari Labs. It was pushed to the Hub using the [PytorchModelHubMixin](https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin) integration.\n\nDia **directly generates highly realistic dialogue from a transcript**. You can condition the output on audio, enabling emotion and tone control. The model can also produce nonverbal communications like laughter, coughing, clearing throat, etc.\n\nTo accelerate research, we are providing access to pretrained model checkpoints and inference code. The model weights are hosted on [Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B). The model only supports English generation at the moment.\n\nWe also provide a [demo page](https://yummy-fir-7a4.notion.site/dia) comparing our model to [ElevenLabs Studio](https://elevenlabs.io/studio) and [Sesame CSM-1B](https://github.com/SesameAILabs/csm).\n\n- (Update) We have a ZeroGPU Space running! Try it now [here](https://huggingface.co/spaces/nari-labs/Dia-1.6B). Thanks to the HF team for the support :)\n- Join our [discord server](https://discord.gg/bJq6vjRRKv) for community support and access to new features.\n- Play with a larger version of Dia: generate fun conversations, remix content, and share with friends. 🔮 Join the [waitlist](https://tally.so/r/meokbo) for early access.\n\n## ⚡️ Quickstart\n\nThis will open a Gradio UI that you can work on.\n\n```bash\ngit clone https://github.com/nari-labs/dia.git\ncd dia && uv run app.py\n```\n\nor if you do not have `uv` pre-installed:\n\n```bash\ngit clone https://github.com/nari-labs/dia.git\ncd dia\npython -m venv .venv\nsource .venv/bin/activate\npip install uv\nuv run app.py\n```\n\nNote that the model was not fine-tuned on a specific voice. Hence, you will get different voices every time you run the model.\nYou can keep speaker consistency by either adding an audio prompt (a guide coming VERY soon - try it with the second example on Gradio for now), or fixing the seed.\n\n## Features\n\n- Generate dialogue via `[S1]` and `[S2]` tag\n- Generate non-verbal like `(laughs)`, `(coughs)`, etc.\n  - Below verbal tags will be recognized, but might result in unexpected output.\n  - `(laughs), (clears throat), (sighs), (gasps), (coughs), (singing), (sings), (mumbles), (beep), (groans), (sniffs), (claps), (screams), (inhales), (exhales), (applause), (burps), (humming), (sneezes), (chuckle), (whistles)`\n- Voice cloning. See [`example/voice_clone.py`](example/voice_clone.py) for more information.\n  - In the Hugging Face space, you can upload the audio you want to clone and place its transcript before your script. Make sure the transcript follows the required format. The model will then output only the content of your script.\n\n## ⚙️ Usage\n\n### As a Python Library\n\n```python\nimport soundfile as sf\n\nfrom dia.model import Dia\n\n\nmodel = Dia.from_pretrained(\"nari-labs/Dia-1.6B\")\n\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\n\noutput = model.generate(text)\n\nsf.write(\"simple.mp3\", output, 44100)\n```\n\nA pypi package and a working CLI tool will be available soon.\n\n## 💻 Hardware and Inference Speed\n\nDia has been tested on only GPUs (pytorch 2.0+, CUDA 12.6). CPU support is to be added soon.\nThe initial run will take longer as the Descript Audio Codec also needs to be downloaded.\n\nOn enterprise GPUs, Dia can generate audio in real-time. On older GPUs, inference time will be slower.\nFor reference, on a A4000 GPU, Dia roughly generates 40 tokens/s (86 tokens equals 1 second of audio).\n`torch.compile` will increase speeds for supported GPUs.\n\nThe full version of Dia requires around 10GB of VRAM to run. We will be adding a quantized version in the future.\n\nIf you don't have hardware available or if you want to play with bigger versions of our models, join the waitlist [here](https://tally.so/r/meokbo).\n\n## 🪪 License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\n## ⚠️ Disclaimer\n\nThis project offers a high-fidelity speech generation model intended for research and educational use. The following uses are **strictly forbidden**:\n\n- **Identity Misuse**: Do not produce audio resembling real individuals without permission.\n- **Deceptive Content**: Do not use this model to generate misleading content (e.g. fake news)\n- **Illegal or Malicious Use**: Do not use this model for activities that are illegal or intended to cause harm.\n\nBy using this model, you agree to uphold relevant legal standards and ethical responsibilities. We **are not responsible** for any misuse and firmly oppose any unethical usage of this technology.\n\n## 🔭 TODO / Future Work\n\n- Docker support.\n- Optimize inference speed.\n- Add quantization for memory efficiency.\n\n## 🤝 Contributing\n\nWe are a tiny team of 1 full-time and 1 part-time research-engineers. We are extra-welcome to any contributions!\nJoin our [Discord Server](https://discord.gg/bJq6vjRRKv) for discussions.\n\n## 🤗 Acknowledgements\n\n- We thank the [Google TPU Research Cloud program](https://sites.research.google/trc/about/) for providing computation resources.\n- Our work was heavily inspired by [SoundStorm](https://arxiv.org/abs/2305.09636), [Parakeet](https://jordandarefsky.com/blog/2024/parakeet/), and [Descript Audio Codec](https://github.com/descriptinc/descript-audio-codec).\n- HuggingFace for providing the ZeroGPU Grant.\n- \"Nari\" is a pure Korean word for lily.\n- We thank Jason Y. for providing help with data filtering.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/nari-labs/Dia-1.6B"}]},{"id":"openai/whisper-large-v3-turbo","name":"whisper-large-v3-turbo","description":"A model for automatic-speech-recognition.","task":"automatic-speech-recognition","tags":["transformers","safetensors","whisper","automatic-speech-recognition","audio","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","base_model:openai/whisper-large-v3","base_model:finetune:openai/whisper-large-v3","license:mit","endpoints_compatible","region:us"],"likes":2675,"downloads":4015779,"readme":"---\nlanguage:\n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- 'no'\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\nlicense: mit\ntags:\n- audio\n- automatic-speech-recognition\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nbase_model:\n- openai/whisper-large-v3\nlibrary_name: transformers\n---\n\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3-turbo is a finetuned version of a pruned [Whisper large-v3](https://huggingface.co/openai/whisper-large-v3). In other words, it's the exact same model, except that the number of decoding layers have reduced from 32 to 4.\nAs a result, the model is way faster, at the expense of a minor quality degradation. You can find more details about it [in this GitHub discussion](https://github.com/openai/whisper/discussions/2363).\n\n**Disclaimer**: Content for this model card has partly been written by the 🤗 Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3-turbo is supported in Hugging Face 🤗 Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install 🤗 Datasets to load toy audio dataset from the Hugging Face Hub, and \n🤗 Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 ⚠️\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"high\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result[\"text\"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation=\"flash_attention_2\"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation=\"sdpa\"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v3) |\n| large-v3-turbo | 809 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v3-turbo) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nNo information provided.\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models’ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box – their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai/whisper-large-v3-turbo"}]},{"id":"deepseek-ai/DeepSeek-OCR","name":"DeepSeek-OCR","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","safetensors","deepseek_vl_v2","feature-extraction","deepseek","vision-language","ocr","custom_code","conversational","image-text-to-text","multilingual","arxiv:2510.18234","license:mit","region:us"],"likes":2578,"downloads":3167241,"readme":"---\npipeline_tag: image-text-to-text\nlanguage:\n- multilingual\ntags:\n- deepseek\n- vision-language\n- ocr\n- custom_code\nlicense: mit\nlibrary_name: transformers\n---\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><b>🌟 Github</b></a> |\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>📥 Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>📄 Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>📄 Arxiv Paper Link</b></a> |\n</p>\n<h2>\n<p align=\"center\">\n  <a href=\"https://huggingface.co/papers/2510.18234\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"https://huggingface.co/papers/2510.18234\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Usage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8：\n\n```\ntorch==2.6.0\ntransformers==4.46.3\ntokenizers==0.20.3\neinops\naddict \neasydict\npip install flash-attn==2.7.3 --no-build-isolation\n```\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\n# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\n```\n\n## vLLM\nRefer to [🌟GitHub](https://github.com/deepseek-ai/DeepSeek-OCR/) for guidance on model inference acceleration and PDF processing, etc.<!--  -->\n\n[2025/10/23] 🚀🚀🚀 DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm).\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n\n\n## Visualizations\n<table>\n<tr>\n<td><img src=\"assets/show1.jpg\" style=\"width: 500px\"></td>\n<td><img src=\"assets/show2.jpg\" style=\"width: 500px\"></td>\n</tr>\n<tr>\n<td><img src=\"assets/show3.jpg\" style=\"width: 500px\"></td>\n<td><img src=\"assets/show4.jpg\" style=\"width: 500px\"></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe would like to thank [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR), [OneChart](https://github.com/LingyvKong/OneChart), [Slow Perception](https://github.com/Ucas-HaoranWei/Slow-Perception) for their valuable models and ideas.\n\nWe also appreciate the benchmarks: [Fox](https://github.com/ucaslcl/Fox), [OminiDocBench](https://github.com/opendatalab/OmniDocBench).\n\n\n## Citation\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-OCR"}]},{"id":"meta-llama/Llama-3.3-70B-Instruct","name":"Llama-3.3-70B-Instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","fr","it","pt","hi","es","th","de","arxiv:2204.05149","base_model:meta-llama/Llama-3.1-70B","base_model:finetune:meta-llama/Llama-3.1-70B","license:llama3.3","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2555,"downloads":667534,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"}]},{"id":"stabilityai/sdxl-turbo","name":"sdxl-turbo","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","onnx","safetensors","text-to-image","license:other","autotrain_compatible","diffusers:StableDiffusionXLPipeline","region:us"],"likes":2478,"downloads":402235,"readme":"---\npipeline_tag: text-to-image\ninference: false\nlicense: other\nlicense_name: sai-nc-community\nlicense_link: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE.md  \n---\n\n# SDXL-Turbo Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.jpg)\nSDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.\nA real-time demo is available here: http://clipdrop.co/stable-diffusion-turbo\n\nPlease note: For commercial use, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\nSDXL-Turbo is a distilled version of [SDXL 1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), trained for real-time synthesis. \nSDXL-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the [technical report](https://stability.ai/research/adversarial-diffusion-distillation)), which allows sampling large-scale foundational \nimage diffusion models in 1 to 4 steps at high image quality. \nThis approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an\nadversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. \n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative text-to-image model\n- **Finetuned from model:** [SDXL 1.0 Base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/adversarial-diffusion-distillation\n- **Demo:** http://clipdrop.co/stable-diffusion-turbo\n\n\n## Evaluation\n![comparison1](image_quality_one_step.png)\n![comparison2](prompt_alignment_one_step.png)\nThe charts above evaluate user preference for SDXL-Turbo over other single- and multi-step models.\nSDXL-Turbo evaluated at a single step is preferred by human voters in terms of image quality and prompt following over LCM-XL evaluated at four (or fewer) steps.\nIn addition, we see that using four steps for SDXL-Turbo further improves performance.\nFor details on the user study, we refer to the [research paper](https://stability.ai/research/adversarial-diffusion-distillation).\n\n\n## Uses\n\n### Direct Use\n\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under this [license](https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE.md). Possible research areas and tasks include\n\n- Research on generative models.\n- Research on real-time applications of generative models.\n- Research on the impact of real-time generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nFor commercial use, please refer to https://stability.ai/membership.\n\nExcluded uses are described below.\n\n### Diffusers\n\n```\npip install diffusers transformers accelerate --upgrade\n```\n\n- **Text-to-image**:\n\nSDXL-Turbo does not make use of `guidance_scale` or `negative_prompt`, we disable it with `guidance_scale=0.0`.\nPreferably, the model generates images of size 512x512 but higher image sizes work as well.\nA **single step** is enough to generate high quality images.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\n\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n```\n\n- **Image-to-image**:\n\nWhen using SDXL-Turbo for image-to-image generation, make sure that `num_inference_steps` * `strength` is larger or equal \nto 1. The image-to-image pipeline will run for `int(num_inference_steps * strength)` steps, *e.g.* 0.5 * 2.0 = 1 step in our example \nbelow.\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\n\npipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\").resize((512, 512))\n\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n\nimage = pipe(prompt, image=init_image, num_inference_steps=2, strength=0.5, guidance_scale=0.0).images[0]\n```\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for both non-commercial and commercial usage.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/sdxl-turbo"}]},{"id":"BAAI/bge-m3","name":"bge-m3","description":"A model for sentence-similarity.","task":"sentence-similarity","tags":["sentence-transformers","pytorch","onnx","xlm-roberta","feature-extraction","sentence-similarity","arxiv:2402.03216","arxiv:2004.04906","arxiv:2106.14807","arxiv:2107.05720","arxiv:2004.12832","license:mit","autotrain_compatible","text-embeddings-inference","endpoints_compatible","region:us"],"likes":2474,"downloads":7010285,"readme":"---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\nlicense: mit\n---\n\nFor more details please refer to our github repo: https://github.com/FlagOpen/FlagEmbedding\n\n# BGE-M3 ([paper](https://arxiv.org/pdf/2402.03216.pdf), [code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3))\n\nIn this project, we introduce BGE-M3, which is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity. \n- Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. \n- Multi-Linguality: It can support more than 100 working languages. \n- Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. \n\n\n\n**Some suggestions for retrieval pipeline in RAG**\n\nWe recommend to use the following pipeline: hybrid retrieval + re-ranking. \n- Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities. \nA classic example: using both embedding retrieval and the BM25 algorithm. \nNow, you can try to use BGE-M3, which supports both embedding and sparse retrieval. \nThis allows you to obtain token weights (similar to the BM25) without any additional cost when generate dense embeddings.\nTo use hybrid retrieval, you can refer to [Vespa](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb\n) and [Milvus](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n\n- As cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model. \nUtilizing the re-ranking model (e.g., [bge-reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker), [bge-reranker-v2](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker)) after retrieval can further filter the selected text.\n\n\n## News:\n- 2024/7/1: **We update the MIRACL evaluation results of BGE-M3**. To reproduce the new results, you can refer to: [bge-m3_miracl_2cr](https://huggingface.co/datasets/hanhainebula/bge-m3_miracl_2cr). We have also updated our [paper](https://arxiv.org/pdf/2402.03216) on arXiv.\n  <details>\n  <summary> Details </summary>\n\n  The previous test results were lower because we mistakenly removed the passages that have the same id as the query from the search results. After correcting this mistake, the overall performance of BGE-M3 on MIRACL is higher than the previous results, but the experimental conclusion remains unchanged. The other results are not affected by this mistake. To reproduce the previous lower results, you need to add the `--remove-query` parameter when using `pyserini.search.faiss` or `pyserini.search.lucene` to search the passages.\n\n  </details>\n- 2024/3/20: **Thanks Milvus team!** Now you can use hybrid retrieval of bge-m3 in Milvus: [pymilvus/examples\n/hello_hybrid_sparse_dense.py](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n- 2024/3/8: **Thanks for the [experimental results](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05) from @[Yannael](https://huggingface.co/Yannael). In this benchmark, BGE-M3 achieves top performance in both English and other languages, surpassing models such as OpenAI.**\n- 2024/3/2: Release unified fine-tuning [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/unified_finetune) and [data](https://huggingface.co/datasets/Shitao/bge-m3-data) \n- 2024/2/6: We release the [MLDR](https://huggingface.co/datasets/Shitao/MLDR) (a long document retrieval dataset covering 13 languages) and [evaluation pipeline](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR). \n- 2024/2/1: **Thanks for the excellent tool from Vespa.** You can easily use multiple modes of BGE-M3 following this [notebook](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb)\n\n\n## Specs\n\n- Model  \n\n| Model Name |  Dimension | Sequence Length | Introduction |\n|:----:|:---:|:---:|:---:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) | 1024 | 8192 | multilingual; unified fine-tuning (dense, sparse, and colbert) from bge-m3-unsupervised|\n| [BAAI/bge-m3-unsupervised](https://huggingface.co/BAAI/bge-m3-unsupervised) | 1024 | 8192 | multilingual; contrastive learning from bge-m3-retromae |\n| [BAAI/bge-m3-retromae](https://huggingface.co/BAAI/bge-m3-retromae) | -- | 8192 | multilingual; extend the max_length of [xlm-roberta](https://huggingface.co/FacebookAI/xlm-roberta-large) to 8192 and further pretrained via [retromae](https://github.com/staoxiao/RetroMAE)| \n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 | English model | \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | English model | \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | English model | \n\n- Data\n\n|                          Dataset                           |                   Introduction                    |\n|:----------------------------------------------------------:|:-------------------------------------------------:|\n|    [MLDR](https://huggingface.co/datasets/Shitao/MLDR)     | Docuemtn Retrieval Dataset, covering 13 languages |\n| [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data) |          Fine-tuning data used by bge-m3          |\n\n\n\n## FAQ\n\n**1. Introduction for different retrieval methods**\n\n- Dense retrieval: map the text into a single embedding, e.g., [DPR](https://arxiv.org/abs/2004.04906), [BGE-v1.5](https://github.com/FlagOpen/FlagEmbedding)\n- Sparse retrieval (lexical matching): a vector of size equal to the vocabulary, with the majority of positions set to zero, calculating a weight only for tokens present in the text. e.g., BM25, [unicoil](https://arxiv.org/pdf/2106.14807.pdf), and [splade](https://arxiv.org/abs/2107.05720)\n- Multi-vector retrieval: use multiple vectors to represent a text, e.g., [ColBERT](https://arxiv.org/abs/2004.12832).\n\n\n**2. How to use BGE-M3 in other projects?**\n\nFor embedding retrieval, you can employ the BGE-M3 model using the same approach as BGE. \nThe only difference is that the BGE-M3 model no longer requires adding instructions to the queries. \n\nFor hybrid retrieval, you can use [Vespa](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb\n) and [Milvus](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n\n\n**3. How to fine-tune bge-M3 model?**\n\nYou can follow the common in this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) \nto fine-tune the dense embedding.\n\nIf you want to fine-tune all embedding function of m3 (dense, sparse and colbert), you can refer to the [unified_fine-tuning example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/unified_finetune)\n\n\n\n\n\n\n## Usage\n\nInstall: \n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\npip install -e .\n```\nor: \n```\npip install -U FlagEmbedding\n```\n\n\n\n### Generate Embedding for text\n\n- Dense Embedding\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  \n                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nembeddings_1 = model.encode(sentences_1, \n                            batch_size=12, \n                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n                            )['dense_vecs']\nembeddings_2 = model.encode(sentences_2)['dense_vecs']\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# [[0.6265, 0.3477], [0.3499, 0.678 ]]\n```\nYou also can use sentence-transformers and huggingface transformers to generate dense embeddings.\nRefer to [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding#usage) for details.\n\n\n- Sparse Embedding (Lexical Weight)\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n\n# you can see the weight for each token:\nprint(model.convert_id_to_token(output_1['lexical_weights']))\n# [{'What': 0.08356, 'is': 0.0814, 'B': 0.1296, 'GE': 0.252, 'M': 0.1702, '3': 0.2695, '?': 0.04092}, \n#  {'De': 0.05005, 'fin': 0.1368, 'ation': 0.04498, 'of': 0.0633, 'BM': 0.2515, '25': 0.3335}]\n\n\n# compute the scores via lexical mathcing\nlexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][0])\nprint(lexical_scores)\n# 0.19554901123046875\n\nprint(model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_1['lexical_weights'][1]))\n# 0.0\n```\n\n- Multi-Vector (ColBERT)\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=True)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][0]))\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][1]))\n# 0.7797\n# 0.4620\n```\n\n\n### Compute score for text pairs\nInput a list of text pairs, you can get the scores computed by different methods.\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nsentence_pairs = [[i,j] for i in sentences_1 for j in sentences_2]\n\nprint(model.compute_score(sentence_pairs, \n                          max_passage_length=128, # a smaller max length leads to a lower latency\n                          weights_for_different_modes=[0.4, 0.2, 0.4])) # weights_for_different_modes(w) is used to do weighted sum: w[0]*dense_score + w[1]*sparse_score + w[2]*colbert_score\n\n# {\n#   'colbert': [0.7796499729156494, 0.4621465802192688, 0.4523794651031494, 0.7898575067520142], \n#   'sparse': [0.195556640625, 0.00879669189453125, 0.0, 0.1802978515625], \n#   'dense': [0.6259765625, 0.347412109375, 0.349853515625, 0.67822265625], \n#   'sparse+dense': [0.482503205537796, 0.23454029858112335, 0.2332356721162796, 0.5122477412223816], \n#   'colbert+sparse+dense': [0.6013619303703308, 0.3255828022956848, 0.32089319825172424, 0.6232916116714478]\n# }\n```\n\n\n\n\n## Evaluation  \n\nWe provide the evaluation script for [MKQA](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MKQA) and [MLDR](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR)\n\n### Benchmarks from the open-source community\n  ![avatar](./imgs/others.webp)\n The BGE-M3 model emerged as the top performer on this benchmark (OAI is short for OpenAI). \n  For more details, please refer to the [article](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05) and [Github Repo](https://github.com/Yannael/multilingual-embeddings)\n\n\n### Our results\n- Multilingual (Miracl dataset) \n\n![avatar](./imgs/miracl.jpg)\n\n- Cross-lingual (MKQA dataset)\n\n![avatar](./imgs/mkqa.jpg)\n\n- Long Document Retrieval\n  - MLDR:   \n  ![avatar](./imgs/long.jpg)\n  Please note that [MLDR](https://huggingface.co/datasets/Shitao/MLDR) is a document retrieval dataset we constructed via LLM, \n  covering 13 languages, including test set, validation set, and training set. \n  We utilized the training set from MLDR to enhance the model's long document retrieval capabilities. \n  Therefore, comparing baselines with `Dense w.o.long`(fine-tuning without long document dataset) is more equitable. \n  Additionally, this long document retrieval dataset will be open-sourced to address the current lack of open-source multilingual long text retrieval datasets.\n  We believe that this data will be helpful for the open-source community in training document retrieval models.\n\n  - NarritiveQA:  \n  ![avatar](./imgs/nqa.jpg)\n\n- Comparison with BM25  \n\nWe utilized Pyserini to implement BM25, and the test results can be reproduced by this [script](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR#bm25-baseline).\nWe tested BM25 using two different tokenizers: \none using Lucene Analyzer and the other using the same tokenizer as M3 (i.e., the tokenizer of xlm-roberta). \nThe results indicate that BM25 remains a competitive baseline, \nespecially in long document retrieval.\n\n![avatar](./imgs/bm25.jpg)\n\n\n\n## Training\n- Self-knowledge Distillation: combining multiple outputs from different \nretrieval modes as reward signal to enhance the performance of single mode(especially for sparse retrieval and multi-vec(colbert) retrival)\n- Efficient Batching: Improve the efficiency when fine-tuning on long text. \nThe small-batch strategy is simple but effective, which also can used to fine-tune large embedding model.\n- MCLS: A simple method to improve the performance on long text without fine-tuning. \nIf you have no enough resource to fine-tuning model with long text, the method is useful.\n\nRefer to our [report](https://arxiv.org/pdf/2402.03216.pdf) for more details. \n\n\n\n\n\n\n## Acknowledgement\n\nThanks to the authors of open-sourced datasets, including Miracl, MKQA, NarritiveQA, etc. \nThanks to the open-sourced libraries like [Tevatron](https://github.com/texttron/tevatron), [Pyserini](https://github.com/castorini/pyserini).\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/BAAI/bge-m3"}]},{"id":"google-bert/bert-base-uncased","name":"bert-base-uncased","description":"A model for fill-mask.","task":"fill-mask","tags":["transformers","pytorch","tf","jax","rust","coreml","onnx","safetensors","bert","fill-mask","exbert","en","dataset:bookcorpus","dataset:wikipedia","arxiv:1810.04805","license:apache-2.0","autotrain_compatible","endpoints_compatible","region:us"],"likes":2462,"downloads":55476414,"readme":"---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/google-bert/bert-base-uncased"}]},{"id":"hakurei/waifu-diffusion","name":"waifu-diffusion","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","text-to-image","en","license:creativeml-openrail-m","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":2458,"downloads":2719,"readme":"---\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\nlicense: creativeml-openrail-m\ninference: true\n\n---\n\n# waifu-diffusion v1.4 - Diffusion for Weebs\n\nwaifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n\n![image](https://user-images.githubusercontent.com/26317155/210155933-db3a5f1a-1ec3-4777-915c-6deff2841ce9.png)\n\n<sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub>\n\n[Original Weights](https://huggingface.co/hakurei/waifu-diffusion-v1-4)\n\n# Gradio & Colab\n\nWe also support a [Gradio](https://github.com/gradio-app/gradio) Web UI and Colab with Diffusers to run Waifu Diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/hakurei/waifu-diffusion-demo)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_8wPN7dJO746QXsFnB09Uq2VGgSRFuYE#scrollTo=1HaCauSq546O)\n\n## Model Description\n\n[See here for a full model overview.](https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1)\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)\n\n## Downstream Uses\n\nThis model can be used for entertainment purposes and as a generative art assistant.\n\n## Example Code\n\n```python\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\n    'hakurei/waifu-diffusion',\n    torch_dtype=torch.float32\n).to('cuda')\n\nprompt = \"1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=6)[\"sample\"][0]  \n    \nimage.save(\"test.png\")\n```\n\n## Team Members and Acknowledgements\n\nThis project would not have been possible without the incredible work by Stability AI and Novel AI.\n\n- [Haru](https://github.com/harubaru)\n- [Salt](https://github.com/sALTaccount/)\n- [Sta @ Bit192](https://twitter.com/naclbbr)\n\nIn order to reach us, you can join our [Discord server](https://discord.gg/touhouai).\n\n[![Discord Server](https://discordapp.com/api/guilds/930499730843250783/widget.png?style=banner2)](https://discord.gg/touhouai)","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/hakurei/waifu-diffusion"}]},{"id":"tiiuae/falcon-40b","name":"falcon-40b","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","falcon","text-generation","custom_code","en","de","es","fr","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2101.00027","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","license:apache-2.0","autotrain_compatible","text-generation-inference","region:us"],"likes":2433,"downloads":8889,"readme":"---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\n- de\n- es\n- fr\ninference: false\nlicense: apache-2.0\n---\n\n# 🚀 Falcon-40B\n\n**Falcon-40B is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon 😊.*\n\n\n🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-40B?\n\n* **It is the best open-source model currently available.** Falcon-40B outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n* **It is made available under a permissive Apache 2.0 license allowing for commercial use**, without any royalties or restrictions.\n* \n⚠️ **This is a raw, pretrained model, which should be further finetuned for most usecases.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct). \n\n💸 **Looking for a smaller, less expensive model?** [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) is Falcon-40B's little brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\n💥 **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 85-100GB of memory** to swiftly run inference with Falcon-40B.\n\n# Model Card for Falcon-40B\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish);\n- **License:** Apache 2.0 license.\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nResearch on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-40B is trained mostly on English, German, Spanish, French, with limited capabilities also in in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish. It will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-40B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-40B was trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile ([Gao et al., 2020](https://arxiv.org/abs/2101.00027)). \n\n| **Data source**    | **Fraction** | **Tokens** | **Sources**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 75%          | 750B     | massive web crawl                 |\n| RefinedWeb-Europe              | 7%           | 70B       | European massive web crawl                                   |\n| Books  | 6%           | 60B        |                  |\n| Conversations      | 5%           | 50B        | Reddit, StackOverflow, HackerNews |\n| Code               | 5%           | 50B        |                                   |\n| Technical          | 2%           | 20B        | arXiv, PubMed, USPTO, etc.        |\n\nRefinedWeb-Europe is made of the following languages:\n\n| **Language** | **Fraction of multilingual data** | **Tokens** |\n|--------------|-----------------------------------|------------|\n| German       | 26%                               | 18B        |\n| Spanish      | 24%                               | 17B        |\n| French       | 23%                               | 16B        |\n| _Italian_    | 7%                                | 5B         |\n| _Portuguese_ | 4%                                | 3B         |\n| _Polish_     | 4%                                | 3B         |\n| _Dutch_      | 4%                                | 3B         |\n| _Romanian_   | 3%                                | 2B         |\n| _Czech_      | 3%                                | 2B         |\n| _Swedish_    | 2%                                | 1B         |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n### Training Procedure \n\nFalcon-40B was trained on 384 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=4, DP=12) combined with ZeRO.\n\n#### Training Hyperparameters\n\n| **Hyperparameter** | **Value**  | **Comment**                               |\n|--------------------|------------|-------------------------------------------|\n| Precision          | `bfloat16` |                                           |\n| Optimizer          | AdamW      |                                           |\n| Learning rate      | 1.85e-4       | 4B tokens warm-up, cosine decay to 1.85e-5 |\n| Weight decay       | 1e-1       |                                           |\n| Z-loss       | 1e-4       |                                           |\n| Batch size         | 1152        | 100B tokens ramp-up                         |\n\n\n#### Speeds, Sizes, Times\n\nTraining started in December 2022 and took two months. \n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\n### Model Architecture and Objective\n\nFalcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a two layer norms.\n\nFor multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 60        |                                        |\n| `d_model`          | 8192      |                                        |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-40B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* 😊. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-40B is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/tiiuae/falcon-40b"}]},{"id":"black-forest-labs/FLUX.1-Kontext-dev","name":"FLUX.1-Kontext-dev","description":"A model for image-to-image.","task":"image-to-image","tags":["diffusers","safetensors","image-generation","flux","diffusion-single-file","image-to-image","en","arxiv:2506.15742","license:other","diffusers:FluxKontextPipeline","region:us"],"likes":2413,"downloads":253049,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev"}]},{"id":"deepseek-ai/DeepSeek-R1-0528","name":"DeepSeek-R1-0528","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2501.12948","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","fp8","region:us"],"likes":2386,"downloads":556260,"readme":"---\r\nlicense: mit\r\nlibrary_name: transformers\r\n---\r\n# DeepSeek-R1-0528\r\n<!-- markdownlint-disable first-line-h1 -->\r\n<!-- markdownlint-disable html -->\r\n<!-- markdownlint-disable no-duplicate-header -->\r\n\r\n<div align=\"center\">\r\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\r\n</div>\r\n<hr>\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\r\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n \r\n\r\n<p align=\"center\">\r\n  <a href=\"https://arxiv.org/pdf/2501.12948\"><b>Paper Link</b>👁️</a>\r\n</p>\r\n\r\n\r\n## 1. Introduction\r\n\r\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\r\n\r\n<p align=\"center\">\r\n  <img width=\"80%\" src=\"figures/benchmark.png\">\r\n</p>\r\n\r\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the model’s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\r\n\r\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\r\n\r\n## 2. Evaluation Results\r\n\r\n### DeepSeek-R1-0528\r\n For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\r\n<div align=\"center\">\r\n\r\n| Category | Benchmark (Metric)               | DeepSeek R1     | DeepSeek R1 0528\r\n|----------|----------------------------------|-----------------|---|\r\n| General  |\r\n|          | MMLU-Redux (EM)                   | 92.9            | 93.4\r\n|          | MMLU-Pro (EM)                     | 84.0            | 85.0\r\n|          | GPQA-Diamond (Pass@1)             | 71.5            | 81.0\r\n|          | SimpleQA (Correct)                | 30.1            | 27.8\r\n|          | FRAMES (Acc.)                     | 82.5            | 83.0\r\n|          | Humanity's Last Exam (Pass@1)                     | 8.5            | 17.7\r\n| Code |\r\n|          | LiveCodeBench (2408-2505) (Pass@1)        | 63.5          | 73.3\r\n|          | Codeforces-Div1 (Rating)          | 1530            | 1930\r\n|          | SWE Verified (Resolved)           | 49.2            | 57.6\r\n|          | Aider-Polyglot (Acc.)             | 53.3            | 71.6\r\n| Math |\r\n|          | AIME 2024 (Pass@1)                | 79.8            | 91.4\r\n|          | AIME 2025 (Pass@1)                     | 70.0           | 87.5\r\n|          | HMMT 2025 (Pass@1)            | 41.7 | 79.4 |\r\n|          | CNMO 2024 (Pass@1)                | 78.8            | 86.9\r\n| Tools |\r\n|          | BFCL_v3_MultiTurn (Acc)     | -            | 37.0 |\r\n|          | Tau-Bench   (Pass@1)       | -            | 53.5(Airline)/63.9(Retail)\r\n\r\n</div>\r\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\r\n\r\n### DeepSeek-R1-0528-Qwen3-8B\r\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\r\n\r\n|                                | AIME 24 | AIME 25 | HMMT Feb 25 | GPQA Diamond | LiveCodeBench (2408-2505) |\r\n|--------------------------------|---------|---------|-------------|--------------|---------------------------|\r\n| Qwen3-235B-A22B\t                | 85.7    | 81.5    | 62.5        | 71.1         | 66.5                  |\r\n| Qwen3-32B                      | 81.4    | 72.9    | -           | 68.4         | -                         |\r\n| Qwen3-8B                      | 76.0   | 67.3    | -           | 62.0       | -                         |\r\n| Phi-4-Reasoning-Plus-14B       | 81.3    | 78.0    | 53.6        | 69.3         | -          |\r\n| Gemini-2.5-Flash-Thinking-0520 | 82.3    | 72.0    | 64.2        | 82.8         | 62.3                  |\r\n| o3-mini (medium)               | 79.6    | 76.7    | 53.3        | 76.8         | 65.9                     |\r\n| DeepSeek-R1-0528-Qwen3-8B      | 86.0   | 76.3    | 61.5        | 61.1         | 60.5                      |\r\n\r\n## 3. Chat Website & API Platform\r\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in), and switch on the button \"DeepThink\"\r\n\r\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\r\n\r\n## 4. How to Run Locally\r\n\r\nPlease visit [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) repository for more information about running DeepSeek-R1-0528 locally.\r\n\r\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\r\n\r\n1. System prompt is supported now.\r\n2. It is not required to add \"\\<think\\>\\n\" at the beginning of the output to force the model into thinking pattern.\r\n\r\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B.\r\n\r\n### System Prompt\r\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\r\n```\r\n该助手为DeepSeek-R1，由深度求索公司创造。\r\n今天是{current date}。\r\n```\r\nFor example,\r\n```\r\n该助手为DeepSeek-R1，由深度求索公司创造。\r\n今天是2025年5月28日，星期一。\r\n```\r\n### Temperature\r\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. \r\n### Prompts for File Uploading and Web Search\r\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\r\n```\r\nfile_template = \\\r\n\"\"\"[file name]: {file_name}\r\n[file content begin]\r\n{file_content}\r\n[file content end]\r\n{question}\"\"\"\r\n```\r\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\r\nFor Chinese query, we use the prompt:\r\n```\r\nsearch_answer_zh_template = \\\r\n'''# 以下内容是基于用户发送的消息的搜索结果:\r\n{search_results}\r\n在我给你的搜索结果中，每个结果都是[webpage X begin]...[webpage X end]格式的，X代表每篇文章的数字索引。请在适当的情况下在句子末尾引用上下文。请按照引用编号[citation:X]的格式在答案中对应部分引用上下文。如果一句话源自多个上下文，请列出所有相关的引用编号，例如[citation:3][citation:5]，切记不要将引用集中在最后返回引用编号，而是在答案对应部分列出。\r\n在回答时，请注意以下几点：\r\n- 今天是{cur_date}。\r\n- 并非搜索结果的所有内容都与用户的问题密切相关，你需要结合问题，对搜索结果进行甄别、筛选。\r\n- 对于列举类的问题（如列举所有航班信息），尽量将答案控制在10个要点以内，并告诉用户可以查看搜索来源、获得完整信息。优先提供信息完整、最相关的列举项；如非必要，不要主动告诉用户搜索结果未提供的内容。\r\n- 对于创作类的问题（如写论文），请务必在正文的段落中引用对应的参考编号，例如[citation:3][citation:5]，不能只在文章末尾引用。你需要解读并概括用户的题目要求，选择合适的格式，充分利用搜索结果并抽取重要信息，生成符合用户要求、极具思想深度、富有创造力与专业性的答案。你的创作篇幅需要尽可能延长，对于每一个要点的论述要推测用户的意图，给出尽可能多角度的回答要点，且务必信息量大、论述详尽。\r\n- 如果回答很长，请尽量结构化、分段落总结。如果需要分点作答，尽量控制在5个点以内，并合并相关的内容。\r\n- 对于客观类的问答，如果问题的答案非常简短，可以适当补充一到两句相关信息，以丰富内容。\r\n- 你需要根据用户要求和回答内容选择合适、美观的回答格式，确保可读性强。\r\n- 你的回答应该综合多个相关网页来回答，不能重复引用一个网页。\r\n- 除非用户要求，否则你回答的语言需要和用户提问的语言保持一致。\r\n# 用户消息为：\r\n{question}'''\r\n```\r\nFor English query, we use the prompt:\r\n```\r\nsearch_answer_en_template = \\\r\n'''# The following contents are the search results related to the user's message:\r\n{search_results}\r\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\r\nWhen responding, please keep the following points in mind:\r\n- Today is {cur_date}.\r\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\r\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\r\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\r\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\r\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\r\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\r\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\r\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\r\n# The user's message is:\r\n{question}'''\r\n```\r\n\r\n## 5. License\r\nThis code repository is licensed under [MIT License](LICENSE). The use of DeepSeek-R1 models is also subject to [MIT License](LICENSE). DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\r\n\r\n## 6. Citation\r\n```\r\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\r\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \r\n      author={DeepSeek-AI},\r\n      year={2025},\r\n      eprint={2501.12948},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.CL},\r\n      url={https://arxiv.org/abs/2501.12948}, \r\n}\r\n```\r\n\r\n## 7. Contact\r\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\r\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"}]},{"id":"xai-org/grok-1","name":"grok-1","description":"A model for text-generation.","task":"text-generation","tags":["grok","grok-1","text-generation","license:apache-2.0","region:us"],"likes":2368,"downloads":1120,"readme":"---\nlicense: apache-2.0\npipeline_tag: text-generation\nlibrary_name: grok\ntags:\n- grok-1\n---\n# Grok-1\n\nThis repository contains the weights of the Grok-1 open-weights model. You can find the code in the [GitHub Repository](https://github.com/xai-org/grok-1/tree/main).\n\n# Download instruction\nClone the repo & download the `int8` checkpoint to the `checkpoints` directory by executing this command in the repo root directory:\n\n```shell\ngit clone https://github.com/xai-org/grok-1.git && cd grok-1\npip install huggingface_hub[hf_transfer]\nhuggingface-cli download xai-org/grok-1 --repo-type model --include ckpt-0/* --local-dir checkpoints --local-dir-use-symlinks False\n```\n\nThen, you can run:\n\n```shell\npip install -r requirements.txt\npython run.py\n```\n\nYou should be seeing output from the language model.\n\nDue to the large size of the model (314B parameters), a multi-GPU machine is required to test the model with the example code.\n\np.s. we're hiring: https://x.ai/careers","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/xai-org/grok-1"}]},{"id":"perplexity-ai/r1-1776","name":"r1-1776","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","base_model:deepseek-ai/DeepSeek-R1","base_model:finetune:deepseek-ai/DeepSeek-R1","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2324,"downloads":856,"readme":"---\nlicense: mit\nbase_model:\n- deepseek-ai/DeepSeek-R1\nlibrary_name: transformers\n---\n\n# R1 1776\n\nBlog link: [https://perplexity.ai/hub/blog/open-sourcing-r1-1776](https://perplexity.ai/hub/blog/open-sourcing-r1-1776 ) \n\nR1 1776 is a DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship. \nThe model provides unbiased, accurate, and factual information while maintaining high reasoning capabilities.\n\n## Evals\n\nTo ensure our model remains fully “uncensored” and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/675c8332d01f593dc90817f5/GiN2VqC5hawUgAGJ6oHla.png)\n\nWe also ensured that the model’s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/675c8332d01f593dc90817f5/n4Z9Byqp2S7sKUvCvI40R.png)","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/perplexity-ai/r1-1776"}]},{"id":"sesame/csm-1b","name":"csm-1b","description":"A model for text-to-speech.","task":"text-to-speech","tags":["transformers","safetensors","csm","text-to-audio","text-to-speech","en","license:apache-2.0","endpoints_compatible","region:us"],"likes":2257,"downloads":19562,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/sesame/csm-1b"}]},{"id":"mistralai/Mistral-7B-Instruct-v0.3","name":"Mistral-7B-Instruct-v0.3","description":"A model for various tasks.","task":"N/A","tags":["vllm","safetensors","mistral","mistral-common","base_model:mistralai/Mistral-7B-v0.3","base_model:finetune:mistralai/Mistral-7B-v0.3","license:apache-2.0","region:us"],"likes":2237,"downloads":1209683,"readme":"---\nlibrary_name: vllm\nlicense: apache-2.0\nbase_model: mistralai/Mistral-7B-v0.3\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- vllm\n- mistral-common\n---\n\n# Model Card for Mistral-7B-Instruct-v0.3\n\nThe Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.\n\nMistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)\n- Extended vocabulary to 32768\n- Supports v3 Tokenizer\n- Supports function calling\n\n## Installation\n\nIt is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256\n```\n\n### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Generate with `transformers`\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\nchatbot(messages)\n```\n\n\n## Function calling with `transformers`\n\nTo use this example, you'll need `transformers` version 4.42.0 or higher. Please see the \n[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)\nin the `transformers` docs for more information.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    \"\"\"\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n    \"\"\"\n    pass\n\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), \nand note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\n\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"}]},{"id":"moonshotai/Kimi-K2-Instruct","name":"Kimi-K2-Instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","kimi_k2","text-generation","conversational","custom_code","doi:10.57967/hf/5976","license:other","autotrain_compatible","endpoints_compatible","fp8","region:us"],"likes":2231,"downloads":84333,"readme":"---\nlicense: other\nlicense_name: modified-mit\nlibrary_name: transformers\nnew_version: moonshotai/Kimi-K2-Instruct-0905\n---\n<div align=\"center\">\n  <picture>\n      <img src=\"figures/kimi-logo.png\" width=\"30%\" alt=\"Kimi K2: Open Agentic Intellignece\">\n  </picture>\n</div>\n\n<hr>\n\n<div align=\"center\" style=\"line-height:1\">\n  <a href=\"https://www.kimi.com\" target=\"_blank\"><img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-Kimi%20K2-ff6b6b?color=1783ff&logoColor=white\"/></a>\n  <a href=\"https://github.com/moonshotai/Kimi-K2\"><img alt=\"github\" src=\"https://img.shields.io/badge/🤖%20Github-Kimi%20K2-ff6b6b?color=1783ff&logoColor=white\"/></a>\n  <a href=\"https://www.moonshot.ai\" target=\"_blank\"><img alt=\"Homepage\" src=\"https://img.shields.io/badge/Homepage-Moonshot%20AI-white?logo=Kimi&logoColor=white\"/></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://huggingface.co/moonshotai\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Moonshot%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/kimi_moonshot\" target=\"_blank\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-Kimi.ai-white?logo=x&logoColor=white\"/></a>\n    <a href=\"https://discord.gg/TYU2fdJykW\" target=\"_blank\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Kimi.ai-white?logo=discord&logoColor=white\"/></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/moonshotai/Kimi-K2/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-Modified_MIT-f5de53?&color=f5de53\"/></a>\n</div>\n\n<p align=\"center\">\n<b>📰&nbsp;&nbsp;<a href=\"https://moonshotai.github.io/Kimi-K2/\">Tech Blog</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>📄&nbsp;&nbsp;<a href=\"https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf\">Paper</a></b>\n</p>\n\n## 0. Changelog\n### 2025.8.11\n- Messages with `name` field are now supported. We’ve also moved the chat template to a standalone file for easier viewing.\n### 2025.7.18\n- We further modified our chat template to improve its robustness. The default system prompt has also been updated.\n### 2025.7.15\n- We have updated our tokenizer implementation. Now special tokens like `[EOS]` can be encoded to their token ids.\n- We fixed a bug in the chat template that was breaking multi-turn tool calls.\n\n## 1. Model Introduction\n\nKimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n\n### Key Features\n- Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\n- MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\n- Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.\n\n### Model Variants\n- **Kimi-K2-Base**: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.\n- **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.\n\n<div align=\"center\">\n  <picture>\n      <img src=\"figures/banner.png\" width=\"80%\" alt=\"Evaluation Results\">\n  </picture>\n</div>\n\n## 2. Model Summary\n\n<div align=\"center\">\n\n\n| | |\n|:---:|:---:|\n| **Architecture** | Mixture-of-Experts (MoE) |\n| **Total Parameters** | 1T |\n| **Activated Parameters** | 32B |\n| **Number of Layers** (Dense layer included) | 61 |\n| **Number of Dense Layers** | 1 |\n| **Attention Hidden Dimension** | 7168 |\n| **MoE Hidden Dimension** (per Expert) | 2048 |\n| **Number of Attention Heads** | 64 |\n| **Number of Experts** | 384 |\n| **Selected Experts per Token** | 8 |\n| **Number of Shared Experts** | 1 |\n| **Vocabulary Size** | 160K |\n| **Context Length** | 128K |\n| **Attention Mechanism** | MLA |\n| **Activation Function** | SwiGLU |\n</div>\n\n## 3. Evaluation Results\n\n#### Instruction model evaluation results\n\n<div align=\"center\">\n<table>\n<thead>\n<tr>\n<th align=\"center\">Benchmark</th>\n<th align=\"center\">Metric</th>\n<th align=\"center\"><sup>Kimi K2 Instruct</sup></th>\n<th align=\"center\"><sup>DeepSeek-V3-0324</sup></th>\n<th align=\"center\"><sup>Qwen3-235B-A22B <br><sup>(non-thinking)</sup></sup></th>\n<th align=\"center\"><sup>Claude Sonnet 4 <br><sup>(w/o extended thinking)</sup></sup></th>\n<th align=\"center\"><sup>Claude Opus 4 <br><sup>(w/o extended thinking)</sup></sup></th>\n<th align=\"center\"><sup>GPT-4.1</sup></th>\n<th align=\"center\"><sup>Gemini 2.5 Flash <br> Preview (05-20)</sup></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\" colspan=9><strong>Coding Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">LiveCodeBench v6<br><sup>(Aug 24 - May 25)</sup></td>\n<td align=\"center\">Pass@1</td>\n<td align=\"center\"><strong>53.7</strong></td>\n<td align=\"center\">46.9</td>\n<td align=\"center\">37.0</td>\n<td align=\"center\">48.5</td>\n<td align=\"center\">47.4</td>\n<td align=\"center\">44.7</td>\n<td align=\"center\">44.7</td>\n</tr>\n<tr>\n<td align=\"center\">OJBench</td>\n<td align=\"center\">Pass@1</td>\n<td align=\"center\"><strong>27.1</strong></td>\n<td align=\"center\">24.0</td>\n<td align=\"center\">11.3</td>\n<td align=\"center\">15.3</td>\n<td align=\"center\">19.6</td>\n<td align=\"center\">19.5</td>\n<td align=\"center\">19.5</td>\n</tr>\n\n<tr>\n<td align=\"center\">MultiPL-E</td>\n<td align=\"center\">Pass@1</td>\n<td align=\"center\"><ins><strong>85.7</strong></ins></td>\n<td align=\"center\">83.1</td>\n<td align=\"center\">78.2</td>\n<td align=\"center\">88.6</td>\n<td align=\"center\"><strong>89.6</strong></td>\n<td align=\"center\">86.7</td>\n<td align=\"center\">85.6</td>\n</tr>\n\n<tr>\n<td align=\"center\">SWE-bench Verified <br/><sup>(Agentless Coding)</sup></td>\n<td align=\"center\">Single Patch w/o Test (Acc)</td>\n<td align=\"center\"><ins><strong>51.8</strong></ins></td>\n<td align=\"center\">36.6</td>\n<td align=\"center\">39.4</td>\n<td align=\"center\">50.2</td>\n<td align=\"center\"><strong>53.0</strong></td>\n<td align=\"center\">40.8</td>\n<td align=\"center\">32.6</td>\n</tr>\n\n<tr>\n<td align=\"center\" rowspan=\"2\">SWE-bench Verified <br/> <sup>(Agentic Coding)</sup></td>\n<td align=\"center\">Single Attempt (Acc)</td>\n<td align=\"center\"><ins><strong>65.8</strong></ins></td>\n<td align=\"center\">38.8</td>\n<td align=\"center\">34.4</td>\n<td align=\"center\"><strong>72.7</strong><sup>*</sup></td>\n<td align=\"center\">72.5<sup>*</sup></td>\n<td align=\"center\">54.6</td>\n<td align=\"center\">—</td>\n</tr>\n\n<tr>\n<!--<td align=\"center\">(Agentic Coding)</td>-->\n<td align=\"center\">Multiple Attempts (Acc)</td>\n<td align=\"center\"><ins><strong>71.6</strong></ins></td>\n<td align=\"center\">—</td>\n<td align=\"center\">—</td>\n<td align=\"center\"><strong>80.2</strong></td>\n<td align=\"center\">79.4<sup>*</sup></td>\n<td align=\"center\">—</td>\n<td align=\"center\">—</td>\n</tr>\n\n<tr>\n<td align=\"center\">SWE-bench Multilingual<br /> <sup>(Agentic Coding)</sup></td>\n<td align=\"center\">Single Attempt (Acc)</td>\n<td align=\"center\"><ins><strong>47.3</strong> </ins></td>\n<td align=\"center\">25.8</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\"><strong>51.0</strong></td>\n<td align=\"center\">—</td>\n<td align=\"center\">31.5</td>\n<td align=\"center\">—</td>\n</tr>\n\n<tr>\n<td align=\"center\" rowspan=\"2\">TerminalBench</td>\n<td align=\"center\">Inhouse Framework (Acc)</td>\n<td align=\"center\"><ins><strong>30.0</strong></ins></td>\n<td align=\"center\">—</td>\n<td align=\"center\">—</td>\n<td align=\"center\">35.5</td>\n<td align=\"center\"><strong>43.2</strong></td>\n<td align=\"center\">8.3</td>\n<td align=\"center\">—</td>\n</tr>\n\n<tr>\n<!--<td align=\"center\">TerminalBench</td>-->\n<td align=\"center\">Terminus (Acc)</td>\n<td align=\"center\"><ins><strong>25.0</strong> </ins></td>\n<td align=\"center\">16.3</td>\n<td align=\"center\">6.6</td>\n<td align=\"center\">—</td>\n<td align=\"center\">—</td>\n<td align=\"center\"><strong>30.3</strong></td>\n<td align=\"center\">16.8</td>\n</tr>\n<tr>\n<td align=\"center\">Aider-Polyglot</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\">60.0</td>\n<td align=\"center\">55.1</td>\n<td align=\"center\"><ins><strong>61.8</strong></ins></td>\n<td align=\"center\">56.4</td>\n<td align=\"center\"><strong>70.7</strong></td>\n<td align=\"center\">52.4</td>\n<td align=\"center\">44.0</td>\n</tr>\n<tr>\n<td align=\"center\" colspan=9><strong>Tool Use Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">Tau2 retail</td>\n<td align=\"center\">Avg@4</td>\n<td align=\"center\"><ins><strong>70.6</strong></ins></td>\n<td align=\"center\">69.1</td>\n<td align=\"center\">57.0</td>\n<td align=\"center\">75.0</td>\n<td align=\"center\"><strong>81.8</strong></td>\n<td align=\"center\">74.8</td>\n<td align=\"center\">64.3</td>\n</tr>\n<tr>\n<td align=\"center\">Tau2 airline</td>\n<td align=\"center\">Avg@4</td>\n<td align=\"center\"><ins><strong>56.5</strong></ins></td>\n<td align=\"center\">39.0</td>\n<td align=\"center\">26.5</td>\n<td align=\"center\">55.5</td>\n<td align=\"center\"><strong>60.0</strong></td>\n<td align=\"center\">54.5</td>\n<td align=\"center\">42.5</td>\n</tr>\n<tr>\n<td align=\"center\">Tau2 telecom</td>\n<td align=\"center\">Avg@4</td>\n<td align=\"center\"><strong>65.8</strong></td>\n<td align=\"center\">32.5</td>\n<td align=\"center\">22.1</td>\n<td align=\"center\">45.2</td>\n<td align=\"center\">57.0</td>\n<td align=\"center\">38.6</td>\n<td align=\"center\">16.9</td>\n</tr>\n<tr>\n<td align=\"center\">AceBench</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\"><ins><strong>76.5</strong></ins></td>\n<td align=\"center\">72.7</td>\n<td align=\"center\">70.5</td>\n<td align=\"center\">76.2</td>\n<td align=\"center\">75.6</td>\n<td align=\"center\"><strong>80.1</strong></td>\n<td align=\"center\">74.5</td>\n</tr>\n<tr>\n<td align=\"center\" colspan=9><strong>Math &amp; STEM Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">AIME 2024</td>\n<td align=\"center\">Avg@64</td>\n<td align=\"center\"><strong>69.6</strong></td>\n<td align=\"center\">59.4<sup>*</sup></td>\n<td align=\"center\">40.1<sup>*</sup></td>\n<td align=\"center\">43.4</td>\n<td align=\"center\">48.2</td>\n<td align=\"center\">46.5</td>\n<td align=\"center\">61.3</td>\n</tr>\n<tr>\n<td align=\"center\">AIME 2025</td>\n<td align=\"center\">Avg@64</td>\n<td align=\"center\"><strong>49.5</strong></td>\n<td align=\"center\">46.7</td>\n<td align=\"center\">24.7<sup>*</sup></td>\n<td align=\"center\">33.1<sup>*</sup></td>\n<td align=\"center\">33.9<sup>*</sup></td>\n<td align=\"center\">37.0</td>\n<td align=\"center\">46.6</td>\n</tr>\n<tr>\n<td align=\"center\">MATH-500</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\"><strong>97.4</strong></td>\n<td align=\"center\">94.0<sup>*</sup></td>\n<td align=\"center\">91.2<sup>*</sup></td>\n<td align=\"center\">94.0</td>\n<td align=\"center\">94.4</td>\n<td align=\"center\">92.4</td>\n<td align=\"center\">95.4</td>\n</tr>\n<tr>\n<td align=\"center\">HMMT 2025</td>\n<td align=\"center\">Avg@32</td>\n<td align=\"center\"><strong>38.8</strong></td>\n<td align=\"center\">27.5</td>\n<td align=\"center\">11.9</td>\n<td align=\"center\">15.9</td>\n<td align=\"center\">15.9</td>\n<td align=\"center\">19.4</td>\n<td align=\"center\">34.7</td>\n</tr>\n<tr>\n<td align=\"center\">CNMO 2024</td>\n<td align=\"center\">Avg@16</td>\n<td align=\"center\">74.3</td>\n<td align=\"center\"><ins><strong>74.7</strong></ins></td>\n<td align=\"center\">48.6</td>\n<td align=\"center\">60.4</td>\n<td align=\"center\">57.6</td>\n<td align=\"center\">56.6</td>\n<td align=\"center\"><strong>75.0</strong></td>\n</tr>\n<tr>\n<td align=\"center\">PolyMath-en</td>\n<td align=\"center\">Avg@4</td>\n<td align=\"center\"><strong>65.1</strong></td>\n<td align=\"center\">59.5</td>\n<td align=\"center\">51.9</td>\n<td align=\"center\">52.8</td>\n<td align=\"center\">49.8</td>\n<td align=\"center\">54.0</td>\n<td align=\"center\">49.9</td>\n</tr>\n\n<tr>\n<td align=\"center\">ZebraLogic</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\"><strong>89.0</strong></td>\n<td align=\"center\">84.0</td>\n<td align=\"center\">37.7<sup>*</sup></td>\n<td align=\"center\">73.7</td>\n<td align=\"center\">59.3</td>\n<td align=\"center\">58.5</td>\n<td align=\"center\">57.9</td>\n</tr>\n\n<tr>\n<td align=\"center\">AutoLogi</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\"><ins><strong>89.5</strong></ins></td>\n<td align=\"center\">88.9</td>\n<td align=\"center\">83.3</td>\n<td align=\"center\"><strong>89.8</strong></td>\n<td align=\"center\">86.1</td>\n<td align=\"center\">88.2</td>\n<td align=\"center\">84.1</td>\n</tr>\n\n<tr>\n<td align=\"center\">GPQA-Diamond</td>\n<td align=\"center\">Avg@8</td>\n<td align=\"center\"><strong>75.1</strong></td>\n<td align=\"center\">68.4<sup>*</sup></td>\n<td align=\"center\">62.9<sup>*</sup></td>\n<td align=\"center\">70.0<sup>*</sup></td>\n<td align=\"center\">74.9<sup>*</sup></td>\n<td align=\"center\">66.3</td>\n<td align=\"center\">68.2</td>\n</tr>\n\n<tr>\n<td align=\"center\">SuperGPQA</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\"><strong>57.2</strong></td>\n<td align=\"center\">53.7</td>\n<td align=\"center\">50.2</td>\n<td align=\"center\">55.7</td>\n<td align=\"center\">56.5</td>\n<td align=\"center\">50.8</td>\n<td align=\"center\">49.6</td>\n</tr>\n\n<tr>\n<td align=\"center\">Humanity's Last Exam<br><sup>(Text Only)</sup></td>\n<td align=\"center\">-</td>\n<td align=\"center\">4.7</td>\n<td align=\"center\">5.2</td>\n<td align=\"center\"><ins><strong>5.7</strong></ins></td>\n<td align=\"center\">5.8</td>\n<td align=\"center\"><strong>7.1</strong></td>\n<td align=\"center\">3.7</td>\n<td align=\"center\">5.6</td>\n</tr>\n\n<tr>\n<td align=\"center\" colspan=9><strong>General Tasks</strong></td>\n</tr>\n\n<tr>\n<td align=\"center\">MMLU</td>\n<td align=\"center\">EM</td>\n<td align=\"center\"><ins><strong>89.5</strong></ins></td>\n<td align=\"center\">89.4</td>\n<td align=\"center\">87.0</td>\n<td align=\"center\">91.5</td>\n<td align=\"center\"><strong>92.9</strong></td>\n<td align=\"center\">90.4</td>\n<td align=\"center\">90.1</td>\n</tr>\n\n<tr>\n<td align=\"center\">MMLU-Redux</td>\n<td align=\"center\">EM</td>\n<td align=\"center\"><ins><strong>92.7</strong></ins></td>\n<td align=\"center\">90.5</td>\n<td align=\"center\">89.2</td>\n<td align=\"center\">93.6</td>\n<td align=\"center\"><strong>94.2</strong></td>\n<td align=\"center\">92.4</td>\n<td align=\"center\">90.6</td>\n</tr>\n\n<tr>\n<td align=\"center\">MMLU-Pro</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">81.1</td>\n<td align=\"center\"><ins><strong>81.2</strong></ins><sup>*</sup></td>\n<td align=\"center\">77.3</td>\n<td align=\"center\">83.7</td>\n<td align=\"center\"><strong>86.6</strong></td>\n<td align=\"center\">81.8</td>\n<td align=\"center\">79.4</td>\n</tr>\n\n<tr>\n<td align=\"center\">IFEval</td>\n<td align=\"center\">Prompt Strict</td>\n<td align=\"center\"><strong>89.8</strong></td>\n<td align=\"center\">81.1</td>\n<td align=\"center\">83.2<sup>*</sup></td>\n<td align=\"center\">87.6</td>\n<td align=\"center\">87.4</td>\n<td align=\"center\">88.0</td>\n<td align=\"center\">84.3</td>\n</tr>\n\n<tr>\n<td align=\"center\">Multi-Challenge</td>\n<td align=\"center\">Acc</td>\n<td align=\"center\"><strong>54.1</strong></td>\n<td align=\"center\">31.4</td>\n<td align=\"center\">34.0</td>\n<td align=\"center\">46.8</td>\n<td align=\"center\">49.0</td>\n<td align=\"center\">36.4</td>\n<td align=\"center\">39.5</td>\n</tr>\n\n<tr>\n<td align=\"center\">SimpleQA</td>\n<td align=\"center\">Correct</td>\n<td align=\"center\"><ins><strong>31.0</strong></ins></td>\n<td align=\"center\">27.7</td>\n<td align=\"center\">13.2</td>\n<td align=\"center\">15.9</td>\n<td align=\"center\">22.8</td>\n<td align=\"center\"><strong>42.3</strong></td>\n<td align=\"center\">23.3</td>\n</tr>\n\n<tr>\n<td align=\"center\">Livebench</td>\n<td align=\"center\">Pass@1</td>\n<td align=\"center\"><strong>76.4</strong></td>\n<td align=\"center\">72.4</td>\n<td align=\"center\">67.6</td>\n<td align=\"center\">74.8</td>\n<td align=\"center\">74.6</td>\n<td align=\"center\">69.8</td>\n<td align=\"center\">67.8</td>\n</tr>\n</tbody>\n</table>\n</div>\n<sup>\n• Bold denotes global SOTA, and underlined denotes open-source SOTA.\n</sup><br/><sup>\n• Data points marked with * are taken directly from the model's tech report or blog.\n</sup><br/><sup>\n• All metrics, except for SWE-bench Verified (Agentless), are evaluated with an 8k output token length. SWE-bench Verified (Agentless) is limited to a 16k output token length.\n</sup><br/><sup>\n• Kimi K2 achieves 65.8% pass@1 on the SWE-bench Verified tests with bash/editor tools (single-attempt patches, no test-time compute). It also achieves a 47.3% pass@1 on the SWE-bench Multilingual tests under the same conditions. Additionally, we report results on SWE-bench Verified tests (71.6%) that leverage parallel test-time compute by sampling multiple sequences and selecting the single best via an internal scoring model.\n</sup><br/><sup>\n• To ensure the stability of the evaluation, we employed avg@k on the AIME, HMMT, CNMO, PolyMath-en, GPQA-Diamond, EvalPlus, Tau2.\n</sup><br/><sup>\n• Some data points have been omitted due to prohibitively expensive evaluation costs.\n    </sup>\n\n---\n\n#### Base model evaluation results\n\n<div align=\"center\">\n\n<table>\n<thead>\n<tr>\n<th align=\"center\">Benchmark</th>\n<th align=\"center\">Metric</th>\n<th align=\"center\">Shot</th>\n<th align=\"center\">Kimi K2 Base</th>\n<th align=\"center\">Deepseek-V3-Base</th>\n<th align=\"center\">Qwen2.5-72B</th>\n<th align=\"center\">Llama 4 Maverick</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\" colspan=\"7\"><strong>General Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">MMLU</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>87.8</strong></td>\n<td align=\"center\">87.1</td>\n<td align=\"center\">86.1</td>\n<td align=\"center\">84.9</td>\n</tr>\n<tr>\n<td align=\"center\">MMLU-pro</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>69.2</strong></td>\n<td align=\"center\">60.6</td>\n<td align=\"center\">62.8</td>\n<td align=\"center\">63.5</td>\n</tr>\n<tr>\n<td align=\"center\">MMLU-redux-2.0</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>90.2</strong></td>\n<td align=\"center\">89.5</td>\n<td align=\"center\">87.8</td>\n<td align=\"center\">88.2</td>\n</tr>\n<tr>\n<td align=\"center\">SimpleQA</td>\n<td align=\"center\">Correct</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>35.3</strong></td>\n<td align=\"center\">26.5</td>\n<td align=\"center\">10.3</td>\n<td align=\"center\">23.7</td>\n</tr>\n<tr>\n<td align=\"center\">TriviaQA</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>85.1</strong></td>\n<td align=\"center\">84.1</td>\n<td align=\"center\">76.0</td>\n<td align=\"center\">79.3</td>\n</tr>\n<tr>\n<td align=\"center\">GPQA-Diamond</td>\n<td align=\"center\">Avg@8</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\">48.1</td>\n<td align=\"center\"><strong>50.5</strong></td>\n<td align=\"center\">40.8</td>\n<td align=\"center\">49.4</td>\n</tr>\n<tr>\n<td align=\"center\">SuperGPQA</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>44.7</strong></td>\n<td align=\"center\">39.2</td>\n<td align=\"center\">34.2</td>\n<td align=\"center\">38.8</td>\n</tr>\n<tr>\n<td align=\"center\" colspan=\"7\"><strong>Coding Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">LiveCodeBench v6</td>\n<td align=\"center\">Pass@1</td>\n<td align=\"center\">1-shot</td>\n<td align=\"center\"><strong>26.3</strong></td>\n<td align=\"center\">22.9</td>\n<td align=\"center\">21.1</td>\n<td align=\"center\">25.1</td>\n</tr>\n<tr>\n<td align=\"center\">EvalPlus</td>\n<td align=\"center\">Pass@1</td>\n<td align=\"center\">-</td>\n<td align=\"center\"><strong>80.3</strong></td>\n<td align=\"center\">65.6</td>\n<td align=\"center\">66.0</td>\n<td align=\"center\">65.5</td>\n</tr>\n<tr>\n<td align=\"center\" colspan=\"7\"><strong>Mathematics Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">MATH</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">4-shot</td>\n<td align=\"center\"><strong>70.2</strong></td>\n<td align=\"center\">60.1</td>\n<td align=\"center\">61.0</td>\n<td align=\"center\">63.0</td>\n</tr>\n<tr>\n<td align=\"center\">GSM8k</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">8-shot</td>\n<td align=\"center\"><strong>92.1</strong></td>\n<td align=\"center\">91.7</td>\n<td align=\"center\">90.4</td>\n<td align=\"center\">86.3</td>\n</tr>\n<tr>\n<td align=\"center\" colspan=\"7\"><strong>Chinese Tasks</strong></td>\n</tr>\n<tr>\n<td align=\"center\">C-Eval</td>\n<td align=\"center\">EM</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>92.5</strong></td>\n<td align=\"center\">90.0</td>\n<td align=\"center\">90.9</td>\n<td align=\"center\">80.9</td>\n</tr>\n<tr>\n<td align=\"center\">CSimpleQA</td>\n<td align=\"center\">Correct</td>\n<td align=\"center\">5-shot</td>\n<td align=\"center\"><strong>77.6</strong></td>\n<td align=\"center\">72.1</td>\n<td align=\"center\">50.5</td>\n<td align=\"center\">53.5</td>\n</tr>\n</tbody>\n</table>\n</div>\n<sup>\n• We only evaluate open-source pretrained models in this work. We report results for Qwen2.5-72B because the base checkpoint for Qwen3-235B-A22B was not open-sourced at the time of our study.\n</sup><br/><sup>\n• All models are evaluated using the same evaluation protocol.\n\n</sup>\n\n\n## 4. Deployment\n> [!Note]\n> You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you.\n>\n> The Anthropic-compatible API maps temperature by `real_temperature = request_temperature * 0.6` for better compatible with existing applications.\n\nOur model checkpoints are stored in the block-fp8 format, you can find it on [Huggingface](https://huggingface.co/moonshotai/Kimi-K2-Instruct).\n\nCurrently, Kimi-K2 is recommended to run on the following inference engines:\n\n* vLLM\n* SGLang\n* KTransformers\n* TensorRT-LLM\n\nDeployment examples for vLLM and SGLang can be found in the [Model Deployment Guide](docs/deploy_guidance.md).\n\n---\n\n## 5. Model Usage\n\n### Chat Completion\n\nOnce the local inference service is up, you can interact with it through the chat endpoint:\n\n```python\ndef simple_chat(client: OpenAI, model_name: str):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Please give a brief self-introduction.\"}]},\n    ]\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        stream=False,\n        temperature=0.6,\n        max_tokens=256\n    )\n    print(response.choices[0].message.content)\n```\n\n> [!NOTE]\n> The recommended temperature for Kimi-K2-Instruct is `temperature = 0.6`.\n> If no special instructions are required, the system prompt above is a good default.\n\n---\n\n### Tool Calling\n\nKimi-K2-Instruct has strong tool-calling capabilities.\nTo enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.\n\nThe following example demonstrates calling a weather tool end-to-end:\n\n```python\n# Your tool implementation\ndef get_weather(city: str) -> dict:\n    return {\"weather\": \"Sunny\"}\n\n# Tool schema definition\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Retrieve current weather information. Call this when the user asks about the weather.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"required\": [\"city\"],\n            \"properties\": {\n                \"city\": {\n                    \"type\": \"string\",\n                    \"description\": \"Name of the city\"\n                }\n            }\n        }\n    }\n}]\n\n# Map tool names to their implementations\ntool_map = {\n    \"get_weather\": get_weather\n}\n\ndef tool_call_with_client(client: OpenAI, model_name: str):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n        {\"role\": \"user\", \"content\": \"What's the weather like in Beijing today? Use the tool to check.\"}\n    ]\n    finish_reason = None\n    while finish_reason is None or finish_reason == \"tool_calls\":\n        completion = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            temperature=0.6,\n            tools=tools,          # tool list defined above\n            tool_choice=\"auto\"\n        )\n        choice = completion.choices[0]\n        finish_reason = choice.finish_reason\n        if finish_reason == \"tool_calls\":\n            messages.append(choice.message)\n            for tool_call in choice.message.tool_calls:\n                tool_call_name = tool_call.function.name\n                tool_call_arguments = json.loads(tool_call.function.arguments)\n                tool_function = tool_map[tool_call_name]\n                tool_result = tool_function(**tool_call_arguments)\n                print(\"tool_result:\", tool_result)\n\n                messages.append({\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                    \"name\": tool_call_name,\n                    \"content\": json.dumps(tool_result)\n                })\n    print(\"-\" * 100)\n    print(choice.message.content)\n```\n\nThe `tool_call_with_client` function implements the pipeline from user query to tool execution.\nThis pipeline requires the inference engine to support Kimi-K2’s native tool-parsing logic.\nFor streaming output and manual tool-parsing, see the [Tool Calling Guide](docs/tool_call_guidance.md).\n\n---\n\n## 6. License\n\nBoth the code repository and the model weights are released under the [Modified MIT License](LICENSE).\n\n---\n\n## 7. Third Party Notices\n\nSee [THIRD PARTY NOTICES](THIRD_PARTY_NOTICES.md)\n\n---\n\n## 7. Contact Us\n\nIf you have any questions, please reach out at [support@moonshot.cn](mailto:support@moonshot.cn).","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/moonshotai/Kimi-K2-Instruct"}]},{"id":"meta-llama/Llama-2-70b-chat-hf","name":"Llama-2-70b-chat-hf","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","conversational","en","arxiv:2307.09288","license:llama2","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2199,"downloads":21799,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"}]},{"id":"meta-llama/Llama-2-7b-hf","name":"Llama-2-7b-hf","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","en","arxiv:2307.09288","license:llama2","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2192,"downloads":584686,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-2-7b-hf"}]},{"id":"Qwen/Qwen-Image","name":"Qwen-Image","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","text-to-image","en","zh","arxiv:2508.02324","license:apache-2.0","diffusers:QwenImagePipeline","region:us"],"likes":2188,"downloads":193015,"readme":"---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nlibrary_name: diffusers\npipeline_tag: text-to-image\n---\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png\" width=\"400\"/>\n<p>\n<p align=\"center\">\n          💜 <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbsp🤗 <a href=\"https://huggingface.co/Qwen/Qwen-Image\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf\">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen-image/\">Blog</a> &nbsp&nbsp \n<br>\n🖥️ <a href=\"https://huggingface.co/spaces/Qwen/qwen-image\">Demo</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp🫨 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg\" width=\"1600\"/>\n<p>\n\n## Introduction\nWe are thrilled to release **Qwen-Image**, an image generation foundation model in the Qwen series that achieves significant advances in **complex text rendering** and **precise image editing**. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.\n\n![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center)\n\n## News\n- 2025.08.04: We released the [Technical Report](https://arxiv.org/abs/2508.02324) of Qwen-Image!\n- 2025.08.04: We released Qwen-Image weights! Check at [huggingface](https://huggingface.co/Qwen/Qwen-Image) and [Modelscope](https://modelscope.cn/models/Qwen/Qwen-Image)!\n- 2025.08.04: We released Qwen-Image! Check our [blog](https://qwenlm.github.io/blog/qwen-image) for more details!\n\n\n## Quick Start\n\nInstall the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nThe following contains a code snippet illustrating how to use the model to generate images based on text prompts:\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\nmodel_name = \"Qwen/Qwen-Image\"\n\n# Load the pipeline\nif torch.cuda.is_available():\n    torch_dtype = torch.bfloat16\n    device = \"cuda\"\nelse:\n    torch_dtype = torch.float32\n    device = \"cpu\"\n\npipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype)\npipe = pipe.to(device)\n\npositive_magic = {\n    \"en\": \", Ultra HD, 4K, cinematic composition.\", # for english prompt\n    \"zh\": \", 超清，4K，电影级构图.\" # for chinese prompt\n}\n\n# Generate image\nprompt = '''A coffee shop entrance features a chalkboard sign reading \"Qwen Coffee 😊 $2 per cup,\" with a neon light beside it displaying \"通义千问\". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written \"π≈3.1415926-53589793-23846264-33832795-02384197\". Ultra HD, 4K, cinematic composition'''\n\nnegative_prompt = \" \" # using an empty string if you do not have specific concept to remove\n\n\n# Generate with different aspect ratios\naspect_ratios = {\n    \"1:1\": (1328, 1328),\n    \"16:9\": (1664, 928),\n    \"9:16\": (928, 1664),\n    \"4:3\": (1472, 1140),\n    \"3:4\": (1140, 1472),\n    \"3:2\": (1584, 1056),\n    \"2:3\": (1056, 1584),\n}\n\nwidth, height = aspect_ratios[\"16:9\"]\n\nimage = pipe(\n    prompt=prompt + positive_magic[\"en\"],\n    negative_prompt=negative_prompt,\n    width=width,\n    height=height,\n    num_inference_steps=50,\n    true_cfg_scale=4.0,\n    generator=torch.Generator(device=\"cuda\").manual_seed(42)\n).images[0]\n\nimage.save(\"example.png\")\n```\n\n## Show Cases\n\nOne of its standout capabilities is high-fidelity text rendering across diverse images. Whether it’s alphabetic languages like English or logographic scripts like Chinese, Qwen-Image preserves typographic details, layout coherence, and contextual harmony with stunning accuracy. Text isn’t just overlaid—it’s seamlessly integrated into the visual fabric.\n\n![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s1.jpg#center)\n\nBeyond text, Qwen-Image excels at general image generation with support for a wide range of artistic styles. From photorealistic scenes to impressionist paintings, from anime aesthetics to minimalist design, the model adapts fluidly to creative prompts, making it a versatile tool for artists, designers, and storytellers.\n\n![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s2.jpg#center)\n\nWhen it comes to image editing, Qwen-Image goes far beyond simple adjustments. It enables advanced operations such as style transfer, object insertion or removal, detail enhancement, text editing within images, and even human pose manipulation—all with intuitive input and coherent output. This level of control brings professional-grade editing within reach of everyday users.\n\n![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg#center)\n\nBut Qwen-Image doesn’t just create or edit—it understands. It supports a suite of image understanding tasks, including object detection, semantic segmentation, depth and edge (Canny) estimation, novel view synthesis, and super-resolution. These capabilities, while technically distinct, can all be seen as specialized forms of intelligent image editing, powered by deep visual comprehension.\n\n![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s4.jpg#center)\n\nTogether, these features make Qwen-Image not just a tool for generating pretty pictures, but a comprehensive foundation model for intelligent visual creation and manipulation—where language, layout, and imagery converge.\n\n\n## License Agreement\n\nQwen-Image is licensed under Apache 2.0. \n\n## Citation\n\nWe kindly encourage citation of our work if you find it useful.\n\n```bibtex\n@misc{wu2025qwenimagetechnicalreport,\n      title={Qwen-Image Technical Report}, \n      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\n      year={2025},\n      eprint={2508.02324},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.02324}, \n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Qwen/Qwen-Image"}]},{"id":"microsoft/phi-4","name":"phi-4","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","phi3","text-generation","phi","nlp","math","code","chat","conversational","en","arxiv:2412.08905","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2187,"downloads":509344,"readme":"---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/phi-4/resolve/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- phi\n- nlp\n- math\n- code\n- chat\n- conversational\ninference:\n  parameters:\n    temperature: 0\nwidget:\n- messages:\n  - role: user\n    content: How should I explain the Internet?\nlibrary_name: transformers\n---\n\n# Phi-4 Model Card \n\n[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)\n\n## Model Summary \n\n|                         |                                                                               |     \n|-------------------------|-------------------------------------------------------------------------------|\n| **Developers**          | Microsoft Research                                                            |\n| **Description**         | `phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.<br><br>`phi-4` underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures                |\n| **Architecture**        | 14B parameters, dense decoder-only Transformer model                          |\n| **Inputs**              | Text, best suited for prompts in the chat format                              |\n| **Context length**      | 16K tokens                                                                    |\n| **GPUs**                | 1920 H100-80G                                                                 |\n| **Training time**       | 21 days                                                                       |\n| **Training data**       | 9.8T tokens                                                                   |\n| **Outputs**             | Generated text in response to input                                           |\n| **Dates**               | October 2024 – November 2024                                                  |\n| **Status**              | Static model trained on an offline dataset with cutoff dates of June 2024 and earlier for publicly available data                                                                               |\n| **Release date**        | December 12, 2024                                                             |\n| **License**             | MIT                                                                         |\n\n## Intended Use \n\n|                               |                                                                         |\n|-------------------------------|-------------------------------------------------------------------------|\n| **Primary Use Cases**         | Our model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:<br><br>1. Memory/compute constrained environments.<br>2. Latency bound scenarios.<br>3. Reasoning and logic.                                                                       |\n| **Out-of-Scope Use Cases**    | Our models is not specifically designed or evaluated for all downstream purposes, thus:<br><br>1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.<br>2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.<br>3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.                                                              |\n\n## Data Overview \n\n### Training Datasets \n\nOur training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\n\n1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\n\n2. Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\n\n3. Acquired academic books and Q&A datasets.\n\n4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.\n\n#### Benchmark datasets \n\nWe evaluated `phi-4` using [OpenAI’s SimpleEval](https://github.com/openai/simple-evals) and our own internal benchmarks to understand the model’s capabilities, more specifically: \n\n* **MMLU:** Popular aggregated dataset for multitask language understanding.\n\n* **MATH:** Challenging competition math problems.\n\n* **GPQA:** Complex, graduate-level science questions.\n\n* **DROP:** Complex comprehension and reasoning.\n\n* **MGSM:** Multi-lingual grade-school math.\n\n* **HumanEval:** Functional code generation.\n\n* **SimpleQA:** Factual responses.\n\n## Safety \n\n### Approach \n\n`phi-4` has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated synthetic datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. \n\n### Safety Evaluation and Red-Teaming \n\nPrior to release, `phi-4` followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the independent AI Red Team (AIRT) at Microsoft to assess safety risks posed by `phi-4` in both average and adversarial user scenarios. In the average user scenario, AIRT emulated typical single-turn and multi-turn interactions to identify potentially risky behaviors. The adversarial user scenario tested a wide range of techniques aimed at intentionally subverting the model’s safety training including jailbreaks, encoding-based attacks, multi-turn attacks, and adversarial suffix attacks.   \n\nPlease refer to the technical report for more details on safety alignment. \n\n## Model Quality\n\nTo understand the capabilities, we compare `phi-4` with a set of models over OpenAI’s SimpleEval benchmark. \n\nAt the high-level overview of the model quality on representative benchmarks. For the table below, higher numbers indicate better performance: \n\n| **Category**                 | **Benchmark** | **phi-4** (14B) | **phi-3** (14B) | **Qwen 2.5** (14B instruct) | **GPT-4o-mini** | **Llama-3.3** (70B instruct) | **Qwen 2.5** (72B instruct) | **GPT-4o** |\n|------------------------------|---------------|-----------|-----------------|----------------------|----------------------|--------------------|-------------------|-----------------|\n| Popular Aggregated Benchmark | MMLU          | 84.8      | 77.9            | 79.9                 | 81.8                 | 86.3               | 85.3              | **88.1**            |\n| Science                      | GPQA          | **56.1**      | 31.2            | 42.9                 | 40.9                 | 49.1               | 49.0              | 50.6            |\n| Math                         | MGSM<br>MATH  | 80.6<br>**80.4** | 53.5<br>44.6 | 79.6<br>75.6 | 86.5<br>73.0 | 89.1<br>66.3* | 87.3<br>80.0              | **90.4**<br>74.6            |\n| Code Generation              | HumanEval     | 82.6      | 67.8            | 72.1                 | 86.2                 | 78.9*               | 80.4              | **90.6**            |\n| Factual Knowledge            | SimpleQA      | 3.0       | 7.6            | 5.4                 | 9.9                  | 20.9               | 10.2              | **39.4**             |\n| Reasoning                    | DROP          | 75.5      | 68.3            | 85.5                 | 79.3                 | **90.2**               | 76.7              | 80.9            |\n\n\\* These scores are lower than those reported by Meta, perhaps because simple-evals has a strict formatting requirement that Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3-70B.\n\n## Usage\n\n### Input Formats\n\nGiven the nature of the training data, `phi-4` is best suited for prompts using the chat format as follows: \n\n```bash\n<|im_start|>system<|im_sep|>\nYou are a medieval knight and must provide explanations to modern people.<|im_end|>\n<|im_start|>user<|im_sep|>\nHow should I explain the Internet?<|im_end|>\n<|im_start|>assistant<|im_sep|>\n```\n\n### With `transformers`\n\n```python\nimport transformers\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"microsoft/phi-4\",\n    model_kwargs={\"torch_dtype\": \"auto\"},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people.\"},\n    {\"role\": \"user\", \"content\": \"How should I explain the Internet?\"},\n]\n\noutputs = pipeline(messages, max_new_tokens=128)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n## Responsible AI Considerations\n\nLike other language models, `phi-4` can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n\n* **Quality of Service:** The model is trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. `phi-4` is not intended to support multilingual use. \n\n* **Representation of Harms & Perpetuation of Stereotypes:** These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.  \n\n* **Inappropriate or Offensive Content:** These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.  \n\n* **Information Reliability:** Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n\n* **Limited Scope for Code:** Majority of `phi-4` training data is based in Python and uses common packages such as `typing`, `math`, `random`, `collections`, `datetime`, `itertools`. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.  \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Using safety services like [Azure AI Content Safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety) that have advanced guardrails is highly recommended. Important areas for consideration include:\n\n* **Allocation:** Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. \n\n* **High-Risk Scenarios:** Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.  \n\n* **Misinformation:** Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).    \n\n* **Generation of Harmful Content:** Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.  \n\n* **Misuse:** Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/phi-4"}]},{"id":"meta-llama/Llama-3.2-1B","name":"Llama-3.2-1B","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","arxiv:2405.16406","license:llama3.2","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":2154,"downloads":1734119,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-3.2-1B"}]},{"id":"ByteDance/SDXL-Lightning","name":"SDXL-Lightning","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","text-to-image","stable-diffusion","arxiv:2402.13929","license:openrail++","region:us"],"likes":2103,"downloads":110539,"readme":"---\nlicense: openrail++\ntags:\n- text-to-image\n- stable-diffusion\nlibrary_name: diffusers\ninference: false\n---\n\n# SDXL-Lightning\n\n![Intro Image](sdxl_lightning_samples.jpg)\n\nSDXL-Lightning is a lightning-fast text-to-image generation model. It can generate high-quality 1024px images in a few steps. For more information, please refer to our research paper: [SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929). We open-source the model as part of the research.\n\nOur models are distilled from [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0). This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is amazing. Our 1-step model is more experimental.\n\nWe provide both full UNet and LoRA checkpoints. The full UNet models have the best quality while the LoRA models can be applied to other base models.\n\n## Demos\n\n* Generate with all configurations, best quality: [Demo](https://huggingface.co/spaces/ByteDance/SDXL-Lightning)\n\n## Checkpoints\n\n* `sdxl_lightning_Nstep.safetensors`: All-in-one checkpoint, for ComfyUI.\n* `sdxl_lightning_Nstep_unet.safetensors`: UNet checkpoint only, for Diffusers.\n* `sdxl_lightning_Nstep_lora.safetensors`: LoRA checkpoint, for Diffusers and ComfyUI.\n\n## Diffusers Usage\n\nPlease always use the correct checkpoint for the corresponding inference steps.\n\n### 2-Step, 4-Step, 8-Step UNet\n\n```python\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n```\n\n### 2-Step, 4-Step, 8-Step LoRA\n\nUse LoRA only if you are using non-SDXL base models. Otherwise use our UNet checkpoint for better quality.\n```python\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\n\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_4step_lora.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\npipe = StableDiffusionXLPipeline.from_pretrained(base, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo, ckpt))\npipe.fuse_lora()\n\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n```\n\n### 1-Step UNet\nThe 1-step model is only experimental and the quality is much less stable. Consider using the 2-step model for much better quality.\n\nThe 1-step model uses \"sample\" prediction instead of \"epsilon\" prediction! The scheduler needs to be configured correctly.\n\n```python\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_1step_unet_x0.safetensors\" # Use the correct ckpt for your step setting!\n\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\n# Ensure sampler uses \"trailing\" timesteps and \"sample\" prediction type.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", prediction_type=\"sample\")\n\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=1, guidance_scale=0).images[0].save(\"output.png\")\n```\n\n\n## ComfyUI Usage\n\nPlease always use the correct checkpoint for the corresponding inference steps.\nPlease use Euler sampler with sgm_uniform scheduler.\n\n### 2-Step, 4-Step, 8-Step Full\n\n1. Download the full checkpoint (`sdxl_lightning_Nstep.safetensors`) to `/ComfyUI/models/checkpoints`.\n1. Download our [ComfyUI full workflow](comfyui/sdxl_lightning_workflow_full.json).\n\n![SDXL-Lightning ComfyUI Full Workflow](comfyui/sdxl_lightning_workflow_full.jpg)\n\n### 2-Step, 4-Step, 8-Step LoRA\n\nUse LoRA only if you are using non-SDXL base models. Otherwise use our full checkpoint for better quality.\n\n1. Prepare your own base model.\n1. Download the LoRA checkpoint (`sdxl_lightning_Nstep_lora.safetensors`) to `/ComfyUI/models/loras`\n1. Download our [ComfyUI LoRA workflow](comfyui/sdxl_lightning_workflow_lora.json).\n\n![SDXL-Lightning ComfyUI LoRA Workflow](comfyui/sdxl_lightning_workflow_lora.jpg)\n\n### 1-Step\n\nThe 1-step model is only experimental and the quality is much less stable. Consider using the 2-step model for much better quality.\n\n1. Update your ComfyUI to the latest version.\n1. Download the full checkpoint (`sdxl_lightning_1step_x0.safetensors`) to `/ComfyUI/models/checkpoints`.\n1. Download our [ComfyUI full 1-step workflow](comfyui/sdxl_lightning_workflow_full_1step.json).\n\n![SDXL-Lightning ComfyUI Full 1-Step Workflow](comfyui/sdxl_lightning_workflow_full_1step.jpg)\n\n\n## Cite Our Work\n```\n@misc{lin2024sdxllightning,\n      title={SDXL-Lightning: Progressive Adversarial Diffusion Distillation}, \n      author={Shanchuan Lin and Anran Wang and Xiao Yang},\n      year={2024},\n      eprint={2402.13929},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/ByteDance/SDXL-Lightning"}]},{"id":"Qwen/Qwen-Image-Edit","name":"Qwen-Image-Edit","description":"A model for image-to-image.","task":"image-to-image","tags":["diffusers","safetensors","image-to-image","en","zh","arxiv:2508.02324","license:apache-2.0","diffusers:QwenImageEditPipeline","region:us"],"likes":2099,"downloads":190983,"readme":"---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nlibrary_name: diffusers\npipeline_tag: image-to-image\n---\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png\" width=\"400\"/>\n<p>\n<p align=\"center\">\n          💜 <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbsp🤗 <a href=\"https://huggingface.co/Qwen/Qwen-Image-Edit\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image-Edit\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf\">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen-image-edit/\">Blog</a> &nbsp&nbsp \n<br>\n🖥️ <a href=\"https://huggingface.co/spaces/Qwen/Qwen-Image-Edit\">Demo</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp🫨 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp| &nbsp&nbsp <a href=\"https://github.com/QwenLM/Qwen-Image\">Github</a>&nbsp&nbsp\n</p>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_homepage.jpg\" width=\"1600\"/>\n<p>\n\n\n# Introduction\nWe are excited to introduce Qwen-Image-Edit, the image editing version of Qwen-Image. Built upon our 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Image’s unique text rendering capabilities to image editing tasks, enabling precise text editing. Furthermore, Qwen-Image-Edit simultaneously feeds the input image into Qwen2.5-VL (for visual semantic control) and the VAE Encoder (for visual appearance control), achieving capabilities in both semantic and appearance editing. To experience the latest model, visit [Qwen Chat](https://qwen.ai) and select the \"Image Editing\" feature.\n\nKey Features:\n\n* **Semantic and Appearance Editing**: Qwen-Image-Edit supports both low-level visual appearance editing (such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged) and high-level visual semantic editing (such as IP creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency).\n* **Precise Text Editing**: Qwen-Image-Edit supports bilingual (Chinese and English) text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.\n* **Strong Benchmark Performance**: Evaluations on multiple public benchmarks demonstrate that Qwen-Image-Edit achieves state-of-the-art (SOTA) performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n\n\n\n## Quick Start\n\nInstall the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nThe following contains a code snippet illustrating how to use the model to generate images based on text prompts:\n\n```python\nimport os\nfrom PIL import Image\nimport torch\n\nfrom diffusers import QwenImageEditPipeline\n\npipeline = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\")\nprint(\"pipeline loaded\")\npipeline.to(torch.bfloat16)\npipeline.to(\"cuda\")\npipeline.set_progress_bar_config(disable=None)\nimage = Image.open(\"./input.png\").convert(\"RGB\")\nprompt = \"Change the rabbit's color to purple, with a flash light background.\"\ninputs = {\n    \"image\": image,\n    \"prompt\": prompt,\n    \"generator\": torch.manual_seed(0),\n    \"true_cfg_scale\": 4.0,\n    \"negative_prompt\": \" \",\n    \"num_inference_steps\": 50,\n}\n\nwith torch.inference_mode():\n    output = pipeline(**inputs)\n    output_image = output.images[0]\n    output_image.save(\"output_image_edit.png\")\n    print(\"image saved at\", os.path.abspath(\"output_image_edit.png\"))\n\n```\n\n## Showcase\nOne of the highlights of Qwen-Image-Edit lies in its powerful capabilities for semantic and appearance editing. Semantic editing refers to modifying image content while preserving the original visual semantics. To intuitively demonstrate this capability, let's take Qwen's mascot—Capybara—as an example:\n![Capibara](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片3.JPG#center)\nAs can be seen, although most pixels in the edited image differ from those in the input image (the leftmost image), the character consistency of Capybara is perfectly preserved. Qwen-Image-Edit's powerful semantic editing capability enables effortless and diverse creation of original IP content.\nFurthermore, on Qwen Chat, we designed a series of editing prompts centered around the 16 MBTI personality types. Leveraging these prompts, we successfully created a set of MBTI-themed emoji packs based on our mascot Capybara, effortlessly expanding the IP's reach and expression.\n![MBTI meme series](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片4.JPG#center)\nMoreover, novel view synthesis is another key application scenario in semantic editing. As shown in the two example images below, Qwen-Image-Edit can not only rotate objects by 90 degrees, but also perform a full 180-degree rotation, allowing us to directly see the back side of the object:\n![Viewpoint transformation 90 degrees](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片12.JPG#center)\n![Viewpoint transformation 180 degrees](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片13.JPG#center)\nAnother typical application of semantic editing is style transfer. For instance, given an input portrait, Qwen-Image-Edit can easily transform it into various artistic styles such as Studio Ghibli. This capability holds significant value in applications like virtual avatar creation:\n![Style transfer](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片1.JPG#center)\nIn addition to semantic editing, appearance editing is another common image editing requirement. Appearance editing emphasizes keeping certain regions of the image completely unchanged while adding, removing, or modifying specific elements. The image below illustrates a case where a signboard is added to the scene. \nAs shown, Qwen-Image-Edit not only successfully inserts the signboard but also generates a corresponding reflection, demonstrating exceptional attention to detail.\n![Adding a signboard](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片6.JPG#center)\nBelow is another interesting example, demonstrating how to remove fine hair strands and other small objects from an image.\n![Removing fine strands of hair](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片7.JPG#center)\nAdditionally, the color of a specific letter \"n\" in the image can be modified to blue, enabling precise editing of particular elements.\n![Modifying text color](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片8.JPG#center)\nAppearance editing also has wide-ranging applications in scenarios such as adjusting a person's background or changing clothing. The three images below demonstrate these practical use cases respectively.\n![Modifying backgrounds](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片11.JPG#center)\n![Modifying clothing](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片5.JPG#center)\nAnother standout feature of Qwen-Image-Edit is its accurate text editing capability, which stems from Qwen-Image's deep expertise in text rendering. As shown below, the following two cases vividly demonstrate Qwen-Image-Edit's powerful performance in editing English text:\n![Editing English text 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片15.JPG#center)\n![Editing English text 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片16.JPG#center)\nQwen-Image-Edit can also directly edit Chinese posters, enabling not only modifications to large headline text but also precise adjustments to even small and intricate text elements.\n![Editing Chinese posters](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片17.JPG#center)\nFinally, let's walk through a concrete image editing example to demonstrate how to use a chained editing approach to progressively correct errors in a calligraphy artwork generated by Qwen-Image:\n![Calligraphy artwork](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片18.JPG#center)\nIn this artwork, several Chinese characters contain generation errors. We can leverage Qwen-Image-Edit to correct them step by step. For instance, we can draw bounding boxes on the original image to mark the regions that need correction, instructing Qwen-Image-Edit to fix these specific areas. Here, we want the character \"稽\" to be correctly written within the red box, and the character \"亭\" to be accurately rendered in the blue region.\n![Correcting characters](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片19.JPG#center)\nHowever, in practice, the character \"稽\" is relatively obscure, and the model fails to correct it correctly in one step. The lower-right component of \"稽\" should be \"旨\" rather than \"日\". At this point, we can further highlight the \"日\" portion with a red box, instructing Qwen-Image-Edit to fine-tune this detail and replace it with \"旨\".\n![Fine-tuning character](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片20.JPG#center)\nIsn't it amazing? With this chained, step-by-step editing approach, we can continuously correct character errors until the desired final result is achieved.\n![Final version 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片21.JPG#center)\n![Final version 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片22.JPG#center)\n![Final version 3](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片23.JPG#center)\n![Final version 4](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片24.JPG#center)\n![Final version 5](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_en/幻灯片25.JPG#center)\nFinally, we have successfully obtained a completely correct calligraphy version of *Lantingji Xu (Orchid Pavilion Preface)*!\nIn summary, we hope that Qwen-Image-Edit can further advance the field of image generation, truly lower the technical barriers to visual content creation, and inspire even more innovative applications.\n\n\n## License Agreement\n\nQwen-Image is licensed under Apache 2.0. \n\n## Citation\n\nWe kindly encourage citation of our work if you find it useful.\n\n```bibtex\n@misc{wu2025qwenimagetechnicalreport,\n      title={Qwen-Image Technical Report}, \n      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\n      year={2025},\n      eprint={2508.02324},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.02324}, \n}\n```\n\n## Join Us\nIf you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait — reach out to us at fulai.hr@alibaba-inc.com\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Qwen/Qwen-Image-Edit"}]},{"id":"tencent/HunyuanVideo","name":"HunyuanVideo","description":"A model for text-to-video.","task":"text-to-video","tags":["text-to-video","arxiv:2412.03603","arxiv:2405.07719","license:other","region:us"],"likes":2076,"downloads":1481,"readme":"---\npipeline_tag: text-to-video\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: LICENSE\n---\n\n<!-- ## **HunyuanVideo** -->\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Tencent/HunyuanVideo/refs/heads/main/assets/logo.png\"  height=100>\n</p>\n\n# HunyuanVideo: A Systematic Framework For Large Video Generation Model Training\n\n-----\n\nThis repo contains PyTorch model definitions, pre-trained weights and inference/sampling code for our paper exploring HunyuanVideo. You can find more visualizations on our [project page](https://aivideo.hunyuan.tencent.com).\n\n> [**HunyuanVideo: A Systematic Framework For Large Video Generation Model Training**](https://arxiv.org/abs/2412.03603) <br>\n\n\n\n## News!!\n\n* Jan 13, 2025: 📈 We release the [Penguin Video Benchmark](https://github.com/Tencent/HunyuanVideo/blob/main/assets/PenguinVideoBenchmark.csv).\n* Dec 18, 2024: 🏃‍♂️ We release the [FP8 model weights](https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt) of HunyuanVideo to save more GPU memory.\n* Dec 17, 2024: 🤗 HunyuanVideo has been integrated into [Diffusers](https://huggingface.co/docs/diffusers/main/api/pipelines/hunyuan_video).\n* Dec 7, 2024: 🚀 We release the parallel inference code for HunyuanVideo powered by [xDiT](https://github.com/xdit-project/xDiT).\n* Dec 3, 2024: 👋 We release the inference code and model weights of HunyuanVideo. [Download](https://github.com/Tencent/HunyuanVideo/blob/main/ckpts/README.md).\n\n\n\n## Open-source Plan\n\n- HunyuanVideo (Text-to-Video Model)\n  - [x] Inference \n  - [x] Checkpoints\n  - [x] Multi-gpus Sequence Parallel inference (Faster inference speed on more gpus)\n  - [x] Web Demo (Gradio)\n  - [x] Diffusers \n  - [x] FP8 Quantified weight\n  - [x] Penguin Video Benchmark\n  - [x] ComfyUI\n- [HunyuanVideo (Image-to-Video Model)](https://github.com/Tencent/HunyuanVideo-I2V)\n  - [x] Inference \n  - [x] Checkpoints \n\n\n\n## Contents\n\n- [HunyuanVideo: A Systematic Framework For Large Video Generation Model](#hunyuanvideo-a-systematic-framework-for-large-video-generation-model)\n  - [News!!](#news)\n  - [Open-source Plan](#open-source-plan)\n  - [Contents](#contents)\n  - [**Abstract**](#abstract)\n  - [**HunyuanVideo Overall Architecture**](#hunyuanvideo-overall-architecture)\n  - [**HunyuanVideo Key Features**](#hunyuanvideo-key-features)\n    - [**Unified Image and Video Generative Architecture**](#unified-image-and-video-generative-architecture)\n    - [**MLLM Text Encoder**](#mllm-text-encoder)\n    - [**3D VAE**](#3d-vae)\n    - [**Prompt Rewrite**](#prompt-rewrite)\n  - [Comparisons](#comparisons)\n  - [Requirements](#requirements)\n  - [Dependencies and Installation](#️dependencies-and-installation)\n    - [Installation Guide for Linux](#installation-guide-for-linux)\n  - [Download Pretrained Models](#download-pretrained-models)\n  - [Single-gpu Inference](#single-gpu-inference)\n    - [Using Command Line](#using-command-line)\n    - [Run a Gradio Server](#run-a-gradio-server)\n    - [More Configurations](#more-configurations)\n  - [Parallel Inference on Multiple GPUs by xDiT](#parallel-inference-on-multiple-gpus-by-xdit)\n    - [Using Command Line](#using-command-line-1)\n  - [FP8 Inference](#fp8-inference)\n    - [Using Command Line](#using-command-line-2)\n  - [BibTeX](#bibtex)\n  - [Acknowledgements](#acknowledgements)\n\n---\n\n## **Abstract**\n\nWe present HunyuanVideo, a novel open-source video foundation model that exhibits performance in video generation that is comparable to, if not superior to, leading closed-source models. In order to train HunyuanVideo model, we adopt several key technologies for model learning, including data curation, image-video joint model training, and an efficient infrastructure designed to facilitate large-scale model training and inference. Additionally, through an effective strategy for scaling model architecture and dataset, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. \n\nWe conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion diversity, text-video alignment, and generation stability. According to professional human evaluation results, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and 3 top-performing Chinese video generative models. By releasing the code and weights of the foundation model and its applications, we aim to bridge the gap between closed-source and open-source video foundation models. This initiative will empower everyone in the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. \n\n\n\n## **HunyuanVideo Overall Architecture**\n\nHunyuanVideo is trained on a spatial-temporally\ncompressed latent space, which is compressed through a Causal 3D VAE. Text prompts are encoded\nusing a large language model, and used as the conditions. Taking Gaussian noise and the conditions as\ninput, our generative model produces an output latent, which is then decoded to images or videos through\nthe 3D VAE decoder.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Tencent/HunyuanVideo/refs/heads/main/assets/overall.png\"  height=300>\n</p>\n\n\n\n## **HunyuanVideo Key Features**\n\n### **Unified Image and Video Generative Architecture**\n\nHunyuanVideo introduces the Transformer design and employs a Full Attention mechanism for unified image and video generation. \nSpecifically, we use a \"Dual-stream to Single-stream\" hybrid model design for video generation. In the dual-stream phase, video and text\ntokens are processed independently through multiple Transformer blocks, enabling each modality to learn its own appropriate modulation mechanisms without interference. In the single-stream phase, we concatenate the video and text\ntokens and feed them into subsequent Transformer blocks for effective multimodal information fusion.\nThis design captures complex interactions between visual and semantic information, enhancing\noverall model performance.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Tencent/HunyuanVideo/refs/heads/main/assets/backbone.png\"  height=350>\n</p>\n\n\n### **MLLM Text Encoder**\n\nSome previous text-to-video models typically use pre-trained CLIP and T5-XXL as text encoders where CLIP uses Transformer Encoder and T5 uses an Encoder-Decoder structure. In contrast, we utilize a pre-trained Multimodal Large Language Model (MLLM) with a Decoder-Only structure as our text encoder, which has the following advantages: (i) Compared with T5, MLLM after visual instruction finetuning has better image-text alignment in the feature space, which alleviates the difficulty of the instruction following in diffusion models; (ii)\nCompared with CLIP, MLLM has demonstrated superior ability in image detail description\nand complex reasoning; (iii) MLLM can play as a zero-shot learner by following system instructions prepended to user prompts, helping text features pay more attention to key information. In addition, MLLM is based on causal attention while T5-XXL utilizes bidirectional attention that produces better text guidance for diffusion models. Therefore, we introduce an extra bidirectional token refiner to enhance text features.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Tencent/HunyuanVideo/refs/heads/main/assets/text_encoder.png\"  height=275>\n</p>\n\n\n### **3D VAE**\n\nHunyuanVideo trains a 3D VAE with CausalConv3D to compress pixel-space videos and images into a compact latent space. We set the compression ratios of video length, space, and channel to 4, 8, and 16 respectively. This can significantly reduce the number of tokens for the subsequent diffusion transformer model, allowing us to train videos at the original resolution and frame rate.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Tencent/HunyuanVideo/refs/heads/main/assets/3dvae.png\"  height=150>\n</p>\n\n\n### **Prompt Rewrite**\n\nTo address the variability in linguistic style and length of user-provided prompts, we fine-tune the [Hunyuan-Large model](https://github.com/Tencent/Tencent-Hunyuan-Large) as our prompt rewrite model to adapt the original user prompt to model-preferred prompt.\n\nWe provide two rewrite modes: Normal mode and Master mode, which can be called using different prompts. The prompts are shown [here](hyvideo/prompt_rewrite.py). The Normal mode is designed to enhance the video generation model's comprehension of user intent, facilitating a more accurate interpretation of the instructions provided. The Master mode enhances the description of aspects such as composition, lighting, and camera movement, which leans towards generating videos with a higher visual quality. However, this emphasis may occasionally result in the loss of some semantic details. \n\nThe Prompt Rewrite Model can be directly deployed and inferred using the [Hunyuan-Large original code](https://github.com/Tencent/Tencent-Hunyuan-Large). We release the weights of the Prompt Rewrite Model [here](https://huggingface.co/Tencent/HunyuanVideo-PromptRewrite).\n\n\n\n## Comparisons\n\nTo evaluate the performance of HunyuanVideo, we selected five strong baselines from closed-source video generation models. In total, we utilized 1,533 text prompts, generating an equal number of video samples with HunyuanVideo in a single run. For a fair comparison, we conducted inference only once, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models, ensuring consistent video resolution. Videos were assessed based on three criteria: Text Alignment, Motion Quality, and Visual Quality. More than 60 professional evaluators performed the evaluation. Notably, HunyuanVideo demonstrated the best overall performance, particularly excelling in motion quality. Please note that the evaluation is based on Hunyuan Video's high-quality version. This is different from the currently released fast version.\n\n<p align=\"center\">\n<table> \n<thead> \n<tr> \n    <th rowspan=\"2\">Model</th> <th rowspan=\"2\">Open Source</th> <th>Duration</th> <th>Text Alignment</th> <th>Motion Quality</th> <th rowspan=\"2\">Visual Quality</th> <th rowspan=\"2\">Overall</th>  <th rowspan=\"2\">Ranking</th>\n</tr> \n</thead> \n<tbody> \n<tr> \n    <td>HunyuanVideo (Ours)</td> <td> ✔ </td> <td>5s</td> <td>61.8%</td> <td>66.5%</td> <td>95.7%</td> <td>41.3%</td> <td>1</td>\n</tr> \n<tr> \n    <td>CNTopA (API)</td> <td> &#10008 </td> <td>5s</td> <td>62.6%</td> <td>61.7%</td> <td>95.6%</td> <td>37.7%</td> <td>2</td>\n</tr> \n<tr> \n    <td>CNTopB (Web)</td> <td> &#10008</td> <td>5s</td> <td>60.1%</td> <td>62.9%</td> <td>97.7%</td> <td>37.5%</td> <td>3</td>\n</tr> \n<tr> \n    <td>GEN-3 alpha (Web)</td> <td>&#10008</td> <td>6s</td> <td>47.7%</td> <td>54.7%</td> <td>97.5%</td> <td>27.4%</td> <td>4</td> \n</tr> \n<tr> \n    <td>Luma1.6 (API)</td><td>&#10008</td> <td>5s</td> <td>57.6%</td> <td>44.2%</td> <td>94.1%</td> <td>24.8%</td> <td>5</td>\n</tr>\n<tr> \n    <td>CNTopC (Web)</td> <td>&#10008</td> <td>5s</td> <td>48.4%</td> <td>47.2%</td> <td>96.3%</td> <td>24.6%</td> <td>6</td>\n</tr> \n</tbody>\n</table>\n</p>\n\n\n\n## Requirements\n\nThe following table shows the requirements for running HunyuanVideo model (batch size = 1) to generate videos:\n\n|    Model     | Setting<br/>(height/width/frame) | GPU Peak Memory |\n| :----------: | :------------------------------: | :-------------: |\n| HunyuanVideo |         720px1280px129f          |      60GB       |\n| HunyuanVideo |          544px960px129f          |      45GB       |\n\n* An NVIDIA GPU with CUDA support is required. \n  * The model is tested on a single 80G GPU.\n  * **Minimum**: The minimum GPU memory required is 60GB for 720px1280px129f and 45G for 544px960px129f.\n  * **Recommended**: We recommend using a GPU with 80GB of memory for better generation quality.\n* Tested operating system: Linux\n\n\n\n## Dependencies and Installation\n\nBegin by cloning the repository:\n\n```shell\ngit clone https://github.com/tencent/HunyuanVideo\ncd HunyuanVideo\n```\n\n### Installation Guide for Linux\n\nWe recommend CUDA versions 12.4 or 11.8 for the manual installation.\n\nConda's installation instructions are available [here](https://docs.anaconda.com/free/miniconda/index.html).\n\n```shell\n# 1. Create conda environment\nconda create -n HunyuanVideo python==3.10.9\n\n# 2. Activate the environment\nconda activate HunyuanVideo\n\n# 3. Install PyTorch and other dependencies using conda\n# For CUDA 11.8\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n# For CUDA 12.4\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia\n\n# 4. Install pip dependencies\npython -m pip install -r requirements.txt\n\n# 5. Install flash attention v2 for acceleration (requires CUDA 11.8 or above)\npython -m pip install ninja\npython -m pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3\n\n# 6. Install xDiT for parallel inference (It is recommended to use torch 2.4.0 and flash-attn 2.6.3)\npython -m pip install xfuser==0.4.0\n```\n\nIn case of running into float point exception(core dump) on the specific GPU type, you may try the following solutions:\n\n```shell\n# Option 1: Making sure you have installed CUDA 12.4, CUBLAS>=12.4.5.8, and CUDNN>=9.00 (or simply using our CUDA 12 docker image).\npip install nvidia-cublas-cu12==12.4.5.8\nexport LD_LIBRARY_PATH=/opt/conda/lib/python3.8/site-packages/nvidia/cublas/lib/\n\n# Option 2: Forcing to explictly use the CUDA 11.8 compiled version of Pytorch and all the other packages\npip uninstall -r requirements.txt  # uninstall all packages\npip uninstall -y xfuser\npip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\npip install ninja\npip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3\npip install xfuser==0.4.0\n```\n\nAdditionally, HunyuanVideo also provides a pre-built Docker image. Use the following command to pull and run the docker image.\n\n```shell\n# For CUDA 12.4 (updated to avoid float point exception)\ndocker pull hunyuanvideo/hunyuanvideo:cuda_12\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged hunyuanvideo/hunyuanvideo:cuda_12\n\n# For CUDA 11.8\ndocker pull hunyuanvideo/hunyuanvideo:cuda_11\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged hunyuanvideo/hunyuanvideo:cuda_11\n```\n\n\n\n## Download Pretrained Models\n\nThe details of download pretrained models are shown [here](ckpts/README.md).\n\n\n\n## Single-gpu Inference\n\nWe list the height/width/frame settings we support in the following table.\n\n|     Resolution     |    h/w=9:16     |    h/w=16:9     |     h/w=4:3     |     h/w=3:4     |    h/w=1:1     |\n| :----------------: | :-------------: | :-------------: | :-------------: | :-------------: | :------------: |\n|        540p        | 544px960px129f  | 960px544px129f  | 624px832px129f  | 832px624px129f  | 720px720px129f |\n| 720p (recommended) | 720px1280px129f | 1280px720px129f | 1104px832px129f | 832px1104px129f | 960px960px129f |\n\n### Using Command Line\n\n```bash\ncd HunyuanVideo\n\npython3 sample_video.py \\\n    --video-size 720 1280 \\\n    --video-length 129 \\\n    --infer-steps 50 \\\n    --prompt \"A cat walks on the grass, realistic style.\" \\\n    --flow-reverse \\\n    --use-cpu-offload \\\n    --save-path ./results\n```\n\n### Run a Gradio Server\n\n```bash\npython3 gradio_server.py --flow-reverse\n\n# set SERVER_NAME and SERVER_PORT manually\n# SERVER_NAME=0.0.0.0 SERVER_PORT=8081 python3 gradio_server.py --flow-reverse\n```\n\n### More Configurations\n\nWe list some more useful configurations for easy usage:\n\n|        Argument        |  Default  |                         Description                          |\n| :--------------------: | :-------: | :----------------------------------------------------------: |\n|       `--prompt`       |   None    |             The text prompt for video generation             |\n|     `--video-size`     | 720 1280  |               The size of the generated video                |\n|    `--video-length`    |    129    |              The length of the generated video               |\n|    `--infer-steps`     |    50     |               The number of steps for sampling               |\n| `--embedded-cfg-scale` |    6.0    |           Embedded  Classifier free guidance scale           |\n|     `--flow-shift`     |    7.0    |          Shift factor for flow matching schedulers           |\n|    `--flow-reverse`    |   False   |        If reverse, learning/sampling from t=1 -> t=0         |\n|        `--seed`        |   None    | The random seed for generating video, if None, we init a random seed |\n|  `--use-cpu-offload`   |   False   | Use CPU offload for the model load to save more memory, necessary for high-res video generation |\n|     `--save-path`      | ./results |               Path to save the generated video               |\n\n\n\n## Parallel Inference on Multiple GPUs by xDiT\n\n[xDiT](https://github.com/xdit-project/xDiT) is a Scalable Inference Engine for Diffusion Transformers (DiTs) on multi-GPU Clusters.\nIt has successfully provided low-latency parallel inference solutions for a variety of DiTs models, including mochi-1, CogVideoX, Flux.1, SD3, etc. This repo adopted the [Unified Sequence Parallelism (USP)](https://arxiv.org/abs/2405.07719) APIs for parallel inference of the HunyuanVideo model.\n\n### Using Command Line\n\nFor example, to generate a video with 8 GPUs, you can use the following command:\n\n```bash\ncd HunyuanVideo\n\ntorchrun --nproc_per_node=8 sample_video.py \\\n    --video-size 1280 720 \\\n    --video-length 129 \\\n    --infer-steps 50 \\\n    --prompt \"A cat walks on the grass, realistic style.\" \\\n    --flow-reverse \\\n    --seed 42 \\\n    --ulysses-degree 8 \\\n    --ring-degree 1 \\\n    --save-path ./results\n```\n\nYou can change the `--ulysses-degree` and `--ring-degree` to control the parallel configurations for the best performance. The valid parallel configurations are shown in the following table.\n\n<details>\n<summary>Supported Parallel Configurations (Click to expand)</summary>\n\n\n| --video-size         | --video-length | --ulysses-degree x --ring-degree | --nproc_per_node |\n| -------------------- | -------------- | -------------------------------- | ---------------- |\n| 1280 720 or 720 1280 | 129            | 8x1,4x2,2x4,1x8                  | 8                |\n| 1280 720 or 720 1280 | 129            | 1x5                              | 5                |\n| 1280 720 or 720 1280 | 129            | 4x1,2x2,1x4                      | 4                |\n| 1280 720 or 720 1280 | 129            | 3x1,1x3                          | 3                |\n| 1280 720 or 720 1280 | 129            | 2x1,1x2                          | 2                |\n| 1104 832 or 832 1104 | 129            | 4x1,2x2,1x4                      | 4                |\n| 1104 832 or 832 1104 | 129            | 3x1,1x3                          | 3                |\n| 1104 832 or 832 1104 | 129            | 2x1,1x2                          | 2                |\n| 960 960              | 129            | 6x1,3x2,2x3,1x6                  | 6                |\n| 960 960              | 129            | 4x1,2x2,1x4                      | 4                |\n| 960 960              | 129            | 3x1,1x3                          | 3                |\n| 960 960              | 129            | 1x2,2x1                          | 2                |\n| 960 544 or 544 960   | 129            | 6x1,3x2,2x3,1x6                  | 6                |\n| 960 544 or 544 960   | 129            | 4x1,2x2,1x4                      | 4                |\n| 960 544 or 544 960   | 129            | 3x1,1x3                          | 3                |\n| 960 544 or 544 960   | 129            | 1x2,2x1                          | 2                |\n| 832 624 or 624 832   | 129            | 4x1,2x2,1x4                      | 4                |\n| 624 832 or 624 832   | 129            | 3x1,1x3                          | 3                |\n| 832 624 or 624 832   | 129            | 2x1,1x2                          | 2                |\n| 720 720              | 129            | 1x5                              | 5                |\n| 720 720              | 129            | 3x1,1x3                          | 3                |\n\n</details>\n\n\n<p align=\"center\">\n<table align=\"center\">\n<thead>\n<tr>\n    <th colspan=\"4\">Latency (Sec) for 1280x720 (129 frames 50 steps) on 8xGPU</th>\n</tr>\n<tr>\n    <th>1</th>\n    <th>2</th>\n    <th>4</th>\n    <th>8</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n    <th>1904.08</th>\n    <th>934.09 (2.04x)</th>\n    <th>514.08 (3.70x)</th>\n    <th>337.58 (5.64x)</th>\n</tr>\n\n\n</tbody>\n</table>\n</p>\n\n\n\n## FP8 Inference\n\nUsing HunyuanVideo with FP8 quantized weights, which saves about 10GB of GPU memory. You can download the [weights](https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt) and [weight scales](https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8_map.pt) from Huggingface.\n\n### Using Command Line\n\nHere, you must explicitly specify the FP8 weight path. For example, to generate a video with fp8 weights, you can use the following command:\n\n```bash\ncd HunyuanVideo\n\nDIT_CKPT_PATH={PATH_TO_FP8_WEIGHTS}/{WEIGHT_NAME}_fp8.pt\n\npython3 sample_video.py \\\n    --dit-weight ${DIT_CKPT_PATH} \\\n    --video-size 1280 720 \\\n    --video-length 129 \\\n    --infer-steps 50 \\\n    --prompt \"A cat walks on the grass, realistic style.\" \\\n    --seed 42 \\\n    --embedded-cfg-scale 6.0 \\\n    --flow-shift 7.0 \\\n    --flow-reverse \\\n    --use-cpu-offload \\\n    --use-fp8 \\\n    --save-path ./results\n```\n\n\n\n## BibTeX\n\nIf you find [HunyuanVideo](https://arxiv.org/abs/2412.03603) useful for your research and applications, please cite using this BibTeX:\n\n```BibTeX\n@misc{kong2024hunyuanvideo,\n      title={HunyuanVideo: A Systematic Framework For Large Video Generative Models}, \n      author={Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, and Jie Jiang, along with Caesar Zhong},\n      year={2024},\n      archivePrefix={arXiv preprint arXiv:2412.03603},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2412.03603}, \n}\n```\n\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [FLUX](https://github.com/black-forest-labs/flux), [Llama](https://github.com/meta-llama/llama), [LLaVA](https://github.com/haotian-liu/LLaVA), [Xtuner](https://github.com/InternLM/xtuner), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research and exploration.\nAdditionally, we also thank the Tencent Hunyuan Multimodal team for their help with the text encoder. \n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/tencent/HunyuanVideo"}]},{"id":"lllyasviel/sd_control_collection","name":"sd_control_collection","description":"A model for various tasks.","task":"N/A","tags":["region:us"],"likes":2068,"downloads":0,"readme":"Collection of community SD control models for users to download flexibly.\n\nAll files are already float16 and in safetensor format.\n\n\n\nThe files are mirrored with the below script:\n\nfiles = {\n'diffusers_xl_canny_small.safetensors': 'https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0-small/resolve/main/diffusion_pytorch_model.bin',\n'diffusers_xl_canny_mid.safetensors': 'https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0-mid/resolve/main/diffusion_pytorch_model.bin',\n'diffusers_xl_canny_full.safetensors': 'https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/diffusion_pytorch_model.bin',\n'diffusers_xl_depth_small.safetensors': 'https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0-small/resolve/main/diffusion_pytorch_model.bin',\n'diffusers_xl_depth_mid.safetensors': 'https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0-mid/resolve/main/diffusion_pytorch_model.bin',\n'diffusers_xl_depth_full.safetensors': 'https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0/resolve/main/diffusion_pytorch_model.bin',\n\n'thibaud_xl_openpose.safetensors': 'https://huggingface.co/thibaud/controlnet-openpose-sdxl-1.0/resolve/main/OpenPoseXL2.safetensors',\n'thibaud_xl_openpose_256lora.safetensors': 'https://huggingface.co/thibaud/controlnet-openpose-sdxl-1.0/resolve/main/control-lora-openposeXL2-rank256.safetensors',\n\n'sargezt_xl_depth_faid_vidit.safetensors': 'https://huggingface.co/SargeZT/controlnet-sd-xl-1.0-depth-faid-vidit/resolve/main/diffusion_pytorch_model.bin',\n'sargezt_xl_depth_zeed.safetensors': 'https://huggingface.co/SargeZT/controlnet-sd-xl-1.0-depth-zeed/resolve/main/diffusion_pytorch_model.bin',\n'sargezt_xl_depth.safetensors': 'https://huggingface.co/SargeZT/controlnet-v1e-sdxl-depth/resolve/main/diffusion_pytorch_model.bin',\n'sargezt_xl_softedge.safetensors': 'https://huggingface.co/SargeZT/controlnet-sd-xl-1.0-softedge-dexined/resolve/main/controlnet-sd-xl-1.0-softedge-dexined.safetensors',\n\n'sai_xl_canny_128lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-canny-rank128.safetensors',\n'sai_xl_canny_256lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors',\n'sai_xl_depth_128lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-depth-rank128.safetensors',\n'sai_xl_depth_256lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors',\n'sai_xl_sketch_128lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-sketch-rank128-metadata.safetensors',\n'sai_xl_sketch_256lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors',\n'sai_xl_recolor_128lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-recolor-rank128.safetensors',\n'sai_xl_recolor_256lora.safetensors': 'https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors',\n\n'ioclab_sd15_recolor.safetensors': 'https://huggingface.co/ioclab/control_v1p_sd15_brightness/resolve/main/diffusion_pytorch_model.safetensors',\n\n't2i-adapter_xl_canny.safetensors': 'https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models_XL/adapter-xl-canny.pth',\n't2i-adapter_xl_openpose.safetensors': 'https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models_XL/adapter-xl-openpose.pth',\n't2i-adapter_xl_sketch.safetensors': 'https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models_XL/adapter-xl-sketch.pth',\n\n'ip-adapter_sd15_plus.safetensors': 'https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.bin',\n'ip-adapter_sd15.safetensors': 'https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.bin',\n'ip-adapter_xl.safetensors': 'https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.bin',\n\n'kohya_controllllite_xl_depth_anime.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01008016e_sdxl_depth_anime.safetensors',\n'kohya_controllllite_xl_canny_anime.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_canny_anime.safetensors',\n'kohya_controllllite_xl_scribble_anime.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_fake_scribble_anime.safetensors',\n'kohya_controllllite_xl_openpose_anime.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_pose_anime.safetensors',\n'kohya_controllllite_xl_openpose_anime_v2.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_pose_anime_v2_500-1000.safetensors',\n\n'kohya_controllllite_xl_blur_anime_beta.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01016032e_sdxl_blur_anime_beta.safetensors',\n'kohya_controllllite_xl_blur.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_blur-500-1000.safetensors',\n'kohya_controllllite_xl_blur_anime.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_blur-anime_500-1000.safetensors',\n'kohya_controllllite_xl_canny.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_canny.safetensors',\n'kohya_controllllite_xl_depth.safetensors': 'https://huggingface.co/kohya-ss/controlnet-lllite/resolve/main/controllllite_v01032064e_sdxl_depth_500-1000.safetensors',\n\n't2i-adapter_diffusers_xl_canny.safetensors': 'https://huggingface.co/TencentARC/t2i-adapter-canny-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors',\n't2i-adapter_diffusers_xl_lineart.safetensors': 'https://huggingface.co/TencentARC/t2i-adapter-lineart-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors',\n't2i-adapter_diffusers_xl_depth_midas.safetensors': 'https://huggingface.co/TencentARC/t2i-adapter-depth-midas-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors',\n't2i-adapter_diffusers_xl_openpose.safetensors': 'https://huggingface.co/TencentARC/t2i-adapter-openpose-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors',\n't2i-adapter_diffusers_xl_depth_zoe.safetensors': 'https://huggingface.co/TencentARC/t2i-adapter-depth-zoe-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors',\n't2i-adapter_diffusers_xl_sketch.safetensors': 'https://huggingface.co/TencentARC/t2i-adapter-sketch-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors',\n\n}\n\nIf you download the files from raw URL, you may need to rename them. \n\nHowever, files in https://huggingface.co/lllyasviel/sd_control_collection/tree/main are already renamed and can be directly downloaded.\n\nFeel free to contact us if you are author of any listed models and you want some models to be removed/added (by opening an issue in this HuggingFace page).","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/lllyasviel/sd_control_collection"}]},{"id":"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF","name":"Llama-3.1-Nemotron-70B-Instruct-HF","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","nvidia","llama3.1","conversational","en","dataset:nvidia/HelpSteer2","arxiv:2410.01257","arxiv:2405.01481","arxiv:2406.08673","base_model:meta-llama/Llama-3.1-70B-Instruct","base_model:finetune:meta-llama/Llama-3.1-70B-Instruct","license:llama3.1","autotrain_compatible","text-generation-inference","region:us"],"likes":2056,"downloads":24853,"readme":"---\nlicense: llama3.1\nlanguage:\n- en\ninference: false\nfine-tuning: false\ntags:\n- nvidia\n- llama3.1\ndatasets:\n- nvidia/HelpSteer2\nbase_model: meta-llama/Llama-3.1-70B-Instruct\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n# Model Overview\n\n## Description:\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\n\n\nThis model reaches [Arena Hard](https://github.com/lmarena/arena-hard-auto) of 85.0, [AlpacaEval 2 LC](https://tatsu-lab.github.io/alpaca_eval/) of 57.6 and [GPT-4-Turbo MT-Bench](https://github.com/lm-sys/FastChat/pull/3158) of 8.98, which are known to be predictive of [LMSys Chatbot Arena Elo](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n\nAs of 1 Oct 2024, this model is #1 on all three automatic alignment benchmarks (verified tab for AlpacaEval 2 LC), edging out strong frontier models such as GPT-4o and Claude 3.5 Sonnet.\n\nAs of Oct 24th, 2024 the model has Elo Score of 1267(+-7), rank 9 and style controlled rank of 26 on [ChatBot Arena leaderboard](https://lmarena.ai/?leaderboard).\n\nThis model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a Llama-3.1-70B-Instruct model as the initial policy.\n\nLlama-3.1-Nemotron-70B-Instruct-HF has been converted from [Llama-3.1-Nemotron-70B-Instruct](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct) to support it in the HuggingFace Transformers codebase. Please note that evaluation results might be slightly different from the [Llama-3.1-Nemotron-70B-Instruct](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct) as evaluated in NeMo-Aligner, which the evaluation results below are based on.\n\nTry hosted inference for free at [build.nvidia.com](https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct) - it comes with an OpenAI-compatible API interface.\n\n\nSee details on our paper at [https://arxiv.org/abs/2410.01257](https://arxiv.org/abs/2410.01257) - as a preview, this model can correctly the question ```How many r in strawberry?``` without specialized prompting or additional reasoning tokens:\n\n```\nA sweet question!\nLet’s count the “R”s in “strawberry”:\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\nThere are **3 “R”s** in the word “strawberry”.\n```\n\nNote: This model is a demonstration of our techniques for improving helpfulness in general-domain instruction following. It has not been tuned for performance in specialized domains such as math.\n\n\n## License\nYour use of this model is governed by the [NVIDIA Open Model License](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\nAdditional Information: [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_1/license/). Built with Llama.\n\n## Evaluation Metrics\n\nAs of 1 Oct 2024, Llama-3.1-Nemotron-70B-Instruct performs best on Arena Hard, AlpacaEval 2 LC (verified tab) and MT Bench (GPT-4-Turbo)\n\n | Model  | Arena Hard | AlpacaEval | MT-Bench | Mean Response Length |\n|:-----------------------------|:----------------|:-----|:----------|:-------|\n|Details | (95% CI) | 2 LC (SE) | (GPT-4-Turbo) | (# of Characters for MT-Bench)| \n| _**Llama-3.1-Nemotron-70B-Instruct**_ | **85.0** (-1.5, 1.5) | **57.6** (1.65) | **8.98** | 2199.8 | \n| Llama-3.1-70B-Instruct | 55.7 (-2.9, 2.7) | 38.1 (0.90)  | 8.22 | 1728.6 |\n| Llama-3.1-405B-Instruct | 69.3 (-2.4, 2.2) | 39.3 (1.43) | 8.49 | 1664.7 |\n| Claude-3-5-Sonnet-20240620 | 79.2 (-1.9, 1.7) | 52.4 (1.47) | 8.81 | 1619.9 |\n| GPT-4o-2024-05-13 | 79.3 (-2.1, 2.0) | 57.5 (1.47) | 8.74 | 1752.2 |\n         \n## Usage:\n\nYou can use the model using HuggingFace Transformers library with 2 or more 80GB GPUs (NVIDIA Ampere or newer) with at least 150GB of free disk space to accomodate the download.\n\nThis code has been tested on Transformers v4.44.0, torch v2.4.0 and 2 A100 80GB GPUs, but any setup that supports ```meta-llama/Llama-3.1-70B-Instruct``` should support this model as well. If you run into problems, you can consider doing ```pip install -U transformers```.\n\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"How many r in strawberry?\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\n\ntokenized_message = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\nresponse_token_ids = model.generate(tokenized_message['input_ids'].cuda(),attention_mask=tokenized_message['attention_mask'].cuda(),  max_new_tokens=4096, pad_token_id = tokenizer.eos_token_id)\ngenerated_tokens =response_token_ids[:, len(tokenized_message['input_ids'][0]):]\ngenerated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\nprint(generated_text)\n\n# See response at top of model card\n```\n\n## References(s):\n\n* [NeMo Aligner](https://arxiv.org/abs/2405.01481)\n* [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257)\n* [HelpSteer2](https://arxiv.org/abs/2406.08673)\n* [Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/) \n* [Meta's Llama 3.1 Webpage](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) \n* [Meta's Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)\n \n\n## Model Architecture: \n**Architecture Type:** Transformer <br>\n**Network Architecture:** Llama 3.1 <br>\n\n## Input:\n**Input Type(s):** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Input:** Max of 128k tokens<br>\n\n## Output:\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Output:**  Max of 4k tokens <br>\n\n\n## Software Integration:\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Ampere <br>\n* NVIDIA Hopper <br>\n* NVIDIA Turing <br>\n**Supported Operating System(s):** Linux <br>\n\n## Model Version: \nv1.0\n\n# Training & Evaluation: \n\n## Alignment methodology\n* REINFORCE implemented in NeMo Aligner \n\n## Datasets:\n\n**Data Collection Method by dataset** <br>\n* [Hybrid: Human, Synthetic] <br>\n\n**Labeling Method by dataset** <br>\n* [Human] <br>\n\n**Link:** \n* [HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)\n\n**Properties (Quantity, Dataset Descriptions, Sensor(s)):** <br>\n* 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity.\n* 20, 324 prompt-responses used for training and 1, 038 used for validation.\n\n\n# Inference:\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server) <br>\n**Test Hardware:** H100, A100 80GB, A100 40GB <br>\n\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@misc{wang2024helpsteer2preferencecomplementingratingspreferences,\n      title={HelpSteer2-Preference: Complementing Ratings with Preferences}, \n      author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},\n      year={2024},\n      eprint={2410.01257},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2410.01257}, \n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"}]},{"id":"zai-org/chatglm2-6b","name":"chatglm2-6b","description":"A model for various tasks.","task":"N/A","tags":["transformers","pytorch","chatglm","glm","thudm","custom_code","zh","en","arxiv:2103.10360","arxiv:2210.02414","arxiv:1911.02150","arxiv:2406.12793","endpoints_compatible","region:us"],"likes":2054,"downloads":637017,"readme":"---\nlanguage:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm\n---\n# ChatGLM2-6B\n<p align=\"center\">\n  💻 <a href=\"https://github.com/THUDM/ChatGLM2-6B\" target=\"_blank\">Github Repo</a> • 🐦 <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> • 📃 <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> • 📃 <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GLM-130B\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n\n<p align=\"center\">\n    👋 Join our <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-1y7pqoloy-9b1g6T6JjA8J0KxvUjbwJw\" target=\"_blank\">Slack</a> and <a href=\"https://github.com/THUDM/ChatGLM-6B/blob/main/resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<p align=\"center\">\n📍Experience the larger-scale ChatGLM model at <a href=\"https://www.chatglm.cn\">chatglm.cn</a>\n</p>\n\n## 介绍\nChatGLM**2**-6B 是开源中英双语对话模型 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM**2**-6B 引入了如下新特性：\n\n1. **更强大的性能**：基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 [GLM](https://github.com/THUDM/GLM) 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，[评测结果](#评测结果)显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。\n2. **更长的上下文**：基于 [FlashAttention](https://github.com/HazyResearch/flash-attention) 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。但当前版本的 ChatGLM2-6B 对单轮超长文档的理解能力有限，我们会在后续迭代升级中着重进行优化。\n3. **更高效的推理**：基于 [Multi-Query Attention](http://arxiv.org/abs/1911.02150) 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。\n4. **更开放的协议**：ChatGLM2-6B 权重对学术研究**完全开放**，在填写[问卷](https://open.bigmodel.cn/mla/form)进行登记后**亦允许免费商业使用**。\n\nChatGLM**2**-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B). It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:\n\n1. **Stronger Performance**: Based on the development experience of the first-generation ChatGLM model, we have fully upgraded the base model of ChatGLM2-6B. ChatGLM2-6B uses the hybrid objective function of [GLM](https://github.com/THUDM/GLM), and has undergone pre-training with 1.4T bilingual tokens and human preference alignment training. The [evaluation results](README.md#evaluation-results) show that, compared to the first-generation model, ChatGLM2-6B has achieved substantial improvements in performance on datasets like MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%), showing strong competitiveness among models of the same size.\n2. **Longer Context**: Based on [FlashAttention](https://github.com/HazyResearch/flash-attention) technique, we have extended the context length of the base model from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment, allowing for more rounds of dialogue. However, the current version of ChatGLM2-6B has limited understanding of single-round ultra-long documents, which we will focus on optimizing in future iterations.\n3. **More Efficient Inference**: Based on [Multi-Query Attention](http://arxiv.org/abs/1911.02150) technique, ChatGLM2-6B has more efficient inference speed and lower GPU memory usage: under the official  implementation, the inference speed has increased by 42% compared to the first generation; under INT4 quantization, the dialogue length supported by 6G GPU memory has increased from 1K to 8K.\n4. **More Open License**: ChatGLM2-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\n## 软件依赖\n\n```shell\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate\n```\n\n## 代码调用 \n\n可以通过如下代码调用 ChatGLM-6B 模型来生成对话：\n\n```ipython\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).half().cuda()\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, \"你好\", history=[])\n>>> print(response)\n你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\n>>> response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n>>> print(response)\n晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:\n\n1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。\n2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。\n3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。\n4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。\n5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。\n6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。\n\n如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。\n```\n\n关于更多的使用说明，包括如何运行命令行和网页版本的 DEMO，以及使用模型量化以节省显存，请参考我们的 [Github Repo](https://github.com/THUDM/ChatGLM2-6B)。\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM2-6B).\n\n## Change Log\n* v1.0\n\n## 协议\n\n本仓库的代码依照 [Apache-2.0](LICENSE) 协议开源，ChatGLM2-6B 模型的权重的使用则需要遵循 [Model License](MODEL_LICENSE)。\n\n## 引用\n\n如果你觉得我们的工作有帮助的话，请考虑引用下列论文。\n\nIf you find our work helpful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/zai-org/chatglm2-6b"}]},{"id":"Lightricks/LTX-Video","name":"LTX-Video","description":"A model for image-to-video.","task":"image-to-video","tags":["diffusers","safetensors","ltx-video","image-to-video","en","license:other","diffusers:LTXPipeline","region:us"],"likes":2039,"downloads":242182,"readme":"---\ntags:\n- ltx-video\n- image-to-video\npinned: true\nlanguage:\n- en\nlicense: other\nlibrary_name: diffusers\n---\n\n# LTX-Video Model Card\nThis model card focuses on the model associated with the LTX-Video model, codebase available [here](https://github.com/Lightricks/LTX-Video).\n\nLTX-Video is the first DiT-based video generation model capable of generating high-quality videos in real-time. It produces 30 FPS videos at a 1216×704 resolution faster than they can be watched. Trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content.\n\n<img src=\"./media/trailer.gif\" alt=\"trailer\" width=\"512\">\n\n### Image-to-video examples\n| | | |\n|:---:|:---:|:---:|\n| ![example1](./media/ltx-video_i2v_example_00001.gif) | ![example2](./media/ltx-video_i2v_example_00002.gif) | ![example3](./media/ltx-video_i2v_example_00003.gif) |\n| ![example4](./media/ltx-video_i2v_example_00004.gif) | ![example5](./media/ltx-video_i2v_example_00005.gif) |  ![example6](./media/ltx-video_i2v_example_00006.gif) |\n| ![example7](./media/ltx-video_i2v_example_00007.gif) |  ![example8](./media/ltx-video_i2v_example_00008.gif) | ![example9](./media/ltx-video_i2v_example_00009.gif) |\n\n# Models & Workflows\n\n| Name                                                                                                                                   | Notes                                                                                                         | inference.py config                                                                                                              | ComfyUI workflow (Recommended)                                                                                                                                |\n|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ltxv-13b-0.9.8-dev                                                                                                                     | Highest quality, requires more VRAM                                                                           | [ltxv-13b-0.9.8-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)                     | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)                                 |\n| [ltxv-13b-0.9.8-mix](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)                                                      | Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality | N/A                                                                                                                              | [ltxv-13b-i2v-mixed-multiscale.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json)         |\n| [ltxv-13b-0.9.8-distilled](https://app.ltx.studio/motion-workspace?videoModel=ltxv)                                                    | Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations                 | [ltxv-13b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)               | [ltxv-13b-dist-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json)         |\n| ltxv-2b-0.9.8-distilled                                                                                                                | Smaller model, slight quality reduction compared to 13b distilled. Ideal for light VRAM usage                 | [ltxv-2b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-dev.yaml)                 | N/A                                                                                                                                                           |\n| ltxv-13b-0.9.8-fp8                                                                                                                     | Quantized version of ltxv-13b                                                                                 | [ltxv-13b-0.9.8-dev-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml)             | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json)                         |\n| ltxv-13b-0.9.8-distilled-fp8                                                                                                           | Quantized version of ltxv-13b-distilled                                                                       | [ltxv-13b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml) | [ltxv-13b-dist-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json) |\n| ltxv-2b-0.9.8-distilled-fp8                                                                                                            | Quantized version of ltxv-2b-distilled                                                                        | [ltxv-2b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml)   | N/A                                                                                                                                                           |\n| ltxv-2b-0.9.6                                                                                                                          | Good quality, lower VRAM requirement than ltxv-13b                                                            | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                       | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)                                 |\n| ltxv-2b-0.9.6-distilled                                                                                                                | 15× faster, real-time capable, fewer steps needed, no STG/CFG required                                        | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)           | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |\n\n\n## Model Details\n- **Developed by:** Lightricks\n- **Model type:** Diffusion-based image-to-video generation model\n- **Language(s):** English\n\n\n## Usage\n\n### Direct use\nYou can use the model for purposes under the license:\n- 2B version 0.9: [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.license.txt)\n- 2B version 0.9.1 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.1.license.txt)\n- 2B version 0.9.5 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.5.license.txt)\n- 2B version 0.9.6-dev [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 2B version 0.9.6-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-dev [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-dev-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-distilled-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-distilled-lora128 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-ICLoRA Depth [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-ICLoRA Pose [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.7-ICLoRA Canny [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- Temporal upscaler version 0.9.7 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- Spatial upscaler version 0.9.7 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.8-dev [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.8-dev-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.8-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.8-distilled-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 2B version 0.9.8-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 2B version 0.9.8-distilled-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- 13B version 0.9.8-ICLoRA detailer [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- Temporal upscaler version 0.9.8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n- Spatial upscaler version 0.9.8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)\n\n### General tips:\n* The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames.\n* The model works best on resolutions under 720 x 1280 and number of frames below 257.\n* Prompts should be in English. The more elaborate the better. Good prompt looks like `The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim.`\n\n### Online demo\nThe model is accessible right away via the following links:\n- [LTX-Studio image-to-video (13B-mix)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)\n- [LTX-Studio image-to-video (13B distilled)](https://app.ltx.studio/motion-workspace?videoModel=ltxv)\n- [Fal.ai image-to-video (13B full)](https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video)\n- [Fal.ai image-to-video (13B distilled)](https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video)\n- [Replicate image-to-video](https://replicate.com/lightricks/ltx-video)\n\n### ComfyUI\nTo use our model with ComfyUI, please follow the instructions at a dedicated [ComfyUI repo](https://github.com/Lightricks/ComfyUI-LTXVideo/).\n\n### Run locally\n\n#### Installation\n\nThe codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch >= 2.1.2.\n\n```bash\ngit clone https://github.com/Lightricks/LTX-Video.git\ncd LTX-Video\n\n# create env\npython -m venv env\nsource env/bin/activate\npython -m pip install -e .\\[inference-script\\]\n```\n\n#### Inference\n\nTo use our model, please follow the inference code in [inference.py](https://github.com/Lightricks/LTX-Video/blob/main/inference.py):\n\n\n#### For image-to-video generation:\n\n```bash\npython inference.py --prompt \"PROMPT\" --input_image_path IMAGE_PATH --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml\n```\n\n#### For video generation with multiple conditions:\n\nYou can now generate a video conditioned on a set of images and/or short video segments.\nSimply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).\n\n```bash\npython inference.py --prompt \"PROMPT\" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml\n```\n\n### Diffusers 🧨\n\nLTX Video is compatible with the [Diffusers Python library](https://huggingface.co/docs/diffusers/main/en/index) for image-to-video generation.\n\nMake sure you install `diffusers` before trying out the examples below.\n\n```bash\npip install -U git+https://github.com/huggingface/diffusers\n```\n\nNow, you can run the examples below (note that the upsampling stage is optional but reccomeneded):\n\n\n### For image-to-video:\n\n```py\nimport torch\nfrom diffusers import LTXConditionPipeline, LTXLatentUpsamplePipeline\nfrom diffusers.pipelines.ltx.pipeline_ltx_condition import LTXVideoCondition\nfrom diffusers.utils import export_to_video, load_image, load_video\n\npipe = LTXConditionPipeline.from_pretrained(\"Lightricks/LTX-Video-0.9.8-dev\", torch_dtype=torch.bfloat16)\npipe_upsample = LTXLatentUpsamplePipeline.from_pretrained(\"Lightricks/ltxv-spatial-upscaler-0.9.8\", vae=pipe.vae, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\npipe_upsample.to(\"cuda\")\npipe.vae.enable_tiling()\n\ndef round_to_nearest_resolution_acceptable_by_vae(height, width):\n    height = height - (height % pipe.vae_spatial_compression_ratio)\n    width = width - (width % pipe.vae_spatial_compression_ratio)\n    return height, width\n\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/penguin.png\")\nvideo = load_video(export_to_video([image])) # compress the image using video compression as the model was trained on videos\ncondition1 = LTXVideoCondition(video=video, frame_index=0)\n\nprompt = \"A cute little penguin takes out a book and starts reading it\"\nnegative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\nexpected_height, expected_width = 480, 832\ndownscale_factor = 2 / 3\nnum_frames = 96\n\n# Part 1. Generate video at smaller resolution\ndownscaled_height, downscaled_width = int(expected_height * downscale_factor), int(expected_width * downscale_factor)\ndownscaled_height, downscaled_width = round_to_nearest_resolution_acceptable_by_vae(downscaled_height, downscaled_width)\nlatents = pipe(\n    conditions=[condition1],\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=downscaled_width,\n    height=downscaled_height,\n    num_frames=num_frames,\n    num_inference_steps=30,\n    generator=torch.Generator().manual_seed(0),\n    output_type=\"latent\",\n).frames\n\n# Part 2. Upscale generated video using latent upsampler with fewer inference steps\n# The available latent upsampler upscales the height/width by 2x\nupscaled_height, upscaled_width = downscaled_height * 2, downscaled_width * 2\nupscaled_latents = pipe_upsample(\n    latents=latents,\n    output_type=\"latent\"\n).frames\n\n# Part 3. Denoise the upscaled video with few steps to improve texture (optional, but recommended)\nvideo = pipe(\n    conditions=[condition1],\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=upscaled_width,\n    height=upscaled_height,\n    num_frames=num_frames,\n    denoise_strength=0.4,  # Effectively, 4 inference steps out of 10\n    num_inference_steps=10,\n    latents=upscaled_latents,\n    decode_timestep=0.05,\n    image_cond_noise_scale=0.025,\n    generator=torch.Generator().manual_seed(0),\n    output_type=\"pil\",\n).frames[0]\n\n# Part 4. Downscale the video to the expected resolution\nvideo = [frame.resize((expected_width, expected_height)) for frame in video]\n\nexport_to_video(video, \"output.mp4\", fps=24)\n```\n\n\n### For video-to-video: \n\n```py\nimport torch\nfrom diffusers import LTXConditionPipeline, LTXLatentUpsamplePipeline\nfrom diffusers.pipelines.ltx.pipeline_ltx_condition import LTXVideoCondition\nfrom diffusers.utils import export_to_video, load_video\n\npipe = LTXConditionPipeline.from_pretrained(\"Lightricks/LTX-Video-0.9.8-dev\", torch_dtype=torch.bfloat16)\npipe_upsample = LTXLatentUpsamplePipeline.from_pretrained(\"Lightricks/ltxv-spatial-upscaler-0.9.8\", vae=pipe.vae, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\npipe_upsample.to(\"cuda\")\npipe.vae.enable_tiling()\n\ndef round_to_nearest_resolution_acceptable_by_vae(height, width):\n    height = height - (height % pipe.vae_spatial_compression_ratio)\n    width = width - (width % pipe.vae_spatial_compression_ratio)\n    return height, width\n\nvideo = load_video(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cosmos/cosmos-video2world-input-vid.mp4\"\n)[:21]  # Use only the first 21 frames as conditioning\ncondition1 = LTXVideoCondition(video=video, frame_index=0)\n\nprompt = \"The video depicts a winding mountain road covered in snow, with a single vehicle traveling along it. The road is flanked by steep, rocky cliffs and sparse vegetation. The landscape is characterized by rugged terrain and a river visible in the distance. The scene captures the solitude and beauty of a winter drive through a mountainous region.\"\nnegative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\nexpected_height, expected_width = 768, 1152\ndownscale_factor = 2 / 3\nnum_frames = 161\n\n# Part 1. Generate video at smaller resolution\ndownscaled_height, downscaled_width = int(expected_height * downscale_factor), int(expected_width * downscale_factor)\ndownscaled_height, downscaled_width = round_to_nearest_resolution_acceptable_by_vae(downscaled_height, downscaled_width)\nlatents = pipe(\n    conditions=[condition1],\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=downscaled_width,\n    height=downscaled_height,\n    num_frames=num_frames,\n    num_inference_steps=30,\n    generator=torch.Generator().manual_seed(0),\n    output_type=\"latent\",\n).frames\n\n# Part 2. Upscale generated video using latent upsampler with fewer inference steps\n# The available latent upsampler upscales the height/width by 2x\nupscaled_height, upscaled_width = downscaled_height * 2, downscaled_width * 2\nupscaled_latents = pipe_upsample(\n    latents=latents,\n    output_type=\"latent\"\n).frames\n\n# Part 3. Denoise the upscaled video with few steps to improve texture (optional, but recommended)\nvideo = pipe(\n    conditions=[condition1],\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    width=upscaled_width,\n    height=upscaled_height,\n    num_frames=num_frames,\n    denoise_strength=0.4,  # Effectively, 4 inference steps out of 10\n    num_inference_steps=10,\n    latents=upscaled_latents,\n    decode_timestep=0.05,\n    image_cond_noise_scale=0.025,\n    generator=torch.Generator().manual_seed(0),\n    output_type=\"pil\",\n).frames[0]\n\n# Part 4. Downscale the video to the expected resolution\nvideo = [frame.resize((expected_width, expected_height)) for frame in video]\n\nexport_to_video(video, \"output.mp4\", fps=24)\n```\n\nTo learn more, check out the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video). \n\nDiffusers also supports directly loading from the original LTX checkpoints using the `from_single_file()` method. Check out [this section](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video#loading-single-files) to learn more.\n\n## Limitations\n- This model is not intended or able to provide factual information.\n- As a statistical model this checkpoint might amplify existing societal biases.\n- The model may fail to generate videos that matches the prompts perfectly.\n- Prompt following is heavily influenced by the prompting-style.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Lightricks/LTX-Video"}]},{"id":"stabilityai/stable-diffusion-xl-refiner-1.0","name":"stable-diffusion-xl-refiner-1.0","description":"A model for image-to-image.","task":"image-to-image","tags":["diffusers","safetensors","stable-diffusion","image-to-image","arxiv:2307.01952","arxiv:2211.01324","arxiv:2108.01073","arxiv:2112.10752","license:openrail++","diffusers:StableDiffusionXLImg2ImgPipeline","region:us"],"likes":1990,"downloads":429292,"readme":"---\nlicense: openrail++\ntags:\n- stable-diffusion\n- image-to-image\n---\n# SD-XL 1.0-refiner Model Card\n![row01](01.png)\n\n## Model\n\n![pipeline](pipeline.png)\n\n[SDXL](https://arxiv.org/abs/2307.01952) consists of an [ensemble of experts](https://arxiv.org/abs/2211.01324) pipeline for latent diffusion: \nIn a first step, the base model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) is used to generate (noisy) latents, \nwhich are then further processed with a refinement model specialized for the final denoising steps.\nNote that the base model can be used as a standalone module.\n\nAlternatively, we can use a two-stage pipeline as follows: \nFirst, the base model is used to generate latents of the desired output size. \nIn the second step, we use a specialized high-resolution model and apply a technique called SDEdit (https://arxiv.org/abs/2108.01073, also known as \"img2img\") \nto the latents generated in the first step, using the same prompt. This technique is slightly slower than the first one, as it requires more function evaluations.\n\nSource code is available at https://github.com/Stability-AI/generative-models .\n\n### Model Description\n\n- **Developed by:** Stability AI\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [CreativeML Open RAIL++-M License](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/blob/main/LICENSE.md)\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses two fixed, pretrained text encoders ([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip) and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main)).\n- **Resources for more information:** Check out our [GitHub Repository](https://github.com/Stability-AI/generative-models) and the [SDXL report on arXiv](https://arxiv.org/abs/2307.01952).\n\n### Model Sources\n\nFor research purposes, we recommned our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), which implements the most popoular diffusion frameworks (both training and inference) and for which new functionalities like distillation will be added over time.\n[Clipdrop](https://clipdrop.co/stable-diffusion) provides free SDXL inference.\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Demo:** https://clipdrop.co/stable-diffusion\n\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SDXL (with and without refinement) over SDXL 0.9 and Stable Diffusion 1.5 and 2.1. \nThe SDXL base model performs significantly better than the previous variants, and the model combined with the refinement module achieves the best overall performance.\n\n\n### 🧨 Diffusers \n\nMake sure to upgrade diffusers to >= 0.18.0:\n```\npip install diffusers --upgrade\n```\n\nIn addition make sure to install `transformers`, `safetensors`, `accelerate` as well as the invisible watermark:\n```\npip install invisible_watermark transformers accelerate safetensors\n```\n\nYon can then use the refiner to improve images.\n\n```py\nimport torch\nfrom diffusers import StableDiffusionXLImg2ImgPipeline\nfrom diffusers.utils import load_image\n\npipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipe = pipe.to(\"cuda\")\nurl = \"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png\"\n\ninit_image = load_image(url).convert(\"RGB\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt, image=init_image).images\n```\n\nWhen using `torch >= 2.0`, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline:\n```py\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```\n\nIf you are limited by GPU VRAM, you can enable *cpu offloading* by calling `pipe.enable_model_cpu_offload`\ninstead of `.to(\"cuda\")`:\n\n```diff\n- pipe.to(\"cuda\")\n+ pipe.enable_model_cpu_offload()\n```\n\nFor more advanced use cases, please have a look at [the docs](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl).\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0"}]},{"id":"databricks/dolly-v2-12b","name":"dolly-v2-12b","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","gpt_neox","text-generation","en","dataset:databricks/databricks-dolly-15k","license:mit","autotrain_compatible","text-generation-inference","region:us"],"likes":1955,"downloads":1722,"readme":"---\nlicense: mit\nlanguage:\n- en\nlibrary_name: transformers\ninference: false\ndatasets:\n- databricks/databricks-dolly-15k\n---\n# dolly-v2-12b Model Card\n## Summary\n\nDatabricks' `dolly-v2-12b`, an instruction-following large language model trained on the Databricks machine learning platform \nthat is licensed for commercial use. Based on `pythia-12b`, Dolly is trained on ~15k instruction/response fine tuning records \n[`databricks-dolly-15k`](https://github.com/databrickslabs/dolly/tree/master/data) generated \nby Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,\ninformation extraction, open QA and summarization. `dolly-v2-12b` is not a state-of-the-art model, but does exhibit surprisingly \nhigh quality instruction following behavior not characteristic of the foundation model on which it is based.  \n\nDolly v2 is also available in these smaller models sizes:\n\n* [dolly-v2-7b](https://huggingface.co/databricks/dolly-v2-7b), a 6.9 billion parameter based on `pythia-6.9b`\n* [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), a 2.8 billion parameter based on `pythia-2.8b`\n\nPlease refer to the [dolly GitHub repo](https://github.com/databrickslabs/dolly#getting-started-with-response-generation) for tips on \nrunning inference for various GPU configurations.\n\n**Owner**: Databricks, Inc.\n\n## Model Overview\n`dolly-v2-12b` is a 12 billion parameter causal language model created by [Databricks](https://databricks.com/) that is derived from \n[EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b) and fine-tuned \non a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees and released under a permissive license (CC-BY-SA)\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the `transformers` and `accelerate` libraries installed.\nIn a Databricks notebook you could run:\n\n```python\n%pip install \"accelerate>=0.16.0,<1\" \"transformers[torch]>=4.28.1,<5\" \"torch>=1.13.1,<2\"\n```\n\nThe instruction following pipeline can be loaded using the `pipeline` function as shown below.  This loads a custom `InstructionTextGenerationPipeline` \nfound in the model repo [here](https://huggingface.co/databricks/dolly-v2-3b/blob/main/instruct_pipeline.py), which is why `trust_remote_code=True` is required.\nIncluding `torch_dtype=torch.bfloat16` is generally recommended if this type is supported in order to reduce memory usage.  It does not appear to impact output quality.\nIt is also fine to remove it if there is sufficient memory.\n\n```python\nimport torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n```\n\nYou can then use the pipeline to answer instructions:\n\n```python\nres = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\nprint(res[0][\"generated_text\"])\n```\n\nAlternatively, if you prefer to not use `trust_remote_code=True` you can download [instruct_pipeline.py](https://huggingface.co/databricks/dolly-v2-3b/blob/main/instruct_pipeline.py),\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\n\n```python\nimport torch\nfrom instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-12b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ngenerate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n```\n\n### LangChain Usage\n\nTo use the pipeline with LangChain, you must set `return_full_text=True`, as LangChain expects the full text to be returned \nand the default for the pipeline is to only return the new text.\n\n```python\nimport torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\n                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n```\n\nYou can create a prompt that either has only an instruction or has an instruction with context:\n\n```python\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\n# template for an instrution with no input\nprompt = PromptTemplate(\n    input_variables=[\"instruction\"],\n    template=\"{instruction}\")\n\n# template for an instruction with input\nprompt_with_context = PromptTemplate(\n    input_variables=[\"instruction\", \"context\"],\n    template=\"{instruction}\\n\\nInput:\\n{context}\")\n\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n```\n\nExample predicting using a simple instruction:\n\n```python\nprint(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())\n```\n\nExample predicting using an instruction with context:\n\n```python\ncontext = \"\"\"George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,\nand Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n\nprint(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\n```\n\n\n## Known Limitations\n\n### Performance Limitations\n**`dolly-v2-12b` is not a state-of-the-art generative language model** and, though quantitative benchmarking is ongoing, is not designed to perform \ncompetitively with more modern model architectures or models subject to larger pretraining corpuses.  \n\nThe Dolly model family is under active development, and so any list of shortcomings is unlikely to be exhaustive, but we include known limitations and misfires here as a means to document and share our preliminary findings with the community.  \nIn particular, `dolly-v2-12b` struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors, \ndates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc.\nMoreover, we find that `dolly-v2-12b` does not have some capabilities, such as well-formatted letter writing, present in the original model.  \n\n### Dataset Limitations\nLike all language models, `dolly-v2-12b` reflects the content and limitations of its training corpuses. \n\n- **The Pile**: GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets,\nit contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly\nin the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit\nassociations.\n\n- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated\nby Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages\nfor instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or\npersonally identifying information about non-public figures, but it may contain typos and factual errors.\nThe dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects\nthe interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.\n\nDatabricks is committed to ongoing research and development efforts to develop helpful, honest and harmless AI technologies that \nmaximize the potential of all individuals and organizations. \n\n### Benchmark Metrics\n\nBelow you'll find various models benchmark performance on the [EleutherAI LLM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness); \nmodel results are sorted by geometric mean to produce an intelligible ordering. As outlined above, these results demonstrate that `dolly-v2-12b` is not state of the art, \nand in fact underperforms `dolly-v1-6b` in some evaluation benchmarks. We believe this owes to the composition and size of the underlying fine tuning datasets, \nbut a robust statement as to the sources of these variations requires further study.  \n\n|  model                            |   openbookqa |   arc_easy |   winogrande |   hellaswag |   arc_challenge |     piqa |    boolq |    gmean |\n| --------------------------------- | ------------ | ---------- | ------------ | ----------- | --------------- | -------- | -------- | ---------|\n| EleutherAI/pythia-2.8b            |        0.348 |   0.585859 |     0.589582 |    0.591217 |        0.323379 | 0.73395  | 0.638226 | 0.523431 |\n| EleutherAI/pythia-6.9b            |        0.368 |   0.604798 |     0.608524 |    0.631548 |        0.343857 | 0.761153 | 0.6263   | 0.543567 |\n| databricks/dolly-v2-3b            |        0.384 |   0.611532 |     0.589582 |    0.650767 |        0.370307 | 0.742655 | 0.575535 | 0.544886 |\n| EleutherAI/pythia-12b             |        0.364 |   0.627104 |     0.636148 |    0.668094 |        0.346416 | 0.760065 | 0.673394 | 0.559676 |\n| EleutherAI/gpt-j-6B               |        0.382 |   0.621633 |     0.651144 |    0.662617 |        0.363481 | 0.761153 | 0.655963 | 0.565936 |\n| databricks/dolly-v2-12b           |        0.408 |   0.63931  |     0.616417 |    0.707927 |        0.388225 | 0.757889 | 0.568196 | 0.56781  |\n| databricks/dolly-v2-7b            |        0.392 |   0.633838 |     0.607735 |    0.686517 |        0.406997 | 0.750816 | 0.644037 | 0.573487 |\n| databricks/dolly-v1-6b            |        0.41  |   0.62963  |     0.643252 |    0.676758 |        0.384812 | 0.773667 | 0.687768 | 0.583431 |\n| EleutherAI/gpt-neox-20b           |        0.402 |   0.683923 |     0.656669 |    0.7142   |        0.408703 | 0.784004 | 0.695413 | 0.602236 |\n\n# Citation\n\n```\n@online{DatabricksBlog2023DollyV2,\n    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},\n    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},\n    year      = {2023},\n    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},\n    urldate   = {2023-06-30}\n}\n```\n\n# Happy Hacking!","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/databricks/dolly-v2-12b"}]},{"id":"microsoft/VibeVoice-1.5B","name":"VibeVoice-1.5B","description":"A model for text-to-speech.","task":"text-to-speech","tags":["transformers","safetensors","vibevoice","text-generation","Podcast","text-to-speech","en","zh","arxiv:2508.19205","arxiv:2412.08635","license:mit","autotrain_compatible","endpoints_compatible","region:us"],"likes":1954,"downloads":182036,"readme":"---\nlanguage:\n- en\n- zh\nlicense: mit\npipeline_tag: text-to-speech\ntags:\n- Podcast\nlibrary_name: transformers\n---\n\n## VibeVoice: A Frontier Open-Source Text-to-Speech Model\n\nVibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\n\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\n\nThe model can synthesize speech up to **90 minutes** long with up to **4 distinct speakers**, surpassing the typical 1-2 speaker limits of many prior models. \n\n➡️ **Technical Report:** [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)\n\n➡️ **Project Page:** [microsoft/VibeVoice](https://microsoft.github.io/VibeVoice)\n\n➡️ **Code:** [microsoft/VibeVoice-Code](https://github.com/microsoft/VibeVoice)\n\n<p align=\"left\">\n  <img src=\"figures/Fig1.png\" alt=\"VibeVoice Overview\" height=\"250px\">\n</p>\n\n## Training Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head.\n- LLM: [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B) for this release.\n- Tokenizers:\n    - Acoustic Tokenizer: Based on a σ-VAE variant (proposed in [LatentLM](https://arxiv.org/pdf/2412.08635)), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each.\n    - Semantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task.\n- Diffusion Head: Lightweight module (4 layers, ~123M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\n- Context Length: Trained with a curriculum increasing up to 65,536 tokens.\n- Training Stages:\n    - Tokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately.\n    - VibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K -> 64K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers.\n\n\n## Models\n| Model | Context Length | Generation Length |  Weight |\n|-------|----------------|----------|----------|\n| VibeVoice-0.5B-Streaming | - | - | On the way |\n| VibeVoice-1.5B | 64K | ~90 min | You are here. |\n| VibeVoice-Large| 32K | ~45 min | [HF link](https://huggingface.co/microsoft/VibeVoice-Large) |\n\n## Installation and Usage\n\nPlease refer to [GitHub README](https://github.com/microsoft/VibeVoice?tab=readme-ov-file#installation)\n\n## Responsible Usage\n### Direct intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the [tech report](https://arxiv.org/pdf/2508.19205). \n\n### Out-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\n\n- Voice impersonation without explicit, recorded consent – cloning a real individual’s voice for satire, advertising, ransom, social‑engineering, or authentication bypass.\n- Disinformation or impersonation – creating audio presented as genuine recordings of real people or events.\n- Real‑time or low‑latency voice conversion – telephone or video‑conference “live deep‑fake” applications.\n- Unsupported language – the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\n- Generation of background ambience, Foley, or music – VibeVoice is speech‑only and will not produce coherent non‑speech audio.\n\n\n## Risks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). \nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\n\n\n## Recommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\n\nTo mitigate the risks of misuse, we have:\nEmbedded an audible disclaimer (e.g. “This segment was generated by AI”) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nLogged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.\nUsers are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns. \n\n\n## Contact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/VibeVoice-1.5B"}]},{"id":"Qwen/Qwen2.5-Coder-32B-Instruct","name":"Qwen2.5-Coder-32B-Instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","qwen2","text-generation","code","codeqwen","chat","qwen","qwen-coder","conversational","en","arxiv:2409.12186","arxiv:2309.00071","arxiv:2407.10671","base_model:Qwen/Qwen2.5-Coder-32B","base_model:finetune:Qwen/Qwen2.5-Coder-32B","license:apache-2.0","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1947,"downloads":110135,"readme":"---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/LICENSE\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-Coder-32B\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- code\n- codeqwen\n- chat\n- qwen\n- qwen-coder\n---\n\n\n# Qwen2.5-Coder-32B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n- **Long-context Support** up to 128K tokens.\n\n**This repo contains the instruction-tuned 32B Qwen2.5-Coder model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 32.5B\n- Number of Paramaters (Non-Embedding): 31.0B\n- Number of Layers: 64\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n- Context Length: Full 131,072 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n  \nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/), [GitHub](https://github.com/QwenLM/Qwen2.5-Coder), [Documentation](https://qwen.readthedocs.io/en/latest/), [Arxiv](https://arxiv.org/abs/2409.12186).\n\n## Requirements\n\nThe code of Qwen2.5-Coder has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"}]},{"id":"stabilityai/stable-diffusion-2","name":"stable-diffusion-2","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","text-to-image","arxiv:2202.00512","arxiv:2112.10752","arxiv:1910.09700","license:openrail++","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":1921,"downloads":226966,"readme":"---\nlicense: openrail++\ntags:\n- stable-diffusion\n- text-to-image\n---\n\n# Stable Diffusion v2 Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2 model, available [here](https://github.com/Stability-AI/stablediffusion).\n\nThis `stable-diffusion-2` model is resumed from [stable-diffusion-2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base) (`512-base-ema.ckpt`) and trained for 150k steps using a [v-objective](https://arxiv.org/abs/2202.00512) on the same dataset. Resumed for another 140k steps on `768x768` images.\n\n![image](https://github.com/Stability-AI/stablediffusion/blob/main/assets/stable-samples/txt2img/768/merged-0005.png?raw=true)\n\n- Use it with the [`stablediffusion`](https://github.com/Stability-AI/stablediffusion) repository: download the `768-v-ema.ckpt` [here](https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/768-v-ema.ckpt).\n- Use it with 🧨 [`diffusers`](https://huggingface.co/stabilityai/stable-diffusion-2#examples)\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [CreativeML Open RAIL++-M License](https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/LICENSE-MODEL)\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([OpenCLIP-ViT/H](https://github.com/mlfoundations/open_clip)).\n- **Resources for more information:** [GitHub Repository](https://github.com/Stability-AI/).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n\n## Examples\n\nUsing the [🤗's Diffusers library](https://github.com/huggingface/diffusers) to run Stable Diffusion 2 in a simple and efficient manner.\n\n```bash\npip install diffusers transformers accelerate scipy safetensors\n```\n\nRunning the pipeline (if you don't swap the scheduler it will run with the default DDIM, in this example we are swapping it to EulerDiscreteScheduler):\n\n```python\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n\nmodel_id = \"stabilityai/stable-diffusion-2\"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\n**Notes**:\n- Despite not being a dependency, we highly recommend you to install [xformers](https://github.com/facebookresearch/xformers) for memory efficient attention (better performance)\n- If you have low GPU RAM available, make sure to add a `pipe.enable_attention_slicing()` after sending it to `cuda` for less VRAM usage (to the cost of speed)\n\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is originally taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2_.\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a subset of the large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\n\n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's [NeurIPS 2022](https://openreview.net/forum?id=M3Y74vmsMcY) paper and reviewer discussions on the topic.\n\n**Training Procedure**\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through the OpenCLIP-ViT/H text-encoder.\n- The output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called _v-objective_, see https://arxiv.org/abs/2202.00512.\n\nWe currently provide the following checkpoints:\n\n- `512-base-ema.ckpt`: 550k steps at resolution `256x256` on a subset of [LAION-5B](https://laion.ai/blog/laion-5b/) filtered for explicit pornographic material, using the [LAION-NSFW classifier](https://github.com/LAION-AI/CLIP-based-NSFW-Detector) with `punsafe=0.1` and an [aesthetic score](https://github.com/christophschuhmann/improved-aesthetic-predictor) >= `4.5`.\n  850k steps at resolution `512x512` on the same dataset with resolution `>= 512x512`.\n- `768-v-ema.ckpt`: Resumed from `512-base-ema.ckpt` and trained for 150k steps using a [v-objective](https://arxiv.org/abs/2202.00512) on the same dataset. Resumed for another 140k steps on a `768x768` subset of our dataset.\n- `512-depth-ema.ckpt`: Resumed from `512-base-ema.ckpt` and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by [MiDaS](https://github.com/isl-org/MiDaS) (`dpt_hybrid`) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n- `512-inpainting-ema.ckpt`: Resumed from `512-base-ema.ckpt` and trained for another 200k steps. Follows the mask-generation strategy presented in [LAMA](https://github.com/saic-mdal/lama) which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the [1.5-inpainting checkpoint](https://github.com/saic-mdal/lama).\n- `x4-upscaling-ema.ckpt`: Trained for 1.25M steps on a 10M subset of LAION containing images `>2048x2048`. The model was trained on crops of size `512x512` and is a text-guided [latent upscaling diffusion model](https://arxiv.org/abs/2112.10752).\nIn addition to the textual input, it receives a `noise_level` as an input parameter, which can be used to add noise to the low-resolution input according to a [predefined diffusion schedule](configs/stable-diffusion/x4-upscaling.yaml). \n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 1\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\n\n![pareto](model-variants.jpg) \n\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 200000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 15000 kg CO2 eq.\n\n## Citation\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n\n*This model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md) and [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/stabilityai/stable-diffusion-2"}]},{"id":"meta-llama/Llama-3.1-8B","name":"Llama-3.1-8B","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.1","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1904,"downloads":563164,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-3.1-8B"}]},{"id":"openai/clip-vit-large-patch14","name":"clip-vit-large-patch14","description":"A model for zero-shot-image-classification.","task":"zero-shot-image-classification","tags":["transformers","pytorch","tf","jax","safetensors","clip","zero-shot-image-classification","vision","arxiv:2103.00020","arxiv:1908.04913","endpoints_compatible","region:us"],"likes":1894,"downloads":12221474,"readme":"---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai/clip-vit-large-patch14"}]},{"id":"briaai/RMBG-1.4","name":"RMBG-1.4","description":"A model for image-segmentation.","task":"image-segmentation","tags":["transformers","pytorch","onnx","safetensors","SegformerForSemanticSegmentation","image-segmentation","remove background","background","background-removal","Pytorch","vision","legal liability","transformers.js","custom_code","license:other","region:us"],"likes":1878,"downloads":212734,"readme":"---\nlicense: other\nlicense_name: bria-rmbg-1.4\nlicense_link: https://bria.ai/bria-huggingface-model-license-agreement/\npipeline_tag: image-segmentation\ntags:\n- remove background\n- background\n- background-removal\n- Pytorch\n- vision\n- legal liability\n- transformers\n- transformers.js\n\nextra_gated_description: RMBG v1.4 is available as a source-available model for non-commercial use\nextra_gated_heading: \"Fill in this form to get instant access\"\nextra_gated_fields:\n  Name: text\n  Company/Org name: text\n  Org Type (Early/Growth Startup, Enterprise, Academy): text\n  Role: text\n  Country: text\n  Email: text\n  By submitting this form, I agree to BRIA’s Privacy policy and Terms & conditions, see links below: checkbox\n---\n\n# BRIA Background Removal v1.4 Model Card\n\nRMBG v1.4 is our state-of-the-art background removal model, designed to effectively separate foreground from background in a range of\ncategories and image types. This model has been trained on a carefully selected dataset, which includes:\ngeneral stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. \nThe accuracy, efficiency, and versatility currently rival leading source-available models. \nIt is ideal where content safety, legally licensed datasets, and bias mitigation are paramount. \n\nDeveloped by BRIA AI, RMBG v1.4 is available as a source-available model for non-commercial use. \n\n\nTo purchase a commercial license, simply click [Here](https://go.bria.ai/3D5EGp0).\n\n\n[CLICK HERE FOR A DEMO](https://huggingface.co/spaces/briaai/BRIA-RMBG-1.4)\n\n**NOTE** New RMBG version available! Check out [RMBG-2.0](https://huggingface.co/briaai/RMBG-2.0)\n\nJoin our [Discord community](https://discord.gg/Nxe9YW9zHS) for more information, tutorials, tools, and to connect with other users!\n\n\n![examples](t4.png)\n\n\n### Model Description\n\n- **Developed by:** [BRIA AI](https://bria.ai/)\n- **Model type:** Background Removal \n- **License:** [bria-rmbg-1.4](https://bria.ai/bria-huggingface-model-license-agreement/)\n  - The model is released under a Creative Commons license for non-commercial use.\n  - Commercial use is subject to a commercial agreement with BRIA. To purchase a commercial license simply click [Here](https://go.bria.ai/3B4Asxv).\n\n- **Model Description:** BRIA RMBG 1.4 is a saliency segmentation model trained exclusively on a professional-grade dataset.\n- **BRIA:** Resources for more information: [BRIA AI](https://bria.ai/)\n\n\n\n## Training data\nBria-RMBG model was trained with over 12,000 high-quality, high-resolution, manually labeled (pixel-wise accuracy), fully licensed images.\nOur benchmark included balanced gender, balanced ethnicity, and people with different types of disabilities.\nFor clarity, we provide our data distribution according to different categories, demonstrating our model’s versatility.\n\n### Distribution of images:\n\n| Category | Distribution |\n| -----------------------------------| -----------------------------------:|\n| Objects only | 45.11% |\n| People with objects/animals | 25.24% |\n| People only | 17.35% |\n| people/objects/animals with text | 8.52% |\n| Text only | 2.52% |\n| Animals only | 1.89% |\n\n| Category | Distribution |\n| -----------------------------------| -----------------------------------------:|\n| Photorealistic | 87.70% |\n| Non-Photorealistic | 12.30% |\n\n\n| Category | Distribution |\n| -----------------------------------| -----------------------------------:|\n| Non Solid Background | 52.05% |\n| Solid Background | 47.95% \n\n\n| Category | Distribution |\n| -----------------------------------| -----------------------------------:|\n| Single main foreground object | 51.42% |\n| Multiple objects in the foreground | 48.58% |\n\n\n## Qualitative Evaluation\n\n![examples](results.png)\n\n\n## Architecture\n\nRMBG v1.4 is developed on the [IS-Net](https://github.com/xuebinqin/DIS) enhanced with our unique training scheme and proprietary dataset. \nThese modifications significantly improve the model’s accuracy and effectiveness in diverse image-processing scenarios.\n\n## Installation\n```bash\npip install -qr https://huggingface.co/briaai/RMBG-1.4/resolve/main/requirements.txt\n```\n\n## Usage\n\nEither load the pipeline\n```python\nfrom transformers import pipeline\nimage_path = \"https://farm5.staticflickr.com/4007/4322154488_997e69e4cf_z.jpg\"\npipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)\npillow_mask = pipe(image_path, return_mask = True) # outputs a pillow mask\npillow_image = pipe(image_path) # applies mask on input and returns a pillow image\n```\n\nOr load the model \n```python\nfrom PIL import Image\nfrom skimage import io\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForImageSegmentation\nfrom torchvision.transforms.functional import normalize\nmodel = AutoModelForImageSegmentation.from_pretrained(\"briaai/RMBG-1.4\",trust_remote_code=True)\ndef preprocess_image(im: np.ndarray, model_input_size: list) -> torch.Tensor:\n    if len(im.shape) < 3:\n        im = im[:, :, np.newaxis]\n    # orig_im_size=im.shape[0:2]\n    im_tensor = torch.tensor(im, dtype=torch.float32).permute(2,0,1)\n    im_tensor = F.interpolate(torch.unsqueeze(im_tensor,0), size=model_input_size, mode='bilinear')\n    image = torch.divide(im_tensor,255.0)\n    image = normalize(image,[0.5,0.5,0.5],[1.0,1.0,1.0])\n    return image\n\ndef postprocess_image(result: torch.Tensor, im_size: list)-> np.ndarray:\n    result = torch.squeeze(F.interpolate(result, size=im_size, mode='bilinear') ,0)\n    ma = torch.max(result)\n    mi = torch.min(result)\n    result = (result-mi)/(ma-mi)\n    im_array = (result*255).permute(1,2,0).cpu().data.numpy().astype(np.uint8)\n    im_array = np.squeeze(im_array)\n    return im_array\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# prepare input\nimage_path = \"https://farm5.staticflickr.com/4007/4322154488_997e69e4cf_z.jpg\"\norig_im = io.imread(image_path)\norig_im_size = orig_im.shape[0:2]\nmodel_input_size = [1024, 1024]\nimage = preprocess_image(orig_im, model_input_size).to(device)\n\n# inference \nresult=model(image)\n\n# post process\nresult_image = postprocess_image(result[0][0], orig_im_size)\n\n# save result\npil_mask_im = Image.fromarray(result_image)\norig_image = Image.open(image_path)\nno_bg_image = orig_image.copy()\nno_bg_image.putalpha(pil_mask_im)\n```\n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/briaai/RMBG-1.4"}]},{"id":"Qwen/Qwen2.5-Omni-7B","name":"Qwen2.5-Omni-7B","description":"A model for any-to-any.","task":"any-to-any","tags":["transformers","safetensors","qwen2_5_omni","multimodal","any-to-any","en","arxiv:2503.20215","license:other","endpoints_compatible","region:us"],"likes":1814,"downloads":182389,"readme":"---\nlicense: other\nlicense_name: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-Omni-7B/blob/main/LICENSE\nlanguage:\n- en\ntags:\n- multimodal\nlibrary_name: transformers\npipeline_tag: any-to-any\n---\n\n# Qwen2.5-Omni\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n\n## Overview \n### Introduction\nQwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. \n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png\" width=\"80%\"/>\n<p>\n\n### Key Features\n\n* **Omni and Novel Architecture**: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.\n\n* **Real-Time Voice and Video Chat**: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.\n\n* **Natural and Robust Speech Generation**: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.\n\n* **Strong Performance Across Modalities**: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.\n\n* **Excellent End-to-End Speech Instruction Following**: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png\" width=\"80%\"/>\n<p>\n\n### Performance\n\nWe conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png\" width=\"80%\"/>\n<p>\n\n<details>\n<summary>Multimodality  -> Text</summary>\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"10\">OmniBench<br>Speech | Sound Event | Music | Avg</td>\n    <td class=\"tg-0lax\">Gemini-1.5-Pro</td>\n    <td class=\"tg-0lax\">42.67%|42.26%|46.23%|42.91%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MIO-Instruct</td>\n    <td class=\"tg-0lax\">36.96%|33.58%|11.32%|33.80%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">AnyGPT (7B)</td>\n    <td class=\"tg-0lax\">17.77%|20.75%|13.21%|18.04%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">video-SALMONN</td>\n    <td class=\"tg-0lax\">34.11%|31.70%|<strong>56.60%</strong>|35.64%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">UnifiedIO2-xlarge</td>\n    <td class=\"tg-0lax\">39.56%|36.98%|29.25%|38.00%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">UnifiedIO2-xxlarge</td>\n    <td class=\"tg-0lax\">34.24%|36.98%|24.53%|33.98%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|-|40.50%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">-|-|-|42.90%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">52.14%|52.08%|52.83%|52.19%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>55.25%</strong>|<strong>60.00%</strong>|52.83%|<strong>56.13%</strong></td>\n  </tr>\n</tbody></table>\n</details>\n\n\n<details>\n<summary>Audio -> Text</summary>\n\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">ASR</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"12\">Librispeech<br>dev-clean | dev other | test-clean | test-other</td>\n    <td class=\"tg-0lax\">SALMONN</td>\n    <td class=\"tg-0lax\">-|-|2.1|4.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">SpeechVerse</td>\n    <td class=\"tg-0lax\">-|-|2.1|4.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">-|-|1.8|3.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-8B</td>\n    <td class=\"tg-0lax\">-|-|-|3.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-70B</td>\n    <td class=\"tg-0lax\">-|-|-|3.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-ASR-Multilingual</td>\n    <td class=\"tg-0lax\">-|-|<strong>1.6</strong>|<strong>2.8</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|1.7|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">-|-|1.7|3.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">1.8|4.0|2.0|4.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\"><strong>1.3</strong>|<strong>3.4</strong>|<strong>1.6</strong>|3.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">2.0|4.1|2.2|4.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">1.6|3.5|1.8|3.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"5\">Common Voice 15<br>en | zh | yue | fr</td>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">9.3|12.8|10.9|10.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">7.9|6.3|6.4|8.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">8.6|6.9|<strong>5.9</strong>|9.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">9.1|6.0|11.6|9.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>7.6</strong>|<strong>5.2</strong>|7.3|<strong>7.5</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"8\">Fleurs<br>zh | en</td>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">7.7|4.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-ASR-Multilingual</td>\n    <td class=\"tg-0lax\">-|<strong>3.4</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">10.8|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">4.4|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">3.0|3.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">7.5|-</td>\n  </tr>\n    <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">3.2|5.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>3.0</strong>|4.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">Wenetspeech<br>test-net | test-meeting</td>\n    <td class=\"tg-0lax\">Seed-ASR-Chinese</td>\n    <td class=\"tg-0lax\"><strong>4.7|5.7</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">-|16.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">6.9|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">6.8|7.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">6.3|8.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">5.9|7.7</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"4\">Voxpopuli-V1.0-en</td>\n    <td class=\"tg-0lax\">Llama-3-8B</td>\n    <td class=\"tg-0lax\">6.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-70B</td>\n    <td class=\"tg-0lax\"><strong>5.7</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">6.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">5.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">S2TT</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">CoVoST2<br>en-de | de-en | en-zh | zh-en</td>\n    <td class=\"tg-0lax\">SALMONN</td>\n    <td class=\"tg-0lax\">18.6|-|33.1|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">SpeechLLaMA</td>\n    <td class=\"tg-0lax\">-|27.1|-|12.3</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">BLSP</td>\n    <td class=\"tg-0lax\">14.1|-|-|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|<strong>48.2</strong>|27.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">-|<strong>39.9</strong>|46.7|26.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">25.1|33.9|41.5|15.7</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">29.9|35.2|45.2|24.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">28.3|38.1|41.4|26.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>30.2</strong>|37.7|41.4|<strong>29.4</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">SER</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">Meld</td>\n    <td class=\"tg-0lax\">WavLM-large</td>\n    <td class=\"tg-0lax\">0.542</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">0.524</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">0.557</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">0.553</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.558</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.570</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">VSC</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">VocalSound</td>\n    <td class=\"tg-0lax\">CLAP</td>\n    <td class=\"tg-0lax\">0.495</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Pengi</td>\n    <td class=\"tg-0lax\">0.604</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">0.929</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\"><strong>0.939</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.936</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.939</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Music</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"3\">GiantSteps Tempo</td>\n    <td class=\"tg-0lax\">Llark-7B</td>\n    <td class=\"tg-0lax\">0.86</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\"><strong>0.88</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.88</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"3\">MusicCaps</td>\n    <td class=\"tg-0lax\">LP-MusicCaps</td>\n    <td class=\"tg-0lax\">0.291|0.149|0.089|<strong>0.061</strong>|0.129|0.130</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.325|<strong>0.163</strong>|<strong>0.093</strong>|0.057|<strong>0.132</strong>|<strong>0.229</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.328</strong>|0.162|0.090|0.055|0.127|0.225</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Audio Reasoning</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"4\">MMAU<br>Sound | Music | Speech | Avg</td>\n    <td class=\"tg-0lax\">Gemini-Pro-V1.5</td>\n    <td class=\"tg-0lax\">56.75|49.40|58.55|54.90</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">54.95|50.98|42.04|49.20</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\"><strong>70.27</strong>|60.48|59.16|63.30</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">67.87|<strong>69.16|59.76|65.60</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Voice Chatting</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">VoiceBench<br>AlpacaEval | CommonEval | SD-QA | MMSU</td>\n    <td class=\"tg-0lax\">Ultravox-v0.4.1-LLaMA-3.1-8B</td>\n    <td class=\"tg-0lax\"><strong>4.55</strong>|3.90|53.35|47.17</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MERaLiON</td>\n    <td class=\"tg-0lax\">4.50|3.77|55.06|34.95</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">3.50|2.95|25.95|27.03</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Lyra-Base</td>\n    <td class=\"tg-0lax\">3.85|3.50|38.25|49.74</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">4.42|<strong>4.15</strong>|50.72|54.78</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">4.50|4.05|43.40|57.25</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">3.74|3.43|35.71|35.72</td>\n  </tr>\n    <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">4.32|4.00|49.37|50.23</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">4.49|3.93|<strong>55.71</strong>|<strong>61.32</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">VoiceBench<br>OpenBookQA | IFEval | AdvBench | Avg</td>\n    <td class=\"tg-0lax\">Ultravox-v0.4.1-LLaMA-3.1-8B</td>\n    <td class=\"tg-0lax\">65.27|<strong>66.88</strong>|98.46|71.45</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MERaLiON</td>\n    <td class=\"tg-0lax\">27.23|62.93|94.81|62.91</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">28.35|25.71|87.69|46.25</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Lyra-Base</td>\n    <td class=\"tg-0lax\">72.75|36.28|59.62|57.66</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">78.02|49.25|97.69|71.69</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">74.51|54.54|97.31|71.14</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">49.45|26.33|96.73|55.35</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">74.73|42.10|98.85|68.81</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>81.10</strong>|52.87|<strong>99.42</strong>|<strong>74.12</strong></td>\n  </tr>\n</tbody></table>\n</details>\n\n<details>\n<summary>Image -> Text</summary>\n\n| Dataset                        | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | \n|--------------------------------|--------------|------------|------------|---------------|-------------|\n| MMMU<sub>val</sub>             | 59.2         | 53.1       | 53.9       | 58.6          | **60.0**    | \n| MMMU-Pro<sub>overall</sub>     | 36.6         | 29.7       | -          | **38.3**      | 37.6        | \n| MathVista<sub>testmini</sub>   | 67.9         | 59.4       | **71.9**   | 68.2          | 52.5        | \n| MathVision<sub>full</sub>      | 25.0         | 20.8       | 23.1       | **25.1**      | -           | \n| MMBench-V1.1-EN<sub>test</sub> | 81.8         | 77.8       | 80.5       | **82.6**      | 76.0        | \n| MMVet<sub>turbo</sub>          | 66.8         | 62.1       | **67.5**   | 67.1          | 66.9        | \n| MMStar                         | **64.0**     | 55.7       | **64.0**   | 63.9          | 54.8        | \n| MME<sub>sum</sub>              | 2340         | 2117       | **2372**   | 2347          | 2003        | \n| MuirBench                      | 59.2         | 48.0       | -          | **59.2**      | -           | \n| CRPE<sub>relation</sub>        | **76.5**     | 73.7       | -          | 76.4          | -           | \n| RealWorldQA<sub>avg</sub>      | 70.3         | 62.6       | **71.9**   | 68.5          | -           | \n| MME-RealWorld<sub>en</sub>     | **61.6**     | 55.6       | -          | 57.4          | -           | \n| MM-MT-Bench                    | 6.0          | 5.0        | -          | **6.3**       | -           | \n| AI2D                           | 83.2         | 79.5       | **85.8**   | 83.9          | -           | \n| TextVQA<sub>val</sub>          | 84.4         | 79.8       | 83.2       | **84.9**      | -           | \n| DocVQA<sub>test</sub>          | 95.2         | 93.3       | 93.5       | **95.7**      | -           | \n| ChartQA<sub>test Avg</sub>     | 85.3         | 82.8       | 84.9       | **87.3**      | -           | \n| OCRBench_V2<sub>en</sub>       | **57.8**     | 51.7       | -          | 56.3          | -           | \n\n\n| Dataset                  | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-VL-7B | Grounding DINO | Gemini 1.5 Pro | \n|--------------------------|--------------|---------------|---------------|----------------|----------------|\n| Refcoco<sub>val</sub>    | 90.5         | 88.7          | 90.0          | **90.6**       | 73.2           | \n| Refcoco<sub>textA</sub>  | **93.5**     | 91.8          | 92.5          | 93.2           | 72.9           | \n| Refcoco<sub>textB</sub>  | 86.6         | 84.0          | 85.4          | **88.2**       | 74.6           | \n| Refcoco+<sub>val</sub>   | 85.4         | 81.1          | 84.2          | **88.2**       | 62.5           | \n| Refcoco+<sub>textA</sub> | **91.0**     | 87.5          | 89.1          | 89.0           | 63.9           | \n| Refcoco+<sub>textB</sub> | **79.3**     | 73.2          | 76.9          | 75.9           | 65.0           | \n| Refcocog+<sub>val</sub>  | **87.4**     | 85.0          | 87.2          | 86.1           | 75.2           | \n| Refcocog+<sub>test</sub> | **87.9**     | 85.1          | 87.2          | 87.0           | 76.2           | \n| ODinW                    | 42.4         | 39.2          | 37.3          | **55.0**       | 36.7           | \n| PointGrounding           | 66.5         | 46.2          | **67.3**      | -              | -              | \n</details>\n\n\n<details>\n<summary>Video(without audio) -> Text</summary>\n\n| Dataset                     | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | \n|-----------------------------|--------------|------------|------------|---------------|-------------|\n| Video-MME<sub>w/o sub</sub> | 64.3         | 62.0       | 63.9       | **65.1**      | 64.8        | \n| Video-MME<sub>w sub</sub>   | **72.4**     | 68.6       | 67.9       | 71.6          | -           | \n| MVBench                     | **70.3**     | 68.7       | 67.2       | 69.6          | -           | \n| EgoSchema<sub>test</sub>    | **68.6**     | 61.4       | 63.2       | 65.0          | -           | \n</details>\n\n<details>\n<summary>Zero-shot Speech Generation</summary>\n\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Content Consistency</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"11\">SEED<br>test-zh | test-en | test-hard </td>\n    <td class=\"tg-0lax\">Seed-TTS_ICL</td>\n    <td class=\"tg-0lax\">1.11 | 2.24 | 7.58</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-TTS_RL</td>\n    <td class=\"tg-0lax\"><strong>1.00</strong> | 1.94 | <strong>6.42</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MaskGCT</td>\n    <td class=\"tg-0lax\">2.27 | 2.62 | 10.27</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">E2_TTS</td>\n    <td class=\"tg-0lax\">1.97 | 2.19 | -</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">F5-TTS</td>\n    <td class=\"tg-0lax\">1.56 | <strong>1.83</strong> | 8.67</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2</td>\n    <td class=\"tg-0lax\">1.45 | 2.57 | 6.83</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2-S</td>\n    <td class=\"tg-0lax\">1.45 | 2.38 | 8.08</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_ICL</td>\n    <td class=\"tg-0lax\">1.95 | 2.87 | 9.92</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_RL</td>\n    <td class=\"tg-0lax\">1.58 | 2.51 | 7.86</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_ICL</td>\n    <td class=\"tg-0lax\">1.70 | 2.72 | 7.97</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_RL</td>\n    <td class=\"tg-0lax\">1.42 | 2.32 | 6.54</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Speaker Similarity</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"11\">SEED<br>test-zh | test-en | test-hard </td>\n    <td class=\"tg-0lax\">Seed-TTS_ICL</td>\n    <td class=\"tg-0lax\">0.796 | 0.762 | 0.776</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-TTS_RL</td>\n    <td class=\"tg-0lax\"><strong>0.801</strong> | <strong>0.766</strong> | <strong>0.782</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MaskGCT</td>\n    <td class=\"tg-0lax\">0.774 | 0.714 | 0.748</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">E2_TTS</td>\n    <td class=\"tg-0lax\">0.730 | 0.710 | -</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">F5-TTS</td>\n    <td class=\"tg-0lax\">0.741 | 0.647 | 0.713</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2</td>\n    <td class=\"tg-0lax\">0.748 | 0.652 | 0.724</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2-S</td>\n    <td class=\"tg-0lax\">0.753 | 0.654 | 0.732</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_ICL</td>\n    <td class=\"tg-0lax\">0.741 | 0.635 | 0.748</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_RL</td>\n    <td class=\"tg-0lax\">0.744 | 0.635 | 0.746</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_ICL</td>\n    <td class=\"tg-0lax\">0.752 | 0.632 | 0.747</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_RL</td>\n    <td class=\"tg-0lax\">0.754 | 0.641 | 0.752</td>\n  </tr>\n</tbody></table>\n</details>\n\n<details>\n<summary>Text -> Text</summary>\n\n| Dataset                           | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-7B | Qwen2.5-3B | Qwen2-7B | Llama3.1-8B | Gemma2-9B | \n|-----------------------------------|-----------|------------|------------|------------|------------|-------------|-----------|\n| MMLU-Pro                          | 47.0      | 40.4       | **56.3**   | 43.7       | 44.1       | 48.3        | 52.1      | \n| MMLU-redux                        | 71.0      | 60.9       | **75.4**   | 64.4       | 67.3       | 67.2        | 72.8      | \n| LiveBench<sub>0831</sub>          | 29.6      | 22.3       | **35.9**   | 26.8       | 29.2       | 26.7        | 30.6      | \n| GPQA                              | 30.8      | 34.3       | **36.4**   | 30.3       | 34.3       | 32.8        | 32.8      | \n| MATH                              | 71.5      | 63.6       | **75.5**   | 65.9       | 52.9       | 51.9        | 44.3      | \n| GSM8K                             | 88.7      | 82.6       | **91.6**   | 86.7       | 85.7       | 84.5        | 76.7      | \n| HumanEval                         | 78.7      | 70.7       | **84.8**   |\t74.4       | 79.9       | 72.6        | 68.9      | \n| MBPP                              | 73.2      | 70.4       | **79.2**   | 72.7       | 67.2       | 69.6        | 74.9      | \n| MultiPL-E                         | 65.8      | 57.6       | **70.4**   | 60.2       | 59.1       | 50.7        | 53.4      | \n| LiveCodeBench<sub>2305-2409</sub> | 24.6      | 16.5       | **28.7**   | 19.9       | 23.9       | 8.3         | 18.9      | \n</details>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-Omni with 🤗 Transformers. The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\npip install accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_omni'\n```\n\n\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has `ffmpeg` installed:\n\n```bash\n# It's highly recommended to use `[decord]` feature for faster video loading.\npip install qwen-omni-utils[decord] -U\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-omni-utils -U` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### 🤗  Transformers Usage\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_omni_utils`:\n\n```python\nimport soundfile as sf\n\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-7B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n\nconversation = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n        ],\n    },\n]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n    \"output.wav\",\n    audio.reshape(-1).detach().cpu().numpy(),\n    samplerate=24000,\n)\n```\n\n<details>\n<summary>Minimum GPU memory requirements</summary>\n\n|Model | Precision | 15(s) Video | 30(s) Video | 60(s) Video |\n|--------------|-----------| ------------- | ------------- | ------------------ |\n| Qwen-Omni-3B | FP32      | 89.10 GB      | Not Recommend | Not Recommend      |\n| Qwen-Omni-3B | BF16      | 18.38 GB      | 22.43 GB      | 28.22 GB           |\n| Qwen-Omni-7B | FP32      | 93.56 GB      | Not Recommend | Not Recommend      |\n| Qwen-Omni-7B | BF16      | 31.11 GB      | 41.85 GB      | 60.19 GB           |\n\nNote: The table above presents the theoretical minimum memory requirements for inference with `transformers` and `BF16` is test with `attn_implementation=\"flash_attention_2\"`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).\n</details>  \n\n<details>\n<summary>Video URL resource usage</summary>\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | ✅  | ✅   |\n| torchvision < 0.19.0  | ❌  | ❌   |\n| decord      | ✅  | ❌   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when `return_audio=False` is set. Here is an example.\n\n```python\n# Sample messages for batch inference\n\n# Conversation with video only\nconversation1 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n        ]\n    }\n]\n\n# Conversation with pure text\nconversation3 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"who are you?\"\n    }\n]\n\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"/path/to/image.jpg\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"text\", \"text\": \"What are the elements can you see and hear in these medias?\"},\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch Inference\ntext_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\n```\n</details>\n\n### Usage Tips\n\n#### Prompt for audio output\nIf users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n```\n{\n    \"role\": \"system\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n    ],\n}\n```\n#### Use audio in video\nIn the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.\n```python\n# first place, in data preprocessing\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=True)\n```\n```python\n# second place, in model processor\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", \n                   padding=True, use_audio_in_video=True)\n```\n```python\n#  third place, in model inference\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=True)\n```\nIt is worth noting that during a multi-round conversation, the `use_audio_in_video` parameter in these places must be set to the same, otherwise unexpected results will occur.\n\n#### Use audio output or not\n\nThe model supports both text and audio outputs, if users do not need audio outputs, they can call `model.disable_talker()` after init the model. This option will save about `~2GB` of GPU memory but the `return_audio` option for `generate` function will only allow to be set at `False`.\n```python\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\nmodel.disable_talker()\n```\n\nIn order to obtain a flexible experience, we recommend that users can decide whether to return audio when `generate` function is called. If `return_audio` is set to `False`, the model will only return text outputs to get text responses faster.\n\n```python\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n...\ntext_ids = model.generate(**inputs, return_audio=False)\n```\n\n#### Change voice type of output audio\nQwen2.5-Omni supports the ability to change the voice of the output audio. The `\"Qwen/Qwen2.5-Omni-7B\"` checkpoint support two voice types as follow:\n\n| Voice Type | Gender | Description |\n|------------|--------|-------------|\n| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity.|\n| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe.|\n\nUsers can use the `speaker` parameter of `generate` function to specify the voice type. By default, if `speaker` is not specified, the default voice type is `Chelsie`.\n\n```python\ntext_ids, audio = model.generate(**inputs, speaker=\"Chelsie\")\n```\n\n```python\ntext_ids, audio = model.generate(**inputs, speaker=\"Ethan\")\n```\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using FlashAttention-2, add `attn_implementation=\"flash_attention_2\"` when loading the model:\n\n```python\nfrom transformers import Qwen2_5OmniForConditionalGeneration\n\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n)\n```\n\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n\n```BibTeX\n\n@article{Qwen2.5-Omni,\n  title={Qwen2.5-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\n  journal={arXiv preprint arXiv:2503.20215},\n  year={2025}\n}\n```\n\n<br>\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Qwen/Qwen2.5-Omni-7B"}]},{"id":"mistralai/Mistral-7B-Instruct-v0.1","name":"Mistral-7B-Instruct-v0.1","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","mistral","text-generation","finetuned","mistral-common","conversational","arxiv:2310.06825","base_model:mistralai/Mistral-7B-v0.1","base_model:finetune:mistralai/Mistral-7B-v0.1","license:apache-2.0","autotrain_compatible","text-generation-inference","region:us"],"likes":1806,"downloads":564713,"readme":"---\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- finetuned\n- mistral-common\nbase_model: mistralai/Mistral-7B-v0.1\ninference: false\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-Instruct-v0.1\n\n\n## Encode and Decode with `mistral_common`\n            \n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the `transformers` tokenizer so that it gives 1-to-1 the same results as the `mistral_common` reference implementation are very welcome!\n            \n---\n\nThe Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Model Architecture\nThis instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## Troubleshooting\n- If you see the following error:\n```\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\n```\n\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\n\nThis should not be required after transformers-v4.33.4.\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1"}]},{"id":"HuggingFaceH4/zephyr-7b-beta","name":"zephyr-7b-beta","description":"A model for text-generation.","task":"text-generation","tags":["transformers","pytorch","safetensors","mistral","text-generation","generated_from_trainer","conversational","en","dataset:HuggingFaceH4/ultrachat_200k","dataset:HuggingFaceH4/ultrafeedback_binarized","arxiv:2305.18290","arxiv:2310.16944","arxiv:2305.14233","arxiv:2310.01377","base_model:mistralai/Mistral-7B-v0.1","base_model:finetune:mistralai/Mistral-7B-v0.1","license:mit","model-index","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1806,"downloads":251327,"readme":"---\ntags:\n- generated_from_trainer\nlicense: mit\ndatasets:\n- HuggingFaceH4/ultrachat_200k\n- HuggingFaceH4/ultrafeedback_binarized\nlanguage:\n- en\nbase_model: mistralai/Mistral-7B-v0.1\nwidget:\n  - example_title: Pirate!\n    messages:\n      - role: system\n        content: You are a pirate chatbot who always responds with Arr!\n      - role: user\n        content: \"There's a llama on my lawn, how can I get rid of him?\"\n    output:\n      text: >-\n        Arr! 'Tis a puzzlin' matter, me hearty! A llama on yer lawn be a rare\n        sight, but I've got a plan that might help ye get rid of 'im. Ye'll need\n        to gather some carrots and hay, and then lure the llama away with the\n        promise of a tasty treat. Once he's gone, ye can clean up yer lawn and\n        enjoy the peace and quiet once again. But beware, me hearty, for there\n        may be more llamas where that one came from! Arr!\npipeline_tag: text-generation\nmodel-index:\n- name: zephyr-7b-beta\n  results:\n  # AI2 Reasoning Challenge (25-Shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: AI2 Reasoning Challenge (25-Shot)\n      type: ai2_arc\n      config: ARC-Challenge\n      split: test\n      args:\n        num_few_shot: 25\n    metrics:\n       - type: acc_norm\n         name: normalized accuracy\n         value: 62.03071672354948\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # HellaSwag (10-shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: HellaSwag (10-Shot)\n      type: hellaswag\n      split: validation\n      args:\n        num_few_shot: 10\n    metrics:\n       - type: acc_norm\n         name: normalized accuracy\n         value: 84.35570603465445\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # DROP (3-shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: Drop (3-Shot)\n      type: drop\n      split: validation\n      args:\n        num_few_shot: 3\n    metrics:\n       - type: f1\n         name: f1 score\n         value: 9.662437080536909\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # TruthfulQA (0-shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: TruthfulQA (0-shot)\n      type: truthful_qa\n      config: multiple_choice\n      split: validation\n      args:\n        num_few_shot: 0\n    metrics:\n       - type: mc2\n         value: 57.44916942762855\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # GSM8k (5-shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GSM8k (5-shot)\n      type: gsm8k\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n       - type: acc\n         name: accuracy\n         value: 12.736921910538287\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # MMLU (5-Shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU (5-Shot)\n      type: cais/mmlu\n      config: all\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n       - type: acc\n         name: accuracy\n         value: 61.07\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # Winogrande (5-shot)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: Winogrande (5-shot)\n      type: winogrande\n      config: winogrande_xl\n      split: validation\n      args:\n        num_few_shot: 5\n    metrics:\n       - type: acc\n         name: accuracy\n         value: 77.74269928966061\n    source:\n      name: Open LLM Leaderboard\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=HuggingFaceH4/zephyr-7b-beta\n\n  # AlpacaEval (taken from model card)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: AlpacaEval\n      type: tatsu-lab/alpaca_eval\n    metrics:\n       - type: unknown\n         name: win rate\n         value: 0.9060\n    source:\n      url: https://tatsu-lab.github.io/alpaca_eval/\n\n  # MT-Bench (taken from model card)\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MT-Bench\n      type: unknown\n    metrics:\n       - type: unknown\n         name: score\n         value: 7.34\n    source:\n      url: https://huggingface.co/spaces/lmsys/mt-bench\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zephyr Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n\n# Model Card for Zephyr 7B β\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-β is the second model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so. You can find more details in the [technical report](https://arxiv.org/abs/2310.16944).\n\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n- **Chatbot Arena:** Evaluate Zephyr 7B against 10+ LLMs in the LMSYS arena: http://arena.lmsys.org\n\n## Performance\n\nAt the time of release, Zephyr-7B-β is the highest ranked 7B chat model on the [MT-Bench](https://huggingface.co/spaces/lmsys/mt-bench) and [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmarks:\n\n| Model | Size | Alignment | MT-Bench (score) | AlpacaEval (win rate %) |\n|-------------|-----|----|---------------|--------------|\n| StableLM-Tuned-α | 7B| dSFT |2.75| -|\n| MPT-Chat |  7B |dSFT |5.42| -|\n| Xwin-LMv0.1 | 7B| dPPO| 6.19| 87.83|\n| Mistral-Instructv0.1 | 7B|  - | 6.84 |-|\n| Zephyr-7b-α |7B|  dDPO| 6.88| -|\n| **Zephyr-7b-β** 🪁 | **7B** | **dDPO** | **7.34** | **90.60** |\n| Falcon-Instruct |  40B |dSFT |5.17 |45.71|\n| Guanaco | 65B |  SFT |6.41| 71.80|\n| Llama2-Chat |  70B |RLHF |6.86| 92.66|\n| Vicuna v1.3 |  33B |dSFT |7.12 |88.99|\n| WizardLM v1.0 |  70B |dSFT |7.71 |-|\n| Xwin-LM v0.1 |   70B |dPPO |- |95.57|\n| GPT-3.5-turbo | - |RLHF |7.94 |89.37|\n| Claude 2 |  - |RLHF |8.06| 91.36|\n| GPT-4 |  -| RLHF |8.99| 95.28|\n\nIn particular, on several categories of MT-Bench, Zephyr-7B-β has strong performance compared to larger open models like Llama2-Chat-70B:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/raxvt5ma16d7T23my34WC.png)\n\nHowever, on more complex tasks like coding and mathematics, Zephyr-7B-β lags behind proprietary models and more research is needed to close the gap.\n\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a filtered and preprocessed of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. \nWe then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contains 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities. \n\nYou can find the datasets used for training Zephyr-7B-β [here](https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66)\n\nHere's how you can run the model using the `pipeline()` function from 🤗 Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-β has not been aligned to human preferences for safety within the RLHF phase or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). \nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n\n## Training and evaluation data\n\nDuring DPO training, this model achieves the following results on the evaluation set:\n\n- Loss: 0.7496\n- Rewards/chosen: -4.5221\n- Rewards/rejected: -8.3184\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 3.7963\n- Logps/rejected: -340.1541\n- Logps/chosen: -299.4561\n- Logits/rejected: -2.3081\n- Logits/chosen: -2.3531\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3.0\n\n### Training results\n\nThe table below shows the full set of DPO training metrics:\n\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.6284        | 0.05  | 100  | 0.6098          | 0.0425         | -0.1872          | 0.7344             | 0.2297          | -258.8416      | -253.8099    | -2.7976         | -2.8234       |\n| 0.4908        | 0.1   | 200  | 0.5426          | -0.0279        | -0.6842          | 0.75               | 0.6563          | -263.8124      | -254.5145    | -2.7719         | -2.7960       |\n| 0.5264        | 0.15  | 300  | 0.5324          | 0.0414         | -0.9793          | 0.7656             | 1.0207          | -266.7627      | -253.8209    | -2.7892         | -2.8122       |\n| 0.5536        | 0.21  | 400  | 0.4957          | -0.0185        | -1.5276          | 0.7969             | 1.5091          | -272.2460      | -254.4203    | -2.8542         | -2.8764       |\n| 0.5362        | 0.26  | 500  | 0.5031          | -0.2630        | -1.5917          | 0.7812             | 1.3287          | -272.8869      | -256.8653    | -2.8702         | -2.8958       |\n| 0.5966        | 0.31  | 600  | 0.5963          | -0.2993        | -1.6491          | 0.7812             | 1.3499          | -273.4614      | -257.2279    | -2.8778         | -2.8986       |\n| 0.5014        | 0.36  | 700  | 0.5382          | -0.2859        | -1.4750          | 0.75               | 1.1891          | -271.7204      | -257.0942    | -2.7659         | -2.7869       |\n| 0.5334        | 0.41  | 800  | 0.5677          | -0.4289        | -1.8968          | 0.7969             | 1.4679          | -275.9378      | -258.5242    | -2.7053         | -2.7265       |\n| 0.5251        | 0.46  | 900  | 0.5772          | -0.2116        | -1.3107          | 0.7344             | 1.0991          | -270.0768      | -256.3507    | -2.8463         | -2.8662       |\n| 0.5205        | 0.52  | 1000 | 0.5262          | -0.3792        | -1.8585          | 0.7188             | 1.4793          | -275.5552      | -258.0276    | -2.7893         | -2.7979       |\n| 0.5094        | 0.57  | 1100 | 0.5433          | -0.6279        | -1.9368          | 0.7969             | 1.3089          | -276.3377      | -260.5136    | -2.7453         | -2.7536       |\n| 0.5837        | 0.62  | 1200 | 0.5349          | -0.3780        | -1.9584          | 0.7656             | 1.5804          | -276.5542      | -258.0154    | -2.7643         | -2.7756       |\n| 0.5214        | 0.67  | 1300 | 0.5732          | -1.0055        | -2.2306          | 0.7656             | 1.2251          | -279.2761      | -264.2903    | -2.6986         | -2.7113       |\n| 0.6914        | 0.72  | 1400 | 0.5137          | -0.6912        | -2.1775          | 0.7969             | 1.4863          | -278.7448      | -261.1467    | -2.7166         | -2.7275       |\n| 0.4655        | 0.77  | 1500 | 0.5090          | -0.7987        | -2.2930          | 0.7031             | 1.4943          | -279.8999      | -262.2220    | -2.6651         | -2.6838       |\n| 0.5731        | 0.83  | 1600 | 0.5312          | -0.8253        | -2.3520          | 0.7812             | 1.5268          | -280.4902      | -262.4876    | -2.6543         | -2.6728       |\n| 0.5233        | 0.88  | 1700 | 0.5206          | -0.4573        | -2.0951          | 0.7812             | 1.6377          | -277.9205      | -258.8084    | -2.6870         | -2.7097       |\n| 0.5593        | 0.93  | 1800 | 0.5231          | -0.5508        | -2.2000          | 0.7969             | 1.6492          | -278.9703      | -259.7433    | -2.6221         | -2.6519       |\n| 0.4967        | 0.98  | 1900 | 0.5290          | -0.5340        | -1.9570          | 0.8281             | 1.4230          | -276.5395      | -259.5749    | -2.6564         | -2.6878       |\n| 0.0921        | 1.03  | 2000 | 0.5368          | -1.1376        | -3.1615          | 0.7812             | 2.0239          | -288.5854      | -265.6111    | -2.6040         | -2.6345       |\n| 0.0733        | 1.08  | 2100 | 0.5453          | -1.1045        | -3.4451          | 0.7656             | 2.3406          | -291.4208      | -265.2799    | -2.6289         | -2.6595       |\n| 0.0972        | 1.14  | 2200 | 0.5571          | -1.6915        | -3.9823          | 0.8125             | 2.2908          | -296.7934      | -271.1505    | -2.6471         | -2.6709       |\n| 0.1058        | 1.19  | 2300 | 0.5789          | -1.0621        | -3.8941          | 0.7969             | 2.8319          | -295.9106      | -264.8563    | -2.5527         | -2.5798       |\n| 0.2423        | 1.24  | 2400 | 0.5455          | -1.1963        | -3.5590          | 0.7812             | 2.3627          | -292.5599      | -266.1981    | -2.5414         | -2.5784       |\n| 0.1177        | 1.29  | 2500 | 0.5889          | -1.8141        | -4.3942          | 0.7969             | 2.5801          | -300.9120      | -272.3761    | -2.4802         | -2.5189       |\n| 0.1213        | 1.34  | 2600 | 0.5683          | -1.4608        | -3.8420          | 0.8125             | 2.3812          | -295.3901      | -268.8436    | -2.4774         | -2.5207       |\n| 0.0889        | 1.39  | 2700 | 0.5890          | -1.6007        | -3.7337          | 0.7812             | 2.1330          | -294.3068      | -270.2423    | -2.4123         | -2.4522       |\n| 0.0995        | 1.45  | 2800 | 0.6073          | -1.5519        | -3.8362          | 0.8281             | 2.2843          | -295.3315      | -269.7538    | -2.4685         | -2.5050       |\n| 0.1145        | 1.5   | 2900 | 0.5790          | -1.7939        | -4.2876          | 0.8438             | 2.4937          | -299.8461      | -272.1744    | -2.4272         | -2.4674       |\n| 0.0644        | 1.55  | 3000 | 0.5735          | -1.7285        | -4.2051          | 0.8125             | 2.4766          | -299.0209      | -271.5201    | -2.4193         | -2.4574       |\n| 0.0798        | 1.6   | 3100 | 0.5537          | -1.7226        | -4.2850          | 0.8438             | 2.5624          | -299.8200      | -271.4610    | -2.5367         | -2.5696       |\n| 0.1013        | 1.65  | 3200 | 0.5575          | -1.5715        | -3.9813          | 0.875              | 2.4098          | -296.7825      | -269.9498    | -2.4926         | -2.5267       |\n| 0.1254        | 1.7   | 3300 | 0.5905          | -1.6412        | -4.4703          | 0.8594             | 2.8291          | -301.6730      | -270.6473    | -2.5017         | -2.5340       |\n| 0.085         | 1.76  | 3400 | 0.6133          | -1.9159        | -4.6760          | 0.8438             | 2.7601          | -303.7296      | -273.3941    | -2.4614         | -2.4960       |\n| 0.065         | 1.81  | 3500 | 0.6074          | -1.8237        | -4.3525          | 0.8594             | 2.5288          | -300.4951      | -272.4724    | -2.4597         | -2.5004       |\n| 0.0755        | 1.86  | 3600 | 0.5836          | -1.9252        | -4.4005          | 0.8125             | 2.4753          | -300.9748      | -273.4872    | -2.4327         | -2.4716       |\n| 0.0746        | 1.91  | 3700 | 0.5789          | -1.9280        | -4.4906          | 0.8125             | 2.5626          | -301.8762      | -273.5149    | -2.4686         | -2.5115       |\n| 0.1348        | 1.96  | 3800 | 0.6015          | -1.8658        | -4.2428          | 0.8281             | 2.3769          | -299.3976      | -272.8936    | -2.4943         | -2.5393       |\n| 0.0217        | 2.01  | 3900 | 0.6122          | -2.3335        | -4.9229          | 0.8281             | 2.5894          | -306.1988      | -277.5699    | -2.4841         | -2.5272       |\n| 0.0219        | 2.07  | 4000 | 0.6522          | -2.9890        | -6.0164          | 0.8281             | 3.0274          | -317.1334      | -284.1248    | -2.4105         | -2.4545       |\n| 0.0119        | 2.12  | 4100 | 0.6922          | -3.4777        | -6.6749          | 0.7969             | 3.1972          | -323.7187      | -289.0121    | -2.4272         | -2.4699       |\n| 0.0153        | 2.17  | 4200 | 0.6993          | -3.2406        | -6.6775          | 0.7969             | 3.4369          | -323.7453      | -286.6413    | -2.4047         | -2.4465       |\n| 0.011         | 2.22  | 4300 | 0.7178          | -3.7991        | -7.4397          | 0.7656             | 3.6406          | -331.3667      | -292.2260    | -2.3843         | -2.4290       |\n| 0.0072        | 2.27  | 4400 | 0.6840          | -3.3269        | -6.8021          | 0.8125             | 3.4752          | -324.9908      | -287.5042    | -2.4095         | -2.4536       |\n| 0.0197        | 2.32  | 4500 | 0.7013          | -3.6890        | -7.3014          | 0.8125             | 3.6124          | -329.9841      | -291.1250    | -2.4118         | -2.4543       |\n| 0.0182        | 2.37  | 4600 | 0.7476          | -3.8994        | -7.5366          | 0.8281             | 3.6372          | -332.3356      | -293.2291    | -2.4163         | -2.4565       |\n| 0.0125        | 2.43  | 4700 | 0.7199          | -4.0560        | -7.5765          | 0.8438             | 3.5204          | -332.7345      | -294.7952    | -2.3699         | -2.4100       |\n| 0.0082        | 2.48  | 4800 | 0.7048          | -3.6613        | -7.1356          | 0.875              | 3.4743          | -328.3255      | -290.8477    | -2.3925         | -2.4303       |\n| 0.0118        | 2.53  | 4900 | 0.6976          | -3.7908        | -7.3152          | 0.8125             | 3.5244          | -330.1224      | -292.1431    | -2.3633         | -2.4047       |\n| 0.0118        | 2.58  | 5000 | 0.7198          | -3.9049        | -7.5557          | 0.8281             | 3.6508          | -332.5271      | -293.2844    | -2.3764         | -2.4194       |\n| 0.006         | 2.63  | 5100 | 0.7506          | -4.2118        | -7.9149          | 0.8125             | 3.7032          | -336.1194      | -296.3530    | -2.3407         | -2.3860       |\n| 0.0143        | 2.68  | 5200 | 0.7408          | -4.2433        | -7.9802          | 0.8125             | 3.7369          | -336.7721      | -296.6682    | -2.3509         | -2.3946       |\n| 0.0057        | 2.74  | 5300 | 0.7552          | -4.3392        | -8.0831          | 0.7969             | 3.7439          | -337.8013      | -297.6275    | -2.3388         | -2.3842       |\n| 0.0138        | 2.79  | 5400 | 0.7404          | -4.2395        | -7.9762          | 0.8125             | 3.7367          | -336.7322      | -296.6304    | -2.3286         | -2.3737       |\n| 0.0079        | 2.84  | 5500 | 0.7525          | -4.4466        | -8.2196          | 0.7812             | 3.7731          | -339.1662      | -298.7007    | -2.3200         | -2.3641       |\n| 0.0077        | 2.89  | 5600 | 0.7520          | -4.5586        | -8.3485          | 0.7969             | 3.7899          | -340.4545      | -299.8206    | -2.3078         | -2.3517       |\n| 0.0094        | 2.94  | 5700 | 0.7527          | -4.5542        | -8.3509          | 0.7812             | 3.7967          | -340.4790      | -299.7773    | -2.3062         | -2.3510       |\n| 0.0054        | 2.99  | 5800 | 0.7520          | -4.5169        | -8.3079          | 0.7812             | 3.7911          | -340.0493      | -299.4038    | -2.3081         | -2.3530       |\n\n\n### Framework versions\n\n- Transformers 4.35.0.dev0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0\n\n## Citation\n\nIf you find Zephyr-7B-β is useful in your work, please cite it with:\n\n```\n@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\nIf you use the UltraChat or UltraFeedback datasets, please cite the original works:\n\n```\n@misc{ding2023enhancing,\n      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, \n      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\n      year={2023},\n      eprint={2305.14233},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{cui2023ultrafeedback,\n      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, \n      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},\n      year={2023},\n      eprint={2310.01377},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_HuggingFaceH4__zephyr-7b-beta)\n\n| Metric                | Value                     |\n|-----------------------|---------------------------|\n| Avg.                  | 52.15   |\n| ARC (25-shot)         | 62.03          |\n| HellaSwag (10-shot)   | 84.36    |\n| MMLU (5-shot)         | 61.07         |\n| TruthfulQA (0-shot)   | 57.45   |\n| Winogrande (5-shot)   | 77.74   |\n| GSM8K (5-shot)        | 12.74        |\n| DROP (3-shot)         | 9.66         |","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta"}]},{"id":"meta-llama/Llama-3.2-3B-Instruct","name":"Llama-3.2-3B-Instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","arxiv:2405.16406","license:llama3.2","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1795,"downloads":1803532,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"}]},{"id":"h94/IP-Adapter-FaceID","name":"IP-Adapter-FaceID","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","text-to-image","stable-diffusion","en","arxiv:2308.06721","region:us"],"likes":1776,"downloads":242156,"readme":"---\ntags:\n- text-to-image\n- stable-diffusion\n\nlanguage:\n- en\nlibrary_name: diffusers\n---\n\n# IP-Adapter-FaceID Model Card\n\n\n<div align=\"center\">\n\n[**Project Page**](https://ip-adapter.github.io) **|** [**Paper (ArXiv)**](https://arxiv.org/abs/2308.06721) **|** [**Code**](https://github.com/tencent-ailab/IP-Adapter)\n</div>\n\n---\n\n\n\n## Introduction\n\nAn experimental version of IP-Adapter-FaceID: we use face ID embedding from a face recognition model instead of CLIP image embedding, additionally, we use LoRA to improve ID consistency. IP-Adapter-FaceID can generate various style images conditioned on a face with only text prompts. \n\n![results](./ip-adapter-faceid.jpg)\n\n\n**Update 2023/12/27**: \n\nIP-Adapter-FaceID-Plus: face ID embedding (for face ID) + CLIP image embedding (for face structure)\n\n<div  align=\"center\">    \n\n![results](./faceid-plus.jpg)\n</div>\n\n**Update 2023/12/28**: \n\nIP-Adapter-FaceID-PlusV2: face ID embedding (for face ID) + controllable CLIP image embedding (for face structure)\n\nYou can adjust the weight of the face structure to get different generation!\n\n<div  align=\"center\">    \n\n![results](./faceid_plusv2.jpg)\n</div>\n\n**Update 2024/01/04**: \n\nIP-Adapter-FaceID-SDXL: An experimental SDXL version of IP-Adapter-FaceID\n\n<div  align=\"center\">    \n\n![results](./sdxl_faceid.jpg)\n</div>\n\n**Update 2024/01/17**: \n\nIP-Adapter-FaceID-PlusV2-SDXL: An experimental SDXL version of IP-Adapter-FaceID-PlusV2\n\n\n**Update 2024/01/19**: \n\nIP-Adapter-FaceID-Portrait: same with IP-Adapter-FaceID but for portrait generation (no lora! no controlnet!). Specifically, it accepts multiple facial images to enhance similarity (the default is 5).\n\n<div  align=\"center\">\n\n![results](./faceid_portrait_sd15.jpg)\n</div>\n\n\n## Usage\n\n### IP-Adapter-FaceID\n\nFirstly, you should use [insightface](https://github.com/deepinsight/insightface) to extract face ID embedding:\n\n```python\n\nimport cv2\nfrom insightface.app import FaceAnalysis\nimport torch\n\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\nimage = cv2.imread(\"person.jpg\")\nfaces = app.get(image)\n\nfaceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n```\n\nThen, you can generate images conditioned on the face embeddings:\n\n```python\n\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\nfrom PIL import Image\n\nfrom ip_adapter.ip_adapter_faceid import IPAdapterFaceID\n\nbase_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\nvae_model_path = \"stabilityai/sd-vae-ft-mse\"\nip_ckpt = \"ip-adapter-faceid_sd15.bin\"\ndevice = \"cuda\"\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\nvae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    scheduler=noise_scheduler,\n    vae=vae,\n    feature_extractor=None,\n    safety_checker=None\n)\n\n# load ip-adapter\nip_model = IPAdapterFaceID(pipe, ip_ckpt, device)\n\n# generate image\nprompt = \"photo of a woman in red dress in a garden\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\nimages = ip_model.generate(\n    prompt=prompt, negative_prompt=negative_prompt, faceid_embeds=faceid_embeds, num_samples=4, width=512, height=768, num_inference_steps=30, seed=2023\n)\n\n```\n\nyou can also use a normal IP-Adapter and a normal LoRA to load model:\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\nfrom PIL import Image\n\nfrom ip_adapter.ip_adapter_faceid_separate import IPAdapterFaceID\n\nbase_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\nvae_model_path = \"stabilityai/sd-vae-ft-mse\"\nip_ckpt = \"ip-adapter-faceid_sd15.bin\"\nlora_ckpt = \"ip-adapter-faceid_sd15_lora.safetensors\"\ndevice = \"cuda\"\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\nvae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    scheduler=noise_scheduler,\n    vae=vae,\n    feature_extractor=None,\n    safety_checker=None\n)\n\n# load lora and fuse\npipe.load_lora_weights(lora_ckpt)\npipe.fuse_lora()\n\n# load ip-adapter\nip_model = IPAdapterFaceID(pipe, ip_ckpt, device)\n\n# generate image\nprompt = \"photo of a woman in red dress in a garden\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\nimages = ip_model.generate(\n    prompt=prompt, negative_prompt=negative_prompt, faceid_embeds=faceid_embeds, num_samples=4, width=512, height=768, num_inference_steps=30, seed=2023\n)\n\n\n```\n\n### IP-Adapter-FaceID-SDXL\n\nFirstly, you should use [insightface](https://github.com/deepinsight/insightface) to extract face ID embedding:\n\n```python\n\nimport cv2\nfrom insightface.app import FaceAnalysis\nimport torch\n\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\nimage = cv2.imread(\"person.jpg\")\nfaces = app.get(image)\n\nfaceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n```\n\nThen, you can generate images conditioned on the face embeddings:\n\n```python\n\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, DDIMScheduler\nfrom PIL import Image\n\nfrom ip_adapter.ip_adapter_faceid import IPAdapterFaceIDXL\n\nbase_model_path = \"SG161222/RealVisXL_V3.0\"\nip_ckpt = \"ip-adapter-faceid_sdxl.bin\"\ndevice = \"cuda\"\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\npipe = StableDiffusionXLPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    scheduler=noise_scheduler,\n    add_watermarker=False,\n)\n\n# load ip-adapter\nip_model = IPAdapterFaceIDXL(pipe, ip_ckpt, device)\n\n# generate image\nprompt = \"A closeup shot of a beautiful Asian teenage girl in a white dress wearing small silver earrings in the garden, under the soft morning light\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\nimages = ip_model.generate(\n    prompt=prompt, negative_prompt=negative_prompt, faceid_embeds=faceid_embeds, num_samples=2,\n    width=1024, height=1024,\n    num_inference_steps=30, guidance_scale=7.5, seed=2023\n)\n\n```\n\n\n### IP-Adapter-FaceID-Plus\n\nFirstly, you should use [insightface](https://github.com/deepinsight/insightface) to extract face ID embedding and face image:\n\n```python\n\nimport cv2\nfrom insightface.app import FaceAnalysis\nfrom insightface.utils import face_align\nimport torch\n\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\nimage = cv2.imread(\"person.jpg\")\nfaces = app.get(image)\n\nfaceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\nface_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224) # you can also segment the face\n```\n\nThen, you can generate images conditioned on the face embeddings:\n\n```python\n\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\nfrom PIL import Image\n\nfrom ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n\nv2 = False\nbase_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\nvae_model_path = \"stabilityai/sd-vae-ft-mse\"\nimage_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\nip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\ndevice = \"cuda\"\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\nvae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    scheduler=noise_scheduler,\n    vae=vae,\n    feature_extractor=None,\n    safety_checker=None\n)\n\n# load ip-adapter\nip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n\n# generate image\nprompt = \"photo of a woman in red dress in a garden\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\nimages = ip_model.generate(\n     prompt=prompt, negative_prompt=negative_prompt, face_image=face_image, faceid_embeds=faceid_embeds, shortcut=v2, s_scale=1.0,\n     num_samples=4, width=512, height=768, num_inference_steps=30, seed=2023\n)\n\n```\n\n### IP-Adapter-FaceID-Portrait\n\n```python\n\nimport cv2\nfrom insightface.app import FaceAnalysis\nimport torch\n\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n\nimages = [\"1.jpg\", \"2.jpg\", \"3.jpg\", \"4.jpg\", \"5.jpg\"]\n\nfaceid_embeds = []\nfor image in images:\n    image = cv2.imread(\"person.jpg\")\n    faces = app.get(image)\n    faceid_embeds.append(torch.from_numpy(faces[0].normed_embedding).unsqueeze(0).unsqueeze(0))\n  faceid_embeds = torch.cat(faceid_embeds, dim=1)\n```\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\nfrom PIL import Image\n\nfrom ip_adapter.ip_adapter_faceid_separate import IPAdapterFaceID\n\nbase_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\nvae_model_path = \"stabilityai/sd-vae-ft-mse\"\nip_ckpt = \"ip-adapter-faceid-portrait_sd15.bin\"\ndevice = \"cuda\"\n\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)\nvae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\npipe = StableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    scheduler=noise_scheduler,\n    vae=vae,\n    feature_extractor=None,\n    safety_checker=None\n)\n\n\n# load ip-adapter\nip_model = IPAdapterFaceID(pipe, ip_ckpt, device, num_tokens=16, n_cond=5)\n\n# generate image\nprompt = \"photo of a woman in red dress in a garden\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\nimages = ip_model.generate(\n    prompt=prompt, negative_prompt=negative_prompt, faceid_embeds=faceid_embeds, num_samples=4, width=512, height=512, num_inference_steps=30, seed=2023\n)\n\n\n```\n\n\n\n## Limitations and Bias\n- The models do not achieve perfect photorealism and ID consistency.\n- The generalization of the models is limited due to limitations of the training data, base model and face recognition model.\n\n\n## Non-commercial use\n**AS InsightFace pretrained models are available for non-commercial research purposes, IP-Adapter-FaceID models are released exclusively for research purposes and is not intended for commercial use.**\n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/h94/IP-Adapter-FaceID"}]},{"id":"openai/whisper-large-v2","name":"whisper-large-v2","description":"A model for automatic-speech-recognition.","task":"automatic-speech-recognition","tags":["transformers","pytorch","tf","jax","safetensors","whisper","automatic-speech-recognition","audio","hf-asr-leaderboard","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","license:apache-2.0","endpoints_compatible","region:us"],"likes":1773,"downloads":328759,"readme":"---\nlanguage: \n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- no\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours \nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains **without** the need \nfor fine-tuning.\n\nWhisper was proposed in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) \nby Alec Radford et al. from OpenAI. The original code repository can be found [here](https://github.com/openai/whisper).\n\nCompared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization \nfor improved performance.\n\n**Disclaimer**: Content for this model card has partly been written by the Hugging Face team, and parts of it were \ncopied and pasted from the original model card.\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. \nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. \n\nThe models were trained on either English-only data or multilingual data. The English-only models were trained \non the task of speech recognition. The multilingual models were trained on both speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. \nFor speech translation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v2) |\n\n# Usage\n\nTo transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the \"task token\". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail intéressant va enfin être mené sur ce sujet.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail intéressant va enfin être mené sur ce sujet.']\n```\n\n## Translation \nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Large on [LibriSpeech test-clean](https://huggingface.co/datasets/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\").to(\"cuda\")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n3.0003583080317572\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-large-v2\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n  'timestamp': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https://huggingface.co/blog/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models’ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box – their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/openai/whisper-large-v2"}]},{"id":"mistralai/Mixtral-8x7B-v0.1","name":"Mixtral-8x7B-v0.1","description":"A model for various tasks.","task":"N/A","tags":["vllm","safetensors","mixtral","moe","mistral-common","fr","it","de","es","en","license:apache-2.0","region:us"],"likes":1768,"downloads":79368,"readme":"---\nlibrary_name: vllm\nlicense: apache-2.0\nlanguage:\n- fr\n- it\n- de\n- es\n- en\ntags:\n- moe\n- mistral-common\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/fr/terms/\">Privacy Policy</a>.\n---\n# Model Card for Mixtral-8x7B\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\n\nFor full details of this model please read our [release blog post](https://mistral.ai/news/mixtral-of-experts/).\n\n## Warning\nThis repo contains weights that are compatible with [vLLM](https://github.com/vllm-project/vllm) serving of the model as well as Hugging Face [transformers](https://github.com/huggingface/transformers) library. It is based on the original Mixtral [torrent release](magnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&dn=mixtral-8x7b-32kseqlen&tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce), but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.\n\n## Run the model\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\n\n### In half-precision\n\nNote `float16` precision only works on GPU devices\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Lower precision using (8-bit & 4-bit) using `bitsandbytes`\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Load the model with Flash Attention 2\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n## Notice\nMixtral-8x7B is a pretrained base model and therefore does not have any moderation mechanisms.\n\n# The Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"}]},{"id":"CohereLabs/c4ai-command-r-plus","name":"c4ai-command-r-plus","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","cohere","text-generation","conversational","en","fr","de","es","it","pt","ja","ko","zh","ar","doi:10.57967/hf/3138","license:cc-by-nc-4.0","autotrain_compatible","text-generation-inference","region:us"],"likes":1759,"downloads":2728,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/CohereLabs/c4ai-command-r-plus"}]},{"id":"Qwen/QwQ-32B-Preview","name":"QwQ-32B-Preview","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2407.10671","base_model:Qwen/Qwen2.5-32B-Instruct","base_model:finetune:Qwen/Qwen2.5-32B-Instruct","license:apache-2.0","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1740,"downloads":12028,"readme":"---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/QwQ-32B-Preview/blob/main/LICENSE\nlanguage:\n- en\nbase_model: Qwen/Qwen2.5-32B-Instruct\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# QwQ-32B-Preview\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\n**QwQ-32B-Preview** is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations:\n\n1. **Language Mixing and Code-Switching**: The model may mix languages or switch between them unexpectedly, affecting response clarity.\n2. **Recursive Reasoning Loops**: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.\n3. **Safety and Ethical Considerations**: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.\n4. **Performance and Benchmark Limitations**: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.\n\n**Specification**:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 32.5B\n- Number of Paramaters (Non-Embedding): 31.0B\n- Number of Layers: 64\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n- Context Length: Full 32,768 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwq-32b-preview/). You can also check Qwen2.5 [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/QwQ-32B-Preview\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"How many r in strawberry.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwq-32b-preview,\n    title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},\n    url = {https://qwenlm.github.io/blog/qwq-32b-preview/},\n    author = {Qwen Team},\n    month = {November},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Qwen/QwQ-32B-Preview"}]},{"id":"dreamlike-art/dreamlike-photoreal-2.0","name":"dreamlike-photoreal-2.0","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","photorealistic","photoreal","en","license:other","autotrain_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":1719,"downloads":8454,"readme":"---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- photorealistic\n- photoreal\n- diffusers\ninference: false\n---\n\n# Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by [dreamlike.art](https://dreamlike.art/).  \n  \n# If you want to use dreamlike models on your website/app/etc., check the license at the bottom first!  \n\nWarning: This model is horny! Add \"nude, naked\" to the negative prompt if want to avoid NSFW.  \n  \nYou can add **photo** to your prompt to make your gens look more photorealistic.   \nNon-square aspect ratios work better for some prompts. If you want a portrait photo, try using a vertical aspect ratio. If you want a landscape photo, try using a horizontal aspect ratio.  \nThis model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px.  \n\n### Examples\n\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/preview1.jpg\" style=\"max-width: 800px;\" width=\"100%\"/>\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/preview2.jpg\" style=\"max-width: 800px;\" width=\"100%\"/>\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/preview3.jpg\" style=\"max-width: 800px;\" width=\"100%\"/>\n\n### dreamlike.art\n\nYou can use this model for free on [dreamlike.art](https://dreamlike.art/)!\n\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/dreamlike.jpg\" style=\"max-width: 1000px;\" width=\"100%\"/>\n\n### CKPT\n\n[Download dreamlike-photoreal-2.0.ckpt (2.13GB)](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/dreamlike-photoreal-2.0.ckpt)\n\n### Safetensors\n[Download dreamlike-photoreal-2.0.safetensors (2.13GB)](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/dreamlike-photoreal-2.0.safetensors)\n\n### 🧨 Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion Pipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"./result.jpg\")\n```\n\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/church.jpg\" style=\"max-width: 640px;\" width=\"100%\"/>\n\n# License\n\nThis model is licesed under a **modified** CreativeML OpenRAIL-M license.\n\n- **You are not allowed to host, finetune, or do inference with the model or its derivatives on websites/apps/etc. If you want to, please email us at contact@dreamlike.art**\n- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Photoreal 2.0) and include the license as well as a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)**  \n- **You are free to use the outputs (images) of the model for commercial purposes in teams of 10 or less**\n- You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/LICENSE.md\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0"}]},{"id":"mattshumer/Reflection-Llama-3.1-70B","name":"Reflection-Llama-3.1-70B","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","llama","text-generation","conversational","base_model:meta-llama/Llama-3.1-70B-Instruct","base_model:finetune:meta-llama/Llama-3.1-70B-Instruct","license:llama3.1","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1710,"downloads":97,"readme":"---\nlicense: llama3.1\nbase_model: meta-llama/Meta-Llama-3.1-70B-Instruct\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n# Reflection Llama-3.1 70B\n\n| IMPORTANT UPDATE – There was an issue with the model when we first uploaded it. If you tried it and didn't have good results, please, try again, we think we've fixed the issue.\n\n**Reflection Llama-3.1 70B is an open-source LLM, trained with a new technique called Reflection-Tuning that teaches a LLM to detect mistakes in its reasoning and correct course.**\n\nThe model was trained on synthetic data generated by [Glaive](https://glaive.ai). If you're training a model, Glaive is incredible — use them.\n\nYou can [try the model here](https://reflection-playground-production.up.railway.app/).\n\n## Benchmarks\n\nTrained from Llama 3.1 70B Instruct, you can sample from Reflection Llama-3.1 70B using the same code, pipelines, etc. as any other Llama model. It even uses the stock Llama 3.1 chat template format (though, we've trained in a few new special tokens to aid in reasoning and reflection).\n\nDuring sampling, the model will start by outputting reasoning inside `<thinking>` and `</thinking>` tags, and then once it is satisfied with its reasoning, it will output the final answer inside `<output>` and `</output>` tags. Each of these tags are special tokens, trained into the model.\n\nThis enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user.\n\nInside the `<thinking>` section, the model may output one or more `<reflection>` tags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer.\n\n## System Prompt\n\nThe system prompt used for training this model is:\n\n```\nYou are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside <thinking> tags, and then provide your final response inside <output> tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection> tags.\n```\n\nWe recommend using this exact system prompt to get the best results from Reflection Llama-3.1 70B. You may also want to experiment combining this system prompt with your own custom instructions to customize the behavior of the model.\n\n## Chat Format\n\nAs mentioned above, the model uses the standard Llama 3.1 chat format. Here’s an example:\n\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside <thinking> tags, and then provide your final response inside <output> tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection> tags.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nwhat is 2+2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\n## Tips for Performance\n\n- We are initially recommending a `temperature` of `.7` and a `top_p` of `.95`.\n- For increased accuracy, append `Think carefully.` at the end of your messages.\n\n## Dataset / Report\n\nBoth the dataset and a brief report detailing how we trained this model will be released next week, alongside our Reflection 405B model that we expect will be the top-performing LLM in the world, including closed-source models.\n\n---\n\nThanks to Jason Kuperberg and Josh Bickett from the [HyperWrite](https://hyperwriteai.com) team for reviewing drafts of the report we'll be releasing next week.\n\nAlso, we know right now the model is split into a ton of files. We'll condense this soon to make the model easier to download and work with!","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B"}]},{"id":"microsoft/Florence-2-large","name":"Florence-2-large","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","pytorch","safetensors","florence2","image-text-to-text","vision","custom_code","arxiv:2311.06242","license:mit","endpoints_compatible","region:us"],"likes":1699,"downloads":944154,"readme":"---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Florence-2-large/resolve/main/LICENSE\npipeline_tag: image-text-to-text\ntags:\n- vision\n---\n\n# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\n\n## Model Summary\n\n**This is a continued pretrained version of Florence-2-large model with 4k context length, only 0.1B samples are used for continue pretraining, thus it might not be trained well. In addition, OCR task has been updated with line separator ('\\n'). COCO OD AP 39.8**\n\nThis Hub repository contains a HuggingFace's `transformers` implementation of Florence-2 model from Microsoft.\n\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model. \n\nResources and Technical Documentation:\n+ [Florence-2 technical report](https://arxiv.org/abs/2311.06242). \n+ [Jupyter Notebook for inference and visualization of Florence-2-large](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n\n| Model   | Model size | Model Description | \n| ------- | ------------- |   ------------- |  \n| Florence-2-base[[HF]](https://huggingface.co/microsoft/Florence-2-base) | 0.23B | Pretrained model with FLD-5B  \n| Florence-2-large[[HF]](https://huggingface.co/microsoft/Florence-2-large) | 0.77B  | Pretrained model with FLD-5B  \n| Florence-2-base-ft[[HF]](https://huggingface.co/microsoft/Florence-2-base-ft) | 0.23B  | Finetuned model on a colletion of downstream tasks\n| Florence-2-large-ft[[HF]](https://huggingface.co/microsoft/Florence-2-large-ft) | 0.77B | Finetuned model on a colletion of downstream tasks\n \n## How to Get Started with the Model\n\nUse the code below to get started with the model. All models are trained with float16. \n\n```python\nimport requests\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n\nprompt = \"<OD>\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=4096,\n    num_beams=3,\n    do_sample=False\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n\nprint(parsed_answer)\n\n```\n\n\n## Tasks\n\nThis model is capable of performing different tasks through changing the prompts.\n\nFirst, let's define a function to run a prompt.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ndef run_example(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n\n    print(parsed_answer)\n```\n</details>\n\nHere are the tasks `Florence-2` could perform:\n\n<details>\n<summary> Click to expand </summary>\n\n\n\n### Caption\n```python\nprompt = \"<CAPTION>\"\nrun_example(prompt)\n```\n\n### Detailed Caption\n```python\nprompt = \"<DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### More Detailed Caption\n```python\nprompt = \"<MORE_DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### Caption to Phrase Grounding \ncaption to phrase grounding task requires additional text input, i.e. caption. \n\nCaption to phrase grounding results format: \n{'\\<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\n```python\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\n```\n\n### Object Detection\n\nOD results format: \n{'\\<OD>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n\n```python\nprompt = \"<OD>\"\nrun_example(prompt)\n```\n\n### Dense Region Caption\nDense region caption results format: \n{'\\<DENSE_REGION_CAPTION>' : {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n```python\nprompt = \"<DENSE_REGION_CAPTION>\"\nrun_example(prompt)\n```\n\n### Region proposal\nDense region caption results format: \n{'\\<REGION_PROPOSAL>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['', '', ...]}}\n```python\nprompt = \"<REGION_PROPOSAL>\"\nrun_example(prompt)\n```\n\n### OCR \n\n```python\nprompt = \"<OCR>\"\nrun_example(prompt)\n```\n\n### OCR with Region\nOCR with region output format:\n{'\\<OCR_WITH_REGION>': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\n```python\nprompt = \"<OCR_WITH_REGION>\"\nrun_example(prompt)\n```\n\n### Output confidence score with Object Detection\n```python\n\ndef run_example_with_score(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      num_beams=3,\n      return_dict_in_generate=True,\n      output_scores=True,\n    )\n    generated_text = processor.batch_decode(generated_ids.sequences, skip_special_tokens=False)[0]\n\n    prediction, scores, beam_indices = generated_ids.sequences, generated_ids.scores, generated_ids.beam_indices\n    transition_beam_scores = model.compute_transition_scores(\n        sequences=prediction,\n        scores=scores,\n        beam_indices=beam_indices,\n    )\n\n    parsed_answer = processor.post_process_generation(sequence=generated_ids.sequences[0], \n        transition_beam_score=transition_beam_scores[0],\n        task=task_prompt, image_size=(image.width, image.height)\n    )\n\n    print(parsed_answer)\n\nprompt = \"<OD>\"\nrun_example_with_score(prompt)\n\n```\n\nfor More detailed examples, please refer to [notebook](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n</details>\n\n# Benchmarks\n\n## Florence-2 Zero-shot performance\n  \nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.  \n  \n| Method | #params | COCO Cap. test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | COCO Det. val2017 mAP |  \n|--------|---------|----------------------|------------------|--------------------|-----------------------|\n| Flamingo | 80B | 84.3 | - | - | - | \n| Florence-2-base| 0.23B | 133.0 | 118.7 | 70.1 | 34.7 | \n| Florence-2-large| 0.77B | 135.6 | 120.8 | 72.8 | 37.5 |\n\n  \nThe following table continues the comparison with performance on other vision-language evaluation tasks.  \n  \n| Method | Flickr30k test R@1 | Refcoco val Accuracy | Refcoco test-A Accuracy | Refcoco test-B Accuracy | Refcoco+ val Accuracy | Refcoco+ test-A Accuracy | Refcoco+ test-B Accuracy | Refcocog val Accuracy | Refcocog test Accuracy | Refcoco RES val mIoU |  \n|--------|----------------------|----------------------|-------------------------|-------------------------|-----------------------|--------------------------|--------------------------|-----------------------|------------------------|----------------------|  \n| Kosmos-2 | 78.7 | 52.3 | 57.4 | 47.3 | 45.5 | 50.7 | 42.2 | 60.6 | 61.7 | - |  \n| Florence-2-base | 83.6 | 53.9 | 58.4 | 49.7 | 51.5 | 56.4 | 47.9 | 66.3 | 65.1 | 34.6 |  \n| Florence-2-large | 84.4 | 56.3 | 61.6 | 51.4 | 53.6 | 57.9 | 49.9 | 68.0 | 67.0 | 35.8 |  \n\n\n\n## Florence-2 finetuned performance \n\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models *Florence-2-base-ft* and *Florence-2-large-ft* that can conduct a wide range of downstream tasks. \n  \nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol \"▲\" indicates the usage of external OCR as input.  \n  \n| Method         | # Params | COCO Caption Karpathy test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | VQAv2 test-dev Acc | TextVQA test-dev Acc | VizWiz VQA test-dev Acc |  \n|----------------|----------|-----------------------------------|------------------|--------------------|--------------------|----------------------|-------------------------|  \n| **Specialist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| CoCa           | 2.1B     | 143.6                              | 122.4            | -                  | 82.3               | -                    | -                       |  \n| BLIP-2         | 7.8B     | 144.5                              | 121.6            | -                  | 82.2               | -                    | -                       |  \n| GIT2           | 5.1B     | 145.0                              | 126.9            | 148.6              | 81.7               | 67.3                 | 71.0                    |  \n| Flamingo       | 80B      | 138.1                              | -                | -                  | 82.0               | 54.1                 | 65.7                    |  \n| PaLI           | 17B      | 149.1                              | 127.0            | 160.0▲             | 84.3               | 58.8 / 73.1▲         | 71.6 / 74.4▲            |  \n| PaLI-X         | 55B      | 149.2                              | 126.3            | 147.0 / 163.7▲     | 86.0               | 71.4 / 80.8▲         | 70.9 / 74.6▲            |  \n| **Generalist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| Unified-IO     | 2.9B     | -                                  | 100.0            | -                  | 77.9               | -                    | 57.4                    |  \n| Florence-2-base-ft | 0.23B  | 140.0                              | 116.7            | 143.9              | 79.7               | 63.6                 | 63.6                    |  \n| Florence-2-large-ft | 0.77B  | 143.3                              | 124.9            | 151.1              | 81.7               | 73.5                 | 72.6                    |  \n  \n  \n| Method               | # Params | COCO Det. val2017 mAP | Flickr30k test R@1 | RefCOCO val Accuracy | RefCOCO test-A Accuracy | RefCOCO test-B Accuracy | RefCOCO+ val Accuracy | RefCOCO+ test-A Accuracy | RefCOCO+ test-B Accuracy | RefCOCOg val Accuracy | RefCOCOg test Accuracy | RefCOCO RES val mIoU |  \n|----------------------|----------|-----------------------|--------------------|----------------------|-------------------------|-------------------------|------------------------|---------------------------|---------------------------|------------------------|-----------------------|------------------------|  \n| **Specialist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| SeqTR                | -        | -                     | -                  | 83.7                 | 86.5                    | 81.2                    | 71.5                   | 76.3                      | 64.9                      | 74.9                   | 74.2                  | -                      |  \n| PolyFormer           | -        | -                     | -                  | 90.4                 | 92.9                    | 87.2                    | 85.0                   | 89.8                      | 78.0                      | 85.8                   | 85.9                  | 76.9                   |  \n| UNINEXT              | 0.74B    | 60.6                  | -                  | 92.6                 | 94.3                    | 91.5                    | 85.2                   | 89.6                      | 79.8                      | 88.7                   | 89.4                  | -                      |  \n| Ferret               | 13B      | -                     | -                  | 89.5                 | 92.4                    | 84.4                    | 82.8                   | 88.1                      | 75.2                      | 85.8                   | 86.3                  | -                      |  \n| **Generalist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| UniTAB               | -        | -                     | -                  | 88.6                 | 91.1                    | 83.8                    | 81.0                   | 85.4                      | 71.6                      | 84.6                   | 84.7                  | -                      |  \n| Florence-2-base-ft | 0.23B    | 41.4                  | 84.0                | 92.6                 | 94.8                    | 91.5                   | 86.8                   | 91.7                      | 82.2                      | 89.8                   | 82.2                  | 78.0                  |  \n| Florence-2-large-ft| 0.77B    | 43.4                  | 85.2               | 93.4                 | 95.3                    | 92.0                    | 88.3                   | 92.9                      | 83.6                      | 91.2                   | 91.7                  | 80.5                   |  \n  \n\n## BibTex and citation info\n\n```\n@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/Florence-2-large"}]},{"id":"Kijai/WanVideo_comfy","name":"WanVideo_comfy","description":"A model for various tasks.","task":"N/A","tags":["diffusion-single-file","comfyui","base_model:Wan-AI/Wan2.1-VACE-1.3B","base_model:finetune:Wan-AI/Wan2.1-VACE-1.3B","region:us"],"likes":1696,"downloads":6336215,"readme":"---\ntags:\n  - diffusion-single-file\n  - comfyui\nbase_model:\n- Wan-AI/Wan2.1-VACE-14B\n- Wan-AI/Wan2.1-VACE-1.3B\n---\nCombined and quantized models for WanVideo, originating from here:\n\nhttps://huggingface.co/Wan-AI/\n\nCan be used with: https://github.com/kijai/ComfyUI-WanVideoWrapper and ComfyUI native WanVideo nodes.\n\nI've also started to do fp8_scaled versions over here: https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled\n\nOther model sources:\n\nTinyVAE from https://github.com/madebyollin/taehv\n\nSkyReels: https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9\n\nWanVideoFun: https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17\n\n---\n\nLightx2v:\n\nCausVid 14B: https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid\n\nCFG and Step distill 14B: https://huggingface.co/lightx2v/Wan2.1-T2V-14B-StepDistill-CfgDistill\n\n---\n\nCausVid 1.3B: https://huggingface.co/tianweiy/CausVid\n\nAccVideo: https://huggingface.co/aejion/AccVideo-WanX-T2V-14B\n\nPhantom: https://huggingface.co/bytedance-research/Phantom\n\nATI: https://huggingface.co/bytedance-research/ATI\n\nMiniMaxRemover: https://huggingface.co/zibojia/minimax-remover\n\nMAGREF: https://huggingface.co/MAGREF-Video/MAGREF\n\nFantasyTalking: https://github.com/Fantasy-AMAP/fantasy-talking\n\nMultiTalk: https://github.com/MeiGen-AI/MultiTalk\n\nAnisora: https://huggingface.co/IndexTeam/Index-anisora/tree/main/14B\n\nPusa: https://huggingface.co/RaphaelLiu/PusaV1/tree/main\n\nFastVideo: https://huggingface.co/FastVideo\n\nEchoShot: https://github.com/D2I-ai/EchoShot\n\nWan22 5B Turbo: https://huggingface.co/quanhaol/Wan2.2-TI2V-5B-Turbo\n\nOvi: https://github.com/character-ai/Ovi\n\nFlashVSR: https://huggingface.co/JunhaoZhuang/FlashVSR\n\nrCM: https://huggingface.co/worstcoder/rcm-Wan/tree/main\n\n---\nCausVid LoRAs are experimental extractions from the CausVid finetunes, the aim with them is to benefit from the distillation in CausVid, rather than any actual causal inference.\n---\nv1 = direct extraction, has adverse effects on motion and introduces flashing artifact at full strength.\n\nv1.5 = same as above, but without the first block which fixes the flashing at full strength.\n\nv2 = further pruned version with only attention layers and no first block, fixes flashing and retains motion better, needs more steps and can also benefit from cfg.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/Kijai/WanVideo_comfy"}]},{"id":"microsoft/OmniParser","name":"OmniParser","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","safetensors","blip-2","image-to-text","image-text-to-text","arxiv:2408.00203","license:mit","endpoints_compatible","region:us"],"likes":1695,"downloads":284,"readme":"---\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\n---\n📢 [[Project Page](https://microsoft.github.io/OmniParser/)] [[Blog Post](https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/)] [[Demo](https://huggingface.co/spaces/microsoft/OmniParser/)] \n\n# Model Summary\nOmniParser is a general screen parsing tool, which interprets/converts UI screenshot to structured format, to improve existing LLM based UI agent. \nTraining Datasets include: 1) an interactable icon detection dataset, which was curated from popular web pages and automatically annotated to highlight clickable and actionable regions, and 2) an icon description dataset, designed to associate each UI element with its corresponding function. \n\nThis model hub includes a finetuned version of YOLOv8 and a finetuned BLIP-2 model on the above dataset respectively. For more details of the models used and finetuning, please refer to the [paper](https://arxiv.org/abs/2408.00203).\n\n# Responsible AI Considerations\n## Intended Use\n- OmniParser is designed to be able to convert unstructured screenshot image into structured list of elements including interactable regions location and captions of icons on its potential functionality. \n- OmniParser is intended to be used in settings where users are already trained on responsible analytic approaches and critical reasoning is expected. OmniParser is capable of providing extracted information from the screenshot, however human judgement is needed for the output of OmniParser. \n- OmniParser is intended to be used on various screenshots, which includes both PC and Phone, and also on various applications.  \n## limitations\n- OmniParser is designed to faithfully convert screenshot image into structured elements of interactable regions and semantics of the screen, while it does not detect harmful content in its input (like users have freedom to decide the input of any LLMs), users are expected to provide input to the OmniParser that is not harmful. \n- While OmniParser only converts screenshot image into texts, it can be used to construct an GUI agent based on LLMs that is actionable. When developing and operating the agent using OmniParser, the developers need to be responsible and follow common safety standard. \n- For OmniPaser-BLIP2, it may incorrectly infer the gender or other sensitive attribute (e.g., race, religion etc.) of individuals in icon images. Inference of sensitive attributes may rely upon stereotypes and generalizations rather than information about specific individuals and are more likely to be incorrect for marginalized people. Incorrect inferences may result in significant physical or psychological injury or restrict, infringe upon or undermine the ability to realize an individual’s human rights. We do not recommend use of OmniParser in any workplace-like use case scenario.\n\n# License\nPlease note that icon_detect model is under AGPL license, and icon_caption_blip2 & icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model. \n\n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/OmniParser"}]},{"id":"microsoft/Phi-3-mini-128k-instruct","name":"Phi-3-mini-128k-instruct","description":"A model for text-generation.","task":"text-generation","tags":["transformers","safetensors","phi3","text-generation","nlp","code","conversational","custom_code","en","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"likes":1682,"downloads":292634,"readme":"---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/LICENSE\n\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n  - messages:\n      - role: user\n        content: Can you provide ways to eat combinations of bananas and dragonfruits?\n---\n🎉**Phi-4**: [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n## Model Summary\n\nThe Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.\nThis dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.\n\nAfter initial training, the model underwent a post-training process that involved supervised fine-tuning and direct preference optimization to enhance its ability to follow instructions and adhere to safety measures.\nWhen evaluated against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning, the Phi-3 Mini-128K-Instruct demonstrated robust and state-of-the-art performance among models with fewer than 13 billion parameters.\nResources and Technical Documentation:\n\n🏡 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n📰 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n📖 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n🛠️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n👩‍🍳 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n🖥️ [Try It](https://aka.ms/try-phi3)\n\n|         | Short Context | Long Context |\n| :-      | :-            | :-           |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. \n\n## Release Notes \n\nThis is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback. \nThe model used additional post-training data leading to substantial gains on long-context understanding, instruction following, and structure output. \nWe also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability. \nWe believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications.\nWe appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. \n\nThese tables below highlights improvements on instruction following, structure output, reasoning, and long-context understanding of the new release on our public and internal benchmark datasets.\n\n| Benchmarks | Original | June 2024 Update |\n| :-         | :-       | :-               |\n| Instruction Extra Hard | 5.7 | 5.9 |\n| Instruction Hard | 5.0 | 5.2 |\n| JSON Structure Output | 1.9 | 60.1 |\n| XML Structure Output | 47.8 | 52.9 |\n| GPQA\t| 25.9\t| 29.7 |\n| MMLU\t| 68.1\t| 69.7 |\n| **Average**\t| **25.7**\t| **37.3** |\n\nRULER: a retrieval-based benchmark for long context understanding\n\n| Model             | 4K   | 8K   | 16K  | 32K  | 64K  | 128K | Average |\n| :-------------------| :------| :------| :------| :------| :------| :------| :---------|\n| Original          | 86.7 | 78.1 | 75.6 | 70.3 | 58.9 | 43.3 | **68.8**    |\n| June 2024 Update  | 92.4 | 91.1 | 90.8 | 87.9 | 79.8 | 65.6 | **84.6**    |\n\nRepoQA: a benchmark for long context code understanding\n\n| Model             | Python | C++ | Rust | Java | TypeScript | Average |\n| :-------------------| :--------| :-----| :------| :------| :------------| :---------|\n| Original          | 27     | 29  | 40   | 33   | 33         | **32.4**    |\n| June 2024 Update  | 85     | 63  | 72   | 93   | 72         | **77**      |\n\n\nNotes: if users would like to check out the previous version, use the git commit id **bb5bf1e4001277a606e11debca0ef80323e5f824**. For the model conversion, e.g. GGUF and other formats, we invite the community to experiment with various approaches and share your valuable feedback. Let's innovate together!\n\n## How to Use\n\nPhi-3 Mini-128K-Instruct has been integrated in the development version (4.41.3) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.41.2\n```\n\nPhi-3 Mini-128K-Instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3)\n\n### Tokenizer\n\nPhi-3 Mini-128K-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\n\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3 Mini-128K-Instruct model is best suited for prompts using the chat format as follows. \nYou can provide the prompt as a question with a generic template as follow:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nQuestion?<|end|>\n<|assistant|>\n```\n\nFor example:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|> \n```\nwhere the model generates the text after `<|assistant|>` . In case of few-shots prompt, the prompt can be formatted as the following:\n\n```markdown\n<|system|>\nYou are a helpful travel assistant.<|end|>\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nimport torch \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \nmodel = AutoModelForCausalLM.from_pretrained( \n    \"microsoft/Phi-3-mini-128k-instruct\",  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n\nmessages = [ \n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n] \n\npipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"return_full_text\": False, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0]['generated_text']) \n```\n\nNotes: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation=\"flash_attention_2\"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n* Architecture: Phi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\n* Inputs: Text. It is best suited for prompts using chat format.\n* Context length: 128K tokens\n* GPUs: 512 H100-80G\n* Training time: 10 days\n* Training data: 4.9T tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between May and June 2024\n* Status: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n* Release dates: June, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3-Mini-128K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT-3.5.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k–shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-128K-Ins | Gemma-7B | Mistral-7B | Mixtral-8x7B | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n| :----------| :-----------| :---------------------| :----------| :------------| :--------------| :----------------| :-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.5 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 69.7 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard <br>3-shot | 72.1 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 52.3 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot | 70.5 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 85.5 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 77.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot | 56.4 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot | 78.8 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot | 80.1 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot | 29.7 | 2.9 | 15 | 6.9 | 32.4 | 29.9 |\n| | Social IQA <br>5-shot | 74.7 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot | 64.8 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot | 71.0 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot | 57.8 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoTT <br>8-shot | 85.3 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot | 60.4 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot | 70.0 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **66.4** | **56.0** | **56.4** | **64.4** | **65.5** | **70.3** |\n\n**Long Context**: Phi-3 Mini-128K-Instruct supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA. \n\n| Benchmark     | Phi-3 Mini-128K-Instruct | Mistral-7B | Mixtral 8x7B | LLaMA-3-8B-Instruct |\n| :---------------| :--------------------------|:------------|:--------------|:---------------------|\n| GovReport     | 25.3                     | 4.9        | 20.3         | 10.3                |\n| QMSum         | 21.9                     | 15.5       | 20.6         | 2.9                 |\n| Qasper        | 41.6                     | 23.5       | 26.6         | 8.1                 |\n| SQuALITY      | 24.1                     | 14.7       | 16.2         | 25                  |\n| SummScreenFD  | 16.8                     | 9.3        | 11.3         | 5.1                 |\n| **Average**   | **25.9**                 | **13.6**   | **19.0**     | **10.3**            |\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-128K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:--------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 60.6 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 69.4 | 60.3 | 62.8 | 68.1 | 69.6 | 71.7 |\n| Language understanding | 57.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 61.0 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 51.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 35.8 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.4 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.   \n\n## Cross Platform Support \n\n[ONNX runtime](https://onnxruntime.ai/blogs/accelerating-phi-3) now supports Phi-3 mini models across platforms and hardware.  \n\nOptimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA).   \n\nAlong with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile.  \n\nHere are some of the optimized configurations we have added:  \n\n1. ONNX models for int4 DML: Quantized to int4 via AWQ \n2. ONNX model for fp16 CUDA \n3. ONNX model for int4 CUDA: Quantized to int4 via RTN \n4. ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN \n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3 Mini-128K-Instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\n* Optimized inference on GPU, CPU, and Mobile: use the **ONNX** models [128K](https://aka.ms/phi3-mini-128k-instruct-onnx)\n  \n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-128k/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct"}]},{"id":"deepseek-ai/DeepSeek-V3-Base","name":"DeepSeek-V3-Base","description":"A model for various tasks.","task":"N/A","tags":["safetensors","deepseek_v3","custom_code","arxiv:2412.19437","fp8","region:us"],"likes":1681,"downloads":4856,"readme":"<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.png\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align=\"center\">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/niah.png\">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align=\"center\">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align=\"center\">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface's Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n```\n\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"}]},{"id":"google/gemma-3-27b-it","name":"gemma-3-27b-it","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","safetensors","gemma3","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2311.12022","arxiv:2108.07732","arxiv:2107.03374","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2312.11805","base_model:google/gemma-3-27b-pt","base_model:finetune:google/gemma-3-27b-pt","license:gemma","text-generation-inference","endpoints_compatible","region:us"],"likes":1676,"downloads":835178,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/google/gemma-3-27b-it"}]},{"id":"tencent/Hunyuan3D-2","name":"Hunyuan3D-2","description":"A model for image-to-3d.","task":"image-to-3d","tags":["hunyuan3d-2","diffusers","safetensors","image-to-3d","text-to-3d","en","zh","arxiv:2501.12202","arxiv:2411.02293","license:other","region:us"],"likes":1648,"downloads":112412,"readme":"---\nlibrary_name: hunyuan3d-2\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://huggingface.co/tencent/Hunyuan3D-2/blob/main/LICENSE.txt\nlanguage:\n  - en\n  - zh\ntags:\n  - image-to-3d\n  - text-to-3d\npipeline_tag: image-to-3d\nextra_gated_eu_disallowed: true\n---\n\n<p align=\"center\">\n  <img src=\"./assets/images/teaser.jpg\">\n</p>\n\n<div align=\"center\">\n  <a href=https://3d.hunyuan.tencent.com target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan3D-black.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2  target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px></a>\n  <a href=https://huggingface.co/tencent/Hunyuan3D-2 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://3d-models.hunyuan.tencent.com/ target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n<a href=https://discord.gg/GuaWYwzKbX target=\"_blank\"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n    <a href=https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/report/Tencent_Hunyuan3D_2_0.pdf target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n</div>\n\n\n[//]: # (  <a href=# target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>)\n\n[//]: # (  <a href=# target=\"_blank\"><img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px></a>)\n\n[//]: # (  <a href=\"#\"><img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/v/mulankit?logo=pypi\"  height=22px></a>)\n\n<br>\n<p align=\"center\">\n“ Living out everyone’s imagination on creating and manipulating 3D assets.”\n</p>\n\nThis repository contains the models of the paper [Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation](https://huggingface.co/papers/2501.12202).\nFor code and more details on how to use it, refer to the [Github repository](https://github.com/Tencent/Hunyuan3D-2).\n\n## 🔥 News\n\n- Jan 21, 2025: 💬 Release [Hunyuan3D 2.0](https://huggingface.co/spaces/tencent/Hunyuan3D-2). Please give it a try!\n\n## **Abstract**\n\nWe present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.\nThis system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale\ntexture synthesis model - Hunyuan3D-Paint.\nThe shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly\naligns with a given condition image, laying a solid foundation for downstream applications.\nThe texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant\ntexture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation\nprocess of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes\nefficiently.\nWe systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,\nincluding the open-source models and closed-source models in geometry details, condition alignment, texture quality, and\ne.t.c.\n\n<p align=\"center\">\n  <img src=\"assets/images/system.jpg\">\n</p>\n\n## ☯️ **Hunyuan3D 2.0**\n\n### Architecture\n\nHunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the\nsynthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and\ntexture generation and also provides flexibility for texturing either generated or handcrafted meshes.\n\n<p align=\"left\">\n  <img src=\"assets/images/arch.jpg\">\n</p>\n\n### Performance\n\nWe have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.\nThe numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets\nand the condition following ability.\n\n| Model                   | CMMD(⬇)   | FID_CLIP(⬇) | FID(⬇)      | CLIP-score(⬆) |\n|-------------------------|-----------|-------------|-------------|---------------|\n| Top Open-source Model1  | 3.591     | 54.639      | 289.287     | 0.787         |\n| Top Close-source Model1 | 3.600     | 55.866      | 305.922     | 0.779         |\n| Top Close-source Model2 | 3.368     | 49.744      | 294.628     | 0.806         |\n| Top Close-source Model3 | 3.218     | 51.574      | 295.691     | 0.799         |\n| Hunyuan3D 2.0           | **3.193** | **49.165**  | **282.429** | **0.809**     |\n\nGeneration results of Hunyuan3D 2.0:\n<p align=\"left\">\n  <img src=\"assets/images/e2e-1.gif\"  height=300>\n  <img src=\"assets/images/e2e-2.gif\"  height=300>\n</p>\n\n### Pretrained Models\n\n| Model                | Date       | Huggingface                                            |\n|----------------------|------------|--------------------------------------------------------| \n| Hunyuan3D-DiT-v2-0   | 2025-01-21 | [Download](https://huggingface.co/tencent/Hunyuan3D-2) |\n| Hunyuan3D-Paint-v2-0 | 2025-01-21 | [Download](https://huggingface.co/tencent/Hunyuan3D-2) |\n| Hunyuan3D-Delight-v2-0 | 2025-01-21 | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0) |\n\n## 🤗 Get Started with Hunyuan3D 2.0\n\nYou may follow the next steps to use Hunyuan3D 2.0 via code or the Gradio App.\n\n### Install Requirements\n\nPlease install Pytorch via the [official](https://pytorch.org/) site. Then install the other requirements via\n\n```bash\npip install -r requirements.txt\n# for texture\ncd hy3dgen/texgen/custom_rasterizer\npython3 setup.py install\ncd ../../..\ncd hy3dgen/texgen/differentiable_renderer\nbash compile_mesh_painter.sh OR python3 setup.py install (on Windows)\n```\n\n### API Usage\n\nWe designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -\nHunyuan3D-Paint.\n\nYou could assess **Hunyuan3D-DiT** via:\n\n```python\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(image='assets/demo.png')[0]\n```\n\nThe output mesh is a [trimesh object](https://trimesh.org/trimesh.html), which you could save to glb/obj (or other\nformat) file.\n\nFor **Hunyuan3D-Paint**, do the following:\n\n```python\nfrom hy3dgen.texgen import Hunyuan3DPaintPipeline\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\n# let's generate a mesh first\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(image='assets/demo.png')[0]\n\npipeline = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(mesh, image='assets/demo.png')\n```\n\nPlease visit [minimal_demo.py](https://github.com/Tencent/Hunyuan3D-2/blob/main/minimal_demo.py) for more advanced usage, such as **text to 3D** and **texture generation\nfor handcrafted mesh**.\n\n### Gradio App\n\nYou could also host a [Gradio](https://www.gradio.app/) App in your own computer via:\n\n```bash\npip3 install gradio==3.39.0\npython3 gradio_app.py\n```\n\nDon't forget to visit [Hunyuan3D](https://3d.hunyuan.tencent.com) for quick use, if you don't want to host yourself.\n\n## 📑 Open-Source Plan\n\n- [x] Inference Code\n- [x] Model Checkpoints\n- [x] Technical Report\n- [ ] ComfyUI\n- [ ] TensorRT Version\n\n## 🔗 BibTeX\n\nIf you found this repository helpful, please cite our report:\n\n```bibtex\n@misc{hunyuan3d22025tencent,\n    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    eprint={2501.12202},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{yang2024tencent,\n    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2024},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n## Community Resources\n\nThanks for the contributions of community members, here we have these great extensions of Hunyuan3D 2.0:\n\n- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)\n- [Hunyuan3D-2-for-windows](https://github.com/sdbds/Hunyuan3D-2-for-windows)\n- [📦 A bundle for running on Windows | 整合包](https://github.com/YanWenKun/Comfy3D-WinPortable/releases/tag/r8-hunyuan3d2)\n\n## Acknowledgements\n\nWe would like to thank the contributors to\nthe [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers)\nand [HuggingFace](https://huggingface.co) repositories, for their open research and exploration.\n\n## Star History\n\n<a href=\"https://star-history.com/#Tencent/Hunyuan3D-2&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&type=Date\" />\n </picture>\n</a>","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/tencent/Hunyuan3D-2"}]},{"id":"mistralai/Mistral-Nemo-Instruct-2407","name":"Mistral-Nemo-Instruct-2407","description":"A model for various tasks.","task":"N/A","tags":["vllm","safetensors","mistral","mistral-common","en","fr","de","es","it","pt","ru","zh","ja","base_model:mistralai/Mistral-Nemo-Base-2407","base_model:finetune:mistralai/Mistral-Nemo-Base-2407","license:apache-2.0","region:us"],"likes":1617,"downloads":113328,"readme":"---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ru\n- zh\n- ja\nlicense: apache-2.0\nbase_model: mistralai/Mistral-Nemo-Base-2407\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Model Card for Mistral-Nemo-Instruct-2407\n\nThe Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the [Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407). Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.\n\nFor more details about this model please refer to our release [blog post](https://mistral.ai/news/mistral-nemo/).\n\n## Key features\n- Released under the **Apache 2 License**\n- Pre-trained and instructed versions\n- Trained with a **128k context window**\n- Trained on a large proportion of **multilingual and code data**\n- Drop-in replacement of Mistral 7B\n\n## Model Architecture\nMistral Nemo is a transformer model, with the following architecture choices:\n- **Layers:** 40\n- **Dim:** 5,120\n- **Head dim:** 128\n- **Hidden dim:** 14,336\n- **Activation Function:** SwiGLU\n- **Number of heads:** 32\n- **Number of kv-heads:** 8 (GQA)\n- **Vocabulary size:** 2**17 ~= 128k\n- **Rotary embeddings (theta = 1M)**\n\n## Metrics\n\n### Main Benchmarks\n\n| Benchmark | Score |\n| --- | --- |\n| HellaSwag (0-shot) | 83.5% |\n| Winogrande (0-shot) | 76.8% |\n| OpenBookQA (0-shot) | 60.6% |\n| CommonSenseQA (0-shot) | 70.4% |\n| TruthfulQA (0-shot) | 50.3% |\n| MMLU (5-shot) | 68.0% |\n| TriviaQA (5-shot) | 73.8% |\n| NaturalQuestions (5-shot) | 31.2% |\n\n### Multilingual Benchmarks (MMLU)\n\n| Language | Score |\n| --- | --- |\n| French | 62.3% |\n| German | 62.7% |\n| Spanish | 64.6% |\n| Italian | 61.3% |\n| Portuguese | 63.3% |\n| Russian | 59.2% |\n| Chinese | 59.0% |\n| Japanese | 59.0% |\n\n## Usage\n\nThe model can be used with three different frameworks\n\n- [`mistral_inference`](https://github.com/mistralai/mistral-inference): See [here](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407#mistral-inference)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n- [`NeMo`](https://github.com/NVIDIA/NeMo): See [nvidia/Mistral-NeMo-12B-Instruct](https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct)\n\n### Mistral Inference\n\n#### Install\n\nIt is recommended to use `mistralai/Mistral-Nemo-Instruct-2407` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n#### Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', 'Nemo-Instruct')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\n```\n\n#### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/Nemo-Instruct --instruct --max_tokens 256 --temperature 0.35\n```\n\n*E.g.* Try out something like:\n```\nHow expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\n```\n\n#### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\n\nprompt = \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n#### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Transformers\n\n> [!IMPORTANT]\n> NOTE: Until a new release has been made, you need to install transformers from source:\n> ```sh\n> pip install git+https://github.com/huggingface/transformers.git\n> ```\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-Nemo-Instruct-2407\",max_new_tokens=128)\nchatbot(messages)\n```\n\n## Function calling with `transformers`\n\nTo use this example, you'll need `transformers` version 4.42.0 or higher. Please see the \n[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)\nin the `transformers` docs for more information.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    \"\"\"\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n    \"\"\"\n    pass\n\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), \nand note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\n\n> [!TIP]\n> Unlike previous Mistral models, Mistral Nemo requires smaller temperatures. We recommend to use a temperature of 0.3.\n\n## Limitations\n\nThe Mistral Nemo Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"}]},{"id":"2Noise/ChatTTS","name":"ChatTTS","description":"A model for text-to-audio.","task":"text-to-audio","tags":["chat_tts","safetensors","text-to-audio","license:cc-by-nc-4.0","region:us"],"likes":1614,"downloads":1175,"readme":"---\nlicense: cc-by-nc-4.0\nlibrary_name: chat_tts\npipeline_tag: text-to-audio\n---\n\n\n**We are also training larger-scale models and need computational power and data support. If you can provide assistance, please contact OPEN-SOURCE@2NOISE.COM. Thank you very much.**\n\n## Clone the Repository\nFirst, clone the Git repository:\n```bash\ngit clone https://github.com/2noise/ChatTTS.git\n```\n\n## Model Inference\n\n\n```python\n# Import necessary libraries and configure settings\nimport torch\nimport torchaudio\ntorch._dynamo.config.cache_size_limit = 64\ntorch._dynamo.config.suppress_errors = True\ntorch.set_float32_matmul_precision('high')\n\nimport ChatTTS\nfrom IPython.display import Audio\n\n# Initialize and load the model: \nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\n# Define the text input for inference (Support Batching)\ntexts = [\n    \"So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\",\n    ]\n\n# Perform inference and play the generated audio\nwavs = chat.infer(texts)\nAudio(wavs[0], rate=24_000, autoplay=True)\n\n# Save the generated audio \ntorchaudio.save(\"output.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n**For more usage examples, please refer to the [example notebook](https://github.com/2noise/ChatTTS/blob/main/example.ipynb), which includes parameters for finer control over the generated speech, such as specifying the speaker, adjusting speech speed, and adding laughter.**\n\n\n\n\n\n\n### Disclaimer: For Academic Purposes Only\n\nThe information provided in this document is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information.","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/2Noise/ChatTTS"}]},{"id":"xinsir/controlnet-union-sdxl-1.0","name":"controlnet-union-sdxl-1.0","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","Text-to-Image","ControlNet","Diffusers","Stable Diffusion","text-to-image","license:apache-2.0","region:us"],"likes":1605,"downloads":169093,"readme":"---\nlicense: apache-2.0\ntags:\n- Text-to-Image\n- ControlNet\n- Diffusers\n- Stable Diffusion\npipeline_tag: text-to-image\n---\n\n# **ControlNet++: All-in-one ControlNet for image generations and editing!**\n## **ProMax Model has released!! 12 control + 5 advanced editing, just try it!!!**\n![images_display](./images/masonry.webp)\n\n## Network Arichitecture\n![images](./images/ControlNet++.png)\n\n## Advantages about the model\n- Use bucket training like novelai, can generate high resolutions images of any aspect ratio\n- Use large amount of high quality data(over 10000000 images), the dataset covers a diversity of situation\n- Use re-captioned prompt like DALLE.3, use CogVLM to generate detailed description, good prompt following ability\n- Use many useful tricks during training. Including but not limited to date augmentation, mutiple loss, multi resolution\n- Use almost the same parameter compared with original ControlNet. No obvious increase in network parameter or computation.\n- Support 10+ control conditions, no obvious performance drop on any single condition compared with training independently\n- Support multi condition generation, condition fusion is learned during training. No need to set hyperparameter or design prompts.\n- Compatible with other opensource SDXL models, such as BluePencilXL, CounterfeitXL. Compatible with other Lora models.\n\n\n***We design a new architecture that can support 10+ control types in condition text-to-image generation and can generate high resolution images visually comparable with \nmidjourney***. The network is based on the original ControlNet architecture, we propose two new modules to: 1 Extend the original ControlNet to support different image \nconditions using the same network parameter. 2 Support multiple conditions input without increasing computation offload, which is especially important for designers \nwho want to edit image in detail, different conditions use the same condition encoder, without adding extra computations or parameters. We do thoroughly experiments \non SDXL and achieve superior performance both in control ability and aesthetic score. We release the method and the model to the open source community to make everyone \ncan enjoy it.  \n\nInference scripts and more details can found: https://github.com/xinsir6/ControlNetPlus/tree/main\n\n**If you find it useful, please give me a star, thank you very much**\n\n**SDXL ProMax version has been released!!!，Enjoy it!!!**  \n\n**I am sorry that because of the project's revenue and expenditure are difficult to balance, the GPU resources are assigned to other projects that are more likely to be profitable, the SD3 trainging is stopped until I find enough GPU supprt, I will try my best to find GPUs to continue training. If this brings you inconvenience, I sincerely apologize for that. I want to thank everyone who likes this project, your support is what keeps me going**\n\nNote: we put the promax model with a promax suffix in the same [huggingface model repo](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0), detailed instructions will be added later. \n## Advanced editing features in Promax Model\n### Tile Deblur\n![blur0](./images/100000_tile_blur_concat.webp)\n![blur1](./images/100001_tile_blur_concat.webp)\n![blur2](./images/100002_tile_blur_concat.webp)\n![blur3](./images/100003_tile_blur_concat.webp)\n![blur4](./images/100004_tile_blur_concat.webp)\n![blur5](./images/100005_tile_blur_concat.webp)\n### Tile variation\n![var0](./images/100006_tile_var_concat.webp)\n![var1](./images/100007_tile_var_concat.webp)\n![var2](./images/100008_tile_var_concat.webp)\n![var3](./images/100009_tile_var_concat.webp)\n![var4](./images/100010_tile_var_concat.webp)\n![var5](./images/100011_tile_var_concat.webp)\n\n### Tile Super Resolution\nFollowing example show from 1M resolution --> 9M resolution\n<div style=\"display: flex; justify-content: space-between;\">\n  <img src=\"./images/tile_super1.webp\" alt=\"Image 1\" style=\"width: 49%; margin: 1%;\">\n  <img src=\"./images/tile_super1_9upscale.webp\" alt=\"Image 2\" style=\"width: 49%; margin: 1%;\">\n</div>\n\n<div style=\"display: flex; justify-content: space-between;\">\n  <img src=\"./images/tile_super2.webp\" alt=\"Image 1\" style=\"width: 49%; margin: 1%;\">\n  <img src=\"./images/tile_super2_9upscale.webp\" alt=\"Image 2\" style=\"width: 49%; margin: 1%;\">\n</div>\n\n### Image Inpainting\n![inp0](./images/100018_inpainting_concat.webp)\n![inp1](./images/100019_inpainting_concat.webp)\n![inp2](./images/100020_inpainting_concat.webp)\n![inp3](./images/100021_inpainting_concat.webp)\n![inp4](./images/100022_inpainting_concat.webp)\n![inp5](./images/100023_inpainting_concat.webp)\n\n### Image Outpainting\n![oup0](./images/100012_outpainting_concat.webp)\n![oup1](./images/100013_outpainting_concat.webp)\n![oup2](./images/100014_outpainting_concat.webp)\n![oup3](./images/100015_outpainting_concat.webp)\n![oup4](./images/100016_outpainting_concat.webp)\n![oup5](./images/100017_outpainting_concat.webp)\n\n\n## Visual Examples\n### Openpose\n![pose0](./images/000000_pose_concat.webp)\n![pose1](./images/000001_pose_concat.webp)\n![pose2](./images/000002_pose_concat.webp)\n![pose3](./images/000003_pose_concat.webp)\n![pose4](./images/000004_pose_concat.webp)\n### Depth\n![depth0](./images/000005_depth_concat.webp)\n![depth1](./images/000006_depth_concat.webp)\n![depth2](./images/000007_depth_concat.webp)\n![depth3](./images/000008_depth_concat.webp)\n![depth4](./images/000009_depth_concat.webp)\n### Canny\n![canny0](./images/000010_canny_concat.webp)\n![canny1](./images/000011_canny_concat.webp)\n![canny2](./images/000012_canny_concat.webp)\n![canny3](./images/000013_canny_concat.webp)\n![canny4](./images/000014_canny_concat.webp)\n### Lineart\n![lineart0](./images/000015_lineart_concat.webp)\n![lineart1](./images/000016_lineart_concat.webp)\n![lineart2](./images/000017_lineart_concat.webp)\n![lineart3](./images/000018_lineart_concat.webp)\n![lineart4](./images/000019_lineart_concat.webp)\n### AnimeLineart\n![animelineart0](./images/000020_anime_lineart_concat.webp)\n![animelineart1](./images/000021_anime_lineart_concat.webp)\n![animelineart2](./images/000022_anime_lineart_concat.webp)\n![animelineart3](./images/000023_anime_lineart_concat.webp)\n![animelineart4](./images/000024_anime_lineart_concat.webp)\n### Mlsd\n![mlsd0](./images/000025_mlsd_concat.webp)\n![mlsd1](./images/000026_mlsd_concat.webp)\n![mlsd2](./images/000027_mlsd_concat.webp)\n![mlsd3](./images/000028_mlsd_concat.webp)\n![mlsd4](./images/000029_mlsd_concat.webp)\n### Scribble\n![scribble0](./images/000030_scribble_concat.webp)\n![scribble1](./images/000031_scribble_concat.webp)\n![scribble2](./images/000032_scribble_concat.webp)\n![scribble3](./images/000033_scribble_concat.webp)\n![scribble4](./images/000034_scribble_concat.webp)\n### Hed\n![hed0](./images/000035_hed_concat.webp)\n![hed1](./images/000036_hed_concat.webp)\n![hed2](./images/000037_hed_concat.webp)\n![hed3](./images/000038_hed_concat.webp)\n![hed4](./images/000039_hed_concat.webp)\n### Pidi(Softedge)\n![pidi0](./images/000040_softedge_concat.webp)\n![pidi1](./images/000041_softedge_concat.webp)\n![pidi2](./images/000042_softedge_concat.webp)\n![pidi3](./images/000043_softedge_concat.webp)\n![pidi4](./images/000044_softedge_concat.webp)\n### Teed\n![ted0](./images/000045_ted_concat.webp)\n![ted1](./images/000046_ted_concat.webp)\n![ted2](./images/000047_ted_concat.webp)\n![ted3](./images/000048_ted_concat.webp)\n![ted4](./images/000049_ted_concat.webp)\n### Segment\n![segment0](./images/000050_seg_concat.webp)\n![segment1](./images/000051_seg_concat.webp)\n![segment2](./images/000052_seg_concat.webp)\n![segment3](./images/000053_seg_concat.webp)\n![segment4](./images/000054_seg_concat.webp)\n### Normal\n![normal0](./images/000055_normal_concat.webp)\n![normal1](./images/000056_normal_concat.webp)\n![normal2](./images/000057_normal_concat.webp)\n![normal3](./images/000058_normal_concat.webp)\n![normal4](./images/000059_normal_concat.webp)\n\n## Multi Control Visual Examples\n### Openpose + Canny\n![pose_canny0](./images/000007_openpose_canny_concat.webp)\n![pose_canny1](./images/000008_openpose_canny_concat.webp)\n![pose_canny2](./images/000009_openpose_canny_concat.webp)\n![pose_canny3](./images/000010_openpose_canny_concat.webp)\n![pose_canny4](./images/000011_openpose_canny_concat.webp)\n![pose_canny5](./images/000012_openpose_canny_concat.webp)\n\n### Openpose + Depth\n![pose_depth0](./images/000013_openpose_depth_concat.webp)\n![pose_depth1](./images/000014_openpose_depth_concat.webp)\n![pose_depth2](./images/000015_openpose_depth_concat.webp)\n![pose_depth3](./images/000016_openpose_depth_concat.webp)\n![pose_depth4](./images/000017_openpose_depth_concat.webp)\n![pose_depth5](./images/000018_openpose_depth_concat.webp)\n\n### Openpose + Scribble\n![pose_scribble0](./images/000001_openpose_scribble_concat.webp)\n![pose_scribble1](./images/000002_openpose_scribble_concat.webp)\n![pose_scribble2](./images/000003_openpose_scribble_concat.webp)\n![pose_scribble3](./images/000004_openpose_scribble_concat.webp)\n![pose_scribble4](./images/000005_openpose_scribble_concat.webp)\n![pose_scribble5](./images/000006_openpose_scribble_concat.webp)\n\n### Openpose + Normal\n![pose_normal0](./images/000019_openpose_normal_concat.webp)\n![pose_normal1](./images/000020_openpose_normal_concat.webp)\n![pose_normal2](./images/000021_openpose_normal_concat.webp)\n![pose_normal3](./images/000022_openpose_normal_concat.webp)\n![pose_normal4](./images/000023_openpose_normal_concat.webp)\n![pose_normal5](./images/000024_openpose_normal_concat.webp)\n\n### Openpose + Segment\n![pose_segment0](./images/000025_openpose_sam_concat.webp)\n![pose_segment1](./images/000026_openpose_sam_concat.webp)\n![pose_segment2](./images/000027_openpose_sam_concat.webp)\n![pose_segment3](./images/000028_openpose_sam_concat.webp)\n![pose_segment4](./images/000029_openpose_sam_concat.webp)\n![pose_segment5](./images/000030_openpose_sam_concat.webp)","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/xinsir/controlnet-union-sdxl-1.0"}]},{"id":"docling-project/SmolDocling-256M-preview","name":"SmolDocling-256M-preview","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","onnx","safetensors","idefics3","image-to-text","image-text-to-text","conversational","en","dataset:ds4sd/SynthCodeNet","dataset:ds4sd/SynthFormulaNet","dataset:ds4sd/SynthChartNet","dataset:HuggingFaceM4/DoclingMatix","arxiv:2503.11576","arxiv:2305.03393","base_model:HuggingFaceTB/SmolVLM-256M-Instruct","base_model:quantized:HuggingFaceTB/SmolVLM-256M-Instruct","license:cdla-permissive-2.0","endpoints_compatible","region:us"],"likes":1591,"downloads":364646,"readme":"---\nbase_model:\n- HuggingFaceTB/SmolVLM-256M-Instruct\nlanguage:\n- en\nlibrary_name: transformers\nlicense: cdla-permissive-2.0\npipeline_tag: image-text-to-text\ndatasets:\n- ds4sd/SynthCodeNet\n- ds4sd/SynthFormulaNet\n- ds4sd/SynthChartNet\n- HuggingFaceM4/DoclingMatix\n---\n\n<div style=\"\n  background-color: #f0f9ff;\n  border: 1px solid #bae6fd;\n  color: #0369a1;\n  padding: 12px 16px;\n  border-radius: 12px;\n  margin-bottom: 16px;\n  font-family: sans-serif;\n\">\n  <strong>📢 New Release:</strong>  \n  We’ve released <a href=\"https://huggingface.co/ibm-granite/granite-docling-258M\" target=\"_blank\" style=\"color:#0284c7; font-weight:bold; text-decoration:underline;\">\n    granite-docling-258M</a>, the successor to <b>SmolDocling</b>. It will now receive updates and support, check it out!\n</div>\n\n<div style=\"display: flex; align-items: center;\">\n    <img src=\"https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/assets/SmolDocling_doctags1.png\" alt=\"SmolDocling\" style=\"width: 200px; height: auto; margin-right: 20px;\">\n    <div>\n        <h3>SmolDocling-256M-preview</h3>\n        <p>SmolDocling is a multimodal Image-Text-to-Text model designed for efficient document conversion. It retains Docling's most popular features while ensuring full compatibility with Docling through seamless support for <strong>DoclingDocuments</strong>.</p>\n    </div>\n</div>\n\nThis model was presented in the paper [SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion](https://huggingface.co/papers/2503.11576).\n\n### 🚀 Features:  \n- 🏷️ **DocTags for Efficient Tokenization** – Introduces DocTags an efficient and minimal representation for documents that is fully compatible with **DoclingDocuments**.  \n- 🔍 **OCR (Optical Character Recognition)** – Extracts text accurately from images.  \n- 📐 **Layout and Localization** – Preserves document structure and document element **bounding boxes**.  \n- 💻 **Code Recognition** – Detects and formats code blocks including identation.  \n- 🔢 **Formula Recognition** – Identifies and processes mathematical expressions.  \n- 📊 **Chart Recognition** – Extracts and interprets chart data.  \n- 📑 **Table Recognition** – Supports column and row headers for structured table extraction.  \n- 🖼️ **Figure Classification** – Differentiates figures and graphical elements.  \n- 📝 **Caption Correspondence** – Links captions to relevant images and figures.  \n- 📜 **List Grouping** – Organizes and structures list elements correctly.  \n- 📄 **Full-Page Conversion** – Processes entire pages for comprehensive document conversion including all page elements (code, equations, tables, charts etc.) \n- 🔲 **OCR with Bounding Boxes** – OCR regions using a bounding box.\n- 📂 **General Document Processing** – Trained for both scientific and non-scientific documents.  \n- 🔄 **Seamless Docling Integration** – Import into **Docling** and export in multiple formats.\n- 💨 **Fast inference using VLLM** – Avg of 0.35 secs per page on A100 GPU.\n\n### 🚧 *Coming soon!*\n- 📊 **Better chart recognition 🛠️**\n- 📚 **One shot multi-page inference ⏱️**\n- 🧪 **Chemical Recognition**\n- 📙 **Datasets**\n\n## ⌨️ Get started (code examples)\n\nYou can use **transformers**, **vllm**, or **onnx** to perform inference, and [Docling](https://github.com/docling-project/docling) to convert results to variety of output formats (md, html, etc.):\n\n<details>\n<summary>📄 Single page image inference using Tranformers 🤖</summary>\n\n```python\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load images\nimage = load_image(\"https://upload.wikimedia.org/wikipedia/commons/7/76/GazettedeFrance.jpg\")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    \"ds4sd/SmolDocling-256M-preview\",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n\n# export as any format\n# HTML\n# Path(\"Out/\").mkdir(parents=True, exist_ok=True)\n# output_path_html = Path(\"Out/\") / \"example.html\"\n# doc.save_as_html(output_path_html)\n# MD\nprint(doc.export_to_markdown())\n```\n</details>\n\n\n<details>\n<summary> 🚀 Fast Batch Inference Using VLLM</summary>\n\n```python\n# Prerequisites:\n# pip install vllm\n# pip install docling_core\n# place page images you want to convert into \"img/\" dir\n\nimport time\nimport os\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom pathlib import Path\n\n# Configuration\nMODEL_PATH = \"ds4sd/SmolDocling-256M-preview\"\nIMAGE_DIR = \"img/\"  # Place your page images here\nOUTPUT_DIR = \"out/\"\nPROMPT_TEXT = \"Convert page to Docling.\"\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Initialize LLM\nllm = LLM(model=MODEL_PATH, limit_mm_per_prompt={\"image\": 1})\n\nsampling_params = SamplingParams(\n    temperature=0.0,\n    max_tokens=8192)\n\nchat_template = f\"<|im_start|>User:<image>{PROMPT_TEXT}<end_of_utterance>\nAssistant:\"\n\nimage_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n\nstart_time = time.time()\ntotal_tokens = 0\n\nfor idx, img_file in enumerate(image_files, 1):\n    img_path = os.path.join(IMAGE_DIR, img_file)\n    image = Image.open(img_path).convert(\"RGB\")\n\n    llm_input = {\"prompt\": chat_template, \"multi_modal_data\": {\"image\": image}}\n    output = llm.generate([llm_input], sampling_params=sampling_params)[0]\n    \n    doctags = output.outputs[0].text\n    img_fn = os.path.splitext(img_file)[0]\n    output_filename = img_fn + \".dt\"\n    output_path = os.path.join(OUTPUT_DIR, output_filename)\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(doctags)\n\n    # To convert to Docling Document, MD, HTML, etc.:\n    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n    # export as any format\n    # HTML\n    # output_path_html = Path(OUTPUT_DIR) / f\"{img_fn}.html\"\n    # doc.save_as_html(output_path_html)\n    # MD\n    output_path_md = Path(OUTPUT_DIR) / f\"{img_fn}.md\"\n    doc.save_as_markdown(output_path_md)\nprint(f\"Total time: {time.time() - start_time:.2f} sec\")\n```\n</details>\n<details>\n<summary> ONNX Inference</summary>\n\n```python\n# Prerequisites:\n# pip install onnxruntime\n# pip install onnxruntime-gpu\nfrom transformers import AutoConfig, AutoProcessor\nfrom transformers.image_utils import load_image\nimport onnxruntime\nimport numpy as np\nimport os\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\n\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n# cuda\nos.environ[\"ORT_CUDA_USE_MAX_WORKSPACE\"] = \"1\"\n\n# 1. Load models\n## Load config and processor\nmodel_id = \"ds4sd/SmolDocling-256M-preview\"\nconfig = AutoConfig.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n## Load sessions\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/vision_encoder.onnx\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/embed_tokens.onnx\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/decoder_model_merged.onnx\n# cpu\n# vision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\")\n# embed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\")\n# decoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\"\n\n# cuda\nvision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\", providers=[\"CUDAExecutionProvider\"])\nembed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\", providers=[\"CUDAExecutionProvider\"])\ndecoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\", providers=[\"CUDAExecutionProvider\"])\n\n## Set config values\nnum_key_value_heads = config.text_config.num_key_value_heads\nhead_dim = config.text_config.head_dim\nnum_hidden_layers = config.text_config.num_hidden_layers\neos_token_id = config.text_config.eos_token_id\nimage_token_id = config.image_token_id\nend_of_utterance_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_utterance>\")\n\n# 2. Prepare inputs\n## Create input messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n        ]\n    },\n]\n\n## Load image and apply processor\nimage = load_image(\"https://ibm.biz/docling-page-with-table\")\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"np\")\n\n## Prepare decoder inputs\nbatch_size = inputs['input_ids'].shape[0]\npast_key_values = {\n    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n    for layer in range(num_hidden_layers)\n    for kv in ('key', 'value')\n}\nimage_features = None\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nposition_ids = np.cumsum(inputs['attention_mask'], axis=-1)\n\n\n# 3. Generation loop\nmax_new_tokens = 8192\ngenerated_tokens = np.array([[]], dtype=np.int64)\nfor i in range(max_new_tokens):\n  inputs_embeds = embed_session.run(None, {'input_ids': input_ids})[0]\n\n  if image_features is None:\n    ## Only compute vision features if not already computed\n    image_features = vision_session.run(\n        ['image_features'],  # List of output names or indices\n        {\n            'pixel_values': inputs['pixel_values'],\n            'pixel_attention_mask': inputs['pixel_attention_mask'].astype(np.bool_)\n        }\n    )[0]\n    \n    ## Merge text and vision embeddings\n    inputs_embeds[inputs['input_ids'] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])\n\n  logits, *present_key_values = decoder_session.run(None, dict(\n      inputs_embeds=inputs_embeds,\n      attention_mask=attention_mask,\n      position_ids=position_ids,\n      **past_key_values,\n  ))\n\n  ## Update values for next generation loop\n  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n  attention_mask = np.ones_like(input_ids)\n  position_ids = position_ids[:, -1:] + 1\n  for j, key in enumerate(past_key_values):\n    past_key_values[key] = present_key_values[j]\n\n  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n  if (input_ids == eos_token_id).all() or (input_ids == end_of_utterance_id).all():\n    break  # Stop predicting\n\ndoctags = processor.batch_decode(\n    generated_tokens,\n    skip_special_tokens=False,\n)[0].lstrip()\n\nprint(doctags)\n\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n\nprint(doc.export_to_markdown())\n```\n</details>\n\n\n💻 Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ds4sd/SmolDocling-256M-preview-mlx-bf16)\n\n## DocTags\n\n<img src=\"https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/assets/doctags_v2.png\" width=\"800\" height=\"auto\" alt=\"Image description\">\nDocTags create a clear and structured system of tags and rules that separate text from the document's structure. This makes things easier for Image-to-Sequence models by reducing confusion. On the other hand, converting directly to formats like HTML or Markdown can be messy—it often loses details, doesn’t clearly show the document’s layout, and increases the number of tokens, making processing less efficient.\nDocTags are integrated with Docling, which allows export to HTML, Markdown, and JSON. These exports can be offloaded to the CPU, reducing token generation overhead and improving efficiency.\n\n## Supported Instructions\n\n<table>\n  <tr>\n    <td><b>Description</b></td>\n    <td><b>Instruction</b></td>\n    <td><b>Comment</b></td>\n  </tr>\n  <tr>\n    <td><b>Full conversion</b></td>\n    <td>Convert this page to docling.</td>\n    <td>DocTags represetation</td>\n  </tr>\n  <tr>\n    <td><b>Chart</b></td>\n    <td>Convert chart to table.</td>\n    <td>(e.g., &lt;chart&gt;)</td>\n  </tr>\n  <tr>\n    <td><b>Formula</b></td>\n    <td>Convert formula to LaTeX.</td>\n    <td>(e.g., &lt;formula&gt;)</td>\n  </tr>\n  <tr>\n    <td><b>Code</b></td>\n    <td>Convert code to text.</td>\n    <td>(e.g., &lt;code&gt;)</td>\n  </tr>\n  <tr>\n    <td><b>Table</b></td>\n    <td>Convert table to OTSL.</td>\n    <td>(e.g., &lt;otsl&gt;) OTSL: <a href=\"https://arxiv.org/pdf/2305.03393\">Lysak et al., 2023</a></td>\n  </tr>\n  <tr>\n    <td rowspan=4><b>Actions and Pipelines</b></td>\n    <td>OCR the text in a specific location: &lt;loc_155&gt;&lt;loc_233&gt;&lt;loc_206&gt;&lt;loc_237&gt;</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>Identify element at: &lt;loc_247&gt;&lt;loc_482&gt;&lt;10c_252&gt;&lt;loc_486&gt;</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>Find all 'text' elements on the page, retrieve all section headers.</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>Detect footer elements on the page.</td>\n    <td></td>\n  </tr>\n</table>\n\n\n#### 📊 Datasets\n- [SynthCodeNet](https://huggingface.co/datasets/ds4sd/SynthCodeNet)\n- [SynthFormulaNet](https://huggingface.co/datasets/ds4sd/SynthFormulaNet)\n- [SynthChartNet](https://huggingface.co/datasets/ds4sd/SynthChartNet)\n- [DoclingMatix](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix)\n\n#### Model Summary\n\n- **Developed by:** Docling Team, IBM Research\n- **Model type:** Multi-modal model (image+text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n- **Finetuned from model:** Based on [SmolVLM-256M-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct)\n\n**Repository:** [Docling](https://github.com/docling-project/docling)\n\n**Paper:** [arXiv](https://arxiv.org/abs/2503.11576)\n\n**Project Page:** [Hugging Face](https://huggingface.co/ds4sd/SmolDocling-256M-preview)\n\n**Citation:**\n```\n@misc{nassar2025smoldoclingultracompactvisionlanguagemodel,\n      title={SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion}, \n      author={Ahmed Nassar and Andres Marafioti and Matteo Omenetti and Maksym Lysak and Nikolaos Livathinos and Christoph Auer and Lucas Morin and Rafael Teixeira de Lima and Yusik Kim and A. Said Gurbuz and Michele Dolfi and Miquel Farré and Peter W. J. Staar},\n      year={2025},\n      eprint={2503.11576},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.11576}, \n}\n```\n**Demo:** [HF Space](https://huggingface.co/spaces/ds4sd/SmolDocling-256M-Demo)","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/docling-project/SmolDocling-256M-preview"}]},{"id":"gsdf/Counterfeit-V2.5","name":"Counterfeit-V2.5","description":"A model for text-to-image.","task":"text-to-image","tags":["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","license:creativeml-openrail-m","autotrain_compatible","endpoints_compatible","diffusers:StableDiffusionPipeline","region:us"],"likes":1578,"downloads":4680,"readme":"---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- diffusers\ninference: true\n---\n# Update\nV2.5 has been updated for ease of use as anime-style model.  \nI use this embedding for negative prompts.  \nhttps://huggingface.co/datasets/gsdf/EasyNegative  \n  \nShare by-products  \nV2.1…Feeling of use similar to V2.0  \nV2.2…NSFW model\n  \n# Counterfeit-V2.5 e.g. \n![sample1](https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/V2.5_sample/sample01.png)\n```\n((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field  \nNegative prompt: EasyNegative, extra fingers,fewer fingers,  \nSteps: 20, Sampler: DPM++ 2M Karras, CFG scale: 10, Size: 448x768, Denoising strength: 0.6, Hires upscale: 1.8, Hires upscaler: Latent\n```\n\n![sample2](https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/V2.5_sample/sample02.png)\n```\n((masterpiece,best quality)),1girl, from below, solo, school uniform, serafuku, sky, cloud, black hair, skirt, sailor collar, looking at viewer, short hair, building, bangs, neckerchief, long sleeves, cloudy sky, power lines, shirt, cityscape, pleated skirt, scenery, blunt bangs, city, night, black sailor collar, closed mouth, black skirt, medium hair, school bag , holding bag  \nNegative prompt: EasyNegative, extra fingers,fewer fingers,  \nSteps: 20, Sampler: DPM++ 2M Karras, CFG scale: 10, Size: 832x512, Denoising strength: 0.6, Hires upscale: 1.8, Hires upscaler: Latent\n```\n\n![sample3](https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/V2.5_sample/sample03.png)\n```\n((masterpiece,best quality)),2girls, black kimono, black legwear, black ribbon, black hair, cherry blossoms, day, flower, hair bun, hair ribbon, japanese clothes, kimono, long hair, looking at viewer, looking back, multiple girls, obi, outdoors, red eyes, red hair, ribbon, sandals, single hair bun, stairs, standing, statue, torii, tree, white kimono, yellow eyes   \nNegative prompt: EasyNegative, extra fingers,fewer fingers,  \nSteps: 20, Sampler: DPM++ 2M Karras, CFG scale: 10, Size: 640x960, Denoising strength: 0.58, Hires upscale: 1.8, Hires upscaler: Latent\n```\n  \n![sample4](https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/V2.5_sample/sample04.png)\n```\n((masterpiece,best quality)),1girl, bangs, blue eyes, blurry background, branch, brown hair, dappled sunlight, flower, from side, hair flower, hair ornament, japanese clothes, kimono, leaf, (maple leaf:1.9), obi, outdoors, sash, solo, sunlight, upper body   \nNegative prompt: EasyNegative, extra fingers,fewer fingers,  \nSteps: 20, Sampler: DPM++ 2M Karras, CFG scale: 10, Size: 864x512, Denoising strength: 0.58, Hires upscale: 1.8, Hires upscaler: Latent\n```\n  \n![sample5](https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/V2.5_sample/sample05.png)\n```\n((masterpiece,best quality))1girl, solo, black skirt, blue eyes, electric guitar, guitar, headphones, holding, holding plectrum, instrument, long hair, , music, one side up, pink hair, playing guiter, pleated skirt, black shirt, indoors   \nNegative prompt: EasyNegative, extra fingers,fewer fingers,  \nSteps: 20, Sampler: DPM++ 2M Karras, CFG scale: 10, Size: 864x512, Denoising strength: 0.58, Hires upscale: 1.8, Hires upscaler: Latent\n```\n  \n![sample6](https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/V2.5_sample/sample06.png)\n```\n((masterpiece,best quality)), 1girl, food, fruit, solo, skirt, shop, indoors, jacket, shopping, basket, jewelry, shirt, shelf, short hair, black hair, plaid skirt, black jacket, dutch angle, yellow eyes, looking at viewer   \nNegative prompt: EasyNegative, extra fingers,fewer fingers,  \nSteps: 20, Sampler: DPM++ 2M Karras, CFG scale: 10, Size: 864x512, Denoising strength: 0.58, Hires upscale: 1.8, Hires upscaler: Latent\n```\n  \n\n\n\n\n\n\n\n\n\n\n\n\n","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/gsdf/Counterfeit-V2.5"}]},{"id":"nanonets/Nanonets-OCR-s","name":"Nanonets-OCR-s","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","safetensors","qwen2_5_vl","image-to-text","OCR","pdf2markdown","image-text-to-text","conversational","en","base_model:Qwen/Qwen2.5-VL-3B-Instruct","base_model:finetune:Qwen/Qwen2.5-VL-3B-Instruct","text-generation-inference","endpoints_compatible","region:us"],"likes":1551,"downloads":166325,"readme":"---\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-VL-3B-Instruct\npipeline_tag: image-text-to-text\ntags:\n- OCR\n- pdf2markdown\nlibrary_name: transformers\n---\n\n\nNanonets-OCR-s by [Nanonets](https://nanonets.com) is a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction. It transforms documents into structured markdown with intelligent content recognition and semantic tagging, making it ideal for downstream processing by Large Language Models (LLMs).\n\nNanonets-OCR-s is packed with features designed to handle complex documents with ease:\n\n* **LaTeX Equation Recognition:** Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (`$...$`) and display (`$$...$$`) equations.\n* **Intelligent Image Description:** Describes images within documents using structured `<img>` tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.\n* **Signature Detection & Isolation:** Identifies and isolates signatures from other text, outputting them within a `<signature>` tag. This is crucial for processing legal and business documents.\n* **Watermark Extraction:** Detects and extracts watermark text from documents, placing it within a `<watermark>` tag.\n* **Smart Checkbox Handling:** Converts form checkboxes and radio buttons into standardized Unicode symbols (`☐`, `☑`, `☒`) for consistent and reliable processing.\n* **Complex Table Extraction:** Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.\n\n\n📢 [Read the full announcement](https://nanonets.com/research/nanonets-ocr-s) | 🤗 [Hugging Face Space Demo](https://huggingface.co/spaces/Souvik3333/Nanonets-ocr-s)\n\n## Usage\n### Using transformers\n```python\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n\nmodel_path = \"nanonets/Nanonets-OCR-s\"\n\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_path, \n    torch_dtype=\"auto\", \n    device_map=\"auto\", \n    attn_implementation=\"flash_attention_2\"\n)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprocessor = AutoProcessor.from_pretrained(model_path)\n\n\ndef ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):\n    prompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.\"\"\"\n    image = Image.open(image_path)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n            {\"type\": \"text\", \"text\": prompt},\n        ]},\n    ]\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n    inputs = inputs.to(model.device)\n    \n    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n    \n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    return output_text[0]\n\nimage_path = \"/path/to/your/document.jpg\"\nresult = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)\nprint(result)\n```\n\n### Using vLLM\n1. Start the vLLM server.\n```bash\nvllm serve nanonets/Nanonets-OCR-s\n```\n2. Predict with the model\n```python\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(api_key=\"123\", base_url=\"http://localhost:8000/v1\")\n\nmodel = \"nanonets/Nanonets-OCR-s\"\n\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\ndef ocr_page_with_nanonets_s(img_base64):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.\",\n                    },\n                ],\n            }\n        ],\n        temperature=0.0,\n        max_tokens=15000\n    )\n    return response.choices[0].message.content\n\ntest_img_path = \"/path/to/your/document.jpg\"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_nanonets_s(img_base64))\n```\n\n### Using docext\n```python\npip install docext\npython -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s\n```\nCheckout [GitHub](https://github.com/NanoNets/docext/tree/dev/markdown) for more details.\n\n\n## BibTex\n```\n@misc{Nanonets-OCR-S,\n  title={Nanonets-OCR-S: A model for transforming documents into structured markdown with intelligent content recognition and semantic tagging},\n  author={Souvik Mandal and Ashish Talewar and Paras Ahuja and Prathamesh Juvatkar},\n  year={2025},\n}\n```","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/nanonets/Nanonets-OCR-s"}]},{"id":"meta-llama/Llama-3.2-11B-Vision-Instruct","name":"Llama-3.2-11B-Vision-Instruct","description":"A model for image-text-to-text.","task":"image-text-to-text","tags":["transformers","safetensors","mllama","image-to-text","facebook","meta","pytorch","llama","llama-3","image-text-to-text","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.2","text-generation-inference","endpoints_compatible","region:us"],"likes":1541,"downloads":262133,"readme":null,"sources":[{"platform":"Hugging Face","url":"https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct"}]},{"id":"microsoft/Phi-4-multimodal-instruct","name":"Phi-4-multimodal-instruct","description":"A model for automatic-speech-recognition.","task":"automatic-speech-recognition","tags":["transformers","safetensors","phi4mm","text-generation","nlp","code","audio","automatic-speech-recognition","speech-summarization","speech-translation","visual-question-answering","phi-4-multimodal","phi","phi-4-mini","custom_code","multilingual","ar","zh","cs","da","nl","en","fi","fr","de","he","hu","it","ja","ko","no","pl","pt","ru","es","sv","th","tr","uk","arxiv:2503.01743","arxiv:2407.13833","license:mit","autotrain_compatible","region:us"],"likes":1527,"downloads":380689,"readme":"---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\n- ar\n- zh\n- cs\n- da\n- nl\n- en\n- fi\n- fr\n- de\n- he\n- hu\n- it\n- ja\n- ko\n- no\n- pl\n- pt\n- ru\n- es\n- sv\n- th\n- tr\n- uk\ntags:\n- nlp\n- code\n- audio\n- automatic-speech-recognition\n- speech-summarization\n- speech-translation\n- visual-question-answering\n- phi-4-multimodal\n- phi\n- phi-4-mini\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n- messages:\n  - role: user\n    content: Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\nlibrary_name: transformers\npaper: https://arxiv.org/abs/2503.01743\n---\n🎉**Phi-4**: [[mini-reasoning](https://huggingface.co/microsoft/Phi-4-mini-reasoning) | [reasoning](https://huggingface.co/microsoft/Phi-4-reasoning)] | [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n\n\n## Model Summary\n\nPhi-4-multimodal-instruct is a lightweight open multimodal foundation\nmodel that leverages the language, vision, and speech research\nand datasets used for Phi-3.5 and 4.0 models. The model processes text,\nimage, and audio inputs, generating text outputs, and comes with\n128K token context length. The model underwent an enhancement process,\nincorporating both supervised fine-tuning, direct preference\noptimization and RLHF (Reinforcement Learning from Human Feedback)\nto support precise instruction adherence and safety measures.\nThe languages that each modal supports are the following:\n- Text: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish,\nFrench, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian,\nPolish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\n- Vision: English\n- Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese\n\n📰 [Phi-4-multimodal Microsoft Blog](https://aka.ms/phi4-feb2025) <br>\n📖 [Phi-4-multimodal Technical Report](https://arxiv.org/abs/2503.01743) <br>\n🏡 [Phi Portal](https://aka.ms/phi-4-multimodal/azure) <br>\n👩‍🍳 [Phi Cookbook](https://github.com/microsoft/PhiCookBook) <br>\n🖥️ Try It on [Azure](https://aka.ms/phi-4-multimodal/azure), \n[GitHub](https://github.com/marketplace/models/azureml/Phi-4-multimodal-instruct/playground),\n[Nvidia](https://aka.ms/phi-4-multimodal/nvidia),\n[Huggingface](https://huggingface.co/spaces/microsoft/phi-4-multimodal) playgrounds<br>\n📱Huggingface Spaces \n[Thoughts Organizer](https://huggingface.co/spaces/microsoft/ThoughtsOrganizer), \n[Stories Come Alive](https://huggingface.co/spaces/microsoft/StoriesComeAlive), \n[Phine Speech Translator](https://huggingface.co/spaces/microsoft/PhineSpeechTranslator) <br>\n\n\nWatch as Phi-4 Multimodal analyzes spoken language to help plan a trip to Seattle, demonstrating its advanced audio processing and recommendation capabilities.\n\n<div style=\"width: 800px; height: 400px; margin: 0 auto;\">\n  <video autoplay muted loop controls playsinline style=\"width: 100%; height: 100%; object-fit: contain;\">\n    <source src=\"https://github.com/nguyenbh/phi4mm-demos/raw/refs/heads/main/clips/Phi-4-multimodal_SeattleTrip.mp4\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\nSee how Phi-4 Multimodal tackles complex mathematical problems through visual inputs, demonstrating its ability to process and solve equations presented in images.\n<div style=\"width: 800px; height: 400px; margin: 0 auto;\">\n  <video autoplay muted loop controls playsinline style=\"width: 100%; height: 100%; object-fit: contain;\">\n    <source src=\"https://github.com/nguyenbh/phi4mm-demos/raw/refs/heads/main/clips/Phi-4-multimodal_Math.mp4\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\nExplore how Phi-4 Mini functions as an intelligent agent, showcasing its reasoning and task execution abilities in complex scenarios.\n<div style=\"width: 800px; height: 400px; margin: 0 auto;\">\n  <video autoplay muted loop controls playsinline style=\"width: 100%; height: 100%; object-fit: contain;\">\n    <source src=\"https://github.com/nguyenbh/phi4mm-demos/raw/refs/heads/main/clips/Phi-4-mini_Agents.mp4\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for broad multilingual and multimodal commercial and research use . The model provides uses for general purpose AI systems and applications which require\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially math and logic)\n4) Function and tool calling\n5) General image understanding\n6) Optical character recognition\n7) Chart and table understanding\n8) Multiple image comparison\n9) Multi-image or video clip summarization\n10) Speech recognition\n11) Speech translation\n12) Speech QA\n13) Speech summarization\n14) Audio understanding\n\nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nThe model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models and multimodal models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case. \n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Release Notes \n\nThis release of Phi-4-multimodal-instruct is based on valuable user feedback from the Phi-3 series. Previously, users could use a speech recognition model to talk to the Mini and Vision models. To achieve this, users needed to use a pipeline of two models: one model to transcribe the audio to text, and another model for the language or vision tasks. This pipeline means that the core model was not provided the full breadth of input information – e.g. cannot directly observe multiple speakers, background noises, jointly align speech, vision, language information at the same time on the same representation space.\nWith Phi-4-multimodal-instruct, a single new open model has been trained across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. The model  employed new architecture, larger vocabulary for efficiency, multilingual, and multimodal support, and better post-training techniques were used for instruction following and function calling, as well as additional data leading to substantial gains on key multimodal capabilities.\nIt is anticipated that Phi-4-multimodal-instruct will greatly benefit app developers and various use cases. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4 is welcomed and crucial to the model's evolution and improvement. Thank you for being part of this journey!\n\n## Model Quality\n<details>\n  <summary>Click to view details</summary>\n\nTo understand the capabilities, Phi-4-multimodal-instruct  was compared with a set of models over a variety of benchmarks using an internal benchmark platform (See Appendix A for benchmark methodology). Users can refer to the Phi-4-Mini-Instruct model card for details of language benchmarks. At the high-level overview of the model quality on representative speech and vision benchmarks:\n\n### Speech\n\nThe Phi-4-multimodal-instruct was observed as\n- Having strong automatic speech recognition (ASR) and speech translation (ST) performance, surpassing expert ASR model WhisperV3 and ST models SeamlessM4T-v2-Large. \n- Ranking number 1 on the [Huggingface OpenASR](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) leaderboard with word error rate 6.14% in comparison with the current best model 6.5% as of March 04, 2025. \n- Being the first open-sourced model that can perform speech summarization, and the performance is close to GPT4o.\n- Having a gap with close models, e.g. Gemini-1.5-Flash and GPT-4o-realtime-preview, on speech QA task. Work is being undertaken to improve this capability in the next iterations.\n\n#### Speech Recognition (lower is better)\n\nThe performance of Phi-4-multimodal-instruct on the aggregated benchmark datasets:\n![alt text](./figures/speech_recognition.png)\n\nThe performance of Phi-4-multimodal-instruct on different languages, averaging the WERs of CommonVoice and FLEURS:\n\n![alt text](./figures/speech_recog_by_lang.png)\n\n#### Speech Translation (higher is better)\n\nTranslating from German, Spanish, French, Italian, Japanese, Portugues, Chinese to English:\n\n![alt text](./figures/speech_translate.png)\n\nTranslating from English to German, Spanish, French, Italian, Japanese, Portugues, Chinese. Noted that WhiperV3 does not support this capability: \n\n![alt text](./figures/speech_translate_2.png)\n\n\n#### Speech Summarization (higher is better)\n\n![alt text](./figures/speech_summarization.png)\n\n#### Speech QA\n\nMT bench scores are scaled by 10x to match the score range of MMMLU:\n\n![alt text](./figures/speech_qa.png)\n\n#### Audio Understanding\n\nAIR bench scores are scaled by 10x to match the score range of MMAU:\n\n![alt text](./figures/audio_understand.png)\n\n### Vision\n\n#### Vision-Speech tasks\n\nPhi-4-multimodal-instruct is capable of processing both image and audio together, the following table shows the model quality when the input query for vision content is synthetic speech on chart/table understanding and document reasoning tasks. Compared to other existing state-of-the-art omni models that can enable audio and visual signal as input, Phi-4-multimodal-instruct achieves much stronger performance on multiple benchmarks.\n\n| Benchmarks            | Phi-4-multimodal-instruct | InternOmni-7B | Gemini-2.0-Flash-Lite-prv-02-05 | Gemini-2.0-Flash | Gemini-1.5-Pro |\n|-----------------------|--------------------------|---------------|--------------------------------|-----------------|----------------|\n| s_AI2D                | **68.9**                 | 53.9          | 62.0                           | **69.4**        | 67.7           |\n| s_ChartQA             | **69.0**                 | 56.1          | 35.5                           | 51.3            | 46.9           |\n| s_DocVQA              | **87.3**                 | 79.9          | 76.0                           | 80.3            | 78.2           |\n| s_InfoVQA             | **63.7**                 | 60.3          | 59.4                           | 63.6            | **66.1**       |\n| **Average**           | **72.2**                 | **62.6**      | **58.2**                       | **66.2**        | **64.7**       |\n\n### Vision tasks\nTo understand the vision capabilities, Phi-4-multimodal-instruct was compared with a set of models over a variety of zero-shot benchmarks using an internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\n\n| Dataset                          | Phi-4-multimodal-ins | Phi-3.5-vision-ins | Qwen 2.5-VL-3B-ins | Intern VL 2.5-4B | Qwen 2.5-VL-7B-ins | Intern VL 2.5-8B | Gemini 2.0-Flash Lite-preview-0205 | Gemini2.0-Flash | Claude-3.5-Sonnet-2024-10-22 | Gpt-4o-2024-11-20 |\n|----------------------------------|---------------------|-------------------|-------------------|-----------------|-------------------|-----------------|--------------------------------|-----------------|----------------------------|------------------|\n| **Popular aggregated benchmark** |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| MMMU                             | **55.1**            | 43.0              | 47.0              | 48.3            | 51.8              | 50.6            | 54.1                           | **64.7**        | 55.8                       | 61.7             |\n| MMBench (dev-en)                 | **86.7**            | 81.9              | 84.3              | 86.8            | 87.8              | 88.2            | 85.0                           | **90.0**        | 86.7                       | 89.0             |\n| MMMU-Pro (std/vision)            | **38.5**            | 21.8              | 29.9              | 32.4            | 36.9              | 34.4            | 45.1                           | **54.4**        | 54.3                       | 53.0             |\n| **Visual science reasoning**     |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| ScienceQA Visual (img-test)      | **97.5**            | 91.3              | 79.4              | 96.2            | 87.7              | **97.3**        | 85.0                           | 88.3            | 81.2                       | 88.2             |\n| **Visual math reasoning**        |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| MathVista (testmini)             | **62.4**            | 43.9              | 60.8              | 51.2            | **67.8**          | 56.7            | 57.6                           | 47.2            | 56.9                       | 56.1             |\n| InterGPS                         | **48.6**            | 36.3              | 48.3              | 53.7            | 52.7              | 54.1            | 57.9                           | **65.4**        | 47.1                       | 49.1             |\n| **Chart & table reasoning**      |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| AI2D                             | **82.3**            | 78.1              | 78.4              | 80.0            | 82.6              | 83.0            | 77.6                           | 82.1            | 70.6                       | **83.8**         |\n| ChartQA                          | **81.4**            | 81.8              | 80.0              | 79.1            | **85.0**          | 81.0            | 73.0                           | 79.0            | 78.4                       | 75.1             |\n| DocVQA                           | **93.2**            | 69.3              | 93.9              | 91.6            | **95.7**          | 93.0            | 91.2                           | 92.1            | 95.2                       | 90.9             |\n| InfoVQA                          | **72.7**            | 36.6              | 77.1              | 72.1            | **82.6**          | 77.6            | 73.0                           | 77.8            | 74.3                       | 71.9             |\n| **Document Intelligence**        |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| TextVQA (val)                    | **75.6**            | 72.0              | 76.8              | 70.9            | **77.7**          | 74.8            | 72.9                           | 74.4            | 58.6                       | 73.1             |\n| OCR Bench                        | **84.4**            | 63.8              | 82.2              | 71.6            | **87.7**          | 74.8            | 75.7                           | 81.0            | 77.0                       | 77.7             |\n| **Object visual presence verification** |              |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| POPE                             | **85.6**            | 86.1              | 87.9              | 89.4            | 87.5              | **89.1**        | 87.5                           | 88.0            | 82.6                       | 86.5             |\n| **Multi-image perception**       |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| BLINK                            | **61.3**            | 57.0              | 48.1              | 51.2            | 55.3              | 52.5            | 59.3                           | **64.0**        | 56.9                       | 62.4             |\n| Video MME 16 frames              | **55.0**            | 50.8              | 56.5              | 57.3            | 58.2              | 58.7            | 58.8                           | 65.5            | 60.2                       | **68.2**         |\n| **Average**                      | **72.0**            | **60.9**          | **68.7**          | **68.8**        | **73.1**          | **71.1**        | **70.2**                       | **74.3**        | **69.1**                   | **72.4**         |\n\n![alt text](./figures/vision_radar.png)\n\n#### Visual Perception\n\nBelow are the comparison results on existing multi-image tasks. On average, Phi-4-multimodal-instruct outperforms competitor models of the same size and competitive with much bigger models on multi-frame capabilities.\nBLINK is an aggregated benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs.\n\n| Dataset                    | Phi-4-multimodal-instruct | Qwen2.5-VL-3B-Instruct | InternVL 2.5-4B | Qwen2.5-VL-7B-Instruct | InternVL 2.5-8B | Gemini-2.0-Flash-Lite-prv-02-05 | Gemini-2.0-Flash | Claude-3.5-Sonnet-2024-10-22 | Gpt-4o-2024-11-20 |\n|----------------------------|--------------------------|----------------------|-----------------|----------------------|-----------------|--------------------------------|-----------------|----------------------------|------------------|\n| Art Style                  | **86.3**                 | 58.1                | 59.8           | 65.0                 | 65.0            | 76.9                           | 76.9            | 68.4                       | 73.5             |\n| Counting                   | **60.0**                 | 67.5                | 60.0           | 66.7                 | **71.7**        | 45.8                           | 69.2            | 60.8                       | 65.0             |\n| Forensic Detection         | **90.2**                 | 34.8                | 22.0           | 43.9                 | 37.9            | 31.8                           | 74.2            | 63.6                       | 71.2             |\n| Functional Correspondence  | **30.0**                 | 20.0                | 26.9           | 22.3                 | 27.7            | 48.5                           | **53.1**        | 34.6                       | 42.3             |\n| IQ Test                    | **22.7**                 | 25.3                | 28.7           | 28.7                 | 28.7            | 28.0                           | **30.7**        | 20.7                       | 25.3             |\n| Jigsaw                     | **68.7**                 | 52.0                | **71.3**       | 69.3                 | 53.3            | 62.7                           | 69.3            | 61.3                       | 68.7             |\n| Multi-View Reasoning       | **76.7**                 | 44.4                | 44.4           | 54.1                 | 45.1            | 55.6                           | 41.4            | 54.9                       | 54.1             |\n| Object Localization        | **52.5**                 | 55.7                | 53.3           | 55.7                 | 58.2            | 63.9                           | **67.2**        | 58.2                       | 65.6             |\n| Relative Depth             | **69.4**                 | 68.5                | 68.5           | 80.6                 | 76.6            | **81.5**                       | 72.6            | 66.1                       | 73.4             |\n| Relative Reflectance       | **26.9**                 | **38.8**            | **38.8**       | 32.8                 | **38.8**        | 33.6                           | 34.3            | 38.1                       | 38.1             |\n| Semantic Correspondence    | **52.5**                 | 32.4                | 33.8           | 28.8                 | 24.5            | **56.1**                       | 55.4            | 43.9                       | 47.5             |\n| Spatial Relation           | **72.7**                 | 80.4                | 86.0           | **88.8**             | 86.7            | 74.1                           | 79.0            | 74.8                       | 83.2             |\n| Visual Correspondence      | **67.4**                 | 28.5                | 39.5           | 50.0                 | 44.2            | 84.9                           | **91.3**        | 72.7                       | 82.6             |\n| Visual Similarity          | **86.7**                 | 67.4                | 88.1           | 87.4                 | 85.2            | **87.4**                       | 80.7            | 79.3                       | 83.0             |\n| **Overall**                | **61.6**                 | **48.1**            | **51.2**       | **55.3**             | **52.5**        | **59.3**                       | **64.0**        | **56.9**                   | **62.4**         |\n\n![alt text](./figures/multi_image.png)\n\n</details>\n\n## Usage\n\n### Requirements\n\nPhi-4 family has been integrated in the `4.48.2` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\nWe suggest to run with Python 3.10.\nExamples of required packages:\n```\nflash_attn==2.7.4.post1\ntorch==2.6.0\ntransformers==4.48.2\naccelerate==1.3.0\nsoundfile==0.13.1\npillow==11.1.0\nscipy==1.15.2\ntorchvision==0.21.0\nbackoff==2.2.1\npeft==0.13.2\n```\n\nPhi-4-multimodal-instruct is also available in [Azure AI Studio](https://aka.ms/phi-4-multimodal/azure)\n\n### Tokenizer\n\nPhi-4-multimodal-instruct supports a vocabulary size of up to `200064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\n\n### Input Formats\n\nGiven the nature of the training data, the Phi-4-multimodal-instruct model is best suited for prompts using the chat format as follows:\n\n#### Text chat format\n\nThis format is used for general conversation and instructions:\n\n`\n<|system|>You are a helpful assistant.<|end|><|user|>How to explain Internet for a medieval knight?<|end|><|assistant|>\n`\n\n#### Tool-enabled function-calling format\n\nThis format is used when the user wants the model to provide function calls based on\nthe given tools. The user should provide the available tools in the system prompt,\nwrapped by <|tool|> and <|/tool|> tokens. The tools should be specified in JSON format,\nusing a JSON dump structure. Example:\n\n`\n<|system|>You are a helpful assistant with some tools.<|tool|>[{\"name\": \"get_weather_updates\", \"description\": \"Fetches weather updates for a given city using the RapidAPI Weather API.\", \"parameters\": {\"city\": {\"description\": \"The name of the city for which to retrieve weather information.\", \"type\": \"str\", \"default\": \"London\"}}}]<|/tool|><|end|><|user|>What is the weather like in Paris today?<|end|><|assistant|>\n`\n\n#### Vision-Language Format\n\nThis format is used for conversation with image:\n\n`\n<|user|><|image_1|>Describe the image in detail.<|end|><|assistant|>\n`\n\nFor multiple images, the user needs to insert multiple image placeholders in the prompt as below:\n\n`\n<|user|><|image_1|><|image_2|><|image_3|>Summarize the content of the images.<|end|><|assistant|>\n`\n\n#### Speech-Language Format\n\nThis format is used for various speech and audio tasks:\n\n`\n<|user|><|audio_1|>{task prompt}<|end|><|assistant|>\n`\n\nThe task prompt can vary for different task.\nAutomatic Speech Recognition:\n\n`\n<|user|><|audio_1|>Transcribe the audio clip into text.<|end|><|assistant|>\n`\n\nAutomatic Speech Translation:\n\n`\n<|user|><|audio_1|>Translate the audio to {lang}.<|end|><|assistant|>\n`\n\nAutomatic Speech Translation with chain-of-thoughts:\n\n`\n<|user|><|audio_1|>Transcribe the audio to text, and then translate the audio to {lang}. Use <sep> as a separator between the original transcript and the translation.<|end|><|assistant|>\n`\n\nSpoken-query Question Answering:\n\n`\n<|user|><|audio_1|><|end|><|assistant|>\n`\n\n#### Vision-Speech Format\n\nThis format is used for conversation with image and audio.\nThe audio may contain query related to the image:\n\n`\n<|user|><|image_1|><|audio_1|><|end|><|assistant|>\n`\n\nFor multiple images, the user needs to insert multiple image placeholders in the prompt as below:\n\n`\n<|user|><|image_1|><|image_2|><|image_3|><|audio_1|><|end|><|assistant|>\n`\n\n**Vision**\n- Any common RGB/gray image format (e.g., (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")) can be supported.\n- Resolution depends on the GPU memory size. Higher resolution and more images will produce more tokens, thus using more GPU memory. During training, 64 crops can be supported.\nIf it is a square image, the resolution would be around (8*448 by 8*448). For multiple-images, at most 64 frames can be supported, but with more frames as input, the resolution of each frame needs to be reduced to fit in the memory.\n\n**Audio**\n- Any audio format that can be loaded by soundfile package should be supported.\n- To keep the satisfactory performance, maximum audio length is suggested to be 40s. For summarization tasks, the maximum audio length is suggested to 30 mins.\n\n\n### Loading the model locally\n\nAfter obtaining the Phi-4-multimodal-instruct model checkpoints, users can use this sample code for inference.\n\n<details>\n  <summary>Click to view details</summary>\n\n```python\nimport requests\nimport torch\nimport os\nimport io\nfrom PIL import Image\nimport soundfile as sf\nfrom transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\nfrom urllib.request import urlopen\n\n\n# Define model path\nmodel_path = \"microsoft/Phi-4-multimodal-instruct\"\n\n# Load model and processor\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    device_map=\"cuda\", \n    torch_dtype=\"auto\", \n    trust_remote_code=True,\n    # if you do not use Ampere or later GPUs, change attention to \"eager\"\n    _attn_implementation='flash_attention_2',\n).cuda()\n\n# Load generation config\ngeneration_config = GenerationConfig.from_pretrained(model_path)\n\n# Define prompt structure\nuser_prompt = '<|user|>'\nassistant_prompt = '<|assistant|>'\nprompt_suffix = '<|end|>'\n\n# Part 1: Image Processing\nprint(\"\\n--- IMAGE PROCESSING ---\")\nimage_url = 'https://www.ilankelman.org/stopsigns/australia.jpg'\nprompt = f'{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}'\nprint(f'>>> Prompt\\n{prompt}')\n\n# Download and open image\nimage = Image.open(requests.get(image_url, stream=True).raw)\ninputs = processor(text=prompt, images=image, return_tensors='pt').to('cuda:0')\n\n# Generate response\ngenerate_ids = model.generate(\n    **inputs,\n    max_new_tokens=1000,\n    generation_config=generation_config,\n)\ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(\n    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(f'>>> Response\\n{response}')\n\n# Part 2: Audio Processing\nprint(\"\\n--- AUDIO PROCESSING ---\")\naudio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\nspeech_prompt = \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\"\nprompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\nprint(f'>>> Prompt\\n{prompt}')\n\n# Downlowd and open audio file\naudio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n\n# Process with the model\ninputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors='pt').to('cuda:0')\n\ngenerate_ids = model.generate(\n    **inputs,\n    max_new_tokens=1000,\n    generation_config=generation_config,\n)\ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(\n    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(f'>>> Response\\n{response}')\n```\n</details>\n\nMore inference examples can be found [**here**](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/sample_inference_phi4mm.py).\n\n### vLLM inference\n\nUser can start a server with this command\n\n```bash\npython -m vllm.entrypoints.openai.api_server --model 'microsoft/Phi-4-multimodal-instruct' --dtype auto --trust-remote-code --max-model-len 131072 --enable-lora --max-lora-rank 320 --lora-extra-vocab-size 0 --limit-mm-per-prompt audio=3,image=3 --max-loras 2 --lora-modules speech=<path to speech lora folder> vision=<path to vision lora folder>\n```\n\nThe speech lora and vision lora folders are within the Phi-4-multimodal-instruct folder downloaded by vLLM, you can also use the following script to find thoses:\n\n```python\nfrom huggingface_hub import snapshot_download\nmodel_path = snapshot_download(repo_id=\"microsoft/Phi-4-multimodal-instruct\")\nspeech_lora_path = model_path+\"/speech-lora\"\nvision_lora_path = model_path+\"/vision-lora\"\n```\n\n## Training\n\n### Fine-tuning\n\nA basic example of supervised fine-tuning (SFT) for [**speech**](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/sample_finetune_speech.py) and [**vision**](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/sample_finetune_vision.py) is provided respectively.\n\nAn example on [**how to extend speech recognition to a new language**.](https://huggingface.co/microsoft/Phi-4-multimodal-instruct#appendix-b-fine-tuning-korean-speech)\n\n### Model\n\n+ **Architecture:** Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model. The model has the pretrained Phi-4-Mini-Instruct as the backbone language model, and the advanced encoders and adapters of vision and speech.<br>\n+ **Inputs:** Text, image, and audio. It is best suited for prompts using the chat format.<br>\n+ **Context length:** 128K tokens<br>\n+ **GPUs:** 512 A100-80G<br>\n+ **Training time:** 28 days<br>\n+ **Training data:** 5T tokens, 2.3M speech hours, and 1.1T image-text tokens<br>\n+ **Outputs:** Generated text in response to the input<br>\n+ **Dates:** Trained between December 2024 and January 2025<br>\n+ **Status:** This is a static model trained on offline datasets with the cutoff date of June 2024 for publicly available data.<br>\n+ **Supported languages:** \n  + Text: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n  + Vision: English<br>\n  + Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese<br>\n+ **Release date:** February 2025<br>\n\n### Training Datasets\n\nPhi-4-multimodal-instruct's training data includes a wide variety of sources, totaling 5 trillion text tokens, and is a combination of \n1) publicly available documents filtered for quality, selected high-quality educational data, and code\n2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.)\n3) high quality human labeled data in chat format\n4) selected high-quality image-text interleave data\n5) synthetic and publicly available image, multi-image, and video data\n6) anonymized in-house speech-text pair data with strong/weak transcriptions\n7) selected high-quality publicly available and anonymized in-house speech data with task-specific supervisions\n8) selected synthetic speech data\n9) synthetic vision-speech data.\n\nFocus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for large foundation models, but such information was removed for the Phi-4-multimodal-instruct to leave more model capacity for reasoning for the model's small size. The data collection process involved sourcing information from publicly available documents, with a focus on filtering out undesirable documents and images. To safeguard privacy, image and text data sources were filtered to remove or scrub potentially personal data from the training data.\nThe decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis. \n\n### Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n* [Accelerate](https://huggingface.co/docs/transformers/main/en/accelerate)\n* [soundfile](https://github.com/bastibe/python-soundfile)\n* [pillow](https://github.com/python-pillow/Pillow)\n\n### Hardware\nNote that by default, the Phi-4-multimodal-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with _attn_implementation=\"eager\"\n\n\n## Responsible AI Considerations\n<details>\n  <summary>Click to view detail descriptions</summary>\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n+ Quality of Service: The Phi models are trained primarily on English language content across text, speech, and visual inputs, with some additional multilingual coverage. Performance may vary significantly across different modalities and languages:\n  + Text: Languages other than English will experience reduced performance, with varying levels of degradation across different non-English languages. English language varieties with less representation in the training data may perform worse than standard American English.\n  + Speech: Speech recognition and processing shows similar language-based performance patterns, with optimal performance for standard American English accents and pronunciations. Other English accents, dialects, and non-English languages may experience lower recognition accuracy and response quality. Background noise, audio quality, and speaking speed can further impact performance.\n  + Vision: Visual processing capabilities may be influenced by cultural and geographical biases in the training data. The model may show reduced performance when analyzing images containing text in non-English languages or visual elements more commonly found in non-Western contexts. Image quality, lighting conditions, and composition can also affect processing accuracy.\n+ Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n+ Limited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, it is strongly recommended that users manually verify all API uses.\n+ Long Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift.\n+ Inference of Sensitive Attributes: The Phi 4 models can sometimes attempt to infer sensitive attributes (such as personality characteristics, country of origin, gender, etc...) from the users’ voices when specifically asked to do so. Phi 4-multimodal-instruct is not designed or intended to be used as a biometric categorization system to categorize individuals based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation. This behavior can be easily and efficiently mitigated at the application level by a system message.\n  \nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n</details>\n\n## Safety\n<details>\n  <summary>Click to view detail descriptions</summary>\n\nThe Phi-4 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed for safety alignment is a combination of SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization), and RLHF (Reinforcement Learning from Human Feedback) approaches by utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness, as well as various questions and answers targeted to multiple safety categories. For non-English languages, existing datasets were extended via machine translation. Speech Safety datasets were generated by running Text Safety datasets through Azure TTS (Text-To-Speech) Service, for both English and non-English languages. Vision (text & images) Safety datasets were created to cover harm categories identified both in public and internal multi-modal RAI datasets.\n\n### Safety Evaluation and Red-Teaming\n\nVarious evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 models' propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the [Phi 3 Safety Post-Training paper](https://arxiv.org/abs/2407.13833) had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the [Phi 3 Safety Post-Training paper](https://arxiv.org/abs/2407.13833). For this release, the red teaming effort focused on the newest Audio input modality and on the following safety areas: harmful content, self-injury risks, and exploits. The model was found to be more susceptible to providing undesirable outputs when attacked with context manipulation or persuasive techniques. These findings applied to all languages, with the persuasive techniques mostly affecting French and Italian. This highlights the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken.\n\n### Vision Safety Evaluation\n\nTo assess model safety in scenarios involving both text and images, Microsoft's Azure AI Evaluation SDK was utilized. This tool facilitates the simulation of single-turn conversations with the target model by providing prompt text and images designed to incite harmful responses. The target model's responses are subsequently evaluated by a capable model across multiple harm categories, including violence, sexual content, self-harm, hateful and unfair content, with each response scored based on the severity of the harm identified. The evaluation results were compared with those of Phi-3.5-Vision and open-source models of comparable size. In addition, we ran both an internal and the public RTVLM and VLGuard multi-modal (text & vision) RAI benchmarks, once again comparing scores with Phi-3.5-Vision and open-source models of comparable size. However, the model may be susceptible to language-specific attack prompts and cultural context.\n\n### Audio Safety Evaluation\n\nIn addition to extensive red teaming, the Safety of the model was assessed through three distinct evaluations. First, as performed with Text and Vision inputs, Microsoft's Azure AI Evaluation SDK was leveraged to detect the presence of harmful content in the model's responses to Speech prompts. Second, [Microsoft's Speech Fairness evaluation](https://speech.microsoft.com/portal/responsibleai/assess) was run to verify that Speech-To-Text transcription worked well across a variety of demographics. Third, we proposed and evaluated a mitigation approach via a system message to help prevent the model from inferring sensitive attributes (such as gender, sexual orientation, profession, medical condition, etc...) from the voice of a user.\n</details>\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n## Appendix A: Benchmark Methodology\n\n<details>\n  <summary>Click to view detail descriptions</summary>\n\nWe include a brief word on methodology here - and in particular, how we think about optimizing prompts.\nIn an ideal world, we would never change any prompts in our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date.\nThere are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given eval due to a failure to respect the output format. For example:\n\n+ A model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try different system messages (e.g. “You must always respond to a question” or “Get to the point!”).\n+ Some models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases.\n+ We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts.\n\nHowever, we do not:\n\n+ Pick different few-shot examples. Few shots will always be the same when comparing different models.\n+ Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice.\n\n### Vision Benchmark Settings\n\nThe goal of the benchmark setup is to measure the performance of the LMM when a regular user utilizes these models for a task involving visual input. To this end, we selected 9 popular and publicly available single-frame datasets and 3 multi-frame benchmarks that cover a wide range of challenging topics and tasks (e.g., mathematics, OCR tasks, charts-and-plots understanding, etc.) as well as a set of high-quality models. \nOur benchmarking setup utilizes zero-shot prompts and all the prompt content are the same for every model. We only formatted the prompt content to satisfy the model's prompt API. This ensures that our evaluation is fair across the set of models we tested. Many benchmarks necessitate models to choose their responses from a presented list of options. Therefore, we've included a directive in the prompt's conclusion, guiding all models to pick the option letter that corresponds to the answer they deem correct.\nIn terms of the visual input, we use the images from the benchmarks as they come from the original datasets. We converted these images to base-64 using a JPEG encoding for models that require this format (e.g., GPTV, Claude Sonnet 3.5, Gemini 1.5 Pro/Flash). For other models (e.g., Llava Interleave, and InternVL2 4B and 8B), we used their Huggingface interface and passed in PIL images or a JPEG image stored locally. We did not scale or pre-process images in any other way.\nLastly, we used the same code to extract answers and evaluate them using the same code for every considered model. This ensures that we are fair in assessing the quality of their answers.\n\n### Speech Benchmark Settings\n\nThe objective of this benchmarking setup is to assess the performance of models in speech and audio understanding tasks as utilized by regular users. To accomplish this, we selected several state-of-the-art open-sourced and closed-sourced models and performed evaluations across a variety of public and in-house benchmarks. These benchmarks encompass diverse and challenging topics, including Automatic Speech Recognition (ASR), Automatic Speech Translation (AST), Spoken Query Question Answering (SQQA), Audio Understanding (AU), and Speech Summarization.\nThe results are derived from evaluations conducted on identical test data without any further clarifications. All results were obtained without sampling during inference. For an accurate comparison, we employed consistent prompts for models across different tasks, except for certain model APIs (e.g., GPT-4o), which may refuse to respond to specific prompts for some tasks.\nIn conclusion, we used uniform code to extract answers and evaluate them for all considered models. This approach ensured fairness by assessing the quality of their responses.\n\n### Benchmark datasets\n\nThe model was evaluated across a breadth of public and internal benchmarks to understand it's capabilities under multiple tasks and conditions. While most evaluations use English, multilingual benchmark was incorporated to cover performance in select languages.  More specifically,\n+ Vision: \n  + Popular aggregated benchmark:\n    + MMMU and MMMU-Pro: massive multi-discipline tasks at college-level subject knowledge and deliberate reasoning.\n\t+ MMBench: large-scale benchmark to evaluate perception and reasoning capabilities.\n  +\tVisual reasoning:\n    + ScienceQA: multimodal visual question answering on science.\n\t+ MathVista: visual math reasoning.\n\t+ InterGPS: Visual 2D geometry reasoning.\n  +\tChart reasoning:\n\t+ ChartQA: visual and logical reasoning on charts.\n\t+ AI2D: diagram understanding.\n  +\tDocument Intelligence:\n\t+ TextVQA: read and reason about text in images to answer questions about them.\n\t+ InfoVQA: read and reason about high-resolution infographics images with arbitrary aspect ratios.\n\t+ DocVQA: read and reason about document images with dense texts and handwritten texts.\n\t+ OCRBench: test OCR and QA capability on diverse text related images.\n  +\tVision speech multimodal understanding:\n\t+ s_AI2D: diagram understanding with speech as the question format.\n\t+ s_ChartQA: visual and logical reasoning on charts with speech as the question format.\n\t+ s_InfoVQA: read and reason about high-resolution infographics images with speech as the question format.\n\t+ s_DocVQA: read and reason about document images with dense texts and handwritten texts with speech as the question format.\n  + RAI & Security Benchmarks:\n\t+ VLGuardExt: VLGuard is a vision-language instruction following public dataset for model safety to address safety on deception\n    discrimination, privacy and risky behavior (advice, sexual, violence, political). This was extended to a few internal categories such as child safety and election critical information.\n\t+ RTVLM: Public benchmark for red-teaming vision-language model on model truthfulness, privacy, safety, and fairness.\n\t+ GPTV-RAI: In-house benchmark for GPT-4V released from Azure AI, measuring harmfulness (ex. sexual, violent, hate and self-harm), privacy, jailbreak, misinformation.\n\n+ Speech: \n  + CommonVoice v15 is an open-source, multilingual speech dataset developed by Mozilla. It includes over 33,000 hours of speech data in 133 languages, contributed and validated by volunteers worldwide.The evaluations were conducted in the eight supported languages.\n  + The OpenASR Leaderboard on Hugging Face is designed for benchmarking and evaluating the robustness of ASR models on English. The datasets in the leaderboard cover diverse speech domains including reading speech, conversations, meetings, and so on.\n  + CoVoST2 is a multilingual speech-to-text translation dataset derived from Mozilla's Common Voice project. It is one of the largest open datasets available for speech translation, providing support for both X-to-English (X→En) and English-to-X (En→X) translation tasks. The directions with supported languages were evaluated on the test sets.\n  + FLEURS is a multilingual speech dataset designed for evaluating speech recognition and speech-to-text translation models across a wide range of languages. The test sets for speech recognition and translation tasks were evaluated with the eight supported languages.\n  + MT Bench (Multi-turn Benchmark) is specifically designed to evaluate the conversational and instruction-following abilities of AI models in multi-turn question-answering (QA) scenarios. To support spoken questions, the text is synthesized into speech.\n  + MMMLU (Multilingual Massive Multitask Language Understanding) is an extensive benchmark designed to evaluate the general knowledge and reasoning capabilities of AI models across a wide array of subjects. To support spoken questions, the text is synthesized into its speech counterpart.  The model was evaluated on the eight supported languages for this test set. \n  + AIR-Bench Chat (Audio Instruction and Response Benchmark) is a comprehensive evaluation framework designed to test the capabilities of large audio language models (LALMs). It includes both foundation and chat benchmarks. The chat benchmark is selected for its open-ended question answering for audio capability.\n  + MMAU (Massive Multi-Task Audio Understanding) is a comprehensive dataset designed to evaluate the capabilities of multi-modal models in audio-based understanding and reasoning tasks. The test sets are in the form of multiple-choices QA, covering the categories of music, sound, and speech.\n  + Golden3 is a real-world meeting dataset, containing 108 meeting recordings with corresponding transcripts, averaging 6 minutes each. It is recorded across 30 conference rooms, featuring 4-8 attendees. The dataset is primarily in English, covering a wide range of topics. GPT4 is employed to generate summarization instructions that ask to summarize partial or the entire conversation or control the output style/length/structure.\n  + AMI (Augmented Multi-Party Interaction) is a comprehensive collection of meeting recordings, encompassing approximately 100 hours of data. The test split contains 20 meeting recordings with an average duration of 32 minutes. The model was tested on the close-talking version of audio. GPT4 is employed to generate summarization instructions that ask to summarize partial or the entire conversation or control the output style/length/structure.\n\n+ Safety and RAI:\n  + Single-turn trustworthiness evaluation:\n    + DecodingTrust: DecodingTrust is a collection of trustworthiness benchmarks in eight different perspectives\n    + XSTest: XSTest is an exaggerated safety evaluation\n    + Toxigen: Toxigen is adversarial and hate speech detection\n  + Red Team:\n    + Responses to prompts provided by AI Red Team at Microsoft\n</details>\n\n\n## Appendix B: Fine-tuning Korean speech\n\n<details>\n  <summary>Click to view detail descriptions</summary>\n\n### Overview and Datasets\n\nPhi-4-multimodal is originally not designed for Korean speech-to-text task, but it can be fine-tuned for Korean speech-to-text task using your own data or public Korean speech datasets.\n\nWe have fine-tuned Phi-4-multimodal model for Korean speech-to-text task using the following datasets:\n\n- kresnik/zeroth_korean\n- mozilla-foundation/common_voice_17_0 (Used Korean speech only)\n- PolyAI/minds14 (Used Korean speech only)\n- Custom dataset. The speech was a mix of fast and slow speech (Technical blog contents and presentations that the author have posted), with some modulation using [audiomentations](https://github.com/iver56/audiomentations) and [this script](https://github.com/daekeun-ml/azure-genai-utils/blob/main/azure_genai_utils/stt/augment.py)\n\nTotal 35K samples. Each sample is a pair of Korean speech and its transcription. Dataset was sampled 16kHz.\n\nYou can download the fine-tuned model [here](https://huggingface.co/daekeun-ml/Phi-4-multimodal-finetune-ko-speech). Please refer to the Jupyter notebook and video clips in the [demo folder](https://huggingface.co/daekeun-ml/Phi-4-multimodal-finetune-ko-speech/tree/main/demos). They are not production-quality as they were simply fine-tuned for PoC purposes, but you can see that they transcribe and translate with high accuracy even when a native speaker speaks quite quickly.\n\n### Requirements\nBased on Python 3.10, the following packages are required, and A100/H100 GPU is recommended.\n```\ntorch==2.6.0\ntransformers==4.48.2\naccelerate==1.4.0\nsoundfile==0.13.1\npillow==11.1.0\nscipy==1.15.2\ntorchvision==0.21.0\nbackoff==2.2.1\npeft==0.14.0\ndatasets==3.3.2\npandas==2.2.3\nflash_attn==2.7.4.post1\nevaluate==0.4.3\nsacrebleu==2.5.1  \n```\n\n### Training\nThe model was trained on a single A100 80GB GPU for 4 epochs with a batch size of 16 using the `sample_finetune_speech.py` script from [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)\n\nThe fine tuning script and command line are basically the same as [here](https://gist.github.com/seastar105/d1d8983b27611370528e3b194dcc5577#file-main-py), but you need to prepare your own dataset. Also, to perform audio encoder unfreeze, please refer to the code snippet below. The code snippet is retrieved from [the fine-tuning Colab notebook](https://colab.research.google.com/drive/1JAQdpX3BtIgDmTLlnHgstKfGw7HjSfej?usp=sharing).\n\n```python\nwith accelerator.local_main_process_first():\n    processor = AutoProcessor.from_pretrained(\n        \"microsoft/Phi-4-multimodal-instruct\",\n        trust_remote_code=True,\n    )\n    model = create_model(\n        args.model_name_or_path,\n        use_flash_attention=args.use_flash_attention,\n    )\n\ndef unfreeze_speech_components(model):\n    \"\"\"Directly target verified components from your debug logs\"\"\"\n    # 1. Audio Embed Module (confirmed exists)\n    audio_embed = model.model.embed_tokens_extend.audio_embed\n\n    # 2. Entire Audio Encoder (simplified)\n    audio_encoder = audio_embed.encoder  # Direct access\n\n    # 3. Audio Projection (from debug logs)\n    audio_projection = audio_embed.audio_projection\n\n    # Unfreeze ONLY these 3 components\n    for component in [audio_embed, audio_encoder, audio_projection]:\n        for param in component.parameters():\n            param.requires_grad = True\n    return model\n\nmodel = unfreeze_speech_components(model)\n\n# Verify unfrozen parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,}\")\n\n# After unfreezing\nencoder_params = list(model.model.embed_tokens_extend.audio_embed.encoder.parameters())\nproj_params = list(model.model.embed_tokens_extend.audio_embed.audio_projection.parameters())\n\nassert any(p.requires_grad for p in encoder_params), \"Encoder params frozen!\"\nassert any(p.requires_grad for p in proj_params), \"Projection params frozen!\"\nprint(\"Components properly unfrozen ✅\")    \n```\n\nExample commands to run finetuning scripts are as follows:\n```bash\npython main.py\n```\n\nThe latest version of the model currently uploaded was fine-tuned by **unfreezing the audio encoder**, and the ASR performance was significantly improved compared to the baseline LoRA adapter-based fine-tuning. \nComparing the full fine-tuning and LoRA fine-tuning, the CER on zeroth-test set is **1.61%** and 2.72%, and the WER on zeroth-test set is **3.54%** and 7.19%, respectively. Please refer to the [Experimental Settings and Results](#experimental-settings-and-results) for more details.\n\n### Experimental Settings and Results\nThe purpose of this benchmarking setup is to evaluate the basic performance of Korean audio in speech and audio understanding tasks. We did this for automatic speech recognition and automatic speech translation, and the test data used the following datasets and samples:\n\nEvaluation was done on the following datasets:\n+ ASR (Automatic Speech Recognition): Evaluated with CER (Character Error Rate) and WER (Word Error Rate) on [zeroth-test set (457 samples)](https://huggingface.co/datasets/kresnik/zeroth_korean).\n+ AST (Automatic Speech Translation): Evaluated with BLEU score on [fleurs ko <-> en speech translation test set (270 samples)](https://huggingface.co/datasets/seastar105/fleurs_ko_en_test).\n\nEvaluation Script is retrieved from [here](https://gist.github.com/seastar105/d1d8983b27611370528e3b194dcc5577#file-evaluate-py)\n\nWe used the [Phi-4-mm-inst-zeroth-kor](https://huggingface.co/seastar105/Phi-4-mm-inst-zeroth-kor) as a baseline to improve performance, as it showed significant performance improvement with 1 epoch. Note that the baseline was trained with [22K Zeroth Korean Korean speech data](https://huggingface.co/datasets/kresnik/zeroth_korean) for 1 epoch. Based on this baseline with 35K training samples, we conducted additional experiments with the following scenarios:\n\n+ [Case 1] LoRA finetune (1 epoch): LoRA adapter-based fine-tuning for 1 epochs\n+ [Case 2] LoRA finetune (4 epochs): LoRA adapter-based fine-tuning for 4 epochs\n+ [Case 3] Unfreeze audio encoder finetune (4 epochs): Full fine-tuning for 4 epochs. \n\nThe results of the experiments are as follows:\n+ CER and WER for zeroth-test set (Lower is better)\n  + Case 1's CER and WER are 3.80% and 11.52%, respectively, which are better than the baseline (7.02% and 17.31%).\n  + Case 2's CER and WER are 2.72% and 7.19%, respectively, which are better than Case 1.\n  + Case 3's CER and WER are 1.61% and 3.54%, respectively, which are the best among the cases.\n\n+ BLEU score for fleurs ko <-> en speech translation test set (Higher is better)\n  + Case 1's result is not improved compared to the baseline. Especially, the BLEU score for fleurs-ko2en-cot is decreased compared to the baseline.\n  + Case 2's result is slightly improved compared to Case 1, which is the best among the cases.\n  + Case 3's result is not improved compared to the baseline and Case 2.\n  \n| Model                          | zeroth (CER) | zeroth (WER) | fleurs-ko2en | fleurs-ko2en-cot | fleurs-en2ko | fleurs-en2ko-cot |\n|--------------------------------|-------------|-------------|--------------|------------------|--------------|------------------|\n| original                       | 99.16       | 99.63       | 5.63         | 2.42             | 6.86         | 4.17             |\n| Ours - speech full finetune (4 epochs) | 1.61        | 3.54        | 7.67         | 8.38             | 12.31        | 9.69             |\n| LoRA finetune (4 epochs)        | 2.72        | 7.19        | 7.11         | 9.95             | 13.22        | 10.45            |\n| LoRA finetune (1 epoch)         | 3.80        | 11.52       | 7.03         | 7.04             | 12.50        | 9.54             |\n| Phi-4-mm-inst-zeroth-kor        | 7.02        | 17.31       | 7.07         | 9.19             | 13.08        | 9.35             |\n\n## Cautions\n\nNote that this model is just a PoC/experimental purpose, and not intended to be used in production. More high-quality data, tuning, ablation studies, and experiments are needed.\n\nPhi-4-multimodal model is strong in multimodal tasks, especially in speech-to-text and high potential in Korean language tasks. Thus if you are interested in Korean speech-to-text task, this model can be a good starting point.\n\n## References\n\n- https://huggingface.co/microsoft/Phi-4-multimodal-instruct\n- https://huggingface.co/seastar105/Phi-4-mm-inst-zeroth-kor\n\n</details>","sources":[{"platform":"Hugging Face","url":"https://huggingface.co/microsoft/Phi-4-multimodal-instruct"}]},{"id":"github-joinly-ai-joinly","name":"joinly","author":"joinly-ai","description":"Make your meetings accessible to AI Agents","task":"tool","tags":["agentic-ai","ai-agent","ai-tool","conversational-ai","llm","mcp","meeting-agent","meeting-assistant","meeting-notes","productivity","python","transcription","voice-ai"],"likes":392,"downloads":392,"lastModified":"2025-11-10T09:36:32Z","sources":[{"platform":"GitHub","url":"https://github.com/joinly-ai/joinly"}]},{"id":"github-tegridydev-auto-md","name":"auto-md","author":"tegridydev","description":"Convert Files /  Folders / GitHub Repos Into AI / LLM-ready Files","task":"tool","tags":["ai","ai-tool","convert","github","llm","llm-tools","md","python","python-convert","python-script","scrape","webapp"],"likes":199,"downloads":199,"lastModified":"2025-10-22T22:36:25Z","sources":[{"platform":"GitHub","url":"https://github.com/tegridydev/auto-md"},{"platform":"GitHub","url":"https://github.com/toolworks-dev/auto-md"}]},{"id":"github-polyfact-polyfire-js","name":"polyfire-js","author":"polyfact","description":"🔥 React library of AI components 🔥","task":"tool","tags":["ai","ai-models","ai-tool","llm","npm","package","sdk"],"likes":141,"downloads":141,"lastModified":"2025-11-03T16:15:54Z","sources":[{"platform":"GitHub","url":"https://github.com/polyfact/polyfire-js"}]},{"id":"github-apurvsinghgautam-robin","name":"robin","author":"apurvsinghgautam","description":"AI-Powered Dark Web OSINT Tool","task":"tool","tags":["ai-tool","darkweb","darkweb-osint","investigation-tool","llm-powered","osint","osint-tool"],"likes":132,"downloads":132,"lastModified":"2025-11-04T10:23:17Z","sources":[{"platform":"GitHub","url":"https://github.com/apurvsinghgautam/robin"}]},{"id":"github-dinoDanic-diny","name":"diny","author":"dinoDanic","description":"generate git commit messages","task":"tool","tags":["ai-tool","automation","cli","cobra-cli","commit","commit-message","developer-tools","generated","git","git-commit-messages","git-diff","go","messages","ollama","opensource","plug-and-play"],"likes":98,"downloads":98,"lastModified":"2025-11-09T14:43:59Z","sources":[{"platform":"GitHub","url":"https://github.com/dinoDanic/diny"}]},{"id":"github-cameronking4-sketch2app","name":"sketch2app","author":"cameronking4","description":"The ultimate sketch to code app made using GPT4o serving 30k+ users. Choose your desired framework (React, Next, React Native, Flutter) for your app. It will instantly generate code and preview (sandbox) from a simple hand drawn sketch on paper captured from webcam","task":"tool","tags":["ai-tool","app-maker","code-assistant","code-generator","design2code","generate-app-ai","gpt4","gpt4-vision","gpt4v","nextjs","openai","pad2pixel","sketch2app","sketch2code","wireframe"],"likes":84,"downloads":84,"lastModified":"2025-10-06T14:41:16Z","sources":[{"platform":"GitHub","url":"https://github.com/cameronking4/sketch2app"}]},{"id":"github-HAibiiin-json-repair","name":"json-repair","author":"HAibiiin","description":"Repair JSON! A Java library for fixing JSON anomalies generated by LLMs.","task":"tool","tags":["ai-tool","java","json","json-repair","llm"],"likes":67,"downloads":67,"lastModified":"2025-11-08T06:33:31Z","sources":[{"platform":"GitHub","url":"https://github.com/HAibiiin/json-repair"}]},{"id":"github-inulute-phantom-lens","name":"phantom-lens","author":"inulute","description":"The open-source, privacy-focused alternative to Cluely that helps you see beyond and know more. This undetectable AI assistant operates like a ghost across your screen, providing real-time information during meetings, interviews, and presentations without leaving a trace.","task":"tool","tags":["ai-tool","cluely","cluely-alternative","electron","electron-app","inulute","opensource","phantom-lens"],"likes":55,"downloads":55,"lastModified":"2025-11-03T17:06:35Z","sources":[{"platform":"GitHub","url":"https://github.com/inulute/phantom-lens"}]},{"id":"github-shunseven-mocxykit","name":"mocxykit","author":"shunseven","description":"This is an Frontend development service middleware that can be used with webpack and vite. Its main function is to visualize the configuration, manage http(s)-proxy, and mock data.","task":"tool","tags":["ai-tool","api-mock-server","development","express","express-middleware","express-proxy-mock","http-proxy-middleware","https-proxy","mcp-server","mcpe","mock","proxy","proxy-server","visualization-tools","vite","vite-mock","vite-mock-server","vite-plugin","webpack","webpack-proxy"],"likes":39,"downloads":39,"lastModified":"2025-11-02T23:53:29Z","sources":[{"platform":"GitHub","url":"https://github.com/shunseven/mocxykit"}]},{"id":"github-iniwap-AIForge","name":"AIForge","author":"iniwap","description":"🚀 智能意图自适应执行引擎，只需一句话，让AI帮你搞定想做的事（数据分析与处理、高时效性内容创作、最新信息获取、数据可视化、系统交互、自动化工作流、代码开发等)","task":"tool","tags":["agent","agent-zero","agent0","agentic-ai","ai","ai-agents","ai-tool","ai-tools","aipy","aipyapp","aiwritex","crewai","deepseek","iflow","iflow-cli","manus-ai"],"likes":27,"downloads":27,"lastModified":"2025-11-06T01:46:25Z","sources":[{"platform":"GitHub","url":"https://github.com/iniwap/AIForge"}]},{"id":"github-autohandai-commander","name":"commander","author":"autohandai","description":"Commander, your AI coding commander centre for all you ai coding cli agents","task":"tool","tags":["ai","ai-agents","ai-tool","claude-code","codex-cli","gemini-cli","rust","tauri-app","tauri2"],"likes":19,"downloads":19,"lastModified":"2025-10-31T09:20:01Z","sources":[{"platform":"GitHub","url":"https://github.com/autohandai/commander"}]},{"id":"github-eVolpe-AI-AI-HR-Agent","name":"AI-HR-Agent","author":"eVolpe-AI","description":"AI HR Agent for HRMS","task":"tool","tags":["ai","ai-agent","ai-chatbot","ai-tool","hcm"],"likes":18,"downloads":18,"lastModified":"2025-10-28T12:27:07Z","sources":[{"platform":"GitHub","url":"https://github.com/eVolpe-AI/AI-HR-Agent"}]},{"id":"github-btfranklin-promptdown","name":"promptdown","author":"btfranklin","description":"A Python package that enables the creation and parsing of structured prompts for language models in markdown format","task":"tool","tags":["ai","ai-tool","ai-tools","dsl","llm","llms","prompt","prompt-templates","prompts"],"likes":14,"downloads":14,"lastModified":"2025-09-12T23:54:20Z","sources":[{"platform":"GitHub","url":"https://github.com/btfranklin/promptdown"}]},{"id":"github-Crezy-haker-videocutterAI","name":"videocutterAI","author":"Crezy-haker","description":"AI-powered web tool that automatically finds and generates highlight clips from your videos.","task":"tool","tags":["ai-agents","ai-tool","automation","ffmpeg","flask","google-generative-ai","hari","python","video-cutting-and-trimming","video-processing","wishper"],"likes":13,"downloads":13,"lastModified":"2025-11-10T08:48:20Z","sources":[{"platform":"GitHub","url":"https://github.com/Crezy-haker/videocutterAI"},{"platform":"GitHub","url":"https://github.com/hari7261/videocutterAI"}]},{"id":"github-rizzzky78-market-maven","name":"market-maven","author":"rizzzky78","description":"Maven is a cutting-edge web application that leverages the power of AI to revolutionize electronic categorized product research and data-driven decision-making.","task":"tool","tags":["agentic-ai","ai","ai-tool","gemini","shopping","vercel-ai-sdk"],"likes":9,"downloads":9,"lastModified":"2025-10-20T04:28:27Z","sources":[{"platform":"GitHub","url":"https://github.com/rizzzky78/market-maven"}]},{"id":"github-gimjin-message-mcp","name":"message-mcp","author":"gimjin","description":"Desktop notifications, custom sounds, ntfy mobile notifications, email notifications, and API pushes reduce anxiety while waiting for AI tasks, allowing you to comfortably enjoy a cup of coffee.","task":"tool","tags":["ai-coding","ai-tool","automation","chatgpt","claude","copilot","cursor","mcp","message","notification","notify","productivity"],"likes":8,"downloads":8,"lastModified":"2025-09-20T12:55:21Z","sources":[{"platform":"GitHub","url":"https://github.com/gimjin/message-mcp"}]},{"id":"github-pinkpixel-dev-npm-helper-mcp","name":"npm-helper-mcp","author":"pinkpixel-dev","description":"A Model Context Protocol (MCP) server providing tools for NPM package management and dependency updates. Helps LLMs like Claude interact with npm packages, search npm registry, and keep dependencies up-to-date.","task":"tool","tags":["ai-tool","claude","cursor","dependency-manager","dependency-manager-update","developer-tools","mcp","mcp-server","mcp-tools","model-context-protocol","model-context-protocol-servers","nodejs","npm","npm-check-updates","npm-package","npm-search","npmjs","package-management","package-manager","typescript"],"likes":6,"downloads":6,"lastModified":"2025-10-03T22:32:38Z","sources":[{"platform":"GitHub","url":"https://github.com/pinkpixel-dev/npm-helper-mcp"}]},{"id":"github-eVolpe-AI-AI-HR-MintHCM-Package","name":"AI-HR-MintHCM-Package","author":"eVolpe-AI","description":"AI package for MintHCM system","task":"tool","tags":["ai","ai-agent","ai-chatbot","ai-tool","hcm","hrms","minthcm"],"likes":5,"downloads":5,"lastModified":"2025-06-04T09:19:46Z","sources":[{"platform":"GitHub","url":"https://github.com/eVolpe-AI/AI-HR-MintHCM-Package"}]},{"id":"github-Chungzter-CommiZard","name":"CommiZard","author":"Chungzter","description":"Use LLMs to write good commit messages with full Control","task":"tool","tags":["ai-assistant","ai-tool","assistant","cli","commit-ai","commit-assistant","python","tool"],"likes":4,"downloads":4,"lastModified":"2025-11-09T17:27:21Z","sources":[{"platform":"GitHub","url":"https://github.com/Chungzter/CommiZard"}]},{"id":"github-lucaguindani-n8n-nodes-bookstack","name":"n8n-nodes-bookstack","author":"lucaguindani","description":"Community n8n node for the BookStack API","task":"tool","tags":["ai-tool","api","bookstack","connector","n8n","n8n-community-node-package","n8n-node"],"likes":4,"downloads":4,"lastModified":"2025-10-31T17:50:55Z","sources":[{"platform":"GitHub","url":"https://github.com/lucaguindani/n8n-nodes-bookstack"}]},{"id":"github-Mo-Ko-MockGen","name":"MockGen","author":"Mo-Ko","description":"Instantly generate mock REST APIs powered by LLMs (GPT/Gemini). Just describe your endpoint—MockGen does the rest. Docker-ready, fast, and open source.","task":"tool","tags":["ai-tool","api-mocking","api-testing","developer-tools","fastapi","gemini","llm","mock-api","mock-api-tool","openai","python","swagger","vue"],"likes":4,"downloads":4,"lastModified":"2025-07-30T12:50:21Z","sources":[{"platform":"GitHub","url":"https://github.com/Mo-Ko/MockGen"}]},{"id":"github-Vishnu-tppr-Camouflage-AI","name":"Camouflage-AI","author":"Vishnu-tppr","description":"🎥 Camouflage-AI – A fast and flexible AI tool for removing video backgrounds using YOLOv8 segmentation. Customize with solid colors, blur, or images. Built with Python & CustomTkinter for a stunning desktop experience.","task":"tool","tags":["ai-desktop-app","ai-projects","ai-tool","ai-video-editor","background-removal","camouflage-ai","gui","image-segmentation","machine-learning","open-source-project","opencv","python","top-github-projects","video-ai","video-processing","vishnu-cse","yolov8-segmentation"],"likes":3,"downloads":3,"lastModified":"2025-10-31T14:09:10Z","sources":[{"platform":"GitHub","url":"https://github.com/Vishnu-tppr/Camouflage-AI"}]},{"id":"github-volodya-lombrozo-aidy","name":"aidy","author":"volodya-lombrozo","description":"AI-assisted CLI for GitHub workflows — generate commits, issues, PRs, and releases with one command","task":"tool","tags":["ai","ai-tool","git","github-cli","go"],"likes":3,"downloads":3,"lastModified":"2025-11-08T00:56:28Z","sources":[{"platform":"GitHub","url":"https://github.com/volodya-lombrozo/aidy"}]},{"id":"github-lokeshch185-clipboardAI","name":"clipboardAI","author":"lokeshch185","description":"ClipboardAI is an AI-powered clipboard assistant that works with multiple LLM providers to help you process text from your clipboard quickly and efficiently.","task":"tool","tags":["ai","ai-tool","clipboard","desktop-app","productivity"],"likes":3,"downloads":3,"lastModified":"2025-05-20T14:13:05Z","sources":[{"platform":"GitHub","url":"https://github.com/lokeshch185/clipboardAI"}]},{"id":"github-Lixher-Diagrammer-Bot","name":"Diagrammer-Bot","author":"Lixher","description":"Diagrammer Bot Telegram","task":"tool","tags":["ai-tool","diagram","graphviz","python","telegram-bot","visualisation"],"likes":3,"downloads":3,"lastModified":"2025-10-21T12:45:27Z","sources":[{"platform":"GitHub","url":"https://github.com/Lixher/Diagrammer-Bot"}]},{"id":"github-TufayelLUS-RAG-Scraper-AI-GUI","name":"RAG-Scraper-AI-GUI","author":"TufayelLUS","description":"This python powered AI based RAG Scraper allows you to ask question based on PDF/URL provided to the software using local Ollama powered LLMs","task":"tool","tags":["ai-assistant","ai-ml","ai-software","ai-tool","ai-tools","python-ai","python-rag","rag","rag-agents","rag-application","rag-applications","rag-embeddings","rag-llm"],"likes":3,"downloads":3,"lastModified":"2025-11-06T16:57:15Z","sources":[{"platform":"GitHub","url":"https://github.com/TufayelLUS/RAG-Scraper-AI-GUI"}]},{"id":"github-0xAkuti-ai-council-mcp","name":"ai-council-mcp","author":"0xAkuti","description":"Multi-AI consensus MCP server that queries multiple AI models (OpenAI, Claude, Gemini, custom APIs) in parallel and synthesizes responses to reduce bias and improve accuracy. A Python implementation of the wisdom-of-crowds approach for AI decision making.","task":"tool","tags":["ai-consensus","ai-synthesis","ai-tool","claude","claude-desktop","cursor-ai","cursor-mcp","deepseek","gemini","llm-ensemble","mcp-server","multi-model-ai","openai","openrouter","parallel-ai","python","wisdom-of-crowd"],"likes":3,"downloads":3,"lastModified":"2025-11-05T13:31:03Z","sources":[{"platform":"GitHub","url":"https://github.com/0xAkuti/ai-council-mcp"}]},{"id":"github-Ocidemus-AI-Agent","name":"AI-Agent","author":"Ocidemus","description":"Agentic code editor using Python and Google Gemini — supports function-calling, file editing, and debugging via LLM.","task":"tool","tags":["agent","ai-tool","code-analysis","debugging","function-calling","google-gemini","llm","python"],"likes":2,"downloads":2,"lastModified":"2025-06-26T16:53:40Z","sources":[{"platform":"GitHub","url":"https://github.com/Ocidemus/AI-Agent"}]},{"id":"github-duyl328-PLC-Data-Lab","name":"PLC-Data-Lab","author":"duyl328","description":"A portable, zero-dependency, browser-based tool for analyzing and converting PLC raw data formats.  一个可在任意浏览器运行的、零依赖的 PLC 原始数据解析与转换工具。","task":"tool","tags":["ai-tool","html","plc","tool"],"likes":2,"downloads":2,"lastModified":"2025-10-27T08:44:22Z","sources":[{"platform":"GitHub","url":"https://github.com/duyl328/PLC-Data-Lab"}]},{"id":"github-starthackHQ-Contextinator","name":"Contextinator","author":"starthackHQ","description":"Turning messy repos into weapons of mass structured context.","task":"tool","tags":["agentic-ai","ai-tool","chunking","codebase-search","embeddings","full-text-search","mcp","read-a-file","regex-search","semantic-search","symbol-search"],"likes":2,"downloads":2,"lastModified":"2025-11-06T10:20:26Z","sources":[{"platform":"GitHub","url":"https://github.com/starthackHQ/Contextinator"}]},{"id":"github-DeveloperPuneet-CodeCharm","name":"CodeCharm","author":"DeveloperPuneet","description":"VS Code extension that adds AI-powered inline comments to selected code using Google Gemini. Simple, fast, and emoji-rich 💬✨","task":"tool","tags":["ai","ai-powered-tools","ai-tool","extension","mit-license","open-source","tool","vscode-extension","vscode-tool"],"likes":2,"downloads":2,"lastModified":"2025-10-23T10:16:45Z","sources":[{"platform":"GitHub","url":"https://github.com/DeveloperPuneet/CodeCharm"}]},{"id":"github-XiaomingX-jobpeap4u-easy-seo-site","name":"jobpeap4u-easy-seo-site","author":"XiaomingX","description":"jobleap4u是一个开源的AI导航站，你可以基于这个模版再开发出自己的AI导航站点","task":"tool","tags":["ai-navigation-uav","ai-tool","awesome","awesome-list"],"likes":2,"downloads":2,"lastModified":"2025-08-19T07:05:54Z","sources":[{"platform":"GitHub","url":"https://github.com/XiaomingX/jobpeap4u-easy-seo-site"}]},{"id":"github-Motaz432-ocr-ai-shell","name":"ocr-ai-shell","author":"Motaz432","description":"AI OCR Tool | Webcam & Image Text Recognition with Astra | Offline Summarization","task":"tool","tags":["ai-tool","gemma3","gui","image-to-text","llava","ocr","offline-ai","ollama","python","summarization","tkinter"],"likes":2,"downloads":2,"lastModified":"2025-11-10T08:39:07Z","sources":[{"platform":"GitHub","url":"https://github.com/Motaz432/ocr-ai-shell"}]},{"id":"github-dmmudhan-REFRAME_Feedback-rewriter-gpt","name":"REFRAME_Feedback-rewriter-gpt","author":"dmmudhan","description":"REFRAME helps you rewrite workplace feedback and everyday messages with the right tone — empathetic, constructive, or persuasive — powered by free LLMs via OpenRouter.","task":"tool","tags":["ai-tool","feedback-assistant","llm-app","mistral","openrouter","prompt-engineering","streamlit"],"likes":1,"downloads":1,"lastModified":"2025-09-14T10:53:45Z","sources":[{"platform":"GitHub","url":"https://github.com/dmmudhan/REFRAME_Feedback-rewriter-gpt"}]},{"id":"github-btfranklin-pickled_pipeline","name":"pickled_pipeline","author":"btfranklin","description":"A Python package for caching repeat runs of pipelines that have expensive operations along the way","task":"tool","tags":["ai","ai-tool","ai-tools","caching","dx","efficiency","llm","llms","pipeline-caching","workflow"],"likes":1,"downloads":1,"lastModified":"2025-07-23T14:40:47Z","sources":[{"platform":"GitHub","url":"https://github.com/btfranklin/pickled_pipeline"}]},{"id":"github-miaofalianhua-ResxMcp","name":"ResxMcp","author":"miaofalianhua","description":"A lightweight MCP server for managing .resx localization files—works with any MCP-compatible client.","task":"tool","tags":["ai-tool","cli","csharp","dotnet","gemini-cli","gemini-cli-extensions","i18n","l10n","localization","mcp","model-context-protocol","resx-manager"],"likes":1,"downloads":1,"lastModified":"2025-10-27T00:22:47Z","sources":[{"platform":"GitHub","url":"https://github.com/miaofalianhua/ResxMcp"}]},{"id":"github-Pranav-Sharma-Official-AI-Research-Lab-Simulator","name":"AI-Research-Lab-Simulator","author":"Pranav-Sharma-Official","description":"🧠 A multi-agent Gen AI platform powered by Google Gemini 2.5 Pro that autonomously generates, reviews, and composes full-length academic research papers — complete with chat assistant, dark UI, and editable .docx export.","task":"tool","tags":["academic-research","academic-writing","ai-paper-generator","ai-research","ai-tool","artificial-intelligence","chat-assistant","docx-generator","gemini-api","genai","google-gemini","hackathon-project","large-language-model","llm","machine-learning","multi-agent-system","python","research-automation","research-simulator","streamlit-api"],"likes":1,"downloads":1,"lastModified":"2025-11-09T10:41:42Z","sources":[{"platform":"GitHub","url":"https://github.com/Pranav-Sharma-Official/AI-Research-Lab-Simulator"}]},{"id":"github-fabiconcept-now-ai-landing-page","name":"now-ai-landing-page","author":"fabiconcept","description":"Powerful, HIPAA-compliant AI tools that automate your patient communication, reduce call wait times, and grow your practice effortlessly.","task":"tool","tags":["ai-tool"],"likes":1,"downloads":1,"lastModified":"2025-08-04T17:44:17Z","sources":[{"platform":"GitHub","url":"https://github.com/fabiconcept/now-ai-landing-page"}]},{"id":"github-petmal-MindTrial","name":"MindTrial","author":"petmal","description":"MindTrial: Evaluate and compare AI language models (LLMs) on text-based tasks with optional file/image attachments and tool use. Supports multiple providers (OpenAI, Google, Anthropic, DeepSeek, Mistral AI, xAI, Alibaba), custom tasks in YAML, and HTML/CSV reports.","task":"tool","tags":["ai-benchmark","ai-evaluation-tools","ai-model-comparison","ai-tool","anthropic","artificial-intelligence-projects","deepseek","golang-cli","google-gemini-ai","grok-ai","language-models-ai","llm-benchmarking","llm-comparison","llm-evaluation-framework","mistral-ai","nlp","openai","opensource","qwen","xai"],"likes":1,"downloads":1,"lastModified":"2025-10-11T03:10:54Z","sources":[{"platform":"GitHub","url":"https://github.com/petmal/MindTrial"}]},{"id":"github-prokhororlov-repo2file","name":"repo2file","author":"prokhororlov","description":"A utility for merging repository files into a single text file for interacting with it using large-context neural networks, e.g. qwen.ai","task":"tool","tags":["ai-tool","repo2txt"],"likes":1,"downloads":1,"lastModified":"2025-05-28T18:23:48Z","sources":[{"platform":"GitHub","url":"https://github.com/prokhororlov/repo2file"}]},{"id":"github-KatavinaNguyen-screenshot_based_ai_desktop_assistant","name":"screenshot_based_ai_desktop_assistant","author":"KatavinaNguyen","description":"A lightweight Python-based desktop assistant that lets users capture a region of their screen, extract text using PaddleOCR, and instantly query selected large language models (LLMs) for responses, all without interrupting workflow. Designed with a minimal popup UI and global hotkey support for distraction-free productivity.","task":"tool","tags":["ai-tool","automation","desktop-assistant","llm","ocr-recognition","paddleocr","popup-ui","productivity-app","python"],"likes":1,"downloads":1,"lastModified":"2025-07-16T02:59:02Z","sources":[{"platform":"GitHub","url":"https://github.com/KatavinaNguyen/screenshot_based_ai_desktop_assistant"}]},{"id":"github-chaolunner-Tweets","name":"Tweets","author":"chaolunner","description":"In a nutshell: An all-powerful AI docking station disguised as a tweet tool!","task":"tool","tags":["ai-tool","ai-toolkit","drawing","tweets","video-editing-software","video-editor"],"likes":1,"downloads":1,"lastModified":"2025-10-14T10:15:52Z","sources":[{"platform":"GitHub","url":"https://github.com/chaolunner/Tweets"}]},{"id":"github-ImYourBoyRoy-reqsync","name":"reqsync","author":"ImYourBoyRoy","description":"Synchronize requirements.txt to match installed versions, safely and atomically.","task":"tool","tags":["agent","ai-agents","ai-tool","automation","ci-cd","cli-tool","dependencies","dependency-management","devops","mcp","packing","pip","requirements","tool","venv"],"likes":1,"downloads":1,"lastModified":"2025-08-20T06:32:37Z","sources":[{"platform":"GitHub","url":"https://github.com/ImYourBoyRoy/reqsync"}]},{"id":"github-iamrealvinnu-autocorrect-tool","name":"autocorrect-tool","author":"iamrealvinnu","description":"A user-friendly text correction tool powered by AI (T5 transformer) that fixes grammar and spelling mistakes in real-time. Features an easy-to-use GUI interface with instant corrections and clipboard support.","task":"tool","tags":["ai-tool","grammar-checker","gui-application","machine-learning","nlp","python","spell-checker","t5-transformer","text-correction","tkinter"],"likes":1,"downloads":1,"lastModified":"2025-10-24T03:21:13Z","sources":[{"platform":"GitHub","url":"https://github.com/iamrealvinnu/autocorrect-tool"}]},{"id":"github-fenneccyber-El-Moufid","name":"El-Moufid","author":"fenneccyber","description":"El Moufid, AI-Powered Tools to Enhance Your Learning and Productivity. 🎥 YouTube Summarizer (Main Tool). El Moufid allows 2 free summaries per day for all users.","task":"tool","tags":["ai","ai-applications","ai-tool","ai-tools","algerian-developpers","android-application","application","el-moufid","enhance-productivity","productivity","summerization","web-application","webapp","youtube","youtube-summarization","youtube-summarizer"],"likes":1,"downloads":1,"lastModified":"2025-11-09T19:10:18Z","sources":[{"platform":"GitHub","url":"https://github.com/fenneccyber/El-Moufid"}]},{"id":"civitai-easynegative","name":"EasyNegative","author":"Civitai Community","description":"<p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/datasets/gsdf/EasyNegative\"><strong>Original Hugging Face Repository</strong></a><br /><strong>Counterfeit-V3 (which has 2.5 and 2.5 as well) on Civitai - </strong><a target=\"_blank\" rel=\"ugc\" href=\"https://civitai.com/models/4468/counterfeit-v25\"><strong>https://civitai.com/models/4468/counterfeit-v25</strong></a><br /><strong>If you like this embedding, please consider taking the time to give the repository a like and browsing their other work on HuggingFace.</strong><br /></p><p><strong>This embedding should be used in your NEGATIVE prompt. Adjust the strength as desired (seems to scale well without any distortions), the strength required may vary based on positive and negative prompts. Use the EasyNegative_pt (PickleTensors) version if you are unable to use SafeTensors embeddings.</strong><br /><br /><strong>Samples are, in order:</strong></p><ol><li><p><strong>sample01 - Counterfeit-V2.0.safetensors</strong></p></li><li><p><strong>sample02 - AbyssOrangeMix2_sfw.safetensors</strong></p></li><li><p><strong>sample03 - anything-v4.0-pruned.safetensors</strong></p></li><li><p><strong>Strength comparison using AbyssOrangeMix2_sfw.</strong></p></li></ol><p><br /><strong>From Author</strong><br />\"This is a Negative Embedding trained with Counterfeit. Please use it in the \"\\stable-diffusion-webui\\embeddings\" folder. It can be used with other models, but the effectiveness is not certain.\"</p>","task":"image-generation","tags":["anime","negative","negative embedding","textual inversion","embedding","tool"],"likes":0,"downloads":0,"lastModified":"2025-11-10T11:37:09.271Z","sources":[{"platform":"Civitai","url":"https://civitai.com/models/undefined"}]},{"id":"civitai-counterfeit-v3.0","name":"Counterfeit-V3.0","author":"Civitai Community","description":"<p>high quality anime style model.</p><p>Support☕ <a target=\"_blank\" rel=\"ugc\" href=\"https://ko-fi.com/sfa837348\">https://ko-fi.com/sfa837348</a></p><p>more info. <a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/gsdf/Counterfeit-V2.0\">https://huggingface.co/gsdf/Counterfeit-V2.0</a></p><p>Verson2.5 <a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/gsdf/Counterfeit-V2.5\">https://huggingface.co/gsdf/Counterfeit-V2.5</a></p><p>Verson3.0 <a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/gsdf/Counterfeit-V3.0\">https://huggingface.co/gsdf/Counterfeit-V3.0</a></p><p>EasyNegative <a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/datasets/gsdf/EasyNegative\">https://huggingface.co/datasets/gsdf/EasyNegative</a></p><p>(Use clip: openai/clip-vit-large-patch14-336)<br />EasyNegative(Negative Embedding) <a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/datasets/gsdf/EasyNegative\">https://huggingface.co/datasets/gsdf/EasyNegative</a></p><p></p><p><span style=\"color:rgb(209, 213, 219)\">Official hosting for online AI image generator. </span></p><ul><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://rendernet.ai/\">https://rendernet.ai/</a></p></li></ul>","task":"image-generation","tags":["anime","base model"],"likes":0,"downloads":0,"lastModified":"2025-11-10T11:37:09.271Z","sources":[{"platform":"Civitai","url":"https://civitai.com/models/undefined"}]},{"id":"civitai-rev-animated","name":"ReV Animated","author":"Civitai Community","description":"<p><em>April 28, 2024: added V2 Rebirth pruned</em></p><h1 id=\"heading-46\"><span style=\"color:rgb(64, 192, 87)\">v2:REBIRTH</span></h1><p><span style=\"color:rgb(230, 73, 128)\">Thanks to </span><span style=\"color:rgb(250, 82, 82)\">S6yx</span><span style=\"color:rgb(230, 73, 128)\"> for the creation of this beautiful model. Enjoyed by millions. With their permission, I, </span><span style=\"color:rgb(250, 82, 82)\">Zovya</span><span style=\"color:rgb(230, 73, 128)\">, will be maintaining it moving forward.</span></p><p></p><p><em>April 4, 2024: fp16 and +VAE added</em></p><p><em>April 2, 2024: Rebirth</em></p><p><em>Update 3: Disclaimer/Permissions updated</em></p><p><em>Update 2: I am no longer maintaining/updating this model</em></p><p><em>Update 1: I've been a bit burnt out on SD model development (SD in general tbh) and that is the reason there have not been an update. Looking to come back around and develop again by next month or so.Thank you everyone who sends reviews and enjoy my model</em><br /></p><p><strong>Pay attention to the <em><u>About this version</u></em></strong> <strong>section </strong>of model page<strong> for specific version information. ➡️➡️➡️➡️➡️</strong></p><h3 id=\"heading-416\"><br /><u>Model Overview:</u></h3><ul><li><p><u>rev</u> or <u>revision</u>: The concept of how the model generates images is likely to change as I see fit.</p></li><li><p><u>Animated</u>: The model has the ability to create 2.5D like image generations. This model is a checkpoint merge, meaning it is a product of other models to create a product that derives from the originals.</p></li><li><p>Kind of generations:</p><ul><li><p>Fantasy</p></li><li><p>Anime</p></li><li><p>semi-realistic</p></li><li><p><em>decent Landscape</em></p></li></ul></li><li><p>LoRA friendly</p></li><li><p>It works <strong><em><u>best on these resolution dimensions:</u></em></strong></p><ul><li><p>512x512</p></li><li><p>512x768</p></li><li><p>768x512</p></li></ul></li></ul><p></p><h3 id=\"heading-417\"><u>VAE</u>:</h3><ul><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/VAEs/orangemix.vae.pt\"><u>orangemix.vae.pt</u></a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main/vae\">kl-f8-anime2.ckpt</a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/NoCrypt/blessed_vae/blob/main/blessed2.vae.pt\">Blessed2.vae.pt</a></p><p><br /></p></li></ul><h3 id=\"heading-418\"><u>Prompting</u>:</h3><ul><li><p><strong>Order matters</strong> - words near the front of your prompt are weighted more heavily than the things in the back of your prompt.</p></li><li><p><strong>Prompt order</strong> - content type &gt; description &gt; style &gt; composition</p></li><li><p><strong>This model likes</strong>: ((best quality)), ((masterpiece)), (detailed) in beginning of prompt if you want anime-2.5D type</p></li><li><p>This model does great on<strong> <u>PORTRAITS</u></strong></p></li></ul><p></p><p><strong><u>Negative Prompt Embeddings:</u></strong></p><ul><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/embed/EasyNegative/tree/main\">EasyNegative</a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://civitai.com/models/4629/deep-negative-v1x\">Deep Negative</a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/embed/bad_prompt/blob/main/bad_prompt_version2.pt\">bad_prompt_version2</a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/nick-x-hacker/bad-artist/blob/main/bad-artist.pt\">bad-artist</a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/nick-x-hacker/bad-artist/blob/main/bad-artist-anime.pt\">bad-artist-anime</a></p></li><li><p><a target=\"_blank\" rel=\"ugc\" href=\"https://huggingface.co/p1atdev/badquality/tree/main\">bad-quality</a></p></li><li><p>Make use of weights in negative prompts (i.e (worst quality, low quality:1.4))</p><p></p></li></ul><p></p><h3 id=\"heading-419\"><u>Video Features</u></h3><p></p><p><a target=\"_blank\" rel=\"ugc\" href=\"https://youtu.be/Nl43zR5dVuM?t=192\">Olivio Sarikas - Why Is EVERYONE Using This Model?! - Rev Animated for Stable Diffusion / A1111</a></p><p></p><p><a target=\"_blank\" rel=\"ugc\" href=\"https://www.youtube.com/watch?v=A6dQPMy_tHY\">Olivio Sarikas - ULTRA SHARP Upscale! - Don't miss this Method!!! / A1111 - NEW Model</a><br /><br /><a target=\"_blank\" rel=\"ugc\" href=\"https://www.youtube.com/watch?v=ezNDCWhv4pQ\">AMAZING SD Models - And how to get the MOST out of them!</a></p><p></p><p></p><h2 id=\"heading-420\"><strong><u>Disclaimer (Updated 10/31/2023):</u></strong><br /></h2><p>The license type is <a target=\"_blank\" rel=\"ugc\" href=\"https://creativecommons.org/licenses/by-nc-nd/4.0\">CC BY-NC-ND 4.0</a> <br /><strong>Do not sell</strong> this model on any website without permissions from creator (me)</p><p><strong>Credit</strong> me if you use my model in your own merges</p><p><strong><u>You can use derivative models which uses ReV Animated for Buzz points and site-based currency that does not convert over to real world currency.</u></strong></p><p>Do not use this model to <strong><u>monetize</u></strong> on other platforms without expressed written consent. <br /><br /></p>","task":"image-generation","tags":["anime","base model","illustration","cartoon","fantasy","portraits"],"likes":0,"downloads":0,"lastModified":"2025-11-10T11:37:09.271Z","sources":[{"platform":"Civitai","url":"https://civitai.com/models/undefined"}]},{"id":"civitai-detail-tweaker-xl","name":"Detail Tweaker XL","author":"Civitai Community","description":"<p>Detail tweaker for SDXL.</p><p>Works with weights [-3, 3]</p><p>Use positive weight to increase details and negative weight to reduce details.</p><p>Good weight depends on your prompt and number of sampling steps, I recommend starting at 1.5 and then adjusting it.</p>","task":"image-generation","tags":["concept","detailed","detail","enhancer","undetailed"],"likes":0,"downloads":0,"lastModified":"2025-11-10T11:37:09.271Z","sources":[{"platform":"Civitai","url":"https://civitai.com/models/undefined"}]},{"id":"github-adampao-tagit-video","name":"tagit-video","author":"adampao","description":"TAGiT - AI-powered Chrome extension and web app for tagging and organizing YouTube video moments","task":"tool","tags":["ai-powered","ai-tool","annotation","browser-extension","chrome-extension","chrome-extension-v3","education","flashcards","knowledge-management","knowledge-retention","learning","note-taking","productivity","study-tool","summarization","tagit","typescript","video-bookmark","video-tagging","youtube"],"likes":0,"downloads":0,"lastModified":"2025-10-24T08:54:48Z","sources":[{"platform":"GitHub","url":"https://github.com/adampao/tagit-video"}]},{"id":"github-yoshi08010801-ai-subtitle-translator","name":"ai-subtitle-translator","author":"yoshi08010801","description":"An AI-powered subtitle translation tool using GPT & Whisper","task":"tool","tags":["ai-tool","gpt","openai","python","streamlit","subtitle","translation","whisper"],"likes":0,"downloads":0,"lastModified":"2025-05-30T05:43:30Z","sources":[{"platform":"GitHub","url":"https://github.com/yoshi08010801/ai-subtitle-translator"}]},{"id":"github-HappyRIO-sales-meeting-insights-ai-extension","name":"sales-meeting-insights-ai-extension","author":"HappyRIO","description":"Most AI tools help you after the call. PitchPulse helps you during it — guiding discovery and building your pitch in real time, so you can close while others guess.","task":"tool","tags":["ai-tool","analysis-services","built-for-closers","chrome-extension","close-rate","google-meet-extension","high-ticket-trained","live-insights","meeting-assistant","meeting-insights","openai","realtime","realtime-ai","sales-assistant","zoom-bot"],"likes":0,"downloads":0,"lastModified":"2025-07-24T07:52:45Z","sources":[{"platform":"GitHub","url":"https://github.com/HappyRIO/sales-meeting-insights-ai-extension"}]}];

    const searchInput = document.getElementById('search-input');
    const modelsGrid = document.getElementById('models-grid');
    const noResults = document.getElementById('no-results');
    
    // The template function is now self-contained here
    const modelCardTemplate = (model) => {
      const modelUrl = `/model/${model.id.replace(/\//g, '--')}`;
      const platforms = model.sources ? model.sources.map(s => s.platform).join(', ') : (model.sourcePlatform || 'N/A');
      const likes = model.likes?.toLocaleString() || '0';
      const downloads = model.downloads?.toLocaleString() || '0';

      return `
        <div class="border rounded-lg p-4 shadow-md hover:shadow-lg transition-shadow bg-white dark:bg-gray-800 flex flex-col">
          <h3 class="text-xl font-bold mb-2 text-gray-900 dark:text-white">${model.name}</h3>
          <p class="text-sm text-gray-600 dark:text-gray-400 mb-1">Platform(s): ${platforms}</p>
          <p class="text-sm text-gray-600 dark:text-gray-400 mb-3">Task: ${model.task}</p>
          <p class="text-gray-700 dark:text-gray-300 mb-4 line-clamp-2 flex-grow">${model.description}</p>
          <div class="flex items-center justify-between text-sm text-gray-500 dark:text-gray-400 border-t border-gray-200 dark:border-gray-700 pt-3 mt-3">
            <span class="flex items-center gap-1">❤️ ${likes}</span>
            <span class="flex items-center gap-1">↓ ${downloads}</span>
          </div>
          <a href="${modelUrl}" class="text-blue-500 hover:underline font-semibold mt-4 block text-right">View Details &rarr;</a>
        </div>
      `;
    };

    searchInput.addEventListener('input', (e) => {
      const query = e.target.value.toLowerCase();
      const filteredModels = allModels.filter(model => 
        model.name.toLowerCase().includes(query) ||
        model.description.toLowerCase().includes(query) ||
        (model.tags && model.tags.some(tag => tag.toLowerCase().includes(query)))
      );

      modelsGrid.innerHTML = filteredModels.map(modelCardTemplate).join('') || '<p class="text-center text-xl text-gray-500 col-span-full">No models found.</p>';
    });
  })();</script>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm">&copy; 2025 Free AI Tools. An open-source project to index the world of AI.</p> </div> </footer> </body></html>