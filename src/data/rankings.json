{
  "hot": [
    {
      "id": "github-ollama-ollama",
      "name": "ollama",
      "author": "ollama",
      "description": "Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.",
      "task": "tool",
      "tags": [
        "deepseek",
        "gemma",
        "gemma3",
        "gemma3n",
        "go",
        "golang",
        "gpt-oss",
        "llama",
        "llama2",
        "llama3",
        "llava",
        "llm",
        "llms",
        "mistral",
        "ollama",
        "phi4",
        "qwen"
      ],
      "likes": 468708,
      "downloads": 468708,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ollama/ollama",
          "homepage": "https://ollama.com",
          "language": "Go",
          "forks": 13681,
          "open_issues": 2256,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/151674099?v=4",
      "velocity": 171815.6,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-transformers",
      "name": "transformers",
      "author": "huggingface",
      "description": "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
      "task": "tool",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "likes": 458185,
      "downloads": 458185,
      "lastModified": "2025-11-20T03:54:06Z",
      "lastModifiedTimestamp": 1763610846000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/transformers",
          "homepage": "https://huggingface.co/transformers",
          "language": "Python",
          "forks": 31170,
          "open_issues": 2129,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 167968.9,
      "is_rising_star": true
    },
    {
      "id": "github-langflow-ai-langflow",
      "name": "langflow",
      "author": "langflow-ai",
      "description": "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.",
      "task": "tool",
      "tags": [
        "agents",
        "chatgpt",
        "generative-ai",
        "large-language-models",
        "multiagent",
        "react-flow"
      ],
      "likes": 416201,
      "downloads": 416201,
      "lastModified": "2025-11-20T03:45:31Z",
      "lastModifiedTimestamp": 1763610331000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langflow-ai/langflow",
          "homepage": "http://www.langflow.org",
          "language": "Python",
          "forks": 8014,
          "open_issues": 893,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85702467?v=4",
      "velocity": 152544.7,
      "is_rising_star": true
    },
    {
      "id": "github-f-awesome-chatgpt-prompts",
      "name": "awesome-chatgpt-prompts",
      "author": "f",
      "description": "This repo includes ChatGPT prompt curation to use ChatGPT and other LLM tools better.",
      "task": "tool",
      "tags": [
        "bots",
        "chatbot",
        "chatgpt",
        "chatgpt-api",
        "language"
      ],
      "likes": 410025,
      "downloads": 410025,
      "lastModified": "2025-11-20T03:59:38Z",
      "lastModifiedTimestamp": 1763611178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/f/awesome-chatgpt-prompts",
          "homepage": "https://prompts.chat",
          "language": "JavaScript",
          "forks": 18173,
          "open_issues": 289,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/196477?v=4",
      "velocity": 150315,
      "is_rising_star": true
    },
    {
      "id": "github-langchain-ai-langchain",
      "name": "langchain",
      "author": "langchain-ai",
      "description": "ü¶úüîó The platform for reliable agents.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "ai-agents-framework",
        "aiagentframework",
        "anthropic",
        "chatgpt",
        "enterprise",
        "framework",
        "gemini",
        "generative-ai",
        "langchain",
        "llm",
        "multiagent",
        "open-source",
        "openai",
        "pydantic",
        "python",
        "rag"
      ],
      "likes": 360158,
      "downloads": 360158,
      "lastModified": "2025-11-20T03:57:49Z",
      "lastModifiedTimestamp": 1763611069000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langchain-ai/langchain",
          "homepage": "https://docs.langchain.com/oss/python/langchain/",
          "language": "Python",
          "forks": 19766,
          "open_issues": 225,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/126733545?v=4",
      "velocity": 132006.6,
      "is_rising_star": true
    },
    {
      "id": "github-langgenius-dify",
      "name": "dify",
      "author": "langgenius",
      "description": "Production-ready platform for agentic workflow development.",
      "task": "tool",
      "tags": [
        "agent",
        "agentic-ai",
        "agentic-framework",
        "agentic-workflow",
        "ai",
        "automation",
        "gemini",
        "genai",
        "gpt",
        "gpt-4",
        "llm",
        "low-code",
        "mcp",
        "nextjs",
        "no-code",
        "openai",
        "orchestration",
        "python",
        "rag",
        "workflow"
      ],
      "likes": 357923,
      "downloads": 357923,
      "lastModified": "2025-11-20T03:40:43Z",
      "lastModifiedTimestamp": 1763610043000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langgenius/dify",
          "homepage": "https://dify.ai",
          "language": "TypeScript",
          "forks": 18481,
          "open_issues": 694,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127165244?v=4",
      "velocity": 131180.5,
      "is_rising_star": true
    },
    {
      "id": "github-open-webui-open-webui",
      "name": "open-webui",
      "author": "open-webui",
      "description": "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
      "task": "tool",
      "tags": [
        "ai",
        "llm",
        "llm-ui",
        "llm-webui",
        "llms",
        "mcp",
        "ollama",
        "ollama-webui",
        "open-webui",
        "openai",
        "openapi",
        "rag",
        "self-hosted",
        "ui",
        "webui"
      ],
      "likes": 347017,
      "downloads": 347017,
      "lastModified": "2025-11-20T04:00:22Z",
      "lastModifiedTimestamp": 1763611222000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/open-webui/open-webui",
          "homepage": "https://openwebui.com",
          "language": "JavaScript",
          "forks": 16200,
          "open_issues": 303,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/158137808?v=4",
      "velocity": 127171,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-generative-ai-for-beginners",
      "name": "generative-ai-for-beginners",
      "author": "microsoft",
      "description": "21 Lessons, Get Started Building with Generative AI ",
      "task": "tool",
      "tags": [
        "ai",
        "azure",
        "chatgpt",
        "dall-e",
        "generative-ai",
        "generativeai",
        "gpt",
        "language-model",
        "llms",
        "microsoft-for-beginners",
        "openai",
        "prompt-engineering",
        "semantic-search",
        "transformers"
      ],
      "likes": 306061,
      "downloads": 306061,
      "lastModified": "2025-11-20T03:55:34Z",
      "lastModifiedTimestamp": 1763610934000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/generative-ai-for-beginners",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 54213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 112205.5,
      "is_rising_star": true
    },
    {
      "id": "github-x1xhlol-system-prompts-and-models-of-ai-tools",
      "name": "system-prompts-and-models-of-ai-tools",
      "author": "x1xhlol",
      "description": "FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus Agent Tools, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, dia & v0. (And other Open Sourced) System Prompts, Internal Tools & AI Models",
      "task": "tool",
      "tags": [
        "ai",
        "bolt",
        "cluely",
        "copilot",
        "cursor",
        "cursorai",
        "devin",
        "github-copilot",
        "lovable",
        "open-source",
        "perplexity",
        "replit",
        "system-prompts",
        "trae",
        "trae-ai",
        "trae-ide",
        "v0",
        "vscode",
        "windsurf",
        "windsurf-ai"
      ],
      "likes": 288923,
      "downloads": 288923,
      "lastModified": "2025-11-20T04:00:54Z",
      "lastModifiedTimestamp": 1763611254000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
          "homepage": "",
          "language": null,
          "forks": 25879,
          "open_issues": 94,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/185671340?v=4",
      "velocity": 105805.7,
      "is_rising_star": true
    },
    {
      "id": "github-ggml-org-llama.cpp",
      "name": "llama.cpp",
      "author": "ggml-org",
      "description": "LLM inference in C/C++",
      "task": "tool",
      "tags": [
        "ggml"
      ],
      "likes": 270241,
      "downloads": 270241,
      "lastModified": "2025-11-20T03:56:01Z",
      "lastModifiedTimestamp": 1763610961000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ggml-org/llama.cpp",
          "homepage": "",
          "language": "C++",
          "forks": 13747,
          "open_issues": 899,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134263123?v=4",
      "velocity": 99053.9,
      "is_rising_star": true
    },
    {
      "id": "github-google-gemini-gemini-cli",
      "name": "gemini-cli",
      "author": "google-gemini",
      "description": "An open-source AI agent that brings the power of Gemini directly into your terminal.",
      "task": "tool",
      "tags": [
        "gemini",
        "gemini-api"
      ],
      "likes": 250395,
      "downloads": 250395,
      "lastModified": "2025-11-20T03:57:37Z",
      "lastModifiedTimestamp": 1763611057000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/google-gemini/gemini-cli",
          "homepage": "https://geminicli.com",
          "language": "TypeScript",
          "forks": 9384,
          "open_issues": 3007,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/161781182?v=4",
      "velocity": 91595.9,
      "is_rising_star": true
    },
    {
      "id": "github-Shubhamsaboo-awesome-llm-apps",
      "name": "awesome-llm-apps",
      "author": "Shubhamsaboo",
      "description": "Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.",
      "task": "tool",
      "tags": [
        "llms",
        "python",
        "rag"
      ],
      "likes": 237097,
      "downloads": 237097,
      "lastModified": "2025-11-20T04:00:02Z",
      "lastModifiedTimestamp": 1763611202000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
          "homepage": "https://www.theunwindai.com",
          "language": "Python",
          "forks": 10516,
          "open_issues": 1,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/31396011?v=4",
      "velocity": 86805.4,
      "is_rising_star": true
    },
    {
      "id": "github-rasbt-LLMs-from-scratch",
      "name": "LLMs-from-scratch",
      "author": "rasbt",
      "description": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "deep-learning",
        "from-scratch",
        "generative-ai",
        "gpt",
        "language-model",
        "large-language-models",
        "llm",
        "machine-learning",
        "neural-networks",
        "python",
        "pytorch",
        "transformers"
      ],
      "likes": 237017,
      "downloads": 237017,
      "lastModified": "2025-11-20T03:52:19Z",
      "lastModifiedTimestamp": 1763610739000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/rasbt/LLMs-from-scratch",
          "homepage": "https://amzn.to/4fqvn0D",
          "language": "Jupyter Notebook",
          "forks": 11706,
          "open_issues": 0,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/5618407?v=4",
      "velocity": 86865.9,
      "is_rising_star": true
    },
    {
      "id": "github-nomic-ai-gpt4all",
      "name": "gpt4all",
      "author": "nomic-ai",
      "description": "GPT4All: Run Local LLMs on Any Device. Open-source and available for commercial use.",
      "task": "tool",
      "tags": [
        "ai-chat",
        "llm-inference"
      ],
      "likes": 230773,
      "downloads": 230773,
      "lastModified": "2025-11-20T02:34:37Z",
      "lastModifiedTimestamp": 1763606077000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/nomic-ai/gpt4all",
          "homepage": "https://nomic.ai/gpt4all",
          "language": "C++",
          "forks": 8303,
          "open_issues": 744,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102670180?v=4",
      "velocity": 84613.1,
      "is_rising_star": true
    },
    {
      "id": "github-browser-use-browser-use",
      "name": "browser-use",
      "author": "browser-use",
      "description": "üåê Make websites accessible for AI agents. Automate tasks online with ease.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "ai-tools",
        "browser-automation",
        "browser-use",
        "llm",
        "playwright",
        "python"
      ],
      "likes": 218221,
      "downloads": 218221,
      "lastModified": "2025-11-20T03:31:37Z",
      "lastModifiedTimestamp": 1763609497000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/browser-use/browser-use",
          "homepage": "https://browser-use.com",
          "language": "Python",
          "forks": 8657,
          "open_issues": 229,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192012301?v=4",
      "velocity": 79990.9,
      "is_rising_star": true
    },
    {
      "id": "github-binary-husky-gpt_academic",
      "name": "gpt_academic",
      "author": "binary-husky",
      "description": "‰∏∫GPT/GLMÁ≠âLLMÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÂÆûÁî®Âåñ‰∫§‰∫íÊé•Âè£ÔºåÁâπÂà´‰ºòÂåñËÆ∫ÊñáÈòÖËØª/Ê∂¶Ëâ≤/ÂÜô‰Ωú‰ΩìÈ™åÔºåÊ®°ÂùóÂåñËÆæËÆ°ÔºåÊîØÊåÅËá™ÂÆö‰πâÂø´Êç∑ÊåâÈíÆ&ÂáΩÊï∞Êèí‰ª∂ÔºåÊîØÊåÅPythonÂíåC++Á≠âÈ°πÁõÆÂâñÊûê&Ëá™ËØëËß£ÂäüËÉΩÔºåPDF/LaTexËÆ∫ÊñáÁøªËØë&ÊÄªÁªìÂäüËÉΩÔºåÊîØÊåÅÂπ∂Ë°åÈóÆËØ¢Â§öÁßçLLMÊ®°ÂûãÔºåÊîØÊåÅchatglm3Á≠âÊú¨Âú∞Ê®°Âûã„ÄÇÊé•ÂÖ•ÈÄö‰πâÂçÉÈóÆ, deepseekcoder, ËÆØÈ£ûÊòüÁÅ´, ÊñáÂøÉ‰∏ÄË®Ä, llama2, rwkv, claude2, mossÁ≠â„ÄÇ",
      "task": "tool",
      "tags": [
        "academic",
        "chatglm-6b",
        "chatgpt",
        "gpt-4",
        "large-language-models"
      ],
      "likes": 209088,
      "downloads": 209088,
      "lastModified": "2025-11-20T03:43:15Z",
      "lastModifiedTimestamp": 1763610195000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/binary-husky/gpt_academic",
          "homepage": "https://github.com/binary-husky/gpt_academic/wiki/online",
          "language": "Python",
          "forks": 8401,
          "open_issues": 291,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/96192199?v=4",
      "velocity": 76661.2,
      "is_rising_star": true
    },
    {
      "id": "github-firecrawl-firecrawl",
      "name": "firecrawl",
      "author": "firecrawl",
      "description": "üî• The Web Data API for AI - Turn entire websites into LLM-ready markdown or structured data",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agents",
        "ai-crawler",
        "ai-scraping",
        "ai-search",
        "crawler",
        "data-extraction",
        "html-to-markdown",
        "llm",
        "markdown",
        "scraper",
        "scraping",
        "web-crawler",
        "web-data",
        "web-data-extraction",
        "web-scraper",
        "web-scraping",
        "web-search",
        "webscraping"
      ],
      "likes": 204312,
      "downloads": 204312,
      "lastModified": "2025-11-20T03:40:22Z",
      "lastModifiedTimestamp": 1763610022000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/firecrawl/firecrawl",
          "homepage": "https://firecrawl.dev",
          "language": "TypeScript",
          "forks": 5298,
          "open_issues": 140,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/135057108?v=4",
      "velocity": 74868.2,
      "is_rising_star": true
    },
    {
      "id": "github-infiniflow-ragflow",
      "name": "ragflow",
      "author": "infiniflow",
      "description": "RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs",
      "task": "tool",
      "tags": [
        "agent",
        "agentic",
        "agentic-ai",
        "agentic-workflow",
        "ai",
        "ai-search",
        "deep-learning",
        "deep-research",
        "deepseek",
        "deepseek-r1",
        "document-parser",
        "document-understanding",
        "graphrag",
        "llm",
        "mcp",
        "multi-agent",
        "ollama",
        "openai",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 204013,
      "downloads": 204013,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/infiniflow/ragflow",
          "homepage": "https://ragflow.io",
          "language": "Python",
          "forks": 7289,
          "open_issues": 2918,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/69962740?v=4",
      "velocity": 74763.7,
      "is_rising_star": true
    },
    {
      "id": "github-lobehub-lobe-chat",
      "name": "lobe-chat",
      "author": "lobehub",
      "description": "ü§Ø LobeHub - an open-source, modern design AI Agent Workspace. Supports multiple AI providers, Knowledge Base (file upload / RAG ), one click install MCP Marketplace and Artifacts / Thinking. One-click FREE deployment of your private AI Agent application.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "artifacts",
        "chat",
        "chatgpt",
        "claude",
        "deepseek",
        "deepseek-r1",
        "function-calling",
        "gemini",
        "gpt",
        "knowledge-base",
        "mcp",
        "nextjs",
        "ollama",
        "openai",
        "rag"
      ],
      "likes": 203567,
      "downloads": 203567,
      "lastModified": "2025-11-20T03:53:05Z",
      "lastModifiedTimestamp": 1763610785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/lobehub/lobe-chat",
          "homepage": "https://lobechat.com",
          "language": "TypeScript",
          "forks": 13985,
          "open_issues": 979,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131470832?v=4",
      "velocity": 74618.5,
      "is_rising_star": true
    },
    {
      "id": "github-mlabonne-llm-course",
      "name": "llm-course",
      "author": "mlabonne",
      "description": "Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.",
      "task": "tool",
      "tags": [
        "course",
        "large-language-models",
        "llm",
        "machine-learning",
        "roadmap"
      ],
      "likes": 203252,
      "downloads": 203252,
      "lastModified": "2025-11-20T03:54:32Z",
      "lastModifiedTimestamp": 1763610872000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mlabonne/llm-course",
          "homepage": "https://mlabonne.github.io/blog/",
          "language": null,
          "forks": 7672,
          "open_issues": 76,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/81252890?v=4",
      "velocity": 74476.6,
      "is_rising_star": true
    },
    {
      "id": "github-ansible-ansible",
      "name": "ansible",
      "author": "ansible",
      "description": "Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.",
      "task": "tool",
      "tags": [
        "ansible",
        "python"
      ],
      "likes": 201152,
      "downloads": 201152,
      "lastModified": "2025-11-20T00:37:21Z",
      "lastModifiedTimestamp": 1763599041000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ansible/ansible",
          "homepage": "https://www.ansible.com/",
          "language": "Python",
          "forks": 24129,
          "open_issues": 872,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1507452?v=4",
      "velocity": 73748.4,
      "is_rising_star": true
    },
    {
      "id": "github-dair-ai-Prompt-Engineering-Guide",
      "name": "Prompt-Engineering-Guide",
      "author": "dair-ai",
      "description": "üêô Guides, papers, lessons, notebooks and resources for prompt engineering, context engineering, RAG, and AI Agents.",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai-agents",
        "chatgpt",
        "deep-learning",
        "generative-ai",
        "language-model",
        "llms",
        "openai",
        "prompt-engineering",
        "rag"
      ],
      "likes": 199720,
      "downloads": 199720,
      "lastModified": "2025-11-20T03:58:38Z",
      "lastModifiedTimestamp": 1763611118000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
          "homepage": "https://www.promptingguide.ai/",
          "language": "MDX",
          "forks": 6945,
          "open_issues": 230,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/30384625?v=4",
      "velocity": 73213.8,
      "is_rising_star": true
    },
    {
      "id": "github-OpenHands-OpenHands",
      "name": "OpenHands",
      "author": "OpenHands",
      "description": "üôå OpenHands: Code Less, Make More",
      "task": "tool",
      "tags": [
        "agent",
        "artificial-intelligence",
        "chatgpt",
        "claude-ai",
        "cli",
        "developer-tools",
        "gpt",
        "llm",
        "openai"
      ],
      "likes": 195291,
      "downloads": 195291,
      "lastModified": "2025-11-20T03:44:01Z",
      "lastModifiedTimestamp": 1763610241000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenHands/OpenHands",
          "homepage": "https://all-hands.dev",
          "language": "Python",
          "forks": 7928,
          "open_issues": 242,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/225919603?v=4",
      "velocity": 71589.1,
      "is_rising_star": true
    },
    {
      "id": "github-PaddlePaddle-PaddleOCR",
      "name": "PaddleOCR",
      "author": "PaddlePaddle",
      "description": "Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.",
      "task": "tool",
      "tags": [
        "ai4science",
        "chineseocr",
        "document-parsing",
        "document-translation",
        "kie",
        "ocr",
        "paddleocr-vl",
        "pdf-extractor-rag",
        "pdf-parser",
        "pdf2markdown",
        "pp-ocr",
        "pp-structure",
        "rag"
      ],
      "likes": 192979,
      "downloads": 192979,
      "lastModified": "2025-11-20T03:32:54Z",
      "lastModifiedTimestamp": 1763609574000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/PaddlePaddle/PaddleOCR",
          "homepage": "https://www.paddleocr.ai",
          "language": "Python",
          "forks": 9361,
          "open_issues": 276,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23534030?v=4",
      "velocity": 70700.3,
      "is_rising_star": true
    },
    {
      "id": "github-vllm-project-vllm",
      "name": "vllm",
      "author": "vllm-project",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "task": "tool",
      "tags": [
        "amd",
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-v3",
        "gpt",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llm",
        "llm-serving",
        "model-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen",
        "qwen3",
        "tpu",
        "transformer"
      ],
      "likes": 190450,
      "downloads": 190450,
      "lastModified": "2025-11-20T03:53:27Z",
      "lastModifiedTimestamp": 1763610807000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/vllm-project/vllm",
          "homepage": "https://docs.vllm.ai",
          "language": "Python",
          "forks": 11390,
          "open_issues": 3172,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/136984999?v=4",
      "velocity": 69779.6,
      "is_rising_star": true
    },
    {
      "id": "github-hiyouga-LLaMA-Factory",
      "name": "LLaMA-Factory",
      "author": "hiyouga",
      "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "deepseek",
        "fine-tuning",
        "gemma",
        "gpt",
        "instruction-tuning",
        "large-language-models",
        "llama",
        "llama3",
        "llm",
        "lora",
        "moe",
        "nlp",
        "peft",
        "qlora",
        "quantization",
        "qwen",
        "rlhf",
        "transformers"
      ],
      "likes": 188197,
      "downloads": 188197,
      "lastModified": "2025-11-20T03:49:20Z",
      "lastModifiedTimestamp": 1763610560000,
      "readme": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)\n[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)\n[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)\n\n[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory)\n[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align=\"center\" markdown=\"1\">\n\n### Supporters ‚ù§Ô∏è\n\n| <div style=\"text-align: center;\"><a href=\"https://warp.dev/llama-factory\"><img alt=\"Warp sponsorship\" width=\"400\" src=\"assets/sponsors/warp.jpg\"></a><br><a href=\"https://warp.dev/llama-factory\" style=\"font-size:larger;\">Warp, the agentic terminal for developers</a><br><a href=\"https://warp.dev/llama-factory\">Available for MacOS, Linux, & Windows</a> | <a href=\"https://serpapi.com\"><img alt=\"SerpAPI sponsorship\" width=\"250\" src=\"assets/sponsors/serpapi.svg\"> </a> |\n| ---- | ---- |\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\nüëã Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.\n\n\\[ English | [‰∏≠Êñá](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nStart local training:\n- Please refer to [usage](#getting-started)\n\nStart cloud training:\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory\n- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\nRead technical notes:\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/\n- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html\n- **Official Blog**: https://blog.llamafactory.net/en/\n- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [LLaMA Factory Online](#llama-factory-online)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                           |\n| ------------ | -------------------------------------------------------------------- |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |\n\n## Blogs\n\n> [!TIP]\n> Now we have a dedicated blog for LLaMA Factory!\n>\n> Website: https://blog.llamafactory.net/en/\n\n- üí° [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&type=project&utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)\n- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n\n<details><summary>All Blogs</summary>\n\n- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n- [A One-Stop Code-Free Model Fine-Tuning \\& Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.\n\n[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.\n\n[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.\n\n[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.\n\n<details><summary>Full Changelog</summary>\n\n[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)'s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)'s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)'s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template             |\n| ----------------------------------------------------------------- | -------------------------------- | -------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2            |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                    |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3             |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere               |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek             |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3            |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1           |\n| [ERNIE-4.5](https://huggingface.co/baidu)                         | 0.3B/21B/300B                    | ernie/ernie_nothink  |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon               |\n| [Falcon-H1](https://huggingface.co/tiiuae)                        | 0.5B/1.5B/3B/7B/34B              | falcon_h1            |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma/gemma2         |\n| [Gemma 3/Gemma 3n](https://huggingface.co/google)                 | 270M/1B/4B/6B/8B/12B/27B         | gemma3/gemma3n       |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/zai-org)         | 9B/32B                           | glm4/glmz1           |\n| [GLM-4.1V](https://huggingface.co/zai-org)                        | 9B                               | glm4v                |\n| [GLM-4.5/GLM-4.5V](https://huggingface.co/zai-org)                | 106B/355B                        | glm4_moe/glm4v_moe   |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                    |\n| [GPT-OSS](https://huggingface.co/openai)                          | 20B/120B                         | gpt                  |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3             |\n| [Granite 4](https://huggingface.co/ibm-granite)                   | 7B                               | granite4             |\n| [Hunyuan (MT)](https://huggingface.co/tencent/)                   | 7B                               | hunyuan              |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index                |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2              |\n| [InternVL 2.5-3.5](https://huggingface.co/OpenGVLab)              | 1B/2B/4B/8B/14B/30B/38B/78B/241B | intern_vl            |\n| [InternLM/Intern-S1-mini](https://huggingface.co/internlm/)       | 8B                               | intern_s1            |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl              |\n| [Ling 2.0 (mini/flash)](https://huggingface.co/inclusionAI)       | 16B/100B                         | bailing_v2           |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                    |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2               |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3               |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4               |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama               |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava                |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next           |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video     |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                 |\n| [MiniCPM 1-4.1](https://huggingface.co/openbmb)                   | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4        |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v  |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral            |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral              |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small        |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                    |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma            |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                    |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                  |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small            |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                 |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral              |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                 |\n| [Qwen3 (MoE/Instruct/Thinking/Next)](https://huggingface.co/Qwen) | 0.6B/1.7B/4B/8B/14B/32B/80B/235B | qwen3/qwen3_nothink  |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio          |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni           |\n| [Qwen3-Omni](https://huggingface.co/Qwen)                         | 30B                              | qwen3_omni           |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl             |\n| [Qwen3-VL](https://huggingface.co/Qwen)                           | 2B/4B/8B/30B/32B/235B            | qwen3_vl             |\n| [Seed (OSS/Coder)](https://huggingface.co/ByteDance-Seed)         | 8B/36B                           | seed_oss/seed_coder  |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1           |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                    |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2            |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse               |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                   |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl                |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                 |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> If the model has both reasoning and non-reasoning versions, please use the `_nothink` suffix to distinguish between them. For example, `qwen3` and `qwen3_nothink`.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> \\*: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> \\*\\*: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |        OFT         |        QOFT        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [CCI3-HQ (zh)](https://huggingface.co/datasets/BAAI/CCI3-HQ)\n- [CCI3-Data (zh)](https://huggingface.co/datasets/BAAI/CCI3-Data)\n- [CCI4.0-M2-Base-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1)\n- [CCI4.0-M2-CoT-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1)\n- [CCI4.0-M2-Extra-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [Infinity Instruct (zh)](https://huggingface.co/datasets/BAAI/Infinity-Instruct)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install \"huggingface_hub<1.0.0\"\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.49.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method                              | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ----------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)             |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)                  |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam/OFT |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA / QOFT                        |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA / QOFT                        |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA / QOFT                        |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n#### Install from Source\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev\n\n#### Install from Docker Image\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nThis image is built on Ubuntu 22.04 (x86\\_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.\n\nFind the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags\n\nPlease refer to [build docker](#build-docker) to build the image yourself.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can't pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**, **[DataFlow](https://github.com/OpenDCAI/DataFlow)** and **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### LLaMA Factory Online\n\nRead our [documentation](https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory).\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Use Docker volumes</summary>\n\nYou can uncomment `VOLUME [ \"/root/.cache/huggingface\", \"/app/shared_data\", \"/app/output\" ]` in the Dockerfile to use data volumes.\n\nWhen building the Docker image, use `-v ./hf_cache:/root/.cache/huggingface` argument to mount the local directory to the container. The following data volumes are available.\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine.\n- `shared_data`: The directionary to store datasets on the host machine.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [[paper]](https://aclanthology.org/2024.findings-acl.830.pdf)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n1. **[EmoLLM](https://github.com/SmartFlowAI/EmoLLM)**: A project about large language models (LLMs) and mental health.\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/hiyouga/LLaMA-Factory",
          "homepage": "https://llamafactory.readthedocs.io",
          "language": "Python",
          "forks": 7592,
          "open_issues": 782,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/16256802?v=4",
      "velocity": 68968.9,
      "is_rising_star": true
    },
    {
      "id": "github-FoundationAgents-MetaGPT",
      "name": "MetaGPT",
      "author": "FoundationAgents",
      "description": "üåü The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "task": "tool",
      "tags": [
        "agent",
        "gpt",
        "llm",
        "metagpt",
        "multi-agent"
      ],
      "likes": 178722,
      "downloads": 178722,
      "lastModified": "2025-11-20T03:51:26Z",
      "lastModifiedTimestamp": 1763610686000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FoundationAgents/MetaGPT",
          "homepage": "https://mgx.dev/",
          "language": "Python",
          "forks": 7283,
          "open_issues": 58,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/198047230?v=4",
      "velocity": 65516,
      "is_rising_star": true
    },
    {
      "id": "github-unclecode-crawl4ai",
      "name": "crawl4ai",
      "author": "unclecode",
      "description": "üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN",
      "task": "tool",
      "tags": [],
      "likes": 168306,
      "downloads": 168306,
      "lastModified": "2025-11-20T03:44:18Z",
      "lastModifiedTimestamp": 1763610258000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unclecode/crawl4ai",
          "homepage": "https://crawl4ai.com",
          "language": "Python",
          "forks": 5627,
          "open_issues": 262,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/12494079?v=4",
      "velocity": 61679.2,
      "is_rising_star": true
    },
    {
      "id": "github-OpenBB-finance-OpenBB",
      "name": "OpenBB",
      "author": "OpenBB-finance",
      "description": "Financial data platform for analysts, quants and AI agents.",
      "task": "tool",
      "tags": [
        "ai",
        "crypto",
        "derivatives",
        "economics",
        "equity",
        "finance",
        "fixed-income",
        "machine-learning",
        "openbb",
        "options",
        "python",
        "quantitative-finance",
        "stocks"
      ],
      "likes": 163943,
      "downloads": 163943,
      "lastModified": "2025-11-20T03:23:51Z",
      "lastModifiedTimestamp": 1763609031000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenBB-finance/OpenBB",
          "homepage": "https://openbb.co",
          "language": "Python",
          "forks": 5282,
          "open_issues": 51,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/80064875?v=4",
      "velocity": 60102.9,
      "is_rising_star": true
    },
    {
      "id": "github-simular-ai-Agent-S",
      "name": "Agent-S",
      "author": "simular-ai",
      "description": "Agent S: an open agentic framework that uses computers like a human",
      "task": "tool",
      "tags": [
        "agent-computer-interface",
        "ai-agents",
        "computer-automation",
        "computer-use",
        "computer-use-agent",
        "cua",
        "grounding",
        "gui-agents",
        "in-context-reinforcement-learning",
        "memory",
        "mllm",
        "planning",
        "retrieval-augmented-generation",
        "agents",
        "anthropic",
        "anthropic-claude",
        "automation",
        "claude",
        "claude-code",
        "claude-code-cli",
        "claude-code-commands",
        "claude-code-plugin",
        "claude-code-plugins",
        "claude-code-subagents",
        "claude-skills",
        "claudecode",
        "claudecode-config",
        "claudecode-subagents",
        "orchestration",
        "sub-agents",
        "subagents",
        "workflows",
        "ai",
        "openai",
        "real-time",
        "video",
        "voice",
        "autonomous-agents",
        "language-model",
        "llm"
      ],
      "likes": 160186,
      "downloads": 160186,
      "lastModified": "2025-11-20T03:36:25Z",
      "lastModifiedTimestamp": 1763609785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/simular-ai/Agent-S",
          "homepage": "https://www.simular.ai",
          "language": "Python",
          "forks": 905,
          "open_issues": 13,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/wshobson/agents",
          "homepage": "https://sethhobson.com",
          "language": "Python",
          "forks": 2349,
          "open_issues": 4,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/contains-studio/agents",
          "homepage": null,
          "language": null,
          "forks": 2126,
          "open_issues": 9,
          "license": "No license"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/livekit/agents",
          "homepage": "https://docs.livekit.io/agents",
          "language": "Python",
          "forks": 1768,
          "open_issues": 452,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/aiwaves-cn/agents",
          "homepage": "",
          "language": "Python",
          "forks": 452,
          "open_issues": 39,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/99358647?v=4",
      "velocity": 58689.4,
      "is_rising_star": true
    },
    {
      "id": "github-cline-cline",
      "name": "cline",
      "author": "cline",
      "description": "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.",
      "task": "tool",
      "tags": [],
      "likes": 157519,
      "downloads": 157519,
      "lastModified": "2025-11-20T03:55:20Z",
      "lastModifiedTimestamp": 1763610920000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/cline/cline",
          "homepage": "https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev",
          "language": "TypeScript",
          "forks": 5242,
          "open_issues": 881,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/184127137?v=4",
      "velocity": 57740.1,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-autogen",
      "name": "autogen",
      "author": "microsoft",
      "description": "A programming framework for agentic AI",
      "task": "tool",
      "tags": [
        "agentic",
        "agentic-agi",
        "agents",
        "ai",
        "autogen",
        "autogen-ecosystem",
        "chatgpt",
        "framework",
        "llm-agent",
        "llm-framework"
      ],
      "likes": 155426,
      "downloads": 155426,
      "lastModified": "2025-11-20T03:15:25Z",
      "lastModifiedTimestamp": 1763608525000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/autogen",
          "homepage": "https://microsoft.github.io/autogen/",
          "language": "Python",
          "forks": 7866,
          "open_issues": 509,
          "license": "Creative Commons Attribution 4.0 International"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 56980,
      "is_rising_star": true
    },
    {
      "id": "github-Mintplex-Labs-anything-llm",
      "name": "anything-llm",
      "author": "Mintplex-Labs",
      "description": "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "custom-ai-agents",
        "deepseek",
        "kimi",
        "llama3",
        "llm",
        "lmstudio",
        "local-llm",
        "localai",
        "mcp",
        "mcp-servers",
        "moonshot",
        "multimodal",
        "no-code",
        "ollama",
        "qwen3",
        "rag",
        "vector-database",
        "web-scraping"
      ],
      "likes": 153633,
      "downloads": 153633,
      "lastModified": "2025-11-20T03:46:02Z",
      "lastModifiedTimestamp": 1763610362000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Mintplex-Labs/anything-llm",
          "homepage": "https://anythingllm.com",
          "language": "JavaScript",
          "forks": 5414,
          "open_issues": 345,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134426827?v=4",
      "velocity": 56316.7,
      "is_rising_star": true
    },
    {
      "id": "github-openai-codex",
      "name": "codex",
      "author": "openai",
      "description": "Lightweight coding agent that runs in your terminal",
      "task": "tool",
      "tags": [],
      "likes": 152654,
      "downloads": 152654,
      "lastModified": "2025-11-20T03:47:36Z",
      "lastModifiedTimestamp": 1763610456000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openai/codex",
          "homepage": "",
          "language": "Rust",
          "forks": 6374,
          "open_issues": 1043,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14957082?v=4",
      "velocity": 55924,
      "is_rising_star": true
    },
    {
      "id": "github-pathwaycom-pathway",
      "name": "pathway",
      "author": "pathwaycom",
      "description": "Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.",
      "task": "tool",
      "tags": [
        "batch-processing",
        "data-analytics",
        "data-pipelines",
        "data-processing",
        "dataflow",
        "etl",
        "etl-framework",
        "iot-analytics",
        "kafka",
        "machine-learning-algorithms",
        "pathway",
        "python",
        "real-time",
        "rust",
        "stream-processing",
        "streaming",
        "time-series-analysis"
      ],
      "likes": 150231,
      "downloads": 150231,
      "lastModified": "2025-11-20T03:36:07Z",
      "lastModifiedTimestamp": 1763609767000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pathwaycom/pathway",
          "homepage": "https://pathway.com",
          "language": "Python",
          "forks": 1453,
          "open_issues": 39,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25750857?v=4",
      "velocity": 55056.1,
      "is_rising_star": true
    },
    {
      "id": "github-opendatalab-MinerU",
      "name": "MinerU",
      "author": "opendatalab",
      "description": "Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.",
      "task": "tool",
      "tags": [
        "ai4science",
        "document-analysis",
        "extract-data",
        "layout-analysis",
        "ocr",
        "parser",
        "pdf",
        "pdf-converter",
        "pdf-extractor-llm",
        "pdf-extractor-pretrain",
        "pdf-extractor-rag",
        "pdf-parser",
        "python"
      ],
      "likes": 147376,
      "downloads": 147376,
      "lastModified": "2025-11-20T03:56:20Z",
      "lastModifiedTimestamp": 1763610980000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/opendatalab/MinerU",
          "homepage": "https://opendatalab.github.io/MinerU/",
          "language": "Python",
          "forks": 4072,
          "open_issues": 123,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/97503431?v=4",
      "velocity": 54001.2,
      "is_rising_star": true
    },
    {
      "id": "github-unslothai-unsloth",
      "name": "unsloth",
      "author": "unslothai",
      "description": "Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.",
      "task": "tool",
      "tags": [
        "agent",
        "deepseek",
        "deepseek-r1",
        "fine-tuning",
        "gemma",
        "gemma3",
        "gpt-oss",
        "llama",
        "llama3",
        "llm",
        "llms",
        "mistral",
        "openai",
        "qwen",
        "qwen3",
        "reinforcement-learning",
        "text-to-speech",
        "tts",
        "unsloth",
        "voice-cloning"
      ],
      "likes": 145378,
      "downloads": 145378,
      "lastModified": "2025-11-20T03:57:06Z",
      "lastModifiedTimestamp": 1763611026000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unslothai/unsloth",
          "homepage": "https://docs.unsloth.ai/",
          "language": "Python",
          "forks": 3987,
          "open_issues": 862,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/150920049?v=4",
      "velocity": 53275.2,
      "is_rising_star": true
    },
    {
      "id": "github-huginn-huginn",
      "name": "huginn",
      "author": "huginn",
      "description": "Create agents that monitor and act on your behalf.  Your agents are standing by!",
      "task": "tool",
      "tags": [
        "agent",
        "automation",
        "feed",
        "feedgenerator",
        "huginn",
        "monitoring",
        "notifications",
        "rss",
        "scraper",
        "twitter",
        "twitter-streaming",
        "webscraping"
      ],
      "likes": 144302,
      "downloads": 144302,
      "lastModified": "2025-11-20T03:59:01Z",
      "lastModifiedTimestamp": 1763611141000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huginn/huginn",
          "homepage": "",
          "language": "Ruby",
          "forks": 4194,
          "open_issues": 693,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23225142?v=4",
      "velocity": 52906.7,
      "is_rising_star": true
    },
    {
      "id": "github-harry0703-MoneyPrinterTurbo",
      "name": "MoneyPrinterTurbo",
      "author": "harry0703",
      "description": "Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.",
      "task": "tool",
      "tags": [
        "ai",
        "automation",
        "chatgpt",
        "moviepy",
        "python",
        "shortvideo",
        "tiktok"
      ],
      "likes": 143513,
      "downloads": 143513,
      "lastModified": "2025-11-20T03:41:13Z",
      "lastModifiedTimestamp": 1763610073000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/harry0703/MoneyPrinterTurbo",
          "homepage": "",
          "language": "Python",
          "forks": 6693,
          "open_issues": 217,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/4928832?v=4",
      "velocity": 52607.5,
      "is_rising_star": true
    },
    {
      "id": "github-pathwaycom-llm-app",
      "name": "llm-app",
      "author": "pathwaycom",
      "description": "Ready-to-run cloud templates for RAG, AI pipelines, and enterprise search with live data. üê≥Docker-friendly.‚ö°Always in sync with Sharepoint, Google Drive, S3, Kafka, PostgreSQL, real-time data APIs, and more.",
      "task": "tool",
      "tags": [
        "chatbot",
        "hugging-face",
        "llm",
        "llm-local",
        "llm-prompting",
        "llm-security",
        "llmops",
        "machine-learning",
        "open-ai",
        "pathway",
        "rag",
        "real-time",
        "retrieval-augmented-generation",
        "vector-database",
        "vector-index"
      ],
      "likes": 141740,
      "downloads": 141740,
      "lastModified": "2025-11-20T03:27:51Z",
      "lastModifiedTimestamp": 1763609271000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pathwaycom/llm-app",
          "homepage": "https://pathway.com/developers/templates/",
          "language": "Jupyter Notebook",
          "forks": 1213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25750857?v=4",
      "velocity": 51950.8,
      "is_rising_star": true
    },
    {
      "id": "github-FlowiseAI-Flowise",
      "name": "Flowise",
      "author": "FlowiseAI",
      "description": "Build AI Agents, Visually",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-workflow",
        "agents",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "javascript",
        "langchain",
        "large-language-models",
        "low-code",
        "multiagent-systems",
        "no-code",
        "openai",
        "rag",
        "react",
        "typescript",
        "workflow-automation"
      ],
      "likes": 140056,
      "downloads": 140056,
      "lastModified": "2025-11-20T03:45:42Z",
      "lastModifiedTimestamp": 1763610342000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FlowiseAI/Flowise",
          "homepage": "https://flowiseai.com",
          "language": "TypeScript",
          "forks": 23126,
          "open_issues": 723,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128289781?v=4",
      "velocity": 51339.2,
      "is_rising_star": true
    },
    {
      "id": "github-run-llama-llama_index",
      "name": "llama_index",
      "author": "run-llama",
      "description": "LlamaIndex is the leading framework for building LLM-powered agents over your data.",
      "task": "tool",
      "tags": [
        "agents",
        "application",
        "data",
        "fine-tuning",
        "framework",
        "llamaindex",
        "llm",
        "multi-agents",
        "rag",
        "vector-database"
      ],
      "likes": 135951,
      "downloads": 135951,
      "lastModified": "2025-11-20T02:11:43Z",
      "lastModifiedTimestamp": 1763604703000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/run-llama/llama_index",
          "homepage": "https://developers.llamaindex.ai",
          "language": "Python",
          "forks": 6524,
          "open_issues": 262,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/130722866?v=4",
      "velocity": 49837.7,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-ai-agents-for-beginners",
      "name": "ai-agents-for-beginners",
      "author": "microsoft",
      "description": "12 Lessons to Get Started Building AI Agents",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-framework",
        "agentic-rag",
        "ai-agents",
        "ai-agents-framework",
        "autogen",
        "generative-ai",
        "semantic-kernel"
      ],
      "likes": 135593,
      "downloads": 135593,
      "lastModified": "2025-11-20T03:40:06Z",
      "lastModifiedTimestamp": 1763610006000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/ai-agents-for-beginners",
          "homepage": "https://aka.ms/ai-agents-beginners",
          "language": "Jupyter Notebook",
          "forks": 15322,
          "open_issues": 11,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 49688.1,
      "is_rising_star": true
    },
    {
      "id": "github-jeecgboot-JeecgBoot",
      "name": "JeecgBoot",
      "author": "jeecgboot",
      "description": "üî•AI‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÔºåÂä©Âäõ‰ºÅ‰∏öÂø´ÈÄüÂÆûÁé∞‰Ωé‰ª£Á†ÅÂºÄÂèëÂíåÊûÑÂª∫AIÂ∫îÁî®ÔºÅ ÈõÜÊàê‰∏ÄÂ•óÂÆåÊï¥AIÂ∫îÁî®Âπ≥Âè∞ÔºöÊ∂µÁõñAIÂ∫îÁî®„ÄÅAIÊ®°Âûã„ÄÅAIËÅäÂ§©Âä©Êâã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAIÊµÅÁ®ãÁºñÊéíÁ≠âÔºåÂÖºÂÆπÂ§öÁßçÂ§ßÊ®°ÂûãÔºõÊèê‰æõÂº∫Â§ß‰ª£Á†ÅÁîüÊàêÂô®ÔºöÂÆûÁé∞ÂâçÂêéÁ´Ø‰∏ÄÈîÆÁîüÊàêÔºåÊó†ÈúÄÊâãÂÜô‰ª£Á†Å! ÂºïÈ¢ÜAIÂºÄÂèëÊ®°ÂºèÔºöAIÁîüÊàê‚ÜíÂú®Á∫øÈÖçÁΩÆ‚Üí‰ª£Á†ÅÁîüÊàê‚ÜíÊâãÂ∑•ÂêàÂπ∂ÔºåËß£ÂÜ≥JavaÈ°πÁõÆ80%ÈáçÂ§çÂ∑•‰ΩúÔºåÊèêÂçáÊïàÁéáËäÇÁúÅÊàêÊú¨ÔºåÂèà‰∏çÂ§±ÁÅµÊ¥ª~",
      "task": "tool",
      "tags": [
        "activiti",
        "agent",
        "ai",
        "aiflow",
        "ant-design-vue",
        "antd",
        "codegenerator",
        "deepseek",
        "flowable",
        "langchain4j",
        "llm",
        "low-code",
        "mcp",
        "mybatis-plus",
        "rag",
        "spring-ai",
        "springboot",
        "springboot3",
        "springcloud",
        "vue3"
      ],
      "likes": 133232,
      "downloads": 133232,
      "lastModified": "2025-11-20T03:18:17Z",
      "lastModifiedTimestamp": 1763608697000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/jeecgboot/JeecgBoot",
          "homepage": "https://jeecgboot.github.io/JeecgBoot/",
          "language": "Java",
          "forks": 15659,
          "open_issues": 53,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/86360035?v=4",
      "velocity": 48846.6,
      "is_rising_star": true
    },
    {
      "id": "github-mem0ai-mem0",
      "name": "mem0",
      "author": "mem0ai",
      "description": "Universal memory layer for AI Agents",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "application",
        "chatbots",
        "chatgpt",
        "genai",
        "hacktoberfest",
        "llm",
        "long-term-memory",
        "memory",
        "memory-management",
        "python",
        "rag",
        "state-management"
      ],
      "likes": 129974,
      "downloads": 129974,
      "lastModified": "2025-11-20T03:24:00Z",
      "lastModifiedTimestamp": 1763609040000,
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"docs/images/banner-sm.png\" width=\"800px\" alt=\"Mem0 - The Memory Layer for Personalized AI\">\n  </a>\n</p>\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/11194\" target=\"blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/11194\" alt=\"mem0ai%2Fmem0 | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai\">Learn more</a>\n  ¬∑\n  <a href=\"https://mem0.dev/DiG\">Join Discord</a>\n  ¬∑\n  <a href=\"https://mem0.dev/demo\">Demo</a>\n  ¬∑\n  <a href=\"https://mem0.dev/openmemory\">OpenMemory</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.dev/DiG\">\n    <img src=\"https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white\" alt=\"Mem0 Discord\">\n  </a>\n  <a href=\"https://pepy.tech/project/mem0ai\">\n    <img src=\"https://img.shields.io/pypi/dm/mem0ai\" alt=\"Mem0 PyPI - Downloads\">\n  </a>\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square\" alt=\"GitHub commit activity\">\n  </a>\n  <a href=\"https://pypi.org/project/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/pypi/v/mem0ai?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n  </a>\n  <a href=\"https://www.npmjs.com/package/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/npm/v/mem0ai\" alt=\"Npm package\">\n  </a>\n  <a href=\"https://www.ycombinator.com/companies/mem0\">\n    <img src=\"https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square\" alt=\"Y Combinator S24\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai/research\"><strong>üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí</strong></a>\n</p>\n<p align=\"center\">\n  <strong>‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens</strong>\n</p>\n\n> **üéâ mem0ai v1.0.0 is now available!** This major release includes API modernization, improved vector store support, and enhanced GCP integration. [See migration guide ‚Üí](MIGRATION_GUIDE_v1.0.md)\n\n##  üî• Research Highlights\n- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark\n- **91% Faster Responses** than full-context, ensuring low-latency at scale\n- **90% Lower Token Usage** than full-context, cutting costs without compromise\n- [Read the full paper](https://mem0.ai/research)\n\n# Introduction\n\n[Mem0](https://mem0.ai) (\"mem-zero\") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.\n\n### Key Features & Use Cases\n\n**Core Capabilities:**\n- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization\n- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option\n\n**Applications:**\n- **AI Assistants**: Consistent, context-rich conversations\n- **Customer Support**: Recall past tickets and user history for tailored help\n- **Healthcare**: Track patient preferences and history for personalized care\n- **Productivity & Gaming**: Adaptive workflows and environments based on user behavior\n\n## üöÄ Quickstart Guide <a name=\"quickstart\"></a>\n\nChoose between our hosted platform or self-hosted package:\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [Mem0 Platform](https://app.mem0.ai)\n2. Embed the memory layer via SDK or API keys\n\n### Self-Hosted (Open Source)\n\nInstall the sdk via pip:\n\n```bash\npip install mem0ai\n```\n\nInstall sdk via npm:\n```bash\nnpm install mem0ai\n```\n\n### Basic Usage\n\nMem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).\n\nFirst step is to instantiate the memory:\n\n```python\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n\n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nFor detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).\n\n## üîó Integrations & Demos\n\n- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))\n- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))\n- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))\n- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))\n\n## üìö Documentation & Support\n\n- Full docs: https://docs.mem0.ai\n- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)\n- Contact: founders@mem0.ai\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n```\n\n## ‚öñÔ∏è License\n\nApache 2.0 ‚Äî see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mem0ai/mem0",
          "homepage": "https://mem0.ai",
          "language": "Python",
          "forks": 4691,
          "open_issues": 516,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/137054526?v=4",
      "velocity": 47636.6,
      "is_rising_star": true
    },
    {
      "id": "github-anthropics-claude-code",
      "name": "claude-code",
      "author": "anthropics",
      "description": "Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.",
      "task": "tool",
      "tags": [],
      "likes": 128639,
      "downloads": 128639,
      "lastModified": "2025-11-20T03:51:58Z",
      "lastModifiedTimestamp": 1763610718000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/anthropics/claude-code",
          "homepage": "https://code.claude.com/docs/en/overview",
          "language": "Shell",
          "forks": 2893,
          "open_issues": 5277,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "velocity": 47107.5,
      "is_rising_star": true
    },
    {
      "id": "github-sst-opencode",
      "name": "opencode",
      "author": "sst",
      "description": "The AI coding agent built for the terminal.",
      "task": "tool",
      "tags": [
        "ai",
        "claude",
        "code",
        "llm",
        "openai"
      ],
      "likes": 128450,
      "downloads": 128450,
      "lastModified": "2025-11-20T03:54:03Z",
      "lastModifiedTimestamp": 1763610843000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/sst/opencode",
          "homepage": "https://opencode.ai",
          "language": "TypeScript",
          "forks": 2658,
          "open_issues": 1451,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/opencode-ai/opencode",
          "homepage": "",
          "language": "Go",
          "forks": 806,
          "open_issues": 163,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/66570915?v=4",
      "velocity": 47018.4,
      "is_rising_star": true
    },
    {
      "id": "github-crewAIInc-crewAI",
      "name": "crewAI",
      "author": "crewAIInc",
      "description": "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "aiagentframework",
        "llms"
      ],
      "likes": 121780,
      "downloads": 121780,
      "lastModified": "2025-11-20T02:15:41Z",
      "lastModifiedTimestamp": 1763604941000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/crewAIInc/crewAI",
          "homepage": "https://crewai.com",
          "language": "Python",
          "forks": 5416,
          "open_issues": 187,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/170677839?v=4",
      "velocity": 44638,
      "is_rising_star": true
    },
    {
      "id": "github-ray-project-ray",
      "name": "ray",
      "author": "ray-project",
      "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
      "task": "tool",
      "tags": [
        "data-science",
        "deep-learning",
        "deployment",
        "distributed",
        "hyperparameter-optimization",
        "hyperparameter-search",
        "large-language-models",
        "llm",
        "llm-inference",
        "llm-serving",
        "machine-learning",
        "optimization",
        "parallel",
        "python",
        "pytorch",
        "ray",
        "reinforcement-learning",
        "rllib",
        "serving",
        "tensorflow"
      ],
      "likes": 119736,
      "downloads": 119736,
      "lastModified": "2025-11-20T03:26:50Z",
      "lastModifiedTimestamp": 1763609210000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ray-project/ray",
          "homepage": "https://ray.io",
          "language": "Python",
          "forks": 6913,
          "open_issues": 3270,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/22125274?v=4",
      "velocity": 43892.2,
      "is_rising_star": true
    },
    {
      "id": "github-zhayujie-chatgpt-on-wechat",
      "name": "chatgpt-on-wechat",
      "author": "zhayujie",
      "description": "Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©ChatGPT/Claude/DeepSeek/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agent",
        "chatgpt",
        "claude-4",
        "deepseek",
        "dingtalk",
        "feishu-bot",
        "gemini",
        "gpt-4",
        "kimi",
        "linkai",
        "llm",
        "mcp",
        "multi-agent",
        "openai",
        "python3",
        "qwen",
        "rag",
        "wechat",
        "wechat-bot"
      ],
      "likes": 119263,
      "downloads": 119263,
      "lastModified": "2025-11-20T03:15:18Z",
      "lastModifiedTimestamp": 1763608518000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/zhayujie/chatgpt-on-wechat",
          "homepage": "https://link-ai.tech",
          "language": "Python",
          "forks": 9499,
          "open_issues": 354,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/26161723?v=4",
      "velocity": 43723.9,
      "is_rising_star": true
    },
    {
      "id": "github-milvus-io-milvus",
      "name": "milvus",
      "author": "milvus-io",
      "description": "Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search",
      "task": "tool",
      "tags": [
        "anns",
        "cloud-native",
        "diskann",
        "distributed",
        "embedding-database",
        "embedding-similarity",
        "embedding-store",
        "faiss",
        "golang",
        "hnsw",
        "image-search",
        "llm",
        "nearest-neighbor-search",
        "rag",
        "vector-database",
        "vector-search",
        "vector-similarity",
        "vector-store"
      ],
      "likes": 118669,
      "downloads": 118669,
      "lastModified": "2025-11-20T03:59:12Z",
      "lastModifiedTimestamp": 1763611152000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/milvus-io/milvus",
          "homepage": "https://milvus.io",
          "language": "Go",
          "forks": 3572,
          "open_issues": 889,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/51735404?v=4",
      "velocity": 43424.7,
      "is_rising_star": true
    },
    {
      "id": "github-janhq-jan",
      "name": "jan",
      "author": "janhq",
      "description": "Jan is an open source alternative to ChatGPT that runs 100% offline on your computer.",
      "task": "tool",
      "tags": [
        "chatgpt",
        "gpt",
        "llamacpp",
        "llm",
        "localai",
        "open-source",
        "self-hosted",
        "tauri"
      ],
      "likes": 118089,
      "downloads": 118089,
      "lastModified": "2025-11-20T03:31:03Z",
      "lastModifiedTimestamp": 1763609463000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/janhq/jan",
          "homepage": "https://jan.ai/",
          "language": "TypeScript",
          "forks": 2394,
          "open_issues": 190,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102363196?v=4",
      "velocity": 43288.3,
      "is_rising_star": true
    },
    {
      "id": "github-mudler-LocalAI",
      "name": "LocalAI",
      "author": "mudler",
      "description": ":robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI,  running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "audio-generation",
        "decentralized",
        "distributed",
        "gemma",
        "image-generation",
        "libp2p",
        "llama",
        "llm",
        "mamba",
        "mcp",
        "mistral",
        "musicgen",
        "object-detection",
        "rerank",
        "rwkv",
        "stable-diffusion",
        "text-generation",
        "tts"
      ],
      "likes": 116477,
      "downloads": 116477,
      "lastModified": "2025-11-20T03:43:49Z",
      "lastModifiedTimestamp": 1763610229000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mudler/LocalAI",
          "homepage": "https://localai.io",
          "language": "Go",
          "forks": 3077,
          "open_issues": 250,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/2420543?v=4",
      "velocity": 42665.7,
      "is_rising_star": true
    },
    {
      "id": "github-QuivrHQ-quivr",
      "name": "quivr",
      "author": "QuivrHQ",
      "description": "Opiniated RAG for integrating GenAI in your apps üß†   Focus on your product rather than the RAG. Easy integration in existing products with customisation!  Any LLM: GPT4, Groq, Llama. Any Vectorstore: PGVector, Faiss. Any Files. Anyway you want. ",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "chatbot",
        "chatgpt",
        "database",
        "docker",
        "framework",
        "frontend",
        "groq",
        "html",
        "javascript",
        "llm",
        "openai",
        "postgresql",
        "privacy",
        "rag",
        "react",
        "security",
        "typescript",
        "vector"
      ],
      "likes": 115876,
      "downloads": 115876,
      "lastModified": "2025-11-19T22:05:27Z",
      "lastModifiedTimestamp": 1763589927000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/QuivrHQ/quivr",
          "homepage": "https://core.quivr.com",
          "language": "Python",
          "forks": 3689,
          "open_issues": 16,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/159330290?v=4",
      "velocity": 42486.4,
      "is_rising_star": true
    },
    {
      "id": "github-2noise-ChatTTS",
      "name": "ChatTTS",
      "author": "2noise",
      "description": "A generative speech model for daily dialogue.",
      "task": "tool",
      "tags": [
        "agent",
        "chat",
        "chatgpt",
        "chattts",
        "chinese",
        "chinese-language",
        "english",
        "english-language",
        "gpt",
        "llm",
        "llm-agent",
        "natural-language-inference",
        "python",
        "text-to-speech",
        "torch",
        "torchaudio",
        "tts"
      ],
      "likes": 114529,
      "downloads": 114529,
      "lastModified": "2025-11-19T23:29:58Z",
      "lastModifiedTimestamp": 1763594998000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/2noise/ChatTTS",
          "homepage": "https://2noise.com",
          "language": "Python",
          "forks": 4146,
          "open_issues": 67,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/164844019?v=4",
      "velocity": 41992.5,
      "is_rising_star": true
    },
    {
      "id": "github-upstash-context7",
      "name": "context7",
      "author": "upstash",
      "description": "Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors",
      "task": "tool",
      "tags": [
        "llm",
        "mcp",
        "mcp-server",
        "vibe-coding"
      ],
      "likes": 112502,
      "downloads": 112502,
      "lastModified": "2025-11-20T03:47:16Z",
      "lastModifiedTimestamp": 1763610436000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/upstash/context7",
          "homepage": "https://context7.com",
          "language": "JavaScript",
          "forks": 1852,
          "open_issues": 100,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/74989412?v=4",
      "velocity": 41188.4,
      "is_rising_star": true
    },
    {
      "id": "github-chatboxai-chatbox",
      "name": "chatbox",
      "author": "chatboxai",
      "description": "User-friendly Desktop Client App for AI Models/LLMs (GPT, Claude, Gemini, Ollama...)",
      "task": "tool",
      "tags": [
        "assistant",
        "chatbot",
        "chatgpt",
        "claude",
        "copilot",
        "deepseek",
        "gemini",
        "gpt",
        "gpt-5",
        "ollama",
        "openai"
      ],
      "likes": 112397,
      "downloads": 112397,
      "lastModified": "2025-11-20T02:37:08Z",
      "lastModifiedTimestamp": 1763606228000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chatboxai/chatbox",
          "homepage": "https://chatboxai.app?utm_medium=github",
          "language": "TypeScript",
          "forks": 3786,
          "open_issues": 966,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/199570308?v=4",
      "velocity": 41207.1,
      "is_rising_star": true
    },
    {
      "id": "github-ToolJet-ToolJet",
      "name": "ToolJet",
      "author": "ToolJet",
      "description": "ToolJet is the open-source foundation of ToolJet AI - the AI-native platform for building internal tools, dashboard, business applications, workflows and AI agents üöÄ",
      "task": "tool",
      "tags": [
        "ai-app-builder",
        "docker",
        "hacktoberfest",
        "internal-applications",
        "internal-project",
        "internal-tool",
        "internal-tools",
        "javascript",
        "kubernetes",
        "low-code",
        "low-code-development-platform",
        "low-code-framework",
        "no-code",
        "nodejs",
        "reactjs",
        "self-hosted",
        "typescript",
        "web-development-tools",
        "workflow-automation"
      ],
      "likes": 110776,
      "downloads": 110776,
      "lastModified": "2025-11-19T23:34:57Z",
      "lastModifiedTimestamp": 1763595297000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ToolJet/ToolJet",
          "homepage": "https://tooljet.ai",
          "language": "JavaScript",
          "forks": 4877,
          "open_issues": 950,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/82193554?v=4",
      "velocity": 40612,
      "is_rising_star": true
    },
    {
      "id": "github-alibaba-arthas",
      "name": "arthas",
      "author": "alibaba",
      "description": "Alibaba Java Diagnostic Tool Arthas/Alibaba JavaËØäÊñ≠Âà©Âô®Arthas",
      "task": "tool",
      "tags": [
        "agent",
        "alibaba",
        "arthas",
        "classloader",
        "diagnosis",
        "java",
        "jvm",
        "trace",
        "trouble-shooting"
      ],
      "likes": 110550,
      "downloads": 110550,
      "lastModified": "2025-11-19T21:23:19Z",
      "lastModifiedTimestamp": 1763587399000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/alibaba/arthas",
          "homepage": "https://arthas.aliyun.com/",
          "language": "Java",
          "forks": 7602,
          "open_issues": 455,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1961952?v=4",
      "velocity": 40532.8,
      "is_rising_star": true
    },
    {
      "id": "github-chatchat-space-Langchain-Chatchat",
      "name": "Langchain-Chatchat",
      "author": "chatchat-space",
      "description": "Langchain-ChatchatÔºàÂéüLangchain-ChatGLMÔºâÂü∫‰∫é Langchain ‰∏é ChatGLM, Qwen ‰∏é Llama Á≠âËØ≠Ë®ÄÊ®°ÂûãÁöÑ RAG ‰∏é Agent Â∫îÁî® | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM, Qwen and Llama) RAG and Agent app with langchain ",
      "task": "tool",
      "tags": [
        "chatbot",
        "chatchat",
        "chatglm",
        "chatgpt",
        "embedding",
        "faiss",
        "fastchat",
        "gpt",
        "knowledge-base",
        "langchain",
        "langchain-chatglm",
        "llama",
        "llm",
        "milvus",
        "ollama",
        "qwen",
        "rag",
        "retrieval-augmented-generation",
        "streamlit",
        "xinference"
      ],
      "likes": 109780,
      "downloads": 109780,
      "lastModified": "2025-11-20T03:27:10Z",
      "lastModifiedTimestamp": 1763609230000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chatchat-space/Langchain-Chatchat",
          "homepage": "",
          "language": "Python",
          "forks": 6067,
          "open_issues": 28,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/139558948?v=4",
      "velocity": 40249,
      "is_rising_star": true
    },
    {
      "id": "github-CherryHQ-cherry-studio",
      "name": "cherry-studio",
      "author": "CherryHQ",
      "description": "üçí Cherry Studio is a desktop client that supports for multiple LLM providers.",
      "task": "tool",
      "tags": [
        "agent",
        "anthropic",
        "assistant",
        "chatbot",
        "chatbotai",
        "electron",
        "llm",
        "mcp-client",
        "openai"
      ],
      "likes": 106775,
      "downloads": 106775,
      "lastModified": "2025-11-20T03:54:49Z",
      "lastModifiedTimestamp": 1763610889000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CherryHQ/cherry-studio",
          "homepage": "https://cherry-ai.com",
          "language": "TypeScript",
          "forks": 3227,
          "open_issues": 532,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/187777663?v=4",
      "velocity": 39125.9,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-LLM101n",
      "name": "LLM101n",
      "author": "karpathy",
      "description": "LLM101n: Let's build a Storyteller",
      "task": "tool",
      "tags": [],
      "likes": 106769,
      "downloads": 106769,
      "lastModified": "2025-11-20T03:50:12Z",
      "lastModifiedTimestamp": 1763610612000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/LLM101n",
          "homepage": "",
          "language": null,
          "forks": 1937,
          "open_issues": 19,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 39143.5,
      "is_rising_star": true
    },
    {
      "id": "github-agno-agi-agno",
      "name": "agno",
      "author": "agno-agi",
      "description": "Multi-agent framework, runtime and control plane. Built for speed, privacy, and scale.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "developer-tools",
        "python"
      ],
      "likes": 106117,
      "downloads": 106117,
      "lastModified": "2025-11-20T03:42:58Z",
      "lastModifiedTimestamp": 1763610178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/agno-agi/agno",
          "homepage": "https://docs.agno.com",
          "language": "Python",
          "forks": 4641,
          "open_issues": 298,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/104874993?v=4",
      "velocity": 38883.9,
      "is_rising_star": true
    },
    {
      "id": "github-Alibaba-NLP-DeepResearch",
      "name": "DeepResearch",
      "author": "Alibaba-NLP",
      "description": "Tongyi Deep Research, the Leading Open-source Deep Research Agent",
      "task": "tool",
      "tags": [
        "agent",
        "alibaba",
        "artificial-intelligence",
        "deep-research",
        "deepresearch",
        "information-seeking",
        "llm",
        "tongyi",
        "web-agent",
        "ai",
        "gpt",
        "o3-mini",
        "research"
      ],
      "likes": 106031,
      "downloads": 106031,
      "lastModified": "2025-11-20T01:44:25Z",
      "lastModifiedTimestamp": 1763603065000,
      "readme": "<div align=\"center\">\n  <picture>\n      <img src=\"./assets/logo.png\" width=\"100%\">\n  </picture>\n</div>\n\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n\n[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&logo=huggingface&logoColor=ffffff&labelColor)](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)\n[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Alibaba-NLP/DeepResearch)\n[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)\n[![Paper](https://img.shields.io/badge/Paper-red?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2510.24701)\n\n</div>\n<p align=\"center\">\n<p align=\"center\">\nü§ó <a href=\"https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\" target=\"_blank\">HuggingFace</a> ÔΩú\n<img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> <a href=\"https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B\" target=\"_blank\">ModelScope</a> |  üí¨ <a href=\"./assets/wechat_new.jpg\">WeChat(ÂæÆ‰ø°)</a> üì∞ <a href=\"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/\">Blog</a> | üìë <a href=\"https://arxiv.org/pdf/2510.24701\">Paper</a>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/14895\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/14895\" alt=\"Alibaba-NLP%2FDeepResearch | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nüëè Welcome to try Tongyi DeepResearch via our **[<img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> Modelscope online demo](https://www.modelscope.cn/studios/jialongwu/Tongyi-DeepResearch)** or **[ü§ó Huggingface online demo](https://huggingface.co/spaces/Alibaba-NLP/Tongyi-DeepResearch)** or <img src=\"./WebAgent/assets/aliyun.png\" width=\"14px\" style=\"display:inline;\"> **[bailian service](https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&tab=app#/app/app-market/deep-search/)**!\n\n> [!NOTE]\n> This demo is for quick exploration only. Response times may vary or fail intermittently due to model latency and tool QPS limits. For a stable experience we recommend local deployment; for a production-ready service, visit <img src=\"./WebAgent/assets/aliyun.png\" width=\"14px\" style=\"display:inline;\"> [bailian](https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&tab=app#/app/app-market/deep-search/) and follow the guided setup.\n\n# Introduction\n\nWe present <img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> **Tongyi DeepResearch**, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.\n\n> Tongyi DeepResearch builds upon our previous work on the <img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> [WebAgent](./WebAgent/) project.\n\nMore details can be found in our üì∞&nbsp;<a href=\"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/\">Tech Blog</a>.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/performance.png\">\n</p>\n\n## Features\n\n- ‚öôÔ∏è **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.\n- üîÑ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.\n- üîÅ **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.\n- ü§ñ **Agent Inference Paradigm Compatibility**: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.\n\n# Model Download\n\nYou can directly download the model by following the links below.\n\n|            Model            |                                                                           Download Links                                                                           | Model Size | Context Length |\n| :-------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------: | :------------: |\n| Tongyi-DeepResearch-30B-A3B | [ü§ó HuggingFace](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)<br> [ü§ñ ModelScope](https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B) |  30B-A3B   |      128K      |\n\n# News\n\n[2025/09/20]üöÄ Tongyi-DeepResearch-30B-A3B is now on [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b)! Follow the [Quick-start](https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model) guide.\n\n[2025/09/17]üî• We have released **Tongyi-DeepResearch-30B-A3B**.\n\n# Deep Research Benchmark Results\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/benchmark.png\">\n</p>\n\n## Quick Start\n\nThis guide provides instructions for setting up the environment and running inference scripts located in the [inference](./inference/) folder.\n\n### 1. Environment Setup\n- Recommended Python version: **3.10.0** (using other versions may cause dependency issues).\n- It is strongly advised to create an isolated environment using `conda` or `virtualenv`.\n\n```bash\n# Example with Conda\nconda create -n react_infer_env python=3.10.0\nconda activate react_infer_env\n```\n\n### 2. Installation\n\nInstall the required dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### 3. Environment Configuration and Prepare Evaluation Data\n\n#### Environment Configuration\n\nConfigure your API keys and settings by copying the example environment file:\n\n```bash\n# Copy the example environment file\ncp .env.example .env\n```\n\nEdit the `.env` file and provide your actual API keys and configuration values:\n\n- **SERPER_KEY_ID**: Get your key from [Serper.dev](https://serper.dev/) for web search and Google Scholar\n- **JINA_API_KEYS**: Get your key from [Jina.ai](https://jina.ai/) for web page reading\n- **API_KEY/API_BASE**: OpenAI-compatible API for page summarization from [OpenAI](https://platform.openai.com/)\n- **DASHSCOPE_API_KEY**: Get your key from [Dashscope](https://dashscope.aliyun.com/) for file parsing\n- **SANDBOX_FUSION_ENDPOINT**: Python interpreter sandbox endpoints (see [SandboxFusion](https://github.com/bytedance/SandboxFusion))\n- **MODEL_PATH**: Path to your model weights\n- **DATASET**: Name of your evaluation dataset\n- **OUTPUT_PATH**: Directory for saving results\n\n> **Note**: The `.env` file is gitignored, so your secrets will not be committed to the repository.\n\n#### Prepare Evaluation Data\n\nThe system supports two input file formats: **JSON** and **JSONL**.\n\n#### Supported File Formats:\n\n**Option 1: JSONL Format (recommended)**\n- Create your data file with `.jsonl` extension (e.g., `my_questions.jsonl`)\n- Each line must be a valid JSON object with `question` and `answer` keys:\n  ```json\n  {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"}\n  {\"question\": \"Explain quantum computing\", \"answer\": \"\"}\n  ```\n\n**Option 2: JSON Format**\n- Create your data file with `.json` extension (e.g., `my_questions.json`)\n- File must contain a JSON array of objects, each with `question` and `answer` keys:\n  ```json\n  [\n    { \"question\": \"What is the capital of France?\", \"answer\": \"Paris\" },\n    { \"question\": \"Explain quantum computing\", \"answer\": \"\" }\n  ]\n  ```\n\n**Important Note:** The `answer` field contains the **ground truth/reference answer** used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.\n\n#### File References for Document Processing:\n\n- If using the _file parser_ tool, **prepend the filename to the `question` field**\n- Place referenced files in `eval_data/file_corpus/` directory\n- Example: `{\"question\": \"(Uploaded 1 file: ['report.pdf'])\\n\\nWhat are the key findings?\", \"answer\": \"...\"}`\n\n#### File Organization:\n```\nproject_root/\n‚îú‚îÄ‚îÄ eval_data/\n‚îÇ   ‚îú‚îÄ‚îÄ my_questions.jsonl          # Your evaluation data\n‚îÇ   ‚îî‚îÄ‚îÄ file_corpus/                # Referenced documents\n‚îÇ       ‚îú‚îÄ‚îÄ report.pdf\n‚îÇ       ‚îî‚îÄ‚îÄ data.xlsx\n```\n\n### 4. Configure the Inference Script\n\n- Open `run_react_infer.sh` and modify the following variables as instructed in the comments:\n  - `MODEL_PATH` - path to the local or remote model weights.\n  - `DATASET` - full path to your evaluation file, e.g. `eval_data/my_questions.jsonl` or `/path/to/my_questions.json`.\n  - `OUTPUT_PATH` - path for saving the prediction results, e.g. `./outputs`.\n- Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required `API_KEY`, `BASE_URL`, or other credentials. Each key is explained inline in the bash script.\n\n### 5. Run the Inference Script\n\n```bash\nbash run_react_infer.sh\n```\n---\n\nWith these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.\n\n### 6. You can use OpenRouter's API to call our model\n\nTongyi-DeepResearch-30B-A3B is now available at [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b). You can run the inference without any GPUs.\n\nYou need to modify the following in the file [inference/react_agent.py](https://github.com/Alibaba-NLP/DeepResearch/blob/main/inference/react_agent.py):\n\n- In the call_server function: Set the API key and URL to your OpenRouter account‚Äôs API and URL.\n- Change the model name to alibaba/tongyi-deepresearch-30b-a3b.\n- Adjust the content concatenation way as described in the comments on lines **88‚Äì90.**\n\n## Benchmark Evaluation\n\nWe provide benchmark evaluation scripts for various datasets. Please refer to the [evaluation scripts](./evaluation/) directory for more details.\n\n## FAQ\n\nPlease refer to the [FAQ](./FAQ.md) for more details.\n\n## Deep Research Agent Family\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/family17.png\">\n</p>\n\nTongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:\n\n[1] [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/pdf/2501.07572) (ACL 2025)<br>\n[2] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648) (NeurIPS 2025)<br>\n[3] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/pdf/2507.02592)<br>\n[4] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/pdf/2507.15061)<br>\n[5] [WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent](https://arxiv.org/pdf/2508.05748)<br>\n[6] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/pdf/2509.13309)<br>\n[7] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/pdf/2509.13313)<br>\n[8] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/pdf/2509.13312)<br>\n[9] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/pdf/2509.13305)<br>\n[10] [Scaling Agents via Continual Pre-training](https://arxiv.org/pdf/2509.13310)<br>\n[11] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/pdf/2509.13311)<br>\n[12] [WebLeaper: Empowering Efficient, Info-Rich Seeking for Web Agents](https://arxiv.org/pdf/2510.24697)\n\n## üåü Misc\n\n<div align=\"center\">\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&type=Date)](https://www.star-history.com/#Alibaba-NLP/DeepResearch&Date)\n\n</div>\n\n## üö© Talent Recruitment\n\nüî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)\n\nüìö **Research Area**ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG\n\n‚òéÔ∏è **Contact**Ôºö[yongjiang.jy@alibaba-inc.com]()\n\n## Contact Information\n\nFor communications, please contact Yong Jiang (yongjiang.jy@alibaba-inc.com).\n\n## Citation\n\n```bibtex\n@article{tongyidr,\n  title={Tongyi DeepResearch Technical Report},\n  author={Team, Tongyi DeepResearch and Li, Baixuan and Zhang, Bo and Zhang, Dingchu and Huang, Fei and Li, Guangyu and Chen, Guoxin and Yin, Huifeng and Wu, Jialong and Zhou, Jingren and others},\n  journal={arXiv preprint arXiv:2510.24701},\n  year={2025}\n}\n```\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Alibaba-NLP/DeepResearch",
          "homepage": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
          "language": "Python",
          "forks": 1314,
          "open_issues": 66,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/dzhng/deep-research",
          "homepage": "",
          "language": "TypeScript",
          "forks": 1866,
          "open_issues": 77,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/64211549?v=4",
      "velocity": 38866.3,
      "is_rising_star": true
    },
    {
      "id": "github-reworkd-AgentGPT",
      "name": "AgentGPT",
      "author": "reworkd",
      "description": "ü§ñ Assemble, configure, and deploy autonomous AI Agents in your browser.",
      "task": "tool",
      "tags": [
        "agent",
        "agentgpt",
        "agi",
        "autogpt",
        "baby-agi",
        "gpt",
        "langchain",
        "next",
        "openai",
        "t3",
        "t3-stack"
      ],
      "likes": 105765,
      "downloads": 105765,
      "lastModified": "2025-11-19T18:55:43Z",
      "lastModifiedTimestamp": 1763578543000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/reworkd/AgentGPT",
          "homepage": "https://agentgpt.reworkd.ai",
          "language": "TypeScript",
          "forks": 9487,
          "open_issues": 214,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/120154269?v=4",
      "velocity": 38778.3,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-qlib",
      "name": "qlib",
      "author": "microsoft",
      "description": "Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.",
      "task": "tool",
      "tags": [
        "algorithmic-trading",
        "auto-quant",
        "deep-learning",
        "finance",
        "fintech",
        "investment",
        "machine-learning",
        "paper",
        "platform",
        "python",
        "quant",
        "quant-dataset",
        "quant-models",
        "quantitative-finance",
        "quantitative-trading",
        "research",
        "research-paper",
        "stock-data"
      ],
      "likes": 101601,
      "downloads": 101601,
      "lastModified": "2025-11-20T03:54:28Z",
      "lastModifiedTimestamp": 1763610868000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/qlib",
          "homepage": "https://qlib.readthedocs.io/en/latest/",
          "language": "Python",
          "forks": 5237,
          "open_issues": 304,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 37231.7,
      "is_rising_star": true
    },
    {
      "id": "github-1Panel-dev-1Panel",
      "name": "1Panel",
      "author": "1Panel-dev",
      "description": "üî• 1Panel provides an intuitive web interface and MCP Server to manage websites, files, containers, databases, and LLMs on a Linux server.",
      "task": "tool",
      "tags": [
        "1panel",
        "cockpit",
        "docker",
        "docker-ui",
        "lamp",
        "linux",
        "lnmp",
        "ollama",
        "webmin"
      ],
      "likes": 96275,
      "downloads": 96275,
      "lastModified": "2025-11-20T03:39:30Z",
      "lastModifiedTimestamp": 1763609970000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/1Panel-dev/1Panel",
          "homepage": "https://1panel.pro",
          "language": "Go",
          "forks": 2839,
          "open_issues": 509,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/109613420?v=4",
      "velocity": 35295.7,
      "is_rising_star": true
    },
    {
      "id": "github-danny-avila-LibreChat",
      "name": "LibreChat",
      "author": "danny-avila",
      "description": "Enhanced ChatGPT Clone: Features Agents, MCP, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-5, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active.",
      "task": "tool",
      "tags": [
        "ai",
        "anthropic",
        "artifacts",
        "aws",
        "azure",
        "chatgpt",
        "chatgpt-clone",
        "claude",
        "clone",
        "deepseek",
        "gemini",
        "google",
        "gpt-5",
        "librechat",
        "mcp",
        "o1",
        "openai",
        "responses-api",
        "vision",
        "webui"
      ],
      "likes": 95365,
      "downloads": 95365,
      "lastModified": "2025-11-20T03:05:26Z",
      "lastModifiedTimestamp": 1763607926000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/danny-avila/LibreChat",
          "homepage": "https://librechat.ai/",
          "language": "TypeScript",
          "forks": 6225,
          "open_issues": 353,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110412045?v=4",
      "velocity": 34945.9,
      "is_rising_star": true
    },
    {
      "id": "github-khoj-ai-khoj",
      "name": "khoj",
      "author": "khoj-ai",
      "description": "Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "assistant",
        "chat",
        "chatgpt",
        "emacs",
        "image-generation",
        "llama3",
        "llamacpp",
        "llm",
        "obsidian",
        "obsidian-md",
        "offline-llm",
        "productivity",
        "rag",
        "research",
        "self-hosted",
        "semantic-search",
        "stt",
        "whatsapp-ai"
      ],
      "likes": 94832,
      "downloads": 94832,
      "lastModified": "2025-11-20T00:45:17Z",
      "lastModifiedTimestamp": 1763599517000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/khoj-ai/khoj",
          "homepage": "https://khoj.dev",
          "language": "Python",
          "forks": 1860,
          "open_issues": 85,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134046886?v=4",
      "velocity": 34764.4,
      "is_rising_star": true
    },
    {
      "id": "github-BerriAI-litellm",
      "name": "litellm",
      "author": "BerriAI",
      "description": "Python SDK, Proxy Server (AI Gateway) to call 100+ LLM APIs in OpenAI (or native) format, with cost tracking, guardrails, loadbalancing and logging. [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, VLLM, NVIDIA NIM]",
      "task": "tool",
      "tags": [
        "ai-gateway",
        "anthropic",
        "azure-openai",
        "bedrock",
        "gateway",
        "langchain",
        "litellm",
        "llm",
        "llm-gateway",
        "llmops",
        "mcp-gateway",
        "openai",
        "openai-proxy",
        "vertex-ai"
      ],
      "likes": 93970,
      "downloads": 93970,
      "lastModified": "2025-11-20T03:54:47Z",
      "lastModifiedTimestamp": 1763610887000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/BerriAI/litellm",
          "homepage": "https://docs.litellm.ai/docs/",
          "language": "Python",
          "forks": 4749,
          "open_issues": 1359,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/121462774?v=4",
      "velocity": 34423.4,
      "is_rising_star": true
    },
    {
      "id": "github-continuedev-continue",
      "name": "continue",
      "author": "continuedev",
      "description": "‚è© Ship faster with Continuous AI. Open-source CLI that can be used in TUI mode as a coding agent or Headless mode to run background agents",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "background-agents",
        "claude",
        "cli",
        "continuous-ai",
        "developer-tools",
        "gemini",
        "gpt",
        "hacktoberfest",
        "jetbrains",
        "llm",
        "open-source",
        "qwen",
        "vscode",
        "workflows"
      ],
      "likes": 89741,
      "downloads": 89741,
      "lastModified": "2025-11-20T04:00:09Z",
      "lastModifiedTimestamp": 1763611209000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/continuedev/continue",
          "homepage": "https://docs.continue.dev/",
          "language": "TypeScript",
          "forks": 3801,
          "open_issues": 656,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127876214?v=4",
      "velocity": 32896.6,
      "is_rising_star": true
    },
    {
      "id": "github-JushBJJ-Mr.-Ranedeer-AI-Tutor",
      "name": "Mr.-Ranedeer-AI-Tutor",
      "author": "JushBJJ",
      "description": "A GPT-4 AI Tutor Prompt for customizable personalized learning experiences.",
      "task": "tool",
      "tags": [
        "ai",
        "education",
        "gpt-4",
        "llm"
      ],
      "likes": 89002,
      "downloads": 89002,
      "lastModified": "2025-11-20T02:26:40Z",
      "lastModifiedTimestamp": 1763605600000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor",
          "homepage": "https://Mr-Ranedeer.com",
          "language": null,
          "forks": 3373,
          "open_issues": 14,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/36951064?v=4",
      "velocity": 32632.6,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-graphrag",
      "name": "graphrag",
      "author": "microsoft",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "task": "tool",
      "tags": [
        "gpt",
        "gpt-4",
        "gpt4",
        "graphrag",
        "llm",
        "llms",
        "rag"
      ],
      "likes": 87739,
      "downloads": 87739,
      "lastModified": "2025-11-20T03:49:55Z",
      "lastModifiedTimestamp": 1763610595000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/graphrag",
          "homepage": "https://microsoft.github.io/graphrag/",
          "language": "Python",
          "forks": 3075,
          "open_issues": 94,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 32151.9,
      "is_rising_star": true
    },
    {
      "id": "github-feder-cr-Jobs_Applier_AI_Agent_AIHawk",
      "name": "Jobs_Applier_AI_Agent_AIHawk",
      "author": "feder-cr",
      "description": "AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.",
      "task": "tool",
      "tags": [
        "agent",
        "application-resume",
        "artificial-intelligence",
        "automate",
        "automation",
        "bot",
        "chatgpt",
        "chrome",
        "gpt",
        "human-resources",
        "job",
        "jobs",
        "jobsearch",
        "jobseeker",
        "opeai",
        "python",
        "resume",
        "scraper",
        "scraping",
        "selenium"
      ],
      "likes": 87229,
      "downloads": 87229,
      "lastModified": "2025-11-19T18:41:18Z",
      "lastModifiedTimestamp": 1763577678000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk",
          "homepage": "",
          "language": "Python",
          "forks": 4422,
          "open_issues": 12,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85809106?v=4",
      "velocity": 31978.1,
      "is_rising_star": true
    },
    {
      "id": "github-666ghj-BettaFish",
      "name": "BettaFish",
      "author": "666ghj",
      "description": "ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ",
      "task": "tool",
      "tags": [
        "agent-framework",
        "data-analysis",
        "deep-research",
        "deep-search",
        "llms",
        "multi-agent-system",
        "nlp",
        "public-opinion-analysis",
        "python3",
        "sentiment-analysis"
      ],
      "likes": 85074,
      "downloads": 85074,
      "lastModified": "2025-11-20T03:55:09Z",
      "lastModifiedTimestamp": 1763610909000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/666ghj/BettaFish",
          "homepage": "",
          "language": "Python",
          "forks": 5428,
          "open_issues": 58,
          "license": "GNU General Public License v2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110395318?v=4",
      "velocity": 31050.8,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-llm.c",
      "name": "llm.c",
      "author": "karpathy",
      "description": "LLM training in simple, raw C/CUDA",
      "task": "tool",
      "tags": [],
      "likes": 84583,
      "downloads": 84583,
      "lastModified": "2025-11-19T23:29:57Z",
      "lastModifiedTimestamp": 1763594997000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/llm.c",
          "homepage": "",
          "language": "Cuda",
          "forks": 3293,
          "open_issues": 215,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 31010.1,
      "is_rising_star": true
    },
    {
      "id": "github-songquanpeng-one-api",
      "name": "one-api",
      "author": "songquanpeng",
      "description": "LLM API ÁÆ°ÁêÜ & ÂàÜÂèëÁ≥ªÁªüÔºåÊîØÊåÅ OpenAI„ÄÅAzure„ÄÅAnthropic Claude„ÄÅGoogle Gemini„ÄÅDeepSeek„ÄÅÂ≠óËäÇË±ÜÂåÖ„ÄÅChatGLM„ÄÅÊñáÂøÉ‰∏ÄË®Ä„ÄÅËÆØÈ£ûÊòüÁÅ´„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅ360 Êô∫ËÑë„ÄÅËÖæËÆØÊ∑∑ÂÖÉÁ≠â‰∏ªÊµÅÊ®°ÂûãÔºåÁªü‰∏Ä API ÈÄÇÈÖçÔºåÂèØÁî®‰∫é key ÁÆ°ÁêÜ‰∏é‰∫åÊ¨°ÂàÜÂèë„ÄÇÂçïÂèØÊâßË°åÊñá‰ª∂ÔºåÊèê‰æõ Docker ÈïúÂÉèÔºå‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÂºÄÁÆ±Âç≥Áî®„ÄÇLLM API management & key redistribution system, unifying multiple providers under a single API. Single binary, Docker-ready, with an English UI.",
      "task": "tool",
      "tags": [
        "api",
        "api-gateway",
        "azure-openai-api",
        "chatgpt",
        "claude",
        "ernie-bot",
        "gemini",
        "gpt",
        "openai",
        "openai-api",
        "proxy"
      ],
      "likes": 84136,
      "downloads": 84136,
      "lastModified": "2025-11-20T03:11:36Z",
      "lastModifiedTimestamp": 1763608296000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/songquanpeng/one-api",
          "homepage": "https://openai.justsong.cn/",
          "language": "JavaScript",
          "forks": 5519,
          "open_issues": 968,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/39998050?v=4",
      "velocity": 30837.4,
      "is_rising_star": true
    },
    {
      "id": "github-OpenBMB-ChatDev",
      "name": "ChatDev",
      "author": "OpenBMB",
      "description": "Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)",
      "task": "tool",
      "tags": [],
      "likes": 83235,
      "downloads": 83235,
      "lastModified": "2025-11-19T19:17:28Z",
      "lastModifiedTimestamp": 1763579848000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenBMB/ChatDev",
          "homepage": "https://arxiv.org/abs/2307.07924",
          "language": "Python",
          "forks": 3487,
          "open_issues": 50,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/89920203?v=4",
      "velocity": 30517.3,
      "is_rising_star": true
    },
    {
      "id": "github-stanford-oval-storm",
      "name": "storm",
      "author": "stanford-oval",
      "description": "An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.",
      "task": "tool",
      "tags": [
        "agentic-rag",
        "deep-research",
        "emnlp2024",
        "knowledge-curation",
        "large-language-models",
        "naacl",
        "nlp",
        "report-generation",
        "retrieval-augmented-generation"
      ],
      "likes": 82855,
      "downloads": 82855,
      "lastModified": "2025-11-20T03:14:26Z",
      "lastModifiedTimestamp": 1763608466000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/stanford-oval/storm",
          "homepage": "http://storm.genie.stanford.edu",
          "language": "Python",
          "forks": 2504,
          "open_issues": 87,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13667124?v=4",
      "velocity": 30376.5,
      "is_rising_star": true
    },
    {
      "id": "github-voideditor-void",
      "name": "void",
      "author": "voideditor",
      "description": "An AI tool from GitHub.",
      "task": "tool",
      "tags": [
        "chatgpt",
        "claude",
        "copilot",
        "cursor",
        "developer-tools",
        "editor",
        "llm",
        "open-source",
        "openai",
        "visual-studio-code",
        "vscode",
        "vscode-extension"
      ],
      "likes": 82654,
      "downloads": 82654,
      "lastModified": "2025-11-20T03:17:19Z",
      "lastModifiedTimestamp": 1763608639000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/voideditor/void",
          "homepage": "https://voideditor.com",
          "language": "TypeScript",
          "forks": 2147,
          "open_issues": 304,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/181171420?v=4",
      "velocity": 30298.4,
      "is_rising_star": true
    },
    {
      "id": "github-nrwl-nx",
      "name": "nx",
      "author": "nrwl",
      "description": "Get to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents.",
      "task": "tool",
      "tags": [
        "angular",
        "build",
        "build-system",
        "build-tool",
        "building-tool",
        "cli",
        "cypress",
        "hacktoberfest",
        "javascript",
        "monorepo",
        "nextjs",
        "nodejs",
        "nx",
        "nx-workspaces",
        "react",
        "storybook",
        "typescript"
      ],
      "likes": 82544,
      "downloads": 82544,
      "lastModified": "2025-11-19T23:18:56Z",
      "lastModifiedTimestamp": 1763594336000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/nrwl/nx",
          "homepage": "https://nx.dev",
          "language": "TypeScript",
          "forks": 2622,
          "open_issues": 786,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23692104?v=4",
      "velocity": 30258.8,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-semantic-kernel",
      "name": "semantic-kernel",
      "author": "microsoft",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "llm",
        "openai",
        "sdk"
      ],
      "likes": 80096,
      "downloads": 80096,
      "lastModified": "2025-11-20T00:29:43Z",
      "lastModifiedTimestamp": 1763598583000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/semantic-kernel",
          "homepage": "https://aka.ms/semantic-kernel",
          "language": "C#",
          "forks": 4349,
          "open_issues": 572,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 29363.4,
      "is_rising_star": true
    },
    {
      "id": "github-labring-FastGPT",
      "name": "FastGPT",
      "author": "labring",
      "description": "FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.",
      "task": "tool",
      "tags": [
        "agent",
        "claude",
        "deepseek",
        "llm",
        "mcp",
        "nextjs",
        "openai",
        "qwen",
        "rag",
        "workflow"
      ],
      "likes": 78949,
      "downloads": 78949,
      "lastModified": "2025-11-20T03:15:06Z",
      "lastModifiedTimestamp": 1763608506000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/labring/FastGPT",
          "homepage": "https://fastgpt.io",
          "language": "TypeScript",
          "forks": 6772,
          "open_issues": 650,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102226726?v=4",
      "velocity": 28942.1,
      "is_rising_star": true
    },
    {
      "id": "github-ComposioHQ-composio",
      "name": "composio",
      "author": "ComposioHQ",
      "description": "Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agents",
        "ai",
        "ai-agents",
        "aiagents",
        "developer-tools",
        "function-calling",
        "gpt-4",
        "javascript",
        "js",
        "llm",
        "llmops",
        "mcp",
        "python",
        "remote-mcp-server",
        "sse",
        "typescript"
      ],
      "likes": 78494,
      "downloads": 78494,
      "lastModified": "2025-11-19T22:09:44Z",
      "lastModifiedTimestamp": 1763590184000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ComposioHQ/composio",
          "homepage": "https://docs.composio.dev",
          "language": "TypeScript",
          "forks": 4397,
          "open_issues": 25,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128464815?v=4",
      "velocity": 28778.2,
      "is_rising_star": true
    },
    {
      "id": "github-datawhalechina-self-llm",
      "name": "self-llm",
      "author": "datawhalechina",
      "description": "„ÄäÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó„ÄãÈíàÂØπ‰∏≠ÂõΩÂÆùÂÆùÈáèË∫´ÊâìÈÄ†ÁöÑÂü∫‰∫éLinuxÁéØÂ¢ÉÂø´ÈÄüÂæÆË∞ÉÔºàÂÖ®ÂèÇÊï∞/LoraÔºâ„ÄÅÈÉ®ÁΩ≤ÂõΩÂÜÖÂ§ñÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºàLLMÔºâ/Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâÊïôÁ®ã",
      "task": "tool",
      "tags": [
        "chatglm",
        "chatglm3",
        "gemma-2b-it",
        "glm-4",
        "internlm2",
        "llama3",
        "llm",
        "lora",
        "minicpm",
        "q-wen",
        "qwen",
        "qwen1-5",
        "qwen2"
      ],
      "likes": 78167,
      "downloads": 78167,
      "lastModified": "2025-11-20T03:28:23Z",
      "lastModifiedTimestamp": 1763609303000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/datawhalechina/self-llm",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 2621,
          "open_issues": 147,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/46047812?v=4",
      "velocity": 28647.3,
      "is_rising_star": true
    },
    {
      "id": "github-Hannibal046-Awesome-LLM",
      "name": "Awesome-LLM",
      "author": "Hannibal046",
      "description": "Awesome-LLM: a curated list of Large Language Model",
      "task": "tool",
      "tags": [],
      "likes": 76756,
      "downloads": 76756,
      "lastModified": "2025-11-20T03:51:48Z",
      "lastModifiedTimestamp": 1763610708000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Hannibal046/Awesome-LLM",
          "homepage": "",
          "language": null,
          "forks": 2186,
          "open_issues": 51,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/38466901?v=4",
      "velocity": 28138,
      "is_rising_star": true
    },
    {
      "id": "github-warpdotdev-Warp",
      "name": "Warp",
      "author": "warpdotdev",
      "description": "Warp is the agentic development environment, built for coding with multiple AI agents.",
      "task": "tool",
      "tags": [
        "bash",
        "linux",
        "macos",
        "rust",
        "shell",
        "terminal",
        "wasm",
        "zsh"
      ],
      "likes": 75913,
      "downloads": 75913,
      "lastModified": "2025-11-20T02:32:56Z",
      "lastModifiedTimestamp": 1763605976000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/warpdotdev/Warp",
          "homepage": "https://warp.dev",
          "language": null,
          "forks": 580,
          "open_issues": 3932,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/71840468?v=4",
      "velocity": 27831.1,
      "is_rising_star": true
    },
    {
      "id": "github-TauricResearch-TradingAgents",
      "name": "TradingAgents",
      "author": "TauricResearch",
      "description": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "task": "tool",
      "tags": [
        "agent",
        "finance",
        "llm",
        "multiagent",
        "trading"
      ],
      "likes": 75667,
      "downloads": 75667,
      "lastModified": "2025-11-20T03:47:34Z",
      "lastModifiedTimestamp": 1763610454000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/TauricResearch/TradingAgents",
          "homepage": "https://arxiv.org/pdf/2412.20138",
          "language": "Python",
          "forks": 4704,
          "open_issues": 196,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192884433?v=4",
      "velocity": 27716.7,
      "is_rising_star": true
    },
    {
      "id": "github-CopilotKit-CopilotKit",
      "name": "CopilotKit",
      "author": "CopilotKit",
      "description": "React UI + elegant infrastructure for AI Copilots, AI chatbots, and in-app AI agents. The Agentic last-mile ü™Å",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai",
        "ai-agent",
        "ai-assistant",
        "assistant",
        "copilot",
        "copilot-chat",
        "hacktoberfest",
        "langchain",
        "langgraph",
        "llm",
        "nextjs",
        "open-source",
        "react",
        "reactjs",
        "ts",
        "typescript"
      ],
      "likes": 75068,
      "downloads": 75068,
      "lastModified": "2025-11-20T00:17:21Z",
      "lastModifiedTimestamp": 1763597841000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CopilotKit/CopilotKit",
          "homepage": "https://docs.copilotkit.ai",
          "language": "TypeScript",
          "forks": 3339,
          "open_issues": 429,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131273140?v=4",
      "velocity": 27513.2,
      "is_rising_star": true
    },
    {
      "id": "github-chroma-core-chroma",
      "name": "chroma",
      "author": "chroma-core",
      "description": "Open-source search and retrieval database for AI applications.",
      "task": "tool",
      "tags": [
        "ai",
        "database",
        "document-retrieval",
        "embeddings",
        "llm",
        "llms",
        "rag",
        "rust",
        "rust-lang",
        "vector-database"
      ],
      "likes": 73468,
      "downloads": 73468,
      "lastModified": "2025-11-20T03:37:47Z",
      "lastModifiedTimestamp": 1763609867000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chroma-core/chroma",
          "homepage": "https://www.trychroma.com/",
          "language": "Rust",
          "forks": 1924,
          "open_issues": 495,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/105881770?v=4",
      "velocity": 26925.8,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-JARVIS",
      "name": "JARVIS",
      "author": "microsoft",
      "description": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf",
      "task": "tool",
      "tags": [
        "deep-learning",
        "platform",
        "pytorch"
      ],
      "likes": 73350,
      "downloads": 73350,
      "lastModified": "2025-11-19T21:32:35Z",
      "lastModifiedTimestamp": 1763587955000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/JARVIS",
          "homepage": "",
          "language": "Python",
          "forks": 2053,
          "open_issues": 332,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26895,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-BitNet",
      "name": "BitNet",
      "author": "microsoft",
      "description": "Official inference framework for 1-bit LLMs",
      "task": "tool",
      "tags": [],
      "likes": 73235,
      "downloads": 73235,
      "lastModified": "2025-11-19T20:17:12Z",
      "lastModifiedTimestamp": 1763583432000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/BitNet",
          "homepage": "",
          "language": "Python",
          "forks": 1895,
          "open_issues": 164,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26852.1,
      "is_rising_star": true
    },
    {
      "id": "github-assafelovic-gpt-researcher",
      "name": "gpt-researcher",
      "author": "assafelovic",
      "description": "An LLM agent that conducts deep research (local and web) on any given topic and generates a long report with citations.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "automation",
        "deepresearch",
        "llms",
        "mcp",
        "mcp-server",
        "python",
        "research",
        "search",
        "webscraping"
      ],
      "likes": 72613,
      "downloads": 72613,
      "lastModified": "2025-11-20T03:46:16Z",
      "lastModifiedTimestamp": 1763610376000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/assafelovic/gpt-researcher",
          "homepage": "https://gptr.dev",
          "language": "Python",
          "forks": 3196,
          "open_issues": 149,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13554167?v=4",
      "velocity": 26610.1,
      "is_rising_star": true
    },
    {
      "id": "github-e2b-dev-awesome-ai-agents",
      "name": "awesome-ai-agents",
      "author": "e2b-dev",
      "description": "A list of AI autonomous agents",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "artificial-intelligence",
        "autogpt",
        "autonomous-agents",
        "awesome",
        "babyagi",
        "copilot",
        "gpt",
        "gpt-4",
        "gpt-engineer",
        "openai",
        "python"
      ],
      "likes": 72609,
      "downloads": 72609,
      "lastModified": "2025-11-20T03:42:10Z",
      "lastModifiedTimestamp": 1763610130000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/e2b-dev/awesome-ai-agents",
          "homepage": "https://e2b.dev/docs",
          "language": null,
          "forks": 2023,
          "open_issues": 78,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/129434473?v=4",
      "velocity": 26607.9,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-smolagents",
      "name": "smolagents",
      "author": "huggingface",
      "description": "ü§ó smolagents: a barebones library for agents that think in code.",
      "task": "tool",
      "tags": [],
      "likes": 72115,
      "downloads": 72115,
      "lastModified": "2025-11-20T03:41:44Z",
      "lastModifiedTimestamp": 1763610104000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/smolagents",
          "homepage": "https://huggingface.co/docs/smolagents",
          "language": "Python",
          "forks": 2138,
          "open_issues": 319,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 26429.7,
      "is_rising_star": true
    },
    {
      "id": "github-gitleaks-gitleaks",
      "name": "gitleaks",
      "author": "gitleaks",
      "description": "Find secrets with Gitleaks üîë",
      "task": "tool",
      "tags": [
        "ai-powered",
        "ci-cd",
        "cicd",
        "cli",
        "data-loss-prevention",
        "devsecops",
        "dlp",
        "git",
        "gitleaks",
        "go",
        "golang",
        "hacktoberfest",
        "llm",
        "llm-inference",
        "llm-training",
        "nhi",
        "open-source",
        "secret",
        "security",
        "security-tools"
      ],
      "likes": 71915,
      "downloads": 71915,
      "lastModified": "2025-11-20T00:55:23Z",
      "lastModifiedTimestamp": 1763600123000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/gitleaks/gitleaks",
          "homepage": "https://gitleaks.io",
          "language": "Go",
          "forks": 1833,
          "open_issues": 318,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/90395851?v=4",
      "velocity": 26361.5,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-OmniParser",
      "name": "OmniParser",
      "author": "microsoft",
      "description": "A simple screen parsing tool towards pure vision based GUI agent",
      "task": "tool",
      "tags": [],
      "likes": 71594,
      "downloads": 71594,
      "lastModified": "2025-11-20T03:02:21Z",
      "lastModifiedTimestamp": 1763607741000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/OmniParser",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 2045,
          "open_issues": 225,
          "license": "Creative Commons Attribution 4.0 International"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26228.4,
      "is_rising_star": true
    },
    {
      "id": "github-HKUDS-LightRAG",
      "name": "LightRAG",
      "author": "HKUDS",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "task": "tool",
      "tags": [
        "genai",
        "gpt",
        "gpt-4",
        "graphrag",
        "knowledge-graph",
        "large-language-models",
        "llm",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 71270,
      "downloads": 71270,
      "lastModified": "2025-11-20T03:56:38Z",
      "lastModifiedTimestamp": 1763610998000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/HKUDS/LightRAG",
          "homepage": "https://arxiv.org/abs/2410.05779",
          "language": "Python",
          "forks": 3484,
          "open_issues": 170,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/118165258?v=4",
      "velocity": 26045.8,
      "is_rising_star": true
    },
    {
      "id": "github-asgeirtj-system_prompts_leaks",
      "name": "system_prompts_leaks",
      "author": "asgeirtj",
      "description": "Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude & Gemini",
      "task": "tool",
      "tags": [
        "ai",
        "anthropic",
        "chatbots",
        "chatgpt",
        "claude",
        "gemini",
        "generative-ai",
        "google-deepmind",
        "large-language-models",
        "llm",
        "openai",
        "prompt-engineering",
        "prompt-injection",
        "prompts"
      ],
      "likes": 71254,
      "downloads": 71254,
      "lastModified": "2025-11-20T03:00:34Z",
      "lastModifiedTimestamp": 1763607634000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/asgeirtj/system_prompts_leaks",
          "homepage": "",
          "language": "JavaScript",
          "forks": 3630,
          "open_issues": 20,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/27446620?v=4",
      "velocity": 26107.4,
      "is_rising_star": true
    },
    {
      "id": "github-Fosowl-agenticSeek",
      "name": "agenticSeek",
      "author": "Fosowl",
      "description": "Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. üîî Official updates only via twitter @Martin993886460 (Beware of fake account)",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agents",
        "ai",
        "autonomous-agents",
        "deepseek-r1",
        "llm",
        "llm-agents",
        "voice-assistant"
      ],
      "likes": 71111,
      "downloads": 71111,
      "lastModified": "2025-11-20T03:40:13Z",
      "lastModifiedTimestamp": 1763610013000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Fosowl/agenticSeek",
          "homepage": "http://agenticseek.tech",
          "language": "Python",
          "forks": 2566,
          "open_issues": 36,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/49105846?v=4",
      "velocity": 26071.1,
      "is_rising_star": true
    }
  ],
  "trending": [
    {
      "id": "github-ollama-ollama",
      "name": "ollama",
      "author": "ollama",
      "description": "Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.",
      "task": "tool",
      "tags": [
        "deepseek",
        "gemma",
        "gemma3",
        "gemma3n",
        "go",
        "golang",
        "gpt-oss",
        "llama",
        "llama2",
        "llama3",
        "llava",
        "llm",
        "llms",
        "mistral",
        "ollama",
        "phi4",
        "qwen"
      ],
      "likes": 468708,
      "downloads": 468708,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ollama/ollama",
          "homepage": "https://ollama.com",
          "language": "Go",
          "forks": 13681,
          "open_issues": 2256,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/151674099?v=4",
      "velocity": 171815.6,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-transformers",
      "name": "transformers",
      "author": "huggingface",
      "description": "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
      "task": "tool",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "likes": 458185,
      "downloads": 458185,
      "lastModified": "2025-11-20T03:54:06Z",
      "lastModifiedTimestamp": 1763610846000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/transformers",
          "homepage": "https://huggingface.co/transformers",
          "language": "Python",
          "forks": 31170,
          "open_issues": 2129,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 167968.9,
      "is_rising_star": true
    },
    {
      "id": "github-langflow-ai-langflow",
      "name": "langflow",
      "author": "langflow-ai",
      "description": "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.",
      "task": "tool",
      "tags": [
        "agents",
        "chatgpt",
        "generative-ai",
        "large-language-models",
        "multiagent",
        "react-flow"
      ],
      "likes": 416201,
      "downloads": 416201,
      "lastModified": "2025-11-20T03:45:31Z",
      "lastModifiedTimestamp": 1763610331000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langflow-ai/langflow",
          "homepage": "http://www.langflow.org",
          "language": "Python",
          "forks": 8014,
          "open_issues": 893,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85702467?v=4",
      "velocity": 152544.7,
      "is_rising_star": true
    },
    {
      "id": "github-f-awesome-chatgpt-prompts",
      "name": "awesome-chatgpt-prompts",
      "author": "f",
      "description": "This repo includes ChatGPT prompt curation to use ChatGPT and other LLM tools better.",
      "task": "tool",
      "tags": [
        "bots",
        "chatbot",
        "chatgpt",
        "chatgpt-api",
        "language"
      ],
      "likes": 410025,
      "downloads": 410025,
      "lastModified": "2025-11-20T03:59:38Z",
      "lastModifiedTimestamp": 1763611178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/f/awesome-chatgpt-prompts",
          "homepage": "https://prompts.chat",
          "language": "JavaScript",
          "forks": 18173,
          "open_issues": 289,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/196477?v=4",
      "velocity": 150315,
      "is_rising_star": true
    },
    {
      "id": "github-langchain-ai-langchain",
      "name": "langchain",
      "author": "langchain-ai",
      "description": "ü¶úüîó The platform for reliable agents.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "ai-agents-framework",
        "aiagentframework",
        "anthropic",
        "chatgpt",
        "enterprise",
        "framework",
        "gemini",
        "generative-ai",
        "langchain",
        "llm",
        "multiagent",
        "open-source",
        "openai",
        "pydantic",
        "python",
        "rag"
      ],
      "likes": 360158,
      "downloads": 360158,
      "lastModified": "2025-11-20T03:57:49Z",
      "lastModifiedTimestamp": 1763611069000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langchain-ai/langchain",
          "homepage": "https://docs.langchain.com/oss/python/langchain/",
          "language": "Python",
          "forks": 19766,
          "open_issues": 225,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/126733545?v=4",
      "velocity": 132006.6,
      "is_rising_star": true
    },
    {
      "id": "github-langgenius-dify",
      "name": "dify",
      "author": "langgenius",
      "description": "Production-ready platform for agentic workflow development.",
      "task": "tool",
      "tags": [
        "agent",
        "agentic-ai",
        "agentic-framework",
        "agentic-workflow",
        "ai",
        "automation",
        "gemini",
        "genai",
        "gpt",
        "gpt-4",
        "llm",
        "low-code",
        "mcp",
        "nextjs",
        "no-code",
        "openai",
        "orchestration",
        "python",
        "rag",
        "workflow"
      ],
      "likes": 357923,
      "downloads": 357923,
      "lastModified": "2025-11-20T03:40:43Z",
      "lastModifiedTimestamp": 1763610043000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langgenius/dify",
          "homepage": "https://dify.ai",
          "language": "TypeScript",
          "forks": 18481,
          "open_issues": 694,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127165244?v=4",
      "velocity": 131180.5,
      "is_rising_star": true
    },
    {
      "id": "github-open-webui-open-webui",
      "name": "open-webui",
      "author": "open-webui",
      "description": "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
      "task": "tool",
      "tags": [
        "ai",
        "llm",
        "llm-ui",
        "llm-webui",
        "llms",
        "mcp",
        "ollama",
        "ollama-webui",
        "open-webui",
        "openai",
        "openapi",
        "rag",
        "self-hosted",
        "ui",
        "webui"
      ],
      "likes": 347017,
      "downloads": 347017,
      "lastModified": "2025-11-20T04:00:22Z",
      "lastModifiedTimestamp": 1763611222000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/open-webui/open-webui",
          "homepage": "https://openwebui.com",
          "language": "JavaScript",
          "forks": 16200,
          "open_issues": 303,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/158137808?v=4",
      "velocity": 127171,
      "is_rising_star": true
    },
    {
      "id": "allenai/wildguard",
      "name": "wildguard",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "pytorch",
        "safetensors",
        "mistral",
        "text-generation",
        "classifier",
        "safety",
        "moderation",
        "llm",
        "lm",
        "en",
        "dataset:allenai/wildguardmix",
        "arxiv:2406.18495",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "likes": 525,
      "downloads": 320115,
      "lastModifiedTimestamp": null,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/allenai/wildguard",
          "files": [],
          "modelId": "allenai/wildguard"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/allenai/wildguard",
          "files": [],
          "modelId": "allenai/wildguard"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/allenai/wildguard",
          "files": [],
          "modelId": "allenai/wildguard"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/allenai/wildguard",
          "files": [],
          "modelId": "allenai/wildguard"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/allenai/wildguard",
          "files": [],
          "modelId": "allenai/wildguard"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "github-microsoft-generative-ai-for-beginners",
      "name": "generative-ai-for-beginners",
      "author": "microsoft",
      "description": "21 Lessons, Get Started Building with Generative AI ",
      "task": "tool",
      "tags": [
        "ai",
        "azure",
        "chatgpt",
        "dall-e",
        "generative-ai",
        "generativeai",
        "gpt",
        "language-model",
        "llms",
        "microsoft-for-beginners",
        "openai",
        "prompt-engineering",
        "semantic-search",
        "transformers"
      ],
      "likes": 306061,
      "downloads": 306061,
      "lastModified": "2025-11-20T03:55:34Z",
      "lastModifiedTimestamp": 1763610934000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/generative-ai-for-beginners",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 54213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 112205.5,
      "is_rising_star": true
    },
    {
      "id": "github-x1xhlol-system-prompts-and-models-of-ai-tools",
      "name": "system-prompts-and-models-of-ai-tools",
      "author": "x1xhlol",
      "description": "FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus Agent Tools, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, dia & v0. (And other Open Sourced) System Prompts, Internal Tools & AI Models",
      "task": "tool",
      "tags": [
        "ai",
        "bolt",
        "cluely",
        "copilot",
        "cursor",
        "cursorai",
        "devin",
        "github-copilot",
        "lovable",
        "open-source",
        "perplexity",
        "replit",
        "system-prompts",
        "trae",
        "trae-ai",
        "trae-ide",
        "v0",
        "vscode",
        "windsurf",
        "windsurf-ai"
      ],
      "likes": 288923,
      "downloads": 288923,
      "lastModified": "2025-11-20T04:00:54Z",
      "lastModifiedTimestamp": 1763611254000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
          "homepage": "",
          "language": null,
          "forks": 25879,
          "open_issues": 94,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/185671340?v=4",
      "velocity": 105805.7,
      "is_rising_star": true
    },
    {
      "id": "github-ggml-org-llama.cpp",
      "name": "llama.cpp",
      "author": "ggml-org",
      "description": "LLM inference in C/C++",
      "task": "tool",
      "tags": [
        "ggml"
      ],
      "likes": 270241,
      "downloads": 270241,
      "lastModified": "2025-11-20T03:56:01Z",
      "lastModifiedTimestamp": 1763610961000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ggml-org/llama.cpp",
          "homepage": "",
          "language": "C++",
          "forks": 13747,
          "open_issues": 899,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134263123?v=4",
      "velocity": 99053.9,
      "is_rising_star": true
    },
    {
      "id": "github-google-gemini-gemini-cli",
      "name": "gemini-cli",
      "author": "google-gemini",
      "description": "An open-source AI agent that brings the power of Gemini directly into your terminal.",
      "task": "tool",
      "tags": [
        "gemini",
        "gemini-api"
      ],
      "likes": 250395,
      "downloads": 250395,
      "lastModified": "2025-11-20T03:57:37Z",
      "lastModifiedTimestamp": 1763611057000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/google-gemini/gemini-cli",
          "homepage": "https://geminicli.com",
          "language": "TypeScript",
          "forks": 9384,
          "open_issues": 3007,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/161781182?v=4",
      "velocity": 91595.9,
      "is_rising_star": true
    },
    {
      "id": "github-Shubhamsaboo-awesome-llm-apps",
      "name": "awesome-llm-apps",
      "author": "Shubhamsaboo",
      "description": "Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.",
      "task": "tool",
      "tags": [
        "llms",
        "python",
        "rag"
      ],
      "likes": 237097,
      "downloads": 237097,
      "lastModified": "2025-11-20T04:00:02Z",
      "lastModifiedTimestamp": 1763611202000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
          "homepage": "https://www.theunwindai.com",
          "language": "Python",
          "forks": 10516,
          "open_issues": 1,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/31396011?v=4",
      "velocity": 86805.4,
      "is_rising_star": true
    },
    {
      "id": "github-rasbt-LLMs-from-scratch",
      "name": "LLMs-from-scratch",
      "author": "rasbt",
      "description": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "deep-learning",
        "from-scratch",
        "generative-ai",
        "gpt",
        "language-model",
        "large-language-models",
        "llm",
        "machine-learning",
        "neural-networks",
        "python",
        "pytorch",
        "transformers"
      ],
      "likes": 237017,
      "downloads": 237017,
      "lastModified": "2025-11-20T03:52:19Z",
      "lastModifiedTimestamp": 1763610739000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/rasbt/LLMs-from-scratch",
          "homepage": "https://amzn.to/4fqvn0D",
          "language": "Jupyter Notebook",
          "forks": 11706,
          "open_issues": 0,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/5618407?v=4",
      "velocity": 86865.9,
      "is_rising_star": true
    },
    {
      "id": "github-nomic-ai-gpt4all",
      "name": "gpt4all",
      "author": "nomic-ai",
      "description": "GPT4All: Run Local LLMs on Any Device. Open-source and available for commercial use.",
      "task": "tool",
      "tags": [
        "ai-chat",
        "llm-inference"
      ],
      "likes": 230773,
      "downloads": 230773,
      "lastModified": "2025-11-20T02:34:37Z",
      "lastModifiedTimestamp": 1763606077000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/nomic-ai/gpt4all",
          "homepage": "https://nomic.ai/gpt4all",
          "language": "C++",
          "forks": 8303,
          "open_issues": 744,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102670180?v=4",
      "velocity": 84613.1,
      "is_rising_star": true
    },
    {
      "id": "h2oai/h2o-danube3-500m-chat",
      "name": "h2o-danube3-500m-chat",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "onnx",
        "safetensors",
        "llama",
        "text-generation",
        "gpt",
        "llm",
        "large language model",
        "h2o-llmstudio",
        "conversational",
        "en",
        "arxiv:2407.09276",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "likes": 570,
      "downloads": 229650,
      "lastModifiedTimestamp": null,
      "readme": "---\nlanguage:\n- en\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- gpt\n- llm\n- large language model\n- h2o-llmstudio\nthumbnail: >-\n  https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\npipeline_tag: text-generation\n---\n\n\n\n<div style=\"width: 90%; max-width: 600px; margin: 0 auto; overflow: hidden; background-color: white\">\n    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/636d18755aaed143cd6698ef/LAzQu_f5WOX7vqKl4yDsY.png\" \n         alt=\"Slightly cropped image\" \n         style=\"width: 102%; height: 102%; object-fit: cover; object-position: center; margin: -5% -5% -5% -5%;\">\n</div>\n\n## Summary\n\n\nh2o-danube3-500m-chat is a chat fine-tuned model by H2O.ai with 500 million parameters. We release two versions of this model:\n\n| Model Name                                                                         |  Description    |\n|:-----------------------------------------------------------------------------------|:----------------|\n|  [h2oai/h2o-danube3-500m-base](https://huggingface.co/h2oai/h2o-danube3-500m-base) | Base model      |\n|  [h2oai/h2o-danube3-500m-chat](https://huggingface.co/h2oai/h2o-danube3-500m-chat) | Chat model |\n\nThis model was trained using [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio).\n\nCan be run natively and fully offline on phones - try it yourself with [H2O AI Personal GPT](https://h2o.ai/platform/danube/personal-gpt/).\n\n## Model Architecture\n\nWe adjust the Llama 2 architecture for a total of around 500m parameters. For details, please refer to our [Technical Report](https://arxiv.org/abs/2407.09276). We use the Mistral tokenizer with a vocabulary size of 32,000 and train our model up to a context length of 8,192.\n\nThe details of the model architecture are:\n\n| Hyperparameter  |  Value |\n|:----------------|:-------|\n|    n_layers     |     16 |\n|     n_heads     |     16 |\n|  n_query_groups |      8 |\n|     n_embd      |   1536 |\n|   vocab size    |  32000 |\n| sequence length |   8192 |\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the `transformers` library installed.\n\n```bash\npip install transformers>=4.42.3\n```\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"h2oai/h2o-danube3-500m-chat\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\n# We use the HF Tokenizer chat template to format each message\n# https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\"role\": \"user\", \"content\": \"Why is drinking water so healthy?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nres = pipe(\n    prompt,\n    return_full_text=False,\n    max_new_tokens=256,\n)\nprint(res[0][\"generated_text\"])\n```\n\nThis will apply and run the correct prompt format out of the box:\n\n```\n<|prompt|>Why is drinking water so healthy?</s><|answer|>\n```\n\nAlternatively, one can also run it via:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"h2oai/h2o-danube3-500m-chat\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Why is drinking water so healthy?\"},\n]\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\ninputs = tokenizer(\n    prompt, return_tensors=\"pt\", add_special_tokens=False\n).to(\"cuda\")\n\n# generate configuration can be modified to your needs\ntokens = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    min_new_tokens=2,\n    max_new_tokens=256,\n)[0]\n\ntokens = tokens[inputs[\"input_ids\"].shape[1]:]\nanswer = tokenizer.decode(tokens, skip_special_tokens=True)\nprint(answer)\n```\n\n## Quantization and sharding\n\nYou can load the models using quantization by specifying ```load_in_8bit=True``` or ```load_in_4bit=True```. Also, sharding on multiple GPUs is possible by setting ```device_map=auto```.\n\n## Model Architecture\n\n```\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 1536, padding_idx=0)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n          (k_proj): Linear(in_features=1536, out_features=768, bias=False)\n          (v_proj): Linear(in_features=1536, out_features=768, bias=False)\n          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=1536, out_features=4096, bias=False)\n          (up_proj): Linear(in_features=1536, out_features=4096, bias=False)\n          (down_proj): Linear(in_features=4096, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=1536, out_features=32000, bias=False)\n)\n```\n\n## Benchmarks\n\n### ü§ó Open LLM Leaderboard v1\n\n| Benchmark     |   acc_n  |\n|:--------------|:--------:|\n| Average       |   40.71  |\n| ARC-challenge |   39.25  |\n| Hellaswag     |   61.02  |\n| MMLU          |   26.33  |\n| TruthfulQA    |   39.96  |\n| Winogrande    |   61.72  |\n| GSM8K         |   16.00  |\n\n### MT-Bench\n\n```\nFirst Turn: 4.16\nSecond Turn: 2.40\nAverage: 3.28\n```\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2o-danube3-500m-chat",
          "files": [],
          "modelId": "h2oai/h2o-danube3-500m-chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2o-danube3-500m-chat",
          "files": [],
          "modelId": "h2oai/h2o-danube3-500m-chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2o-danube3-500m-chat",
          "files": [],
          "modelId": "h2oai/h2o-danube3-500m-chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2o-danube3-500m-chat",
          "files": [],
          "modelId": "h2oai/h2o-danube3-500m-chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2o-danube3-500m-chat",
          "files": [],
          "modelId": "h2oai/h2o-danube3-500m-chat"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "github-browser-use-browser-use",
      "name": "browser-use",
      "author": "browser-use",
      "description": "üåê Make websites accessible for AI agents. Automate tasks online with ease.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "ai-tools",
        "browser-automation",
        "browser-use",
        "llm",
        "playwright",
        "python"
      ],
      "likes": 218221,
      "downloads": 218221,
      "lastModified": "2025-11-20T03:31:37Z",
      "lastModifiedTimestamp": 1763609497000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/browser-use/browser-use",
          "homepage": "https://browser-use.com",
          "language": "Python",
          "forks": 8657,
          "open_issues": 229,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192012301?v=4",
      "velocity": 79990.9,
      "is_rising_star": true
    },
    {
      "id": "github-binary-husky-gpt_academic",
      "name": "gpt_academic",
      "author": "binary-husky",
      "description": "‰∏∫GPT/GLMÁ≠âLLMÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÂÆûÁî®Âåñ‰∫§‰∫íÊé•Âè£ÔºåÁâπÂà´‰ºòÂåñËÆ∫ÊñáÈòÖËØª/Ê∂¶Ëâ≤/ÂÜô‰Ωú‰ΩìÈ™åÔºåÊ®°ÂùóÂåñËÆæËÆ°ÔºåÊîØÊåÅËá™ÂÆö‰πâÂø´Êç∑ÊåâÈíÆ&ÂáΩÊï∞Êèí‰ª∂ÔºåÊîØÊåÅPythonÂíåC++Á≠âÈ°πÁõÆÂâñÊûê&Ëá™ËØëËß£ÂäüËÉΩÔºåPDF/LaTexËÆ∫ÊñáÁøªËØë&ÊÄªÁªìÂäüËÉΩÔºåÊîØÊåÅÂπ∂Ë°åÈóÆËØ¢Â§öÁßçLLMÊ®°ÂûãÔºåÊîØÊåÅchatglm3Á≠âÊú¨Âú∞Ê®°Âûã„ÄÇÊé•ÂÖ•ÈÄö‰πâÂçÉÈóÆ, deepseekcoder, ËÆØÈ£ûÊòüÁÅ´, ÊñáÂøÉ‰∏ÄË®Ä, llama2, rwkv, claude2, mossÁ≠â„ÄÇ",
      "task": "tool",
      "tags": [
        "academic",
        "chatglm-6b",
        "chatgpt",
        "gpt-4",
        "large-language-models"
      ],
      "likes": 209088,
      "downloads": 209088,
      "lastModified": "2025-11-20T03:43:15Z",
      "lastModifiedTimestamp": 1763610195000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/binary-husky/gpt_academic",
          "homepage": "https://github.com/binary-husky/gpt_academic/wiki/online",
          "language": "Python",
          "forks": 8401,
          "open_issues": 291,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/96192199?v=4",
      "velocity": 76661.2,
      "is_rising_star": true
    },
    {
      "id": "github-firecrawl-firecrawl",
      "name": "firecrawl",
      "author": "firecrawl",
      "description": "üî• The Web Data API for AI - Turn entire websites into LLM-ready markdown or structured data",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agents",
        "ai-crawler",
        "ai-scraping",
        "ai-search",
        "crawler",
        "data-extraction",
        "html-to-markdown",
        "llm",
        "markdown",
        "scraper",
        "scraping",
        "web-crawler",
        "web-data",
        "web-data-extraction",
        "web-scraper",
        "web-scraping",
        "web-search",
        "webscraping"
      ],
      "likes": 204312,
      "downloads": 204312,
      "lastModified": "2025-11-20T03:40:22Z",
      "lastModifiedTimestamp": 1763610022000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/firecrawl/firecrawl",
          "homepage": "https://firecrawl.dev",
          "language": "TypeScript",
          "forks": 5298,
          "open_issues": 140,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/135057108?v=4",
      "velocity": 74868.2,
      "is_rising_star": true
    },
    {
      "id": "github-infiniflow-ragflow",
      "name": "ragflow",
      "author": "infiniflow",
      "description": "RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs",
      "task": "tool",
      "tags": [
        "agent",
        "agentic",
        "agentic-ai",
        "agentic-workflow",
        "ai",
        "ai-search",
        "deep-learning",
        "deep-research",
        "deepseek",
        "deepseek-r1",
        "document-parser",
        "document-understanding",
        "graphrag",
        "llm",
        "mcp",
        "multi-agent",
        "ollama",
        "openai",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 204013,
      "downloads": 204013,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/infiniflow/ragflow",
          "homepage": "https://ragflow.io",
          "language": "Python",
          "forks": 7289,
          "open_issues": 2918,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/69962740?v=4",
      "velocity": 74763.7,
      "is_rising_star": true
    },
    {
      "id": "github-lobehub-lobe-chat",
      "name": "lobe-chat",
      "author": "lobehub",
      "description": "ü§Ø LobeHub - an open-source, modern design AI Agent Workspace. Supports multiple AI providers, Knowledge Base (file upload / RAG ), one click install MCP Marketplace and Artifacts / Thinking. One-click FREE deployment of your private AI Agent application.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "artifacts",
        "chat",
        "chatgpt",
        "claude",
        "deepseek",
        "deepseek-r1",
        "function-calling",
        "gemini",
        "gpt",
        "knowledge-base",
        "mcp",
        "nextjs",
        "ollama",
        "openai",
        "rag"
      ],
      "likes": 203567,
      "downloads": 203567,
      "lastModified": "2025-11-20T03:53:05Z",
      "lastModifiedTimestamp": 1763610785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/lobehub/lobe-chat",
          "homepage": "https://lobechat.com",
          "language": "TypeScript",
          "forks": 13985,
          "open_issues": 979,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131470832?v=4",
      "velocity": 74618.5,
      "is_rising_star": true
    },
    {
      "id": "github-mlabonne-llm-course",
      "name": "llm-course",
      "author": "mlabonne",
      "description": "Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.",
      "task": "tool",
      "tags": [
        "course",
        "large-language-models",
        "llm",
        "machine-learning",
        "roadmap"
      ],
      "likes": 203252,
      "downloads": 203252,
      "lastModified": "2025-11-20T03:54:32Z",
      "lastModifiedTimestamp": 1763610872000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mlabonne/llm-course",
          "homepage": "https://mlabonne.github.io/blog/",
          "language": null,
          "forks": 7672,
          "open_issues": 76,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/81252890?v=4",
      "velocity": 74476.6,
      "is_rising_star": true
    },
    {
      "id": "github-ansible-ansible",
      "name": "ansible",
      "author": "ansible",
      "description": "Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.",
      "task": "tool",
      "tags": [
        "ansible",
        "python"
      ],
      "likes": 201152,
      "downloads": 201152,
      "lastModified": "2025-11-20T00:37:21Z",
      "lastModifiedTimestamp": 1763599041000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ansible/ansible",
          "homepage": "https://www.ansible.com/",
          "language": "Python",
          "forks": 24129,
          "open_issues": 872,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1507452?v=4",
      "velocity": 73748.4,
      "is_rising_star": true
    },
    {
      "id": "github-dair-ai-Prompt-Engineering-Guide",
      "name": "Prompt-Engineering-Guide",
      "author": "dair-ai",
      "description": "üêô Guides, papers, lessons, notebooks and resources for prompt engineering, context engineering, RAG, and AI Agents.",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai-agents",
        "chatgpt",
        "deep-learning",
        "generative-ai",
        "language-model",
        "llms",
        "openai",
        "prompt-engineering",
        "rag"
      ],
      "likes": 199720,
      "downloads": 199720,
      "lastModified": "2025-11-20T03:58:38Z",
      "lastModifiedTimestamp": 1763611118000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
          "homepage": "https://www.promptingguide.ai/",
          "language": "MDX",
          "forks": 6945,
          "open_issues": 230,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/30384625?v=4",
      "velocity": 73213.8,
      "is_rising_star": true
    },
    {
      "id": "github-OpenHands-OpenHands",
      "name": "OpenHands",
      "author": "OpenHands",
      "description": "üôå OpenHands: Code Less, Make More",
      "task": "tool",
      "tags": [
        "agent",
        "artificial-intelligence",
        "chatgpt",
        "claude-ai",
        "cli",
        "developer-tools",
        "gpt",
        "llm",
        "openai"
      ],
      "likes": 195291,
      "downloads": 195291,
      "lastModified": "2025-11-20T03:44:01Z",
      "lastModifiedTimestamp": 1763610241000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenHands/OpenHands",
          "homepage": "https://all-hands.dev",
          "language": "Python",
          "forks": 7928,
          "open_issues": 242,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/225919603?v=4",
      "velocity": 71589.1,
      "is_rising_star": true
    },
    {
      "id": "github-PaddlePaddle-PaddleOCR",
      "name": "PaddleOCR",
      "author": "PaddlePaddle",
      "description": "Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.",
      "task": "tool",
      "tags": [
        "ai4science",
        "chineseocr",
        "document-parsing",
        "document-translation",
        "kie",
        "ocr",
        "paddleocr-vl",
        "pdf-extractor-rag",
        "pdf-parser",
        "pdf2markdown",
        "pp-ocr",
        "pp-structure",
        "rag"
      ],
      "likes": 192979,
      "downloads": 192979,
      "lastModified": "2025-11-20T03:32:54Z",
      "lastModifiedTimestamp": 1763609574000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/PaddlePaddle/PaddleOCR",
          "homepage": "https://www.paddleocr.ai",
          "language": "Python",
          "forks": 9361,
          "open_issues": 276,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23534030?v=4",
      "velocity": 70700.3,
      "is_rising_star": true
    },
    {
      "id": "github-vllm-project-vllm",
      "name": "vllm",
      "author": "vllm-project",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "task": "tool",
      "tags": [
        "amd",
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-v3",
        "gpt",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llm",
        "llm-serving",
        "model-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen",
        "qwen3",
        "tpu",
        "transformer"
      ],
      "likes": 190450,
      "downloads": 190450,
      "lastModified": "2025-11-20T03:53:27Z",
      "lastModifiedTimestamp": 1763610807000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/vllm-project/vllm",
          "homepage": "https://docs.vllm.ai",
          "language": "Python",
          "forks": 11390,
          "open_issues": 3172,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/136984999?v=4",
      "velocity": 69779.6,
      "is_rising_star": true
    },
    {
      "id": "github-hiyouga-LLaMA-Factory",
      "name": "LLaMA-Factory",
      "author": "hiyouga",
      "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "deepseek",
        "fine-tuning",
        "gemma",
        "gpt",
        "instruction-tuning",
        "large-language-models",
        "llama",
        "llama3",
        "llm",
        "lora",
        "moe",
        "nlp",
        "peft",
        "qlora",
        "quantization",
        "qwen",
        "rlhf",
        "transformers"
      ],
      "likes": 188197,
      "downloads": 188197,
      "lastModified": "2025-11-20T03:49:20Z",
      "lastModifiedTimestamp": 1763610560000,
      "readme": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)\n[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)\n[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)\n\n[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory)\n[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align=\"center\" markdown=\"1\">\n\n### Supporters ‚ù§Ô∏è\n\n| <div style=\"text-align: center;\"><a href=\"https://warp.dev/llama-factory\"><img alt=\"Warp sponsorship\" width=\"400\" src=\"assets/sponsors/warp.jpg\"></a><br><a href=\"https://warp.dev/llama-factory\" style=\"font-size:larger;\">Warp, the agentic terminal for developers</a><br><a href=\"https://warp.dev/llama-factory\">Available for MacOS, Linux, & Windows</a> | <a href=\"https://serpapi.com\"><img alt=\"SerpAPI sponsorship\" width=\"250\" src=\"assets/sponsors/serpapi.svg\"> </a> |\n| ---- | ---- |\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\nüëã Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.\n\n\\[ English | [‰∏≠Êñá](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nStart local training:\n- Please refer to [usage](#getting-started)\n\nStart cloud training:\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory\n- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\nRead technical notes:\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/\n- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html\n- **Official Blog**: https://blog.llamafactory.net/en/\n- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [LLaMA Factory Online](#llama-factory-online)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                           |\n| ------------ | -------------------------------------------------------------------- |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |\n\n## Blogs\n\n> [!TIP]\n> Now we have a dedicated blog for LLaMA Factory!\n>\n> Website: https://blog.llamafactory.net/en/\n\n- üí° [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&type=project&utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)\n- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n\n<details><summary>All Blogs</summary>\n\n- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n- [A One-Stop Code-Free Model Fine-Tuning \\& Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.\n\n[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.\n\n[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.\n\n[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.\n\n<details><summary>Full Changelog</summary>\n\n[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)'s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)'s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)'s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template             |\n| ----------------------------------------------------------------- | -------------------------------- | -------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2            |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                    |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3             |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere               |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek             |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3            |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1           |\n| [ERNIE-4.5](https://huggingface.co/baidu)                         | 0.3B/21B/300B                    | ernie/ernie_nothink  |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon               |\n| [Falcon-H1](https://huggingface.co/tiiuae)                        | 0.5B/1.5B/3B/7B/34B              | falcon_h1            |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma/gemma2         |\n| [Gemma 3/Gemma 3n](https://huggingface.co/google)                 | 270M/1B/4B/6B/8B/12B/27B         | gemma3/gemma3n       |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/zai-org)         | 9B/32B                           | glm4/glmz1           |\n| [GLM-4.1V](https://huggingface.co/zai-org)                        | 9B                               | glm4v                |\n| [GLM-4.5/GLM-4.5V](https://huggingface.co/zai-org)                | 106B/355B                        | glm4_moe/glm4v_moe   |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                    |\n| [GPT-OSS](https://huggingface.co/openai)                          | 20B/120B                         | gpt                  |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3             |\n| [Granite 4](https://huggingface.co/ibm-granite)                   | 7B                               | granite4             |\n| [Hunyuan (MT)](https://huggingface.co/tencent/)                   | 7B                               | hunyuan              |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index                |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2              |\n| [InternVL 2.5-3.5](https://huggingface.co/OpenGVLab)              | 1B/2B/4B/8B/14B/30B/38B/78B/241B | intern_vl            |\n| [InternLM/Intern-S1-mini](https://huggingface.co/internlm/)       | 8B                               | intern_s1            |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl              |\n| [Ling 2.0 (mini/flash)](https://huggingface.co/inclusionAI)       | 16B/100B                         | bailing_v2           |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                    |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2               |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3               |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4               |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama               |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava                |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next           |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video     |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                 |\n| [MiniCPM 1-4.1](https://huggingface.co/openbmb)                   | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4        |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v  |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral            |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral              |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small        |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                    |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma            |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                    |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                  |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small            |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                 |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral              |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                 |\n| [Qwen3 (MoE/Instruct/Thinking/Next)](https://huggingface.co/Qwen) | 0.6B/1.7B/4B/8B/14B/32B/80B/235B | qwen3/qwen3_nothink  |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio          |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni           |\n| [Qwen3-Omni](https://huggingface.co/Qwen)                         | 30B                              | qwen3_omni           |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl             |\n| [Qwen3-VL](https://huggingface.co/Qwen)                           | 2B/4B/8B/30B/32B/235B            | qwen3_vl             |\n| [Seed (OSS/Coder)](https://huggingface.co/ByteDance-Seed)         | 8B/36B                           | seed_oss/seed_coder  |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1           |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                    |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2            |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse               |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                   |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl                |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                 |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> If the model has both reasoning and non-reasoning versions, please use the `_nothink` suffix to distinguish between them. For example, `qwen3` and `qwen3_nothink`.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> \\*: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> \\*\\*: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |        OFT         |        QOFT        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [CCI3-HQ (zh)](https://huggingface.co/datasets/BAAI/CCI3-HQ)\n- [CCI3-Data (zh)](https://huggingface.co/datasets/BAAI/CCI3-Data)\n- [CCI4.0-M2-Base-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1)\n- [CCI4.0-M2-CoT-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1)\n- [CCI4.0-M2-Extra-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [Infinity Instruct (zh)](https://huggingface.co/datasets/BAAI/Infinity-Instruct)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install \"huggingface_hub<1.0.0\"\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.49.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method                              | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ----------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)             |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)                  |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam/OFT |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA / QOFT                        |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA / QOFT                        |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA / QOFT                        |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n#### Install from Source\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev\n\n#### Install from Docker Image\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nThis image is built on Ubuntu 22.04 (x86\\_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.\n\nFind the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags\n\nPlease refer to [build docker](#build-docker) to build the image yourself.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can't pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**, **[DataFlow](https://github.com/OpenDCAI/DataFlow)** and **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### LLaMA Factory Online\n\nRead our [documentation](https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory).\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Use Docker volumes</summary>\n\nYou can uncomment `VOLUME [ \"/root/.cache/huggingface\", \"/app/shared_data\", \"/app/output\" ]` in the Dockerfile to use data volumes.\n\nWhen building the Docker image, use `-v ./hf_cache:/root/.cache/huggingface` argument to mount the local directory to the container. The following data volumes are available.\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine.\n- `shared_data`: The directionary to store datasets on the host machine.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [[paper]](https://aclanthology.org/2024.findings-acl.830.pdf)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n1. **[EmoLLM](https://github.com/SmartFlowAI/EmoLLM)**: A project about large language models (LLMs) and mental health.\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/hiyouga/LLaMA-Factory",
          "homepage": "https://llamafactory.readthedocs.io",
          "language": "Python",
          "forks": 7592,
          "open_issues": 782,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/16256802?v=4",
      "velocity": 68968.9,
      "is_rising_star": true
    },
    {
      "id": "github-FoundationAgents-MetaGPT",
      "name": "MetaGPT",
      "author": "FoundationAgents",
      "description": "üåü The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "task": "tool",
      "tags": [
        "agent",
        "gpt",
        "llm",
        "metagpt",
        "multi-agent"
      ],
      "likes": 178722,
      "downloads": 178722,
      "lastModified": "2025-11-20T03:51:26Z",
      "lastModifiedTimestamp": 1763610686000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FoundationAgents/MetaGPT",
          "homepage": "https://mgx.dev/",
          "language": "Python",
          "forks": 7283,
          "open_issues": 58,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/198047230?v=4",
      "velocity": 65516,
      "is_rising_star": true
    },
    {
      "id": "github-unclecode-crawl4ai",
      "name": "crawl4ai",
      "author": "unclecode",
      "description": "üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN",
      "task": "tool",
      "tags": [],
      "likes": 168306,
      "downloads": 168306,
      "lastModified": "2025-11-20T03:44:18Z",
      "lastModifiedTimestamp": 1763610258000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unclecode/crawl4ai",
          "homepage": "https://crawl4ai.com",
          "language": "Python",
          "forks": 5627,
          "open_issues": 262,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/12494079?v=4",
      "velocity": 61679.2,
      "is_rising_star": true
    },
    {
      "id": "github-OpenBB-finance-OpenBB",
      "name": "OpenBB",
      "author": "OpenBB-finance",
      "description": "Financial data platform for analysts, quants and AI agents.",
      "task": "tool",
      "tags": [
        "ai",
        "crypto",
        "derivatives",
        "economics",
        "equity",
        "finance",
        "fixed-income",
        "machine-learning",
        "openbb",
        "options",
        "python",
        "quantitative-finance",
        "stocks"
      ],
      "likes": 163943,
      "downloads": 163943,
      "lastModified": "2025-11-20T03:23:51Z",
      "lastModifiedTimestamp": 1763609031000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenBB-finance/OpenBB",
          "homepage": "https://openbb.co",
          "language": "Python",
          "forks": 5282,
          "open_issues": 51,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/80064875?v=4",
      "velocity": 60102.9,
      "is_rising_star": true
    },
    {
      "id": "github-simular-ai-Agent-S",
      "name": "Agent-S",
      "author": "simular-ai",
      "description": "Agent S: an open agentic framework that uses computers like a human",
      "task": "tool",
      "tags": [
        "agent-computer-interface",
        "ai-agents",
        "computer-automation",
        "computer-use",
        "computer-use-agent",
        "cua",
        "grounding",
        "gui-agents",
        "in-context-reinforcement-learning",
        "memory",
        "mllm",
        "planning",
        "retrieval-augmented-generation",
        "agents",
        "anthropic",
        "anthropic-claude",
        "automation",
        "claude",
        "claude-code",
        "claude-code-cli",
        "claude-code-commands",
        "claude-code-plugin",
        "claude-code-plugins",
        "claude-code-subagents",
        "claude-skills",
        "claudecode",
        "claudecode-config",
        "claudecode-subagents",
        "orchestration",
        "sub-agents",
        "subagents",
        "workflows",
        "ai",
        "openai",
        "real-time",
        "video",
        "voice",
        "autonomous-agents",
        "language-model",
        "llm"
      ],
      "likes": 160186,
      "downloads": 160186,
      "lastModified": "2025-11-20T03:36:25Z",
      "lastModifiedTimestamp": 1763609785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/simular-ai/Agent-S",
          "homepage": "https://www.simular.ai",
          "language": "Python",
          "forks": 905,
          "open_issues": 13,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/wshobson/agents",
          "homepage": "https://sethhobson.com",
          "language": "Python",
          "forks": 2349,
          "open_issues": 4,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/contains-studio/agents",
          "homepage": null,
          "language": null,
          "forks": 2126,
          "open_issues": 9,
          "license": "No license"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/livekit/agents",
          "homepage": "https://docs.livekit.io/agents",
          "language": "Python",
          "forks": 1768,
          "open_issues": 452,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/aiwaves-cn/agents",
          "homepage": "",
          "language": "Python",
          "forks": 452,
          "open_issues": 39,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/99358647?v=4",
      "velocity": 58689.4,
      "is_rising_star": true
    },
    {
      "id": "github-cline-cline",
      "name": "cline",
      "author": "cline",
      "description": "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.",
      "task": "tool",
      "tags": [],
      "likes": 157519,
      "downloads": 157519,
      "lastModified": "2025-11-20T03:55:20Z",
      "lastModifiedTimestamp": 1763610920000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/cline/cline",
          "homepage": "https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev",
          "language": "TypeScript",
          "forks": 5242,
          "open_issues": 881,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/184127137?v=4",
      "velocity": 57740.1,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-autogen",
      "name": "autogen",
      "author": "microsoft",
      "description": "A programming framework for agentic AI",
      "task": "tool",
      "tags": [
        "agentic",
        "agentic-agi",
        "agents",
        "ai",
        "autogen",
        "autogen-ecosystem",
        "chatgpt",
        "framework",
        "llm-agent",
        "llm-framework"
      ],
      "likes": 155426,
      "downloads": 155426,
      "lastModified": "2025-11-20T03:15:25Z",
      "lastModifiedTimestamp": 1763608525000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/autogen",
          "homepage": "https://microsoft.github.io/autogen/",
          "language": "Python",
          "forks": 7866,
          "open_issues": 509,
          "license": "Creative Commons Attribution 4.0 International"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 56980,
      "is_rising_star": true
    },
    {
      "id": "github-Mintplex-Labs-anything-llm",
      "name": "anything-llm",
      "author": "Mintplex-Labs",
      "description": "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "custom-ai-agents",
        "deepseek",
        "kimi",
        "llama3",
        "llm",
        "lmstudio",
        "local-llm",
        "localai",
        "mcp",
        "mcp-servers",
        "moonshot",
        "multimodal",
        "no-code",
        "ollama",
        "qwen3",
        "rag",
        "vector-database",
        "web-scraping"
      ],
      "likes": 153633,
      "downloads": 153633,
      "lastModified": "2025-11-20T03:46:02Z",
      "lastModifiedTimestamp": 1763610362000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Mintplex-Labs/anything-llm",
          "homepage": "https://anythingllm.com",
          "language": "JavaScript",
          "forks": 5414,
          "open_issues": 345,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134426827?v=4",
      "velocity": 56316.7,
      "is_rising_star": true
    },
    {
      "id": "github-openai-codex",
      "name": "codex",
      "author": "openai",
      "description": "Lightweight coding agent that runs in your terminal",
      "task": "tool",
      "tags": [],
      "likes": 152654,
      "downloads": 152654,
      "lastModified": "2025-11-20T03:47:36Z",
      "lastModifiedTimestamp": 1763610456000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openai/codex",
          "homepage": "",
          "language": "Rust",
          "forks": 6374,
          "open_issues": 1043,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14957082?v=4",
      "velocity": 55924,
      "is_rising_star": true
    },
    {
      "id": "github-pathwaycom-pathway",
      "name": "pathway",
      "author": "pathwaycom",
      "description": "Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.",
      "task": "tool",
      "tags": [
        "batch-processing",
        "data-analytics",
        "data-pipelines",
        "data-processing",
        "dataflow",
        "etl",
        "etl-framework",
        "iot-analytics",
        "kafka",
        "machine-learning-algorithms",
        "pathway",
        "python",
        "real-time",
        "rust",
        "stream-processing",
        "streaming",
        "time-series-analysis"
      ],
      "likes": 150231,
      "downloads": 150231,
      "lastModified": "2025-11-20T03:36:07Z",
      "lastModifiedTimestamp": 1763609767000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pathwaycom/pathway",
          "homepage": "https://pathway.com",
          "language": "Python",
          "forks": 1453,
          "open_issues": 39,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25750857?v=4",
      "velocity": 55056.1,
      "is_rising_star": true
    },
    {
      "id": "github-opendatalab-MinerU",
      "name": "MinerU",
      "author": "opendatalab",
      "description": "Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.",
      "task": "tool",
      "tags": [
        "ai4science",
        "document-analysis",
        "extract-data",
        "layout-analysis",
        "ocr",
        "parser",
        "pdf",
        "pdf-converter",
        "pdf-extractor-llm",
        "pdf-extractor-pretrain",
        "pdf-extractor-rag",
        "pdf-parser",
        "python"
      ],
      "likes": 147376,
      "downloads": 147376,
      "lastModified": "2025-11-20T03:56:20Z",
      "lastModifiedTimestamp": 1763610980000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/opendatalab/MinerU",
          "homepage": "https://opendatalab.github.io/MinerU/",
          "language": "Python",
          "forks": 4072,
          "open_issues": 123,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/97503431?v=4",
      "velocity": 54001.2,
      "is_rising_star": true
    },
    {
      "id": "github-unslothai-unsloth",
      "name": "unsloth",
      "author": "unslothai",
      "description": "Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.",
      "task": "tool",
      "tags": [
        "agent",
        "deepseek",
        "deepseek-r1",
        "fine-tuning",
        "gemma",
        "gemma3",
        "gpt-oss",
        "llama",
        "llama3",
        "llm",
        "llms",
        "mistral",
        "openai",
        "qwen",
        "qwen3",
        "reinforcement-learning",
        "text-to-speech",
        "tts",
        "unsloth",
        "voice-cloning"
      ],
      "likes": 145378,
      "downloads": 145378,
      "lastModified": "2025-11-20T03:57:06Z",
      "lastModifiedTimestamp": 1763611026000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unslothai/unsloth",
          "homepage": "https://docs.unsloth.ai/",
          "language": "Python",
          "forks": 3987,
          "open_issues": 862,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/150920049?v=4",
      "velocity": 53275.2,
      "is_rising_star": true
    },
    {
      "id": "github-huginn-huginn",
      "name": "huginn",
      "author": "huginn",
      "description": "Create agents that monitor and act on your behalf.  Your agents are standing by!",
      "task": "tool",
      "tags": [
        "agent",
        "automation",
        "feed",
        "feedgenerator",
        "huginn",
        "monitoring",
        "notifications",
        "rss",
        "scraper",
        "twitter",
        "twitter-streaming",
        "webscraping"
      ],
      "likes": 144302,
      "downloads": 144302,
      "lastModified": "2025-11-20T03:59:01Z",
      "lastModifiedTimestamp": 1763611141000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huginn/huginn",
          "homepage": "",
          "language": "Ruby",
          "forks": 4194,
          "open_issues": 693,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23225142?v=4",
      "velocity": 52906.7,
      "is_rising_star": true
    },
    {
      "id": "github-harry0703-MoneyPrinterTurbo",
      "name": "MoneyPrinterTurbo",
      "author": "harry0703",
      "description": "Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.",
      "task": "tool",
      "tags": [
        "ai",
        "automation",
        "chatgpt",
        "moviepy",
        "python",
        "shortvideo",
        "tiktok"
      ],
      "likes": 143513,
      "downloads": 143513,
      "lastModified": "2025-11-20T03:41:13Z",
      "lastModifiedTimestamp": 1763610073000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/harry0703/MoneyPrinterTurbo",
          "homepage": "",
          "language": "Python",
          "forks": 6693,
          "open_issues": 217,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/4928832?v=4",
      "velocity": 52607.5,
      "is_rising_star": true
    },
    {
      "id": "github-pathwaycom-llm-app",
      "name": "llm-app",
      "author": "pathwaycom",
      "description": "Ready-to-run cloud templates for RAG, AI pipelines, and enterprise search with live data. üê≥Docker-friendly.‚ö°Always in sync with Sharepoint, Google Drive, S3, Kafka, PostgreSQL, real-time data APIs, and more.",
      "task": "tool",
      "tags": [
        "chatbot",
        "hugging-face",
        "llm",
        "llm-local",
        "llm-prompting",
        "llm-security",
        "llmops",
        "machine-learning",
        "open-ai",
        "pathway",
        "rag",
        "real-time",
        "retrieval-augmented-generation",
        "vector-database",
        "vector-index"
      ],
      "likes": 141740,
      "downloads": 141740,
      "lastModified": "2025-11-20T03:27:51Z",
      "lastModifiedTimestamp": 1763609271000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pathwaycom/llm-app",
          "homepage": "https://pathway.com/developers/templates/",
          "language": "Jupyter Notebook",
          "forks": 1213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25750857?v=4",
      "velocity": 51950.8,
      "is_rising_star": true
    },
    {
      "id": "github-FlowiseAI-Flowise",
      "name": "Flowise",
      "author": "FlowiseAI",
      "description": "Build AI Agents, Visually",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-workflow",
        "agents",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "javascript",
        "langchain",
        "large-language-models",
        "low-code",
        "multiagent-systems",
        "no-code",
        "openai",
        "rag",
        "react",
        "typescript",
        "workflow-automation"
      ],
      "likes": 140056,
      "downloads": 140056,
      "lastModified": "2025-11-20T03:45:42Z",
      "lastModifiedTimestamp": 1763610342000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FlowiseAI/Flowise",
          "homepage": "https://flowiseai.com",
          "language": "TypeScript",
          "forks": 23126,
          "open_issues": 723,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128289781?v=4",
      "velocity": 51339.2,
      "is_rising_star": true
    },
    {
      "id": "github-run-llama-llama_index",
      "name": "llama_index",
      "author": "run-llama",
      "description": "LlamaIndex is the leading framework for building LLM-powered agents over your data.",
      "task": "tool",
      "tags": [
        "agents",
        "application",
        "data",
        "fine-tuning",
        "framework",
        "llamaindex",
        "llm",
        "multi-agents",
        "rag",
        "vector-database"
      ],
      "likes": 135951,
      "downloads": 135951,
      "lastModified": "2025-11-20T02:11:43Z",
      "lastModifiedTimestamp": 1763604703000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/run-llama/llama_index",
          "homepage": "https://developers.llamaindex.ai",
          "language": "Python",
          "forks": 6524,
          "open_issues": 262,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/130722866?v=4",
      "velocity": 49837.7,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-ai-agents-for-beginners",
      "name": "ai-agents-for-beginners",
      "author": "microsoft",
      "description": "12 Lessons to Get Started Building AI Agents",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-framework",
        "agentic-rag",
        "ai-agents",
        "ai-agents-framework",
        "autogen",
        "generative-ai",
        "semantic-kernel"
      ],
      "likes": 135593,
      "downloads": 135593,
      "lastModified": "2025-11-20T03:40:06Z",
      "lastModifiedTimestamp": 1763610006000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/ai-agents-for-beginners",
          "homepage": "https://aka.ms/ai-agents-beginners",
          "language": "Jupyter Notebook",
          "forks": 15322,
          "open_issues": 11,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 49688.1,
      "is_rising_star": true
    },
    {
      "id": "github-jeecgboot-JeecgBoot",
      "name": "JeecgBoot",
      "author": "jeecgboot",
      "description": "üî•AI‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÔºåÂä©Âäõ‰ºÅ‰∏öÂø´ÈÄüÂÆûÁé∞‰Ωé‰ª£Á†ÅÂºÄÂèëÂíåÊûÑÂª∫AIÂ∫îÁî®ÔºÅ ÈõÜÊàê‰∏ÄÂ•óÂÆåÊï¥AIÂ∫îÁî®Âπ≥Âè∞ÔºöÊ∂µÁõñAIÂ∫îÁî®„ÄÅAIÊ®°Âûã„ÄÅAIËÅäÂ§©Âä©Êâã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAIÊµÅÁ®ãÁºñÊéíÁ≠âÔºåÂÖºÂÆπÂ§öÁßçÂ§ßÊ®°ÂûãÔºõÊèê‰æõÂº∫Â§ß‰ª£Á†ÅÁîüÊàêÂô®ÔºöÂÆûÁé∞ÂâçÂêéÁ´Ø‰∏ÄÈîÆÁîüÊàêÔºåÊó†ÈúÄÊâãÂÜô‰ª£Á†Å! ÂºïÈ¢ÜAIÂºÄÂèëÊ®°ÂºèÔºöAIÁîüÊàê‚ÜíÂú®Á∫øÈÖçÁΩÆ‚Üí‰ª£Á†ÅÁîüÊàê‚ÜíÊâãÂ∑•ÂêàÂπ∂ÔºåËß£ÂÜ≥JavaÈ°πÁõÆ80%ÈáçÂ§çÂ∑•‰ΩúÔºåÊèêÂçáÊïàÁéáËäÇÁúÅÊàêÊú¨ÔºåÂèà‰∏çÂ§±ÁÅµÊ¥ª~",
      "task": "tool",
      "tags": [
        "activiti",
        "agent",
        "ai",
        "aiflow",
        "ant-design-vue",
        "antd",
        "codegenerator",
        "deepseek",
        "flowable",
        "langchain4j",
        "llm",
        "low-code",
        "mcp",
        "mybatis-plus",
        "rag",
        "spring-ai",
        "springboot",
        "springboot3",
        "springcloud",
        "vue3"
      ],
      "likes": 133232,
      "downloads": 133232,
      "lastModified": "2025-11-20T03:18:17Z",
      "lastModifiedTimestamp": 1763608697000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/jeecgboot/JeecgBoot",
          "homepage": "https://jeecgboot.github.io/JeecgBoot/",
          "language": "Java",
          "forks": 15659,
          "open_issues": 53,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/86360035?v=4",
      "velocity": 48846.6,
      "is_rising_star": true
    },
    {
      "id": "github-mem0ai-mem0",
      "name": "mem0",
      "author": "mem0ai",
      "description": "Universal memory layer for AI Agents",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "application",
        "chatbots",
        "chatgpt",
        "genai",
        "hacktoberfest",
        "llm",
        "long-term-memory",
        "memory",
        "memory-management",
        "python",
        "rag",
        "state-management"
      ],
      "likes": 129974,
      "downloads": 129974,
      "lastModified": "2025-11-20T03:24:00Z",
      "lastModifiedTimestamp": 1763609040000,
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"docs/images/banner-sm.png\" width=\"800px\" alt=\"Mem0 - The Memory Layer for Personalized AI\">\n  </a>\n</p>\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/11194\" target=\"blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/11194\" alt=\"mem0ai%2Fmem0 | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai\">Learn more</a>\n  ¬∑\n  <a href=\"https://mem0.dev/DiG\">Join Discord</a>\n  ¬∑\n  <a href=\"https://mem0.dev/demo\">Demo</a>\n  ¬∑\n  <a href=\"https://mem0.dev/openmemory\">OpenMemory</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.dev/DiG\">\n    <img src=\"https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white\" alt=\"Mem0 Discord\">\n  </a>\n  <a href=\"https://pepy.tech/project/mem0ai\">\n    <img src=\"https://img.shields.io/pypi/dm/mem0ai\" alt=\"Mem0 PyPI - Downloads\">\n  </a>\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square\" alt=\"GitHub commit activity\">\n  </a>\n  <a href=\"https://pypi.org/project/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/pypi/v/mem0ai?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n  </a>\n  <a href=\"https://www.npmjs.com/package/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/npm/v/mem0ai\" alt=\"Npm package\">\n  </a>\n  <a href=\"https://www.ycombinator.com/companies/mem0\">\n    <img src=\"https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square\" alt=\"Y Combinator S24\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai/research\"><strong>üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí</strong></a>\n</p>\n<p align=\"center\">\n  <strong>‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens</strong>\n</p>\n\n> **üéâ mem0ai v1.0.0 is now available!** This major release includes API modernization, improved vector store support, and enhanced GCP integration. [See migration guide ‚Üí](MIGRATION_GUIDE_v1.0.md)\n\n##  üî• Research Highlights\n- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark\n- **91% Faster Responses** than full-context, ensuring low-latency at scale\n- **90% Lower Token Usage** than full-context, cutting costs without compromise\n- [Read the full paper](https://mem0.ai/research)\n\n# Introduction\n\n[Mem0](https://mem0.ai) (\"mem-zero\") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.\n\n### Key Features & Use Cases\n\n**Core Capabilities:**\n- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization\n- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option\n\n**Applications:**\n- **AI Assistants**: Consistent, context-rich conversations\n- **Customer Support**: Recall past tickets and user history for tailored help\n- **Healthcare**: Track patient preferences and history for personalized care\n- **Productivity & Gaming**: Adaptive workflows and environments based on user behavior\n\n## üöÄ Quickstart Guide <a name=\"quickstart\"></a>\n\nChoose between our hosted platform or self-hosted package:\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [Mem0 Platform](https://app.mem0.ai)\n2. Embed the memory layer via SDK or API keys\n\n### Self-Hosted (Open Source)\n\nInstall the sdk via pip:\n\n```bash\npip install mem0ai\n```\n\nInstall sdk via npm:\n```bash\nnpm install mem0ai\n```\n\n### Basic Usage\n\nMem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).\n\nFirst step is to instantiate the memory:\n\n```python\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n\n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nFor detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).\n\n## üîó Integrations & Demos\n\n- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))\n- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))\n- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))\n- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))\n\n## üìö Documentation & Support\n\n- Full docs: https://docs.mem0.ai\n- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)\n- Contact: founders@mem0.ai\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n```\n\n## ‚öñÔ∏è License\n\nApache 2.0 ‚Äî see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mem0ai/mem0",
          "homepage": "https://mem0.ai",
          "language": "Python",
          "forks": 4691,
          "open_issues": 516,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/137054526?v=4",
      "velocity": 47636.6,
      "is_rising_star": true
    },
    {
      "id": "github-anthropics-claude-code",
      "name": "claude-code",
      "author": "anthropics",
      "description": "Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.",
      "task": "tool",
      "tags": [],
      "likes": 128639,
      "downloads": 128639,
      "lastModified": "2025-11-20T03:51:58Z",
      "lastModifiedTimestamp": 1763610718000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/anthropics/claude-code",
          "homepage": "https://code.claude.com/docs/en/overview",
          "language": "Shell",
          "forks": 2893,
          "open_issues": 5277,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "velocity": 47107.5,
      "is_rising_star": true
    },
    {
      "id": "github-sst-opencode",
      "name": "opencode",
      "author": "sst",
      "description": "The AI coding agent built for the terminal.",
      "task": "tool",
      "tags": [
        "ai",
        "claude",
        "code",
        "llm",
        "openai"
      ],
      "likes": 128450,
      "downloads": 128450,
      "lastModified": "2025-11-20T03:54:03Z",
      "lastModifiedTimestamp": 1763610843000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/sst/opencode",
          "homepage": "https://opencode.ai",
          "language": "TypeScript",
          "forks": 2658,
          "open_issues": 1451,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/opencode-ai/opencode",
          "homepage": "",
          "language": "Go",
          "forks": 806,
          "open_issues": 163,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/66570915?v=4",
      "velocity": 47018.4,
      "is_rising_star": true
    },
    {
      "id": "OrionStarAI/Orion-14B-Chat",
      "name": "Orion-14B-Chat",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "pytorch",
        "gguf",
        "orion",
        "text-generation",
        "code",
        "model",
        "llm",
        "conversational",
        "custom_code",
        "en",
        "zh",
        "ja",
        "ko",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "likes": 1005,
      "downloads": 125175,
      "lastModifiedTimestamp": null,
      "readme": "---\nlanguage:\n- en\n- zh\n- ja\n- ko\nmetrics:\n- accuracy\npipeline_tag: text-generation\ntags:\n- code\n- model\n- llm\n---\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<div align=\"center\">\n  <img src=\"./assets/imgs/orion_start.PNG\" alt=\"logo\" width=\"50%\" />\n</div>\n\n<div align=\"center\">\n<h1>\n  Orion-14B\n</h1>\n</div>\n\n<div align=\"center\">\n\n<div align=\"center\">\n     <b>üåêEnglish</b> | <a href=\"https://huggingface.co/OrionStarAI/Orion-14B-Chat/blob/main/README_zh.md\" target=\"_blank\">üá®üá≥‰∏≠Êñá</a> | <a href=\"https://huggingface.co/OrionStarAI/Orion-14B-Chat/blob/main/README_ja.md\" target=\"_blank\">üáØüáµÊó•Êú¨Ë™û</a> | <a href=\"https://huggingface.co/OrionStarAI/Orion-14B-Chat/blob/main/README_ko.md\" target=\"_blank\">üá∞üá∑ÌïúÍµ≠Ïñ¥</a>\n</div>\n\n<h4 align=\"center\">\n    <p>\n        ü§ó <a href=\"https://huggingface.co/OrionStarAI\" target=\"_blank\">HuggingFace Mainpage</a> | ü§ñ <a href=\"https://modelscope.cn/organization/OrionStarAI\" target=\"_blank\">ModelScope Mainpage</a><br>üé¨ <a href=\"https://huggingface.co/spaces/OrionStarAI/Orion-14B-App-Demo\" target=\"_blank\">HuggingFace Demo</a> | üé´ <a href=\"https://modelscope.cn/studios/OrionStarAI/Orion-14B-App-Demo/summary\" target=\"_blank\">ModelScope Demo</a><br>üò∫ <a href=\"https://github.com/OrionStarAI/Orion\" target=\"_blank\">GitHub</a><br>üìñ <a href=\"https://github.com/OrionStarAI/Orion/blob/master/doc/Orion14B_v3.pdf\" target=\"_blank\">Tech Report</a>\n    <p>\n</h4>\n\n</div>\n\n\n\n# Table of Contents\n\n- [üìñ Model Introduction](#model-introduction)\n- [üîó Model Download](#model-download)\n- [üîñ Model Benchmark](#model-benchmark)\n- [üìä Model Inference](#model-inference)[<img src=\"./assets/imgs/vllm_1.png\" alt=\"vllm\" style=\"margin: 0;display: initial;\" height=\"20\" />](#vllm) [<img src=\"./assets/imgs/llama_cpp_1.png\" alt=\"llamacpp\" style=\"margin: 0;display: initial;\" height=\"20\" />](#llama-cpp)\n- [üìú Declarations & License](#declarations-license)\n- [ü•á Company Introduction](#company-introduction)\n\n<a name=\"model-introduction\"></a><br>\n# 1. Model Introduction\n\n- Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  The base model is trained on 2.5T multilingual corpus, including Chinese, English, Japanese, Korean, etc, and it exhibits superior performance in these languages.  For details, please refer to [tech report](https://github.com/OrionStarAI/Orion/blob/master/doc/Orion14B_v3.pdf).\n\n- The Orion-14B series models exhibit the following features:\n  - Among models with 20B-parameter scale level, Orion-14B-Base model shows outstanding performance in comprehensive evaluations.\n  - Strong multilingual capabilities, significantly outperforming in Japanese and Korean testsets.\n  - The fine-tuned models demonstrate strong adaptability, excelling in human-annotated blind tests.\n  - The long-chat version supports extremely long texts, performing exceptionally well at a token length of 200k and can support up to a maximum of 320k.\n  - The quantized versions reduce model size by 70%, improve inference speed by 30%, with performance loss less than 1%.\n <table style=\"border-collapse: collapse; width: 100%;\">\n   <tr>\n     <td style=\"border: none; padding: 10px; box-sizing: border-box;\">\n       <img src=\"./assets/imgs/opencompass_en.png\" alt=\"opencompass\" style=\"width: 100%; height: auto;\">\n     </td>\n     <td style=\"border: none; padding: 10px; box-sizing: border-box;\">\n       <img src=\"./assets/imgs/model_cap_en.png\" alt=\"modelcap\" style=\"width: 100%; height: auto;\">\n     </td>\n   </tr>\n </table>\n\n- Orion-14B series models including:\n  - **Orion-14B-Base:**  A multilingual large language foundational model with 14 billion parameters, pretrained on a diverse dataset of 2.5 trillion tokens.\n  - **Orion-14B-Chat:**  A chat-model fine-tuned on a high-quality corpus aims to provide an excellence interactive experience for users in the large model community.\n  - **Orion-14B-LongChat:**  The long-context version excels at handling extremely lengthy texts, performing exceptionally well at a token length of 200k and can support up to a maximum of 320k.\n  - **Orion-14B-Chat-RAG:**  A chat-model fine-tuned on a custom retrieval augmented generation dataset, achieving superior performance in retrieval augmented generation tasks.\n  - **Orion-14B-Chat-Plugin:**  A chat-model specifically tailored for plugin and function calling tasks, ideal for agent-related scenarios where the LLM acts as a plugin and function call system.\n  - **Orion-14B-Base-Int4:**  A quantized base model utilizing 4-bit integer weights. It significantly reduces the model size by 70% and increases the inference speed by 30% while incurring a minimal performance loss of only 1%.\n  - **Orion-14B-Chat-Int4:**  A quantized chat model utilizing 4-bit integer weights.\n\n\n<a name=\"model-download\"></a><br>\n# 2. Model Download\n\nModel release and download links are provided in the table below:\n\n| Model Name              | HuggingFace Download Links                                                        | ModelScope Download Links                                                                       |\n|-------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n| ‚öæOrion-14B-Base        | [Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base)               | [Orion-14B-Base](https://modelscope.cn/models/OrionStarAI/Orion-14B-Base/summary)               |\n| üòõOrion-14B-Chat        | [Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)               | [Orion-14B-Chat](https://modelscope.cn/models/OrionStarAI/Orion-14B-Chat/summary)               |\n| üìÉOrion-14B-LongChat    | [Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat)       | [Orion-14B-LongChat](https://modelscope.cn/models/OrionStarAI/Orion-14B-LongChat/summary)       |\n| üîéOrion-14B-Chat-RAG    | [Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG)       | [Orion-14B-Chat-RAG](https://modelscope.cn/models/OrionStarAI/Orion-14B-Chat-RAG/summary)       |\n| üîåOrion-14B-Chat-Plugin | [Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin) | [Orion-14B-Chat-Plugin](https://modelscope.cn/models/OrionStarAI/Orion-14B-Chat-Plugin/summary) |\n| üíºOrion-14B-Base-Int4   | [Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4)     | [Orion-14B-Base-Int4](https://modelscope.cn/models/OrionStarAI/Orion-14B-Base-Int4/summary)     |\n| üì¶Orion-14B-Chat-Int4   | [Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4)     | [Orion-14B-Chat-Int4](https://modelscope.cn/models/OrionStarAI/Orion-14B-Chat-Int4/summary)     |\n\n<a name=\"model-benchmark\"></a><br>\n# 3. Model Benchmarks\n\n## 3.1. Base Model Orion-14B-Base Benchmarks\n### 3.1.1. LLM evaluation results on examination and professional knowledge\n| Model              | C-Eval   | CMMLU    | MMLU     | AGIEval  | Gaokao   | BBH      |\n|--------------------|----------|----------|----------|----------|----------|----------|\n| LLaMA2-13B         |   41.4   |   38.4   |   55.0   |   30.9   |   18.2   |   45.6   |\n| Skywork-13B        |   59.1   |   61.4   |   62.7   |   43.6   |   56.1   |   48.3   |\n| Baichuan2-13B      |   59.0   |   61.3   |   59.5   |   37.4   |   45.6   |   49.0   |\n| QWEN-14B           |   71.7   |   70.2   |   67.9   |   51.9   | **62.5** |   53.7   |\n| InternLM-20B       |   58.8   |   59.0   |   62.1   |   44.6   |   45.5   |   52.5   |\n| **Orion-14B-Base** | **72.9** | **70.6** | **69.9** | **54.7** |   62.1   | **56.5** |\n\n### 3.1.2. LLM evaluation results on language understanding and common knowledge\n| Model             |RACE-middle|RACE-high |HellaSwag | PIQA     | Lambada  | WSC      |\n|--------------------|----------|----------|----------|----------|----------|----------|\n| LLaMA 2-13B        |   63.0   |   58.9   |   77.5   |   79.8   |   76.5   |   66.3   |\n| Skywork-13B        |   87.6   |   84.1   |   73.7   |   78.3   |   71.8   |   66.3   |\n| Baichuan 2-13B     |   68.9   |   67.2   |   70.8   |   78.1   |   74.1   |   66.3   |\n| QWEN-14B           |   93.0   |   90.3   | **80.2** |   79.8   |   71.4   |   66.3   |\n| InternLM-20B       |   86.4   |   83.3   |   78.1   | **80.3** |   71.8   |   68.3   |\n| **Orion-14B-Base** | **93.2** | **91.3** |   78.5   |   79.5   | **78.8** | **70.2** |\n\n### 3.1.3. LLM evaluation results of OpenCompass testsets\n| Model | Average  | Examination | Language | Knowledge | Understanding | Reasoning |\n|------------------|----------|----------|----------|----------|----------|----------|\n| LLaMA 2-13B      |   47.3   |   45.2   |   47.0   |   58.3   |   50.9   |   43.6   |\n| Skywork-13B      |   53.6   |   61.1   |   51.3   |   52.7   |   64.5   |   45.2   |\n| Baichuan 2-13B   |   49.4   |   51.8   |   47.5   |   48.9   |   58.1   |   44.2   |\n| QWEN-14B         |   62.4   |   71.3   |   52.67  |   56.1   |   68.8   |   60.1   |\n| InternLM-20B     |   59.4   |   62.5   |   55.0   | **60.1** |   67.3   |   54.9   |\n|**Orion-14B-Base**| **64.3** | **71.4** | **55.0** |   60.0   | **71.9** | **61.6** |\n\n### 3.1.4. Comparison of LLM performances on Japanese testsets\n| Model             |**Average**|  JCQA    |  JNLI    |  MARC    |  JSQD    |  JQK     |  XLS     |  XWN     |  MGSM    |\n|--------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| PLaMo-13B          |   52.3   |   56.7   |   42.8   |   95.8   |   70.6   |   71.0   |   8.70   |   70.5   |   2.40   |\n| WebLab-10B         |   50.7   |   66.6   |   53.7   |   82.1   |   62.9   |   56.2   |   10.0   |   72.0   |   2.40   |\n| ELYZA-jp-7B        |   48.8   |   71.7   |   25.3   |   86.6   |   70.8   |   64.1   |   2.50   |   62.1   |   7.20   |\n| StableLM-jp-7B     |   51.1   |   33.4   |   43.3   | **96.7** |   70.6   |   78.1   |   10.7   |   72.8   |   2.80   |\n| LLaMA 2-13B        |   46.3   |   75.0   |   47.6   |   38.8   |   76.1   |   67.7   |   18.1   |   63.2   |   10.4   |\n| Baichuan 2-13B     |   57.1   |   73.7   |   31.3   |   91.6   |   80.5   |   63.3   |   18.6   |   72.2   |   25.2   |\n| QWEN-14B           |   65.8   |   85.9   |   60.7   |   97.0   |   83.3   |   71.8   |   18.8   |   70.6   |   38.0   |\n| Yi-34B             |   67.1   |   83.8   |   61.2   |   95.2   | **86.1** |   78.5   | **27.2** |   69.2   |   35.2   |\n| **Orion-14B-Base** | **69.1** | **88.2** | **75.8** |   94.1   |   75.7   | **85.1** |   17.3   | **78.8** | **38.0** |\n\n### 3.1.5. Comparison of LLM performances on Korean testsets. n = 0 and n = 5 stand for n-shot prompts used in the evaluation\n|Model      | **Average**<br>n=0&nbsp;&nbsp;n=5 | HellaSwag<br>n=0&nbsp;&nbsp;n=5 | COPA<br> n=0&nbsp;&nbsp;n=5 | BooIQ<br>n=0&nbsp;&nbsp;n=5 | SentiNeg<br>n=0&nbsp;&nbsp;n=5|\n|------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|\n| KoGPT            |  53.0   &nbsp;&nbsp;   70.1  |  55.9   &nbsp;&nbsp;   58.3  |  73.5   &nbsp;&nbsp;   72.9  |  45.1   &nbsp;&nbsp;   59.8  |  37.5   &nbsp;&nbsp;   89.4  |\n| Polyglot-ko-13B  |  69.6   &nbsp;&nbsp;   73.7  |**59.5** &nbsp;&nbsp; **63.1**|**79.4** &nbsp;&nbsp; **81.1**|  48.2   &nbsp;&nbsp;   60.4  |  91.2   &nbsp;&nbsp;   90.2  |\n| LLaMA 2-13B      |  46.7   &nbsp;&nbsp;   63.7  |  41.3   &nbsp;&nbsp;   44.0  |  59.3   &nbsp;&nbsp;   63.8  |  34.9   &nbsp;&nbsp;   73.8  |  51.5   &nbsp;&nbsp;   73.4  |\n| Baichuan 2-13B   |  52.1   &nbsp;&nbsp;   58.7  |  39.2   &nbsp;&nbsp;   39.6  |  60.6   &nbsp;&nbsp;   60.6  |  58.4   &nbsp;&nbsp;   61.5  |  50.3   &nbsp;&nbsp;   72.9  |\n| QWEN-14B         |  53.8   &nbsp;&nbsp;   73.7  |  45.3   &nbsp;&nbsp;   46.8  |  64.9   &nbsp;&nbsp;   68.9  |  33.4   &nbsp;&nbsp;   83.5  |  71.5   &nbsp;&nbsp;   95.7  |\n| Yi-34B           |  54.2   &nbsp;&nbsp;   72.1  |  44.6   &nbsp;&nbsp;   44.7  |  58.0   &nbsp;&nbsp;   60.6  |  65.9   &nbsp;&nbsp;   90.2  |  48.3   &nbsp;&nbsp;   92.9  |\n|**Orion-14B-Chat**|**74.5** &nbsp;&nbsp; **79.6**|  47.0   &nbsp;&nbsp;   49.6  |  77.7   &nbsp;&nbsp;   79.4  |**81.6** &nbsp;&nbsp; **90.7**|**92.4** &nbsp;&nbsp; **98.7**|\n\n### 3.1.6. Multilingual evaluation\n| Model              | Train Lang | Japanese | Korean   | Chinese  |  English |\n|--------------------|------------|----------|----------|----------|----------|\n| PLaMo-13B          |  En,Jp     |   52.3   |   *      |   *      |   *      |\n| Weblab-10B         |  En,Jp     |   50.7   |   *      |   *      |   *      |\n| ELYZA-jp-7B        |  En,Jp     |   48.8   |   *      |   *      |   *      |\n| StableLM-jp-7B     |  En,Jp     |   51.1   |   *      |   *      |   *      |\n| KoGPT-6B           |  En,Ko     |   *      |   70.1   |   *      |   *      |\n| Polyglot-ko-13B    |  En,Ko     |   *      |   70.7   |   *      |   *      |\n| Baichuan2-13B      |  Multi     |   57.1   |   58.7   |   50.8   |   57.1   |\n| Qwen-14B           |  Multi     |   65.8   |   73.7   |   64.5   |   65.4   |\n| Llama2-13B         |  Multi     |   46.3   |   63.7   |   41.4   |   55.3   |\n| Yi-34B             |  Multi     |   67.1   |   72.2   |   58.7   | **68.8** |\n| **Orion-14B-Chat** |  Multi     | **69.1** | **79.5** | **67.9** |   67.3   |\n\n\n## 3.2. Chat Model Orion-14B-Chat Benchmarks\n### 3.2.1. Chat model subjective evaluation of MTBench\n| Model        | First-Turn | Second-Turn | **Average** |\n|----------------------|----------|----------|----------|\n| Baichuan2-13B-Chat   |   7.05   |   6.47   |   6.76   |\n| Qwen-14B-Chat        |   7.30   |   6.62   |   6.96   |\n| Llama2-13B-Chat      |   7.10   |   6.20   |   6.65   |\n| InternLM-20B-Chat    |   7.03   |   5.93   |   6.48   |\n| **Orion-14B-Chat**   | **7.68** | **7.07** | **7.37** |\n\\* use vllm for inference\n\n### 3.2.2. Chat model subjective evaluation of AlignBench\n| Model              | Math.  |  Logi. | Basic. | Chi.   | Comp.  | Writ.  | Role.  | Prof.  |**Avg.**|\n|--------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n| Baichuan2-13B-Chat |  3.76  |  4.07  |  6.22  |  6.05  |  7.11  |  6.97  |  6.75  |  6.43  |  5.25  |\n| Qwen-14B-Chat      |**4.91**|**4.71**|**6.90**|  6.36  |  6.74  |  6.64  |  6.59  |  6.56  |**5.72**|\n| Llama2-13B-Chat    |  3.05  |  3.79  |  5.43  |  4.40  |  6.76  |  6.63  |  6.99  |  5.65  |  4.70  |\n| InternLM-20B-Chat  |  3.39  |  3.92  |  5.96  |  5.50  |**7.18**|  6.19  |  6.49  |  6.22  |  4.96  |\n| **Orion-14B-Chat** |  4.00  |  4.24  |  6.18  |**6.57**|  7.16  |**7.36**|**7.16**|**6.99**|  5.51  |\n\\* use vllm for inference\n\n## 3.3. LongChat Model Orion-14B-LongChat Benchmarks\n### 3.3.1. LongChat evaluation of LongBench\n| Model           | NarrativeQA|MultiFieldQA-en|MultiFieldQA-zh| DuReader  | QMSum     | VCSUM     | TREC      | TriviaQA  | LSHT      |RepoBench-P|\n|--------------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| GPT-3.5-Turbo-16k        | **23.60** | **52.30** | **61.20** |   28.70   |   23.40   | **16.00** |   68.00   | **91.40** |   29.20   |   53.60   |\n| LongChat-v1.5-7B-32k     |   16.90   |   41.40   |   29.10   |   19.50   |   22.70   |    9.90   |   63.50   |   82.30   |   23.20   |   55.30   |\n| Vicuna-v1.5-7B-16k       |   19.40   |   38.50   |   43.00   |   19.30   |   22.80   |   15.10   |   71.50   |   86.20   |   28.80   |   43.50   |\n| Yi-6B-200K               |   14.11   |   36.74   |   22.68   |   14.01   |   20.44   |    8.08   |   72.00   |   86.61   |   38.00   | **63.29** |\n| Orion-14B-LongChat       |   19.47   |   48.11   |   55.84   | **37.02** | **24.87** |   15.44   | **77.00** |   89.12   | **45.50** |   54.31   |\n\n\n## 3.4. Chat RAG Model Benchmarks\n### 3.4.1. LLM evaluation results of self-built RAG testsets\n|Model|Effectiveness of Response(Keyword)|*Effectiveness of ResponseÔºàsubjective evaluationÔºâ|Quoting Ability|Fallback Ability|*AutoQA|*Data Extraction|\n|---------------------|------|------|------|------|------|------|\n| Baichuan2-13B-Chat  |  85  |  76  |  1   |  0   |  69  |  51  |\n| Qwen-14B-Chat       |  79  |  77  |  75  |  47  |  68  |  72  |\n| Qwen-72B-Chat(Int4) |  87  |  89  |  90  |  32  |  67  |  76  |\n| GPT-4               |  91  |  94  |  96  |  95  |  75  |  86  |\n| Orion-14B-Chat-RAG  |  86  |  87  |  91  |  97  |  73  |  71  |\n \\* means manual assessment\n\n## 3.5. Chat Plugin Model Orion-14B-Chat-Plugin Benchmarks\n### 3.5.1. LLM evaluation results of self-built plugin testsets\n|Model |Intent Recognition with Full Params |Intent Recognition with Missing Params |Non-Plugin Invocation Recognition |\n|-----------------------|--------|-----------|--------|\n| Baichuan2-13B-Chat    |   25   |   0       |   0    |\n| Qwen-14B-Chat         |   55   |   0       |   50   |\n| GPT-4                 | **95** |   52.38   |   70   |\n| Orion-14B-Chat-Plugin |  92.5  | **60.32** | **90** |\n\n## 3.6. Quantized Model Orion-14B-Base-Int4 Benchmarks\n### 3.6.1. Comparison of before and after quantization\n|Model |Size(GB)|Inference Speed(tokens/s)|C-Eval|CMMLU|MMLU|RACE|HellaSwag|\n|-------------------------|-------|-----|------|------|------|------|------|\n| OrionStar-14B-Base      |  28.0 | 135 | 72.8 | 70.6 | 70.0 | 93.3 | 78.5 |\n| OrionStar-14B-Base-Int4 |  8.3  | 178 | 71.8 | 69.8 | 69.2 | 93.1 | 78.0 |\n\n\n<a name=\"model-inference\"></a><br>\n# 4. Model Inference\n\nModel weights, source code, and configuration needed for inference are published on Hugging Face, and the download link\nis available in the table at the beginning of this document. We demonstrate various inference methods here, and the\nprogram will automatically download the necessary resources from Hugging Face.\n\n## 4.1. Python Code\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B\", device_map=\"auto\",\n                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n\nmodel.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B\")\nmessages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\nresponse = model.chat(tokenizer, messages, streaming=False)\nprint(response)\n\n```\n\nIn the above Python code, the model is loaded with `device_map='auto'` to utilize all available GPUs. To specify the\ndevice, you can use something like `export CUDA_VISIBLE_DEVICES=0,1` (using GPUs 0 and 1).\n\n## 4.2. Command Line Tool\n\n```shell\nCUDA_VISIBLE_DEVICES=0 python cli_demo.py\n```\n\nThis command-line tool is designed for chat scenarios, and thus, it does not support calling the base model.\n\n## 4.3. Direct Script Inference\n\n```shell\n\n# base model\nCUDA_VISIBLE_DEVICES=0 python demo/text_generation_base.py --model OrionStarAI/Orion-14B --tokenizer OrionStarAI/Orion-14B --prompt hello\n\n# chat model\nCUDA_VISIBLE_DEVICES=0 python demo/text_generation.py --model OrionStarAI/Orion-14B-Chat --tokenizer OrionStarAI/Orion-14B-Chat --prompt hi\n\n```\n<a name=\"vllm\"></a><br>\n## 4.4. Inference by vllm\n\n- Project URL<br>\n  https://github.com/vllm-project/vllm\n\n- Pull Request<br>\n  https://github.com/vllm-project/vllm/pull/2539\n\n<a name=\"llama-cpp\"></a><br>\n## 4.5. Inference by llama.cpp\n\n- Project URL<br>\n  https://github.com/ggerganov/llama.cpp\n\n- Pull Request<br>\n  https://github.com/ggerganov/llama.cpp/pull/5118\n\n- How to convert to GGUF model\n\n  ```shell\n  python convert-hf-to-gguf.py path/to/Orion-14B-Chat --outfile chat.gguf\n  ```\n\n- How to run generation\n\n  ```shell\n  ./main --frequency-penalty 0.5 --frequency-penalty 0.5 --top-k 5 --top-p 0.9 -m chat.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e\n  ```\n\n## 4.6 Example Output\n\n### 4.6.1. Casual Chat\n\n`````\nUser: Hello\nOrion-14B: Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\nUser: What functions do you have?\nOrion-14B: As an AI language model, I have a wide range of functions and capabilities, including but not limited to:\n  Answering general knowledge questions: I can provide information on various topics such as history, science, geography, and more.\n  Providing definitions and explanations: I can explain complex concepts and provide definitions for words and phrases.\n  Assisting with tasks: I can help with simple tasks like setting reminders, converting units of measurement, and even ordering food from restaurants.\n  Generating text: I can write short stories, essays, articles, and other types of content based on prompts given to me.\n  Translating languages: I can translate text from one language to another.\n  Chatting: I can engage in conversation with users and respond to their queries in a conversational manner.\n  Summarizing long texts: I can summarize longer texts like articles, books, and research papers.\n  Generating random numbers and names: I can generate random numbers within a specified range and also suggest names based on certain criteria.\n  Playing games: I can play simple games like Hangman, Trivia, and more.\n  Providing recommendations: I can recommend movies, music, books, and other forms of entertainment based on user preferences.\n  These are just some of the many functions that I am capable of performing. If you have a specific task in mind, feel free to ask me about it.\nUser: Tell me a joke.\nOrion-14B: Sure, here's a classic one-liner: Why don't scientists trust atoms? Because they make up everything.\n`````\n\n### 4.6.2. Japanese & Korean Chat\n\n`````\nUserÔºöËá™Â∑±„ÇíÁ¥π‰ªã„Åó„Å¶„Åè„Å†„Åï„ÅÑ\nOrion-14BÔºö„Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÂêçÂâç„ÅØChatMax„Åß„ÄÅOrionStar„Å´„Çà„Å£„Å¶ÈñãÁô∫„Åï„Çå„ÅüAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇÁßÅ„ÅØ„É¶„Éº„Ç∂„Éº„Å´Ê≠£Á¢∫„Åß„ÄÅÂÆ¢Ë¶≥ÁöÑ„Åß„ÄÅÊúâÁî®„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„ÄÅÂΩº„Çâ„ÅåÂïèÈ°å„ÇíËß£Ê±∫„Åó„ÄÅÊÉÖÂ†±„ÇíÂèñÂæó„Åô„Çã„ÅÆ„ÇíÂä©„Åë„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\nUserÔºöÏûêÍ∏∞ÏÜåÍ∞úÎ•º Ìï¥Ï£ºÏÑ∏Ïöî.\nOrion-14BÔºöÏïàÎÖïÌïòÏÑ∏Ïöî, Ï†ú Ïù¥Î¶ÑÏùÄ ChatMaxÏûÖÎãàÎã§. OrionStarÏóêÏÑú Í∞úÎ∞úÌïú AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. Ï†ÄÎäî ÏÇ¨Ïö©ÏûêÎì§ÏóêÍ≤å Ï†ïÌôïÌïòÍ≥† Í∞ùÍ¥ÄÏ†ÅÏù¥Î©∞ Ïú†Ïö©Ìïú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÏó¨ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ≥† Ï†ïÎ≥¥Î•º ÏñªÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï§Ñ Ïàò ÏûàÏäµÎãàÎã§.\n`````\n\n<a name=\"declarations-license\"></a><br>\n# 5. Declarations, License\n\n## 5.1. Declarations\n\nWe strongly urge all users not to use the Orion-14B model for any activities that may harm national or social security or violate the law.\nAdditionally, we request users not to use the Orion-14B model for internet services without proper security review and filing.\nWe hope all users abide by this principle to ensure that technological development takes place in a regulated and legal environment.\nWe have done our best to ensure the compliance of the data used in the model training process. However, despite our\nsignificant efforts, unforeseen issues may still arise due to the complexity of the model and data. Therefore, if any\nproblems arise due to the use of the Orion-14B open-source model, including but not limited to data security\nissues, public opinion risks, or any risks and issues arising from the model being misled, abused, disseminated, or\nimproperly utilized, we will not assume any responsibility.\n\n## 5.2. License\n\nCommunity use of the Orion-14B series models\n- For code, please comply with  [Apache License Version 2.0](./LICENSE)<br>\n- For model, please comply with [„ÄêOrion-14B Series„Äë Models Community License Agreement](./ModelsCommunityLicenseAgreement)\n\n\n<a name=\"company-introduction\"></a><br>\n# 6. Company Introduction\n\nOrionStar is a leading global service robot solutions company, founded in September 2016. OrionStar is dedicated to\nusing artificial intelligence technology to create the next generation of revolutionary robots, allowing people to break\nfree from repetitive physical labor and making human work and life more intelligent and enjoyable. Through technology,\nOrionStar aims to make society and the world a better place.\n\nOrionStar possesses fully self-developed end-to-end artificial intelligence technologies, such as voice interaction and\nvisual navigation. It integrates product development capabilities and technological application capabilities. Based on\nthe Orion robotic arm platform, it has launched products such as OrionStar AI Robot Greeting, AI Robot Greeting Mini,\nLucki, Coffee Master, and established the open platform OrionOS for Orion robots. Following the philosophy of \"Born for\nTruly Useful Robots\", OrionStar empowers more people through AI technology.\n\n**The core strengths of OrionStar lies in possessing end-to-end AI application capabilities,** including big data preprocessing, large model pretraining, fine-tuning, prompt engineering, agent, etc.  With comprehensive end-to-end model training capabilities, including systematic data processing workflows and the parallel model training capability of hundreds of GPUs, it has been successfully applied in various industry scenarios such as government affairs, cloud services, international e-commerce, and fast-moving consumer goods.\n\nCompanies with demands for deploying large-scale model applications are welcome to contact us.<br>\n**Enquiry Hotline: 400-898-7779**<br>\n**E-mail: ai@orionstar.com**<br>\n**Discord Link: https://discord.gg/zumjDWgdAs**\n\n<div align=\"center\">\n  <img src=\"./assets/imgs/wechat_group.jpg\" alt=\"wechat\" width=\"40%\" />\n</div>\n\n\n\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/OrionStarAI/Orion-14B-Chat",
          "files": [],
          "modelId": "OrionStarAI/Orion-14B-Chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/OrionStarAI/Orion-14B-Chat",
          "files": [],
          "modelId": "OrionStarAI/Orion-14B-Chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/OrionStarAI/Orion-14B-Chat",
          "files": [],
          "modelId": "OrionStarAI/Orion-14B-Chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/OrionStarAI/Orion-14B-Chat",
          "files": [],
          "modelId": "OrionStarAI/Orion-14B-Chat"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/OrionStarAI/Orion-14B-Chat",
          "files": [],
          "modelId": "OrionStarAI/Orion-14B-Chat"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "PokeeAI/pokee_research_7b",
      "name": "pokee_research_7b",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "agent",
        "deepresearch",
        "llm",
        "rl",
        "reinforcementlearning",
        "conversational",
        "en",
        "dataset:miromind-ai/MiroRL-GenQA",
        "arxiv:2510.15862",
        "base_model:Qwen/Qwen2.5-7B-Instruct",
        "base_model:finetune:Qwen/Qwen2.5-7B-Instruct",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "likes": 1485,
      "downloads": 122565,
      "lastModifiedTimestamp": null,
      "readme": "---\nbase_model:\n- Qwen/Qwen2.5-7B-Instruct\ndatasets:\n- miromind-ai/MiroRL-GenQA\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- agent\n- deepresearch\n- llm\n- rl\n- reinforcementlearning\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n\n# Model Card for PokeeResearch\n\n## Model Details\n\n### Model Description\n\n**PokeeResearch-7B** is a **7-billion-parameter deep research agent** developed by **Pokee AI** to advance reliable, aligned, and scalable research-grade reasoning in tool-augmented LLMs.  \nThe model integrates **Reinforcement Learning from AI Feedback (RLAIF)** with a **robust reasoning scaffold**, enabling it to conduct complex, multi-step research workflows that include self-correction, verification, and synthesis across multiple independent research threads.\n\n- **Developed by:** Pokee AI\n- **Model type:** Tool-augmented large language model (LLM) research agent  \n- **Language(s):** English, Chinese and many more\n- **License:** Apache 2.0  \n- **Finetuned from model:** Qwen2.5-7B-Instruct\n\n### Model Sources\n\n- **Repository:** [https://github.com/Pokee-AI/PokeeResearchOSS](https://github.com/Pokee-AI/PokeeResearchOSS)  \n- **Paper:** [*PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold*](https://arxiv.org/pdf/2510.15862), Pokee AI, October 2025\n- **Project Page:** [https://pokee.ai/deepresearch-preview](https://pokee.ai/deepresearch-preview)\n\n---\n\n## Uses\n\n### Direct Use\nPokeeResearch-7B is designed for **deep research automation**, where the model autonomously:\n- Decomposes complex user queries  \n- Retrieves and reads from external sources  \n- Synthesizes factual, verifiable, and grounded answers  \n\nIt can be used as a **standalone research assistant** or integrated into **multi-agent systems** to support academic, enterprise, or product-level research tasks.\n\n### Downstream Use\nPokeeResearch-7B can be **fine-tuned** or **extended** for:\n- Domain-specific scientific discovery  \n- Autonomous document retrieval and synthesis  \n- Multi-source verification and summarization pipelines  \n- Integration into reinforcement learning research agents (RLHF/RLAIF frameworks)\n\n### Out-of-Scope Use\nThe model should **not** be used for:\n- Generating unverified or speculative claims  \n- Automated decision-making in high-stakes domains (medical, legal, or financial)  \n- Applications requiring strict factual precision without external verification  \n- Generating content without citation or evidence tracing  \n\n---\n\n## Bias, Risks, and Limitations\n\nPokeeResearch-7B is optimized for factual grounding and robustness, but limitations include:\n- Dependence on **external data quality** and **retrieval accuracy**  \n- Potential **semantic bias** introduced by AI-based feedback signals  \n- Limited coverage for **non-English** or **multi-modal** reasoning tasks  \n- Risk of **hallucinated synthesis** when sources conflict or lack clarity  \n\n### Recommendations\nUsers should:\n- Cross-verify answers, especially in multi-hop reasoning cases  \n- Monitor output for citation accuracy and alignment with source data  \n- Refrain from using outputs as sole evidence in decision-critical contexts  \n\n---\n\n## How to Get Started with the Model\nplease refer to the following codebase for how to use PokeeResearch-7B\nhttps://github.com/Pokee-AI/PokeeResearchOSS/blob/main/README.md\n\n---\n\n## Training Details\n\n### Training Data\n- **Dataset:** MiroRL-GenQA dataset (MiroMind AI, 2025)  \n- **Data characteristics:** Complex, multi-turn question‚Äìanswer pairs requiring multi-step reasoning  \n- **Data filtering:** No benchmark data used for testing; the model was trained only on open-domain text Q&A samples  \n\n### Training Procedure\n\n#### Preprocessing\n- Normalization and tokenization aligned with Qwen2.5 tokenizer  \n- Structured prompt‚Äìresponse pairs in research/verification format (`<tool_call>`, `<answer>`, `<verification>`)\n\n#### Training Hyperparameters\n- **Algorithm:** RLOO (REINFORCE Leave-One-Out)  \n- **Batch size:** 64  \n- **Research threads per prompt:** 8  \n- **Learning rate:** 3e-6  \n- **Context limit:** 32,768 tokens  \n- **Steps:** 140 fine-tuning iterations  \n- **Regularization:** None (no entropy or KL regularization)  \n- **Precision regime:** bf16 mixed precision  \n\n#### Reward Design\n- Combined reward signal from:\n  - **AI feedback** (semantic equivalence via external LLM judge)  \n  - **Format adherence reward** (ensures correct agent behavior)  \n\n#### Speeds, Sizes, Times\n- **Model size:** 7 billion parameters  \n- **Training duration:** ~5 days on 8 √ó A100 80G GPUs  \n- **Checkpoint size:** ~13 GB  \n\n---\n\n## Evaluation\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n10 open-domain research and QA benchmarks:\n- NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle, GAIA, BrowseComp, Humanity‚Äôs Last Exam\n\n#### Factors\n- Benchmarks differ by reasoning depth, retrieval dependence, and factual precision requirements.  \n- Evaluations disaggregate by dataset difficulty and task type (single-hop vs multi-hop).  \n\n#### Metrics\n- Mean accuracy (mean@4 across independent research threads) based on \n\n### Results\n\n**PokeeResearch-7B (RTS variant)** and **PokeeResearch-7B** outperforms all baselines at 7B scale across 10 benchmarks.  \nHighlights (mean@4 accuracy):  \n| **Method** | **HLE** | **GAIA** | **BrowseComp** | **BAMB** | **2WIKI** | **TQ** | **NQ** | **POPQA** | **MUSIQUE** | **HOTPOTQA** |\n|-------------|----------|-----------|----------------|-----------|-----------|----------|----------|-------------|---------------|----------------|\n| R1searcher | 5.4 | 8.3 | 1.0 | 63.2 | 61.4 | 77.2 | 59.6 | 51.8 | 35.8 | 62.4 |\n| SearchR1 | 13.0 | 18.7 | 0.4 | 67.8 | 62.8 | 81.0 | 67.6 | 59.6 | 33.2 | 63.2 |\n| ZeroSearch | 8.6 | 9.9 | 1.4 | 51.4 | 33.6 | 61.6 | 48.2 | 38.0 | 19.0 | 32.4 |\n| ASearcher | 13.8 | 22.1 | 3.2 | 68.8 | 69.2 | 85.2 | 71.2 | 58.2 | 35.8 | 71.0 |\n| DeepResearcher | 6.0 | 24.03 | 1.8 | 71.0 | 58.8 | 82.2 | 60.2 | 55.2 | 26.8 | 56.6 |\n| **PR** | **15.2** | **36.9** | **5.4** | **74.5** | **74.0** | **91.3** | **75.1** | **59.8** | **39.8** | **71.2** |\n| **PR+** | **17.6** | **41.3** | **8.4** | **75.0** | **75.0** | **91.8** | **75.0** | **60.0** | **41.4** | **71.6** |\n\n#### Summary\nPokeeResearch-7B variants achieves **state-of-the-art performance among 7B-scale open deep research agents**, validating RLAIF and reasoning scaffold design for robust, verifiable research workflows.\n\n---\n\n## Technical Specifications\n\n### Model Architecture and Objective\n- **Base Architecture:** Transformer decoder (Qwen2.5-7B-Instruct backbone)  \n- **Objective:** Reinforcement learning with AI feedback to maximize semantic correctness and alignment with human-style reasoning  \n\n### Compute Infrastructure\n#### Hardware\n- NVIDIA A100 80GB GPUs √ó8 for training and x1 for inference\n---\n\n## Citation\n\n**BibTeX:**\n```bibtex\n@article{pokee2025deepresearch,\n  title={PokeeResearch: Effective Deep Research via\n          Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold},\n  author={Yi Wan* and Jiuqi Wang* and Liam Li\n          and Jinsong Liu and Ruihao Zhu and Zheqing Zhu},\n  journal={Pokee AI Technical Report},\n  year={2025},\n  url={https://arxiv.org/pdf/2510.15862}\n}\n```\n\n**APA:**\nWan, Y., Wang, J., Li, L., Liu, J., Zhu, R., & Zhu, Z. (2025). *PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold.* Pokee AI.\n\n---\n\n## Glossary\n\n- **RLAIF:** Reinforcement Learning from AI Feedback ‚Äì optimization using LLM-based reward signals.  \n- **RLOO:** REINFORCE Leave-One-Out ‚Äì unbiased policy gradient variant for on-policy learning.  \n- **RTS:** Research Threads Synthesis ‚Äì synthesis of multiple independent reasoning threads at inference time.  \n\n---\n\n## More Information\nFor technical details, visit: [https://github.com/Pokee-AI/PokeeResearchOSS](https://github.com/Pokee-AI/PokeeResearchOSS)  \nFor inquiries, contact: hello@pokee.ai  \n\n---\n\n## Model Card Authors\n**Yi Wan**, **Jiuqi Wang**, Liam Li, Jinsong Liu, Ruihao Zhu, and Zheqing Zhu ‚Äî Pokee AI Research Team  \n\n## Model Card Contact\nPokee AI Team ‚Äî hello@pokee.ai",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/PokeeAI/pokee_research_7b",
          "files": [],
          "modelId": "PokeeAI/pokee_research_7b"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/PokeeAI/pokee_research_7b",
          "files": [],
          "modelId": "PokeeAI/pokee_research_7b"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/PokeeAI/pokee_research_7b",
          "files": [],
          "modelId": "PokeeAI/pokee_research_7b"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/PokeeAI/pokee_research_7b",
          "files": [],
          "modelId": "PokeeAI/pokee_research_7b"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/PokeeAI/pokee_research_7b",
          "files": [],
          "modelId": "PokeeAI/pokee_research_7b"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "github-crewAIInc-crewAI",
      "name": "crewAI",
      "author": "crewAIInc",
      "description": "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "aiagentframework",
        "llms"
      ],
      "likes": 121780,
      "downloads": 121780,
      "lastModified": "2025-11-20T02:15:41Z",
      "lastModifiedTimestamp": 1763604941000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/crewAIInc/crewAI",
          "homepage": "https://crewai.com",
          "language": "Python",
          "forks": 5416,
          "open_issues": 187,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/170677839?v=4",
      "velocity": 44638,
      "is_rising_star": true
    },
    {
      "id": "github-ray-project-ray",
      "name": "ray",
      "author": "ray-project",
      "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
      "task": "tool",
      "tags": [
        "data-science",
        "deep-learning",
        "deployment",
        "distributed",
        "hyperparameter-optimization",
        "hyperparameter-search",
        "large-language-models",
        "llm",
        "llm-inference",
        "llm-serving",
        "machine-learning",
        "optimization",
        "parallel",
        "python",
        "pytorch",
        "ray",
        "reinforcement-learning",
        "rllib",
        "serving",
        "tensorflow"
      ],
      "likes": 119736,
      "downloads": 119736,
      "lastModified": "2025-11-20T03:26:50Z",
      "lastModifiedTimestamp": 1763609210000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ray-project/ray",
          "homepage": "https://ray.io",
          "language": "Python",
          "forks": 6913,
          "open_issues": 3270,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/22125274?v=4",
      "velocity": 43892.2,
      "is_rising_star": true
    },
    {
      "id": "github-zhayujie-chatgpt-on-wechat",
      "name": "chatgpt-on-wechat",
      "author": "zhayujie",
      "description": "Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©ChatGPT/Claude/DeepSeek/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agent",
        "chatgpt",
        "claude-4",
        "deepseek",
        "dingtalk",
        "feishu-bot",
        "gemini",
        "gpt-4",
        "kimi",
        "linkai",
        "llm",
        "mcp",
        "multi-agent",
        "openai",
        "python3",
        "qwen",
        "rag",
        "wechat",
        "wechat-bot"
      ],
      "likes": 119263,
      "downloads": 119263,
      "lastModified": "2025-11-20T03:15:18Z",
      "lastModifiedTimestamp": 1763608518000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/zhayujie/chatgpt-on-wechat",
          "homepage": "https://link-ai.tech",
          "language": "Python",
          "forks": 9499,
          "open_issues": 354,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/26161723?v=4",
      "velocity": 43723.9,
      "is_rising_star": true
    },
    {
      "id": "github-milvus-io-milvus",
      "name": "milvus",
      "author": "milvus-io",
      "description": "Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search",
      "task": "tool",
      "tags": [
        "anns",
        "cloud-native",
        "diskann",
        "distributed",
        "embedding-database",
        "embedding-similarity",
        "embedding-store",
        "faiss",
        "golang",
        "hnsw",
        "image-search",
        "llm",
        "nearest-neighbor-search",
        "rag",
        "vector-database",
        "vector-search",
        "vector-similarity",
        "vector-store"
      ],
      "likes": 118669,
      "downloads": 118669,
      "lastModified": "2025-11-20T03:59:12Z",
      "lastModifiedTimestamp": 1763611152000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/milvus-io/milvus",
          "homepage": "https://milvus.io",
          "language": "Go",
          "forks": 3572,
          "open_issues": 889,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/51735404?v=4",
      "velocity": 43424.7,
      "is_rising_star": true
    },
    {
      "id": "github-janhq-jan",
      "name": "jan",
      "author": "janhq",
      "description": "Jan is an open source alternative to ChatGPT that runs 100% offline on your computer.",
      "task": "tool",
      "tags": [
        "chatgpt",
        "gpt",
        "llamacpp",
        "llm",
        "localai",
        "open-source",
        "self-hosted",
        "tauri"
      ],
      "likes": 118089,
      "downloads": 118089,
      "lastModified": "2025-11-20T03:31:03Z",
      "lastModifiedTimestamp": 1763609463000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/janhq/jan",
          "homepage": "https://jan.ai/",
          "language": "TypeScript",
          "forks": 2394,
          "open_issues": 190,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102363196?v=4",
      "velocity": 43288.3,
      "is_rising_star": true
    },
    {
      "id": "byroneverson/Mistral-Small-Instruct-2409-abliterated",
      "name": "Mistral-Small-Instruct-2409-abliterated",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "safetensors",
        "mistral",
        "text-generation",
        "llm",
        "chat",
        "instruct",
        "it",
        "abliterated",
        "conversational",
        "en",
        "base_model:mistralai/Mistral-Small-Instruct-2409",
        "base_model:finetune:mistralai/Mistral-Small-Instruct-2409",
        "license:other",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us",
        "deploy:azure"
      ],
      "likes": 210,
      "downloads": 117930,
      "lastModifiedTimestamp": null,
      "readme": "---\nbase_model: mistralai/Mistral-Small-Instruct-2409\nlicense: other\nlicense_name: mrl\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\npipeline_tag: text-generation\nlanguage:\n- en\ntags:\n- llm\n- mistral\n- chat\n- instruct\n- it\n- abliterated\nlibrary_name: transformers\n---\n\n\n\n# Mistral-Small-Instruct-2409-abliterated\n\n## Now accepting abliteration requests. If you would like to see a model abliterated, follow me and leave me a message with model link.\n\nCheck out the <a href=\"https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated/blob/main/abliterate-mistral-small-instruct-2409.ipynb\">jupyter notebook</a> for details of how this model was abliterated.\n\n![Logo](https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated/resolve/main/logo.png \"Logo\")\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated",
          "files": [],
          "modelId": "byroneverson/Mistral-Small-Instruct-2409-abliterated"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated",
          "files": [],
          "modelId": "byroneverson/Mistral-Small-Instruct-2409-abliterated"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated",
          "files": [],
          "modelId": "byroneverson/Mistral-Small-Instruct-2409-abliterated"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated",
          "files": [],
          "modelId": "byroneverson/Mistral-Small-Instruct-2409-abliterated"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated",
          "files": [],
          "modelId": "byroneverson/Mistral-Small-Instruct-2409-abliterated"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "github-mudler-LocalAI",
      "name": "LocalAI",
      "author": "mudler",
      "description": ":robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI,  running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "audio-generation",
        "decentralized",
        "distributed",
        "gemma",
        "image-generation",
        "libp2p",
        "llama",
        "llm",
        "mamba",
        "mcp",
        "mistral",
        "musicgen",
        "object-detection",
        "rerank",
        "rwkv",
        "stable-diffusion",
        "text-generation",
        "tts"
      ],
      "likes": 116477,
      "downloads": 116477,
      "lastModified": "2025-11-20T03:43:49Z",
      "lastModifiedTimestamp": 1763610229000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mudler/LocalAI",
          "homepage": "https://localai.io",
          "language": "Go",
          "forks": 3077,
          "open_issues": 250,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/2420543?v=4",
      "velocity": 42665.7,
      "is_rising_star": true
    },
    {
      "id": "github-QuivrHQ-quivr",
      "name": "quivr",
      "author": "QuivrHQ",
      "description": "Opiniated RAG for integrating GenAI in your apps üß†   Focus on your product rather than the RAG. Easy integration in existing products with customisation!  Any LLM: GPT4, Groq, Llama. Any Vectorstore: PGVector, Faiss. Any Files. Anyway you want. ",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "chatbot",
        "chatgpt",
        "database",
        "docker",
        "framework",
        "frontend",
        "groq",
        "html",
        "javascript",
        "llm",
        "openai",
        "postgresql",
        "privacy",
        "rag",
        "react",
        "security",
        "typescript",
        "vector"
      ],
      "likes": 115876,
      "downloads": 115876,
      "lastModified": "2025-11-19T22:05:27Z",
      "lastModifiedTimestamp": 1763589927000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/QuivrHQ/quivr",
          "homepage": "https://core.quivr.com",
          "language": "Python",
          "forks": 3689,
          "open_issues": 16,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/159330290?v=4",
      "velocity": 42486.4,
      "is_rising_star": true
    },
    {
      "id": "github-2noise-ChatTTS",
      "name": "ChatTTS",
      "author": "2noise",
      "description": "A generative speech model for daily dialogue.",
      "task": "tool",
      "tags": [
        "agent",
        "chat",
        "chatgpt",
        "chattts",
        "chinese",
        "chinese-language",
        "english",
        "english-language",
        "gpt",
        "llm",
        "llm-agent",
        "natural-language-inference",
        "python",
        "text-to-speech",
        "torch",
        "torchaudio",
        "tts"
      ],
      "likes": 114529,
      "downloads": 114529,
      "lastModified": "2025-11-19T23:29:58Z",
      "lastModifiedTimestamp": 1763594998000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/2noise/ChatTTS",
          "homepage": "https://2noise.com",
          "language": "Python",
          "forks": 4146,
          "open_issues": 67,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/164844019?v=4",
      "velocity": 41992.5,
      "is_rising_star": true
    },
    {
      "id": "github-upstash-context7",
      "name": "context7",
      "author": "upstash",
      "description": "Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors",
      "task": "tool",
      "tags": [
        "llm",
        "mcp",
        "mcp-server",
        "vibe-coding"
      ],
      "likes": 112502,
      "downloads": 112502,
      "lastModified": "2025-11-20T03:47:16Z",
      "lastModifiedTimestamp": 1763610436000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/upstash/context7",
          "homepage": "https://context7.com",
          "language": "JavaScript",
          "forks": 1852,
          "open_issues": 100,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/74989412?v=4",
      "velocity": 41188.4,
      "is_rising_star": true
    },
    {
      "id": "github-chatboxai-chatbox",
      "name": "chatbox",
      "author": "chatboxai",
      "description": "User-friendly Desktop Client App for AI Models/LLMs (GPT, Claude, Gemini, Ollama...)",
      "task": "tool",
      "tags": [
        "assistant",
        "chatbot",
        "chatgpt",
        "claude",
        "copilot",
        "deepseek",
        "gemini",
        "gpt",
        "gpt-5",
        "ollama",
        "openai"
      ],
      "likes": 112397,
      "downloads": 112397,
      "lastModified": "2025-11-20T02:37:08Z",
      "lastModifiedTimestamp": 1763606228000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chatboxai/chatbox",
          "homepage": "https://chatboxai.app?utm_medium=github",
          "language": "TypeScript",
          "forks": 3786,
          "open_issues": 966,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/199570308?v=4",
      "velocity": 41207.1,
      "is_rising_star": true
    },
    {
      "id": "github-ToolJet-ToolJet",
      "name": "ToolJet",
      "author": "ToolJet",
      "description": "ToolJet is the open-source foundation of ToolJet AI - the AI-native platform for building internal tools, dashboard, business applications, workflows and AI agents üöÄ",
      "task": "tool",
      "tags": [
        "ai-app-builder",
        "docker",
        "hacktoberfest",
        "internal-applications",
        "internal-project",
        "internal-tool",
        "internal-tools",
        "javascript",
        "kubernetes",
        "low-code",
        "low-code-development-platform",
        "low-code-framework",
        "no-code",
        "nodejs",
        "reactjs",
        "self-hosted",
        "typescript",
        "web-development-tools",
        "workflow-automation"
      ],
      "likes": 110776,
      "downloads": 110776,
      "lastModified": "2025-11-19T23:34:57Z",
      "lastModifiedTimestamp": 1763595297000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ToolJet/ToolJet",
          "homepage": "https://tooljet.ai",
          "language": "JavaScript",
          "forks": 4877,
          "open_issues": 950,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/82193554?v=4",
      "velocity": 40612,
      "is_rising_star": true
    },
    {
      "id": "github-alibaba-arthas",
      "name": "arthas",
      "author": "alibaba",
      "description": "Alibaba Java Diagnostic Tool Arthas/Alibaba JavaËØäÊñ≠Âà©Âô®Arthas",
      "task": "tool",
      "tags": [
        "agent",
        "alibaba",
        "arthas",
        "classloader",
        "diagnosis",
        "java",
        "jvm",
        "trace",
        "trouble-shooting"
      ],
      "likes": 110550,
      "downloads": 110550,
      "lastModified": "2025-11-19T21:23:19Z",
      "lastModifiedTimestamp": 1763587399000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/alibaba/arthas",
          "homepage": "https://arthas.aliyun.com/",
          "language": "Java",
          "forks": 7602,
          "open_issues": 455,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1961952?v=4",
      "velocity": 40532.8,
      "is_rising_star": true
    },
    {
      "id": "github-chatchat-space-Langchain-Chatchat",
      "name": "Langchain-Chatchat",
      "author": "chatchat-space",
      "description": "Langchain-ChatchatÔºàÂéüLangchain-ChatGLMÔºâÂü∫‰∫é Langchain ‰∏é ChatGLM, Qwen ‰∏é Llama Á≠âËØ≠Ë®ÄÊ®°ÂûãÁöÑ RAG ‰∏é Agent Â∫îÁî® | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM, Qwen and Llama) RAG and Agent app with langchain ",
      "task": "tool",
      "tags": [
        "chatbot",
        "chatchat",
        "chatglm",
        "chatgpt",
        "embedding",
        "faiss",
        "fastchat",
        "gpt",
        "knowledge-base",
        "langchain",
        "langchain-chatglm",
        "llama",
        "llm",
        "milvus",
        "ollama",
        "qwen",
        "rag",
        "retrieval-augmented-generation",
        "streamlit",
        "xinference"
      ],
      "likes": 109780,
      "downloads": 109780,
      "lastModified": "2025-11-20T03:27:10Z",
      "lastModifiedTimestamp": 1763609230000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chatchat-space/Langchain-Chatchat",
          "homepage": "",
          "language": "Python",
          "forks": 6067,
          "open_issues": 28,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/139558948?v=4",
      "velocity": 40249,
      "is_rising_star": true
    },
    {
      "id": "github-CherryHQ-cherry-studio",
      "name": "cherry-studio",
      "author": "CherryHQ",
      "description": "üçí Cherry Studio is a desktop client that supports for multiple LLM providers.",
      "task": "tool",
      "tags": [
        "agent",
        "anthropic",
        "assistant",
        "chatbot",
        "chatbotai",
        "electron",
        "llm",
        "mcp-client",
        "openai"
      ],
      "likes": 106775,
      "downloads": 106775,
      "lastModified": "2025-11-20T03:54:49Z",
      "lastModifiedTimestamp": 1763610889000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CherryHQ/cherry-studio",
          "homepage": "https://cherry-ai.com",
          "language": "TypeScript",
          "forks": 3227,
          "open_issues": 532,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/187777663?v=4",
      "velocity": 39125.9,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-LLM101n",
      "name": "LLM101n",
      "author": "karpathy",
      "description": "LLM101n: Let's build a Storyteller",
      "task": "tool",
      "tags": [],
      "likes": 106769,
      "downloads": 106769,
      "lastModified": "2025-11-20T03:50:12Z",
      "lastModifiedTimestamp": 1763610612000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/LLM101n",
          "homepage": "",
          "language": null,
          "forks": 1937,
          "open_issues": 19,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 39143.5,
      "is_rising_star": true
    },
    {
      "id": "github-agno-agi-agno",
      "name": "agno",
      "author": "agno-agi",
      "description": "Multi-agent framework, runtime and control plane. Built for speed, privacy, and scale.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "developer-tools",
        "python"
      ],
      "likes": 106117,
      "downloads": 106117,
      "lastModified": "2025-11-20T03:42:58Z",
      "lastModifiedTimestamp": 1763610178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/agno-agi/agno",
          "homepage": "https://docs.agno.com",
          "language": "Python",
          "forks": 4641,
          "open_issues": 298,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/104874993?v=4",
      "velocity": 38883.9,
      "is_rising_star": true
    },
    {
      "id": "github-Alibaba-NLP-DeepResearch",
      "name": "DeepResearch",
      "author": "Alibaba-NLP",
      "description": "Tongyi Deep Research, the Leading Open-source Deep Research Agent",
      "task": "tool",
      "tags": [
        "agent",
        "alibaba",
        "artificial-intelligence",
        "deep-research",
        "deepresearch",
        "information-seeking",
        "llm",
        "tongyi",
        "web-agent",
        "ai",
        "gpt",
        "o3-mini",
        "research"
      ],
      "likes": 106031,
      "downloads": 106031,
      "lastModified": "2025-11-20T01:44:25Z",
      "lastModifiedTimestamp": 1763603065000,
      "readme": "<div align=\"center\">\n  <picture>\n      <img src=\"./assets/logo.png\" width=\"100%\">\n  </picture>\n</div>\n\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n\n[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&logo=huggingface&logoColor=ffffff&labelColor)](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)\n[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Alibaba-NLP/DeepResearch)\n[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)\n[![Paper](https://img.shields.io/badge/Paper-red?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2510.24701)\n\n</div>\n<p align=\"center\">\n<p align=\"center\">\nü§ó <a href=\"https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\" target=\"_blank\">HuggingFace</a> ÔΩú\n<img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> <a href=\"https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B\" target=\"_blank\">ModelScope</a> |  üí¨ <a href=\"./assets/wechat_new.jpg\">WeChat(ÂæÆ‰ø°)</a> üì∞ <a href=\"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/\">Blog</a> | üìë <a href=\"https://arxiv.org/pdf/2510.24701\">Paper</a>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/14895\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/14895\" alt=\"Alibaba-NLP%2FDeepResearch | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nüëè Welcome to try Tongyi DeepResearch via our **[<img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> Modelscope online demo](https://www.modelscope.cn/studios/jialongwu/Tongyi-DeepResearch)** or **[ü§ó Huggingface online demo](https://huggingface.co/spaces/Alibaba-NLP/Tongyi-DeepResearch)** or <img src=\"./WebAgent/assets/aliyun.png\" width=\"14px\" style=\"display:inline;\"> **[bailian service](https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&tab=app#/app/app-market/deep-search/)**!\n\n> [!NOTE]\n> This demo is for quick exploration only. Response times may vary or fail intermittently due to model latency and tool QPS limits. For a stable experience we recommend local deployment; for a production-ready service, visit <img src=\"./WebAgent/assets/aliyun.png\" width=\"14px\" style=\"display:inline;\"> [bailian](https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&tab=app#/app/app-market/deep-search/) and follow the guided setup.\n\n# Introduction\n\nWe present <img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> **Tongyi DeepResearch**, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.\n\n> Tongyi DeepResearch builds upon our previous work on the <img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> [WebAgent](./WebAgent/) project.\n\nMore details can be found in our üì∞&nbsp;<a href=\"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/\">Tech Blog</a>.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/performance.png\">\n</p>\n\n## Features\n\n- ‚öôÔ∏è **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.\n- üîÑ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.\n- üîÅ **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.\n- ü§ñ **Agent Inference Paradigm Compatibility**: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.\n\n# Model Download\n\nYou can directly download the model by following the links below.\n\n|            Model            |                                                                           Download Links                                                                           | Model Size | Context Length |\n| :-------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------: | :------------: |\n| Tongyi-DeepResearch-30B-A3B | [ü§ó HuggingFace](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)<br> [ü§ñ ModelScope](https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B) |  30B-A3B   |      128K      |\n\n# News\n\n[2025/09/20]üöÄ Tongyi-DeepResearch-30B-A3B is now on [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b)! Follow the [Quick-start](https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model) guide.\n\n[2025/09/17]üî• We have released **Tongyi-DeepResearch-30B-A3B**.\n\n# Deep Research Benchmark Results\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/benchmark.png\">\n</p>\n\n## Quick Start\n\nThis guide provides instructions for setting up the environment and running inference scripts located in the [inference](./inference/) folder.\n\n### 1. Environment Setup\n- Recommended Python version: **3.10.0** (using other versions may cause dependency issues).\n- It is strongly advised to create an isolated environment using `conda` or `virtualenv`.\n\n```bash\n# Example with Conda\nconda create -n react_infer_env python=3.10.0\nconda activate react_infer_env\n```\n\n### 2. Installation\n\nInstall the required dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### 3. Environment Configuration and Prepare Evaluation Data\n\n#### Environment Configuration\n\nConfigure your API keys and settings by copying the example environment file:\n\n```bash\n# Copy the example environment file\ncp .env.example .env\n```\n\nEdit the `.env` file and provide your actual API keys and configuration values:\n\n- **SERPER_KEY_ID**: Get your key from [Serper.dev](https://serper.dev/) for web search and Google Scholar\n- **JINA_API_KEYS**: Get your key from [Jina.ai](https://jina.ai/) for web page reading\n- **API_KEY/API_BASE**: OpenAI-compatible API for page summarization from [OpenAI](https://platform.openai.com/)\n- **DASHSCOPE_API_KEY**: Get your key from [Dashscope](https://dashscope.aliyun.com/) for file parsing\n- **SANDBOX_FUSION_ENDPOINT**: Python interpreter sandbox endpoints (see [SandboxFusion](https://github.com/bytedance/SandboxFusion))\n- **MODEL_PATH**: Path to your model weights\n- **DATASET**: Name of your evaluation dataset\n- **OUTPUT_PATH**: Directory for saving results\n\n> **Note**: The `.env` file is gitignored, so your secrets will not be committed to the repository.\n\n#### Prepare Evaluation Data\n\nThe system supports two input file formats: **JSON** and **JSONL**.\n\n#### Supported File Formats:\n\n**Option 1: JSONL Format (recommended)**\n- Create your data file with `.jsonl` extension (e.g., `my_questions.jsonl`)\n- Each line must be a valid JSON object with `question` and `answer` keys:\n  ```json\n  {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"}\n  {\"question\": \"Explain quantum computing\", \"answer\": \"\"}\n  ```\n\n**Option 2: JSON Format**\n- Create your data file with `.json` extension (e.g., `my_questions.json`)\n- File must contain a JSON array of objects, each with `question` and `answer` keys:\n  ```json\n  [\n    { \"question\": \"What is the capital of France?\", \"answer\": \"Paris\" },\n    { \"question\": \"Explain quantum computing\", \"answer\": \"\" }\n  ]\n  ```\n\n**Important Note:** The `answer` field contains the **ground truth/reference answer** used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.\n\n#### File References for Document Processing:\n\n- If using the _file parser_ tool, **prepend the filename to the `question` field**\n- Place referenced files in `eval_data/file_corpus/` directory\n- Example: `{\"question\": \"(Uploaded 1 file: ['report.pdf'])\\n\\nWhat are the key findings?\", \"answer\": \"...\"}`\n\n#### File Organization:\n```\nproject_root/\n‚îú‚îÄ‚îÄ eval_data/\n‚îÇ   ‚îú‚îÄ‚îÄ my_questions.jsonl          # Your evaluation data\n‚îÇ   ‚îî‚îÄ‚îÄ file_corpus/                # Referenced documents\n‚îÇ       ‚îú‚îÄ‚îÄ report.pdf\n‚îÇ       ‚îî‚îÄ‚îÄ data.xlsx\n```\n\n### 4. Configure the Inference Script\n\n- Open `run_react_infer.sh` and modify the following variables as instructed in the comments:\n  - `MODEL_PATH` - path to the local or remote model weights.\n  - `DATASET` - full path to your evaluation file, e.g. `eval_data/my_questions.jsonl` or `/path/to/my_questions.json`.\n  - `OUTPUT_PATH` - path for saving the prediction results, e.g. `./outputs`.\n- Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required `API_KEY`, `BASE_URL`, or other credentials. Each key is explained inline in the bash script.\n\n### 5. Run the Inference Script\n\n```bash\nbash run_react_infer.sh\n```\n---\n\nWith these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.\n\n### 6. You can use OpenRouter's API to call our model\n\nTongyi-DeepResearch-30B-A3B is now available at [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b). You can run the inference without any GPUs.\n\nYou need to modify the following in the file [inference/react_agent.py](https://github.com/Alibaba-NLP/DeepResearch/blob/main/inference/react_agent.py):\n\n- In the call_server function: Set the API key and URL to your OpenRouter account‚Äôs API and URL.\n- Change the model name to alibaba/tongyi-deepresearch-30b-a3b.\n- Adjust the content concatenation way as described in the comments on lines **88‚Äì90.**\n\n## Benchmark Evaluation\n\nWe provide benchmark evaluation scripts for various datasets. Please refer to the [evaluation scripts](./evaluation/) directory for more details.\n\n## FAQ\n\nPlease refer to the [FAQ](./FAQ.md) for more details.\n\n## Deep Research Agent Family\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/family17.png\">\n</p>\n\nTongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:\n\n[1] [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/pdf/2501.07572) (ACL 2025)<br>\n[2] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648) (NeurIPS 2025)<br>\n[3] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/pdf/2507.02592)<br>\n[4] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/pdf/2507.15061)<br>\n[5] [WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent](https://arxiv.org/pdf/2508.05748)<br>\n[6] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/pdf/2509.13309)<br>\n[7] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/pdf/2509.13313)<br>\n[8] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/pdf/2509.13312)<br>\n[9] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/pdf/2509.13305)<br>\n[10] [Scaling Agents via Continual Pre-training](https://arxiv.org/pdf/2509.13310)<br>\n[11] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/pdf/2509.13311)<br>\n[12] [WebLeaper: Empowering Efficient, Info-Rich Seeking for Web Agents](https://arxiv.org/pdf/2510.24697)\n\n## üåü Misc\n\n<div align=\"center\">\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&type=Date)](https://www.star-history.com/#Alibaba-NLP/DeepResearch&Date)\n\n</div>\n\n## üö© Talent Recruitment\n\nüî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)\n\nüìö **Research Area**ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG\n\n‚òéÔ∏è **Contact**Ôºö[yongjiang.jy@alibaba-inc.com]()\n\n## Contact Information\n\nFor communications, please contact Yong Jiang (yongjiang.jy@alibaba-inc.com).\n\n## Citation\n\n```bibtex\n@article{tongyidr,\n  title={Tongyi DeepResearch Technical Report},\n  author={Team, Tongyi DeepResearch and Li, Baixuan and Zhang, Bo and Zhang, Dingchu and Huang, Fei and Li, Guangyu and Chen, Guoxin and Yin, Huifeng and Wu, Jialong and Zhou, Jingren and others},\n  journal={arXiv preprint arXiv:2510.24701},\n  year={2025}\n}\n```\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Alibaba-NLP/DeepResearch",
          "homepage": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
          "language": "Python",
          "forks": 1314,
          "open_issues": 66,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/dzhng/deep-research",
          "homepage": "",
          "language": "TypeScript",
          "forks": 1866,
          "open_issues": 77,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/64211549?v=4",
      "velocity": 38866.3,
      "is_rising_star": true
    },
    {
      "id": "github-reworkd-AgentGPT",
      "name": "AgentGPT",
      "author": "reworkd",
      "description": "ü§ñ Assemble, configure, and deploy autonomous AI Agents in your browser.",
      "task": "tool",
      "tags": [
        "agent",
        "agentgpt",
        "agi",
        "autogpt",
        "baby-agi",
        "gpt",
        "langchain",
        "next",
        "openai",
        "t3",
        "t3-stack"
      ],
      "likes": 105765,
      "downloads": 105765,
      "lastModified": "2025-11-19T18:55:43Z",
      "lastModifiedTimestamp": 1763578543000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/reworkd/AgentGPT",
          "homepage": "https://agentgpt.reworkd.ai",
          "language": "TypeScript",
          "forks": 9487,
          "open_issues": 214,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/120154269?v=4",
      "velocity": 38778.3,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-qlib",
      "name": "qlib",
      "author": "microsoft",
      "description": "Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.",
      "task": "tool",
      "tags": [
        "algorithmic-trading",
        "auto-quant",
        "deep-learning",
        "finance",
        "fintech",
        "investment",
        "machine-learning",
        "paper",
        "platform",
        "python",
        "quant",
        "quant-dataset",
        "quant-models",
        "quantitative-finance",
        "quantitative-trading",
        "research",
        "research-paper",
        "stock-data"
      ],
      "likes": 101601,
      "downloads": 101601,
      "lastModified": "2025-11-20T03:54:28Z",
      "lastModifiedTimestamp": 1763610868000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/qlib",
          "homepage": "https://qlib.readthedocs.io/en/latest/",
          "language": "Python",
          "forks": 5237,
          "open_issues": 304,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 37231.7,
      "is_rising_star": true
    },
    {
      "id": "github-1Panel-dev-1Panel",
      "name": "1Panel",
      "author": "1Panel-dev",
      "description": "üî• 1Panel provides an intuitive web interface and MCP Server to manage websites, files, containers, databases, and LLMs on a Linux server.",
      "task": "tool",
      "tags": [
        "1panel",
        "cockpit",
        "docker",
        "docker-ui",
        "lamp",
        "linux",
        "lnmp",
        "ollama",
        "webmin"
      ],
      "likes": 96275,
      "downloads": 96275,
      "lastModified": "2025-11-20T03:39:30Z",
      "lastModifiedTimestamp": 1763609970000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/1Panel-dev/1Panel",
          "homepage": "https://1panel.pro",
          "language": "Go",
          "forks": 2839,
          "open_issues": 509,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/109613420?v=4",
      "velocity": 35295.7,
      "is_rising_star": true
    },
    {
      "id": "github-danny-avila-LibreChat",
      "name": "LibreChat",
      "author": "danny-avila",
      "description": "Enhanced ChatGPT Clone: Features Agents, MCP, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-5, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active.",
      "task": "tool",
      "tags": [
        "ai",
        "anthropic",
        "artifacts",
        "aws",
        "azure",
        "chatgpt",
        "chatgpt-clone",
        "claude",
        "clone",
        "deepseek",
        "gemini",
        "google",
        "gpt-5",
        "librechat",
        "mcp",
        "o1",
        "openai",
        "responses-api",
        "vision",
        "webui"
      ],
      "likes": 95365,
      "downloads": 95365,
      "lastModified": "2025-11-20T03:05:26Z",
      "lastModifiedTimestamp": 1763607926000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/danny-avila/LibreChat",
          "homepage": "https://librechat.ai/",
          "language": "TypeScript",
          "forks": 6225,
          "open_issues": 353,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110412045?v=4",
      "velocity": 34945.9,
      "is_rising_star": true
    },
    {
      "id": "github-khoj-ai-khoj",
      "name": "khoj",
      "author": "khoj-ai",
      "description": "Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "assistant",
        "chat",
        "chatgpt",
        "emacs",
        "image-generation",
        "llama3",
        "llamacpp",
        "llm",
        "obsidian",
        "obsidian-md",
        "offline-llm",
        "productivity",
        "rag",
        "research",
        "self-hosted",
        "semantic-search",
        "stt",
        "whatsapp-ai"
      ],
      "likes": 94832,
      "downloads": 94832,
      "lastModified": "2025-11-20T00:45:17Z",
      "lastModifiedTimestamp": 1763599517000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/khoj-ai/khoj",
          "homepage": "https://khoj.dev",
          "language": "Python",
          "forks": 1860,
          "open_issues": 85,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134046886?v=4",
      "velocity": 34764.4,
      "is_rising_star": true
    },
    {
      "id": "github-BerriAI-litellm",
      "name": "litellm",
      "author": "BerriAI",
      "description": "Python SDK, Proxy Server (AI Gateway) to call 100+ LLM APIs in OpenAI (or native) format, with cost tracking, guardrails, loadbalancing and logging. [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, VLLM, NVIDIA NIM]",
      "task": "tool",
      "tags": [
        "ai-gateway",
        "anthropic",
        "azure-openai",
        "bedrock",
        "gateway",
        "langchain",
        "litellm",
        "llm",
        "llm-gateway",
        "llmops",
        "mcp-gateway",
        "openai",
        "openai-proxy",
        "vertex-ai"
      ],
      "likes": 93970,
      "downloads": 93970,
      "lastModified": "2025-11-20T03:54:47Z",
      "lastModifiedTimestamp": 1763610887000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/BerriAI/litellm",
          "homepage": "https://docs.litellm.ai/docs/",
          "language": "Python",
          "forks": 4749,
          "open_issues": 1359,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/121462774?v=4",
      "velocity": 34423.4,
      "is_rising_star": true
    },
    {
      "id": "github-continuedev-continue",
      "name": "continue",
      "author": "continuedev",
      "description": "‚è© Ship faster with Continuous AI. Open-source CLI that can be used in TUI mode as a coding agent or Headless mode to run background agents",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "background-agents",
        "claude",
        "cli",
        "continuous-ai",
        "developer-tools",
        "gemini",
        "gpt",
        "hacktoberfest",
        "jetbrains",
        "llm",
        "open-source",
        "qwen",
        "vscode",
        "workflows"
      ],
      "likes": 89741,
      "downloads": 89741,
      "lastModified": "2025-11-20T04:00:09Z",
      "lastModifiedTimestamp": 1763611209000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/continuedev/continue",
          "homepage": "https://docs.continue.dev/",
          "language": "TypeScript",
          "forks": 3801,
          "open_issues": 656,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127876214?v=4",
      "velocity": 32896.6,
      "is_rising_star": true
    },
    {
      "id": "github-JushBJJ-Mr.-Ranedeer-AI-Tutor",
      "name": "Mr.-Ranedeer-AI-Tutor",
      "author": "JushBJJ",
      "description": "A GPT-4 AI Tutor Prompt for customizable personalized learning experiences.",
      "task": "tool",
      "tags": [
        "ai",
        "education",
        "gpt-4",
        "llm"
      ],
      "likes": 89002,
      "downloads": 89002,
      "lastModified": "2025-11-20T02:26:40Z",
      "lastModifiedTimestamp": 1763605600000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor",
          "homepage": "https://Mr-Ranedeer.com",
          "language": null,
          "forks": 3373,
          "open_issues": 14,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/36951064?v=4",
      "velocity": 32632.6,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-graphrag",
      "name": "graphrag",
      "author": "microsoft",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "task": "tool",
      "tags": [
        "gpt",
        "gpt-4",
        "gpt4",
        "graphrag",
        "llm",
        "llms",
        "rag"
      ],
      "likes": 87739,
      "downloads": 87739,
      "lastModified": "2025-11-20T03:49:55Z",
      "lastModifiedTimestamp": 1763610595000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/graphrag",
          "homepage": "https://microsoft.github.io/graphrag/",
          "language": "Python",
          "forks": 3075,
          "open_issues": 94,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 32151.9,
      "is_rising_star": true
    },
    {
      "id": "github-feder-cr-Jobs_Applier_AI_Agent_AIHawk",
      "name": "Jobs_Applier_AI_Agent_AIHawk",
      "author": "feder-cr",
      "description": "AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.",
      "task": "tool",
      "tags": [
        "agent",
        "application-resume",
        "artificial-intelligence",
        "automate",
        "automation",
        "bot",
        "chatgpt",
        "chrome",
        "gpt",
        "human-resources",
        "job",
        "jobs",
        "jobsearch",
        "jobseeker",
        "opeai",
        "python",
        "resume",
        "scraper",
        "scraping",
        "selenium"
      ],
      "likes": 87229,
      "downloads": 87229,
      "lastModified": "2025-11-19T18:41:18Z",
      "lastModifiedTimestamp": 1763577678000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk",
          "homepage": "",
          "language": "Python",
          "forks": 4422,
          "open_issues": 12,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85809106?v=4",
      "velocity": 31978.1,
      "is_rising_star": true
    },
    {
      "id": "github-666ghj-BettaFish",
      "name": "BettaFish",
      "author": "666ghj",
      "description": "ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ",
      "task": "tool",
      "tags": [
        "agent-framework",
        "data-analysis",
        "deep-research",
        "deep-search",
        "llms",
        "multi-agent-system",
        "nlp",
        "public-opinion-analysis",
        "python3",
        "sentiment-analysis"
      ],
      "likes": 85074,
      "downloads": 85074,
      "lastModified": "2025-11-20T03:55:09Z",
      "lastModifiedTimestamp": 1763610909000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/666ghj/BettaFish",
          "homepage": "",
          "language": "Python",
          "forks": 5428,
          "open_issues": 58,
          "license": "GNU General Public License v2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110395318?v=4",
      "velocity": 31050.8,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-llm.c",
      "name": "llm.c",
      "author": "karpathy",
      "description": "LLM training in simple, raw C/CUDA",
      "task": "tool",
      "tags": [],
      "likes": 84583,
      "downloads": 84583,
      "lastModified": "2025-11-19T23:29:57Z",
      "lastModifiedTimestamp": 1763594997000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/llm.c",
          "homepage": "",
          "language": "Cuda",
          "forks": 3293,
          "open_issues": 215,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 31010.1,
      "is_rising_star": true
    },
    {
      "id": "github-songquanpeng-one-api",
      "name": "one-api",
      "author": "songquanpeng",
      "description": "LLM API ÁÆ°ÁêÜ & ÂàÜÂèëÁ≥ªÁªüÔºåÊîØÊåÅ OpenAI„ÄÅAzure„ÄÅAnthropic Claude„ÄÅGoogle Gemini„ÄÅDeepSeek„ÄÅÂ≠óËäÇË±ÜÂåÖ„ÄÅChatGLM„ÄÅÊñáÂøÉ‰∏ÄË®Ä„ÄÅËÆØÈ£ûÊòüÁÅ´„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅ360 Êô∫ËÑë„ÄÅËÖæËÆØÊ∑∑ÂÖÉÁ≠â‰∏ªÊµÅÊ®°ÂûãÔºåÁªü‰∏Ä API ÈÄÇÈÖçÔºåÂèØÁî®‰∫é key ÁÆ°ÁêÜ‰∏é‰∫åÊ¨°ÂàÜÂèë„ÄÇÂçïÂèØÊâßË°åÊñá‰ª∂ÔºåÊèê‰æõ Docker ÈïúÂÉèÔºå‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÂºÄÁÆ±Âç≥Áî®„ÄÇLLM API management & key redistribution system, unifying multiple providers under a single API. Single binary, Docker-ready, with an English UI.",
      "task": "tool",
      "tags": [
        "api",
        "api-gateway",
        "azure-openai-api",
        "chatgpt",
        "claude",
        "ernie-bot",
        "gemini",
        "gpt",
        "openai",
        "openai-api",
        "proxy"
      ],
      "likes": 84136,
      "downloads": 84136,
      "lastModified": "2025-11-20T03:11:36Z",
      "lastModifiedTimestamp": 1763608296000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/songquanpeng/one-api",
          "homepage": "https://openai.justsong.cn/",
          "language": "JavaScript",
          "forks": 5519,
          "open_issues": 968,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/39998050?v=4",
      "velocity": 30837.4,
      "is_rising_star": true
    },
    {
      "id": "github-OpenBMB-ChatDev",
      "name": "ChatDev",
      "author": "OpenBMB",
      "description": "Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)",
      "task": "tool",
      "tags": [],
      "likes": 83235,
      "downloads": 83235,
      "lastModified": "2025-11-19T19:17:28Z",
      "lastModifiedTimestamp": 1763579848000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenBMB/ChatDev",
          "homepage": "https://arxiv.org/abs/2307.07924",
          "language": "Python",
          "forks": 3487,
          "open_issues": 50,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/89920203?v=4",
      "velocity": 30517.3,
      "is_rising_star": true
    },
    {
      "id": "github-stanford-oval-storm",
      "name": "storm",
      "author": "stanford-oval",
      "description": "An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.",
      "task": "tool",
      "tags": [
        "agentic-rag",
        "deep-research",
        "emnlp2024",
        "knowledge-curation",
        "large-language-models",
        "naacl",
        "nlp",
        "report-generation",
        "retrieval-augmented-generation"
      ],
      "likes": 82855,
      "downloads": 82855,
      "lastModified": "2025-11-20T03:14:26Z",
      "lastModifiedTimestamp": 1763608466000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/stanford-oval/storm",
          "homepage": "http://storm.genie.stanford.edu",
          "language": "Python",
          "forks": 2504,
          "open_issues": 87,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13667124?v=4",
      "velocity": 30376.5,
      "is_rising_star": true
    },
    {
      "id": "github-voideditor-void",
      "name": "void",
      "author": "voideditor",
      "description": "An AI tool from GitHub.",
      "task": "tool",
      "tags": [
        "chatgpt",
        "claude",
        "copilot",
        "cursor",
        "developer-tools",
        "editor",
        "llm",
        "open-source",
        "openai",
        "visual-studio-code",
        "vscode",
        "vscode-extension"
      ],
      "likes": 82654,
      "downloads": 82654,
      "lastModified": "2025-11-20T03:17:19Z",
      "lastModifiedTimestamp": 1763608639000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/voideditor/void",
          "homepage": "https://voideditor.com",
          "language": "TypeScript",
          "forks": 2147,
          "open_issues": 304,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/181171420?v=4",
      "velocity": 30298.4,
      "is_rising_star": true
    },
    {
      "id": "github-nrwl-nx",
      "name": "nx",
      "author": "nrwl",
      "description": "Get to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents.",
      "task": "tool",
      "tags": [
        "angular",
        "build",
        "build-system",
        "build-tool",
        "building-tool",
        "cli",
        "cypress",
        "hacktoberfest",
        "javascript",
        "monorepo",
        "nextjs",
        "nodejs",
        "nx",
        "nx-workspaces",
        "react",
        "storybook",
        "typescript"
      ],
      "likes": 82544,
      "downloads": 82544,
      "lastModified": "2025-11-19T23:18:56Z",
      "lastModifiedTimestamp": 1763594336000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/nrwl/nx",
          "homepage": "https://nx.dev",
          "language": "TypeScript",
          "forks": 2622,
          "open_issues": 786,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23692104?v=4",
      "velocity": 30258.8,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-semantic-kernel",
      "name": "semantic-kernel",
      "author": "microsoft",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "llm",
        "openai",
        "sdk"
      ],
      "likes": 80096,
      "downloads": 80096,
      "lastModified": "2025-11-20T00:29:43Z",
      "lastModifiedTimestamp": 1763598583000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/semantic-kernel",
          "homepage": "https://aka.ms/semantic-kernel",
          "language": "C#",
          "forks": 4349,
          "open_issues": 572,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 29363.4,
      "is_rising_star": true
    },
    {
      "id": "h2oai/h2ovl-mississippi-800m",
      "name": "h2ovl-mississippi-800m",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "safetensors",
        "h2ovl_chat",
        "feature-extraction",
        "gpt",
        "llm",
        "multimodal large language model",
        "ocr",
        "text-generation",
        "conversational",
        "custom_code",
        "en",
        "arxiv:2410.13611",
        "license:apache-2.0",
        "region:us"
      ],
      "likes": 585,
      "downloads": 79980,
      "lastModifiedTimestamp": null,
      "readme": "---\nlanguage:\n- en\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- gpt\n- llm\n- multimodal large language model\n- ocr\nthumbnail: >-\n  https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\npipeline_tag: text-generation\n---\n# Model Card\n[\\[üìú H2OVL-Mississippi Paper\\]](https://arxiv.org/abs/2410.13611)\n[\\[ü§ó HF Demo\\]](https://huggingface.co/spaces/h2oai/h2ovl-mississippi)\n[\\[üöÄ Quick Start\\]](#quick-start)\n\nThe H2OVL-Mississippi-800M is a compact yet powerful vision-language model from H2O.ai, featuring 0.8 billion parameters. Despite its small size, it delivers state-of-the-art performance in text recognition, excelling in the Text Recognition segment of OCRBench and outperforming much larger models in this domain. Built upon the robust architecture of our H2O-Danube language models, the Mississippi-800M extends their capabilities by seamlessly integrating vision and language tasks.\n\n<div align=\"center\">\n  <img src=\"./assets/text_recognition.png\" alt=\"Mississippi-2B Benchmarks\" width=\"600\"/>\n</div>\n\n## Key Features:\n\n- 0.8 Billion Parameters: Balance between performance and efficiency, making it suitable for OCR and document processing.\n- Trained on 19 million image-text pairs, with a focus on OCR, document comprehension, and chart, figure, and table interpretation, the model is optimized for superior OCR performance.\n\n\n<div align=\"center\">\n  <img src=\"./assets/perf_size.png\" alt=\"Mississippi-2B Benchmarks\" width=\"600\"/>\n</div>\n\n\n## Benchmarks\n\n### Performance Comparison of Similar Sized Models Across Multiple Benchmarks - OpenVLM Leaderboard\n\n| **Models**                 | **Params (B)** | **Avg. Score** | **MMBench** | **MMStar** | **MMMU<sub>VAL</sub>** | **Math Vista** | **Hallusion** | **AI2D<sub>TEST</sub>** | **OCRBench** | **MMVet** |\n|----------------------------|----------------|----------------|-------------|------------|-----------------------|----------------|---------------|-------------------------|--------------|-----------|\n| Qwen2-VL-2B                | 2.1            | **57.2**       | **72.2**    | 47.5       | 42.2                  | 47.8           | **42.4**      | 74.7                    | **797**      | **51.5**  |\n| **H2OVL-Mississippi-2B**    | 2.1            | 54.4           | 64.8        | 49.6       | 35.2                  | **56.8**       | 36.4          | 69.9                    | 782          | 44.7      |\n| InternVL2-2B               | 2.1            | 53.9           | 69.6        | **49.8**   | 36.3                  | 46.0           | 38.0          | 74.1                    | 781          | 39.7      |\n| Phi-3-Vision               | 4.2            | 53.6           | 65.2        | 47.7       | **46.1**              | 44.6           | 39.0          | **78.4**                 | 637          | 44.1      |\n| MiniMonkey                 | 2.2            | 52.7           | 68.9        | 48.1       | 35.7                  | 45.3           | 30.9          | 73.7                    | **794**      | 39.8      |\n| MiniCPM-V-2                | 2.8            | 47.9           | 65.8        | 39.1       | 38.2                  | 39.8           | 36.1          | 62.9                    | 605          | 41.0      |\n| InternVL2-1B               | 0.8            | 48.3           | 59.7        | 45.6       | 36.7                  | 39.4           | 34.3          | 63.8                    | 755          | 31.5      |\n| PaliGemma-3B-mix-448       | 2.9            | 46.5           | 65.6        | 48.3       | 34.9                  | 28.7           | 32.2          | 68.3                    | 614          | 33.1      |\n| **H2OVL-Mississippi-0.8B** | 0.8            | 43.5           | 47.7        | 39.1       | 34.0                  | 39.0           | 29.6          | 53.6                    | 751          | 30.0      |\n| DeepSeek-VL-1.3B           | 2.0            | 39.6           | 63.8        | 39.9       | 33.8                  | 29.8           | 27.6          | 51.5                    | 413          | 29.2      |\n\n\n\n## Quick Start\n\n### Install dependencies:\n```bash\npip install transformers torch torchvision einops timm peft sentencepiece flash_attn\n```\n\n### Sample demo:\n\n```python\nimport torch\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n\n# Set up the model and tokenizer\nmodel_path = 'h2oai/h2ovl-mississippi-800m'\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\nconfig.llm_config._attn_implementation = 'flash_attention_2'\nmodel = AutoModel.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    config=config,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True).eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=2048, do_sample=True)\n\n# pure-text conversation\nquestion = 'Hello, how are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n\n# Example for single image\nimage_file = './examples/image.jpg'\nquestion = '<image>\\nRead the text in the image.'\nresponse, history = model.chat(tokenizer, image_file, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n\n```\n\n\n## Prompt Engineering for JSON Extraction\n\n### Overview\n\nThis guide demonstrates how to create prompts for extracting information and converting it into structured JSON outputs. It starts with basic examples and progresses to more complex JSON structures, including handling data from images of tables and charts. The objective is to help users design effective prompts that can be used in various applications, such as natural language processing, chatbots, or data extraction from visual inputs.\n\n### Table of Contents\n\n1. [Getting Started](#getting-started)\n2. [Extracting Simple Information](#example-1-extracting-simple-information-from-an-image)\n3. [Extracting Nested Information](#example-2-extracting-nested-information-from-an-image)\n4. [Extracting Lists and Arrays](#example-3-extracting-lists-and-arrays-from-an-image)\n5. [Extracting Tables](#example-4-extracting-table-data-from-an-image)\n6. [Extracting Charts](#example-5-extracting-chart-data-from-an-image)\n7. [Best Practices](#best-practices)\n\n---\n\n### Getting Started\n\nTo get started with JSON extraction from images, it's essential to have a clear understanding of the visual content you want to extract and the structure of the desired JSON output. The following examples will guide you through crafting prompts to achieve this.\n\n\n#### Example 1: Extracting Simple Information from an Image\n\n**Hypothetical Scenario:**\nYou have an image of a form that contains basic details like \"Name,\" \"Date of Birth,\" and \"Address.\"\n\n**Prompt:**\n```\nExtract the details from the form image and structure them into JSON format:\n{\n    \"name\": \"\",\n    \"date_of_birth\": \"\",\n    \"address\": \"\"\n}\n```\n\n**Expected Output:**\n```json\n{\n  \"name\": \"John Doe\",\n  \"date_of_birth\": \"1990-01-01\",\n  \"address\": \"1234 Elm Street, Springfield\"\n}\n```\n\n#### Example 2: Extracting Nested Information from an Image\n\n**Hypothetical Scenario:**\nYou have an image of a form that contains detailed personal information, including contact details and emergency contacts.\n\n**Prompt:**\n```\nExtract the information from the form and format it as follows:\n{\n    \"personal_details\": {\n        \"name\": \"\",\n        \"age\": 0,\n        \"gender\": \"\"\n    },\n    \"contact\": {\n        \"phone\": \"\",\n        \"email\": \"\"\n    },\n    \"emergency_contact\": {\n        \"name\": \"\",\n        \"relation\": \"\",\n        \"phone\": \"\"\n    }\n}\n```\n\n**Expected Output:**\n```json\n{\n  \"personal_details\": {\n    \"name\": \"Sarah Connor\",\n    \"age\": 35,\n    \"gender\": \"Female\"\n  },\n  \"contact\": {\n    \"phone\": \"555-1234\",\n    \"email\": \"sarah.connor@example.com\"\n  },\n  \"emergency_contact\": {\n    \"name\": \"Kyle Reese\",\n    \"relation\": \"Friend\",\n    \"phone\": \"555-5678\"\n  }\n}\n```\n\n\n#### Example 3: Extracting Lists and Arrays from an Image\n\n**Hypothetical Scenario:**\nYou have an image of a schedule that lists several events, their times, and locations.\n\n**Prompt:**\n```\nExtract the event details from the schedule image and structure them into JSON:\n{\n    \"events\": [\n        {\n            \"name\": \"\",\n            \"time\": \"\",\n            \"location\": \"\"\n        }\n    ]\n}\n```\n\n**Expected Output:**\n```json\n{\n  \"events\": [\n    {\n      \"name\": \"Morning Meeting\",\n      \"time\": \"09:00 AM\",\n      \"location\": \"Conference Room 1\"\n    },\n    {\n      \"name\": \"Lunch Break\",\n      \"time\": \"12:00 PM\",\n      \"location\": \"Cafeteria\"\n    },\n    {\n      \"name\": \"Project Update\",\n      \"time\": \"02:00 PM\",\n      \"location\": \"Conference Room 2\"\n    }\n  ]\n}\n```\n\n\n#### Example 4: Extracting Table Data from an Image\n\nImages of tables often contain structured data that needs to be parsed and converted to JSON. The following example demonstrates how to handle tabular data extraction.\n\n**Hypothetical Scenario:**\nYou have an image of a table listing product names, prices, and quantities.\n\n**Prompt:**\n```\nExtract the data from the table image and format it as JSON:\n{\n    \"products\": [\n        {\n            \"product_name\": \"\",\n            \"price\": \"\",\n            \"quantity\": 0\n        }\n    ]\n}\n```\n\n**Expected Output:**\n```json\n{\n  \"products\": [\n    {\n      \"product_name\": \"Apples\",\n      \"price\": \"$2\",\n      \"quantity\": 10\n    },\n    {\n      \"product_name\": \"Bananas\",\n      \"price\": \"$1\",\n      \"quantity\": 20\n    },\n    {\n      \"product_name\": \"Oranges\",\n      \"price\": \"$3\",\n      \"quantity\": 15\n    }\n  ]\n}\n```\n\n\n#### Example 5: Extracting Chart Data from an Image\n\nCharts include metadata and data points that need to be accurately extracted. Here's how to structure prompts to extract chart data from images.\n\n**Hypothetical Scenario:**\nYou have an image of a bar chart that shows monthly sales figures.\n\n**Prompt:**\n```\nExtract the details of the bar chart from the image, including the title, axis labels, and data points and format it as JSON:\n{\n    \"chart\": {\n        \"title\": \"\",\n        \"x_axis\": \"\",\n        \"y_axis\": \"\",\n        \"data_points\": [\n            {\n                \"label\": \"\",\n                \"value\": 0\n            }\n        ]\n    }\n}\n```\n\n**Expected Output:**\n```json\n{\n  \"chart\": {\n    \"title\": \"Monthly Sales Report\",\n    \"x_axis\": \"Months\",\n    \"y_axis\": \"Sales (in $)\",\n    \"data_points\": [\n      {\n        \"label\": \"January\",\n        \"value\": 500\n      },\n      {\n        \"label\": \"February\",\n        \"value\": 600\n      },\n      {\n        \"label\": \"March\",\n        \"value\": 700\n      }\n    ]\n  }\n}\n```\n\n## Best Practices\n\n1. **Be Explicit**: Clearly define the desired keys and structure in your prompt to avoid ambiguity.\n2. **Use Examples**: Provide sample outputs so that the system can understand the expected format.\n3. **Anticipate Variations**: Consider possible variations in the visual data and ensure the prompt can accommodate them.\n4. **Start Simple**: Begin with simple structures, and progressively increase complexity as needed.\n5. **Test and Iterate**: Refine your prompts through testing to ensure accuracy and consistency in outputs.\n\n## Acknowledgments\n\nWe would like to express our gratitude to the [InternVL team at OpenGVLab](https://github.com/OpenGVLab/InternVL) for their research and codebases, upon which we have built and expanded. We also acknowledge the work of the [LLaVA team](https://github.com/haotian-liu/LLaVA) and the [Monkey team](https://github.com/Yuliang-Liu/Monkey/tree/main/project/mini_monkey) for their insights and techniques used in improving multimodal models.\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2ovl-mississippi-800m",
          "files": [],
          "modelId": "h2oai/h2ovl-mississippi-800m"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2ovl-mississippi-800m",
          "files": [],
          "modelId": "h2oai/h2ovl-mississippi-800m"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2ovl-mississippi-800m",
          "files": [],
          "modelId": "h2oai/h2ovl-mississippi-800m"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2ovl-mississippi-800m",
          "files": [],
          "modelId": "h2oai/h2ovl-mississippi-800m"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/h2oai/h2ovl-mississippi-800m",
          "files": [],
          "modelId": "h2oai/h2ovl-mississippi-800m"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "github-labring-FastGPT",
      "name": "FastGPT",
      "author": "labring",
      "description": "FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.",
      "task": "tool",
      "tags": [
        "agent",
        "claude",
        "deepseek",
        "llm",
        "mcp",
        "nextjs",
        "openai",
        "qwen",
        "rag",
        "workflow"
      ],
      "likes": 78949,
      "downloads": 78949,
      "lastModified": "2025-11-20T03:15:06Z",
      "lastModifiedTimestamp": 1763608506000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/labring/FastGPT",
          "homepage": "https://fastgpt.io",
          "language": "TypeScript",
          "forks": 6772,
          "open_issues": 650,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102226726?v=4",
      "velocity": 28942.1,
      "is_rising_star": true
    },
    {
      "id": "github-ComposioHQ-composio",
      "name": "composio",
      "author": "ComposioHQ",
      "description": "Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agents",
        "ai",
        "ai-agents",
        "aiagents",
        "developer-tools",
        "function-calling",
        "gpt-4",
        "javascript",
        "js",
        "llm",
        "llmops",
        "mcp",
        "python",
        "remote-mcp-server",
        "sse",
        "typescript"
      ],
      "likes": 78494,
      "downloads": 78494,
      "lastModified": "2025-11-19T22:09:44Z",
      "lastModifiedTimestamp": 1763590184000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ComposioHQ/composio",
          "homepage": "https://docs.composio.dev",
          "language": "TypeScript",
          "forks": 4397,
          "open_issues": 25,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128464815?v=4",
      "velocity": 28778.2,
      "is_rising_star": true
    },
    {
      "id": "github-datawhalechina-self-llm",
      "name": "self-llm",
      "author": "datawhalechina",
      "description": "„ÄäÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó„ÄãÈíàÂØπ‰∏≠ÂõΩÂÆùÂÆùÈáèË∫´ÊâìÈÄ†ÁöÑÂü∫‰∫éLinuxÁéØÂ¢ÉÂø´ÈÄüÂæÆË∞ÉÔºàÂÖ®ÂèÇÊï∞/LoraÔºâ„ÄÅÈÉ®ÁΩ≤ÂõΩÂÜÖÂ§ñÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºàLLMÔºâ/Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâÊïôÁ®ã",
      "task": "tool",
      "tags": [
        "chatglm",
        "chatglm3",
        "gemma-2b-it",
        "glm-4",
        "internlm2",
        "llama3",
        "llm",
        "lora",
        "minicpm",
        "q-wen",
        "qwen",
        "qwen1-5",
        "qwen2"
      ],
      "likes": 78167,
      "downloads": 78167,
      "lastModified": "2025-11-20T03:28:23Z",
      "lastModifiedTimestamp": 1763609303000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/datawhalechina/self-llm",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 2621,
          "open_issues": 147,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/46047812?v=4",
      "velocity": 28647.3,
      "is_rising_star": true
    },
    {
      "id": "github-Hannibal046-Awesome-LLM",
      "name": "Awesome-LLM",
      "author": "Hannibal046",
      "description": "Awesome-LLM: a curated list of Large Language Model",
      "task": "tool",
      "tags": [],
      "likes": 76756,
      "downloads": 76756,
      "lastModified": "2025-11-20T03:51:48Z",
      "lastModifiedTimestamp": 1763610708000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Hannibal046/Awesome-LLM",
          "homepage": "",
          "language": null,
          "forks": 2186,
          "open_issues": 51,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/38466901?v=4",
      "velocity": 28138,
      "is_rising_star": true
    },
    {
      "id": "github-warpdotdev-Warp",
      "name": "Warp",
      "author": "warpdotdev",
      "description": "Warp is the agentic development environment, built for coding with multiple AI agents.",
      "task": "tool",
      "tags": [
        "bash",
        "linux",
        "macos",
        "rust",
        "shell",
        "terminal",
        "wasm",
        "zsh"
      ],
      "likes": 75913,
      "downloads": 75913,
      "lastModified": "2025-11-20T02:32:56Z",
      "lastModifiedTimestamp": 1763605976000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/warpdotdev/Warp",
          "homepage": "https://warp.dev",
          "language": null,
          "forks": 580,
          "open_issues": 3932,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/71840468?v=4",
      "velocity": 27831.1,
      "is_rising_star": true
    },
    {
      "id": "github-TauricResearch-TradingAgents",
      "name": "TradingAgents",
      "author": "TauricResearch",
      "description": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "task": "tool",
      "tags": [
        "agent",
        "finance",
        "llm",
        "multiagent",
        "trading"
      ],
      "likes": 75667,
      "downloads": 75667,
      "lastModified": "2025-11-20T03:47:34Z",
      "lastModifiedTimestamp": 1763610454000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/TauricResearch/TradingAgents",
          "homepage": "https://arxiv.org/pdf/2412.20138",
          "language": "Python",
          "forks": 4704,
          "open_issues": 196,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192884433?v=4",
      "velocity": 27716.7,
      "is_rising_star": true
    },
    {
      "id": "github-CopilotKit-CopilotKit",
      "name": "CopilotKit",
      "author": "CopilotKit",
      "description": "React UI + elegant infrastructure for AI Copilots, AI chatbots, and in-app AI agents. The Agentic last-mile ü™Å",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai",
        "ai-agent",
        "ai-assistant",
        "assistant",
        "copilot",
        "copilot-chat",
        "hacktoberfest",
        "langchain",
        "langgraph",
        "llm",
        "nextjs",
        "open-source",
        "react",
        "reactjs",
        "ts",
        "typescript"
      ],
      "likes": 75068,
      "downloads": 75068,
      "lastModified": "2025-11-20T00:17:21Z",
      "lastModifiedTimestamp": 1763597841000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CopilotKit/CopilotKit",
          "homepage": "https://docs.copilotkit.ai",
          "language": "TypeScript",
          "forks": 3339,
          "open_issues": 429,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131273140?v=4",
      "velocity": 27513.2,
      "is_rising_star": true
    },
    {
      "id": "katanemo/Arch-Router-1.5B",
      "name": "Arch-Router-1.5B",
      "description": "A model for text-generation.",
      "task": "text-generation",
      "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "routing",
        "preference",
        "arxiv:2506.16655",
        "llm",
        "conversational",
        "en",
        "base_model:Qwen/Qwen2.5-1.5B-Instruct",
        "base_model:finetune:Qwen/Qwen2.5-1.5B-Instruct",
        "license:other",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "likes": 3345,
      "downloads": 74930,
      "lastModifiedTimestamp": null,
      "readme": "---\nbase_model:\n- Qwen/Qwen2.5-1.5B-Instruct\nlanguage:\n- en\nlibrary_name: transformers\nlicense: other\nlicense_name: katanemo-research\nlicense_link: https://huggingface.co/katanemo/Arch-Router-1.5B/blob/main/LICENSE\npipeline_tag: text-generation\ntags:\n- routing\n- preference\n- arxiv:2506.16655\n- llm\npaper: https://arxiv.org/abs/2506.16655\n---\n\n# katanemo/Arch-Router-1.5B\n\n## Overview\nWith the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. \n\nWe introduce a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce Arch-Router, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. \n\nThis model is described in the paper: https://arxiv.org/abs/2506.16655, and powers [Arch](https://github.com/katanemo/arch) the models-native proxy server for agents.\n\n### How It Works\n\nTo support effective routing, Arch-Router introduces two key concepts:\n- **Domain** ‚Äì the high-level thematic category or subject matter of a request (e.g., legal, healthcare, programming).\n- **Action** ‚Äì the specific type of operation the user wants performed (e.g., summarization, code generation, booking appointment, translation).\n\nBoth domain and action configs are associated with preferred models or model variants. At inference time, Arch-Router analyzes the incoming prompt to infer its domain and action using semantic similarity, task indicators, and contextual cues. It then applies the user-defined routing preferences to select the model best suited to handle the request.\n\n### Key Features\n\n- **Structured Preference Routing**: Aligns prompt request with model strengths using explicit domain‚Äìaction mappings.\n- **Transparent and Controllable**: Makes routing decisions transparent and configurable, empowering users to customize system behavior.\n- **Flexible and Adaptive**: Supports evolving user needs, model updates, and new domains/actions without retraining the router.\n- **Production-Ready Performance**: Optimized for low-latency, high-throughput applications in multi-model environments.\n\n# Requirements\nThe code of Arch-Router-1.5B has been in the Hugging Face `transformers` library and we advise you to install latest version:\n```bash\npip install transformers>=4.37.0\n```\n\n# How to use\nWe use the following example to illustrate how to use our model to perform routing tasks. Please note that, our model works best with our provided prompt format. \n### Quickstart\n````python\nimport json\nfrom typing import Any, Dict, List\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"katanemo/Arch-Router-1.5B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Please use our provided prompt for best performance\nTASK_INSTRUCTION = \"\"\"\nYou are a helpful assistant designed to find the best suited route.\nYou are provided with route description within <routes></routes> XML tags:\n<routes>\n\n{routes}\n\n</routes>\n\n<conversation>\n\n{conversation}\n\n</conversation>\n\"\"\"\n\nFORMAT_PROMPT = \"\"\"\nYour task is to decide which route is best suit with user intent on the conversation in <conversation></conversation> XML tags.  Follow the instruction:\n1. If the latest intent from user is irrelevant or user intent is full filled, response with other route {\"route\": \"other\"}.\n2. You must analyze the route descriptions and find the best match route for user latest intent. \n3. You only response the name of the route that best matches the user's request, use the exact name in the <routes></routes>.\n\nBased on your analysis, provide your response in the following JSON formats if you decide to match any route:\n{\"route\": \"route_name\"} \n\"\"\"\n\n# Define route config\nroute_config = [\n    {\n        \"name\": \"code_generation\",\n        \"description\": \"Generating new code snippets, functions, or boilerplate based on user prompts or requirements\",\n    },\n    {\n        \"name\": \"bug_fixing\",\n        \"description\": \"Identifying and fixing errors or bugs in the provided code across different programming languages\",\n    },\n    {\n        \"name\": \"performance_optimization\",\n        \"description\": \"Suggesting improvements to make code more efficient, readable, or scalable\",\n    },\n    {\n        \"name\": \"api_help\",\n        \"description\": \"Assisting with understanding or integrating external APIs and libraries\",\n    },\n    {\n        \"name\": \"programming\",\n        \"description\": \"Answering general programming questions, theory, or best practices\",\n    },\n]\n\n# Helper function to create the system prompt for our model\ndef format_prompt(\n    route_config: List[Dict[str, Any]], conversation: List[Dict[str, Any]]\n):\n    return (\n        TASK_INSTRUCTION.format(\n            routes=json.dumps(route_config), conversation=json.dumps(conversation)\n        )\n        + FORMAT_PROMPT\n    )\n\n# Define conversations\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": \"fix this module 'torch.utils._pytree' has no attribute 'register_pytree_node'. did you mean: '_register_pytree_node'?\",\n    }\n]\n\nroute_prompt = format_prompt(route_config, conversation)\n\nmessages = [\n    {\"role\": \"user\", \"content\": route_prompt},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True, return_tensors=\"pt\"\n).to(model.device)\n\n# 2. Generate\ngenerated_ids = model.generate(\n    input_ids=input_ids,  # or just positional: model.generate(input_ids, ‚Ä¶)\n    max_new_tokens=32768,\n)\n\n# 3. Strip the prompt from each sequence\nprompt_lengths = input_ids.shape[1]  # same length for every row here\ngenerated_only = [\n    output_ids[prompt_lengths:]  # slice off the prompt tokens\n    for output_ids in generated_ids\n]\n\n# 4. Decode if you want text\nresponse = tokenizer.batch_decode(generated_only, skip_special_tokens=True)[0]\nprint(response)\n````\n\nThen you should be able to see the following output string in JSON format:\n````python\n{\"route\": \"bug_fixing\"}\n````\n\nTo better understand how to create the route descriptions, please take a look at our [Katanemo API](https://docs.archgw.com/guides/llm_router.html).\n\n# License\nKatanemo Arch-Router model is distributed under the [Katanemo license](https://huggingface.co/katanemo/Arch-Router-1.5B/blob/main/LICENSE).\n\nGitHub: https://github.com/katanemo/arch",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/katanemo/Arch-Router-1.5B",
          "files": [],
          "modelId": "katanemo/Arch-Router-1.5B"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/katanemo/Arch-Router-1.5B",
          "files": [],
          "modelId": "katanemo/Arch-Router-1.5B"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/katanemo/Arch-Router-1.5B",
          "files": [],
          "modelId": "katanemo/Arch-Router-1.5B"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/katanemo/Arch-Router-1.5B",
          "files": [],
          "modelId": "katanemo/Arch-Router-1.5B"
        },
        {
          "platform": "Hugging Face",
          "url": "https://huggingface.co/katanemo/Arch-Router-1.5B",
          "files": [],
          "modelId": "katanemo/Arch-Router-1.5B"
        }
      ],
      "thumbnail": null,
      "velocity": null,
      "is_rising_star": false
    },
    {
      "id": "github-chroma-core-chroma",
      "name": "chroma",
      "author": "chroma-core",
      "description": "Open-source search and retrieval database for AI applications.",
      "task": "tool",
      "tags": [
        "ai",
        "database",
        "document-retrieval",
        "embeddings",
        "llm",
        "llms",
        "rag",
        "rust",
        "rust-lang",
        "vector-database"
      ],
      "likes": 73468,
      "downloads": 73468,
      "lastModified": "2025-11-20T03:37:47Z",
      "lastModifiedTimestamp": 1763609867000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chroma-core/chroma",
          "homepage": "https://www.trychroma.com/",
          "language": "Rust",
          "forks": 1924,
          "open_issues": 495,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/105881770?v=4",
      "velocity": 26925.8,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-JARVIS",
      "name": "JARVIS",
      "author": "microsoft",
      "description": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf",
      "task": "tool",
      "tags": [
        "deep-learning",
        "platform",
        "pytorch"
      ],
      "likes": 73350,
      "downloads": 73350,
      "lastModified": "2025-11-19T21:32:35Z",
      "lastModifiedTimestamp": 1763587955000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/JARVIS",
          "homepage": "",
          "language": "Python",
          "forks": 2053,
          "open_issues": 332,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26895,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-BitNet",
      "name": "BitNet",
      "author": "microsoft",
      "description": "Official inference framework for 1-bit LLMs",
      "task": "tool",
      "tags": [],
      "likes": 73235,
      "downloads": 73235,
      "lastModified": "2025-11-19T20:17:12Z",
      "lastModifiedTimestamp": 1763583432000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/BitNet",
          "homepage": "",
          "language": "Python",
          "forks": 1895,
          "open_issues": 164,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26852.1,
      "is_rising_star": true
    },
    {
      "id": "github-assafelovic-gpt-researcher",
      "name": "gpt-researcher",
      "author": "assafelovic",
      "description": "An LLM agent that conducts deep research (local and web) on any given topic and generates a long report with citations.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "automation",
        "deepresearch",
        "llms",
        "mcp",
        "mcp-server",
        "python",
        "research",
        "search",
        "webscraping"
      ],
      "likes": 72613,
      "downloads": 72613,
      "lastModified": "2025-11-20T03:46:16Z",
      "lastModifiedTimestamp": 1763610376000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/assafelovic/gpt-researcher",
          "homepage": "https://gptr.dev",
          "language": "Python",
          "forks": 3196,
          "open_issues": 149,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13554167?v=4",
      "velocity": 26610.1,
      "is_rising_star": true
    }
  ],
  "newest": [
    {
      "id": "github-usestrix-strix",
      "name": "strix",
      "author": "usestrix",
      "description": "Open-source AI agents for penetration testing",
      "task": "tool",
      "tags": [
        "agents",
        "artificial-intelligence",
        "cybersecurity",
        "generative-ai",
        "llm",
        "penetration-testing"
      ],
      "likes": 37780,
      "downloads": 37780,
      "lastModified": "2025-11-20T04:01:07Z",
      "lastModifiedTimestamp": 1763611267000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/usestrix/strix",
          "homepage": "https://usestrix.com/",
          "language": "Python",
          "forks": 1151,
          "open_issues": 23,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/187630752?v=4",
      "velocity": 13719.2,
      "is_rising_star": true
    },
    {
      "id": "github-HqWu-HITCS-Awesome-Chinese-LLM",
      "name": "Awesome-Chinese-LLM",
      "author": "HqWu-HITCS",
      "description": "Êï¥ÁêÜÂºÄÊ∫êÁöÑ‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ª•ËßÑÊ®°ËæÉÂ∞è„ÄÅÂèØÁßÅÊúâÂåñÈÉ®ÁΩ≤„ÄÅËÆ≠ÁªÉÊàêÊú¨ËæÉ‰ΩéÁöÑÊ®°Âûã‰∏∫‰∏ªÔºåÂåÖÊã¨Â∫ïÂ∫ßÊ®°ÂûãÔºåÂûÇÁõ¥È¢ÜÂüüÂæÆË∞ÉÂèäÂ∫îÁî®ÔºåÊï∞ÊçÆÈõÜ‰∏éÊïôÁ®ãÁ≠â„ÄÇ",
      "task": "tool",
      "tags": [
        "awesome-lists",
        "chatglm",
        "chinese",
        "llama",
        "llm",
        "nlp"
      ],
      "likes": 65147,
      "downloads": 65147,
      "lastModified": "2025-11-20T04:01:00Z",
      "lastModifiedTimestamp": 1763611260000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/HqWu-HITCS/Awesome-Chinese-LLM",
          "homepage": "",
          "language": null,
          "forks": 2062,
          "open_issues": 5,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/29895268?v=4",
      "velocity": 23885.4,
      "is_rising_star": true
    },
    {
      "id": "github-x1xhlol-system-prompts-and-models-of-ai-tools",
      "name": "system-prompts-and-models-of-ai-tools",
      "author": "x1xhlol",
      "description": "FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus Agent Tools, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, dia & v0. (And other Open Sourced) System Prompts, Internal Tools & AI Models",
      "task": "tool",
      "tags": [
        "ai",
        "bolt",
        "cluely",
        "copilot",
        "cursor",
        "cursorai",
        "devin",
        "github-copilot",
        "lovable",
        "open-source",
        "perplexity",
        "replit",
        "system-prompts",
        "trae",
        "trae-ai",
        "trae-ide",
        "v0",
        "vscode",
        "windsurf",
        "windsurf-ai"
      ],
      "likes": 288923,
      "downloads": 288923,
      "lastModified": "2025-11-20T04:00:54Z",
      "lastModifiedTimestamp": 1763611254000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
          "homepage": "",
          "language": null,
          "forks": 25879,
          "open_issues": 94,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/185671340?v=4",
      "velocity": 105805.7,
      "is_rising_star": true
    },
    {
      "id": "github-GibsonAI-Memori",
      "name": "Memori",
      "author": "GibsonAI",
      "description": "Open-Source Memory Engine for LLMs, AI Agents & Multi-Agent Systems",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "aiagent",
        "awesome",
        "chatgpt",
        "hacktoberfest",
        "hacktoberfest2025",
        "llm",
        "long-short-term-memory",
        "memori-ai",
        "memory",
        "memory-management",
        "python",
        "rag",
        "state-management"
      ],
      "likes": 15952,
      "downloads": 15952,
      "lastModified": "2025-11-20T04:00:37Z",
      "lastModifiedTimestamp": 1763611237000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/GibsonAI/Memori",
          "homepage": "https://memorilabs.ai",
          "language": "Python",
          "forks": 388,
          "open_issues": 40,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/158103259?v=4",
      "velocity": 5656.2,
      "is_rising_star": true
    },
    {
      "id": "github-ag-ui-protocol-ag-ui",
      "name": "ag-ui",
      "author": "ag-ui-protocol",
      "description": "AG-UI: the Agent-User Interaction Protocol. Bring Agents into Frontend Applications.",
      "task": "tool",
      "tags": [
        "ag-ui-protocol",
        "agent-frontend",
        "agent-ui",
        "agentic-workflow",
        "ai-agents"
      ],
      "likes": 29430,
      "downloads": 29430,
      "lastModified": "2025-11-20T04:00:28Z",
      "lastModifiedTimestamp": 1763611228000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ag-ui-protocol/ag-ui",
          "homepage": "https://ag-ui.com",
          "language": "TypeScript",
          "forks": 908,
          "open_issues": 157,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/209775067?v=4",
      "velocity": 10776.7,
      "is_rising_star": true
    },
    {
      "id": "github-open-webui-open-webui",
      "name": "open-webui",
      "author": "open-webui",
      "description": "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
      "task": "tool",
      "tags": [
        "ai",
        "llm",
        "llm-ui",
        "llm-webui",
        "llms",
        "mcp",
        "ollama",
        "ollama-webui",
        "open-webui",
        "openai",
        "openapi",
        "rag",
        "self-hosted",
        "ui",
        "webui"
      ],
      "likes": 347017,
      "downloads": 347017,
      "lastModified": "2025-11-20T04:00:22Z",
      "lastModifiedTimestamp": 1763611222000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/open-webui/open-webui",
          "homepage": "https://openwebui.com",
          "language": "JavaScript",
          "forks": 16200,
          "open_issues": 303,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/158137808?v=4",
      "velocity": 127171,
      "is_rising_star": true
    },
    {
      "id": "github-continuedev-continue",
      "name": "continue",
      "author": "continuedev",
      "description": "‚è© Ship faster with Continuous AI. Open-source CLI that can be used in TUI mode as a coding agent or Headless mode to run background agents",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "background-agents",
        "claude",
        "cli",
        "continuous-ai",
        "developer-tools",
        "gemini",
        "gpt",
        "hacktoberfest",
        "jetbrains",
        "llm",
        "open-source",
        "qwen",
        "vscode",
        "workflows"
      ],
      "likes": 89741,
      "downloads": 89741,
      "lastModified": "2025-11-20T04:00:09Z",
      "lastModifiedTimestamp": 1763611209000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/continuedev/continue",
          "homepage": "https://docs.continue.dev/",
          "language": "TypeScript",
          "forks": 3801,
          "open_issues": 656,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127876214?v=4",
      "velocity": 32896.6,
      "is_rising_star": true
    },
    {
      "id": "github-Shubhamsaboo-awesome-llm-apps",
      "name": "awesome-llm-apps",
      "author": "Shubhamsaboo",
      "description": "Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.",
      "task": "tool",
      "tags": [
        "llms",
        "python",
        "rag"
      ],
      "likes": 237097,
      "downloads": 237097,
      "lastModified": "2025-11-20T04:00:02Z",
      "lastModifiedTimestamp": 1763611202000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
          "homepage": "https://www.theunwindai.com",
          "language": "Python",
          "forks": 10516,
          "open_issues": 1,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/31396011?v=4",
      "velocity": 86805.4,
      "is_rising_star": true
    },
    {
      "id": "github-volcengine-verl",
      "name": "verl",
      "author": "volcengine",
      "description": "verl: Volcano Engine Reinforcement Learning for LLMs",
      "task": "tool",
      "tags": [],
      "likes": 48351,
      "downloads": 48351,
      "lastModified": "2025-11-20T03:59:44Z",
      "lastModifiedTimestamp": 1763611184000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/volcengine/verl",
          "homepage": "https://verl.readthedocs.io/en/latest/index.html",
          "language": "Python",
          "forks": 2590,
          "open_issues": 1433,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/67365215?v=4",
      "velocity": 17685.8,
      "is_rising_star": true
    },
    {
      "id": "github-f-awesome-chatgpt-prompts",
      "name": "awesome-chatgpt-prompts",
      "author": "f",
      "description": "This repo includes ChatGPT prompt curation to use ChatGPT and other LLM tools better.",
      "task": "tool",
      "tags": [
        "bots",
        "chatbot",
        "chatgpt",
        "chatgpt-api",
        "language"
      ],
      "likes": 410025,
      "downloads": 410025,
      "lastModified": "2025-11-20T03:59:38Z",
      "lastModifiedTimestamp": 1763611178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/f/awesome-chatgpt-prompts",
          "homepage": "https://prompts.chat",
          "language": "JavaScript",
          "forks": 18173,
          "open_issues": 289,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/196477?v=4",
      "velocity": 150315,
      "is_rising_star": true
    },
    {
      "id": "github-oraios-serena",
      "name": "serena",
      "author": "oraios",
      "description": "A powerful coding agent toolkit providing semantic retrieval and editing capabilities (MCP server & other integrations)",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "ai-coding",
        "claude",
        "claude-code",
        "language-server",
        "llms",
        "mcp-server",
        "programming",
        "vibe-coding"
      ],
      "likes": 48585,
      "downloads": 48585,
      "lastModified": "2025-11-20T03:59:34Z",
      "lastModifiedTimestamp": 1763611174000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/oraios/serena",
          "homepage": "https://oraios.github.io/serena",
          "language": "Python",
          "forks": 1098,
          "open_issues": 84,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/181485370?v=4",
      "velocity": 17782.6,
      "is_rising_star": true
    },
    {
      "id": "github-future-architect-vuls",
      "name": "vuls",
      "author": "future-architect",
      "description": "Agent-less vulnerability scanner for Linux, FreeBSD, Container, WordPress, Programming language libraries, Network devices",
      "task": "tool",
      "tags": [
        "administrator",
        "cybersecurity",
        "freebsd",
        "go",
        "golang",
        "linux",
        "security",
        "security-audit",
        "security-automation",
        "security-hardening",
        "security-scanner",
        "security-tools",
        "security-vulnerability",
        "vulnerabilities",
        "vulnerability-assessment",
        "vulnerability-detection",
        "vulnerability-management",
        "vulnerability-scanner",
        "vulnerability-scanners",
        "vuls"
      ],
      "likes": 35486,
      "downloads": 35486,
      "lastModified": "2025-11-20T03:59:16Z",
      "lastModifiedTimestamp": 1763611156000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/future-architect/vuls",
          "homepage": "https://vuls.io/",
          "language": "Go",
          "forks": 1206,
          "open_issues": 63,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14890632?v=4",
      "velocity": 13008.6,
      "is_rising_star": true
    },
    {
      "id": "github-milvus-io-milvus",
      "name": "milvus",
      "author": "milvus-io",
      "description": "Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search",
      "task": "tool",
      "tags": [
        "anns",
        "cloud-native",
        "diskann",
        "distributed",
        "embedding-database",
        "embedding-similarity",
        "embedding-store",
        "faiss",
        "golang",
        "hnsw",
        "image-search",
        "llm",
        "nearest-neighbor-search",
        "rag",
        "vector-database",
        "vector-search",
        "vector-similarity",
        "vector-store"
      ],
      "likes": 118669,
      "downloads": 118669,
      "lastModified": "2025-11-20T03:59:12Z",
      "lastModifiedTimestamp": 1763611152000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/milvus-io/milvus",
          "homepage": "https://milvus.io",
          "language": "Go",
          "forks": 3572,
          "open_issues": 889,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/51735404?v=4",
      "velocity": 43424.7,
      "is_rising_star": true
    },
    {
      "id": "github-ashishpatel26-500-AI-Agents-Projects",
      "name": "500-AI-Agents-Projects",
      "author": "ashishpatel26",
      "description": "The 500 AI Agents Projects is a curated collection of AI agent use cases across various industries. It showcases practical applications and provides links to open-source projects for implementation, illustrating how AI agents are transforming sectors such as healthcare, finance, education, retail, and more.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "genai"
      ],
      "likes": 48947,
      "downloads": 48947,
      "lastModified": "2025-11-20T03:59:11Z",
      "lastModifiedTimestamp": 1763611151000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ashishpatel26/500-AI-Agents-Projects",
          "homepage": "https://github.com/ashishpatel26/500-AI-Agents-Projects",
          "language": null,
          "forks": 2963,
          "open_issues": 19,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/3095771?v=4",
      "velocity": 17881.6,
      "is_rising_star": true
    },
    {
      "id": "github-huginn-huginn",
      "name": "huginn",
      "author": "huginn",
      "description": "Create agents that monitor and act on your behalf.  Your agents are standing by!",
      "task": "tool",
      "tags": [
        "agent",
        "automation",
        "feed",
        "feedgenerator",
        "huginn",
        "monitoring",
        "notifications",
        "rss",
        "scraper",
        "twitter",
        "twitter-streaming",
        "webscraping"
      ],
      "likes": 144302,
      "downloads": 144302,
      "lastModified": "2025-11-20T03:59:01Z",
      "lastModifiedTimestamp": 1763611141000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huginn/huginn",
          "homepage": "",
          "language": "Ruby",
          "forks": 4194,
          "open_issues": 693,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23225142?v=4",
      "velocity": 52906.7,
      "is_rising_star": true
    },
    {
      "id": "github-dair-ai-Prompt-Engineering-Guide",
      "name": "Prompt-Engineering-Guide",
      "author": "dair-ai",
      "description": "üêô Guides, papers, lessons, notebooks and resources for prompt engineering, context engineering, RAG, and AI Agents.",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai-agents",
        "chatgpt",
        "deep-learning",
        "generative-ai",
        "language-model",
        "llms",
        "openai",
        "prompt-engineering",
        "rag"
      ],
      "likes": 199720,
      "downloads": 199720,
      "lastModified": "2025-11-20T03:58:38Z",
      "lastModifiedTimestamp": 1763611118000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
          "homepage": "https://www.promptingguide.ai/",
          "language": "MDX",
          "forks": 6945,
          "open_issues": 230,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/30384625?v=4",
      "velocity": 73213.8,
      "is_rising_star": true
    },
    {
      "id": "github-topoteretes-cognee",
      "name": "cognee",
      "author": "topoteretes",
      "description": "Memory for AI Agents in 6 lines of code",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agents",
        "ai-memory",
        "cognitive-architecture",
        "cognitive-memory",
        "context-engineering",
        "contributions-welcome",
        "good-first-issue",
        "good-first-pr",
        "graph-database",
        "graph-rag",
        "graphrag",
        "help-wanted",
        "knowledge",
        "knowledge-graph",
        "neo4j",
        "open-source",
        "openai",
        "rag",
        "vector-database"
      ],
      "likes": 26463,
      "downloads": 26463,
      "lastModified": "2025-11-20T03:58:21Z",
      "lastModifiedTimestamp": 1763611101000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/topoteretes/cognee",
          "homepage": "https://docs.cognee.ai",
          "language": "Python",
          "forks": 815,
          "open_issues": 57,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/125468716?v=4",
      "velocity": 9654.7,
      "is_rising_star": true
    },
    {
      "id": "github-apache-doris",
      "name": "doris",
      "author": "apache",
      "description": "Apache Doris is an easy-to-use, high performance and unified analytics database.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "bigquery",
        "database",
        "dbt",
        "delta-lake",
        "elt",
        "hudi",
        "iceberg",
        "lakehouse",
        "olap",
        "paimon",
        "query-engine",
        "real-time",
        "redshift",
        "snowflake",
        "spark",
        "sql"
      ],
      "likes": 43833,
      "downloads": 43833,
      "lastModified": "2025-11-20T03:58:20Z",
      "lastModifiedTimestamp": 1763611100000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/apache/doris",
          "homepage": "https://doris.apache.org",
          "language": "Java",
          "forks": 3608,
          "open_issues": 783,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/47359?v=4",
      "velocity": 16069.9,
      "is_rising_star": true
    },
    {
      "id": "github-github-copilot-cli",
      "name": "copilot-cli",
      "author": "github",
      "description": "GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal. ",
      "task": "tool",
      "tags": [],
      "likes": 15936,
      "downloads": 15936,
      "lastModified": "2025-11-20T03:58:13Z",
      "lastModifiedTimestamp": 1763611093000,
      "readme": "# GitHub Copilot CLI (Public Preview)\n\nThe power of GitHub Copilot, now in your terminal.\n\nGitHub Copilot CLI brings AI-powered coding assistance directly to your command line, enabling you to build, debug, and understand code through natural language conversations. Powered by the same agentic harness as GitHub's Copilot coding agent, it provides intelligent assistance while staying deeply integrated with your GitHub workflow.\n\nSee [our official documentation](https://docs.github.com/copilot/concepts/agents/about-copilot-cli) for more information.\n\n![Image of the splash screen for the Copilot CLI](https://github.com/user-attachments/assets/51ac25d2-c074-467a-9c88-38a8d76690e3)\n\n## üöÄ Introduction and Overview\n\nWe're bringing the power of GitHub Copilot coding agent directly to your terminal. With GitHub Copilot CLI, you can work locally and synchronously with an AI agent that understands your code and GitHub context.\n\n- **Terminal-native development:** Work with Copilot coding agent directly in your command line ‚Äî no context switching required.\n- **GitHub integration out of the box:** Access your repositories, issues, and pull requests using natural language, all authenticated with your existing GitHub account.\n- **Agentic capabilities:** Build, edit, debug, and refactor code with an AI collaborator that can plan and execute complex tasks.\n- **MCP-powered extensibility:** Take advantage of the fact that the coding agent ships with GitHub's MCP server by default and supports custom MCP servers to extend capabilities.\n- **Full control:** Preview every action before execution ‚Äî nothing happens without your explicit approval.\n\nWe're still early in our journey, but with your feedback, we're rapidly iterating to make the GitHub Copilot CLI the best possible companion in your terminal.\n\n## üì¶ Getting Started\n\n### Supported Platforms\n\n- **Linux**\n- **macOS**\n- **Windows**\n\n### Prerequisites\n\n- **Node.js** v22 or higher\n- **npm** v10 or higher\n- (On Windows) **PowerShell** v6 or higher\n- An **active Copilot subscription**. See [Copilot plans](https://github.com/features/copilot/plans?ref_cta=Copilot+plans+signup&ref_loc=install-copilot-cli&ref_page=docs).\n\nIf you have access to GitHub Copilot via your organization of enterprise, you cannot use GitHub Copilot CLI if your organization owner or enterprise administrator has disabled it in the organization or enterprise settings. See [Managing policies and features for GitHub Copilot in your organization](http://docs.github.com/copilot/managing-copilot/managing-github-copilot-in-your-organization/managing-github-copilot-features-in-your-organization/managing-policies-for-copilot-in-your-organization) for more information.\n\n### Installation\n\nInstall globally with npm:\n```bash\nnpm install -g @github/copilot\n```\n\n### Launching the CLI\n\n```bash\ncopilot\n```\n\nOn first launch, you'll be greeted with our adorable animated banner! If you'd like to see this banner again, launch `copilot` with the `--banner` flag. \n\nIf you're not currently logged in to GitHub, you'll be prompted to use the `/login` slash command. Enter this command and follow the on-screen instructions to authenticate.\n\n#### Authenticate with a Personal Access Token (PAT)\n\nYou can also authenticate using a fine-grained PAT with the \"Copilot Requests\" permission enabled.\n\n1. Visit https://github.com/settings/personal-access-tokens/new\n2. Under \"Permissions,\" click \"add permissions\" and select \"Copilot Requests\"\n3. Generate your token\n4. Add the token to your environment via the environment variable `GH_TOKEN` or `GITHUB_TOKEN` (in order of precedence)\n\n### Using the CLI\n\nLaunch `copilot` in a folder that contains code you want to work with. \n\nBy default, `copilot` utilizes Claude Sonnet 4.5. Run the `/model` slash command to choose from other available models, including Claude Sonnet 4 and GPT-5\n\nEach time you submit a prompt to GitHub Copilot CLI, your monthly quota of premium requests is reduced by one. For information about premium requests, see [About premium requests](https://docs.github.com/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests).\n\nFor more information about how to use the GitHub Copilot CLI, see [our official documentation](https://docs.github.com/copilot/concepts/agents/about-copilot-cli).\n\n\n## üì¢ Feedback and Participation\n\nWe're excited to have you join us early in the Copilot CLI journey.\n\nThis is an early-stage preview, and we're building quickly. Expect frequent updates--please keep your client up to date for the latest features and fixes!\n\nYour insights are invaluable! Open issue in this repo, join Discussions, and run `/feedback` from the CLI to submit a confidential feedback survey!\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/github/copilot-cli",
          "homepage": "",
          "language": null,
          "forks": 481,
          "open_issues": 311,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/9919?v=4",
      "velocity": 5814.6,
      "is_rising_star": true
    },
    {
      "id": "github-QuantumNous-new-api",
      "name": "new-api",
      "author": "QuantumNous",
      "description": "AIÊ®°ÂûãËÅöÂêàÁÆ°ÁêÜ‰∏≠ËΩ¨ÂàÜÂèëÁ≥ªÁªüÔºå‰∏Ä‰∏™Â∫îÁî®ÁÆ°ÁêÜÊÇ®ÁöÑÊâÄÊúâAIÊ®°ÂûãÔºåÊîØÊåÅÂ∞ÜÂ§öÁßçÂ§ßÊ®°ÂûãËΩ¨‰∏∫Áªü‰∏ÄÊ†ºÂºèË∞ÉÁî®ÔºåÊîØÊåÅOpenAI„ÄÅClaude„ÄÅGeminiÁ≠âÊ†ºÂºèÔºåÂèØ‰æõ‰∏™‰∫∫ÊàñËÄÖ‰ºÅ‰∏öÂÜÖÈÉ®ÁÆ°ÁêÜ‰∏éÂàÜÂèëÊ∏†ÈÅì‰ΩøÁî®„ÄÇüç• The next-generation LLM gateway and AI asset management system supports multiple languages.",
      "task": "tool",
      "tags": [
        "ai-gateway",
        "claude",
        "deepseek",
        "gemini",
        "openai",
        "rerank"
      ],
      "likes": 37572,
      "downloads": 37572,
      "lastModified": "2025-11-20T03:58:09Z",
      "lastModifiedTimestamp": 1763611089000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/QuantumNous/new-api",
          "homepage": "https://www.newapi.ai",
          "language": "JavaScript",
          "forks": 2411,
          "open_issues": 454,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/205075698?v=4",
      "velocity": 13754.4,
      "is_rising_star": true
    },
    {
      "id": "github-diet103-claude-code-infrastructure-showcase",
      "name": "claude-code-infrastructure-showcase",
      "author": "diet103",
      "description": "Examples of my Claude Code infrastructure with skill auto-activation, hooks, and agents",
      "task": "tool",
      "tags": [],
      "likes": 20563,
      "downloads": 20563,
      "lastModified": "2025-11-20T03:58:04Z",
      "lastModifiedTimestamp": 1763611084000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/diet103/claude-code-infrastructure-showcase",
          "homepage": null,
          "language": "Shell",
          "forks": 881,
          "open_issues": 13,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/215228613?v=4",
      "velocity": 7509.7,
      "is_rising_star": true
    },
    {
      "id": "github-Arindam200-awesome-ai-apps",
      "name": "awesome-ai-apps",
      "author": "Arindam200",
      "description": "A collection of projects showcasing RAG, agents, workflows, and other AI use cases",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "hacktoberfest",
        "llm",
        "mcp"
      ],
      "likes": 22488,
      "downloads": 22488,
      "lastModified": "2025-11-20T03:58:03Z",
      "lastModifiedTimestamp": 1763611083000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Arindam200/awesome-ai-apps",
          "homepage": "https://ggl.link/arindam-youtube",
          "language": "Python",
          "forks": 895,
          "open_issues": 26,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/109217591?v=4",
      "velocity": 8181.8,
      "is_rising_star": true
    },
    {
      "id": "github-QwenLM-qwen-code",
      "name": "qwen-code",
      "author": "QwenLM",
      "description": "Qwen Code is a coding agent that lives in the digital world.",
      "task": "tool",
      "tags": [],
      "likes": 47001,
      "downloads": 47001,
      "lastModified": "2025-11-20T03:57:56Z",
      "lastModifiedTimestamp": 1763611076000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/QwenLM/qwen-code",
          "homepage": "https://qwenlm.github.io/qwen-code-docs/zh/",
          "language": "TypeScript",
          "forks": 1298,
          "open_issues": 336,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/141221163?v=4",
      "velocity": 17211.7,
      "is_rising_star": true
    },
    {
      "id": "github-coder-coder",
      "name": "coder",
      "author": "coder",
      "description": "Secure environments for developers and their agents",
      "task": "tool",
      "tags": [
        "agents",
        "dev-tools",
        "development-environment",
        "go",
        "golang",
        "ide",
        "jetbrains",
        "remote-development",
        "terraform",
        "vscode"
      ],
      "likes": 34534,
      "downloads": 34534,
      "lastModified": "2025-11-20T03:57:55Z",
      "lastModifiedTimestamp": 1763611075000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/coder/coder",
          "homepage": "https://coder.com",
          "language": "Go",
          "forks": 1085,
          "open_issues": 770,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/95932066?v=4",
      "velocity": 12658.8,
      "is_rising_star": true
    },
    {
      "id": "github-langchain-ai-langchain",
      "name": "langchain",
      "author": "langchain-ai",
      "description": "ü¶úüîó The platform for reliable agents.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "ai-agents-framework",
        "aiagentframework",
        "anthropic",
        "chatgpt",
        "enterprise",
        "framework",
        "gemini",
        "generative-ai",
        "langchain",
        "llm",
        "multiagent",
        "open-source",
        "openai",
        "pydantic",
        "python",
        "rag"
      ],
      "likes": 360158,
      "downloads": 360158,
      "lastModified": "2025-11-20T03:57:49Z",
      "lastModifiedTimestamp": 1763611069000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langchain-ai/langchain",
          "homepage": "https://docs.langchain.com/oss/python/langchain/",
          "language": "Python",
          "forks": 19766,
          "open_issues": 225,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/126733545?v=4",
      "velocity": 132006.6,
      "is_rising_star": true
    },
    {
      "id": "github-datajuicer-data-juicer",
      "name": "data-juicer",
      "author": "datajuicer",
      "description": "Data processing for and with foundation models!  üçé üçã üåΩ ‚û°Ô∏è ‚û°Ô∏èüç∏ üçπ üç∑",
      "task": "tool",
      "tags": [
        "data",
        "data-analysis",
        "data-pipeline",
        "data-processing",
        "data-science",
        "data-visualization",
        "foundation-models",
        "instruction-tuning",
        "large-language-models",
        "llm",
        "llms",
        "multi-modal",
        "pre-training",
        "synthetic-data"
      ],
      "likes": 16588,
      "downloads": 16588,
      "lastModified": "2025-11-20T03:57:46Z",
      "lastModifiedTimestamp": 1763611066000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/datajuicer/data-juicer",
          "homepage": "https://datajuicer.github.io/data-juicer/",
          "language": "Python",
          "forks": 290,
          "open_issues": 66,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/223222708?v=4",
      "velocity": 6080.8,
      "is_rising_star": true
    },
    {
      "id": "github-aishwaryanr-awesome-generative-ai-guide",
      "name": "awesome-generative-ai-guide",
      "author": "aishwaryanr",
      "description": "A one stop repository for generative AI research updates, interview resources, notebooks and much more!",
      "task": "tool",
      "tags": [
        "awesome",
        "awesome-list",
        "generative-ai",
        "interview-questions",
        "large-language-models",
        "llms",
        "notebook-jupyter",
        "vision-and-language"
      ],
      "likes": 63944,
      "downloads": 63944,
      "lastModified": "2025-11-20T03:57:45Z",
      "lastModifiedTimestamp": 1763611065000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/aishwaryanr/awesome-generative-ai-guide",
          "homepage": "https://www.linkedin.com/in/areganti/",
          "language": null,
          "forks": 4605,
          "open_issues": 4,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/12550285?v=4",
      "velocity": 23315.6,
      "is_rising_star": true
    },
    {
      "id": "github-google-gemini-gemini-cli",
      "name": "gemini-cli",
      "author": "google-gemini",
      "description": "An open-source AI agent that brings the power of Gemini directly into your terminal.",
      "task": "tool",
      "tags": [
        "gemini",
        "gemini-api"
      ],
      "likes": 250395,
      "downloads": 250395,
      "lastModified": "2025-11-20T03:57:37Z",
      "lastModifiedTimestamp": 1763611057000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/google-gemini/gemini-cli",
          "homepage": "https://geminicli.com",
          "language": "TypeScript",
          "forks": 9384,
          "open_issues": 3007,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/161781182?v=4",
      "velocity": 91595.9,
      "is_rising_star": true
    },
    {
      "id": "github-AstrBotDevs-AstrBot",
      "name": "AstrBot",
      "author": "AstrBotDevs",
      "description": "‚ú® Agentic IM ChatBot Infrastructure ‚ú® Integration with multiple IMs, easy-to-use plugin system, supports OpenAI, Gemini, Anthropic, Dify, Coze, built-in Knowledge Base, Agent. ‚ú® ‰∏ÄÁ´ôÂºèÂ§ßÊ®°ÂûãËÅäÂ§©Êú∫Âô®‰∫∫Âπ≥Âè∞ÂèäÂºÄÂèëÊ°ÜÊû∂ ‚ú® Â§öÊ∂àÊÅØÂπ≥Âè∞ÔºàQQ, Telegram, ‰ºÅÂæÆ, È£û‰π¶, ÈíâÈíâÁ≠âÔºâÈõÜÊàêÔºåÊòìÁî®ÁöÑÊèí‰ª∂Á≥ªÁªüÔºåÊîØÊåÅÊé•ÂÖ• OpenAI, Gemini, Anthropic, Dify, Coze, ÈòøÈáå‰∫ëÁôæÁÇºÂ∫îÁî®Á≠âÂπ≥Âè∞ÔºåÂÜÖÁΩÆÁü•ËØÜÂ∫ì„ÄÅAgent Êô∫ËÉΩ‰Ωì",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "chatbot",
        "chatgpt",
        "docker",
        "gemini",
        "gpt",
        "llama",
        "llm",
        "mcp",
        "openai",
        "python",
        "qq",
        "qqbot",
        "qqchannel",
        "telegram"
      ],
      "likes": 40388,
      "downloads": 40388,
      "lastModified": "2025-11-20T03:57:34Z",
      "lastModifiedTimestamp": 1763611054000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/AstrBotDevs/AstrBot",
          "homepage": "https://astrbot.app",
          "language": "Python",
          "forks": 1002,
          "open_issues": 314,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/197911947?v=4",
      "velocity": 14792.8,
      "is_rising_star": true
    },
    {
      "id": "github-krillinai-KrillinAI",
      "name": "KrillinAI",
      "author": "krillinai",
      "description": "Video translation and dubbing tool powered by LLMs. The video translator offers 100 language translations and one-click full-process deployment. The video translation output is optimized for platforms like YouTubeÔºåTikTok.   AIËßÜÈ¢ëÁøªËØëÈÖçÈü≥Â∑•ÂÖ∑Ôºå100ÁßçËØ≠Ë®ÄÂèåÂêëÁøªËØëÔºå‰∏ÄÈîÆÈÉ®ÁΩ≤ÂÖ®ÊµÅÁ®ãÔºåÂèØ‰ª•ÁîüÊäñÈü≥ÔºåÂ∞èÁ∫¢‰π¶ÔºåÂìîÂì©ÂìîÂì©ÔºåËßÜÈ¢ëÂè∑ÔºåTikTokÔºåYoutubeÁ≠âÂΩ¢ÊÄÅÁöÑÂÜÖÂÆπÊàêÈÄÇÈÖç",
      "task": "tool",
      "tags": [
        "dubbing",
        "localization",
        "tts",
        "video-transcription",
        "video-translation"
      ],
      "likes": 26686,
      "downloads": 26686,
      "lastModified": "2025-11-20T03:57:32Z",
      "lastModifiedTimestamp": 1763611052000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/krillinai/KrillinAI",
          "homepage": "https://www.klic.studio",
          "language": "Go",
          "forks": 729,
          "open_issues": 17,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/2386538?v=4",
      "velocity": 9781.2,
      "is_rising_star": true
    },
    {
      "id": "github-unslothai-unsloth",
      "name": "unsloth",
      "author": "unslothai",
      "description": "Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.",
      "task": "tool",
      "tags": [
        "agent",
        "deepseek",
        "deepseek-r1",
        "fine-tuning",
        "gemma",
        "gemma3",
        "gpt-oss",
        "llama",
        "llama3",
        "llm",
        "llms",
        "mistral",
        "openai",
        "qwen",
        "qwen3",
        "reinforcement-learning",
        "text-to-speech",
        "tts",
        "unsloth",
        "voice-cloning"
      ],
      "likes": 145378,
      "downloads": 145378,
      "lastModified": "2025-11-20T03:57:06Z",
      "lastModifiedTimestamp": 1763611026000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unslothai/unsloth",
          "homepage": "https://docs.unsloth.ai/",
          "language": "Python",
          "forks": 3987,
          "open_issues": 862,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/150920049?v=4",
      "velocity": 53275.2,
      "is_rising_star": true
    },
    {
      "id": "github-sgl-project-sglang",
      "name": "sglang",
      "author": "sgl-project",
      "description": "SGLang is a fast serving framework for large language models and vision language models.",
      "task": "tool",
      "tags": [
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-r1",
        "deepseek-v3",
        "deepseek-v3-2",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llama3",
        "llava",
        "llm",
        "llm-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen3",
        "transformer",
        "vlm"
      ],
      "likes": 60739,
      "downloads": 60739,
      "lastModified": "2025-11-20T03:56:45Z",
      "lastModifiedTimestamp": 1763611005000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/sgl-project/sglang",
          "homepage": "https://docs.sglang.ai/",
          "language": "Python",
          "forks": 3445,
          "open_issues": 1429,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/147780389?v=4",
      "velocity": 22280.5,
      "is_rising_star": true
    },
    {
      "id": "github-HKUDS-LightRAG",
      "name": "LightRAG",
      "author": "HKUDS",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "task": "tool",
      "tags": [
        "genai",
        "gpt",
        "gpt-4",
        "graphrag",
        "knowledge-graph",
        "large-language-models",
        "llm",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 71270,
      "downloads": 71270,
      "lastModified": "2025-11-20T03:56:38Z",
      "lastModifiedTimestamp": 1763610998000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/HKUDS/LightRAG",
          "homepage": "https://arxiv.org/abs/2410.05779",
          "language": "Python",
          "forks": 3484,
          "open_issues": 170,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/118165258?v=4",
      "velocity": 26045.8,
      "is_rising_star": true
    },
    {
      "id": "github-Chainlit-chainlit",
      "name": "chainlit",
      "author": "Chainlit",
      "description": "Build Conversational AI in minutes ‚ö°Ô∏è",
      "task": "tool",
      "tags": [
        "chatgpt",
        "langchain",
        "llm",
        "openai",
        "openai-chatgpt",
        "python",
        "ui"
      ],
      "likes": 33048,
      "downloads": 33048,
      "lastModified": "2025-11-20T03:56:33Z",
      "lastModifiedTimestamp": 1763610993000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Chainlit/chainlit",
          "homepage": "https://docs.chainlit.io",
          "language": "TypeScript",
          "forks": 1577,
          "open_issues": 121,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128686189?v=4",
      "velocity": 12113.2,
      "is_rising_star": true
    },
    {
      "id": "github-pydantic-pydantic-ai",
      "name": "pydantic-ai",
      "author": "pydantic",
      "description": "GenAI Agent Framework, the Pydantic way",
      "task": "tool",
      "tags": [
        "agent-framework",
        "genai",
        "llm",
        "pydantic",
        "python"
      ],
      "likes": 40334,
      "downloads": 40334,
      "lastModified": "2025-11-20T03:56:26Z",
      "lastModifiedTimestamp": 1763610986000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pydantic/pydantic-ai",
          "homepage": "https://ai.pydantic.dev",
          "language": "Python",
          "forks": 1405,
          "open_issues": 353,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110818415?v=4",
      "velocity": 14779.6,
      "is_rising_star": true
    },
    {
      "id": "github-browseros-ai-BrowserOS",
      "name": "BrowserOS",
      "author": "browseros-ai",
      "description": "üåê The open-source Agentic browser; privacy-first alternative to ChatGPT Atlas, Perplexity Comet, Dia.",
      "task": "tool",
      "tags": [
        "browser",
        "browseros",
        "chromium",
        "hacktoberfest",
        "linux",
        "macos",
        "windows"
      ],
      "likes": 22888,
      "downloads": 22888,
      "lastModified": "2025-11-20T03:56:23Z",
      "lastModifiedTimestamp": 1763610983000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/browseros-ai/BrowserOS",
          "homepage": "https://BrowserOS.com",
          "language": "C++",
          "forks": 717,
          "open_issues": 40,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/218857586?v=4",
      "velocity": 8377.6,
      "is_rising_star": true
    },
    {
      "id": "github-opendatalab-MinerU",
      "name": "MinerU",
      "author": "opendatalab",
      "description": "Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.",
      "task": "tool",
      "tags": [
        "ai4science",
        "document-analysis",
        "extract-data",
        "layout-analysis",
        "ocr",
        "parser",
        "pdf",
        "pdf-converter",
        "pdf-extractor-llm",
        "pdf-extractor-pretrain",
        "pdf-extractor-rag",
        "pdf-parser",
        "python"
      ],
      "likes": 147376,
      "downloads": 147376,
      "lastModified": "2025-11-20T03:56:20Z",
      "lastModifiedTimestamp": 1763610980000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/opendatalab/MinerU",
          "homepage": "https://opendatalab.github.io/MinerU/",
          "language": "Python",
          "forks": 4072,
          "open_issues": 123,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/97503431?v=4",
      "velocity": 54001.2,
      "is_rising_star": true
    },
    {
      "id": "github-linshenkx-prompt-optimizer",
      "name": "prompt-optimizer",
      "author": "linshenkx",
      "description": "‰∏ÄÊ¨æÊèêÁ§∫ËØç‰ºòÂåñÂô®ÔºåÂä©Âäõ‰∫éÁºñÂÜôÈ´òË¥®ÈáèÁöÑÊèêÁ§∫ËØç",
      "task": "tool",
      "tags": [
        "llm",
        "prompt",
        "prompt-engineering",
        "prompt-optimization",
        "prompt-toolkit",
        "prompt-tuning"
      ],
      "likes": 51218,
      "downloads": 51218,
      "lastModified": "2025-11-20T03:56:12Z",
      "lastModifiedTimestamp": 1763610972000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/linshenkx/prompt-optimizer",
          "homepage": "https://prompt.always200.com",
          "language": "TypeScript",
          "forks": 2132,
          "open_issues": 25,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/32978552?v=4",
      "velocity": 18759.4,
      "is_rising_star": true
    },
    {
      "id": "github-bytedance-trae-agent",
      "name": "trae-agent",
      "author": "bytedance",
      "description": "Trae Agent is an LLM-based agent for general purpose software engineering tasks.",
      "task": "tool",
      "tags": [
        "agent",
        "llm",
        "software-engineering"
      ],
      "likes": 30153,
      "downloads": 30153,
      "lastModified": "2025-11-20T03:56:06Z",
      "lastModifiedTimestamp": 1763610966000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/bytedance/trae-agent",
          "homepage": "https://www.trae.ai/",
          "language": "Python",
          "forks": 1042,
          "open_issues": 93,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/4158466?v=4",
      "velocity": 11047.3,
      "is_rising_star": true
    },
    {
      "id": "github-openvinotoolkit-openvino",
      "name": "openvino",
      "author": "openvinotoolkit",
      "description": "OpenVINO‚Ñ¢ is an open source toolkit for optimizing and deploying AI inference",
      "task": "tool",
      "tags": [
        "ai",
        "computer-vision",
        "deep-learning",
        "deploy-ai",
        "diffusion-models",
        "generative-ai",
        "good-first-issue",
        "inference",
        "llm-inference",
        "natural-language-processing",
        "nlp",
        "openvino",
        "optimize-ai",
        "performance-boost",
        "recommendation-system",
        "speech-recognition",
        "stable-diffusion",
        "transformers",
        "yolo"
      ],
      "likes": 27674,
      "downloads": 27674,
      "lastModified": "2025-11-20T03:56:05Z",
      "lastModifiedTimestamp": 1763610965000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openvinotoolkit/openvino",
          "homepage": "https://docs.openvino.ai",
          "language": "C++",
          "forks": 2829,
          "open_issues": 559,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/55443902?v=4",
      "velocity": 10144.2,
      "is_rising_star": true
    },
    {
      "id": "github-ggml-org-llama.cpp",
      "name": "llama.cpp",
      "author": "ggml-org",
      "description": "LLM inference in C/C++",
      "task": "tool",
      "tags": [
        "ggml"
      ],
      "likes": 270241,
      "downloads": 270241,
      "lastModified": "2025-11-20T03:56:01Z",
      "lastModifiedTimestamp": 1763610961000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ggml-org/llama.cpp",
          "homepage": "",
          "language": "C++",
          "forks": 13747,
          "open_issues": 899,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134263123?v=4",
      "velocity": 99053.9,
      "is_rising_star": true
    },
    {
      "id": "github-aliasrobotics-cai",
      "name": "cai",
      "author": "aliasrobotics",
      "description": "Cybersecurity AI (CAI), the framework for AI Security",
      "task": "tool",
      "tags": [
        "artificial-intelligence",
        "cybersecurity",
        "framework",
        "generative-ai",
        "llm",
        "pentesting"
      ],
      "likes": 15965,
      "downloads": 15965,
      "lastModified": "2025-11-20T03:55:57Z",
      "lastModifiedTimestamp": 1763610957000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/aliasrobotics/cai",
          "homepage": "https://aliasrobotics.github.io/cai/",
          "language": "Python",
          "forks": 736,
          "open_issues": 6,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/26189319?v=4",
      "velocity": 5839.9,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-generative-ai-for-beginners",
      "name": "generative-ai-for-beginners",
      "author": "microsoft",
      "description": "21 Lessons, Get Started Building with Generative AI ",
      "task": "tool",
      "tags": [
        "ai",
        "azure",
        "chatgpt",
        "dall-e",
        "generative-ai",
        "generativeai",
        "gpt",
        "language-model",
        "llms",
        "microsoft-for-beginners",
        "openai",
        "prompt-engineering",
        "semantic-search",
        "transformers"
      ],
      "likes": 306061,
      "downloads": 306061,
      "lastModified": "2025-11-20T03:55:34Z",
      "lastModifiedTimestamp": 1763610934000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/generative-ai-for-beginners",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 54213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 112205.5,
      "is_rising_star": true
    },
    {
      "id": "github-cline-cline",
      "name": "cline",
      "author": "cline",
      "description": "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.",
      "task": "tool",
      "tags": [],
      "likes": 157519,
      "downloads": 157519,
      "lastModified": "2025-11-20T03:55:20Z",
      "lastModifiedTimestamp": 1763610920000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/cline/cline",
          "homepage": "https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev",
          "language": "TypeScript",
          "forks": 5242,
          "open_issues": 881,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/184127137?v=4",
      "velocity": 57740.1,
      "is_rising_star": true
    },
    {
      "id": "github-666ghj-BettaFish",
      "name": "BettaFish",
      "author": "666ghj",
      "description": "ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ",
      "task": "tool",
      "tags": [
        "agent-framework",
        "data-analysis",
        "deep-research",
        "deep-search",
        "llms",
        "multi-agent-system",
        "nlp",
        "public-opinion-analysis",
        "python3",
        "sentiment-analysis"
      ],
      "likes": 85074,
      "downloads": 85074,
      "lastModified": "2025-11-20T03:55:09Z",
      "lastModifiedTimestamp": 1763610909000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/666ghj/BettaFish",
          "homepage": "",
          "language": "Python",
          "forks": 5428,
          "open_issues": 58,
          "license": "GNU General Public License v2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110395318?v=4",
      "velocity": 31050.8,
      "is_rising_star": true
    },
    {
      "id": "github-ollama-ollama",
      "name": "ollama",
      "author": "ollama",
      "description": "Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.",
      "task": "tool",
      "tags": [
        "deepseek",
        "gemma",
        "gemma3",
        "gemma3n",
        "go",
        "golang",
        "gpt-oss",
        "llama",
        "llama2",
        "llama3",
        "llava",
        "llm",
        "llms",
        "mistral",
        "ollama",
        "phi4",
        "qwen"
      ],
      "likes": 468708,
      "downloads": 468708,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ollama/ollama",
          "homepage": "https://ollama.com",
          "language": "Go",
          "forks": 13681,
          "open_issues": 2256,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/151674099?v=4",
      "velocity": 171815.6,
      "is_rising_star": true
    },
    {
      "id": "github-infiniflow-ragflow",
      "name": "ragflow",
      "author": "infiniflow",
      "description": "RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs",
      "task": "tool",
      "tags": [
        "agent",
        "agentic",
        "agentic-ai",
        "agentic-workflow",
        "ai",
        "ai-search",
        "deep-learning",
        "deep-research",
        "deepseek",
        "deepseek-r1",
        "document-parser",
        "document-understanding",
        "graphrag",
        "llm",
        "mcp",
        "multi-agent",
        "ollama",
        "openai",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 204013,
      "downloads": 204013,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/infiniflow/ragflow",
          "homepage": "https://ragflow.io",
          "language": "Python",
          "forks": 7289,
          "open_issues": 2918,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/69962740?v=4",
      "velocity": 74763.7,
      "is_rising_star": true
    },
    {
      "id": "github-CherryHQ-cherry-studio",
      "name": "cherry-studio",
      "author": "CherryHQ",
      "description": "üçí Cherry Studio is a desktop client that supports for multiple LLM providers.",
      "task": "tool",
      "tags": [
        "agent",
        "anthropic",
        "assistant",
        "chatbot",
        "chatbotai",
        "electron",
        "llm",
        "mcp-client",
        "openai"
      ],
      "likes": 106775,
      "downloads": 106775,
      "lastModified": "2025-11-20T03:54:49Z",
      "lastModifiedTimestamp": 1763610889000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CherryHQ/cherry-studio",
          "homepage": "https://cherry-ai.com",
          "language": "TypeScript",
          "forks": 3227,
          "open_issues": 532,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/187777663?v=4",
      "velocity": 39125.9,
      "is_rising_star": true
    },
    {
      "id": "github-BerriAI-litellm",
      "name": "litellm",
      "author": "BerriAI",
      "description": "Python SDK, Proxy Server (AI Gateway) to call 100+ LLM APIs in OpenAI (or native) format, with cost tracking, guardrails, loadbalancing and logging. [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, VLLM, NVIDIA NIM]",
      "task": "tool",
      "tags": [
        "ai-gateway",
        "anthropic",
        "azure-openai",
        "bedrock",
        "gateway",
        "langchain",
        "litellm",
        "llm",
        "llm-gateway",
        "llmops",
        "mcp-gateway",
        "openai",
        "openai-proxy",
        "vertex-ai"
      ],
      "likes": 93970,
      "downloads": 93970,
      "lastModified": "2025-11-20T03:54:47Z",
      "lastModifiedTimestamp": 1763610887000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/BerriAI/litellm",
          "homepage": "https://docs.litellm.ai/docs/",
          "language": "Python",
          "forks": 4749,
          "open_issues": 1359,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/121462774?v=4",
      "velocity": 34423.4,
      "is_rising_star": true
    },
    {
      "id": "github-toon-format-toon",
      "name": "toon",
      "author": "toon-format",
      "description": "üéí Token-Oriented Object Notation (TOON) ‚Äì Compact, human-readable, schema-aware JSON for LLM prompts. Spec, benchmarks, TypeScript SDK.",
      "task": "tool",
      "tags": [
        "data-format",
        "llm",
        "serialization",
        "tokenization"
      ],
      "likes": 55067,
      "downloads": 55067,
      "lastModified": "2025-11-20T03:54:40Z",
      "lastModifiedTimestamp": 1763610880000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/toon-format/toon",
          "homepage": "https://toonformat.dev",
          "language": "TypeScript",
          "forks": 769,
          "open_issues": 24,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241380424?v=4",
      "velocity": 19994.7,
      "is_rising_star": true
    },
    {
      "id": "github-mlabonne-llm-course",
      "name": "llm-course",
      "author": "mlabonne",
      "description": "Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.",
      "task": "tool",
      "tags": [
        "course",
        "large-language-models",
        "llm",
        "machine-learning",
        "roadmap"
      ],
      "likes": 203252,
      "downloads": 203252,
      "lastModified": "2025-11-20T03:54:32Z",
      "lastModifiedTimestamp": 1763610872000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mlabonne/llm-course",
          "homepage": "https://mlabonne.github.io/blog/",
          "language": null,
          "forks": 7672,
          "open_issues": 76,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/81252890?v=4",
      "velocity": 74476.6,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-RD-Agent",
      "name": "RD-Agent",
      "author": "microsoft",
      "description": "Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "automation",
        "data-mining",
        "data-science",
        "development",
        "llm",
        "research"
      ],
      "likes": 28530,
      "downloads": 28530,
      "lastModified": "2025-11-20T03:54:30Z",
      "lastModifiedTimestamp": 1763610870000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/RD-Agent",
          "homepage": "https://rdagent.azurewebsites.net/",
          "language": "Python",
          "forks": 1013,
          "open_issues": 121,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 10452.2,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-qlib",
      "name": "qlib",
      "author": "microsoft",
      "description": "Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.",
      "task": "tool",
      "tags": [
        "algorithmic-trading",
        "auto-quant",
        "deep-learning",
        "finance",
        "fintech",
        "investment",
        "machine-learning",
        "paper",
        "platform",
        "python",
        "quant",
        "quant-dataset",
        "quant-models",
        "quantitative-finance",
        "quantitative-trading",
        "research",
        "research-paper",
        "stock-data"
      ],
      "likes": 101601,
      "downloads": 101601,
      "lastModified": "2025-11-20T03:54:28Z",
      "lastModifiedTimestamp": 1763610868000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/qlib",
          "homepage": "https://qlib.readthedocs.io/en/latest/",
          "language": "Python",
          "forks": 5237,
          "open_issues": 304,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 37231.7,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-transformers",
      "name": "transformers",
      "author": "huggingface",
      "description": "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
      "task": "tool",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "likes": 458185,
      "downloads": 458185,
      "lastModified": "2025-11-20T03:54:06Z",
      "lastModifiedTimestamp": 1763610846000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/transformers",
          "homepage": "https://huggingface.co/transformers",
          "language": "Python",
          "forks": 31170,
          "open_issues": 2129,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 167968.9,
      "is_rising_star": true
    },
    {
      "id": "github-sst-opencode",
      "name": "opencode",
      "author": "sst",
      "description": "The AI coding agent built for the terminal.",
      "task": "tool",
      "tags": [
        "ai",
        "claude",
        "code",
        "llm",
        "openai"
      ],
      "likes": 128450,
      "downloads": 128450,
      "lastModified": "2025-11-20T03:54:03Z",
      "lastModifiedTimestamp": 1763610843000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/sst/opencode",
          "homepage": "https://opencode.ai",
          "language": "TypeScript",
          "forks": 2658,
          "open_issues": 1451,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/opencode-ai/opencode",
          "homepage": "",
          "language": "Go",
          "forks": 806,
          "open_issues": 163,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/66570915?v=4",
      "velocity": 47018.4,
      "is_rising_star": true
    },
    {
      "id": "github-vllm-project-vllm",
      "name": "vllm",
      "author": "vllm-project",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "task": "tool",
      "tags": [
        "amd",
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-v3",
        "gpt",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llm",
        "llm-serving",
        "model-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen",
        "qwen3",
        "tpu",
        "transformer"
      ],
      "likes": 190450,
      "downloads": 190450,
      "lastModified": "2025-11-20T03:53:27Z",
      "lastModifiedTimestamp": 1763610807000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/vllm-project/vllm",
          "homepage": "https://docs.vllm.ai",
          "language": "Python",
          "forks": 11390,
          "open_issues": 3172,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/136984999?v=4",
      "velocity": 69779.6,
      "is_rising_star": true
    },
    {
      "id": "github-alibaba-spring-ai-alibaba",
      "name": "spring-ai-alibaba",
      "author": "alibaba",
      "description": "Agentic AI Framework for Java Developers",
      "task": "tool",
      "tags": [
        "agentic",
        "artificial-intelligence",
        "context-engineering",
        "graph",
        "java",
        "multi-agent",
        "reactagent",
        "spring-ai",
        "workflow"
      ],
      "likes": 21143,
      "downloads": 21143,
      "lastModified": "2025-11-20T03:53:19Z",
      "lastModifiedTimestamp": 1763610799000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/alibaba/spring-ai-alibaba",
          "homepage": "https://java2ai.com",
          "language": "Java",
          "forks": 1482,
          "open_issues": 340,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1961952?v=4",
      "velocity": 7736.3,
      "is_rising_star": true
    },
    {
      "id": "github-lobehub-lobe-chat",
      "name": "lobe-chat",
      "author": "lobehub",
      "description": "ü§Ø LobeHub - an open-source, modern design AI Agent Workspace. Supports multiple AI providers, Knowledge Base (file upload / RAG ), one click install MCP Marketplace and Artifacts / Thinking. One-click FREE deployment of your private AI Agent application.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "artifacts",
        "chat",
        "chatgpt",
        "claude",
        "deepseek",
        "deepseek-r1",
        "function-calling",
        "gemini",
        "gpt",
        "knowledge-base",
        "mcp",
        "nextjs",
        "ollama",
        "openai",
        "rag"
      ],
      "likes": 203567,
      "downloads": 203567,
      "lastModified": "2025-11-20T03:53:05Z",
      "lastModifiedTimestamp": 1763610785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/lobehub/lobe-chat",
          "homepage": "https://lobechat.com",
          "language": "TypeScript",
          "forks": 13985,
          "open_issues": 979,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131470832?v=4",
      "velocity": 74618.5,
      "is_rising_star": true
    },
    {
      "id": "github-RooCodeInc-Roo-Code",
      "name": "Roo-Code",
      "author": "RooCodeInc",
      "description": "Roo Code gives you a whole dev team of AI agents in your code editor.",
      "task": "tool",
      "tags": [],
      "likes": 62621,
      "downloads": 62621,
      "lastModified": "2025-11-20T03:52:58Z",
      "lastModifiedTimestamp": 1763610778000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/RooCodeInc/Roo-Code",
          "homepage": "https://roocode.com",
          "language": "TypeScript",
          "forks": 2531,
          "open_issues": 358,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/211522643?v=4",
      "velocity": 22951.5,
      "is_rising_star": true
    },
    {
      "id": "github-Skyvern-AI-skyvern",
      "name": "skyvern",
      "author": "Skyvern-AI",
      "description": "Automate browser based workflows with AI",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "automation",
        "browser",
        "browser-automation",
        "computer",
        "gpt",
        "llm",
        "playwright",
        "powerautomate",
        "puppeteer",
        "python",
        "rpa",
        "selenium",
        "vision",
        "workflow"
      ],
      "likes": 55306,
      "downloads": 55306,
      "lastModified": "2025-11-20T03:52:30Z",
      "lastModifiedTimestamp": 1763610750000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Skyvern-AI/skyvern",
          "homepage": "https://www.skyvern.com",
          "language": "Python",
          "forks": 1582,
          "open_issues": 200,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/141457985?v=4",
      "velocity": 20222.4,
      "is_rising_star": true
    },
    {
      "id": "github-dyad-sh-dyad",
      "name": "dyad",
      "author": "dyad-sh",
      "description": "Free, local, open-source AI app builder ‚ú® v0 / lovable / Bolt alternative üåü Star if you like it!",
      "task": "tool",
      "tags": [
        "ai-app-builder",
        "anthropic",
        "artificial-intelligence",
        "bolt",
        "deepseek",
        "gemini",
        "generative-ai",
        "github",
        "llm",
        "llms",
        "lovable",
        "nextjs",
        "ollama",
        "openai",
        "qwen",
        "react",
        "typescript",
        "v0",
        "vercel"
      ],
      "likes": 52818,
      "downloads": 52818,
      "lastModified": "2025-11-20T03:52:20Z",
      "lastModifiedTimestamp": 1763610740000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/dyad-sh/dyad",
          "homepage": "https://dyad.sh",
          "language": "TypeScript",
          "forks": 1931,
          "open_issues": 258,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/183970190?v=4",
      "velocity": 19364.4,
      "is_rising_star": true
    },
    {
      "id": "github-rasbt-LLMs-from-scratch",
      "name": "LLMs-from-scratch",
      "author": "rasbt",
      "description": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "deep-learning",
        "from-scratch",
        "generative-ai",
        "gpt",
        "language-model",
        "large-language-models",
        "llm",
        "machine-learning",
        "neural-networks",
        "python",
        "pytorch",
        "transformers"
      ],
      "likes": 237017,
      "downloads": 237017,
      "lastModified": "2025-11-20T03:52:19Z",
      "lastModifiedTimestamp": 1763610739000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/rasbt/LLMs-from-scratch",
          "homepage": "https://amzn.to/4fqvn0D",
          "language": "Jupyter Notebook",
          "forks": 11706,
          "open_issues": 0,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/5618407?v=4",
      "velocity": 86865.9,
      "is_rising_star": true
    },
    {
      "id": "github-shareAI-lab-analysis_claude_code",
      "name": "analysis_claude_code",
      "author": "shareAI-lab",
      "description": "Êú¨‰ªìÂ∫ìÂåÖÂê´ÂØπ Claude Code v1.0.33 ËøõË°åÈÄÜÂêëÂ∑•Á®ãÁöÑÂÆåÊï¥Á†îÁ©∂ÂíåÂàÜÊûêËµÑÊñô„ÄÇÂåÖÊã¨ÂØπÊ∑∑Ê∑ÜÊ∫ê‰ª£Á†ÅÁöÑÊ∑±Â∫¶ÊäÄÊúØÂàÜÊûê„ÄÅÁ≥ªÁªüÊû∂ÊûÑÊñáÊ°£Ôºå‰ª•ÂèäÈáçÊûÑ Claude      Code agent Á≥ªÁªüÁöÑÂÆûÁé∞ËìùÂõæ„ÄÇ‰∏ªË¶ÅÂèëÁé∞ÂåÖÊã¨ÂÆûÊó∂ Steering Êú∫Âà∂„ÄÅÂ§ö Agent      Êû∂ÊûÑ„ÄÅÊô∫ËÉΩ‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂíåÂ∑•ÂÖ∑ÊâßË°åÁÆ°ÈÅì„ÄÇËØ•È°πÁõÆ‰∏∫ÁêÜËß£Áé∞‰ª£ AI agent Á≥ªÁªüËÆæËÆ°ÂíåÂÆûÁé∞Êèê‰æõÊäÄÊúØÂèÇËÄÉ„ÄÇ",
      "task": "tool",
      "tags": [],
      "likes": 33921,
      "downloads": 33921,
      "lastModified": "2025-11-20T03:52:10Z",
      "lastModifiedTimestamp": 1763610730000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/shareAI-lab/analysis_claude_code",
          "homepage": "",
          "language": "JavaScript",
          "forks": 2958,
          "open_issues": 0,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/189210346?v=4",
      "velocity": 12426.7,
      "is_rising_star": true
    },
    {
      "id": "github-anthropics-claude-code",
      "name": "claude-code",
      "author": "anthropics",
      "description": "Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.",
      "task": "tool",
      "tags": [],
      "likes": 128639,
      "downloads": 128639,
      "lastModified": "2025-11-20T03:51:58Z",
      "lastModifiedTimestamp": 1763610718000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/anthropics/claude-code",
          "homepage": "https://code.claude.com/docs/en/overview",
          "language": "Shell",
          "forks": 2893,
          "open_issues": 5277,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "velocity": 47107.5,
      "is_rising_star": true
    },
    {
      "id": "github-openai-agents.md",
      "name": "agents.md",
      "author": "openai",
      "description": "AGENTS.md ‚Äî a simple, open format for guiding coding agents",
      "task": "tool",
      "tags": [],
      "likes": 25103,
      "downloads": 25103,
      "lastModified": "2025-11-20T03:51:50Z",
      "lastModifiedTimestamp": 1763610710000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openai/agents.md",
          "homepage": "https://agents.md",
          "language": "TypeScript",
          "forks": 650,
          "open_issues": 82,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14957082?v=4",
      "velocity": 9181.7,
      "is_rising_star": true
    },
    {
      "id": "github-grab-cursor-talk-to-figma-mcp",
      "name": "cursor-talk-to-figma-mcp",
      "author": "grab",
      "description": "TalkToFigma: MCP integration between Cursor and Figma, allowing Cursor Agentic AI to communicate with Figma for reading designs and modifying them programmatically.",
      "task": "tool",
      "tags": [
        "agent",
        "agentic",
        "agentic-ai",
        "ai",
        "ai-agents",
        "automation",
        "cursor",
        "design",
        "figma",
        "generative-ai",
        "llm",
        "llms",
        "mcp",
        "model-context-protocol"
      ],
      "likes": 17070,
      "downloads": 17070,
      "lastModified": "2025-11-20T03:51:49Z",
      "lastModifiedTimestamp": 1763610709000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/grab/cursor-talk-to-figma-mcp",
          "homepage": "https://x.com/sonnylazuardi/status/1901325190388428999",
          "language": "JavaScript",
          "forks": 594,
          "open_issues": 72,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/17284363?v=4",
      "velocity": 6256.8,
      "is_rising_star": true
    },
    {
      "id": "github-Hannibal046-Awesome-LLM",
      "name": "Awesome-LLM",
      "author": "Hannibal046",
      "description": "Awesome-LLM: a curated list of Large Language Model",
      "task": "tool",
      "tags": [],
      "likes": 76756,
      "downloads": 76756,
      "lastModified": "2025-11-20T03:51:48Z",
      "lastModifiedTimestamp": 1763610708000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Hannibal046/Awesome-LLM",
          "homepage": "",
          "language": null,
          "forks": 2186,
          "open_issues": 51,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/38466901?v=4",
      "velocity": 28138,
      "is_rising_star": true
    },
    {
      "id": "github-FoundationAgents-MetaGPT",
      "name": "MetaGPT",
      "author": "FoundationAgents",
      "description": "üåü The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "task": "tool",
      "tags": [
        "agent",
        "gpt",
        "llm",
        "metagpt",
        "multi-agent"
      ],
      "likes": 178722,
      "downloads": 178722,
      "lastModified": "2025-11-20T03:51:26Z",
      "lastModifiedTimestamp": 1763610686000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FoundationAgents/MetaGPT",
          "homepage": "https://mgx.dev/",
          "language": "Python",
          "forks": 7283,
          "open_issues": 58,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/198047230?v=4",
      "velocity": 65516,
      "is_rising_star": true
    },
    {
      "id": "github-trycua-cua",
      "name": "cua",
      "author": "trycua",
      "description": "Open-source infrastructure for Computer-Use Agents. Sandboxes, SDKs, and benchmarks to train and evaluate AI agents that can control full desktops (macOS, Linux, Windows).",
      "task": "tool",
      "tags": [
        "agent",
        "ai-agent",
        "apple",
        "computer-use",
        "computer-use-agent",
        "containerization",
        "cua",
        "desktop-automation",
        "hacktoberfest",
        "lume",
        "macos",
        "manus",
        "operator",
        "swift",
        "virtualization",
        "virtualization-framework",
        "windows",
        "windows-sandbox"
      ],
      "likes": 33893,
      "downloads": 33893,
      "lastModified": "2025-11-20T03:50:51Z",
      "lastModifiedTimestamp": 1763610651000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/trycua/cua",
          "homepage": "https://cua.ai",
          "language": "Python",
          "forks": 654,
          "open_issues": 74,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/191107687?v=4",
      "velocity": 12422.3,
      "is_rising_star": true
    },
    {
      "id": "github-tensorzero-tensorzero",
      "name": "tensorzero",
      "author": "tensorzero",
      "description": "TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.",
      "task": "tool",
      "tags": [
        "ai",
        "ai-engineering",
        "anthropic",
        "artificial-intelligence",
        "deep-learning",
        "genai",
        "generative-ai",
        "gpt",
        "large-language-models",
        "llama",
        "llm",
        "llmops",
        "llms",
        "machine-learning",
        "ml",
        "ml-engineering",
        "mlops",
        "openai",
        "python",
        "rust"
      ],
      "likes": 31721,
      "downloads": 31721,
      "lastModified": "2025-11-20T03:50:46Z",
      "lastModifiedTimestamp": 1763610646000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/tensorzero/tensorzero",
          "homepage": "https://tensorzero.com",
          "language": "Rust",
          "forks": 727,
          "open_issues": 419,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/148420822?v=4",
      "velocity": 11628.1,
      "is_rising_star": true
    },
    {
      "id": "github-mastra-ai-mastra",
      "name": "mastra",
      "author": "mastra-ai",
      "description": "The TypeScript AI agent framework. ‚ö° Assistants, RAG, observability. Supports any LLM: GPT-4, Claude, Gemini, Llama.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "chatbots",
        "evals",
        "javascript",
        "llm",
        "mcp",
        "nextjs",
        "nodejs",
        "reactjs",
        "tts",
        "typescript",
        "workflows"
      ],
      "likes": 55046,
      "downloads": 55046,
      "lastModified": "2025-11-20T03:50:20Z",
      "lastModifiedTimestamp": 1763610620000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mastra-ai/mastra",
          "homepage": "https://mastra.ai",
          "language": "TypeScript",
          "forks": 1294,
          "open_issues": 475,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/149120496?v=4",
      "velocity": 20167.4,
      "is_rising_star": true
    },
    {
      "id": "github-keploy-keploy",
      "name": "keploy",
      "author": "keploy",
      "description": "API, Integration, E2E Testing Agent for Developers that actually work. Generate tests, mocks/stubs for your APIs!",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "ai-testing-tool",
        "api-testing",
        "code-quality",
        "mock",
        "mock-data-generator",
        "mock-framework",
        "test-automation",
        "test-automation-framework",
        "test-generation",
        "testing",
        "testing-library",
        "testing-tool",
        "testing-tools"
      ],
      "likes": 38295,
      "downloads": 38295,
      "lastModified": "2025-11-20T03:50:20Z",
      "lastModifiedTimestamp": 1763610620000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/keploy/keploy",
          "homepage": "https://keploy.io",
          "language": "Go",
          "forks": 1790,
          "open_issues": 340,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/92252339?v=4",
      "velocity": 14028.3,
      "is_rising_star": true
    },
    {
      "id": "github-openai-openai-agents-python",
      "name": "openai-agents-python",
      "author": "openai",
      "description": "A lightweight, powerful framework for multi-agent workflows",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "framework",
        "llm",
        "openai",
        "python"
      ],
      "likes": 52215,
      "downloads": 52215,
      "lastModified": "2025-11-20T03:50:15Z",
      "lastModifiedTimestamp": 1763610615000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openai/openai-agents-python",
          "homepage": "https://openai.github.io/openai-agents-python/",
          "language": "Python",
          "forks": 2881,
          "open_issues": 163,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14957082?v=4",
      "velocity": 19132.3,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-LLM101n",
      "name": "LLM101n",
      "author": "karpathy",
      "description": "LLM101n: Let's build a Storyteller",
      "task": "tool",
      "tags": [],
      "likes": 106769,
      "downloads": 106769,
      "lastModified": "2025-11-20T03:50:12Z",
      "lastModifiedTimestamp": 1763610612000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/LLM101n",
          "homepage": "",
          "language": null,
          "forks": 1937,
          "open_issues": 19,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 39143.5,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-graphrag",
      "name": "graphrag",
      "author": "microsoft",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "task": "tool",
      "tags": [
        "gpt",
        "gpt-4",
        "gpt4",
        "graphrag",
        "llm",
        "llms",
        "rag"
      ],
      "likes": 87739,
      "downloads": 87739,
      "lastModified": "2025-11-20T03:49:55Z",
      "lastModifiedTimestamp": 1763610595000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/graphrag",
          "homepage": "https://microsoft.github.io/graphrag/",
          "language": "Python",
          "forks": 3075,
          "open_issues": 94,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 32151.9,
      "is_rising_star": true
    },
    {
      "id": "github-ChromeDevTools-chrome-devtools-mcp",
      "name": "chrome-devtools-mcp",
      "author": "ChromeDevTools",
      "description": "Chrome DevTools for coding agents",
      "task": "tool",
      "tags": [
        "browser",
        "chrome",
        "chrome-devtools",
        "debugging",
        "devtools",
        "mcp",
        "mcp-server",
        "puppeteer"
      ],
      "likes": 44819,
      "downloads": 44819,
      "lastModified": "2025-11-20T03:49:55Z",
      "lastModifiedTimestamp": 1763610595000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ChromeDevTools/chrome-devtools-mcp",
          "homepage": "https://npmjs.org/package/chrome-devtools-mcp",
          "language": "TypeScript",
          "forks": 909,
          "open_issues": 62,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/11260967?v=4",
      "velocity": 16388.9,
      "is_rising_star": true
    },
    {
      "id": "github-hsliuping-TradingAgents-CN",
      "name": "TradingAgents-CN",
      "author": "hsliuping",
      "description": "Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìLLMÁöÑ‰∏≠ÊñáÈáëËûç‰∫§ÊòìÊ°ÜÊû∂ - TradingAgents‰∏≠ÊñáÂ¢ûÂº∫Áâà",
      "task": "tool",
      "tags": [],
      "likes": 39045,
      "downloads": 39045,
      "lastModified": "2025-11-20T03:49:43Z",
      "lastModifiedTimestamp": 1763610583000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/hsliuping/TradingAgents-CN",
          "homepage": null,
          "language": "Python",
          "forks": 2802,
          "open_issues": 52,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128790537?v=4",
      "velocity": 14296.7,
      "is_rising_star": true
    },
    {
      "id": "github-jeinlee1991-chinese-llm-benchmark",
      "name": "chinese-llm-benchmark",
      "author": "jeinlee1991",
      "description": "ReLEËØÑÊµãÔºö‰∏≠ÊñáAIÂ§ßÊ®°ÂûãËÉΩÂäõËØÑÊµãÔºàÊåÅÁª≠Êõ¥Êñ∞ÔºâÔºöÁõÆÂâçÂ∑≤ÂõäÊã¨303‰∏™Â§ßÊ®°ÂûãÔºåË¶ÜÁõñchatgpt„ÄÅgpt-5„ÄÅo4-mini„ÄÅË∞∑Ê≠ågemini-2.5„ÄÅClaude4.5„ÄÅÊô∫Ë∞±GLM-Z1„ÄÅÊñáÂøÉ‰∏ÄË®Ä„ÄÅqwen3-max„ÄÅÁôæÂ∑ù„ÄÅËÆØÈ£ûÊòüÁÅ´„ÄÅÂïÜÊ±§senseChat„ÄÅminimaxÁ≠âÂïÜÁî®Ê®°ÂûãÔºå ‰ª•Âèäkimi-k2„ÄÅernie4.5„ÄÅminimax-M1„ÄÅDeepSeek-R1-0528„ÄÅdeepseek-v3.2„ÄÅqwen3-2507„ÄÅllama4„ÄÅGLM4.5„ÄÅgemma3„ÄÅmistralÁ≠âÂºÄÊ∫êÂ§ßÊ®°Âûã„ÄÇ‰∏ç‰ªÖÊèê‰æõÊéíË°åÊ¶úÔºå‰πüÊèê‰æõËßÑÊ®°Ë∂Ö200‰∏áÁöÑÂ§ßÊ®°ÂûãÁº∫Èô∑Â∫ìÔºÅÊñπ‰æøÂπøÂ§ßÁ§æÂå∫Á†îÁ©∂ÂàÜÊûê„ÄÅÊîπËøõÂ§ßÊ®°Âûã„ÄÇ",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "artificial-intelligence",
        "llm-agent",
        "llm-evaluation"
      ],
      "likes": 15402,
      "downloads": 15402,
      "lastModified": "2025-11-20T03:49:42Z",
      "lastModifiedTimestamp": 1763610582000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/jeinlee1991/chinese-llm-benchmark",
          "homepage": "https://nonelinear.com",
          "language": null,
          "forks": 206,
          "open_issues": 10,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/46815718?v=4",
      "velocity": 5645.2,
      "is_rising_star": true
    },
    {
      "id": "github-hiyouga-LLaMA-Factory",
      "name": "LLaMA-Factory",
      "author": "hiyouga",
      "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "deepseek",
        "fine-tuning",
        "gemma",
        "gpt",
        "instruction-tuning",
        "large-language-models",
        "llama",
        "llama3",
        "llm",
        "lora",
        "moe",
        "nlp",
        "peft",
        "qlora",
        "quantization",
        "qwen",
        "rlhf",
        "transformers"
      ],
      "likes": 188197,
      "downloads": 188197,
      "lastModified": "2025-11-20T03:49:20Z",
      "lastModifiedTimestamp": 1763610560000,
      "readme": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)\n[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)\n[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)\n\n[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory)\n[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align=\"center\" markdown=\"1\">\n\n### Supporters ‚ù§Ô∏è\n\n| <div style=\"text-align: center;\"><a href=\"https://warp.dev/llama-factory\"><img alt=\"Warp sponsorship\" width=\"400\" src=\"assets/sponsors/warp.jpg\"></a><br><a href=\"https://warp.dev/llama-factory\" style=\"font-size:larger;\">Warp, the agentic terminal for developers</a><br><a href=\"https://warp.dev/llama-factory\">Available for MacOS, Linux, & Windows</a> | <a href=\"https://serpapi.com\"><img alt=\"SerpAPI sponsorship\" width=\"250\" src=\"assets/sponsors/serpapi.svg\"> </a> |\n| ---- | ---- |\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\nüëã Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.\n\n\\[ English | [‰∏≠Êñá](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nStart local training:\n- Please refer to [usage](#getting-started)\n\nStart cloud training:\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory\n- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\nRead technical notes:\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/\n- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html\n- **Official Blog**: https://blog.llamafactory.net/en/\n- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [LLaMA Factory Online](#llama-factory-online)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                           |\n| ------------ | -------------------------------------------------------------------- |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |\n\n## Blogs\n\n> [!TIP]\n> Now we have a dedicated blog for LLaMA Factory!\n>\n> Website: https://blog.llamafactory.net/en/\n\n- üí° [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&type=project&utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)\n- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n\n<details><summary>All Blogs</summary>\n\n- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n- [A One-Stop Code-Free Model Fine-Tuning \\& Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.\n\n[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.\n\n[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.\n\n[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.\n\n<details><summary>Full Changelog</summary>\n\n[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)'s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)'s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)'s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template             |\n| ----------------------------------------------------------------- | -------------------------------- | -------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2            |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                    |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3             |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere               |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek             |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3            |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1           |\n| [ERNIE-4.5](https://huggingface.co/baidu)                         | 0.3B/21B/300B                    | ernie/ernie_nothink  |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon               |\n| [Falcon-H1](https://huggingface.co/tiiuae)                        | 0.5B/1.5B/3B/7B/34B              | falcon_h1            |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma/gemma2         |\n| [Gemma 3/Gemma 3n](https://huggingface.co/google)                 | 270M/1B/4B/6B/8B/12B/27B         | gemma3/gemma3n       |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/zai-org)         | 9B/32B                           | glm4/glmz1           |\n| [GLM-4.1V](https://huggingface.co/zai-org)                        | 9B                               | glm4v                |\n| [GLM-4.5/GLM-4.5V](https://huggingface.co/zai-org)                | 106B/355B                        | glm4_moe/glm4v_moe   |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                    |\n| [GPT-OSS](https://huggingface.co/openai)                          | 20B/120B                         | gpt                  |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3             |\n| [Granite 4](https://huggingface.co/ibm-granite)                   | 7B                               | granite4             |\n| [Hunyuan (MT)](https://huggingface.co/tencent/)                   | 7B                               | hunyuan              |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index                |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2              |\n| [InternVL 2.5-3.5](https://huggingface.co/OpenGVLab)              | 1B/2B/4B/8B/14B/30B/38B/78B/241B | intern_vl            |\n| [InternLM/Intern-S1-mini](https://huggingface.co/internlm/)       | 8B                               | intern_s1            |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl              |\n| [Ling 2.0 (mini/flash)](https://huggingface.co/inclusionAI)       | 16B/100B                         | bailing_v2           |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                    |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2               |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3               |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4               |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama               |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava                |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next           |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video     |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                 |\n| [MiniCPM 1-4.1](https://huggingface.co/openbmb)                   | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4        |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v  |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral            |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral              |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small        |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                    |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma            |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                    |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                  |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small            |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                 |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral              |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                 |\n| [Qwen3 (MoE/Instruct/Thinking/Next)](https://huggingface.co/Qwen) | 0.6B/1.7B/4B/8B/14B/32B/80B/235B | qwen3/qwen3_nothink  |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio          |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni           |\n| [Qwen3-Omni](https://huggingface.co/Qwen)                         | 30B                              | qwen3_omni           |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl             |\n| [Qwen3-VL](https://huggingface.co/Qwen)                           | 2B/4B/8B/30B/32B/235B            | qwen3_vl             |\n| [Seed (OSS/Coder)](https://huggingface.co/ByteDance-Seed)         | 8B/36B                           | seed_oss/seed_coder  |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1           |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                    |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2            |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse               |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                   |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl                |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                 |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> If the model has both reasoning and non-reasoning versions, please use the `_nothink` suffix to distinguish between them. For example, `qwen3` and `qwen3_nothink`.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> \\*: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> \\*\\*: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |        OFT         |        QOFT        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [CCI3-HQ (zh)](https://huggingface.co/datasets/BAAI/CCI3-HQ)\n- [CCI3-Data (zh)](https://huggingface.co/datasets/BAAI/CCI3-Data)\n- [CCI4.0-M2-Base-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1)\n- [CCI4.0-M2-CoT-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1)\n- [CCI4.0-M2-Extra-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [Infinity Instruct (zh)](https://huggingface.co/datasets/BAAI/Infinity-Instruct)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install \"huggingface_hub<1.0.0\"\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.49.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method                              | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ----------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)             |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)                  |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam/OFT |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA / QOFT                        |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA / QOFT                        |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA / QOFT                        |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n#### Install from Source\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev\n\n#### Install from Docker Image\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nThis image is built on Ubuntu 22.04 (x86\\_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.\n\nFind the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags\n\nPlease refer to [build docker](#build-docker) to build the image yourself.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can't pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**, **[DataFlow](https://github.com/OpenDCAI/DataFlow)** and **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### LLaMA Factory Online\n\nRead our [documentation](https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory).\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Use Docker volumes</summary>\n\nYou can uncomment `VOLUME [ \"/root/.cache/huggingface\", \"/app/shared_data\", \"/app/output\" ]` in the Dockerfile to use data volumes.\n\nWhen building the Docker image, use `-v ./hf_cache:/root/.cache/huggingface` argument to mount the local directory to the container. The following data volumes are available.\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine.\n- `shared_data`: The directionary to store datasets on the host machine.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [[paper]](https://aclanthology.org/2024.findings-acl.830.pdf)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n1. **[EmoLLM](https://github.com/SmartFlowAI/EmoLLM)**: A project about large language models (LLMs) and mental health.\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/hiyouga/LLaMA-Factory",
          "homepage": "https://llamafactory.readthedocs.io",
          "language": "Python",
          "forks": 7592,
          "open_issues": 782,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/16256802?v=4",
      "velocity": 68968.9,
      "is_rising_star": true
    },
    {
      "id": "github-evidentlyai-evidently",
      "name": "evidently",
      "author": "evidentlyai",
      "description": "Evidently is ‚Äã‚Äãan open-source ML and LLM observability framework. Evaluate, test, and monitor any AI-powered system or data pipeline. From tabular data to Gen AI. 100+ metrics.",
      "task": "tool",
      "tags": [
        "data-drift",
        "data-quality",
        "data-science",
        "data-validation",
        "generative-ai",
        "hacktoberfest",
        "html-report",
        "jupyter-notebook",
        "llm",
        "llmops",
        "machine-learning",
        "mlops",
        "model-monitoring",
        "pandas-dataframe"
      ],
      "likes": 20501,
      "downloads": 20501,
      "lastModified": "2025-11-20T03:49:01Z",
      "lastModifiedTimestamp": 1763610541000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/evidentlyai/evidently",
          "homepage": "https://discord.gg/xZjKRaNp8b",
          "language": "Jupyter Notebook",
          "forks": 748,
          "open_issues": 231,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/75031056?v=4",
      "velocity": 7514.1,
      "is_rising_star": true
    },
    {
      "id": "github-eugeneyan-open-llms",
      "name": "open-llms",
      "author": "eugeneyan",
      "description": "üìã A list of open LLMs available for commercial use.",
      "task": "tool",
      "tags": [
        "commercial",
        "large-language-models",
        "llm",
        "llms"
      ],
      "likes": 37550,
      "downloads": 37550,
      "lastModified": "2025-11-20T03:48:59Z",
      "lastModifiedTimestamp": 1763610539000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/eugeneyan/open-llms",
          "homepage": "",
          "language": null,
          "forks": 932,
          "open_issues": 5,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6831355?v=4",
      "velocity": 13765.4,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-agent-lightning",
      "name": "agent-lightning",
      "author": "microsoft",
      "description": "The absolute trainer to light up AI agents.",
      "task": "tool",
      "tags": [
        "agent",
        "agentic-ai",
        "llm",
        "mlops",
        "reinforcement-learning"
      ],
      "likes": 25875,
      "downloads": 25875,
      "lastModified": "2025-11-20T03:48:38Z",
      "lastModifiedTimestamp": 1763610518000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/agent-lightning",
          "homepage": "https://microsoft.github.io/agent-lightning/",
          "language": "Python",
          "forks": 686,
          "open_issues": 74,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 9458.9,
      "is_rising_star": true
    },
    {
      "id": "github-agent0ai-agent-zero",
      "name": "agent-zero",
      "author": "agent0ai",
      "description": "Agent Zero AI framework",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "assistant",
        "autonomous",
        "linux",
        "zero"
      ],
      "likes": 36896,
      "downloads": 36896,
      "lastModified": "2025-11-20T03:48:21Z",
      "lastModifiedTimestamp": 1763610501000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/agent0ai/agent-zero",
          "homepage": "https://agent-zero.ai",
          "language": "Python",
          "forks": 2410,
          "open_issues": 240,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/216033749?v=4",
      "velocity": 13521.2,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-peft",
      "name": "peft",
      "author": "huggingface",
      "description": "ü§ó PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.",
      "task": "tool",
      "tags": [
        "adapter",
        "diffusion",
        "fine-tuning",
        "llm",
        "lora",
        "parameter-efficient-learning",
        "peft",
        "python",
        "pytorch",
        "transformers"
      ],
      "likes": 60246,
      "downloads": 60246,
      "lastModified": "2025-11-20T03:48:18Z",
      "lastModifiedTimestamp": 1763610498000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/peft",
          "homepage": "https://huggingface.co/docs/peft",
          "language": "Python",
          "forks": 2102,
          "open_issues": 54,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 22085.8,
      "is_rising_star": true
    },
    {
      "id": "github-chaitin-PandaWiki",
      "name": "PandaWiki",
      "author": "chaitin",
      "description": "PandaWiki ÊòØ‰∏ÄÊ¨æ AI Â§ßÊ®°ÂûãÈ©±Âä®ÁöÑÂºÄÊ∫êÁü•ËØÜÂ∫ìÊê≠Âª∫Á≥ªÁªüÔºåÂ∏ÆÂä©‰Ω†Âø´ÈÄüÊûÑÂª∫Êô∫ËÉΩÂåñÁöÑ ‰∫ßÂìÅÊñáÊ°£„ÄÅÊäÄÊúØÊñáÊ°£„ÄÅFAQ„ÄÅÂçöÂÆ¢Á≥ªÁªüÔºåÂÄüÂä©Â§ßÊ®°ÂûãÁöÑÂäõÈáè‰∏∫‰Ω†Êèê‰æõ AI Âàõ‰Ωú„ÄÅAI ÈóÆÁ≠î„ÄÅAI ÊêúÁ¥¢Á≠âËÉΩÂäõ„ÄÇ",
      "task": "tool",
      "tags": [
        "ai",
        "docs",
        "document",
        "documentation",
        "kb",
        "knownledge",
        "llm",
        "self-hosted",
        "wiki"
      ],
      "likes": 23999,
      "downloads": 23999,
      "lastModified": "2025-11-20T03:48:16Z",
      "lastModifiedTimestamp": 1763610496000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chaitin/PandaWiki",
          "homepage": "https://pandawiki.docs.baizhi.cloud/",
          "language": "TypeScript",
          "forks": 708,
          "open_issues": 347,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/7302766?v=4",
      "velocity": 8790.1,
      "is_rising_star": true
    },
    {
      "id": "github-cloudwego-eino",
      "name": "eino",
      "author": "cloudwego",
      "description": "The ultimate LLM/AI application development framework in Golang.",
      "task": "tool",
      "tags": [
        "ai",
        "ai-application",
        "ai-framework",
        "langchain",
        "langchain-for-go",
        "langchaingo",
        "llm-application"
      ],
      "likes": 24735,
      "downloads": 24735,
      "lastModified": "2025-11-20T03:48:13Z",
      "lastModifiedTimestamp": 1763610493000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/cloudwego/eino",
          "homepage": "https://www.cloudwego.io/docs/eino/",
          "language": "Go",
          "forks": 623,
          "open_issues": 86,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/79236453?v=4",
      "velocity": 9060.7,
      "is_rising_star": true
    },
    {
      "id": "github-linkedin-Liger-Kernel",
      "name": "Liger-Kernel",
      "author": "linkedin",
      "description": "Efficient Triton Kernels for LLM Training",
      "task": "tool",
      "tags": [
        "finetuning",
        "gemma2",
        "hacktoberfest",
        "llama",
        "llama3",
        "llm-training",
        "llms",
        "mistral",
        "phi3",
        "triton",
        "triton-kernels"
      ],
      "likes": 17546,
      "downloads": 17546,
      "lastModified": "2025-11-20T03:47:49Z",
      "lastModifiedTimestamp": 1763610469000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/linkedin/Liger-Kernel",
          "homepage": "https://openreview.net/pdf?id=36SjAIT42G",
          "language": "Python",
          "forks": 433,
          "open_issues": 110,
          "license": "BSD 2-Clause \"Simplified\" License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/357098?v=4",
      "velocity": 6430.6,
      "is_rising_star": true
    },
    {
      "id": "github-openai-codex",
      "name": "codex",
      "author": "openai",
      "description": "Lightweight coding agent that runs in your terminal",
      "task": "tool",
      "tags": [],
      "likes": 152654,
      "downloads": 152654,
      "lastModified": "2025-11-20T03:47:36Z",
      "lastModifiedTimestamp": 1763610456000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openai/codex",
          "homepage": "",
          "language": "Rust",
          "forks": 6374,
          "open_issues": 1043,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14957082?v=4",
      "velocity": 55924,
      "is_rising_star": true
    },
    {
      "id": "github-TauricResearch-TradingAgents",
      "name": "TradingAgents",
      "author": "TauricResearch",
      "description": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "task": "tool",
      "tags": [
        "agent",
        "finance",
        "llm",
        "multiagent",
        "trading"
      ],
      "likes": 75667,
      "downloads": 75667,
      "lastModified": "2025-11-20T03:47:34Z",
      "lastModifiedTimestamp": 1763610454000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/TauricResearch/TradingAgents",
          "homepage": "https://arxiv.org/pdf/2412.20138",
          "language": "Python",
          "forks": 4704,
          "open_issues": 196,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192884433?v=4",
      "velocity": 27716.7,
      "is_rising_star": true
    },
    {
      "id": "github-qax-os-excelize",
      "name": "excelize",
      "author": "qax-os",
      "description": "Go language library for reading and writing Microsoft Excel‚Ñ¢ (XLAM / XLSM / XLSX / XLTM / XLTX) spreadsheets",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "analytics",
        "chart",
        "ecma-376",
        "excel",
        "excelize",
        "formula",
        "go",
        "mcp",
        "microsoft",
        "office",
        "ooxml",
        "spreadsheet",
        "statistics",
        "table",
        "vba",
        "visualization",
        "xlsx",
        "xml"
      ],
      "likes": 59846,
      "downloads": 59846,
      "lastModified": "2025-11-20T03:47:21Z",
      "lastModifiedTimestamp": 1763610441000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/qax-os/excelize",
          "homepage": "https://xuri.me/excelize",
          "language": "Go",
          "forks": 1852,
          "open_issues": 134,
          "license": "BSD 3-Clause \"New\" or \"Revised\" License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/29733149?v=4",
      "velocity": 21938.4,
      "is_rising_star": true
    },
    {
      "id": "github-upstash-context7",
      "name": "context7",
      "author": "upstash",
      "description": "Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors",
      "task": "tool",
      "tags": [
        "llm",
        "mcp",
        "mcp-server",
        "vibe-coding"
      ],
      "likes": 112502,
      "downloads": 112502,
      "lastModified": "2025-11-20T03:47:16Z",
      "lastModifiedTimestamp": 1763610436000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/upstash/context7",
          "homepage": "https://context7.com",
          "language": "JavaScript",
          "forks": 1852,
          "open_issues": 100,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/74989412?v=4",
      "velocity": 41188.4,
      "is_rising_star": true
    },
    {
      "id": "github-xming521-WeClone",
      "name": "WeClone",
      "author": "xming521",
      "description": "üöÄ One-stop solution for creating your digital avatar from chat history üí° Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life.  ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°à  ",
      "task": "tool",
      "tags": [
        "chat-history",
        "digital-avatar",
        "llm",
        "qwen",
        "telegram"
      ],
      "likes": 47334,
      "downloads": 47334,
      "lastModified": "2025-11-20T03:46:56Z",
      "lastModifiedTimestamp": 1763610416000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/xming521/WeClone",
          "homepage": "https://weclone.love",
          "language": "Python",
          "forks": 1254,
          "open_issues": 36,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/32786500?v=4",
      "velocity": 17355.8,
      "is_rising_star": true
    },
    {
      "id": "github-Farama-Foundation-Gymnasium",
      "name": "Gymnasium",
      "author": "Farama-Foundation",
      "description": "An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)",
      "task": "tool",
      "tags": [
        "api",
        "gym",
        "reinforcement-learning"
      ],
      "likes": 32035,
      "downloads": 32035,
      "lastModified": "2025-11-20T03:46:53Z",
      "lastModifiedTimestamp": 1763610413000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Farama-Foundation/Gymnasium",
          "homepage": "https://gymnasium.farama.org",
          "language": "Python",
          "forks": 1190,
          "open_issues": 81,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/62961550?v=4",
      "velocity": 11738.1,
      "is_rising_star": true
    },
    {
      "id": "github-lyogavin-airllm",
      "name": "airllm",
      "author": "lyogavin",
      "description": "AirLLM 70B inference with single 4GB GPU",
      "task": "tool",
      "tags": [
        "chinese-llm",
        "chinese-nlp",
        "finetune",
        "generative-ai",
        "instruct-gpt",
        "instruction-set",
        "llama",
        "llm",
        "lora",
        "open-models",
        "open-source",
        "open-source-models",
        "qlora"
      ],
      "likes": 19075,
      "downloads": 19075,
      "lastModified": "2025-11-20T03:46:39Z",
      "lastModifiedTimestamp": 1763610399000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/lyogavin/airllm",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 493,
          "open_issues": 115,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1113905?v=4",
      "velocity": 6957.5,
      "is_rising_star": true
    },
    {
      "id": "github-assafelovic-gpt-researcher",
      "name": "gpt-researcher",
      "author": "assafelovic",
      "description": "An LLM agent that conducts deep research (local and web) on any given topic and generates a long report with citations.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "automation",
        "deepresearch",
        "llms",
        "mcp",
        "mcp-server",
        "python",
        "research",
        "search",
        "webscraping"
      ],
      "likes": 72613,
      "downloads": 72613,
      "lastModified": "2025-11-20T03:46:16Z",
      "lastModifiedTimestamp": 1763610376000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/assafelovic/gpt-researcher",
          "homepage": "https://gptr.dev",
          "language": "Python",
          "forks": 3196,
          "open_issues": 149,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13554167?v=4",
      "velocity": 26610.1,
      "is_rising_star": true
    },
    {
      "id": "github-Mintplex-Labs-anything-llm",
      "name": "anything-llm",
      "author": "Mintplex-Labs",
      "description": "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "custom-ai-agents",
        "deepseek",
        "kimi",
        "llama3",
        "llm",
        "lmstudio",
        "local-llm",
        "localai",
        "mcp",
        "mcp-servers",
        "moonshot",
        "multimodal",
        "no-code",
        "ollama",
        "qwen3",
        "rag",
        "vector-database",
        "web-scraping"
      ],
      "likes": 153633,
      "downloads": 153633,
      "lastModified": "2025-11-20T03:46:02Z",
      "lastModifiedTimestamp": 1763610362000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Mintplex-Labs/anything-llm",
          "homepage": "https://anythingllm.com",
          "language": "JavaScript",
          "forks": 5414,
          "open_issues": 345,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134426827?v=4",
      "velocity": 56316.7,
      "is_rising_star": true
    },
    {
      "id": "github-humanlayer-12-factor-agents",
      "name": "12-factor-agents",
      "author": "humanlayer",
      "description": "What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?",
      "task": "tool",
      "tags": [
        "12-factor",
        "12-factor-agents",
        "agents",
        "ai",
        "context-window",
        "framework",
        "llms",
        "memory",
        "orchestration",
        "prompt-engineering",
        "rag"
      ],
      "likes": 48842,
      "downloads": 48842,
      "lastModified": "2025-11-20T03:45:55Z",
      "lastModifiedTimestamp": 1763610355000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/humanlayer/12-factor-agents",
          "homepage": "",
          "language": "TypeScript",
          "forks": 1246,
          "open_issues": 15,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/177409041?v=4",
      "velocity": 17894.8,
      "is_rising_star": true
    },
    {
      "id": "github-FlowiseAI-Flowise",
      "name": "Flowise",
      "author": "FlowiseAI",
      "description": "Build AI Agents, Visually",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-workflow",
        "agents",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "javascript",
        "langchain",
        "large-language-models",
        "low-code",
        "multiagent-systems",
        "no-code",
        "openai",
        "rag",
        "react",
        "typescript",
        "workflow-automation"
      ],
      "likes": 140056,
      "downloads": 140056,
      "lastModified": "2025-11-20T03:45:42Z",
      "lastModifiedTimestamp": 1763610342000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FlowiseAI/Flowise",
          "homepage": "https://flowiseai.com",
          "language": "TypeScript",
          "forks": 23126,
          "open_issues": 723,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128289781?v=4",
      "velocity": 51339.2,
      "is_rising_star": true
    },
    {
      "id": "github-langflow-ai-langflow",
      "name": "langflow",
      "author": "langflow-ai",
      "description": "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.",
      "task": "tool",
      "tags": [
        "agents",
        "chatgpt",
        "generative-ai",
        "large-language-models",
        "multiagent",
        "react-flow"
      ],
      "likes": 416201,
      "downloads": 416201,
      "lastModified": "2025-11-20T03:45:31Z",
      "lastModifiedTimestamp": 1763610331000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langflow-ai/langflow",
          "homepage": "http://www.langflow.org",
          "language": "Python",
          "forks": 8014,
          "open_issues": 893,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85702467?v=4",
      "velocity": 152544.7,
      "is_rising_star": true
    },
    {
      "id": "github-transitive-bullshit-agentic",
      "name": "agentic",
      "author": "transitive-bullshit",
      "description": "Your API ‚áí Paid MCP. Instantly.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "llms",
        "openai"
      ],
      "likes": 54113,
      "downloads": 54113,
      "lastModified": "2025-11-20T03:45:30Z",
      "lastModifiedTimestamp": 1763610330000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/transitive-bullshit/agentic",
          "homepage": "https://agentic.so/publishing",
          "language": "TypeScript",
          "forks": 2247,
          "open_issues": 15,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/552829?v=4",
      "velocity": 19840.7,
      "is_rising_star": true
    }
  ],
  "rising": [
    {
      "id": "github-ollama-ollama",
      "name": "ollama",
      "author": "ollama",
      "description": "Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.",
      "task": "tool",
      "tags": [
        "deepseek",
        "gemma",
        "gemma3",
        "gemma3n",
        "go",
        "golang",
        "gpt-oss",
        "llama",
        "llama2",
        "llama3",
        "llava",
        "llm",
        "llms",
        "mistral",
        "ollama",
        "phi4",
        "qwen"
      ],
      "likes": 468708,
      "downloads": 468708,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ollama/ollama",
          "homepage": "https://ollama.com",
          "language": "Go",
          "forks": 13681,
          "open_issues": 2256,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/151674099?v=4",
      "velocity": 171815.6,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-transformers",
      "name": "transformers",
      "author": "huggingface",
      "description": "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
      "task": "tool",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "likes": 458185,
      "downloads": 458185,
      "lastModified": "2025-11-20T03:54:06Z",
      "lastModifiedTimestamp": 1763610846000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/transformers",
          "homepage": "https://huggingface.co/transformers",
          "language": "Python",
          "forks": 31170,
          "open_issues": 2129,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 167968.9,
      "is_rising_star": true
    },
    {
      "id": "github-langflow-ai-langflow",
      "name": "langflow",
      "author": "langflow-ai",
      "description": "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.",
      "task": "tool",
      "tags": [
        "agents",
        "chatgpt",
        "generative-ai",
        "large-language-models",
        "multiagent",
        "react-flow"
      ],
      "likes": 416201,
      "downloads": 416201,
      "lastModified": "2025-11-20T03:45:31Z",
      "lastModifiedTimestamp": 1763610331000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langflow-ai/langflow",
          "homepage": "http://www.langflow.org",
          "language": "Python",
          "forks": 8014,
          "open_issues": 893,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85702467?v=4",
      "velocity": 152544.7,
      "is_rising_star": true
    },
    {
      "id": "github-f-awesome-chatgpt-prompts",
      "name": "awesome-chatgpt-prompts",
      "author": "f",
      "description": "This repo includes ChatGPT prompt curation to use ChatGPT and other LLM tools better.",
      "task": "tool",
      "tags": [
        "bots",
        "chatbot",
        "chatgpt",
        "chatgpt-api",
        "language"
      ],
      "likes": 410025,
      "downloads": 410025,
      "lastModified": "2025-11-20T03:59:38Z",
      "lastModifiedTimestamp": 1763611178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/f/awesome-chatgpt-prompts",
          "homepage": "https://prompts.chat",
          "language": "JavaScript",
          "forks": 18173,
          "open_issues": 289,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/196477?v=4",
      "velocity": 150315,
      "is_rising_star": true
    },
    {
      "id": "github-langchain-ai-langchain",
      "name": "langchain",
      "author": "langchain-ai",
      "description": "ü¶úüîó The platform for reliable agents.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "ai-agents-framework",
        "aiagentframework",
        "anthropic",
        "chatgpt",
        "enterprise",
        "framework",
        "gemini",
        "generative-ai",
        "langchain",
        "llm",
        "multiagent",
        "open-source",
        "openai",
        "pydantic",
        "python",
        "rag"
      ],
      "likes": 360158,
      "downloads": 360158,
      "lastModified": "2025-11-20T03:57:49Z",
      "lastModifiedTimestamp": 1763611069000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langchain-ai/langchain",
          "homepage": "https://docs.langchain.com/oss/python/langchain/",
          "language": "Python",
          "forks": 19766,
          "open_issues": 225,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/126733545?v=4",
      "velocity": 132006.6,
      "is_rising_star": true
    },
    {
      "id": "github-langgenius-dify",
      "name": "dify",
      "author": "langgenius",
      "description": "Production-ready platform for agentic workflow development.",
      "task": "tool",
      "tags": [
        "agent",
        "agentic-ai",
        "agentic-framework",
        "agentic-workflow",
        "ai",
        "automation",
        "gemini",
        "genai",
        "gpt",
        "gpt-4",
        "llm",
        "low-code",
        "mcp",
        "nextjs",
        "no-code",
        "openai",
        "orchestration",
        "python",
        "rag",
        "workflow"
      ],
      "likes": 357923,
      "downloads": 357923,
      "lastModified": "2025-11-20T03:40:43Z",
      "lastModifiedTimestamp": 1763610043000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/langgenius/dify",
          "homepage": "https://dify.ai",
          "language": "TypeScript",
          "forks": 18481,
          "open_issues": 694,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127165244?v=4",
      "velocity": 131180.5,
      "is_rising_star": true
    },
    {
      "id": "github-open-webui-open-webui",
      "name": "open-webui",
      "author": "open-webui",
      "description": "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
      "task": "tool",
      "tags": [
        "ai",
        "llm",
        "llm-ui",
        "llm-webui",
        "llms",
        "mcp",
        "ollama",
        "ollama-webui",
        "open-webui",
        "openai",
        "openapi",
        "rag",
        "self-hosted",
        "ui",
        "webui"
      ],
      "likes": 347017,
      "downloads": 347017,
      "lastModified": "2025-11-20T04:00:22Z",
      "lastModifiedTimestamp": 1763611222000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/open-webui/open-webui",
          "homepage": "https://openwebui.com",
          "language": "JavaScript",
          "forks": 16200,
          "open_issues": 303,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/158137808?v=4",
      "velocity": 127171,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-generative-ai-for-beginners",
      "name": "generative-ai-for-beginners",
      "author": "microsoft",
      "description": "21 Lessons, Get Started Building with Generative AI ",
      "task": "tool",
      "tags": [
        "ai",
        "azure",
        "chatgpt",
        "dall-e",
        "generative-ai",
        "generativeai",
        "gpt",
        "language-model",
        "llms",
        "microsoft-for-beginners",
        "openai",
        "prompt-engineering",
        "semantic-search",
        "transformers"
      ],
      "likes": 306061,
      "downloads": 306061,
      "lastModified": "2025-11-20T03:55:34Z",
      "lastModifiedTimestamp": 1763610934000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/generative-ai-for-beginners",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 54213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 112205.5,
      "is_rising_star": true
    },
    {
      "id": "github-x1xhlol-system-prompts-and-models-of-ai-tools",
      "name": "system-prompts-and-models-of-ai-tools",
      "author": "x1xhlol",
      "description": "FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus Agent Tools, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, dia & v0. (And other Open Sourced) System Prompts, Internal Tools & AI Models",
      "task": "tool",
      "tags": [
        "ai",
        "bolt",
        "cluely",
        "copilot",
        "cursor",
        "cursorai",
        "devin",
        "github-copilot",
        "lovable",
        "open-source",
        "perplexity",
        "replit",
        "system-prompts",
        "trae",
        "trae-ai",
        "trae-ide",
        "v0",
        "vscode",
        "windsurf",
        "windsurf-ai"
      ],
      "likes": 288923,
      "downloads": 288923,
      "lastModified": "2025-11-20T04:00:54Z",
      "lastModifiedTimestamp": 1763611254000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
          "homepage": "",
          "language": null,
          "forks": 25879,
          "open_issues": 94,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/185671340?v=4",
      "velocity": 105805.7,
      "is_rising_star": true
    },
    {
      "id": "github-ggml-org-llama.cpp",
      "name": "llama.cpp",
      "author": "ggml-org",
      "description": "LLM inference in C/C++",
      "task": "tool",
      "tags": [
        "ggml"
      ],
      "likes": 270241,
      "downloads": 270241,
      "lastModified": "2025-11-20T03:56:01Z",
      "lastModifiedTimestamp": 1763610961000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ggml-org/llama.cpp",
          "homepage": "",
          "language": "C++",
          "forks": 13747,
          "open_issues": 899,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134263123?v=4",
      "velocity": 99053.9,
      "is_rising_star": true
    },
    {
      "id": "github-google-gemini-gemini-cli",
      "name": "gemini-cli",
      "author": "google-gemini",
      "description": "An open-source AI agent that brings the power of Gemini directly into your terminal.",
      "task": "tool",
      "tags": [
        "gemini",
        "gemini-api"
      ],
      "likes": 250395,
      "downloads": 250395,
      "lastModified": "2025-11-20T03:57:37Z",
      "lastModifiedTimestamp": 1763611057000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/google-gemini/gemini-cli",
          "homepage": "https://geminicli.com",
          "language": "TypeScript",
          "forks": 9384,
          "open_issues": 3007,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/161781182?v=4",
      "velocity": 91595.9,
      "is_rising_star": true
    },
    {
      "id": "github-rasbt-LLMs-from-scratch",
      "name": "LLMs-from-scratch",
      "author": "rasbt",
      "description": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "deep-learning",
        "from-scratch",
        "generative-ai",
        "gpt",
        "language-model",
        "large-language-models",
        "llm",
        "machine-learning",
        "neural-networks",
        "python",
        "pytorch",
        "transformers"
      ],
      "likes": 237017,
      "downloads": 237017,
      "lastModified": "2025-11-20T03:52:19Z",
      "lastModifiedTimestamp": 1763610739000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/rasbt/LLMs-from-scratch",
          "homepage": "https://amzn.to/4fqvn0D",
          "language": "Jupyter Notebook",
          "forks": 11706,
          "open_issues": 0,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/5618407?v=4",
      "velocity": 86865.9,
      "is_rising_star": true
    },
    {
      "id": "github-Shubhamsaboo-awesome-llm-apps",
      "name": "awesome-llm-apps",
      "author": "Shubhamsaboo",
      "description": "Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.",
      "task": "tool",
      "tags": [
        "llms",
        "python",
        "rag"
      ],
      "likes": 237097,
      "downloads": 237097,
      "lastModified": "2025-11-20T04:00:02Z",
      "lastModifiedTimestamp": 1763611202000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Shubhamsaboo/awesome-llm-apps",
          "homepage": "https://www.theunwindai.com",
          "language": "Python",
          "forks": 10516,
          "open_issues": 1,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/31396011?v=4",
      "velocity": 86805.4,
      "is_rising_star": true
    },
    {
      "id": "github-nomic-ai-gpt4all",
      "name": "gpt4all",
      "author": "nomic-ai",
      "description": "GPT4All: Run Local LLMs on Any Device. Open-source and available for commercial use.",
      "task": "tool",
      "tags": [
        "ai-chat",
        "llm-inference"
      ],
      "likes": 230773,
      "downloads": 230773,
      "lastModified": "2025-11-20T02:34:37Z",
      "lastModifiedTimestamp": 1763606077000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/nomic-ai/gpt4all",
          "homepage": "https://nomic.ai/gpt4all",
          "language": "C++",
          "forks": 8303,
          "open_issues": 744,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102670180?v=4",
      "velocity": 84613.1,
      "is_rising_star": true
    },
    {
      "id": "github-browser-use-browser-use",
      "name": "browser-use",
      "author": "browser-use",
      "description": "üåê Make websites accessible for AI agents. Automate tasks online with ease.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "ai-tools",
        "browser-automation",
        "browser-use",
        "llm",
        "playwright",
        "python"
      ],
      "likes": 218221,
      "downloads": 218221,
      "lastModified": "2025-11-20T03:31:37Z",
      "lastModifiedTimestamp": 1763609497000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/browser-use/browser-use",
          "homepage": "https://browser-use.com",
          "language": "Python",
          "forks": 8657,
          "open_issues": 229,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192012301?v=4",
      "velocity": 79990.9,
      "is_rising_star": true
    },
    {
      "id": "github-binary-husky-gpt_academic",
      "name": "gpt_academic",
      "author": "binary-husky",
      "description": "‰∏∫GPT/GLMÁ≠âLLMÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÂÆûÁî®Âåñ‰∫§‰∫íÊé•Âè£ÔºåÁâπÂà´‰ºòÂåñËÆ∫ÊñáÈòÖËØª/Ê∂¶Ëâ≤/ÂÜô‰Ωú‰ΩìÈ™åÔºåÊ®°ÂùóÂåñËÆæËÆ°ÔºåÊîØÊåÅËá™ÂÆö‰πâÂø´Êç∑ÊåâÈíÆ&ÂáΩÊï∞Êèí‰ª∂ÔºåÊîØÊåÅPythonÂíåC++Á≠âÈ°πÁõÆÂâñÊûê&Ëá™ËØëËß£ÂäüËÉΩÔºåPDF/LaTexËÆ∫ÊñáÁøªËØë&ÊÄªÁªìÂäüËÉΩÔºåÊîØÊåÅÂπ∂Ë°åÈóÆËØ¢Â§öÁßçLLMÊ®°ÂûãÔºåÊîØÊåÅchatglm3Á≠âÊú¨Âú∞Ê®°Âûã„ÄÇÊé•ÂÖ•ÈÄö‰πâÂçÉÈóÆ, deepseekcoder, ËÆØÈ£ûÊòüÁÅ´, ÊñáÂøÉ‰∏ÄË®Ä, llama2, rwkv, claude2, mossÁ≠â„ÄÇ",
      "task": "tool",
      "tags": [
        "academic",
        "chatglm-6b",
        "chatgpt",
        "gpt-4",
        "large-language-models"
      ],
      "likes": 209088,
      "downloads": 209088,
      "lastModified": "2025-11-20T03:43:15Z",
      "lastModifiedTimestamp": 1763610195000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/binary-husky/gpt_academic",
          "homepage": "https://github.com/binary-husky/gpt_academic/wiki/online",
          "language": "Python",
          "forks": 8401,
          "open_issues": 291,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/96192199?v=4",
      "velocity": 76661.2,
      "is_rising_star": true
    },
    {
      "id": "github-firecrawl-firecrawl",
      "name": "firecrawl",
      "author": "firecrawl",
      "description": "üî• The Web Data API for AI - Turn entire websites into LLM-ready markdown or structured data",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agents",
        "ai-crawler",
        "ai-scraping",
        "ai-search",
        "crawler",
        "data-extraction",
        "html-to-markdown",
        "llm",
        "markdown",
        "scraper",
        "scraping",
        "web-crawler",
        "web-data",
        "web-data-extraction",
        "web-scraper",
        "web-scraping",
        "web-search",
        "webscraping"
      ],
      "likes": 204312,
      "downloads": 204312,
      "lastModified": "2025-11-20T03:40:22Z",
      "lastModifiedTimestamp": 1763610022000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/firecrawl/firecrawl",
          "homepage": "https://firecrawl.dev",
          "language": "TypeScript",
          "forks": 5298,
          "open_issues": 140,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/135057108?v=4",
      "velocity": 74868.2,
      "is_rising_star": true
    },
    {
      "id": "github-infiniflow-ragflow",
      "name": "ragflow",
      "author": "infiniflow",
      "description": "RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs",
      "task": "tool",
      "tags": [
        "agent",
        "agentic",
        "agentic-ai",
        "agentic-workflow",
        "ai",
        "ai-search",
        "deep-learning",
        "deep-research",
        "deepseek",
        "deepseek-r1",
        "document-parser",
        "document-understanding",
        "graphrag",
        "llm",
        "mcp",
        "multi-agent",
        "ollama",
        "openai",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 204013,
      "downloads": 204013,
      "lastModified": "2025-11-20T03:55:04Z",
      "lastModifiedTimestamp": 1763610904000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/infiniflow/ragflow",
          "homepage": "https://ragflow.io",
          "language": "Python",
          "forks": 7289,
          "open_issues": 2918,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/69962740?v=4",
      "velocity": 74763.7,
      "is_rising_star": true
    },
    {
      "id": "github-lobehub-lobe-chat",
      "name": "lobe-chat",
      "author": "lobehub",
      "description": "ü§Ø LobeHub - an open-source, modern design AI Agent Workspace. Supports multiple AI providers, Knowledge Base (file upload / RAG ), one click install MCP Marketplace and Artifacts / Thinking. One-click FREE deployment of your private AI Agent application.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "artifacts",
        "chat",
        "chatgpt",
        "claude",
        "deepseek",
        "deepseek-r1",
        "function-calling",
        "gemini",
        "gpt",
        "knowledge-base",
        "mcp",
        "nextjs",
        "ollama",
        "openai",
        "rag"
      ],
      "likes": 203567,
      "downloads": 203567,
      "lastModified": "2025-11-20T03:53:05Z",
      "lastModifiedTimestamp": 1763610785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/lobehub/lobe-chat",
          "homepage": "https://lobechat.com",
          "language": "TypeScript",
          "forks": 13985,
          "open_issues": 979,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131470832?v=4",
      "velocity": 74618.5,
      "is_rising_star": true
    },
    {
      "id": "github-mlabonne-llm-course",
      "name": "llm-course",
      "author": "mlabonne",
      "description": "Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.",
      "task": "tool",
      "tags": [
        "course",
        "large-language-models",
        "llm",
        "machine-learning",
        "roadmap"
      ],
      "likes": 203252,
      "downloads": 203252,
      "lastModified": "2025-11-20T03:54:32Z",
      "lastModifiedTimestamp": 1763610872000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mlabonne/llm-course",
          "homepage": "https://mlabonne.github.io/blog/",
          "language": null,
          "forks": 7672,
          "open_issues": 76,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/81252890?v=4",
      "velocity": 74476.6,
      "is_rising_star": true
    },
    {
      "id": "github-ansible-ansible",
      "name": "ansible",
      "author": "ansible",
      "description": "Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.",
      "task": "tool",
      "tags": [
        "ansible",
        "python"
      ],
      "likes": 201152,
      "downloads": 201152,
      "lastModified": "2025-11-20T00:37:21Z",
      "lastModifiedTimestamp": 1763599041000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ansible/ansible",
          "homepage": "https://www.ansible.com/",
          "language": "Python",
          "forks": 24129,
          "open_issues": 872,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1507452?v=4",
      "velocity": 73748.4,
      "is_rising_star": true
    },
    {
      "id": "github-dair-ai-Prompt-Engineering-Guide",
      "name": "Prompt-Engineering-Guide",
      "author": "dair-ai",
      "description": "üêô Guides, papers, lessons, notebooks and resources for prompt engineering, context engineering, RAG, and AI Agents.",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai-agents",
        "chatgpt",
        "deep-learning",
        "generative-ai",
        "language-model",
        "llms",
        "openai",
        "prompt-engineering",
        "rag"
      ],
      "likes": 199720,
      "downloads": 199720,
      "lastModified": "2025-11-20T03:58:38Z",
      "lastModifiedTimestamp": 1763611118000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
          "homepage": "https://www.promptingguide.ai/",
          "language": "MDX",
          "forks": 6945,
          "open_issues": 230,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/30384625?v=4",
      "velocity": 73213.8,
      "is_rising_star": true
    },
    {
      "id": "github-OpenHands-OpenHands",
      "name": "OpenHands",
      "author": "OpenHands",
      "description": "üôå OpenHands: Code Less, Make More",
      "task": "tool",
      "tags": [
        "agent",
        "artificial-intelligence",
        "chatgpt",
        "claude-ai",
        "cli",
        "developer-tools",
        "gpt",
        "llm",
        "openai"
      ],
      "likes": 195291,
      "downloads": 195291,
      "lastModified": "2025-11-20T03:44:01Z",
      "lastModifiedTimestamp": 1763610241000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenHands/OpenHands",
          "homepage": "https://all-hands.dev",
          "language": "Python",
          "forks": 7928,
          "open_issues": 242,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/225919603?v=4",
      "velocity": 71589.1,
      "is_rising_star": true
    },
    {
      "id": "github-PaddlePaddle-PaddleOCR",
      "name": "PaddleOCR",
      "author": "PaddlePaddle",
      "description": "Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.",
      "task": "tool",
      "tags": [
        "ai4science",
        "chineseocr",
        "document-parsing",
        "document-translation",
        "kie",
        "ocr",
        "paddleocr-vl",
        "pdf-extractor-rag",
        "pdf-parser",
        "pdf2markdown",
        "pp-ocr",
        "pp-structure",
        "rag"
      ],
      "likes": 192979,
      "downloads": 192979,
      "lastModified": "2025-11-20T03:32:54Z",
      "lastModifiedTimestamp": 1763609574000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/PaddlePaddle/PaddleOCR",
          "homepage": "https://www.paddleocr.ai",
          "language": "Python",
          "forks": 9361,
          "open_issues": 276,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23534030?v=4",
      "velocity": 70700.3,
      "is_rising_star": true
    },
    {
      "id": "github-vllm-project-vllm",
      "name": "vllm",
      "author": "vllm-project",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "task": "tool",
      "tags": [
        "amd",
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-v3",
        "gpt",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llm",
        "llm-serving",
        "model-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen",
        "qwen3",
        "tpu",
        "transformer"
      ],
      "likes": 190450,
      "downloads": 190450,
      "lastModified": "2025-11-20T03:53:27Z",
      "lastModifiedTimestamp": 1763610807000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/vllm-project/vllm",
          "homepage": "https://docs.vllm.ai",
          "language": "Python",
          "forks": 11390,
          "open_issues": 3172,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/136984999?v=4",
      "velocity": 69779.6,
      "is_rising_star": true
    },
    {
      "id": "github-hiyouga-LLaMA-Factory",
      "name": "LLaMA-Factory",
      "author": "hiyouga",
      "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "deepseek",
        "fine-tuning",
        "gemma",
        "gpt",
        "instruction-tuning",
        "large-language-models",
        "llama",
        "llama3",
        "llm",
        "lora",
        "moe",
        "nlp",
        "peft",
        "qlora",
        "quantization",
        "qwen",
        "rlhf",
        "transformers"
      ],
      "likes": 188197,
      "downloads": 188197,
      "lastModified": "2025-11-20T03:49:20Z",
      "lastModifiedTimestamp": 1763610560000,
      "readme": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)\n[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)\n[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)\n\n[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory)\n[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align=\"center\" markdown=\"1\">\n\n### Supporters ‚ù§Ô∏è\n\n| <div style=\"text-align: center;\"><a href=\"https://warp.dev/llama-factory\"><img alt=\"Warp sponsorship\" width=\"400\" src=\"assets/sponsors/warp.jpg\"></a><br><a href=\"https://warp.dev/llama-factory\" style=\"font-size:larger;\">Warp, the agentic terminal for developers</a><br><a href=\"https://warp.dev/llama-factory\">Available for MacOS, Linux, & Windows</a> | <a href=\"https://serpapi.com\"><img alt=\"SerpAPI sponsorship\" width=\"250\" src=\"assets/sponsors/serpapi.svg\"> </a> |\n| ---- | ---- |\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\nüëã Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.\n\n\\[ English | [‰∏≠Êñá](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nStart local training:\n- Please refer to [usage](#getting-started)\n\nStart cloud training:\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory\n- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\nRead technical notes:\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/\n- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html\n- **Official Blog**: https://blog.llamafactory.net/en/\n- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [LLaMA Factory Online](#llama-factory-online)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                           |\n| ------------ | -------------------------------------------------------------------- |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |\n\n## Blogs\n\n> [!TIP]\n> Now we have a dedicated blog for LLaMA Factory!\n>\n> Website: https://blog.llamafactory.net/en/\n\n- üí° [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&type=project&utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)\n- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n\n<details><summary>All Blogs</summary>\n\n- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n- [A One-Stop Code-Free Model Fine-Tuning \\& Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.\n\n[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.\n\n[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.\n\n[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.\n\n<details><summary>Full Changelog</summary>\n\n[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)'s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)'s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)'s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template             |\n| ----------------------------------------------------------------- | -------------------------------- | -------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2            |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                    |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3             |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere               |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek             |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3            |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1           |\n| [ERNIE-4.5](https://huggingface.co/baidu)                         | 0.3B/21B/300B                    | ernie/ernie_nothink  |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon               |\n| [Falcon-H1](https://huggingface.co/tiiuae)                        | 0.5B/1.5B/3B/7B/34B              | falcon_h1            |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma/gemma2         |\n| [Gemma 3/Gemma 3n](https://huggingface.co/google)                 | 270M/1B/4B/6B/8B/12B/27B         | gemma3/gemma3n       |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/zai-org)         | 9B/32B                           | glm4/glmz1           |\n| [GLM-4.1V](https://huggingface.co/zai-org)                        | 9B                               | glm4v                |\n| [GLM-4.5/GLM-4.5V](https://huggingface.co/zai-org)                | 106B/355B                        | glm4_moe/glm4v_moe   |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                    |\n| [GPT-OSS](https://huggingface.co/openai)                          | 20B/120B                         | gpt                  |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3             |\n| [Granite 4](https://huggingface.co/ibm-granite)                   | 7B                               | granite4             |\n| [Hunyuan (MT)](https://huggingface.co/tencent/)                   | 7B                               | hunyuan              |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index                |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2              |\n| [InternVL 2.5-3.5](https://huggingface.co/OpenGVLab)              | 1B/2B/4B/8B/14B/30B/38B/78B/241B | intern_vl            |\n| [InternLM/Intern-S1-mini](https://huggingface.co/internlm/)       | 8B                               | intern_s1            |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl              |\n| [Ling 2.0 (mini/flash)](https://huggingface.co/inclusionAI)       | 16B/100B                         | bailing_v2           |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                    |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2               |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3               |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4               |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama               |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava                |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next           |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video     |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                 |\n| [MiniCPM 1-4.1](https://huggingface.co/openbmb)                   | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4        |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v  |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral            |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral              |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small        |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                    |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma            |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                    |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                  |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small            |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                 |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral              |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                 |\n| [Qwen3 (MoE/Instruct/Thinking/Next)](https://huggingface.co/Qwen) | 0.6B/1.7B/4B/8B/14B/32B/80B/235B | qwen3/qwen3_nothink  |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio          |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni           |\n| [Qwen3-Omni](https://huggingface.co/Qwen)                         | 30B                              | qwen3_omni           |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl             |\n| [Qwen3-VL](https://huggingface.co/Qwen)                           | 2B/4B/8B/30B/32B/235B            | qwen3_vl             |\n| [Seed (OSS/Coder)](https://huggingface.co/ByteDance-Seed)         | 8B/36B                           | seed_oss/seed_coder  |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1           |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                    |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2            |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse               |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                   |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl                |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                 |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> If the model has both reasoning and non-reasoning versions, please use the `_nothink` suffix to distinguish between them. For example, `qwen3` and `qwen3_nothink`.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> \\*: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> \\*\\*: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |        OFT         |        QOFT        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [CCI3-HQ (zh)](https://huggingface.co/datasets/BAAI/CCI3-HQ)\n- [CCI3-Data (zh)](https://huggingface.co/datasets/BAAI/CCI3-Data)\n- [CCI4.0-M2-Base-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1)\n- [CCI4.0-M2-CoT-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1)\n- [CCI4.0-M2-Extra-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [Infinity Instruct (zh)](https://huggingface.co/datasets/BAAI/Infinity-Instruct)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install \"huggingface_hub<1.0.0\"\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.49.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method                              | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ----------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)             |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)                  |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam/OFT |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA / QOFT                        |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA / QOFT                        |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA / QOFT                        |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n#### Install from Source\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev\n\n#### Install from Docker Image\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nThis image is built on Ubuntu 22.04 (x86\\_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.\n\nFind the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags\n\nPlease refer to [build docker](#build-docker) to build the image yourself.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can't pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**, **[DataFlow](https://github.com/OpenDCAI/DataFlow)** and **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### LLaMA Factory Online\n\nRead our [documentation](https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory).\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Use Docker volumes</summary>\n\nYou can uncomment `VOLUME [ \"/root/.cache/huggingface\", \"/app/shared_data\", \"/app/output\" ]` in the Dockerfile to use data volumes.\n\nWhen building the Docker image, use `-v ./hf_cache:/root/.cache/huggingface` argument to mount the local directory to the container. The following data volumes are available.\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine.\n- `shared_data`: The directionary to store datasets on the host machine.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [[paper]](https://aclanthology.org/2024.findings-acl.830.pdf)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n1. **[EmoLLM](https://github.com/SmartFlowAI/EmoLLM)**: A project about large language models (LLMs) and mental health.\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/hiyouga/LLaMA-Factory",
          "homepage": "https://llamafactory.readthedocs.io",
          "language": "Python",
          "forks": 7592,
          "open_issues": 782,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/16256802?v=4",
      "velocity": 68968.9,
      "is_rising_star": true
    },
    {
      "id": "github-FoundationAgents-MetaGPT",
      "name": "MetaGPT",
      "author": "FoundationAgents",
      "description": "üåü The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
      "task": "tool",
      "tags": [
        "agent",
        "gpt",
        "llm",
        "metagpt",
        "multi-agent"
      ],
      "likes": 178722,
      "downloads": 178722,
      "lastModified": "2025-11-20T03:51:26Z",
      "lastModifiedTimestamp": 1763610686000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FoundationAgents/MetaGPT",
          "homepage": "https://mgx.dev/",
          "language": "Python",
          "forks": 7283,
          "open_issues": 58,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/198047230?v=4",
      "velocity": 65516,
      "is_rising_star": true
    },
    {
      "id": "github-unclecode-crawl4ai",
      "name": "crawl4ai",
      "author": "unclecode",
      "description": "üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN",
      "task": "tool",
      "tags": [],
      "likes": 168306,
      "downloads": 168306,
      "lastModified": "2025-11-20T03:44:18Z",
      "lastModifiedTimestamp": 1763610258000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unclecode/crawl4ai",
          "homepage": "https://crawl4ai.com",
          "language": "Python",
          "forks": 5627,
          "open_issues": 262,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/12494079?v=4",
      "velocity": 61679.2,
      "is_rising_star": true
    },
    {
      "id": "github-OpenBB-finance-OpenBB",
      "name": "OpenBB",
      "author": "OpenBB-finance",
      "description": "Financial data platform for analysts, quants and AI agents.",
      "task": "tool",
      "tags": [
        "ai",
        "crypto",
        "derivatives",
        "economics",
        "equity",
        "finance",
        "fixed-income",
        "machine-learning",
        "openbb",
        "options",
        "python",
        "quantitative-finance",
        "stocks"
      ],
      "likes": 163943,
      "downloads": 163943,
      "lastModified": "2025-11-20T03:23:51Z",
      "lastModifiedTimestamp": 1763609031000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenBB-finance/OpenBB",
          "homepage": "https://openbb.co",
          "language": "Python",
          "forks": 5282,
          "open_issues": 51,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/80064875?v=4",
      "velocity": 60102.9,
      "is_rising_star": true
    },
    {
      "id": "github-simular-ai-Agent-S",
      "name": "Agent-S",
      "author": "simular-ai",
      "description": "Agent S: an open agentic framework that uses computers like a human",
      "task": "tool",
      "tags": [
        "agent-computer-interface",
        "ai-agents",
        "computer-automation",
        "computer-use",
        "computer-use-agent",
        "cua",
        "grounding",
        "gui-agents",
        "in-context-reinforcement-learning",
        "memory",
        "mllm",
        "planning",
        "retrieval-augmented-generation",
        "agents",
        "anthropic",
        "anthropic-claude",
        "automation",
        "claude",
        "claude-code",
        "claude-code-cli",
        "claude-code-commands",
        "claude-code-plugin",
        "claude-code-plugins",
        "claude-code-subagents",
        "claude-skills",
        "claudecode",
        "claudecode-config",
        "claudecode-subagents",
        "orchestration",
        "sub-agents",
        "subagents",
        "workflows",
        "ai",
        "openai",
        "real-time",
        "video",
        "voice",
        "autonomous-agents",
        "language-model",
        "llm"
      ],
      "likes": 160186,
      "downloads": 160186,
      "lastModified": "2025-11-20T03:36:25Z",
      "lastModifiedTimestamp": 1763609785000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/simular-ai/Agent-S",
          "homepage": "https://www.simular.ai",
          "language": "Python",
          "forks": 905,
          "open_issues": 13,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/wshobson/agents",
          "homepage": "https://sethhobson.com",
          "language": "Python",
          "forks": 2349,
          "open_issues": 4,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/contains-studio/agents",
          "homepage": null,
          "language": null,
          "forks": 2126,
          "open_issues": 9,
          "license": "No license"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/livekit/agents",
          "homepage": "https://docs.livekit.io/agents",
          "language": "Python",
          "forks": 1768,
          "open_issues": 452,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/aiwaves-cn/agents",
          "homepage": "",
          "language": "Python",
          "forks": 452,
          "open_issues": 39,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/99358647?v=4",
      "velocity": 58689.4,
      "is_rising_star": true
    },
    {
      "id": "github-cline-cline",
      "name": "cline",
      "author": "cline",
      "description": "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.",
      "task": "tool",
      "tags": [],
      "likes": 157519,
      "downloads": 157519,
      "lastModified": "2025-11-20T03:55:20Z",
      "lastModifiedTimestamp": 1763610920000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/cline/cline",
          "homepage": "https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev",
          "language": "TypeScript",
          "forks": 5242,
          "open_issues": 881,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/184127137?v=4",
      "velocity": 57740.1,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-autogen",
      "name": "autogen",
      "author": "microsoft",
      "description": "A programming framework for agentic AI",
      "task": "tool",
      "tags": [
        "agentic",
        "agentic-agi",
        "agents",
        "ai",
        "autogen",
        "autogen-ecosystem",
        "chatgpt",
        "framework",
        "llm-agent",
        "llm-framework"
      ],
      "likes": 155426,
      "downloads": 155426,
      "lastModified": "2025-11-20T03:15:25Z",
      "lastModifiedTimestamp": 1763608525000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/autogen",
          "homepage": "https://microsoft.github.io/autogen/",
          "language": "Python",
          "forks": 7866,
          "open_issues": 509,
          "license": "Creative Commons Attribution 4.0 International"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 56980,
      "is_rising_star": true
    },
    {
      "id": "github-Mintplex-Labs-anything-llm",
      "name": "anything-llm",
      "author": "Mintplex-Labs",
      "description": "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.",
      "task": "tool",
      "tags": [
        "ai-agents",
        "custom-ai-agents",
        "deepseek",
        "kimi",
        "llama3",
        "llm",
        "lmstudio",
        "local-llm",
        "localai",
        "mcp",
        "mcp-servers",
        "moonshot",
        "multimodal",
        "no-code",
        "ollama",
        "qwen3",
        "rag",
        "vector-database",
        "web-scraping"
      ],
      "likes": 153633,
      "downloads": 153633,
      "lastModified": "2025-11-20T03:46:02Z",
      "lastModifiedTimestamp": 1763610362000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Mintplex-Labs/anything-llm",
          "homepage": "https://anythingllm.com",
          "language": "JavaScript",
          "forks": 5414,
          "open_issues": 345,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134426827?v=4",
      "velocity": 56316.7,
      "is_rising_star": true
    },
    {
      "id": "github-openai-codex",
      "name": "codex",
      "author": "openai",
      "description": "Lightweight coding agent that runs in your terminal",
      "task": "tool",
      "tags": [],
      "likes": 152654,
      "downloads": 152654,
      "lastModified": "2025-11-20T03:47:36Z",
      "lastModifiedTimestamp": 1763610456000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/openai/codex",
          "homepage": "",
          "language": "Rust",
          "forks": 6374,
          "open_issues": 1043,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/14957082?v=4",
      "velocity": 55924,
      "is_rising_star": true
    },
    {
      "id": "github-pathwaycom-pathway",
      "name": "pathway",
      "author": "pathwaycom",
      "description": "Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.",
      "task": "tool",
      "tags": [
        "batch-processing",
        "data-analytics",
        "data-pipelines",
        "data-processing",
        "dataflow",
        "etl",
        "etl-framework",
        "iot-analytics",
        "kafka",
        "machine-learning-algorithms",
        "pathway",
        "python",
        "real-time",
        "rust",
        "stream-processing",
        "streaming",
        "time-series-analysis"
      ],
      "likes": 150231,
      "downloads": 150231,
      "lastModified": "2025-11-20T03:36:07Z",
      "lastModifiedTimestamp": 1763609767000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pathwaycom/pathway",
          "homepage": "https://pathway.com",
          "language": "Python",
          "forks": 1453,
          "open_issues": 39,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25750857?v=4",
      "velocity": 55056.1,
      "is_rising_star": true
    },
    {
      "id": "github-opendatalab-MinerU",
      "name": "MinerU",
      "author": "opendatalab",
      "description": "Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.",
      "task": "tool",
      "tags": [
        "ai4science",
        "document-analysis",
        "extract-data",
        "layout-analysis",
        "ocr",
        "parser",
        "pdf",
        "pdf-converter",
        "pdf-extractor-llm",
        "pdf-extractor-pretrain",
        "pdf-extractor-rag",
        "pdf-parser",
        "python"
      ],
      "likes": 147376,
      "downloads": 147376,
      "lastModified": "2025-11-20T03:56:20Z",
      "lastModifiedTimestamp": 1763610980000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/opendatalab/MinerU",
          "homepage": "https://opendatalab.github.io/MinerU/",
          "language": "Python",
          "forks": 4072,
          "open_issues": 123,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/97503431?v=4",
      "velocity": 54001.2,
      "is_rising_star": true
    },
    {
      "id": "github-unslothai-unsloth",
      "name": "unsloth",
      "author": "unslothai",
      "description": "Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.",
      "task": "tool",
      "tags": [
        "agent",
        "deepseek",
        "deepseek-r1",
        "fine-tuning",
        "gemma",
        "gemma3",
        "gpt-oss",
        "llama",
        "llama3",
        "llm",
        "llms",
        "mistral",
        "openai",
        "qwen",
        "qwen3",
        "reinforcement-learning",
        "text-to-speech",
        "tts",
        "unsloth",
        "voice-cloning"
      ],
      "likes": 145378,
      "downloads": 145378,
      "lastModified": "2025-11-20T03:57:06Z",
      "lastModifiedTimestamp": 1763611026000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/unslothai/unsloth",
          "homepage": "https://docs.unsloth.ai/",
          "language": "Python",
          "forks": 3987,
          "open_issues": 862,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/150920049?v=4",
      "velocity": 53275.2,
      "is_rising_star": true
    },
    {
      "id": "github-huginn-huginn",
      "name": "huginn",
      "author": "huginn",
      "description": "Create agents that monitor and act on your behalf.  Your agents are standing by!",
      "task": "tool",
      "tags": [
        "agent",
        "automation",
        "feed",
        "feedgenerator",
        "huginn",
        "monitoring",
        "notifications",
        "rss",
        "scraper",
        "twitter",
        "twitter-streaming",
        "webscraping"
      ],
      "likes": 144302,
      "downloads": 144302,
      "lastModified": "2025-11-20T03:59:01Z",
      "lastModifiedTimestamp": 1763611141000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huginn/huginn",
          "homepage": "",
          "language": "Ruby",
          "forks": 4194,
          "open_issues": 693,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23225142?v=4",
      "velocity": 52906.7,
      "is_rising_star": true
    },
    {
      "id": "github-harry0703-MoneyPrinterTurbo",
      "name": "MoneyPrinterTurbo",
      "author": "harry0703",
      "description": "Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.",
      "task": "tool",
      "tags": [
        "ai",
        "automation",
        "chatgpt",
        "moviepy",
        "python",
        "shortvideo",
        "tiktok"
      ],
      "likes": 143513,
      "downloads": 143513,
      "lastModified": "2025-11-20T03:41:13Z",
      "lastModifiedTimestamp": 1763610073000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/harry0703/MoneyPrinterTurbo",
          "homepage": "",
          "language": "Python",
          "forks": 6693,
          "open_issues": 217,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/4928832?v=4",
      "velocity": 52607.5,
      "is_rising_star": true
    },
    {
      "id": "github-pathwaycom-llm-app",
      "name": "llm-app",
      "author": "pathwaycom",
      "description": "Ready-to-run cloud templates for RAG, AI pipelines, and enterprise search with live data. üê≥Docker-friendly.‚ö°Always in sync with Sharepoint, Google Drive, S3, Kafka, PostgreSQL, real-time data APIs, and more.",
      "task": "tool",
      "tags": [
        "chatbot",
        "hugging-face",
        "llm",
        "llm-local",
        "llm-prompting",
        "llm-security",
        "llmops",
        "machine-learning",
        "open-ai",
        "pathway",
        "rag",
        "real-time",
        "retrieval-augmented-generation",
        "vector-database",
        "vector-index"
      ],
      "likes": 141740,
      "downloads": 141740,
      "lastModified": "2025-11-20T03:27:51Z",
      "lastModifiedTimestamp": 1763609271000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/pathwaycom/llm-app",
          "homepage": "https://pathway.com/developers/templates/",
          "language": "Jupyter Notebook",
          "forks": 1213,
          "open_issues": 6,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25750857?v=4",
      "velocity": 51950.8,
      "is_rising_star": true
    },
    {
      "id": "github-FlowiseAI-Flowise",
      "name": "Flowise",
      "author": "FlowiseAI",
      "description": "Build AI Agents, Visually",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-workflow",
        "agents",
        "artificial-intelligence",
        "chatbot",
        "chatgpt",
        "javascript",
        "langchain",
        "large-language-models",
        "low-code",
        "multiagent-systems",
        "no-code",
        "openai",
        "rag",
        "react",
        "typescript",
        "workflow-automation"
      ],
      "likes": 140056,
      "downloads": 140056,
      "lastModified": "2025-11-20T03:45:42Z",
      "lastModifiedTimestamp": 1763610342000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/FlowiseAI/Flowise",
          "homepage": "https://flowiseai.com",
          "language": "TypeScript",
          "forks": 23126,
          "open_issues": 723,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128289781?v=4",
      "velocity": 51339.2,
      "is_rising_star": true
    },
    {
      "id": "github-run-llama-llama_index",
      "name": "llama_index",
      "author": "run-llama",
      "description": "LlamaIndex is the leading framework for building LLM-powered agents over your data.",
      "task": "tool",
      "tags": [
        "agents",
        "application",
        "data",
        "fine-tuning",
        "framework",
        "llamaindex",
        "llm",
        "multi-agents",
        "rag",
        "vector-database"
      ],
      "likes": 135951,
      "downloads": 135951,
      "lastModified": "2025-11-20T02:11:43Z",
      "lastModifiedTimestamp": 1763604703000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/run-llama/llama_index",
          "homepage": "https://developers.llamaindex.ai",
          "language": "Python",
          "forks": 6524,
          "open_issues": 262,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/130722866?v=4",
      "velocity": 49837.7,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-ai-agents-for-beginners",
      "name": "ai-agents-for-beginners",
      "author": "microsoft",
      "description": "12 Lessons to Get Started Building AI Agents",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agentic-framework",
        "agentic-rag",
        "ai-agents",
        "ai-agents-framework",
        "autogen",
        "generative-ai",
        "semantic-kernel"
      ],
      "likes": 135593,
      "downloads": 135593,
      "lastModified": "2025-11-20T03:40:06Z",
      "lastModifiedTimestamp": 1763610006000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/ai-agents-for-beginners",
          "homepage": "https://aka.ms/ai-agents-beginners",
          "language": "Jupyter Notebook",
          "forks": 15322,
          "open_issues": 11,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 49688.1,
      "is_rising_star": true
    },
    {
      "id": "github-jeecgboot-JeecgBoot",
      "name": "JeecgBoot",
      "author": "jeecgboot",
      "description": "üî•AI‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÔºåÂä©Âäõ‰ºÅ‰∏öÂø´ÈÄüÂÆûÁé∞‰Ωé‰ª£Á†ÅÂºÄÂèëÂíåÊûÑÂª∫AIÂ∫îÁî®ÔºÅ ÈõÜÊàê‰∏ÄÂ•óÂÆåÊï¥AIÂ∫îÁî®Âπ≥Âè∞ÔºöÊ∂µÁõñAIÂ∫îÁî®„ÄÅAIÊ®°Âûã„ÄÅAIËÅäÂ§©Âä©Êâã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAIÊµÅÁ®ãÁºñÊéíÁ≠âÔºåÂÖºÂÆπÂ§öÁßçÂ§ßÊ®°ÂûãÔºõÊèê‰æõÂº∫Â§ß‰ª£Á†ÅÁîüÊàêÂô®ÔºöÂÆûÁé∞ÂâçÂêéÁ´Ø‰∏ÄÈîÆÁîüÊàêÔºåÊó†ÈúÄÊâãÂÜô‰ª£Á†Å! ÂºïÈ¢ÜAIÂºÄÂèëÊ®°ÂºèÔºöAIÁîüÊàê‚ÜíÂú®Á∫øÈÖçÁΩÆ‚Üí‰ª£Á†ÅÁîüÊàê‚ÜíÊâãÂ∑•ÂêàÂπ∂ÔºåËß£ÂÜ≥JavaÈ°πÁõÆ80%ÈáçÂ§çÂ∑•‰ΩúÔºåÊèêÂçáÊïàÁéáËäÇÁúÅÊàêÊú¨ÔºåÂèà‰∏çÂ§±ÁÅµÊ¥ª~",
      "task": "tool",
      "tags": [
        "activiti",
        "agent",
        "ai",
        "aiflow",
        "ant-design-vue",
        "antd",
        "codegenerator",
        "deepseek",
        "flowable",
        "langchain4j",
        "llm",
        "low-code",
        "mcp",
        "mybatis-plus",
        "rag",
        "spring-ai",
        "springboot",
        "springboot3",
        "springcloud",
        "vue3"
      ],
      "likes": 133232,
      "downloads": 133232,
      "lastModified": "2025-11-20T03:18:17Z",
      "lastModifiedTimestamp": 1763608697000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/jeecgboot/JeecgBoot",
          "homepage": "https://jeecgboot.github.io/JeecgBoot/",
          "language": "Java",
          "forks": 15659,
          "open_issues": 53,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/86360035?v=4",
      "velocity": 48846.6,
      "is_rising_star": true
    },
    {
      "id": "github-mem0ai-mem0",
      "name": "mem0",
      "author": "mem0ai",
      "description": "Universal memory layer for AI Agents",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "application",
        "chatbots",
        "chatgpt",
        "genai",
        "hacktoberfest",
        "llm",
        "long-term-memory",
        "memory",
        "memory-management",
        "python",
        "rag",
        "state-management"
      ],
      "likes": 129974,
      "downloads": 129974,
      "lastModified": "2025-11-20T03:24:00Z",
      "lastModifiedTimestamp": 1763609040000,
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"docs/images/banner-sm.png\" width=\"800px\" alt=\"Mem0 - The Memory Layer for Personalized AI\">\n  </a>\n</p>\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/11194\" target=\"blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/11194\" alt=\"mem0ai%2Fmem0 | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai\">Learn more</a>\n  ¬∑\n  <a href=\"https://mem0.dev/DiG\">Join Discord</a>\n  ¬∑\n  <a href=\"https://mem0.dev/demo\">Demo</a>\n  ¬∑\n  <a href=\"https://mem0.dev/openmemory\">OpenMemory</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.dev/DiG\">\n    <img src=\"https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white\" alt=\"Mem0 Discord\">\n  </a>\n  <a href=\"https://pepy.tech/project/mem0ai\">\n    <img src=\"https://img.shields.io/pypi/dm/mem0ai\" alt=\"Mem0 PyPI - Downloads\">\n  </a>\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square\" alt=\"GitHub commit activity\">\n  </a>\n  <a href=\"https://pypi.org/project/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/pypi/v/mem0ai?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n  </a>\n  <a href=\"https://www.npmjs.com/package/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/npm/v/mem0ai\" alt=\"Npm package\">\n  </a>\n  <a href=\"https://www.ycombinator.com/companies/mem0\">\n    <img src=\"https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square\" alt=\"Y Combinator S24\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai/research\"><strong>üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí</strong></a>\n</p>\n<p align=\"center\">\n  <strong>‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens</strong>\n</p>\n\n> **üéâ mem0ai v1.0.0 is now available!** This major release includes API modernization, improved vector store support, and enhanced GCP integration. [See migration guide ‚Üí](MIGRATION_GUIDE_v1.0.md)\n\n##  üî• Research Highlights\n- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark\n- **91% Faster Responses** than full-context, ensuring low-latency at scale\n- **90% Lower Token Usage** than full-context, cutting costs without compromise\n- [Read the full paper](https://mem0.ai/research)\n\n# Introduction\n\n[Mem0](https://mem0.ai) (\"mem-zero\") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.\n\n### Key Features & Use Cases\n\n**Core Capabilities:**\n- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization\n- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option\n\n**Applications:**\n- **AI Assistants**: Consistent, context-rich conversations\n- **Customer Support**: Recall past tickets and user history for tailored help\n- **Healthcare**: Track patient preferences and history for personalized care\n- **Productivity & Gaming**: Adaptive workflows and environments based on user behavior\n\n## üöÄ Quickstart Guide <a name=\"quickstart\"></a>\n\nChoose between our hosted platform or self-hosted package:\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [Mem0 Platform](https://app.mem0.ai)\n2. Embed the memory layer via SDK or API keys\n\n### Self-Hosted (Open Source)\n\nInstall the sdk via pip:\n\n```bash\npip install mem0ai\n```\n\nInstall sdk via npm:\n```bash\nnpm install mem0ai\n```\n\n### Basic Usage\n\nMem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).\n\nFirst step is to instantiate the memory:\n\n```python\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n\n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nFor detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).\n\n## üîó Integrations & Demos\n\n- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))\n- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))\n- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))\n- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))\n\n## üìö Documentation & Support\n\n- Full docs: https://docs.mem0.ai\n- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)\n- Contact: founders@mem0.ai\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n```\n\n## ‚öñÔ∏è License\n\nApache 2.0 ‚Äî see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mem0ai/mem0",
          "homepage": "https://mem0.ai",
          "language": "Python",
          "forks": 4691,
          "open_issues": 516,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/137054526?v=4",
      "velocity": 47636.6,
      "is_rising_star": true
    },
    {
      "id": "github-anthropics-claude-code",
      "name": "claude-code",
      "author": "anthropics",
      "description": "Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.",
      "task": "tool",
      "tags": [],
      "likes": 128639,
      "downloads": 128639,
      "lastModified": "2025-11-20T03:51:58Z",
      "lastModifiedTimestamp": 1763610718000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/anthropics/claude-code",
          "homepage": "https://code.claude.com/docs/en/overview",
          "language": "Shell",
          "forks": 2893,
          "open_issues": 5277,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/76263028?v=4",
      "velocity": 47107.5,
      "is_rising_star": true
    },
    {
      "id": "github-sst-opencode",
      "name": "opencode",
      "author": "sst",
      "description": "The AI coding agent built for the terminal.",
      "task": "tool",
      "tags": [
        "ai",
        "claude",
        "code",
        "llm",
        "openai"
      ],
      "likes": 128450,
      "downloads": 128450,
      "lastModified": "2025-11-20T03:54:03Z",
      "lastModifiedTimestamp": 1763610843000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/sst/opencode",
          "homepage": "https://opencode.ai",
          "language": "TypeScript",
          "forks": 2658,
          "open_issues": 1451,
          "license": "MIT License"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/opencode-ai/opencode",
          "homepage": "",
          "language": "Go",
          "forks": 806,
          "open_issues": 163,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/66570915?v=4",
      "velocity": 47018.4,
      "is_rising_star": true
    },
    {
      "id": "github-crewAIInc-crewAI",
      "name": "crewAI",
      "author": "crewAIInc",
      "description": "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "aiagentframework",
        "llms"
      ],
      "likes": 121780,
      "downloads": 121780,
      "lastModified": "2025-11-20T02:15:41Z",
      "lastModifiedTimestamp": 1763604941000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/crewAIInc/crewAI",
          "homepage": "https://crewai.com",
          "language": "Python",
          "forks": 5416,
          "open_issues": 187,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/170677839?v=4",
      "velocity": 44638,
      "is_rising_star": true
    },
    {
      "id": "github-ray-project-ray",
      "name": "ray",
      "author": "ray-project",
      "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
      "task": "tool",
      "tags": [
        "data-science",
        "deep-learning",
        "deployment",
        "distributed",
        "hyperparameter-optimization",
        "hyperparameter-search",
        "large-language-models",
        "llm",
        "llm-inference",
        "llm-serving",
        "machine-learning",
        "optimization",
        "parallel",
        "python",
        "pytorch",
        "ray",
        "reinforcement-learning",
        "rllib",
        "serving",
        "tensorflow"
      ],
      "likes": 119736,
      "downloads": 119736,
      "lastModified": "2025-11-20T03:26:50Z",
      "lastModifiedTimestamp": 1763609210000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ray-project/ray",
          "homepage": "https://ray.io",
          "language": "Python",
          "forks": 6913,
          "open_issues": 3270,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/22125274?v=4",
      "velocity": 43892.2,
      "is_rising_star": true
    },
    {
      "id": "github-zhayujie-chatgpt-on-wechat",
      "name": "chatgpt-on-wechat",
      "author": "zhayujie",
      "description": "Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©ChatGPT/Claude/DeepSeek/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ",
      "task": "tool",
      "tags": [
        "ai",
        "ai-agent",
        "chatgpt",
        "claude-4",
        "deepseek",
        "dingtalk",
        "feishu-bot",
        "gemini",
        "gpt-4",
        "kimi",
        "linkai",
        "llm",
        "mcp",
        "multi-agent",
        "openai",
        "python3",
        "qwen",
        "rag",
        "wechat",
        "wechat-bot"
      ],
      "likes": 119263,
      "downloads": 119263,
      "lastModified": "2025-11-20T03:15:18Z",
      "lastModifiedTimestamp": 1763608518000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/zhayujie/chatgpt-on-wechat",
          "homepage": "https://link-ai.tech",
          "language": "Python",
          "forks": 9499,
          "open_issues": 354,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/26161723?v=4",
      "velocity": 43723.9,
      "is_rising_star": true
    },
    {
      "id": "github-milvus-io-milvus",
      "name": "milvus",
      "author": "milvus-io",
      "description": "Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search",
      "task": "tool",
      "tags": [
        "anns",
        "cloud-native",
        "diskann",
        "distributed",
        "embedding-database",
        "embedding-similarity",
        "embedding-store",
        "faiss",
        "golang",
        "hnsw",
        "image-search",
        "llm",
        "nearest-neighbor-search",
        "rag",
        "vector-database",
        "vector-search",
        "vector-similarity",
        "vector-store"
      ],
      "likes": 118669,
      "downloads": 118669,
      "lastModified": "2025-11-20T03:59:12Z",
      "lastModifiedTimestamp": 1763611152000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/milvus-io/milvus",
          "homepage": "https://milvus.io",
          "language": "Go",
          "forks": 3572,
          "open_issues": 889,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/51735404?v=4",
      "velocity": 43424.7,
      "is_rising_star": true
    },
    {
      "id": "github-janhq-jan",
      "name": "jan",
      "author": "janhq",
      "description": "Jan is an open source alternative to ChatGPT that runs 100% offline on your computer.",
      "task": "tool",
      "tags": [
        "chatgpt",
        "gpt",
        "llamacpp",
        "llm",
        "localai",
        "open-source",
        "self-hosted",
        "tauri"
      ],
      "likes": 118089,
      "downloads": 118089,
      "lastModified": "2025-11-20T03:31:03Z",
      "lastModifiedTimestamp": 1763609463000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/janhq/jan",
          "homepage": "https://jan.ai/",
          "language": "TypeScript",
          "forks": 2394,
          "open_issues": 190,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102363196?v=4",
      "velocity": 43288.3,
      "is_rising_star": true
    },
    {
      "id": "github-mudler-LocalAI",
      "name": "LocalAI",
      "author": "mudler",
      "description": ":robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI,  running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "audio-generation",
        "decentralized",
        "distributed",
        "gemma",
        "image-generation",
        "libp2p",
        "llama",
        "llm",
        "mamba",
        "mcp",
        "mistral",
        "musicgen",
        "object-detection",
        "rerank",
        "rwkv",
        "stable-diffusion",
        "text-generation",
        "tts"
      ],
      "likes": 116477,
      "downloads": 116477,
      "lastModified": "2025-11-20T03:43:49Z",
      "lastModifiedTimestamp": 1763610229000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/mudler/LocalAI",
          "homepage": "https://localai.io",
          "language": "Go",
          "forks": 3077,
          "open_issues": 250,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/2420543?v=4",
      "velocity": 42665.7,
      "is_rising_star": true
    },
    {
      "id": "github-QuivrHQ-quivr",
      "name": "quivr",
      "author": "QuivrHQ",
      "description": "Opiniated RAG for integrating GenAI in your apps üß†   Focus on your product rather than the RAG. Easy integration in existing products with customisation!  Any LLM: GPT4, Groq, Llama. Any Vectorstore: PGVector, Faiss. Any Files. Anyway you want. ",
      "task": "tool",
      "tags": [
        "ai",
        "api",
        "chatbot",
        "chatgpt",
        "database",
        "docker",
        "framework",
        "frontend",
        "groq",
        "html",
        "javascript",
        "llm",
        "openai",
        "postgresql",
        "privacy",
        "rag",
        "react",
        "security",
        "typescript",
        "vector"
      ],
      "likes": 115876,
      "downloads": 115876,
      "lastModified": "2025-11-19T22:05:27Z",
      "lastModifiedTimestamp": 1763589927000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/QuivrHQ/quivr",
          "homepage": "https://core.quivr.com",
          "language": "Python",
          "forks": 3689,
          "open_issues": 16,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/159330290?v=4",
      "velocity": 42486.4,
      "is_rising_star": true
    },
    {
      "id": "github-2noise-ChatTTS",
      "name": "ChatTTS",
      "author": "2noise",
      "description": "A generative speech model for daily dialogue.",
      "task": "tool",
      "tags": [
        "agent",
        "chat",
        "chatgpt",
        "chattts",
        "chinese",
        "chinese-language",
        "english",
        "english-language",
        "gpt",
        "llm",
        "llm-agent",
        "natural-language-inference",
        "python",
        "text-to-speech",
        "torch",
        "torchaudio",
        "tts"
      ],
      "likes": 114529,
      "downloads": 114529,
      "lastModified": "2025-11-19T23:29:58Z",
      "lastModifiedTimestamp": 1763594998000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/2noise/ChatTTS",
          "homepage": "https://2noise.com",
          "language": "Python",
          "forks": 4146,
          "open_issues": 67,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/164844019?v=4",
      "velocity": 41992.5,
      "is_rising_star": true
    },
    {
      "id": "github-chatboxai-chatbox",
      "name": "chatbox",
      "author": "chatboxai",
      "description": "User-friendly Desktop Client App for AI Models/LLMs (GPT, Claude, Gemini, Ollama...)",
      "task": "tool",
      "tags": [
        "assistant",
        "chatbot",
        "chatgpt",
        "claude",
        "copilot",
        "deepseek",
        "gemini",
        "gpt",
        "gpt-5",
        "ollama",
        "openai"
      ],
      "likes": 112397,
      "downloads": 112397,
      "lastModified": "2025-11-20T02:37:08Z",
      "lastModifiedTimestamp": 1763606228000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chatboxai/chatbox",
          "homepage": "https://chatboxai.app?utm_medium=github",
          "language": "TypeScript",
          "forks": 3786,
          "open_issues": 966,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/199570308?v=4",
      "velocity": 41207.1,
      "is_rising_star": true
    },
    {
      "id": "github-upstash-context7",
      "name": "context7",
      "author": "upstash",
      "description": "Context7 MCP Server -- Up-to-date code documentation for LLMs and AI code editors",
      "task": "tool",
      "tags": [
        "llm",
        "mcp",
        "mcp-server",
        "vibe-coding"
      ],
      "likes": 112502,
      "downloads": 112502,
      "lastModified": "2025-11-20T03:47:16Z",
      "lastModifiedTimestamp": 1763610436000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/upstash/context7",
          "homepage": "https://context7.com",
          "language": "JavaScript",
          "forks": 1852,
          "open_issues": 100,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/74989412?v=4",
      "velocity": 41188.4,
      "is_rising_star": true
    },
    {
      "id": "github-ToolJet-ToolJet",
      "name": "ToolJet",
      "author": "ToolJet",
      "description": "ToolJet is the open-source foundation of ToolJet AI - the AI-native platform for building internal tools, dashboard, business applications, workflows and AI agents üöÄ",
      "task": "tool",
      "tags": [
        "ai-app-builder",
        "docker",
        "hacktoberfest",
        "internal-applications",
        "internal-project",
        "internal-tool",
        "internal-tools",
        "javascript",
        "kubernetes",
        "low-code",
        "low-code-development-platform",
        "low-code-framework",
        "no-code",
        "nodejs",
        "reactjs",
        "self-hosted",
        "typescript",
        "web-development-tools",
        "workflow-automation"
      ],
      "likes": 110776,
      "downloads": 110776,
      "lastModified": "2025-11-19T23:34:57Z",
      "lastModifiedTimestamp": 1763595297000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ToolJet/ToolJet",
          "homepage": "https://tooljet.ai",
          "language": "JavaScript",
          "forks": 4877,
          "open_issues": 950,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/82193554?v=4",
      "velocity": 40612,
      "is_rising_star": true
    },
    {
      "id": "github-alibaba-arthas",
      "name": "arthas",
      "author": "alibaba",
      "description": "Alibaba Java Diagnostic Tool Arthas/Alibaba JavaËØäÊñ≠Âà©Âô®Arthas",
      "task": "tool",
      "tags": [
        "agent",
        "alibaba",
        "arthas",
        "classloader",
        "diagnosis",
        "java",
        "jvm",
        "trace",
        "trouble-shooting"
      ],
      "likes": 110550,
      "downloads": 110550,
      "lastModified": "2025-11-19T21:23:19Z",
      "lastModifiedTimestamp": 1763587399000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/alibaba/arthas",
          "homepage": "https://arthas.aliyun.com/",
          "language": "Java",
          "forks": 7602,
          "open_issues": 455,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/1961952?v=4",
      "velocity": 40532.8,
      "is_rising_star": true
    },
    {
      "id": "github-chatchat-space-Langchain-Chatchat",
      "name": "Langchain-Chatchat",
      "author": "chatchat-space",
      "description": "Langchain-ChatchatÔºàÂéüLangchain-ChatGLMÔºâÂü∫‰∫é Langchain ‰∏é ChatGLM, Qwen ‰∏é Llama Á≠âËØ≠Ë®ÄÊ®°ÂûãÁöÑ RAG ‰∏é Agent Â∫îÁî® | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM, Qwen and Llama) RAG and Agent app with langchain ",
      "task": "tool",
      "tags": [
        "chatbot",
        "chatchat",
        "chatglm",
        "chatgpt",
        "embedding",
        "faiss",
        "fastchat",
        "gpt",
        "knowledge-base",
        "langchain",
        "langchain-chatglm",
        "llama",
        "llm",
        "milvus",
        "ollama",
        "qwen",
        "rag",
        "retrieval-augmented-generation",
        "streamlit",
        "xinference"
      ],
      "likes": 109780,
      "downloads": 109780,
      "lastModified": "2025-11-20T03:27:10Z",
      "lastModifiedTimestamp": 1763609230000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chatchat-space/Langchain-Chatchat",
          "homepage": "",
          "language": "Python",
          "forks": 6067,
          "open_issues": 28,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/139558948?v=4",
      "velocity": 40249,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-LLM101n",
      "name": "LLM101n",
      "author": "karpathy",
      "description": "LLM101n: Let's build a Storyteller",
      "task": "tool",
      "tags": [],
      "likes": 106769,
      "downloads": 106769,
      "lastModified": "2025-11-20T03:50:12Z",
      "lastModifiedTimestamp": 1763610612000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/LLM101n",
          "homepage": "",
          "language": null,
          "forks": 1937,
          "open_issues": 19,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 39143.5,
      "is_rising_star": true
    },
    {
      "id": "github-CherryHQ-cherry-studio",
      "name": "cherry-studio",
      "author": "CherryHQ",
      "description": "üçí Cherry Studio is a desktop client that supports for multiple LLM providers.",
      "task": "tool",
      "tags": [
        "agent",
        "anthropic",
        "assistant",
        "chatbot",
        "chatbotai",
        "electron",
        "llm",
        "mcp-client",
        "openai"
      ],
      "likes": 106775,
      "downloads": 106775,
      "lastModified": "2025-11-20T03:54:49Z",
      "lastModifiedTimestamp": 1763610889000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CherryHQ/cherry-studio",
          "homepage": "https://cherry-ai.com",
          "language": "TypeScript",
          "forks": 3227,
          "open_issues": 532,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/187777663?v=4",
      "velocity": 39125.9,
      "is_rising_star": true
    },
    {
      "id": "github-agno-agi-agno",
      "name": "agno",
      "author": "agno-agi",
      "description": "Multi-agent framework, runtime and control plane. Built for speed, privacy, and scale.",
      "task": "tool",
      "tags": [
        "agents",
        "ai",
        "ai-agents",
        "developer-tools",
        "python"
      ],
      "likes": 106117,
      "downloads": 106117,
      "lastModified": "2025-11-20T03:42:58Z",
      "lastModifiedTimestamp": 1763610178000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/agno-agi/agno",
          "homepage": "https://docs.agno.com",
          "language": "Python",
          "forks": 4641,
          "open_issues": 298,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/104874993?v=4",
      "velocity": 38883.9,
      "is_rising_star": true
    },
    {
      "id": "github-Alibaba-NLP-DeepResearch",
      "name": "DeepResearch",
      "author": "Alibaba-NLP",
      "description": "Tongyi Deep Research, the Leading Open-source Deep Research Agent",
      "task": "tool",
      "tags": [
        "agent",
        "alibaba",
        "artificial-intelligence",
        "deep-research",
        "deepresearch",
        "information-seeking",
        "llm",
        "tongyi",
        "web-agent",
        "ai",
        "gpt",
        "o3-mini",
        "research"
      ],
      "likes": 106031,
      "downloads": 106031,
      "lastModified": "2025-11-20T01:44:25Z",
      "lastModifiedTimestamp": 1763603065000,
      "readme": "<div align=\"center\">\n  <picture>\n      <img src=\"./assets/logo.png\" width=\"100%\">\n  </picture>\n</div>\n\n<hr>\n\n<div align=\"center\" style=\"line-height: 1;\">\n\n[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&logo=huggingface&logoColor=ffffff&labelColor)](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)\n[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Alibaba-NLP/DeepResearch)\n[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)\n[![Paper](https://img.shields.io/badge/Paper-red?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2510.24701)\n\n</div>\n<p align=\"center\">\n<p align=\"center\">\nü§ó <a href=\"https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\" target=\"_blank\">HuggingFace</a> ÔΩú\n<img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> <a href=\"https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B\" target=\"_blank\">ModelScope</a> |  üí¨ <a href=\"./assets/wechat_new.jpg\">WeChat(ÂæÆ‰ø°)</a> üì∞ <a href=\"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/\">Blog</a> | üìë <a href=\"https://arxiv.org/pdf/2510.24701\">Paper</a>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/14895\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/14895\" alt=\"Alibaba-NLP%2FDeepResearch | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nüëè Welcome to try Tongyi DeepResearch via our **[<img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> Modelscope online demo](https://www.modelscope.cn/studios/jialongwu/Tongyi-DeepResearch)** or **[ü§ó Huggingface online demo](https://huggingface.co/spaces/Alibaba-NLP/Tongyi-DeepResearch)** or <img src=\"./WebAgent/assets/aliyun.png\" width=\"14px\" style=\"display:inline;\"> **[bailian service](https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&tab=app#/app/app-market/deep-search/)**!\n\n> [!NOTE]\n> This demo is for quick exploration only. Response times may vary or fail intermittently due to model latency and tool QPS limits. For a stable experience we recommend local deployment; for a production-ready service, visit <img src=\"./WebAgent/assets/aliyun.png\" width=\"14px\" style=\"display:inline;\"> [bailian](https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&tab=app#/app/app-market/deep-search/) and follow the guided setup.\n\n# Introduction\n\nWe present <img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> **Tongyi DeepResearch**, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.\n\n> Tongyi DeepResearch builds upon our previous work on the <img src=\"./assets/tongyi.png\" width=\"14px\" style=\"display:inline;\"> [WebAgent](./WebAgent/) project.\n\nMore details can be found in our üì∞&nbsp;<a href=\"https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/\">Tech Blog</a>.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/performance.png\">\n</p>\n\n## Features\n\n- ‚öôÔ∏è **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.\n- üîÑ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.\n- üîÅ **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.\n- ü§ñ **Agent Inference Paradigm Compatibility**: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.\n\n# Model Download\n\nYou can directly download the model by following the links below.\n\n|            Model            |                                                                           Download Links                                                                           | Model Size | Context Length |\n| :-------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------: | :------------: |\n| Tongyi-DeepResearch-30B-A3B | [ü§ó HuggingFace](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)<br> [ü§ñ ModelScope](https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B) |  30B-A3B   |      128K      |\n\n# News\n\n[2025/09/20]üöÄ Tongyi-DeepResearch-30B-A3B is now on [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b)! Follow the [Quick-start](https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model) guide.\n\n[2025/09/17]üî• We have released **Tongyi-DeepResearch-30B-A3B**.\n\n# Deep Research Benchmark Results\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/benchmark.png\">\n</p>\n\n## Quick Start\n\nThis guide provides instructions for setting up the environment and running inference scripts located in the [inference](./inference/) folder.\n\n### 1. Environment Setup\n- Recommended Python version: **3.10.0** (using other versions may cause dependency issues).\n- It is strongly advised to create an isolated environment using `conda` or `virtualenv`.\n\n```bash\n# Example with Conda\nconda create -n react_infer_env python=3.10.0\nconda activate react_infer_env\n```\n\n### 2. Installation\n\nInstall the required dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### 3. Environment Configuration and Prepare Evaluation Data\n\n#### Environment Configuration\n\nConfigure your API keys and settings by copying the example environment file:\n\n```bash\n# Copy the example environment file\ncp .env.example .env\n```\n\nEdit the `.env` file and provide your actual API keys and configuration values:\n\n- **SERPER_KEY_ID**: Get your key from [Serper.dev](https://serper.dev/) for web search and Google Scholar\n- **JINA_API_KEYS**: Get your key from [Jina.ai](https://jina.ai/) for web page reading\n- **API_KEY/API_BASE**: OpenAI-compatible API for page summarization from [OpenAI](https://platform.openai.com/)\n- **DASHSCOPE_API_KEY**: Get your key from [Dashscope](https://dashscope.aliyun.com/) for file parsing\n- **SANDBOX_FUSION_ENDPOINT**: Python interpreter sandbox endpoints (see [SandboxFusion](https://github.com/bytedance/SandboxFusion))\n- **MODEL_PATH**: Path to your model weights\n- **DATASET**: Name of your evaluation dataset\n- **OUTPUT_PATH**: Directory for saving results\n\n> **Note**: The `.env` file is gitignored, so your secrets will not be committed to the repository.\n\n#### Prepare Evaluation Data\n\nThe system supports two input file formats: **JSON** and **JSONL**.\n\n#### Supported File Formats:\n\n**Option 1: JSONL Format (recommended)**\n- Create your data file with `.jsonl` extension (e.g., `my_questions.jsonl`)\n- Each line must be a valid JSON object with `question` and `answer` keys:\n  ```json\n  {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"}\n  {\"question\": \"Explain quantum computing\", \"answer\": \"\"}\n  ```\n\n**Option 2: JSON Format**\n- Create your data file with `.json` extension (e.g., `my_questions.json`)\n- File must contain a JSON array of objects, each with `question` and `answer` keys:\n  ```json\n  [\n    { \"question\": \"What is the capital of France?\", \"answer\": \"Paris\" },\n    { \"question\": \"Explain quantum computing\", \"answer\": \"\" }\n  ]\n  ```\n\n**Important Note:** The `answer` field contains the **ground truth/reference answer** used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.\n\n#### File References for Document Processing:\n\n- If using the _file parser_ tool, **prepend the filename to the `question` field**\n- Place referenced files in `eval_data/file_corpus/` directory\n- Example: `{\"question\": \"(Uploaded 1 file: ['report.pdf'])\\n\\nWhat are the key findings?\", \"answer\": \"...\"}`\n\n#### File Organization:\n```\nproject_root/\n‚îú‚îÄ‚îÄ eval_data/\n‚îÇ   ‚îú‚îÄ‚îÄ my_questions.jsonl          # Your evaluation data\n‚îÇ   ‚îî‚îÄ‚îÄ file_corpus/                # Referenced documents\n‚îÇ       ‚îú‚îÄ‚îÄ report.pdf\n‚îÇ       ‚îî‚îÄ‚îÄ data.xlsx\n```\n\n### 4. Configure the Inference Script\n\n- Open `run_react_infer.sh` and modify the following variables as instructed in the comments:\n  - `MODEL_PATH` - path to the local or remote model weights.\n  - `DATASET` - full path to your evaluation file, e.g. `eval_data/my_questions.jsonl` or `/path/to/my_questions.json`.\n  - `OUTPUT_PATH` - path for saving the prediction results, e.g. `./outputs`.\n- Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required `API_KEY`, `BASE_URL`, or other credentials. Each key is explained inline in the bash script.\n\n### 5. Run the Inference Script\n\n```bash\nbash run_react_infer.sh\n```\n---\n\nWith these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.\n\n### 6. You can use OpenRouter's API to call our model\n\nTongyi-DeepResearch-30B-A3B is now available at [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b). You can run the inference without any GPUs.\n\nYou need to modify the following in the file [inference/react_agent.py](https://github.com/Alibaba-NLP/DeepResearch/blob/main/inference/react_agent.py):\n\n- In the call_server function: Set the API key and URL to your OpenRouter account‚Äôs API and URL.\n- Change the model name to alibaba/tongyi-deepresearch-30b-a3b.\n- Adjust the content concatenation way as described in the comments on lines **88‚Äì90.**\n\n## Benchmark Evaluation\n\nWe provide benchmark evaluation scripts for various datasets. Please refer to the [evaluation scripts](./evaluation/) directory for more details.\n\n## FAQ\n\nPlease refer to the [FAQ](./FAQ.md) for more details.\n\n## Deep Research Agent Family\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"./assets/family17.png\">\n</p>\n\nTongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:\n\n[1] [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/pdf/2501.07572) (ACL 2025)<br>\n[2] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648) (NeurIPS 2025)<br>\n[3] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/pdf/2507.02592)<br>\n[4] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/pdf/2507.15061)<br>\n[5] [WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent](https://arxiv.org/pdf/2508.05748)<br>\n[6] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/pdf/2509.13309)<br>\n[7] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/pdf/2509.13313)<br>\n[8] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/pdf/2509.13312)<br>\n[9] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/pdf/2509.13305)<br>\n[10] [Scaling Agents via Continual Pre-training](https://arxiv.org/pdf/2509.13310)<br>\n[11] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/pdf/2509.13311)<br>\n[12] [WebLeaper: Empowering Efficient, Info-Rich Seeking for Web Agents](https://arxiv.org/pdf/2510.24697)\n\n## üåü Misc\n\n<div align=\"center\">\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&type=Date)](https://www.star-history.com/#Alibaba-NLP/DeepResearch&Date)\n\n</div>\n\n## üö© Talent Recruitment\n\nüî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)\n\nüìö **Research Area**ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG\n\n‚òéÔ∏è **Contact**Ôºö[yongjiang.jy@alibaba-inc.com]()\n\n## Contact Information\n\nFor communications, please contact Yong Jiang (yongjiang.jy@alibaba-inc.com).\n\n## Citation\n\n```bibtex\n@article{tongyidr,\n  title={Tongyi DeepResearch Technical Report},\n  author={Team, Tongyi DeepResearch and Li, Baixuan and Zhang, Bo and Zhang, Dingchu and Huang, Fei and Li, Guangyu and Chen, Guoxin and Yin, Huifeng and Wu, Jialong and Zhou, Jingren and others},\n  journal={arXiv preprint arXiv:2510.24701},\n  year={2025}\n}\n```\n",
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Alibaba-NLP/DeepResearch",
          "homepage": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
          "language": "Python",
          "forks": 1314,
          "open_issues": 66,
          "license": "Apache License 2.0"
        },
        {
          "platform": "GitHub",
          "url": "https://github.com/dzhng/deep-research",
          "homepage": "",
          "language": "TypeScript",
          "forks": 1866,
          "open_issues": 77,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/64211549?v=4",
      "velocity": 38866.3,
      "is_rising_star": true
    },
    {
      "id": "github-reworkd-AgentGPT",
      "name": "AgentGPT",
      "author": "reworkd",
      "description": "ü§ñ Assemble, configure, and deploy autonomous AI Agents in your browser.",
      "task": "tool",
      "tags": [
        "agent",
        "agentgpt",
        "agi",
        "autogpt",
        "baby-agi",
        "gpt",
        "langchain",
        "next",
        "openai",
        "t3",
        "t3-stack"
      ],
      "likes": 105765,
      "downloads": 105765,
      "lastModified": "2025-11-19T18:55:43Z",
      "lastModifiedTimestamp": 1763578543000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/reworkd/AgentGPT",
          "homepage": "https://agentgpt.reworkd.ai",
          "language": "TypeScript",
          "forks": 9487,
          "open_issues": 214,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/120154269?v=4",
      "velocity": 38778.3,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-qlib",
      "name": "qlib",
      "author": "microsoft",
      "description": "Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.",
      "task": "tool",
      "tags": [
        "algorithmic-trading",
        "auto-quant",
        "deep-learning",
        "finance",
        "fintech",
        "investment",
        "machine-learning",
        "paper",
        "platform",
        "python",
        "quant",
        "quant-dataset",
        "quant-models",
        "quantitative-finance",
        "quantitative-trading",
        "research",
        "research-paper",
        "stock-data"
      ],
      "likes": 101601,
      "downloads": 101601,
      "lastModified": "2025-11-20T03:54:28Z",
      "lastModifiedTimestamp": 1763610868000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/qlib",
          "homepage": "https://qlib.readthedocs.io/en/latest/",
          "language": "Python",
          "forks": 5237,
          "open_issues": 304,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 37231.7,
      "is_rising_star": true
    },
    {
      "id": "github-1Panel-dev-1Panel",
      "name": "1Panel",
      "author": "1Panel-dev",
      "description": "üî• 1Panel provides an intuitive web interface and MCP Server to manage websites, files, containers, databases, and LLMs on a Linux server.",
      "task": "tool",
      "tags": [
        "1panel",
        "cockpit",
        "docker",
        "docker-ui",
        "lamp",
        "linux",
        "lnmp",
        "ollama",
        "webmin"
      ],
      "likes": 96275,
      "downloads": 96275,
      "lastModified": "2025-11-20T03:39:30Z",
      "lastModifiedTimestamp": 1763609970000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/1Panel-dev/1Panel",
          "homepage": "https://1panel.pro",
          "language": "Go",
          "forks": 2839,
          "open_issues": 509,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/109613420?v=4",
      "velocity": 35295.7,
      "is_rising_star": true
    },
    {
      "id": "github-danny-avila-LibreChat",
      "name": "LibreChat",
      "author": "danny-avila",
      "description": "Enhanced ChatGPT Clone: Features Agents, MCP, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-5, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active.",
      "task": "tool",
      "tags": [
        "ai",
        "anthropic",
        "artifacts",
        "aws",
        "azure",
        "chatgpt",
        "chatgpt-clone",
        "claude",
        "clone",
        "deepseek",
        "gemini",
        "google",
        "gpt-5",
        "librechat",
        "mcp",
        "o1",
        "openai",
        "responses-api",
        "vision",
        "webui"
      ],
      "likes": 95365,
      "downloads": 95365,
      "lastModified": "2025-11-20T03:05:26Z",
      "lastModifiedTimestamp": 1763607926000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/danny-avila/LibreChat",
          "homepage": "https://librechat.ai/",
          "language": "TypeScript",
          "forks": 6225,
          "open_issues": 353,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110412045?v=4",
      "velocity": 34945.9,
      "is_rising_star": true
    },
    {
      "id": "github-khoj-ai-khoj",
      "name": "khoj",
      "author": "khoj-ai",
      "description": "Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "assistant",
        "chat",
        "chatgpt",
        "emacs",
        "image-generation",
        "llama3",
        "llamacpp",
        "llm",
        "obsidian",
        "obsidian-md",
        "offline-llm",
        "productivity",
        "rag",
        "research",
        "self-hosted",
        "semantic-search",
        "stt",
        "whatsapp-ai"
      ],
      "likes": 94832,
      "downloads": 94832,
      "lastModified": "2025-11-20T00:45:17Z",
      "lastModifiedTimestamp": 1763599517000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/khoj-ai/khoj",
          "homepage": "https://khoj.dev",
          "language": "Python",
          "forks": 1860,
          "open_issues": 85,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/134046886?v=4",
      "velocity": 34764.4,
      "is_rising_star": true
    },
    {
      "id": "github-BerriAI-litellm",
      "name": "litellm",
      "author": "BerriAI",
      "description": "Python SDK, Proxy Server (AI Gateway) to call 100+ LLM APIs in OpenAI (or native) format, with cost tracking, guardrails, loadbalancing and logging. [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, VLLM, NVIDIA NIM]",
      "task": "tool",
      "tags": [
        "ai-gateway",
        "anthropic",
        "azure-openai",
        "bedrock",
        "gateway",
        "langchain",
        "litellm",
        "llm",
        "llm-gateway",
        "llmops",
        "mcp-gateway",
        "openai",
        "openai-proxy",
        "vertex-ai"
      ],
      "likes": 93970,
      "downloads": 93970,
      "lastModified": "2025-11-20T03:54:47Z",
      "lastModifiedTimestamp": 1763610887000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/BerriAI/litellm",
          "homepage": "https://docs.litellm.ai/docs/",
          "language": "Python",
          "forks": 4749,
          "open_issues": 1359,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/121462774?v=4",
      "velocity": 34423.4,
      "is_rising_star": true
    },
    {
      "id": "github-continuedev-continue",
      "name": "continue",
      "author": "continuedev",
      "description": "‚è© Ship faster with Continuous AI. Open-source CLI that can be used in TUI mode as a coding agent or Headless mode to run background agents",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "background-agents",
        "claude",
        "cli",
        "continuous-ai",
        "developer-tools",
        "gemini",
        "gpt",
        "hacktoberfest",
        "jetbrains",
        "llm",
        "open-source",
        "qwen",
        "vscode",
        "workflows"
      ],
      "likes": 89741,
      "downloads": 89741,
      "lastModified": "2025-11-20T04:00:09Z",
      "lastModifiedTimestamp": 1763611209000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/continuedev/continue",
          "homepage": "https://docs.continue.dev/",
          "language": "TypeScript",
          "forks": 3801,
          "open_issues": 656,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/127876214?v=4",
      "velocity": 32896.6,
      "is_rising_star": true
    },
    {
      "id": "github-JushBJJ-Mr.-Ranedeer-AI-Tutor",
      "name": "Mr.-Ranedeer-AI-Tutor",
      "author": "JushBJJ",
      "description": "A GPT-4 AI Tutor Prompt for customizable personalized learning experiences.",
      "task": "tool",
      "tags": [
        "ai",
        "education",
        "gpt-4",
        "llm"
      ],
      "likes": 89002,
      "downloads": 89002,
      "lastModified": "2025-11-20T02:26:40Z",
      "lastModifiedTimestamp": 1763605600000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor",
          "homepage": "https://Mr-Ranedeer.com",
          "language": null,
          "forks": 3373,
          "open_issues": 14,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/36951064?v=4",
      "velocity": 32632.6,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-graphrag",
      "name": "graphrag",
      "author": "microsoft",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "task": "tool",
      "tags": [
        "gpt",
        "gpt-4",
        "gpt4",
        "graphrag",
        "llm",
        "llms",
        "rag"
      ],
      "likes": 87739,
      "downloads": 87739,
      "lastModified": "2025-11-20T03:49:55Z",
      "lastModifiedTimestamp": 1763610595000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/graphrag",
          "homepage": "https://microsoft.github.io/graphrag/",
          "language": "Python",
          "forks": 3075,
          "open_issues": 94,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 32151.9,
      "is_rising_star": true
    },
    {
      "id": "github-feder-cr-Jobs_Applier_AI_Agent_AIHawk",
      "name": "Jobs_Applier_AI_Agent_AIHawk",
      "author": "feder-cr",
      "description": "AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.",
      "task": "tool",
      "tags": [
        "agent",
        "application-resume",
        "artificial-intelligence",
        "automate",
        "automation",
        "bot",
        "chatgpt",
        "chrome",
        "gpt",
        "human-resources",
        "job",
        "jobs",
        "jobsearch",
        "jobseeker",
        "opeai",
        "python",
        "resume",
        "scraper",
        "scraping",
        "selenium"
      ],
      "likes": 87229,
      "downloads": 87229,
      "lastModified": "2025-11-19T18:41:18Z",
      "lastModifiedTimestamp": 1763577678000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk",
          "homepage": "",
          "language": "Python",
          "forks": 4422,
          "open_issues": 12,
          "license": "GNU Affero General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/85809106?v=4",
      "velocity": 31978.1,
      "is_rising_star": true
    },
    {
      "id": "github-666ghj-BettaFish",
      "name": "BettaFish",
      "author": "666ghj",
      "description": "ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ",
      "task": "tool",
      "tags": [
        "agent-framework",
        "data-analysis",
        "deep-research",
        "deep-search",
        "llms",
        "multi-agent-system",
        "nlp",
        "public-opinion-analysis",
        "python3",
        "sentiment-analysis"
      ],
      "likes": 85074,
      "downloads": 85074,
      "lastModified": "2025-11-20T03:55:09Z",
      "lastModifiedTimestamp": 1763610909000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/666ghj/BettaFish",
          "homepage": "",
          "language": "Python",
          "forks": 5428,
          "open_issues": 58,
          "license": "GNU General Public License v2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/110395318?v=4",
      "velocity": 31050.8,
      "is_rising_star": true
    },
    {
      "id": "github-karpathy-llm.c",
      "name": "llm.c",
      "author": "karpathy",
      "description": "LLM training in simple, raw C/CUDA",
      "task": "tool",
      "tags": [],
      "likes": 84583,
      "downloads": 84583,
      "lastModified": "2025-11-19T23:29:57Z",
      "lastModifiedTimestamp": 1763594997000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/karpathy/llm.c",
          "homepage": "",
          "language": "Cuda",
          "forks": 3293,
          "open_issues": 215,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/241138?v=4",
      "velocity": 31010.1,
      "is_rising_star": true
    },
    {
      "id": "github-songquanpeng-one-api",
      "name": "one-api",
      "author": "songquanpeng",
      "description": "LLM API ÁÆ°ÁêÜ & ÂàÜÂèëÁ≥ªÁªüÔºåÊîØÊåÅ OpenAI„ÄÅAzure„ÄÅAnthropic Claude„ÄÅGoogle Gemini„ÄÅDeepSeek„ÄÅÂ≠óËäÇË±ÜÂåÖ„ÄÅChatGLM„ÄÅÊñáÂøÉ‰∏ÄË®Ä„ÄÅËÆØÈ£ûÊòüÁÅ´„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅ360 Êô∫ËÑë„ÄÅËÖæËÆØÊ∑∑ÂÖÉÁ≠â‰∏ªÊµÅÊ®°ÂûãÔºåÁªü‰∏Ä API ÈÄÇÈÖçÔºåÂèØÁî®‰∫é key ÁÆ°ÁêÜ‰∏é‰∫åÊ¨°ÂàÜÂèë„ÄÇÂçïÂèØÊâßË°åÊñá‰ª∂ÔºåÊèê‰æõ Docker ÈïúÂÉèÔºå‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÂºÄÁÆ±Âç≥Áî®„ÄÇLLM API management & key redistribution system, unifying multiple providers under a single API. Single binary, Docker-ready, with an English UI.",
      "task": "tool",
      "tags": [
        "api",
        "api-gateway",
        "azure-openai-api",
        "chatgpt",
        "claude",
        "ernie-bot",
        "gemini",
        "gpt",
        "openai",
        "openai-api",
        "proxy"
      ],
      "likes": 84136,
      "downloads": 84136,
      "lastModified": "2025-11-20T03:11:36Z",
      "lastModifiedTimestamp": 1763608296000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/songquanpeng/one-api",
          "homepage": "https://openai.justsong.cn/",
          "language": "JavaScript",
          "forks": 5519,
          "open_issues": 968,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/39998050?v=4",
      "velocity": 30837.4,
      "is_rising_star": true
    },
    {
      "id": "github-OpenBMB-ChatDev",
      "name": "ChatDev",
      "author": "OpenBMB",
      "description": "Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)",
      "task": "tool",
      "tags": [],
      "likes": 83235,
      "downloads": 83235,
      "lastModified": "2025-11-19T19:17:28Z",
      "lastModifiedTimestamp": 1763579848000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/OpenBMB/ChatDev",
          "homepage": "https://arxiv.org/abs/2307.07924",
          "language": "Python",
          "forks": 3487,
          "open_issues": 50,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/89920203?v=4",
      "velocity": 30517.3,
      "is_rising_star": true
    },
    {
      "id": "github-stanford-oval-storm",
      "name": "storm",
      "author": "stanford-oval",
      "description": "An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.",
      "task": "tool",
      "tags": [
        "agentic-rag",
        "deep-research",
        "emnlp2024",
        "knowledge-curation",
        "large-language-models",
        "naacl",
        "nlp",
        "report-generation",
        "retrieval-augmented-generation"
      ],
      "likes": 82855,
      "downloads": 82855,
      "lastModified": "2025-11-20T03:14:26Z",
      "lastModifiedTimestamp": 1763608466000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/stanford-oval/storm",
          "homepage": "http://storm.genie.stanford.edu",
          "language": "Python",
          "forks": 2504,
          "open_issues": 87,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13667124?v=4",
      "velocity": 30376.5,
      "is_rising_star": true
    },
    {
      "id": "github-voideditor-void",
      "name": "void",
      "author": "voideditor",
      "description": "An AI tool from GitHub.",
      "task": "tool",
      "tags": [
        "chatgpt",
        "claude",
        "copilot",
        "cursor",
        "developer-tools",
        "editor",
        "llm",
        "open-source",
        "openai",
        "visual-studio-code",
        "vscode",
        "vscode-extension"
      ],
      "likes": 82654,
      "downloads": 82654,
      "lastModified": "2025-11-20T03:17:19Z",
      "lastModifiedTimestamp": 1763608639000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/voideditor/void",
          "homepage": "https://voideditor.com",
          "language": "TypeScript",
          "forks": 2147,
          "open_issues": 304,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/181171420?v=4",
      "velocity": 30298.4,
      "is_rising_star": true
    },
    {
      "id": "github-nrwl-nx",
      "name": "nx",
      "author": "nrwl",
      "description": "Get to green PRs in half the time. Nx optimizes your builds, scales your CI, and fixes failed PRs. Built for developers and AI agents.",
      "task": "tool",
      "tags": [
        "angular",
        "build",
        "build-system",
        "build-tool",
        "building-tool",
        "cli",
        "cypress",
        "hacktoberfest",
        "javascript",
        "monorepo",
        "nextjs",
        "nodejs",
        "nx",
        "nx-workspaces",
        "react",
        "storybook",
        "typescript"
      ],
      "likes": 82544,
      "downloads": 82544,
      "lastModified": "2025-11-19T23:18:56Z",
      "lastModifiedTimestamp": 1763594336000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/nrwl/nx",
          "homepage": "https://nx.dev",
          "language": "TypeScript",
          "forks": 2622,
          "open_issues": 786,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/23692104?v=4",
      "velocity": 30258.8,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-semantic-kernel",
      "name": "semantic-kernel",
      "author": "microsoft",
      "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
      "task": "tool",
      "tags": [
        "ai",
        "artificial-intelligence",
        "llm",
        "openai",
        "sdk"
      ],
      "likes": 80096,
      "downloads": 80096,
      "lastModified": "2025-11-20T00:29:43Z",
      "lastModifiedTimestamp": 1763598583000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/semantic-kernel",
          "homepage": "https://aka.ms/semantic-kernel",
          "language": "C#",
          "forks": 4349,
          "open_issues": 572,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 29363.4,
      "is_rising_star": true
    },
    {
      "id": "github-labring-FastGPT",
      "name": "FastGPT",
      "author": "labring",
      "description": "FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.",
      "task": "tool",
      "tags": [
        "agent",
        "claude",
        "deepseek",
        "llm",
        "mcp",
        "nextjs",
        "openai",
        "qwen",
        "rag",
        "workflow"
      ],
      "likes": 78949,
      "downloads": 78949,
      "lastModified": "2025-11-20T03:15:06Z",
      "lastModifiedTimestamp": 1763608506000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/labring/FastGPT",
          "homepage": "https://fastgpt.io",
          "language": "TypeScript",
          "forks": 6772,
          "open_issues": 650,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/102226726?v=4",
      "velocity": 28942.1,
      "is_rising_star": true
    },
    {
      "id": "github-ComposioHQ-composio",
      "name": "composio",
      "author": "ComposioHQ",
      "description": "Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agents",
        "ai",
        "ai-agents",
        "aiagents",
        "developer-tools",
        "function-calling",
        "gpt-4",
        "javascript",
        "js",
        "llm",
        "llmops",
        "mcp",
        "python",
        "remote-mcp-server",
        "sse",
        "typescript"
      ],
      "likes": 78494,
      "downloads": 78494,
      "lastModified": "2025-11-19T22:09:44Z",
      "lastModifiedTimestamp": 1763590184000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/ComposioHQ/composio",
          "homepage": "https://docs.composio.dev",
          "language": "TypeScript",
          "forks": 4397,
          "open_issues": 25,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/128464815?v=4",
      "velocity": 28778.2,
      "is_rising_star": true
    },
    {
      "id": "github-datawhalechina-self-llm",
      "name": "self-llm",
      "author": "datawhalechina",
      "description": "„ÄäÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó„ÄãÈíàÂØπ‰∏≠ÂõΩÂÆùÂÆùÈáèË∫´ÊâìÈÄ†ÁöÑÂü∫‰∫éLinuxÁéØÂ¢ÉÂø´ÈÄüÂæÆË∞ÉÔºàÂÖ®ÂèÇÊï∞/LoraÔºâ„ÄÅÈÉ®ÁΩ≤ÂõΩÂÜÖÂ§ñÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºàLLMÔºâ/Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâÊïôÁ®ã",
      "task": "tool",
      "tags": [
        "chatglm",
        "chatglm3",
        "gemma-2b-it",
        "glm-4",
        "internlm2",
        "llama3",
        "llm",
        "lora",
        "minicpm",
        "q-wen",
        "qwen",
        "qwen1-5",
        "qwen2"
      ],
      "likes": 78167,
      "downloads": 78167,
      "lastModified": "2025-11-20T03:28:23Z",
      "lastModifiedTimestamp": 1763609303000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/datawhalechina/self-llm",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 2621,
          "open_issues": 147,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/46047812?v=4",
      "velocity": 28647.3,
      "is_rising_star": true
    },
    {
      "id": "github-Hannibal046-Awesome-LLM",
      "name": "Awesome-LLM",
      "author": "Hannibal046",
      "description": "Awesome-LLM: a curated list of Large Language Model",
      "task": "tool",
      "tags": [],
      "likes": 76756,
      "downloads": 76756,
      "lastModified": "2025-11-20T03:51:48Z",
      "lastModifiedTimestamp": 1763610708000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Hannibal046/Awesome-LLM",
          "homepage": "",
          "language": null,
          "forks": 2186,
          "open_issues": 51,
          "license": "Creative Commons Zero v1.0 Universal"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/38466901?v=4",
      "velocity": 28138,
      "is_rising_star": true
    },
    {
      "id": "github-warpdotdev-Warp",
      "name": "Warp",
      "author": "warpdotdev",
      "description": "Warp is the agentic development environment, built for coding with multiple AI agents.",
      "task": "tool",
      "tags": [
        "bash",
        "linux",
        "macos",
        "rust",
        "shell",
        "terminal",
        "wasm",
        "zsh"
      ],
      "likes": 75913,
      "downloads": 75913,
      "lastModified": "2025-11-20T02:32:56Z",
      "lastModifiedTimestamp": 1763605976000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/warpdotdev/Warp",
          "homepage": "https://warp.dev",
          "language": null,
          "forks": 580,
          "open_issues": 3932,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/71840468?v=4",
      "velocity": 27831.1,
      "is_rising_star": true
    },
    {
      "id": "github-TauricResearch-TradingAgents",
      "name": "TradingAgents",
      "author": "TauricResearch",
      "description": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "task": "tool",
      "tags": [
        "agent",
        "finance",
        "llm",
        "multiagent",
        "trading"
      ],
      "likes": 75667,
      "downloads": 75667,
      "lastModified": "2025-11-20T03:47:34Z",
      "lastModifiedTimestamp": 1763610454000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/TauricResearch/TradingAgents",
          "homepage": "https://arxiv.org/pdf/2412.20138",
          "language": "Python",
          "forks": 4704,
          "open_issues": 196,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/192884433?v=4",
      "velocity": 27716.7,
      "is_rising_star": true
    },
    {
      "id": "github-CopilotKit-CopilotKit",
      "name": "CopilotKit",
      "author": "CopilotKit",
      "description": "React UI + elegant infrastructure for AI Copilots, AI chatbots, and in-app AI agents. The Agentic last-mile ü™Å",
      "task": "tool",
      "tags": [
        "agent",
        "agents",
        "ai",
        "ai-agent",
        "ai-assistant",
        "assistant",
        "copilot",
        "copilot-chat",
        "hacktoberfest",
        "langchain",
        "langgraph",
        "llm",
        "nextjs",
        "open-source",
        "react",
        "reactjs",
        "ts",
        "typescript"
      ],
      "likes": 75068,
      "downloads": 75068,
      "lastModified": "2025-11-20T00:17:21Z",
      "lastModifiedTimestamp": 1763597841000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/CopilotKit/CopilotKit",
          "homepage": "https://docs.copilotkit.ai",
          "language": "TypeScript",
          "forks": 3339,
          "open_issues": 429,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/131273140?v=4",
      "velocity": 27513.2,
      "is_rising_star": true
    },
    {
      "id": "github-chroma-core-chroma",
      "name": "chroma",
      "author": "chroma-core",
      "description": "Open-source search and retrieval database for AI applications.",
      "task": "tool",
      "tags": [
        "ai",
        "database",
        "document-retrieval",
        "embeddings",
        "llm",
        "llms",
        "rag",
        "rust",
        "rust-lang",
        "vector-database"
      ],
      "likes": 73468,
      "downloads": 73468,
      "lastModified": "2025-11-20T03:37:47Z",
      "lastModifiedTimestamp": 1763609867000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/chroma-core/chroma",
          "homepage": "https://www.trychroma.com/",
          "language": "Rust",
          "forks": 1924,
          "open_issues": 495,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/105881770?v=4",
      "velocity": 26925.8,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-JARVIS",
      "name": "JARVIS",
      "author": "microsoft",
      "description": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf",
      "task": "tool",
      "tags": [
        "deep-learning",
        "platform",
        "pytorch"
      ],
      "likes": 73350,
      "downloads": 73350,
      "lastModified": "2025-11-19T21:32:35Z",
      "lastModifiedTimestamp": 1763587955000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/JARVIS",
          "homepage": "",
          "language": "Python",
          "forks": 2053,
          "open_issues": 332,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26895,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-BitNet",
      "name": "BitNet",
      "author": "microsoft",
      "description": "Official inference framework for 1-bit LLMs",
      "task": "tool",
      "tags": [],
      "likes": 73235,
      "downloads": 73235,
      "lastModified": "2025-11-19T20:17:12Z",
      "lastModifiedTimestamp": 1763583432000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/BitNet",
          "homepage": "",
          "language": "Python",
          "forks": 1895,
          "open_issues": 164,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26852.1,
      "is_rising_star": true
    },
    {
      "id": "github-assafelovic-gpt-researcher",
      "name": "gpt-researcher",
      "author": "assafelovic",
      "description": "An LLM agent that conducts deep research (local and web) on any given topic and generates a long report with citations.",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "automation",
        "deepresearch",
        "llms",
        "mcp",
        "mcp-server",
        "python",
        "research",
        "search",
        "webscraping"
      ],
      "likes": 72613,
      "downloads": 72613,
      "lastModified": "2025-11-20T03:46:16Z",
      "lastModifiedTimestamp": 1763610376000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/assafelovic/gpt-researcher",
          "homepage": "https://gptr.dev",
          "language": "Python",
          "forks": 3196,
          "open_issues": 149,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/13554167?v=4",
      "velocity": 26610.1,
      "is_rising_star": true
    },
    {
      "id": "github-e2b-dev-awesome-ai-agents",
      "name": "awesome-ai-agents",
      "author": "e2b-dev",
      "description": "A list of AI autonomous agents",
      "task": "tool",
      "tags": [
        "agent",
        "ai",
        "artificial-intelligence",
        "autogpt",
        "autonomous-agents",
        "awesome",
        "babyagi",
        "copilot",
        "gpt",
        "gpt-4",
        "gpt-engineer",
        "openai",
        "python"
      ],
      "likes": 72609,
      "downloads": 72609,
      "lastModified": "2025-11-20T03:42:10Z",
      "lastModifiedTimestamp": 1763610130000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/e2b-dev/awesome-ai-agents",
          "homepage": "https://e2b.dev/docs",
          "language": null,
          "forks": 2023,
          "open_issues": 78,
          "license": "Other"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/129434473?v=4",
      "velocity": 26607.9,
      "is_rising_star": true
    },
    {
      "id": "github-huggingface-smolagents",
      "name": "smolagents",
      "author": "huggingface",
      "description": "ü§ó smolagents: a barebones library for agents that think in code.",
      "task": "tool",
      "tags": [],
      "likes": 72115,
      "downloads": 72115,
      "lastModified": "2025-11-20T03:41:44Z",
      "lastModifiedTimestamp": 1763610104000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/huggingface/smolagents",
          "homepage": "https://huggingface.co/docs/smolagents",
          "language": "Python",
          "forks": 2138,
          "open_issues": 319,
          "license": "Apache License 2.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/25720743?v=4",
      "velocity": 26429.7,
      "is_rising_star": true
    },
    {
      "id": "github-gitleaks-gitleaks",
      "name": "gitleaks",
      "author": "gitleaks",
      "description": "Find secrets with Gitleaks üîë",
      "task": "tool",
      "tags": [
        "ai-powered",
        "ci-cd",
        "cicd",
        "cli",
        "data-loss-prevention",
        "devsecops",
        "dlp",
        "git",
        "gitleaks",
        "go",
        "golang",
        "hacktoberfest",
        "llm",
        "llm-inference",
        "llm-training",
        "nhi",
        "open-source",
        "secret",
        "security",
        "security-tools"
      ],
      "likes": 71915,
      "downloads": 71915,
      "lastModified": "2025-11-20T00:55:23Z",
      "lastModifiedTimestamp": 1763600123000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/gitleaks/gitleaks",
          "homepage": "https://gitleaks.io",
          "language": "Go",
          "forks": 1833,
          "open_issues": 318,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/90395851?v=4",
      "velocity": 26361.5,
      "is_rising_star": true
    },
    {
      "id": "github-microsoft-OmniParser",
      "name": "OmniParser",
      "author": "microsoft",
      "description": "A simple screen parsing tool towards pure vision based GUI agent",
      "task": "tool",
      "tags": [],
      "likes": 71594,
      "downloads": 71594,
      "lastModified": "2025-11-20T03:02:21Z",
      "lastModifiedTimestamp": 1763607741000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/microsoft/OmniParser",
          "homepage": "",
          "language": "Jupyter Notebook",
          "forks": 2045,
          "open_issues": 225,
          "license": "Creative Commons Attribution 4.0 International"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/6154722?v=4",
      "velocity": 26228.4,
      "is_rising_star": true
    },
    {
      "id": "github-asgeirtj-system_prompts_leaks",
      "name": "system_prompts_leaks",
      "author": "asgeirtj",
      "description": "Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude & Gemini",
      "task": "tool",
      "tags": [
        "ai",
        "anthropic",
        "chatbots",
        "chatgpt",
        "claude",
        "gemini",
        "generative-ai",
        "google-deepmind",
        "large-language-models",
        "llm",
        "openai",
        "prompt-engineering",
        "prompt-injection",
        "prompts"
      ],
      "likes": 71254,
      "downloads": 71254,
      "lastModified": "2025-11-20T03:00:34Z",
      "lastModifiedTimestamp": 1763607634000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/asgeirtj/system_prompts_leaks",
          "homepage": "",
          "language": "JavaScript",
          "forks": 3630,
          "open_issues": 20,
          "license": "No license"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/27446620?v=4",
      "velocity": 26107.4,
      "is_rising_star": true
    },
    {
      "id": "github-Fosowl-agenticSeek",
      "name": "agenticSeek",
      "author": "Fosowl",
      "description": "Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. üîî Official updates only via twitter @Martin993886460 (Beware of fake account)",
      "task": "tool",
      "tags": [
        "agentic-ai",
        "agents",
        "ai",
        "autonomous-agents",
        "deepseek-r1",
        "llm",
        "llm-agents",
        "voice-assistant"
      ],
      "likes": 71111,
      "downloads": 71111,
      "lastModified": "2025-11-20T03:40:13Z",
      "lastModifiedTimestamp": 1763610013000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/Fosowl/agenticSeek",
          "homepage": "http://agenticseek.tech",
          "language": "Python",
          "forks": 2566,
          "open_issues": 36,
          "license": "GNU General Public License v3.0"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/49105846?v=4",
      "velocity": 26071.1,
      "is_rising_star": true
    },
    {
      "id": "github-HKUDS-LightRAG",
      "name": "LightRAG",
      "author": "HKUDS",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "task": "tool",
      "tags": [
        "genai",
        "gpt",
        "gpt-4",
        "graphrag",
        "knowledge-graph",
        "large-language-models",
        "llm",
        "rag",
        "retrieval-augmented-generation"
      ],
      "likes": 71270,
      "downloads": 71270,
      "lastModified": "2025-11-20T03:56:38Z",
      "lastModifiedTimestamp": 1763610998000,
      "readme": null,
      "downloadUrl": null,
      "sources": [
        {
          "platform": "GitHub",
          "url": "https://github.com/HKUDS/LightRAG",
          "homepage": "https://arxiv.org/abs/2410.05779",
          "language": "Python",
          "forks": 3484,
          "open_issues": 170,
          "license": "MIT License"
        }
      ],
      "thumbnail": "https://avatars.githubusercontent.com/u/118165258?v=4",
      "velocity": 26045.8,
      "is_rising_star": true
    }
  ],
  "generatedAt": "2025-11-20T04:02:10.738Z"
}