---
/**
 * Knowledge Base Article Template
 * V4.4 Phase 3 - Dynamic article rendering with SEO
 */

export const prerender = false;

import Layout from '../../layouts/Layout.astro';

// Get article slug
const { slug } = Astro.params;

// Article content database
const articles: Record<string, { title: string; description: string; category: string; content: string }> = {
  'what-is-mmlu': {
    title: 'What is MMLU?',
    description: 'Massive Multitask Language Understanding (MMLU) is a benchmark designed to measure knowledge acquired during pretraining by evaluating models on 57 subjects.',
    category: 'Benchmarks',
    content: `
## Overview

**MMLU (Massive Multitask Language Understanding)** is one of the most widely used benchmarks for evaluating large language models. It tests a model's knowledge and reasoning abilities across 57 different subjects.

## What It Measures

MMLU evaluates models on:

- **Humanities**: History, Philosophy, Law
- **STEM**: Mathematics, Physics, Computer Science
- **Social Sciences**: Economics, Psychology, Sociology
- **Other**: Professional exams, General knowledge

## Scoring

Models are scored as a percentage of correct answers:

| Score Range | Interpretation |
|-------------|----------------|
| 85%+ | Excellent (PhD-level) |
| 70-85% | Good (Graduate-level) |
| 50-70% | Fair (Undergraduate-level) |
| <50% | Needs improvement |

## Top Performers (2024)

1. **GPT-4o**: ~90%
2. **Claude 3.5 Sonnet**: ~88%
3. **Qwen2.5-72B**: ~85%
4. **Llama 3.1 70B**: ~82%

## Why It Matters

MMLU is important because:

- Tests broad knowledge, not just language fluency
- Covers real-world subjects relevant to users
- Widely adopted, enabling fair comparison
- Correlates well with real-world usefulness

## Limitations

- Primarily English-focused
- Multiple-choice format only
- Static dataset (knowledge cutoff)
- Doesn't test reasoning chains

## Related Benchmarks

- **MMLU-Pro**: Extended version with harder questions
- **ARC**: Science reasoning questions
- **HellaSwag**: Commonsense reasoning
    `
  },
  'what-is-humaneval': {
    title: 'What is HumanEval?',
    description: 'HumanEval is a benchmark for evaluating code generation capabilities of language models using Python programming problems.',
    category: 'Benchmarks',
    content: `
## Overview

**HumanEval** is OpenAI's benchmark for evaluating code generation capabilities. It consists of 164 hand-written Python programming problems.

## What It Measures

Each problem includes:
- Function signature
- Docstring describing the task
- Unit tests for verification

Models must generate working code that passes all tests.

## Scoring (pass@k)

- **pass@1**: Probability of first attempt being correct
- **pass@10**: Probability of at least one of 10 attempts being correct
- **pass@100**: With 100 attempts

## Top Performers (2024)

| Model | pass@1 |
|-------|--------|
| Claude 3.5 Sonnet | 92% |
| GPT-4o | 91% |
| DeepSeek-V2.5 | 85% |
| Qwen2.5-72B | 87% |

## Why It Matters

- Tests practical programming ability
- Objectively verified (code runs or doesn't)
- Relevant for coding assistants

## Related Benchmarks

- **MBPP**: Mostly Basic Python Problems
- **HumanEval+**: Extended with more tests
- **SWE-Bench**: Real-world GitHub issues
    `
  },
  'what-is-context-length': {
    title: 'What is Context Length?',
    description: 'Context length is the maximum number of tokens a language model can process at once, determining how much text it can "remember".',
    category: 'Architecture',
    content: `
## Overview

**Context length** (or context window) is the maximum number of tokens a language model can process in a single request.

## Token Basics

- 1 token ‚âà 4 characters in English
- 1 token ‚âà 0.75 words
- 1000 tokens ‚âà 750 words

## Common Context Lengths

| Size | Tokens | Approx. Words |
|------|--------|---------------|
| Small | 4K | 3,000 |
| Medium | 8K | 6,000 |
| Large | 32K | 24,000 |
| Extended | 128K | 96,000 |
| Ultra | 1M+ | 750,000+ |

## Why It Matters

**Longer context = more capability:**
- Process entire documents
- Maintain conversation history
- Analyze codebases
- Compare multiple sources

## Trade-offs

| Larger Context | Smaller Context |
|----------------|-----------------|
| ‚úÖ More information | ‚úÖ Faster inference |
| ‚ùå Slower processing | ‚ùå Limited memory |
| ‚ùå Higher memory usage | ‚úÖ Lower cost |
| ‚ùå May lose focus | ‚úÖ More focused |

## Models by Context Length

- **128K+**: GPT-4o, Claude 3.5, DeepSeek-V2.5
- **32K**: Qwen2.5, Mistral
- **8K**: Llama 3.1, Gemma 2
    `
  },
  'what-is-fni': {
    title: 'What is FNI?',
    description: 'Fair Nexus Index (FNI) is our transparent, multi-dimensional scoring system for ranking AI models on Free2AITools.',
    category: 'Metrics',
    content: `
## Overview

**FNI (Fair Nexus Index)** is Free2AITools' proprietary scoring system for ranking AI models. It provides a transparent, multi-dimensional view of model quality.

## The Four Dimensions

### P - Popularity (30%)
- Download counts
- Like counts
- Community adoption

### V - Velocity (25%)
- 7-day growth rate
- Trending momentum
- Recent activity

### C - Credibility (25%)
- Benchmark scores (MMLU, HumanEval, etc.)
- Organization reputation
- Documentation quality

### U - Utility (20%)
- Deploy score
- Format availability (GGUF, Ollama)
- Practical usability

## Score Range

| FNI Score | Percentile | Interpretation |
|-----------|------------|----------------|
| 90+ | Top 1% | Exceptional |
| 75-90 | Top 10% | Excellent |
| 50-75 | Top 25% | Good |
| 25-50 | Average | Moderate |
| <25 | Below Average | Limited |

## Why FNI?

- **Transparent**: All factors explained
- **Multi-dimensional**: Not just downloads
- **Updated daily**: Fresh rankings
- **Fair**: Considers smaller models too

## How to Improve FNI

1. Publish quality benchmarks
2. Provide GGUF/Ollama support
3. Write good documentation
4. Engage community
    `
  },
  'what-is-deploy-score': {
    title: 'What is Deploy Score?',
    description: 'Deploy Score measures how easily a model can be deployed and run locally, considering factors like size, format availability, and tool support.',
    category: 'Metrics',
    content: `
## Overview

**Deploy Score** is a 0-1.0 metric that measures how easily an AI model can be deployed and run locally or in production.

## Score Formula (V4.4)

\`\`\`
Deploy Score = 
  0.25 √ó GGUF availability +
  0.25 √ó Ollama availability +
  0.20 √ó Context length factor +
  0.15 √ó Size factor +
  0.15 √ó Quantization formats
\`\`\`

## Factors Explained

### GGUF Availability (+25%)
Models with GGUF quantized versions are much easier to run locally using llama.cpp, Ollama, or LM Studio.

### Ollama Support (+25%)
If a model is available in the Ollama library, deployment is as simple as:
\`\`\`bash
ollama run model-name
\`\`\`

### Context Length (+0-20%)
Longer context = more useful, but also harder to run:
- 32K+: +20%
- 8K-32K: +15%
- 4K-8K: +10%
- <4K: +5%

### Model Size (+5-15%)
Smaller models are easier to deploy:
- <10B: +15%
- 10-40B: +10%
- 40B+: +5%

### Quantization Formats (+0-15%)
More quantization options = more flexibility:
- Q4, Q5, Q8, FP16, etc.

## Score Interpretation

| Score | Deployment Ease |
|-------|-----------------|
| 0.8+ | Excellent - One-click deploy |
| 0.5-0.8 | Good - Some setup required |
| 0.3-0.5 | Moderate - Technical knowledge needed |
| <0.3 | Complex - Significant resources required |
    `
  }
};

// Get current article or 404
const article = articles[slug || ''];
const pageTitle = article ? `${article.title} - AI Knowledge Base` : 'Article Not Found';
const pageDescription = article?.description || 'This article was not found.';

// Related articles (same category)
const relatedArticles = Object.entries(articles)
  .filter(([key, a]) => a.category === article?.category && key !== slug)
  .slice(0, 3);
---

<Layout title={pageTitle} description={pageDescription}>
  <div class="article-container">
    {article ? (
      <>
        <!-- Article Header -->
        <header class="article-header">
          <nav class="breadcrumb">
            <a href="/knowledge">Knowledge Base</a>
            <span>‚Ä∫</span>
            <span>{article.category}</span>
          </nav>
          <h1 class="article-title">{article.title}</h1>
          <p class="article-description">{article.description}</p>
        </header>

        <!-- Article Content -->
        <article class="article-content" set:html={article.content.replace(/\n/g, '<br>').replace(/## /g, '<h2>').replace(/<br><h2>/g, '</p><h2>').replace(/<h2>([^<]+)/g, '<h2>$1</h2><p>')} />

        <!-- Related Articles -->
        {relatedArticles.length > 0 && (
          <aside class="related-articles">
            <h3>Related Articles</h3>
            <div class="related-grid">
              {relatedArticles.map(([key, a]) => (
                <a href={`/knowledge/${key}`} class="related-card">
                  <span class="related-title">{a.title}</span>
                  <span class="related-desc">{a.description.substring(0, 80)}...</span>
                </a>
              ))}
            </div>
          </aside>
        )}

        <!-- V4.9.1 CEO Iron Rule: Used In Entity Pages (3+ references mandatory) -->
        <aside class="used-in-section">
          <h3>üîó Used In (Models referencing this concept)</h3>
          <p class="used-in-description">
            This knowledge base article is referenced by the following model pages:
          </p>
          <div class="used-in-grid">
            {/* Static examples - will be populated dynamically from L8 cache */}
            {slug === 'what-is-mmlu' && (
              <>
                <a href="/model/meta-llama--Llama-3-2-1B" class="used-in-card">üìä Llama 3.2 1B</a>
                <a href="/model/Qwen--Qwen2.5-72B" class="used-in-card">üìä Qwen 2.5 72B</a>
                <a href="/model/mistralai--Mistral-7B-v0.1" class="used-in-card">üìä Mistral 7B</a>
              </>
            )}
            {slug === 'what-is-humaneval' && (
              <>
                <a href="/model/deepseek-ai--deepseek-coder-33b-instruct" class="used-in-card">üíª DeepSeek Coder 33B</a>
                <a href="/model/codellama--CodeLlama-70b-hf" class="used-in-card">üíª CodeLlama 70B</a>
                <a href="/model/meta-llama--Llama-3-2-3B" class="used-in-card">üíª Llama 3.2 3B</a>
              </>
            )}
            {slug === 'what-is-fni' && (
              <>
                <a href="/explore" class="used-in-card">üè† All Models (FNI Score)</a>
                <a href="/leaderboard" class="used-in-card">üèÜ Leaderboard</a>
                <a href="/methodology" class="used-in-card">üìã Methodology</a>
              </>
            )}
            {slug === 'what-is-context-length' && (
              <>
                <a href="/model/mistralai--Mistral-7B-v0.1" class="used-in-card">üìê Mistral 7B (32K)</a>
                <a href="/model/deepseek-ai--deepseek-v2.5" class="used-in-card">üìê DeepSeek V2.5 (128K)</a>
                <a href="/model/Qwen--Qwen2.5-72B" class="used-in-card">üìê Qwen 2.5 72B (128K)</a>
              </>
            )}
            {slug === 'what-is-deploy-score' && (
              <>
                <a href="/model/meta-llama--Llama-3-2-1B" class="used-in-card">‚ö° Llama 3.2 1B (easy deploy)</a>
                <a href="/model/Qwen--Qwen2.5-7B" class="used-in-card">‚ö° Qwen 2.5 7B</a>
                <a href="/explore?sort=deploy" class="used-in-card">‚ö° Deploy Score Sort</a>
              </>
            )}
            {/* Fallback for articles without specific links */}
            {!['what-is-mmlu', 'what-is-humaneval', 'what-is-fni', 'what-is-context-length', 'what-is-deploy-score'].includes(slug || '') && (
              <>
                <a href="/explore" class="used-in-card">üîç Explore Models</a>
                <a href="/leaderboard" class="used-in-card">üèÜ Leaderboard</a>
                <a href="/knowledge" class="used-in-card">üìö More Articles</a>
              </>
            )}
          </div>
          <p class="used-in-note">
            üìñ CEO Iron Rule: Each knowledge article must be cited by 3+ entity pages
          </p>
        </aside>

        <!-- CTA -->
        <div class="article-cta">
          <a href="/leaderboard" class="btn-primary">View Benchmark Leaderboard</a>
          <a href="/explore" class="btn-secondary">Explore Models</a>
        </div>
      </>
    ) : (
      <div class="not-found">
        <h1>Article Not Found</h1>
        <p>The article you're looking for doesn't exist.</p>
        <a href="/knowledge" class="btn-primary">Back to Knowledge Base</a>
      </div>
    )}
  </div>
</Layout>

<style>
  .article-container {
    max-width: 800px;
    margin: 0 auto;
    padding: 2rem 1rem;
  }

  .article-header {
    margin-bottom: 2rem;
  }

  /* V4.9.1 WCAG 2.1 AA: Improved contrast */
  .breadcrumb {
    display: flex;
    gap: 0.5rem;
    color: #8b9aae;  /* Was #64748b - improved for WCAG AA */
    font-size: 0.875rem;
    margin-bottom: 1rem;
  }

  .breadcrumb a {
    color: #818cf8;
    text-decoration: none;
  }

  .breadcrumb a:hover {
    text-decoration: underline;
  }

  .article-title {
    font-size: 2.25rem;
    font-weight: 800;
    color: #f8fafc;
    margin: 0 0 0.75rem;
  }

  .article-description {
    color: #b8c5d6;  /* Was #94a3b8 - improved for WCAG AA */
    font-size: 1.125rem;
    line-height: 1.6;
    margin: 0;
  }

  .article-content {
    color: #cbd5e1;
    line-height: 1.8;
  }

  .article-content :global(h2) {
    color: #e2e8f0;
    font-size: 1.5rem;
    font-weight: 700;
    margin: 2rem 0 1rem;
  }

  .article-content :global(p) {
    margin: 1rem 0;
  }

  .article-content :global(ul), .article-content :global(ol) {
    padding-left: 1.5rem;
    margin: 1rem 0;
  }

  .article-content :global(li) {
    margin: 0.5rem 0;
  }

  .article-content :global(table) {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
  }

  .article-content :global(th), .article-content :global(td) {
    padding: 0.75rem;
    border: 1px solid rgba(51, 65, 85, 0.5);
    text-align: left;
  }

  .article-content :global(th) {
    background: rgba(99, 102, 241, 0.15);
    color: #e2e8f0;
  }

  .article-content :global(code) {
    background: rgba(30, 41, 59, 0.8);
    padding: 0.125rem 0.375rem;
    border-radius: 4px;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.875em;
    color: #4ade80;
  }

  .article-content :global(pre) {
    background: rgba(15, 23, 42, 0.9);
    padding: 1rem;
    border-radius: 8px;
    overflow-x: auto;
  }

  /* Related Articles */
  .related-articles {
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 1px solid rgba(99, 102, 241, 0.2);
  }

  .related-articles h3 {
    color: #e2e8f0;
    font-size: 1.125rem;
    margin: 0 0 1rem;
  }

  .related-grid {
    display: grid;
    gap: 1rem;
  }

  .related-card {
    display: block;
    padding: 1rem;
    background: rgba(30, 41, 59, 0.6);
    border: 1px solid rgba(99, 102, 241, 0.2);
    border-radius: 8px;
    text-decoration: none;
    transition: all 0.2s;
  }

  .related-card:hover {
    border-color: #6366f1;
    transform: translateX(4px);
  }

  .related-title {
    display: block;
    color: #e2e8f0;
    font-weight: 600;
    margin-bottom: 0.25rem;
  }

  .related-desc {
    display: block;
    color: #64748b;
    font-size: 0.8rem;
  }

  /* V4.9.1: Used In Entity Links Section */
  .used-in-section {
    margin-top: 2rem;
    padding: 1.5rem;
    background: rgba(99, 102, 241, 0.08);
    border: 1px solid rgba(99, 102, 241, 0.25);
    border-radius: 12px;
  }

  .used-in-section h3 {
    color: #e2e8f0;
    font-size: 1rem;
    margin: 0 0 0.5rem;
  }

  .used-in-description {
    color: #94a3b8;
    font-size: 0.875rem;
    margin: 0 0 1rem;
  }

  .used-in-grid {
    display: flex;
    flex-wrap: wrap;
    gap: 0.75rem;
    margin-bottom: 1rem;
  }

  .used-in-card {
    padding: 0.5rem 1rem;
    background: rgba(30, 41, 59, 0.8);
    border: 1px solid rgba(99, 102, 241, 0.3);
    border-radius: 8px;
    color: #818cf8;
    font-size: 0.875rem;
    text-decoration: none;
    transition: all 0.2s;
  }

  .used-in-card:hover {
    border-color: #6366f1;
    background: rgba(99, 102, 241, 0.15);
    transform: translateY(-2px);
  }

  .used-in-note {
    color: #64748b;
    font-size: 0.75rem;
    margin: 0;
    font-style: italic;
  }

  /* CTA */
  .article-cta {
    display: flex;
    gap: 1rem;
    margin-top: 2rem;
    padding-top: 2rem;
    border-top: 1px solid rgba(99, 102, 241, 0.2);
  }

  .btn-primary {
    padding: 0.75rem 1.5rem;
    background: linear-gradient(135deg, #6366f1, #8b5cf6);
    color: white;
    border-radius: 8px;
    font-weight: 600;
    text-decoration: none;
  }

  .btn-secondary {
    padding: 0.75rem 1.5rem;
    border: 1px solid rgba(99, 102, 241, 0.5);
    color: #818cf8;
    border-radius: 8px;
    text-decoration: none;
  }

  /* Not Found */
  .not-found {
    text-align: center;
    padding: 4rem 2rem;
  }

  .not-found h1 {
    color: #e2e8f0;
    margin-bottom: 1rem;
  }

  .not-found p {
    color: #94a3b8;
    margin-bottom: 2rem;
  }
</style>
