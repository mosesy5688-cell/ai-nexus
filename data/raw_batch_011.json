[
  {
    "id": "arxiv:2512.07747v1",
    "name": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation",
    "author": "Shihao Zhao",
    "description": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limit...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07747v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.",
    "meta_json": "{\"arxiv_id\":\"2512.07747v1\",\"authors\":[\"Shihao Zhao\",\"Yitong Chen\",\"Zeyinzi Jiang\",\"Bojia Zi\",\"Shaozhe Hao\",\"Yu Liu\",\"Chaojie Mao\",\"Kwan-Yee K. Wong\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:34:15Z\",\"updated_date\":\"2025-12-08T17:34:15Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c247ce0dce6aa228105ad4e3bb31b22a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07747v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07745v1",
    "name": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving",
    "author": "Jialv Zou",
    "description": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which lev...",
    "tags": [
      "arxiv:cs.CV",
      "diffusion",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07745v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2",
    "meta_json": "{\"arxiv_id\":\"2512.07745v1\",\"authors\":[\"Jialv Zou\",\"Shaoyu Chen\",\"Bencheng Liao\",\"Zhiyu Zheng\",\"Yuehao Song\",\"Lefei Zhang\",\"Qian Zhang\",\"Wenyu Liu\",\"Xinggang Wang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:29:52Z\",\"updated_date\":\"2025-12-08T17:29:52Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:hustvl:DiffusionDriveV2\",\"source_url\":\"https://github.com/hustvl/DiffusionDriveV2\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "294e7933981b1ba14f72f4044c5e897b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07745v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07741v1",
    "name": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data",
    "author": "Agnes Norbury",
    "description": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demon...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.SD",
      "multimodal"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07741v1",
    "image_url": null,
    "type": "paper",
    "body_content": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC&gt;0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.",
    "meta_json": "{\"arxiv_id\":\"2512.07741v1\",\"authors\":[\"Agnes Norbury\",\"George Fairs\",\"Alexandra L. Georgescu\",\"Matthew M. Nour\",\"Emilia Molimpakis\",\"Stefano Goria\"],\"categories\":[\"cs.LG\",\"cs.SD\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:28:09Z\",\"updated_date\":\"2025-12-08T17:28:09Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SD\",\"source_url\":\"https://arxiv.org/abs/cs.SD\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "0d969e74eebd47b75b2b26a824ffdcde",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07741v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07738v1",
    "name": "HLTCOE Evaluation Team at TREC 2025: VQA Track",
    "author": "Dengjia Zhang",
    "description": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07738v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.",
    "meta_json": "{\"arxiv_id\":\"2512.07738v1\",\"authors\":[\"Dengjia Zhang\",\"Charles Weng\",\"Katherine Guerrerio\",\"Yi Lu\",\"Kenton Murray\",\"Alexander Martin\",\"Reno Kriz\",\"Benjamin Van Durme\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:25:13Z\",\"updated_date\":\"2025-12-08T17:25:13Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "05dc6fed9d57d41afd7093cdfd958996",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07738v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07737v1",
    "name": "A scalable and real-time neural decoder for topological quantum codes",
    "author": "Andrew W. Senior",
    "description": "Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error ...",
    "tags": [
      "arxiv:quant-ph",
      "arxiv:cs.LG",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07737v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.",
    "meta_json": "{\"arxiv_id\":\"2512.07737v1\",\"authors\":[\"Andrew W. Senior\",\"Thomas Edlich\",\"Francisco J. H. Heras\",\"Lei M. Zhang\",\"Oscar Higgott\",\"James S. Spencer\",\"Taylor Applebaum\",\"Sam Blackwell\",\"Justin Ledford\",\"Akvilė Žemgulytė\",\"Augustin Žídek\",\"Noah Shutty\",\"Andrew Cowie\",\"Yin Li\",\"George Holland\",\"Peter Brooks\",\"Charlie Beattie\",\"Michael Newman\",\"Alex Davies\",\"Cody Jones\",\"Sergio Boixo\",\"Hartmut Neven\",\"Pushmeet Kohli\",\"Johannes Bausch\"],\"categories\":[\"quant-ph\",\"cs.LG\"],\"primary_category\":\"quant-ph\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:24:59Z\",\"updated_date\":\"2025-12-08T17:24:59Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:quant-ph\",\"source_url\":\"https://arxiv.org/abs/quant-ph\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "be304af23a9bab2603d90bb9696ce208",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07737v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07733v1",
    "name": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
    "author": "Meng Cao",
    "description": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visua...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07733v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.",
    "meta_json": "{\"arxiv_id\":\"2512.07733v1\",\"authors\":[\"Meng Cao\",\"Xingyu Li\",\"Xue Liu\",\"Ian Reid\",\"Xiaodan Liang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:20:50Z\",\"updated_date\":\"2025-12-08T17:20:50Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "baf1956a15eb0482813c5a60c65030e1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07733v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07730v1",
    "name": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination",
    "author": "Sangha Park",
    "description": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's vis...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07730v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.",
    "meta_json": "{\"arxiv_id\":\"2512.07730v1\",\"authors\":[\"Sangha Park\",\"Seungryong Yoo\",\"Jisoo Mok\",\"Sungroh Yoon\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:20:07Z\",\"updated_date\":\"2025-12-08T17:20:07Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:wiarae:SAVE.\",\"source_url\":\"https://github.com/wiarae/SAVE.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "ec4b769cbc3b2bb96ab9f4e42af3c96e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07730v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07729v1",
    "name": "Improving action classification with brain-inspired deep networks",
    "author": "Aidas Aglinskas",
    "description": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, with...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07729v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.",
    "meta_json": "{\"arxiv_id\":\"2512.07729v1\",\"authors\":[\"Aidas Aglinskas\",\"Stefano Anzellotti\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:19:47Z\",\"updated_date\":\"2025-12-08T17:19:47Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "4c4e82327b56f966d2044504e9a92cea",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07729v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07724v1",
    "name": "The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic",
    "author": "Zhengzheng Tang",
    "description": "The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To travers...",
    "tags": [
      "arxiv:cs.ET",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07724v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap \"from stochastic ions to deterministic floats,\" we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.",
    "meta_json": "{\"arxiv_id\":\"2512.07724v1\",\"authors\":[\"Zhengzheng Tang\"],\"categories\":[\"cs.ET\",\"cs.AI\"],\"primary_category\":\"cs.ET\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:15:46Z\",\"updated_date\":\"2025-12-08T17:15:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.ET\",\"source_url\":\"https://arxiv.org/abs/cs.ET\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "b9df6f59533e049aa20872a6495aeea6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07724v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07723v1",
    "name": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity",
    "author": "Yonggeon Lee",
    "description": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into gri...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "transformer"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07723v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.",
    "meta_json": "{\"arxiv_id\":\"2512.07723v1\",\"authors\":[\"Yonggeon Lee\",\"Jibin Hwang\",\"Alfred Malengo Kondoro\",\"Juhyun Song\",\"Youngtae Noh\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:14:32Z\",\"updated_date\":\"2025-12-08T17:14:32Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "8b4fae06b0e18fe29814b26550219598",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07723v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07720v1",
    "name": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation",
    "author": "Fan Yang",
    "description": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable beh...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07720v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa",
    "meta_json": "{\"arxiv_id\":\"2512.07720v1\",\"authors\":[\"Fan Yang\",\"Heyuan Li\",\"Peihao Li\",\"Weihao Yuan\",\"Lingteng Qiu\",\"Chaoyue Song\",\"Cheng Chen\",\"Yisheng He\",\"Shifeng Zhang\",\"Xiaoguang Han\",\"Steven Hoi\",\"Guosheng Lin\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:10:29Z\",\"updated_date\":\"2025-12-08T17:10:29Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "b53457b8127d620d33078e230f12db56",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07720v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07712v1",
    "name": "UnCageNet: Tracking and Pose Estimation of Caged Animal",
    "author": "Sayak Dutta",
    "description": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction o...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07712v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.",
    "meta_json": "{\"arxiv_id\":\"2512.07712v1\",\"authors\":[\"Sayak Dutta\",\"Harish Katti\",\"Shashikant Verma\",\"Shanmuganathan Raman\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:00:06Z\",\"updated_date\":\"2025-12-08T17:00:06Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "ac756852d8f2a688e18d54ce78dff3e2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07712v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07710v1",
    "name": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE",
    "author": "Anxiang Zeng",
    "description": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.LG",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07710v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.",
    "meta_json": "{\"arxiv_id\":\"2512.07710v1\",\"authors\":[\"Anxiang Zeng\",\"Haibo Zhang\",\"Hailing Zhang\",\"Kaixiang Mo\",\"Liang Yao\",\"Ling Hu\",\"Long Zhang\",\"Shuman Liu\",\"Shuyi Xie\",\"Yanshi Li\",\"Yizhang Chen\",\"Yuepeng Sheng\",\"Yuwei Huang\",\"Zhaochen Xu\",\"Zhiqiang Zhou\",\"Ziqin Liew\"],\"categories\":[\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:57:43Z\",\"updated_date\":\"2025-12-08T16:57:43Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "8448f8ee1614c2cd26b1235fa253ab8b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07710v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07705v1",
    "name": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models",
    "author": "Saroj Gopali",
    "description": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Goog...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07705v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data. This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning. These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.",
    "meta_json": "{\"arxiv_id\":\"2512.07705v1\",\"authors\":[\"Saroj Gopali\",\"Bipin Chhetri\",\"Deepika Giri\",\"Sima Siami-Namini\",\"Akbar Siami Namin\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:52:46Z\",\"updated_date\":\"2025-12-08T16:52:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "818920a1c5cb08d1f07586651b936ec3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07705v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07703v1",
    "name": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation",
    "author": "Leo Fillioux",
    "description": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones wi...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07703v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.",
    "meta_json": "{\"arxiv_id\":\"2512.07703v1\",\"authors\":[\"Leo Fillioux\",\"Enzo Ferrante\",\"Paul-Henry Cournède\",\"Maria Vakalopoulou\",\"Stergios Christodoulidis\"],\"categories\":[\"cs.CV\",\"cs.LG\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:50:21Z\",\"updated_date\":\"2025-12-08T16:50:21Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:leofillioux:pvera.\",\"source_url\":\"https://github.com/leofillioux/pvera.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a613ac1a322d81ea749f792c7446e721",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07703v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07702v1",
    "name": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment",
    "author": "Sangha Park",
    "description": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those d...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07702v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.",
    "meta_json": "{\"arxiv_id\":\"2512.07702v1\",\"authors\":[\"Sangha Park\",\"Eunji Kim\",\"Yeongtak Oh\",\"Jooyoung Choi\",\"Sungroh Yoon\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:49:19Z\",\"updated_date\":\"2025-12-08T16:49:19Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:wiarae:NPC.\",\"source_url\":\"https://github.com/wiarae/NPC.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a1faca1515c9df65d9132edcb37805de",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07702v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07698v1",
    "name": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only",
    "author": "Arslan Artykov",
    "description": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular ...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.RO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07698v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/",
    "meta_json": "{\"arxiv_id\":\"2512.07698v1\",\"authors\":[\"Arslan Artykov\",\"Corentin Sautier\",\"Vincent Lepetit\"],\"categories\":[\"cs.CV\",\"cs.RO\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:38:30Z\",\"updated_date\":\"2025-12-08T16:38:30Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "02464ada83cf86e528cd90e11040f43c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07698v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07694v1",
    "name": "Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map",
    "author": "Francois Vandenhende",
    "description": "In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (Safe...",
    "tags": [
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07694v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (&gt;95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.",
    "meta_json": "{\"arxiv_id\":\"2512.07694v1\",\"authors\":[\"Francois Vandenhende\",\"Anna Georgiou\",\"Michalis Georgiou\",\"Theodoros Psaras\",\"Ellie Karekla\",\"Elena Hadjicosta\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:33:03Z\",\"updated_date\":\"2025-12-08T16:33:03Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "2ac6cb6d99544c6824fc285a3bd8616b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07694v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07687v1",
    "name": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs",
    "author": "Sujoy Nath",
    "description": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.CV",
      "llm",
      "vision",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07687v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.",
    "meta_json": "{\"arxiv_id\":\"2512.07687v1\",\"authors\":[\"Sujoy Nath\",\"Arkaprabha Basu\",\"Sharanya Dasgupta\",\"Swagatam Das\"],\"categories\":[\"cs.CL\",\"cs.CV\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:24:46Z\",\"updated_date\":\"2025-12-08T16:24:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:C0mRD:HalluShift_Plus.\",\"source_url\":\"https://github.com/C0mRD/HalluShift_Plus.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "5a535ca7d5260ce175bcbb2406f15b16",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07687v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07684v1",
    "name": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks",
    "author": "Zihan Chen",
    "description": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression,...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "arxiv:cs.SI",
      "neural",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07684v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.",
    "meta_json": "{\"arxiv_id\":\"2512.07684v1\",\"authors\":[\"Zihan Chen\",\"Lanyu Yu\"],\"categories\":[\"cs.CL\",\"cs.AI\",\"cs.SI\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:22:40Z\",\"updated_date\":\"2025-12-08T16:22:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SI\",\"source_url\":\"https://arxiv.org/abs/cs.SI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "10b50a2cad4e37c8663c1bff8157209e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07684v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07676v1",
    "name": "A Bootstrap Perspective on Stochastic Gradient Descent",
    "author": "Hongjian Lan",
    "description": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized expe...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:stat.ML"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07676v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.",
    "meta_json": "{\"arxiv_id\":\"2512.07676v1\",\"authors\":[\"Hongjian Lan\",\"Yucong Liu\",\"Florian Schäfer\"],\"categories\":[\"cs.LG\",\"stat.ML\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:10:56Z\",\"updated_date\":\"2025-12-08T16:10:56Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "c4cbf099ac98b9b09b39ac3beaa0ba24",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07676v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07674v1",
    "name": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations",
    "author": "Mehmet Yigit Avci",
    "description": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variabili...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "clip"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07674v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.",
    "meta_json": "{\"arxiv_id\":\"2512.07674v1\",\"authors\":[\"Mehmet Yigit Avci\",\"Pedro Borges\",\"Virginia Fernandez\",\"Paul Wright\",\"Mehmet Yigitsoy\",\"Sebastien Ourselin\",\"Jorge Cardoso\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:09:10Z\",\"updated_date\":\"2025-12-08T16:09:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "78a6b7287b4e6cd75f4b98aab824cce1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07674v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07668v1",
    "name": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset",
    "author": "Ronan John",
    "description": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-fac...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07668v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .",
    "meta_json": "{\"arxiv_id\":\"2512.07668v1\",\"authors\":[\"Ronan John\",\"Aditya Kesari\",\"Vincenzo DiMatteo\",\"Kristin Dana\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:03:21Z\",\"updated_date\":\"2025-12-08T16:03:21Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ComputerVisionRutgers:EgoCampus\",\"source_url\":\"https://github.com/ComputerVisionRutgers/EgoCampus\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "04afb9a09849550c632acf99d9a4d296",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07668v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07667v1",
    "name": "Depth-Wise Activation Steering for Honest Language Models",
    "author": "Gracjan Góral",
    "description": "Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK be...",
    "tags": [
      "arxiv:cs.LG",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07667v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.",
    "meta_json": "{\"arxiv_id\":\"2512.07667v1\",\"authors\":[\"Gracjan Góral\",\"Marysia Winkels\",\"Steven Basart\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:03:06Z\",\"updated_date\":\"2025-12-08T16:03:06Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "760fa51fc5595ddd69d7bb42036b9f00",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07667v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07666v1",
    "name": "Bridging Code Graphs and Large Language Models for Better Code Understanding",
    "author": "Zeqi Chen",
    "description": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with ...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.SE",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07666v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.",
    "meta_json": "{\"arxiv_id\":\"2512.07666v1\",\"authors\":[\"Zeqi Chen\",\"Zhaoyang Chu\",\"Yi Gui\",\"Feng Guo\",\"Yao Wan\",\"Chuan Shi\"],\"categories\":[\"cs.CL\",\"cs.SE\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T16:00:29Z\",\"updated_date\":\"2025-12-08T16:00:29Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SE\",\"source_url\":\"https://arxiv.org/abs/cs.SE\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "6dea0e041a79033cfef641b7aabe3c0d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07666v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07661v1",
    "name": "Optimization-Guided Diffusion for Interactive Scene Generation",
    "author": "Shiaho Li",
    "description": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, a...",
    "tags": [
      "arxiv:cs.CV",
      "diffusion"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07661v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.",
    "meta_json": "{\"arxiv_id\":\"2512.07661v1\",\"authors\":[\"Shiaho Li\",\"Naisheng Ye\",\"Tianyu Li\",\"Kashyap Chitta\",\"Tuo An\",\"Peng Su\",\"Boyang Wang\",\"Haiou Liu\",\"Chen Lv\",\"Hongyang Li\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:56:18Z\",\"updated_date\":\"2025-12-08T15:56:18Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "b73a7593c0c03250125e041253b6e700",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07661v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07652v1",
    "name": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research",
    "author": "Hamad Almazrouei",
    "description": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction,...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07652v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.",
    "meta_json": "{\"arxiv_id\":\"2512.07652v1\",\"authors\":[\"Hamad Almazrouei\",\"Mariam Al Nasseri\",\"Maha Alzaabi\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:45:40Z\",\"updated_date\":\"2025-12-08T15:45:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "066ad8b1482c160516b19c47fc8f1790",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07652v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07651v1",
    "name": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method",
    "author": "Yuanye Liu",
    "description": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain s...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07651v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.",
    "meta_json": "{\"arxiv_id\":\"2512.07651v1\",\"authors\":[\"Yuanye Liu\",\"Hanxiao Zhang\",\"Nannan Shi\",\"Yuxin Shi\",\"Arif Mahmood\",\"Murtaza Taj\",\"Xiahai Zhuang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:44:24Z\",\"updated_date\":\"2025-12-08T15:44:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "411d9681665c09337749a579fb6a4ba9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07651v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07650v1",
    "name": "Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation",
    "author": "Fuyuan Lyu",
    "description": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lie...",
    "tags": [
      "arxiv:cs.IR",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07650v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.",
    "meta_json": "{\"arxiv_id\":\"2512.07650v1\",\"authors\":[\"Fuyuan Lyu\",\"Zhentai Chen\",\"Jingyan Jiang\",\"Lingjie Li\",\"Xing Tang\",\"Xiuqiang He\",\"Xue Liu\"],\"categories\":[\"cs.IR\",\"cs.LG\"],\"primary_category\":\"cs.IR\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:41:10Z\",\"updated_date\":\"2025-12-08T15:41:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.IR\",\"source_url\":\"https://arxiv.org/abs/cs.IR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "0c94a4526b292186c8d4055e458964c2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07650v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07647v1",
    "name": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance",
    "author": "Georgios Tzachristas",
    "description": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we deriv...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "attention"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07647v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=τ\\|μ_{\\mathrm{tail}}-μ_{\\mathrm{head}}\\|_2$ with $τ=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\leτ\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approxΦ_c(σ+Φ^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.",
    "meta_json": "{\"arxiv_id\":\"2512.07647v1\",\"authors\":[\"Georgios Tzachristas\",\"Lei Deng\",\"Ioannis Tzachristas\",\"Gong Zhang\",\"Renhai Chen\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:36:41Z\",\"updated_date\":\"2025-12-08T15:36:41Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "faff58da621436210edb19095a9e7fcd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07647v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07631v1",
    "name": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds",
    "author": "Shahar Lutati",
    "description": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource re...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.CC",
      "arxiv:cs.IT",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07631v1",
    "image_url": null,
    "type": "paper",
    "body_content": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\",
    "meta_json": "{\"arxiv_id\":\"2512.07631v1\",\"authors\":[\"Shahar Lutati\"],\"categories\":[\"cs.AI\",\"cs.CC\",\"cs.IT\",\"cs.LG\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:21:52Z\",\"updated_date\":\"2025-12-08T15:21:52Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CC\",\"source_url\":\"https://arxiv.org/abs/cs.CC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.IT\",\"source_url\":\"https://arxiv.org/abs/cs.IT\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "c6b60172827af349de9230018a75030a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07631v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07628v1",
    "name": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation",
    "author": "Zhiqi Li",
    "description": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of u...",
    "tags": [
      "arxiv:cs.CV",
      "attention"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07628v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA",
    "meta_json": "{\"arxiv_id\":\"2512.07628v1\",\"authors\":[\"Zhiqi Li\",\"Wenhuan Li\",\"Tengfei Wang\",\"Zhenwei Wang\",\"Junta Wu\",\"Haoyuan Wang\",\"Yunhan Yang\",\"Zehuan Huang\",\"Yang Li\",\"Peidong Liu\",\"Chunchao Guo\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:17:01Z\",\"updated_date\":\"2025-12-08T15:17:01Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "f766f4a20c1a979ba9d7d9be3b5d898e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07628v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07627v1",
    "name": "Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization",
    "author": "Maximos Kaliakatsos-Papakostas",
    "description": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization th...",
    "tags": [
      "arxiv:cs.SD",
      "arxiv:cs.AI",
      "arxiv:cs.SC",
      "transformer"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07627v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.",
    "meta_json": "{\"arxiv_id\":\"2512.07627v1\",\"authors\":[\"Maximos Kaliakatsos-Papakostas\",\"Konstantinos Soiledis\",\"Theodoros Tsamis\",\"Dimos Makris\",\"Vassilis Katsouros\",\"Emilios Cambouropoulos\"],\"categories\":[\"cs.SD\",\"cs.AI\",\"cs.SC\"],\"primary_category\":\"cs.SD\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:16:33Z\",\"updated_date\":\"2025-12-08T15:16:33Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SD\",\"source_url\":\"https://arxiv.org/abs/cs.SD\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SC\",\"source_url\":\"https://arxiv.org/abs/cs.SC\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "76b249f8b43fd2110eb7fc7c3ffc03c4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07627v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07624v1",
    "name": "Time Series Foundation Models for Process Model Forecasting",
    "author": "Yongbo Yu",
    "description": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07624v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.",
    "meta_json": "{\"arxiv_id\":\"2512.07624v1\",\"authors\":[\"Yongbo Yu\",\"Jari Peeperkorn\",\"Johannes De Smedt\",\"Jochen De Weerdt\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:08:50Z\",\"updated_date\":\"2025-12-08T15:08:50Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "cdd8e7772d482c91935d457d1f30b59d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07624v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07612v1",
    "name": "PCMind-2.1-Kaiyuan-2B Technical Report",
    "author": "Kairong Luo",
    "description": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchma...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07612v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.",
    "meta_json": "{\"arxiv_id\":\"2512.07612v1\",\"authors\":[\"Kairong Luo\",\"Zhenbo Sun\",\"Xinyu Shi\",\"Shengqi Chen\",\"Bowen Yu\",\"Yunyi Chen\",\"Chenyi Dang\",\"Hengtao Tao\",\"Hui Wang\",\"Fangming Liu\",\"Kaifeng Lyu\",\"Wenguang Chen\"],\"categories\":[\"cs.CL\",\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T15:00:10Z\",\"updated_date\":\"2025-12-08T15:00:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "1bd267b9bbc4418c0280d7ff3b0f466e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07612v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07611v1",
    "name": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement",
    "author": "Yongsheng Lian",
    "description": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement dif...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.LG",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07611v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark. Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.",
    "meta_json": "{\"arxiv_id\":\"2512.07611v1\",\"authors\":[\"Yongsheng Lian\"],\"categories\":[\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:58:19Z\",\"updated_date\":\"2025-12-08T14:58:19Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "9f45a14a5109bc4138d349c7564d152f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07611v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07608v1",
    "name": "Metric-Fair Prompting: Treating Similar Samples Similarly",
    "author": "Jing Wang",
    "description": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in ...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07608v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.",
    "meta_json": "{\"arxiv_id\":\"2512.07608v1\",\"authors\":[\"Jing Wang\",\"Jie Shen\",\"Xing Niu\",\"Tong Zhang\",\"Jeremy Weiss\"],\"categories\":[\"cs.CL\",\"cs.AI\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:56:46Z\",\"updated_date\":\"2025-12-08T14:56:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "80f5a1091115bc56936458899228dd73",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07608v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07606v1",
    "name": "Decomposition Sampling for Efficient Region Annotations in Active Learning",
    "author": "Jingna Qiu",
    "description": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods ...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07606v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.",
    "meta_json": "{\"arxiv_id\":\"2512.07606v1\",\"authors\":[\"Jingna Qiu\",\"Frauke Wilm\",\"Mathias Öttl\",\"Jonas Utz\",\"Maja Schlereth\",\"Moritz Schillinger\",\"Marc Aubreville\",\"Katharina Breininger\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:55:48Z\",\"updated_date\":\"2025-12-08T14:55:48Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:JingnaQiu:DECOMP.git.\",\"source_url\":\"https://github.com/JingnaQiu/DECOMP.git.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "d5a8b53e7ee8df37139fe016f24a1232",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07606v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07599v1",
    "name": "Online Segment Any 3D Thing as Instance Tracking",
    "author": "Hanshi Wang",
    "description": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07599v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.",
    "meta_json": "{\"arxiv_id\":\"2512.07599v1\",\"authors\":[\"Hanshi Wang\",\"Zijian Cai\",\"Jin Gao\",\"Yiwei Zhang\",\"Weiming Hu\",\"Ke Wang\",\"Zhipeng Zhang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:48:51Z\",\"updated_date\":\"2025-12-08T14:48:51Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "6be1578a393f56a67b1baad360c034d5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07599v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07596v1",
    "name": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery",
    "author": "Wenzhen Dong",
    "description": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assiste...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.RO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07596v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.",
    "meta_json": "{\"arxiv_id\":\"2512.07596v1\",\"authors\":[\"Wenzhen Dong\",\"Jieming Yu\",\"Yiming Huang\",\"Hongqiu Wang\",\"Lei Zhu\",\"Albert C. S. Chung\",\"Hongliang Ren\",\"Long Bai\"],\"categories\":[\"cs.CV\",\"cs.RO\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:43:40Z\",\"updated_date\":\"2025-12-08T14:43:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "88c513c85fc1e26cbd1192df8c414e5a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07596v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07590v1",
    "name": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation",
    "author": "Kaili Qi",
    "description": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial different...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07590v1",
    "image_url": null,
    "type": "paper",
    "body_content": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.",
    "meta_json": "{\"arxiv_id\":\"2512.07590v1\",\"authors\":[\"Kaili Qi\",\"Zhongyi Huang\",\"Wenli Yang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:33:52Z\",\"updated_date\":\"2025-12-08T14:33:52Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "935168315776f78e1d6866a7abed90a1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07590v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07584v1",
    "name": "LongCat-Image Technical Report",
    "author": "Meituan LongCat Team",
    "description": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This ...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07584v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
    "meta_json": "{\"arxiv_id\":\"2512.07584v1\",\"authors\":[\"Meituan LongCat Team\",\"Hanghang Ma\",\"Haoxian Tan\",\"Jiale Huang\",\"Junqiang Wu\",\"Jun-Yan He\",\"Lishuai Gao\",\"Songlin Xiao\",\"Xiaoming Wei\",\"Xiaoqi Ma\",\"Xunliang Cai\",\"Yayong Guan\",\"Jie Hu\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:26:40Z\",\"updated_date\":\"2025-12-08T14:26:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "8e7c20cc83912aac755e15c0acbb28ab",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07584v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07583v1",
    "name": "Complementary Learning Approach for Text Classification using Large Language Models",
    "author": "Navid Asgari",
    "description": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abdu...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07583v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).",
    "meta_json": "{\"arxiv_id\":\"2512.07583v1\",\"authors\":[\"Navid Asgari\",\"Benjamin M. Cole\"],\"categories\":[\"cs.CL\",\"cs.AI\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:26:31Z\",\"updated_date\":\"2025-12-08T14:26:31Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "3d7d16f2558d3ce3e700fdc85c0a0fd2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07583v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07580v1",
    "name": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs",
    "author": "Yahong Wang",
    "description": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressi...",
    "tags": [
      "arxiv:cs.CV",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07580v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.",
    "meta_json": "{\"arxiv_id\":\"2512.07580v1\",\"authors\":[\"Yahong Wang\",\"Juncheng Wu\",\"Zhangkai Ni\",\"Longzhen Yang\",\"Yihang Liu\",\"Chengmei Yang\",\"Ying Wen\",\"Xianfeng Tang\",\"Hui Liu\",\"Yuyin Zhou\",\"Lianghua He\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:16:01Z\",\"updated_date\":\"2025-12-08T14:16:01Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:YahongWang1:Information-Horizon.\",\"source_url\":\"https://github.com/YahongWang1/Information-Horizon.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "0ca643d7d0b265f614eb5e762cc10efc",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07580v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07578v1",
    "name": "$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations",
    "author": "Dongseok Kim",
    "description": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and con...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.LG",
      "arxiv:stat.ME"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07578v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.",
    "meta_json": "{\"arxiv_id\":\"2512.07578v1\",\"authors\":[\"Dongseok Kim\",\"Hyoungsun Choi\",\"Mohamed Jismy Aashik Rasool\",\"Gisung Oh\"],\"categories\":[\"stat.ML\",\"cs.LG\",\"stat.ME\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:14:01Z\",\"updated_date\":\"2025-12-08T14:14:01Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ME\",\"source_url\":\"https://arxiv.org/abs/stat.ME\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c714cee161337ff126a2b4194cee10e3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07578v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07576v1",
    "name": "R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation",
    "author": "Xuecheng Li",
    "description": "Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images a...",
    "tags": [
      "arxiv:eess.IV",
      "arxiv:cs.AI",
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07576v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.",
    "meta_json": "{\"arxiv_id\":\"2512.07576v1\",\"authors\":[\"Xuecheng Li\",\"Weikuan Jia\",\"Komildzhon Sharipov\",\"Sharipov Hotam Beknazarovich\",\"Farzona S. Ataeva\",\"Qurbonaliev Alisher\",\"Yuanjie Zheng\"],\"categories\":[\"eess.IV\",\"cs.AI\",\"cs.CV\"],\"primary_category\":\"eess.IV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:12:52Z\",\"updated_date\":\"2025-12-08T14:12:52Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.IV\",\"source_url\":\"https://arxiv.org/abs/eess.IV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "faf24ac97afa3aeb84c4bc90d1ab81ae",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07576v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07574v1",
    "name": "Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework",
    "author": "Xuecheng Li",
    "description": "Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that cou...",
    "tags": [
      "arxiv:eess.IV",
      "arxiv:cs.CR",
      "arxiv:cs.CV",
      "deep learning"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07574v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.",
    "meta_json": "{\"arxiv_id\":\"2512.07574v1\",\"authors\":[\"Xuecheng Li\",\"Weikuan Jia\",\"Komildzhon Sharipov\",\"Alimov Ruslan\",\"Lutfuloev Mazbutdzhon\",\"Ismoilov Shuhratjon\",\"Yuanjie Zheng\"],\"categories\":[\"eess.IV\",\"cs.CR\",\"cs.CV\"],\"primary_category\":\"eess.IV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:09:21Z\",\"updated_date\":\"2025-12-08T14:09:21Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.IV\",\"source_url\":\"https://arxiv.org/abs/eess.IV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CR\",\"source_url\":\"https://arxiv.org/abs/cs.CR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "8d107420ce60295066c8183d46439eab",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07574v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07571v1",
    "name": "A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification",
    "author": "Nicolas Calbucura",
    "description": "This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficu...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.MM",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07571v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).",
    "meta_json": "{\"arxiv_id\":\"2512.07571v1\",\"authors\":[\"Nicolas Calbucura\",\"Valentin Barriere\"],\"categories\":[\"cs.CL\",\"cs.MM\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:05:40Z\",\"updated_date\":\"2025-12-08T14:05:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:salocinc:EACL26SpeechTokFallacy\",\"source_url\":\"https://github.com/salocinc/EACL26SpeechTokFallacy\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.MM\",\"source_url\":\"https://arxiv.org/abs/cs.MM\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "dedf07a866708356af8d945c96e9f507",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07571v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07569v1",
    "name": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting",
    "author": "Joel Ekstrand",
    "description": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consist...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07569v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.",
    "meta_json": "{\"arxiv_id\":\"2512.07569v1\",\"authors\":[\"Joel Ekstrand\",\"Tor Mattsson\",\"Zahra Taghiyarrenani\",\"Slawomir Nowaczyk\",\"Jens Lundström\",\"Mikael Lindén\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:02:31Z\",\"updated_date\":\"2025-12-08T14:02:31Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "85e52fbd229fc9c0ea0f2547f948ac6f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07569v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07568v1",
    "name": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation",
    "author": "Xuecheng Li",
    "description": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "arxiv:eess.IV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07568v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.",
    "meta_json": "{\"arxiv_id\":\"2512.07568v1\",\"authors\":[\"Xuecheng Li\",\"Weikuan Jia\",\"Alisher Kurbonaliev\",\"Qurbonaliev Alisher\",\"Khudzhamkulov Rustam\",\"Ismoilov Shuhratjon\",\"Eshmatov Javhariddin\",\"Yuanjie Zheng\"],\"categories\":[\"cs.CV\",\"cs.AI\",\"eess.IV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T14:01:16Z\",\"updated_date\":\"2025-12-08T14:01:16Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.IV\",\"source_url\":\"https://arxiv.org/abs/eess.IV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a3c3e35dc88cb4cb97b9c2bb468f4b04",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07568v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]