[
  {
    "id": "huggingface:hidream-ai:hidream-i1-full",
    "name": "HiDream-I1-Full",
    "author": "HiDream-ai",
    "description": "--- license: mit tags: - image-generation - HiDream.ai language: - en pipeline_tag: text-to-image library_name: diffusers --- !HiDream-I1 Demo is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. <span style=\"color: #FF5733; font-weight: bold\">For more features and to experience the full capabilities of our product, please visit https://vivago.ai/.</span> - ğŸŒŸ **July 16, 2025**: We've open-sourced th...",
    "tags": [
      "diffusers",
      "safetensors",
      "image-generation",
      "hidream.ai",
      "text-to-image",
      "en",
      "arxiv:2505.22705",
      "license:mit",
      "diffusers:hidreamimagepipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 980,
    "downloads": 21240,
    "source": "huggingface",
    "source_url": "https://huggingface.co/HiDream-ai/HiDream-I1-Full",
    "image_url": "https://huggingface.co/HiDream-ai/HiDream-I1-Full/resolve/main/demo.jpg",
    "type": "model",
    "body_content": "---\nlicense: mit\ntags:\n- image-generation\n- HiDream.ai\nlanguage:\n- en\npipeline_tag: text-to-image\nlibrary_name: diffusers\n---\n\n![HiDream-I1 Demo](demo.jpg)\n\n`HiDream-I1` is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.\n\n<span style=\"color: #FF5733; font-weight: bold\">For more features and to experience the full capabilities of our product, please visit [https://vivago.ai/](https://vivago.ai/).</span>\n\n## Project Updates\n- ğŸŒŸ **July 16, 2025**: We've open-sourced the updated image editing model [**HiDream-E1.1**](https://huggingface.co/HiDream-ai/HiDream-E1-1). \n- ğŸ“ **May 28, 2025**: We've released our technical report [HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer](https://arxiv.org/abs/2505.22705).\n- ğŸš€ **April 28, 2025**: We've open-sourced the instruction-based-image-editing model [**HiDream-E1-Full**](https://github.com/HiDream-ai/HiDream-E1). Experience at [https://huggingface.co/spaces/HiDream-ai/HiDream-E1-Full](https://huggingface.co/spaces/HiDream-ai/HiDream-E1-Full)!.\n\n## Key Features\n- âœ¨ **Superior Image Quality** - Produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. Achieves state-of-the-art HPS v2.1 score, which aligns with human preferences.\n- ğŸ¯ **Best-in-Class Prompt Following** - Achieves industry-leading scores on GenEval and DPG benchmarks, outperforming all other open-source models.\n- ğŸ”“ **Open Source** - Released under the MIT license to foster scientific advancement and enable creative innovation.\n- ğŸ’¼ **Commercial-Friendly** - Generated images can be freely used for personal projects, scientific research, and commercial applications.\n\n## Quick Start\nPlease make sure you have installed [Flash Attention](https://github.com/Dao-AILab/flash-attention). We recommend CUDA version 12.4 for the manual installation.\n```\npip install -r requirements.txt\n```\nClone the GitHub repo:\n```\ngit clone https://github.com/HiDream-ai/HiDream-I1\n```\n\nThen you can run the inference scripts to generate images:\n\n```python\n# For full model inference\npython ./inference.py --model_type full\n\n# For distilled dev model inference\npython ./inference.py --model_type dev\n\n# For distilled fast model inference\npython ./inference.py --model_type fast\n```\n> **Note:** The inference script will automatically download `meta-llama/Meta-Llama-3.1-8B-Instruct` model files. If you encounter network issues, you can download these files ahead of time and place them in the appropriate cache directory to avoid download failures during inference.\n\n## Gradio Demo\n\nWe also provide a Gradio demo for interactive image generation. You can run the demo with:\n\n```python\npython gradio_demo.py \n```\n\n## Evaluation Metrics\n\n### DPG-Bench\n| Model           | Overall   | Global    | Entity    | Attribute | Relation  | Other     |\n|-----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| PixArt-alpha    |    71.11  | 74.97     | 79.32     | 78.60     | 82.57     | 76.96     |\n| SDXL            |    74.65  | 83.27     | 82.43     | 80.91     | 86.76     | 80.41     |\n| DALL-E 3        |    83.50  | 90.97     | 89.61     | 88.39     | 90.58     | 89.83     |\n| Flux.1-dev      |    83.79  | 85.80     | 86.79     | 89.98     | 90.04     | 89.90     |\n| SD3-Medium      |    84.08  | 87.90     | 91.01     | 88.83     | 80.70     | 88.68     |\n| Janus-Pro-7B    |    84.19  | 86.90     | 88.90     | 89.40     | 89.32     | 89.48     |\n| CogView4-6B     |    85.13  | 83.85     | 90.35     | 91.17     | 91.14     | 87.29     |\n| **HiDream-I1**  |  **85.89**| 76.44 \t  | 90.22     | 89.48     | 93.74     | 91.83     | \n\n### GenEval\n\n| Model           | Overall  | Single Obj. | Two Obj. | Counting | Colors   | Position | Color attribution |\n|-----------------|----------|-------------|----------|----------|----------|----------|-------------------|\n| SDXL            |    0.55  | 0.98        | 0.74     | 0.39     | 0.85     | 0.15     | 0.23              |\n| PixArt-alpha    |    0.48  | 0.98        | 0.50     | 0.44     | 0.80     | 0.08     | 0.07              |\n| Flux.1-dev      |    0.66  | 0.98        | 0.79     | 0.73     | 0.77     | 0.22     | 0.45              |\n| DALL-E 3        |    0.67  | 0.96        | 0.87     | 0.47     | 0.83     | 0.43     | 0.45              |\n| CogView4-6B     |    0.73  | 0.99        | 0.86     | 0.66     | 0.79     | 0.48     | 0.58              |\n| SD3-Medium      |    0.74  | 0.99        | 0.94     | 0.72     | 0.89     | 0.33     | 0.60              |\n| Janus-Pro-7B    |    0.80  | 0.99        | 0.89     | 0.59     | 0.90     | 0.79     | 0.66              |\n| **HiDream-I1**  |  **0.83**| 1.00        | 0.98 \t  | 0.79 \t | 0.91 \t| 0.60 \t   | 0.72              |\n\n### HPSv2.1 benchmark\n\n|  Model                  |     Averaged   | Animation  |  Concept-art  |   Painting   |   Photo    |\n|-------------------------|----------------|------------|---------------|--------------|------------|\n|  Stable Diffusion v2.0  |       26.38    |\t27.09   |      26.02    |    25.68     |    26.73   |\n|  Midjourney V6          |       30.29    |    32.02   |      30.29    |    29.74     |    29.10   |\n|  SDXL\t                  |       30.64    |    32.84   |      31.36    |    30.86     |    27.48   |\n|  Dall-E3\t              |       31.44    |    32.39   |      31.09    |    31.18     |    31.09   |\n|  SD3                    |       31.53    |    32.60   |      31.82    |    32.06     |    29.62   |\n|  Midjourney V5          |       32.33    |    34.05   |      32.47    |    32.24     |    30.56   |\n|  CogView4-6B            |       32.31    |    33.23   |      32.60    |    32.89     |    30.52   |\n|  Flux.1-dev             |       32.47    |    33.87   |      32.27    |    32.62     |    31.11   |\n|  stable cascade         |       32.95    |    34.58   |      33.13    |    33.29     |    30.78   |\n|  **HiDream-I1**         |     **33.82**  |    35.05   |      33.74    |    33.88     |    32.61   |\n\n\n## License Agreement\nThe Transformer models in this repository are licensed under the MIT License. The VAE is from `FLUX.1 [schnell]`, and the text encoders from `google/t5-v1_1-xxl` and `meta-llama/Meta-Llama-3.1-8B-Instruct`. Please follow the license terms specified for these components. You own all content you create with this model. You can use your generated content freely, but you must comply with this license agreement. You are responsible for how you use the models. Do not create illegal content, harmful material, personal information that could harm others, false information, or content targeting vulnerable groups.\n\n\n## Acknowledgements\n- The VAE component is from `FLUX.1 [schnell]`, licensed under Apache 2.0. \n- The text encoders are from `google/t5-v1_1-xxl` (licensed under Apache 2.0) and `meta-llama/Meta-Llama-3.1-8B-Instruct` (licensed under the Llama 3.1 Community License Agreement).\n\n\n## Citation\n\n```bibtex\n@article{hidreami1technicalreport,\n  title={HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer},\n  author={Cai, Qi and Chen, Jingwen and Chen, Yang and Li, Yehao and Long, Fuchen and Pan, Yingwei and Qiu, Zhaofan and Zhang, Yiheng and Gao, Fengbin and Xu, Peihan and others},\n  journal={arXiv preprint arXiv:2505.22705},\n  year={2025}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":47186202978,\"files_count\":36,\"spaces_count\":98,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"HiDreamImagePipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:HiDream-ai:HiDream-E1\",\"source_url\":\"https://github.com/HiDream-ai/HiDream-E1\"},{\"type\":\"has_code\",\"target_id\":\"github:Dao-AILab:flash-attention\",\"source_url\":\"https://github.com/Dao-AILab/flash-attention\"},{\"type\":\"has_code\",\"target_id\":\"github:HiDream-ai:HiDream-I1\",\"source_url\":\"https://github.com/HiDream-ai/HiDream-I1\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.22705\",\"source_url\":\"https://arxiv.org/abs/2505.22705\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 84.9,
    "content_hash": "1732e90072de6d5076b1a7d1d433efd2",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/HiDream-ai/HiDream-I1-Full/resolve/main/demo.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/HiDream-ai/HiDream-I1-Full\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:moka-ai:m3e-base",
    "name": "m3e-base",
    "author": "moka-ai",
    "description": "--- language: - zh - en tags: - embedding - text-embedding library_name: sentence-transformers --- m3e-small | m3e-base M3E æ˜¯ Moka Massive Mixed Embedding çš„ç¼©å†™ - Mokaï¼Œæ­¤æ¨¡å‹ç”± MokaAI è®­ç»ƒï¼Œå¼€æºå’Œè¯„æµ‹ï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniem ï¼Œè¯„æµ‹ BenchMark ä½¿ç”¨ MTEB-zh - Massiveï¼Œæ­¤æ¨¡å‹é€šè¿‡**åƒä¸‡çº§** (2200w+) çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒ - Mixedï¼Œæ­¤æ¨¡å‹æ”¯æŒä¸­è‹±åŒè¯­çš„åŒè´¨æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¼‚è´¨æ–‡æœ¬æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œæœªæ¥è¿˜ä¼šæ”¯æŒä»£ç æ£€ç´¢ - Embeddingï¼Œæ­¤æ¨¡å‹æ˜¯æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå¯ä»¥å°†è‡ªç„¶è¯­è¨€è½¬æ¢æˆç¨ å¯†çš„å‘é‡ - 2023.06.24ï¼Œæ·»åŠ å¾®è°ƒ M3E çš„æ•™ç¨‹ notebookï¼Œå‡ è¡Œä»£ç ï¼Œæ›´ä½³é€‚é…ï¼<a target=\"_blank\" href=\"https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "safetensors",
      "bert",
      "embedding",
      "text-embedding",
      "zh",
      "en",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 975,
    "downloads": 160193,
    "source": "huggingface",
    "source_url": "https://huggingface.co/moka-ai/m3e-base",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- zh\n- en\ntags:\n- embedding\n- text-embedding\nlibrary_name: sentence-transformers\n---\n\n# ğŸ…œ M3E Models\n\n[m3e-small](https://huggingface.co/moka-ai/m3e-small) | [m3e-base](https://huggingface.co/moka-ai/m3e-base)\n\nM3E æ˜¯ Moka Massive Mixed Embedding çš„ç¼©å†™\n\n- Mokaï¼Œæ­¤æ¨¡å‹ç”± MokaAI è®­ç»ƒï¼Œå¼€æºå’Œè¯„æµ‹ï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py) ï¼Œè¯„æµ‹ BenchMark ä½¿ç”¨ [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)\n- Massiveï¼Œæ­¤æ¨¡å‹é€šè¿‡**åƒä¸‡çº§** (2200w+) çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒ\n- Mixedï¼Œæ­¤æ¨¡å‹æ”¯æŒä¸­è‹±åŒè¯­çš„åŒè´¨æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¼‚è´¨æ–‡æœ¬æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œæœªæ¥è¿˜ä¼šæ”¯æŒä»£ç æ£€ç´¢\n- Embeddingï¼Œæ­¤æ¨¡å‹æ˜¯æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå¯ä»¥å°†è‡ªç„¶è¯­è¨€è½¬æ¢æˆç¨ å¯†çš„å‘é‡\n\n## ğŸ†• æ›´æ–°è¯´æ˜\n\n- 2023.06.24ï¼Œæ·»åŠ å¾®è°ƒ M3E çš„æ•™ç¨‹ [notebook](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)ï¼Œå‡ è¡Œä»£ç ï¼Œæ›´ä½³é€‚é…ï¼<a target=\"_blank\" href=\"https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n- 2023.06.14ï¼Œæ·»åŠ äº†ä¸‰ä¸ªä¸­æ–‡å¼€æºæ–‡æœ¬åµŒå…¥æ¨¡å‹åˆ°è¯„æµ‹ä¸­ï¼ŒåŒ…æ‹¬ UER, ErLangShen, DMetaSoul\n- 2023.06.08ï¼Œæ·»åŠ æ£€ç´¢ä»»åŠ¡çš„è¯„æµ‹ç»“æœï¼Œåœ¨ T2Ranking 1W ä¸­æ–‡æ•°æ®é›†ä¸Šï¼Œm3e-base åœ¨ ndcg@10 ä¸Šè¾¾åˆ°äº† 0.8004ï¼Œè¶…è¿‡äº† openai-ada-002 çš„ 0.7786\n- 2023.06.07ï¼Œæ·»åŠ æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¯„æµ‹ç»“æœï¼Œåœ¨ 6 ç§æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸Šï¼Œm3e-base åœ¨ accuracy ä¸Šè¾¾åˆ°äº† 0.6157ï¼Œè¶…è¿‡äº† openai-ada-002 çš„ 0.5956\n\n## âš–ï¸ æ¨¡å‹å¯¹æ¯”\n\n|           | å‚æ•°æ•°é‡ | ç»´åº¦ | ä¸­æ–‡ | è‹±æ–‡ | s2s | s2p | s2c | å¼€æº | å…¼å®¹æ€§ | s2s Acc | s2p ndcg@10 |\n| --------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | ---- | ---------- | ------------ | -------- |\n| m3e-small | 24M      | 512      | æ˜¯       | å¦       | æ˜¯       | å¦       | å¦       | æ˜¯   | ä¼˜         | 0.5834       | 0.7262   |\n| m3e-base  | 110M     | 768      | æ˜¯       | æ˜¯       | æ˜¯       | æ˜¯       | å¦       | æ˜¯   | ä¼˜         | **0.6157**       | **0.8004**   |\n| text2vec  | 110M     | 768      | æ˜¯       | å¦       | æ˜¯       | å¦       | å¦       | æ˜¯   | ä¼˜         | 0.5755       | 0.6346   |\n| openai-ada-002    | æœªçŸ¥     | 1536     | æ˜¯       | æ˜¯       | æ˜¯       | æ˜¯       | æ˜¯       | å¦   | ä¼˜         | 0.5956       | 0.7786   |\n\nè¯´æ˜ï¼š\n- s2s, å³ sentence to sentence ï¼Œä»£è¡¨äº†åŒè´¨æ–‡æœ¬ä¹‹é—´çš„åµŒå…¥èƒ½åŠ›ï¼Œé€‚ç”¨ä»»åŠ¡ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œé‡å¤é—®é¢˜æ£€æµ‹ï¼Œæ–‡æœ¬åˆ†ç±»ç­‰\n- s2p, å³ sentence to passage ï¼Œä»£è¡¨äº†å¼‚è´¨æ–‡æœ¬ä¹‹é—´çš„åµŒå…¥èƒ½åŠ›ï¼Œé€‚ç”¨ä»»åŠ¡ï¼šæ–‡æœ¬æ£€ç´¢ï¼ŒGPT è®°å¿†æ¨¡å—ç­‰\n- s2c, å³ sentence to code ï¼Œä»£è¡¨äº†è‡ªç„¶è¯­è¨€å’Œç¨‹åºè¯­è¨€ä¹‹é—´çš„åµŒå…¥èƒ½åŠ›ï¼Œé€‚ç”¨ä»»åŠ¡ï¼šä»£ç æ£€ç´¢\n- å…¼å®¹æ€§ï¼Œä»£è¡¨äº†æ¨¡å‹åœ¨å¼€æºç¤¾åŒºä¸­å„ç§é¡¹ç›®è¢«æ”¯æŒçš„ç¨‹åº¦ï¼Œç”±äº m3e å’Œ text2vec éƒ½å¯ä»¥ç›´æ¥é€šè¿‡ sentence-transformers ç›´æ¥ä½¿ç”¨ï¼Œæ‰€ä»¥å’Œ openai åœ¨ç¤¾åŒºçš„æ”¯æŒåº¦ä¸Šç›¸å½“\n- ACC & ndcg@10ï¼Œè¯¦æƒ…è§ä¸‹æ–¹çš„è¯„æµ‹\n\nTips:\n- ä½¿ç”¨åœºæ™¯ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œå°‘é‡è‹±æ–‡çš„æƒ…å†µï¼Œå»ºè®®ä½¿ç”¨ m3e ç³»åˆ—çš„æ¨¡å‹\n- å¤šè¯­è¨€ä½¿ç”¨åœºæ™¯ï¼Œå¹¶ä¸”ä¸ä»‹æ„æ•°æ®éšç§çš„è¯ï¼Œæˆ‘å»ºè®®ä½¿ç”¨ openai text-embedding-ada-002\n- ä»£ç æ£€ç´¢åœºæ™¯ï¼Œæ¨èä½¿ç”¨ openai text-embedding-ada-002\n- æ–‡æœ¬æ£€ç´¢åœºæ™¯ï¼Œè¯·ä½¿ç”¨å…·å¤‡æ–‡æœ¬æ£€ç´¢èƒ½åŠ›çš„æ¨¡å‹ï¼Œåªåœ¨ S2S ä¸Šè®­ç»ƒçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œæ²¡æœ‰åŠæ³•å®Œæˆæ–‡æœ¬æ£€ç´¢ä»»åŠ¡\n\n## ğŸ”§ ä½¿ç”¨ M3E\n\næ‚¨éœ€è¦å…ˆå®‰è£… sentence-transformers\n\n```bash\npip install -U sentence-transformers\n```\n\nå®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ¥ä½¿ç”¨ M3E Models\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('moka-ai/m3e-base')\n\n#Our sentences we like to encode\nsentences = [\n    '* Moka æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹ç”± MokaAI è®­ç»ƒå¹¶å¼€æºï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniem',\n    '* Massive æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹é€šè¿‡**åƒä¸‡çº§**çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒ',\n    '* Mixed æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹æ”¯æŒä¸­è‹±åŒè¯­çš„åŒè´¨æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¼‚è´¨æ–‡æœ¬æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œæœªæ¥è¿˜ä¼šæ”¯æŒä»£ç æ£€ç´¢ï¼ŒALL in one'\n]\n\n#Sentences are encoded by calling model.encode()\nembeddings = model.encode(sentences)\n\n#Print the embeddings\nfor sentence, embedding in zip(sentences, embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")\n```\n\n\nM3E ç³»åˆ—çš„æ‰€æœ‰æ¨¡å‹åœ¨è®¾è®¡çš„æ—¶å€™å°±è€ƒè™‘åˆ°å®Œå…¨å…¼å®¹ [sentence-transformers](https://www.sbert.net/) ï¼Œæ‰€ä»¥ä½ å¯ä»¥é€šè¿‡**æ›¿æ¢åç§°å­—ç¬¦ä¸²**çš„æ–¹å¼åœ¨æ‰€æœ‰æ”¯æŒ sentence-transformers çš„é¡¹ç›®ä¸­**æ— ç¼**ä½¿ç”¨ M3E Modelsï¼Œæ¯”å¦‚ [chroma](https://docs.trychroma.com/getting-started), [guidance](https://github.com/microsoft/guidance), [semantic-kernel](https://github.com/microsoft/semantic-kernel) ã€‚\n\n## ğŸ¨ å¾®è°ƒæ¨¡å‹\n\n`uniem` æä¾›äº†éå¸¸æ˜“ç”¨çš„ finetune æ¥å£ï¼Œå‡ è¡Œä»£ç ï¼Œå³åˆ»é€‚é…ï¼\n\n```python\nfrom datasets import load_dataset\n\nfrom uniem.finetuner import FineTuner\n\ndataset = load_dataset('shibing624/nli_zh', 'STS-B')\n# æŒ‡å®šè®­ç»ƒçš„æ¨¡å‹ä¸º m3e-small\nfinetuner = FineTuner.from_pretrained('moka-ai/m3e-small', dataset=dataset)\nfinetuner.run(epochs=1)\n```\n\nè¯¦è§ [uniem å¾®è°ƒæ•™ç¨‹](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n## â¿ è®­ç»ƒæ–¹æ¡ˆ\n\nM3E ä½¿ç”¨ in-batch è´Ÿé‡‡æ ·çš„å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼åœ¨å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä¸ºäº†ä¿è¯ in-batch è´Ÿé‡‡æ ·çš„æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨ A100 80G æ¥æœ€å¤§åŒ– batch-sizeï¼Œå¹¶åœ¨å…±è®¡ 2200W+ çš„å¥å¯¹æ•°æ®é›†ä¸Šè®­ç»ƒäº† 1 epochã€‚è®­ç»ƒè„šæœ¬ä½¿ç”¨ [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py)ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹å…·ä½“ç»†èŠ‚ã€‚\n\n## ğŸŒŸ ç‰¹æ€§\n\n- ä¸­æ–‡è®­ç»ƒé›†ï¼ŒM3E åœ¨å¤§è§„æ¨¡å¥å¯¹æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼ŒåŒ…å«ä¸­æ–‡ç™¾ç§‘ï¼Œé‡‘èï¼ŒåŒ»ç–—ï¼Œæ³•å¾‹ï¼Œæ–°é—»ï¼Œå­¦æœ¯ç­‰å¤šä¸ªé¢†åŸŸå…±è®¡ 2200W å¥å¯¹æ ·æœ¬ï¼Œæ•°æ®é›†è¯¦è§ [M3E æ•°æ®é›†](#M3Eæ•°æ®é›†)\n- è‹±æ–‡è®­ç»ƒé›†ï¼ŒM3E ä½¿ç”¨ MEDI 145W è‹±æ–‡ä¸‰å…ƒç»„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ•°æ®é›†è¯¦è§ [MEDI æ•°æ®é›†](https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view)ï¼Œæ­¤æ•°æ®é›†ç”± [instructor team](https://github.com/HKUNLP/instructor-embedding) æä¾›\n- æŒ‡ä»¤æ•°æ®é›†ï¼ŒM3E ä½¿ç”¨äº† 300W + çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œè¿™ä½¿å¾— M3E å¯¹æ–‡æœ¬ç¼–ç çš„æ—¶å€™å¯ä»¥éµä»æŒ‡ä»¤ï¼Œè¿™éƒ¨åˆ†çš„å·¥ä½œä¸»è¦è¢«å¯å‘äº [instructor-embedding](https://github.com/HKUNLP/instructor-embedding)\n- åŸºç¡€æ¨¡å‹ï¼ŒM3E ä½¿ç”¨ hfl å®éªŒå®¤çš„ [Roberta](https://huggingface.co/hfl/chinese-roberta-wwm-ext) ç³»åˆ—æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œç›®å‰æä¾›  small å’Œ  base ä¸¤ä¸ªç‰ˆæœ¬ï¼Œå¤§å®¶åˆ™éœ€é€‰ç”¨\n- ALL IN ONEï¼ŒM3E æ—¨åœ¨æä¾›ä¸€ä¸ª ALL IN ONE çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œä¸ä»…æ”¯æŒåŒè´¨å¥å­ç›¸ä¼¼åº¦åˆ¤æ–­ï¼Œè¿˜æ”¯æŒå¼‚è´¨æ–‡æœ¬æ£€ç´¢ï¼Œä½ åªéœ€è¦ä¸€ä¸ªæ¨¡å‹å°±å¯ä»¥è¦†ç›–å…¨éƒ¨çš„åº”ç”¨åœºæ™¯ï¼Œæœªæ¥è¿˜ä¼šæ”¯æŒä»£ç æ£€ç´¢\n\n## ğŸ’¯ MTEB-zh è¯„æµ‹\n\n- è¯„æµ‹æ¨¡å‹ï¼Œ[text2vec](https://github.com/shibing624/text2vec), m3e-base, m3e-small, openai text-embedding-ada-002, [DMetaSoul](https://huggingface.co/DMetaSoul/sbert-chinese-general-v2), [UER](https://huggingface.co/uer/sbert-base-chinese-nli), [ErLangShen](https://huggingface.co/IDEA-CCNL/Erlangshen-SimCSE-110M-Chinese)\n- è¯„æµ‹è„šæœ¬ï¼Œå…·ä½“å‚è€ƒ [MTEB-zh] (https://github.com/wangyuxinwhy/uniem/blob/main/mteb-zh)\n\n### æ–‡æœ¬åˆ†ç±»\n\n- æ•°æ®é›†é€‰æ‹©ï¼Œé€‰æ‹©å¼€æºåœ¨ HuggingFace ä¸Šçš„ 6 ç§æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ–°é—»ã€ç”µå•†è¯„è®ºã€è‚¡ç¥¨è¯„è®ºã€é•¿æ–‡æœ¬ç­‰\n- è¯„æµ‹æ–¹å¼ï¼Œä½¿ç”¨ MTEB çš„æ–¹å¼è¿›è¡Œè¯„æµ‹ï¼ŒæŠ¥å‘Š Accuracyã€‚\n\n|                   | text2vec | m3e-small | m3e-base | openai | DMetaSoul   | uer     | erlangshen  |\n| ----------------- | -------- | --------- | -------- | ------ | ----------- | ------- | ----------- |\n| TNews             | 0.43     | 0.4443    | **0.4827**   | 0.4594 | 0.3084      | 0.3539  | 0.4361      |\n| JDIphone          | 0.8214   | 0.8293    | **0.8533**   | 0.746  | 0.7972      | 0.8283  | 0.8356      |\n| GubaEastmony      | 0.7472   | 0.712     | 0.7621   | 0.7574 | 0.735       | 0.7534  | **0.7787**      |\n| TYQSentiment      | 0.6099   | 0.6596    | **0.7188**   | 0.68   | 0.6437      | 0.6662  | 0.6444      |\n| StockComSentiment | 0.4307   | 0.4291    | 0.4363   | **0.4819** | 0.4309      | 0.4555  | 0.4482      |\n| IFlyTek           | 0.414    | 0.4263    | 0.4409   | **0.4486** | 0.3969      | 0.3762  | 0.4241      |\n| Average           | 0.5755   | 0.5834    | **0.6157**   | 0.5956 | 0.552016667 | 0.57225 | 0.594516667 |\n\n### æ£€ç´¢æ’åº\n\n#### T2Ranking 1W\n\n- æ•°æ®é›†é€‰æ‹©ï¼Œä½¿ç”¨ [T2Ranking](https://github.com/THUIR/T2Ranking/tree/main) æ•°æ®é›†ï¼Œç”±äº T2Ranking çš„æ•°æ®é›†å¤ªå¤§ï¼Œopenai è¯„æµ‹èµ·æ¥çš„æ—¶é—´æˆæœ¬å’Œ api è´¹ç”¨æœ‰äº›é«˜ï¼Œæ‰€ä»¥æˆ‘ä»¬åªé€‰æ‹©äº† T2Ranking ä¸­çš„å‰ 10000 ç¯‡æ–‡ç« \n- è¯„æµ‹æ–¹å¼ï¼Œä½¿ç”¨ MTEB çš„æ–¹å¼è¿›è¡Œè¯„æµ‹ï¼ŒæŠ¥å‘Š map@1, map@10, mrr@1, mrr@10, ndcg@1, ndcg@10\n- æ³¨æ„ï¼ä»å®éªŒç»“æœå’Œè®­ç»ƒæ–¹å¼æ¥çœ‹ï¼Œé™¤äº† M3E æ¨¡å‹å’Œ openai æ¨¡å‹å¤–ï¼Œå…¶ä½™æ¨¡å‹éƒ½æ²¡æœ‰åšæ£€ç´¢ä»»åŠ¡çš„è®­ç»ƒï¼Œæ‰€ä»¥ç»“æœä»…ä¾›å‚è€ƒã€‚\n\n|         | text2vec | openai-ada-002 | m3e-small | m3e-base | DMetaSoul | uer     | erlangshen |\n| ------- | -------- | -------------- | --------- | -------- | --------- | ------- | ---------- |\n| map@1   | 0.4684   | 0.6133         | 0.5574    | **0.626**    | 0.25203   | 0.08647 | 0.25394    |\n| map@10  | 0.5877   | 0.7423         | 0.6878    | **0.7656**   | 0.33312   | 0.13008 | 0.34714    |\n| mrr@1   | 0.5345   | 0.6931         | 0.6324    | **0.7047**   | 0.29258   | 0.10067 | 0.29447    |\n| mrr@10  | 0.6217   | 0.7668         | 0.712     | **0.7841**   | 0.36287   | 0.14516 | 0.3751     |\n| ndcg@1  | 0.5207   | 0.6764         | 0.6159    | **0.6881**   | 0.28358   | 0.09748 | 0.28578    |\n| ndcg@10 | 0.6346   | 0.7786         | 0.7262    | **0.8004**   | 0.37468   | 0.15783 | 0.39329    |\n\n#### T2Ranking\n\n- æ•°æ®é›†é€‰æ‹©ï¼Œä½¿ç”¨ T2Rankingï¼Œåˆ¨é™¤ openai-ada-002 æ¨¡å‹åï¼Œæˆ‘ä»¬å¯¹å‰©ä½™çš„ä¸‰ä¸ªæ¨¡å‹ï¼Œè¿›è¡Œ T2Ranking 10W å’Œ T2Ranking 50W çš„è¯„æµ‹ã€‚ï¼ˆT2Ranking è¯„æµ‹å¤ªè€—å†…å­˜äº†... 128G éƒ½ä¸è¡Œï¼‰\n- è¯„æµ‹æ–¹å¼ï¼Œä½¿ç”¨ MTEB çš„æ–¹å¼è¿›è¡Œè¯„æµ‹ï¼ŒæŠ¥å‘Š ndcg@10\n\n|         | text2vec | m3e-small | m3e-base |\n| ------- | -------- | --------- | -------- |\n| t2r-1w  | 0.6346   | 0.72621   | **0.8004**   |\n| t2r-10w | 0.44644  | 0.5251    | **0.6263**   |\n| t2r-50w | 0.33482  | 0.38626   | **0.47364**  |\n\nè¯´æ˜ï¼š\n- æ£€ç´¢æ’åºå¯¹äº text2vec å¹¶ä¸å…¬å¹³ï¼Œå› ä¸º text2vec åœ¨è®­ç»ƒçš„æ—¶å€™æ²¡æœ‰ä½¿ç”¨è¿‡æ£€ç´¢ç›¸å…³çš„æ•°æ®é›†ï¼Œæ‰€ä»¥æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„å®Œæˆæ£€ç´¢ä»»åŠ¡ä¹Ÿæ˜¯æ­£å¸¸çš„ã€‚\n\n## ğŸ“‚ M3Eæ•°æ®é›†\n\nå¦‚æœæ‚¨æƒ³è¦ä½¿ç”¨è¿™äº›æ•°æ®é›†ï¼Œä½ å¯ä»¥åœ¨ [uniem process_zh_datasets](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/process_zh_datasets.py) ä¸­æ‰¾åˆ°åŠ è½½ huggingface æ•°æ®é›†çš„è„šæœ¬ï¼Œé huggingface æ•°æ®é›†éœ€è¦æ‚¨æ ¹æ®ä¸‹æ–¹æä¾›çš„é“¾æ¥è‡ªè¡Œä¸‹è½½å’Œå¤„ç†ã€‚\n\n| æ•°æ®é›†åç§°           | é¢†åŸŸ | æ•°é‡      | ä»»åŠ¡ç±»å‹          | Prompt | è´¨é‡ | æ•°æ®æä¾›è€…                                                   | è¯´æ˜                                                         | æ˜¯å¦å¼€æº/ç ”ç©¶ä½¿ç”¨ | æ˜¯å¦å•†ç”¨ | è„šæœ¬ | Done | URL                                                          | æ˜¯å¦åŒè´¨ |\n| -------------------- | ---- | --------- | ----------------- | ------ | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------- | -------- | ---- | ---- | ------------------------------------------------------------ | -------- |\n| cmrc2018             | ç™¾ç§‘ | 14,363    | é—®ç­”              | é—®ç­”   | ä¼˜   | Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu | https://github.com/ymcui/cmrc2018/blob/master/README_CN.md ä¸“å®¶æ ‡æ³¨çš„åŸºäºç»´åŸºç™¾ç§‘çš„ä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œå°†é—®é¢˜å’Œä¸Šä¸‹æ–‡è§†ä¸ºæ­£ä¾‹ | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/cmrc2018                     | å¦       |\n| belle_2m             | ç™¾ç§‘ | 2,000,000 | æŒ‡ä»¤å¾®è°ƒ          | æ—      | ä¼˜   | LianjiaTech/BELLE                                            | belle çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œä½¿ç”¨ self instruct æ–¹æ³•åŸºäº gpt3.5 ç”Ÿæˆ | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/BelleGroup/train_2M_CN       | å¦       |\n| firefily             | ç™¾ç§‘ | 1,649,399 | æŒ‡ä»¤å¾®è°ƒ          | æ—      | ä¼˜   | YeungNLP                                                     | Fireflyï¼ˆæµè¤ï¼‰ æ˜¯ä¸€ä¸ªå¼€æºçš„ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œè°ƒä¼˜ã€‚ä½¿ç”¨äº†è¯è¡¨è£å‰ªã€ZeROç­‰æŠ€æœ¯ï¼Œæœ‰æ•ˆé™ä½æ˜¾å­˜æ¶ˆè€—å’Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚ åœ¨è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ›´å°çš„æ¨¡å‹å‚æ•°é‡ï¼Œä»¥åŠæ›´å°‘çš„è®¡ç®—èµ„æºã€‚ | æœªè¯´æ˜            | æœªè¯´æ˜   | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M  | å¦       |\n| alpaca_gpt4          | ç™¾ç§‘ | 48,818    | æŒ‡ä»¤å¾®è°ƒ          | æ—      | ä¼˜   | Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao | æœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚ | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/shibing624/alpaca-zh         | å¦       |\n| zhihu_kol            | ç™¾ç§‘ | 1,006,218 | é—®ç­”              | é—®ç­”   | ä¼˜   | wangrui6                                                     | çŸ¥ä¹é—®ç­”                                                     | æœªè¯´æ˜            | æœªè¯´æ˜   | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/wangrui6/Zhihu-KOL           | å¦       |\n| hc3_chinese          | ç™¾ç§‘ | 39,781    | é—®ç­”              | é—®ç­”   | è‰¯   | Hello-SimpleAI                                               | é—®ç­”æ•°æ®ï¼ŒåŒ…æ‹¬äººå·¥å›ç­”å’Œ GPT å›ç­”                            | æ˜¯                | æœªè¯´æ˜   | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese   | å¦       |\n| amazon_reviews_multi | ç”µå•† | 210,000   | é—®ç­” æ–‡æœ¬åˆ†ç±»     | æ‘˜è¦   | ä¼˜   | äºšé©¬é€Š                                                       | äºšé©¬é€Šäº§å“è¯„è®ºæ•°æ®é›†                                         | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/amazon_reviews_multi/viewer/zh/train?row=8 | å¦       |\n| mlqa                 | ç™¾ç§‘ | 85,853    | é—®ç­”              | é—®ç­”   | è‰¯   | patrickvonplaten                                             | ä¸€ä¸ªç”¨äºè¯„ä¼°è·¨è¯­è¨€é—®ç­”æ€§èƒ½çš„åŸºå‡†æ•°æ®é›†                       | æ˜¯                | æœªè¯´æ˜   | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/mlqa/viewer/mlqa-translate-train.zh/train?p=2 | å¦       |\n| xlsum                | æ–°é—» | 93,404    | æ‘˜è¦              | æ‘˜è¦   | è‰¯   | BUET CSE NLP Group                                           | BBCçš„ä¸“ä¸šæ³¨é‡Šæ–‡ç« æ‘˜è¦å¯¹                                      | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/csebuetnlp/xlsum/viewer/chinese_simplified/train?row=259 | å¦       |\n| ocnli                | å£è¯­ | 17,726    | è‡ªç„¶è¯­è¨€æ¨ç†      | æ¨ç†   | è‰¯   | Thomas Wolf                                                  | è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†                                           | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/clue/viewer/ocnli            | æ˜¯       |\n| BQ                   | é‡‘è | 60,000    | æ–‡æœ¬åˆ†ç±»          | ç›¸ä¼¼   | è‰¯   | Intelligent Computing Research Center, Harbin Institute of Technology(Shenzhen) | http://icrc.hitsz.edu.cn/info/1037/1162.htm BQ è¯­æ–™åº“åŒ…å«æ¥è‡ªç½‘ä¸Šé“¶è¡Œè‡ªå®šä¹‰æœåŠ¡æ—¥å¿—çš„ 120ï¼Œ000 ä¸ªé—®é¢˜å¯¹ã€‚å®ƒåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š100ï¼Œ000 å¯¹ç”¨äºè®­ç»ƒï¼Œ10ï¼Œ000 å¯¹ç”¨äºéªŒè¯ï¼Œ10ï¼Œ000 å¯¹ç”¨äºæµ‹è¯•ã€‚ æ•°æ®æä¾›è€…ï¼š å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ï¼ˆæ·±åœ³ï¼‰æ™ºèƒ½è®¡ç®—ç ”ç©¶ä¸­å¿ƒ | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/shibing624/nli_zh/viewer/BQ  | æ˜¯       |\n| lcqmc                | å£è¯­ | 149,226   | æ–‡æœ¬åˆ†ç±»          | ç›¸ä¼¼   | è‰¯   | Ming Xu                                                      | å“ˆå·¥å¤§æ–‡æœ¬åŒ¹é…æ•°æ®é›†ï¼ŒLCQMC æ˜¯å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å›½é™…é¡¶ä¼š COLING2018 æ„å»ºçš„é—®é¢˜è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œå…¶ç›®æ ‡æ˜¯åˆ¤æ–­ä¸¤ä¸ªé—®é¢˜çš„è¯­ä¹‰æ˜¯å¦ç›¸åŒ | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/shibing624/nli_zh/viewer/LCQMC/train | æ˜¯       |\n| paws-x               | ç™¾ç§‘ | 23,576    | æ–‡æœ¬åˆ†ç±»          | ç›¸ä¼¼   | ä¼˜   | Bhavitvya Malik                                              | PAWS Wikiä¸­çš„ç¤ºä¾‹                                            | æ˜¯                | æ˜¯       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/paws-x/viewer/zh/train       | æ˜¯       |\n| wiki_atomic_edit     | ç™¾ç§‘ | 1,213,780 | å¹³è¡Œè¯­ä¹‰          | ç›¸ä¼¼   | ä¼˜   | abhishek thakur                                              | åŸºäºä¸­æ–‡ç»´åŸºç™¾ç§‘çš„ç¼–è¾‘è®°å½•æ”¶é›†çš„æ•°æ®é›†                       | æœªè¯´æ˜            | æœªè¯´æ˜   | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/wiki_atomic_edits            | æ˜¯       |\n| chatmed_consult      | åŒ»è¯ | 549,326   | é—®ç­”              | é—®ç­”   | ä¼˜   | Wei Zhu                                                      | çœŸå®ä¸–ç•Œçš„åŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œä½¿ç”¨ gpt3.5 è¿›è¡Œå›ç­”               | æ˜¯                | å¦       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset | å¦       |\n| webqa                | ç™¾ç§‘ | 42,216    | é—®ç­”              | é—®ç­”   | ä¼˜   | suolyer                                                      | ç™¾åº¦äº2016å¹´å¼€æºçš„æ•°æ®é›†ï¼Œæ•°æ®æ¥è‡ªäºç™¾åº¦çŸ¥é“ï¼›æ ¼å¼ä¸ºä¸€ä¸ªé—®é¢˜å¤šç¯‡æ„æ€åŸºæœ¬ä¸€è‡´çš„æ–‡ç« ï¼Œåˆ†ä¸ºäººä¸ºæ ‡æ³¨ä»¥åŠæµè§ˆå™¨æ£€ç´¢ï¼›æ•°æ®æ•´ä½“è´¨é‡ä¸­ï¼Œå› ä¸ºæ··åˆäº†å¾ˆå¤šæ£€ç´¢è€Œæ¥çš„æ–‡ç«  | æ˜¯                | æœªè¯´æ˜   | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/suolyer/webqa/viewer/suolyer--webqa/train?p=3 | å¦       |\n| dureader_robust      | ç™¾ç§‘ | 65,937    | æœºå™¨é˜…è¯»ç†è§£ é—®ç­” | é—®ç­”   | ä¼˜   | ç™¾åº¦                                                         | DuReader robustæ—¨åœ¨åˆ©ç”¨çœŸå®åº”ç”¨ä¸­çš„æ•°æ®æ ·æœ¬æ¥è¡¡é‡é˜…è¯»ç†è§£æ¨¡å‹çš„é²æ£’æ€§ï¼Œè¯„æµ‹æ¨¡å‹çš„è¿‡æ•æ„Ÿæ€§ã€è¿‡ç¨³å®šæ€§ä»¥åŠæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¯é¦–ä¸ªä¸­æ–‡é˜…è¯»ç†è§£é²æ£’æ€§æ•°æ®é›†ã€‚ | æ˜¯                | æ˜¯       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/PaddlePaddle/dureader_robust/viewer/plain_text/train?row=96 | å¦       |\n| csl                  | å­¦æœ¯ | 395,927   | è¯­æ–™              | æ‘˜è¦   | ä¼˜   | Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao and Hui Zhang | æä¾›é¦–ä¸ªä¸­æ–‡ç§‘å­¦æ–‡çŒ®æ•°æ®é›†ï¼ˆCSLï¼‰ï¼ŒåŒ…å« 396,209 ç¯‡ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠè®ºæ–‡å…ƒä¿¡æ¯ ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯ã€å­¦ç§‘ã€é—¨ç±»ï¼‰ã€‚CSL æ•°æ®é›†å¯ä»¥ä½œä¸ºé¢„è®­ç»ƒè¯­æ–™ï¼Œä¹Ÿå¯ä»¥æ„å»ºè®¸å¤šNLPä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ï¼ˆæ ‡é¢˜é¢„æµ‹ï¼‰ã€ å…³é”®è¯ç”Ÿæˆå’Œæ–‡æœ¬åˆ†ç±»ç­‰ã€‚ | æ˜¯                | æ˜¯       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/neuclir/csl                  | å¦       |\n| miracl-corpus        | ç™¾ç§‘ | 4,934,368 | è¯­æ–™              | æ‘˜è¦   | ä¼˜   | MIRACL                                                       | The corpus for each language is prepared from a Wikipedia dump, where we keep only the plain text and discard images, tables, etc. Each article is segmented into multiple passages using WikiExtractor based on natural discourse units (e.g., \\n\\n in the wiki markup). Each of these passages comprises a \"document\" or unit of retrieval. We preserve the Wikipedia article title of each passage. | æ˜¯                | æ˜¯       | æ˜¯   | æ˜¯   | https://huggingface.co/datasets/miracl/miracl-corpus         | å¦       |\n| lawzhidao            | æ³•å¾‹ | 36,368    | é—®ç­”              | é—®ç­”   | ä¼˜   | å’Œé²¸ç¤¾åŒº-Ustinian                                            | ç™¾åº¦çŸ¥é“æ¸…æ´—åçš„æ³•å¾‹é—®ç­”                                     | æ˜¯                | æ˜¯       | å¦   | æ˜¯   | https://www.heywhale.com/mw/dataset/5e953ca8e7ec38002d02fca7/content | å¦       |\n| CINLID               | æˆè¯­ | 34,746    | å¹³è¡Œè¯­ä¹‰          | ç›¸ä¼¼   | ä¼˜   | é«˜é•¿å®½                                                       | ä¸­æ–‡æˆè¯­è¯­ä¹‰æ¨ç†æ•°æ®é›†ï¼ˆChinese Idioms Natural Language Inference Datasetï¼‰æ”¶é›†äº†106832æ¡ç”±äººå·¥æ’°å†™çš„æˆè¯­å¯¹ï¼ˆå«å°‘é‡æ­‡åè¯­ã€ä¿—è¯­ç­‰çŸ­æ–‡æœ¬ï¼‰ï¼Œé€šè¿‡äººå·¥æ ‡æ³¨çš„æ–¹å¼è¿›è¡Œå¹³è¡¡åˆ†ç±»ï¼Œæ ‡ç­¾ä¸ºentailmentã€contradictionå’Œneutralï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰çš„ä»»åŠ¡ã€‚ | æ˜¯                | å¦       | å¦   | æ˜¯   | https://www.luge.ai/#/luge/dataDetail?id=39                  | æ˜¯       |\n| DuSQL                | SQL  | 25,003    | NL2SQL            | SQL    | ä¼˜   | ç™¾åº¦                                                         | DuSQLæ˜¯ä¸€ä¸ªé¢å‘å®é™…åº”ç”¨çš„æ•°æ®é›†ï¼ŒåŒ…å«200ä¸ªæ•°æ®åº“ï¼Œè¦†ç›–äº†164ä¸ªé¢†åŸŸï¼Œé—®é¢˜è¦†ç›–äº†åŒ¹é…ã€è®¡ç®—ã€æ¨ç†ç­‰å®é™…åº”ç”¨ä¸­å¸¸è§å½¢å¼ã€‚è¯¥æ•°æ®é›†æ›´è´´è¿‘çœŸå®åº”ç”¨åœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹é¢†åŸŸæ— å…³ã€é—®é¢˜æ— å…³ï¼Œä¸”å…·å¤‡è®¡ç®—æ¨ç†ç­‰èƒ½åŠ›ã€‚ | æ˜¯                | å¦       | å¦   | æ˜¯   | https://www.luge.ai/#/luge/dataDetail?id=13                  | å¦       |\n| Zhuiyi-NL2SQL        | SQL  | 45,918    | NL2SQL            | SQL    | ä¼˜   | è¿½ä¸€ç§‘æŠ€ åˆ˜äº‘å³°                                              | NL2SQLæ˜¯ä¸€ä¸ªå¤šé¢†åŸŸçš„ç®€å•æ•°æ®é›†ï¼Œå…¶ä¸»è¦åŒ…å«åŒ¹é…ç±»å‹é—®é¢˜ã€‚è¯¥æ•°æ®é›†ä¸»è¦éªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶è¦æ±‚æ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€é—®é¢˜æ³›åŒ–èƒ½åŠ›ã€‚ | æ˜¯                | å¦       | å¦   | æ˜¯   | https://www.luge.ai/#/luge/dataDetail?id=12                  | å¦       |\n| Cspider              | SQL  | 7,785     | NL2SQL            | SQL    | ä¼˜   | è¥¿æ¹–å¤§å­¦ å¼ å²³                                                | CSpideræ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ•°æ®é›†ï¼Œå…¶é—®é¢˜ä»¥ä¸­æ–‡è¡¨è¾¾ï¼Œæ•°æ®åº“ä»¥è‹±æ–‡å­˜å‚¨ï¼Œè¿™ç§åŒè¯­æ¨¡å¼åœ¨å®é™…åº”ç”¨ä¸­ä¹Ÿéå¸¸å¸¸è§ï¼Œå°¤å…¶æ˜¯æ•°æ®åº“å¼•æ“å¯¹ä¸­æ–‡æ”¯æŒä¸å¥½çš„æƒ…å†µä¸‹ã€‚è¯¥æ•°æ®é›†è¦æ±‚æ¨¡å‹é¢†åŸŸæ— å…³ã€é—®é¢˜æ— å…³ï¼Œä¸”èƒ½å¤Ÿå®ç°å¤šè¯­è¨€åŒ¹é…ã€‚ | æ˜¯                | å¦       | å¦   | æ˜¯   | https://www.luge.ai/#/luge/dataDetail?id=11                  | å¦       |\n| news2016zh           | æ–°é—» | 2,507,549 | è¯­æ–™              | æ‘˜è¦   | è‰¯   | Bright Xu                                                    | åŒ…å«äº†250ä¸‡ç¯‡æ–°é—»ã€‚æ–°é—»æ¥æºæ¶µç›–äº†6.3ä¸‡ä¸ªåª’ä½“ï¼Œå«æ ‡é¢˜ã€å…³é”®è¯ã€æè¿°ã€æ­£æ–‡ã€‚ | æ˜¯                | æ˜¯       | å¦   | æ˜¯   | https://github.com/brightmart/nlp_chinese_corpus             | å¦       |\n| baike2018qa          | ç™¾ç§‘ | 1,470,142 | é—®ç­”              | é—®ç­”   | è‰¯   | Bright Xu                                                    | å«æœ‰150ä¸‡ä¸ªé¢„å…ˆè¿‡æ»¤è¿‡çš„ã€é«˜è´¨é‡é—®é¢˜å’Œç­”æ¡ˆï¼Œæ¯ä¸ªé—®é¢˜å±äºä¸€ä¸ªç±»åˆ«ã€‚æ€»å…±æœ‰492ä¸ªç±»åˆ«ï¼Œå…¶ä¸­é¢‘ç‡è¾¾åˆ°æˆ–è¶…è¿‡10æ¬¡çš„ç±»åˆ«æœ‰434ä¸ªã€‚ | æ˜¯                | æ˜¯       | å¦   | æ˜¯   | https://github.com/brightmart/nlp_chinese_corpus             | å¦       |\n| webtext2019zh        | ç™¾ç§‘ | 4,258,310 | é—®ç­”              | é—®ç­”   | ä¼˜   | Bright Xu                                                    | å«æœ‰410ä¸‡ä¸ªé¢„å…ˆè¿‡æ»¤è¿‡çš„ã€é«˜è´¨é‡é—®é¢˜å’Œå›å¤ã€‚æ¯ä¸ªé—®é¢˜å±äºä¸€ä¸ªã€è¯é¢˜ã€‘ï¼Œæ€»å…±æœ‰2.8ä¸‡ä¸ªå„å¼è¯é¢˜ï¼Œè¯é¢˜åŒ…ç½—ä¸‡è±¡ã€‚ | æ˜¯                | æ˜¯       | å¦   | æ˜¯   | https://github.com/brightmart/nlp_chinese_corpus             | å¦       |\n| SimCLUE              | ç™¾ç§‘ | 775,593   | å¹³è¡Œè¯­ä¹‰          | ç›¸ä¼¼   | è‰¯   | æ•°æ®é›†åˆï¼Œè¯·åœ¨ simCLUE ä¸­æŸ¥çœ‹                                | æ•´åˆäº†ä¸­æ–‡é¢†åŸŸç»å¤§å¤šæ•°å¯ç”¨çš„å¼€æºçš„è¯­ä¹‰ç›¸ä¼¼åº¦å’Œè‡ªç„¶è¯­è¨€æ¨ç†çš„æ•°æ®é›†ï¼Œå¹¶é‡æ–°åšäº†æ•°æ®æ‹†åˆ†å’Œæ•´ç†ã€‚ | æ˜¯                | å¦       | å¦   | æ˜¯   | https://github.com/CLUEbenchmark/SimCLUE                     | æ˜¯       |\n| Chinese-SQuAD        | æ–°é—» | 76,449    | æœºå™¨é˜…è¯»ç†è§£      | é—®ç­”   | ä¼˜   | junzeng-pluto                                                | ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œé€šè¿‡æœºå™¨ç¿»è¯‘åŠ äººå·¥æ ¡æ­£çš„æ–¹å¼ä»åŸå§‹Squadè½¬æ¢è€Œæ¥ | æ˜¯                | å¦       | å¦   | æ˜¯   | https://github.com/pluto-junzeng/ChineseSquad                | å¦       |\n\n## ğŸ—“ï¸ è®¡åˆ’è¡¨\n\n- [x] å®Œæˆ MTEB ä¸­æ–‡è¯„æµ‹ BenchMark, [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)\n- [x] å®Œæˆ Large æ¨¡å‹çš„è®­ç»ƒå’Œå¼€æº\n- [x] å®Œæˆ Finetuner ï¼Œå…è®¸æ›´ä¼˜é›…çš„å¾®è°ƒ\n- [ ] å®Œæˆæ”¯æŒä»£ç æ£€ç´¢çš„æ¨¡å‹\n- [ ] å¯¹ M3E æ•°æ®é›†è¿›è¡Œæ¸…æ´—ï¼Œä¿ç•™é«˜è´¨é‡çš„éƒ¨åˆ†ï¼Œç»„æˆ m3e-hqï¼Œå¹¶åœ¨ huggingface ä¸Šå¼€æº\n- [ ] åœ¨ m3e-hq çš„æ•°æ®é›†ä¸Šè¡¥å…… hard negative çš„æ ·æœ¬åŠç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç»„æˆ m3e-hq-with-scoreï¼Œå¹¶åœ¨ huggingface ä¸Šå¼€æº\n- [ ] åœ¨ m3e-hq-with-score ä¸Šé€šè¿‡ [cosent loss](https://github.com/wangyuxinwhy/uniem/blob/main/uniem/criteria.py#LL24C39-L24C39) loss è¿›è¡Œè®­ç»ƒå¹¶å¼€æºæ¨¡å‹ï¼ŒCoSent åŸç†å‚è€ƒè¿™ç¯‡[åšå®¢](https://kexue.fm/archives/8847)\n- [ ] å¼€æºå•†ç”¨ç‰ˆæœ¬çš„ M3E models\n\n## ğŸ™ è‡´è°¢\n\næ„Ÿè°¢å¼€æºç¤¾åŒºæä¾›çš„ä¸­æ–‡è¯­æ–™ï¼Œæ„Ÿè°¢æ‰€æœ‰åœ¨æ­¤å·¥ä½œä¸­æä¾›å¸®åŠ©çš„äººä»¬ï¼Œå¸Œæœ›ä¸­æ–‡ç¤¾åŒºè¶Šæ¥è¶Šå¥½ï¼Œå…±å‹‰ï¼\n\n## ğŸ“œ License\n\nM3E models ä½¿ç”¨çš„æ•°æ®é›†ä¸­åŒ…æ‹¬å¤§é‡éå•†ç”¨çš„æ•°æ®é›†ï¼Œæ‰€ä»¥ M3E models ä¹Ÿæ˜¯éå•†ç”¨çš„ï¼Œä»…ä¾›ç ”ç©¶ä½¿ç”¨ã€‚ä¸è¿‡æˆ‘ä»¬å·²ç»åœ¨ M3E æ•°æ®é›†ä¸Šæ ‡è¯†äº†å•†ç”¨å’Œéå•†ç”¨çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚è‡ªè¡Œè®­ç»ƒã€‚\n\n## Citation\nPlease cite this model using the following format:\n```\n  @software {Moka Massive Mixed Embedding,  \n  author = {Wang Yuxin,Sun Qingxuan,He sicheng},  \n  title = {M3E: Moka Massive Mixed Embedding Model},  \n  year = {2023}\n  }\n```",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":102268160,\"storage_bytes\":818238909,\"files_count\":12,\"spaces_count\":29,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BertModel\"],\"model_type\":\"bert\",\"tokenizer_config\":{\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\",\"pad_token\":\"[PAD]\",\"sep_token\":\"[SEP]\",\"unk_token\":\"[UNK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:guidance\",\"source_url\":\"https://github.com/microsoft/guidance\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:semantic-kernel\",\"source_url\":\"https://github.com/microsoft/semantic-kernel\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:HKUNLP:instructor-embedding\",\"source_url\":\"https://github.com/HKUNLP/instructor-embedding\"},{\"type\":\"has_code\",\"target_id\":\"github:HKUNLP:instructor-embedding\",\"source_url\":\"https://github.com/HKUNLP/instructor-embedding\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:THUIR:T2Ranking\",\"source_url\":\"https://github.com/THUIR/T2Ranking\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:ymcui:cmrc2018\",\"source_url\":\"https://github.com/ymcui/cmrc2018\"},{\"type\":\"has_code\",\"target_id\":\"github:brightmart:nlp_chinese_corpus\",\"source_url\":\"https://github.com/brightmart/nlp_chinese_corpus\"},{\"type\":\"has_code\",\"target_id\":\"github:brightmart:nlp_chinese_corpus\",\"source_url\":\"https://github.com/brightmart/nlp_chinese_corpus\"},{\"type\":\"has_code\",\"target_id\":\"github:brightmart:nlp_chinese_corpus\",\"source_url\":\"https://github.com/brightmart/nlp_chinese_corpus\"},{\"type\":\"has_code\",\"target_id\":\"github:CLUEbenchmark:SimCLUE\",\"source_url\":\"https://github.com/CLUEbenchmark/SimCLUE\"},{\"type\":\"has_code\",\"target_id\":\"github:pluto-junzeng:ChineseSquad\",\"source_url\":\"https://github.com/pluto-junzeng/ChineseSquad\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"},{\"type\":\"has_code\",\"target_id\":\"github:wangyuxinwhy:uniem\",\"source_url\":\"https://github.com/wangyuxinwhy/uniem\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 69.9,
    "content_hash": "358e1476a8c12009fb539a9716e5d7f6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/moka-ai/m3e-base\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:bytedance:animatediff-lightning",
    "name": "AnimateDiff-Lightning",
    "author": "ByteDance",
    "description": "--- license: creativeml-openrail-m tags: - text-to-video - stable-diffusion - animatediff library_name: diffusers inference: false --- <video src='https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_t2v.mp4' width=\"100%\" autoplay muted loop playsinline style='margin:0'></video> <video src='https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_v2v.mp4' width=\"100%\" autoplay muted loop playsinline style='m...",
    "tags": [
      "diffusers",
      "text-to-video",
      "stable-diffusion",
      "animatediff",
      "arxiv:2403.12706",
      "license:creativeml-openrail-m",
      "region:us"
    ],
    "pipeline_tag": "text-to-video",
    "likes": 974,
    "downloads": 47077,
    "source": "huggingface",
    "source_url": "https://huggingface.co/ByteDance/AnimateDiff-Lightning",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\ntags:\n- text-to-video\n- stable-diffusion\n- animatediff\nlibrary_name: diffusers\ninference: false\n---\n# AnimateDiff-Lightning\n\n<video src='https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_t2v.mp4' width=\"100%\" autoplay muted loop playsinline style='margin:0'></video>\n<video src='https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_v2v.mp4' width=\"100%\" autoplay muted loop playsinline style='margin:0'></video>\n\nAnimateDiff-Lightning is a lightning-fast text-to-video generation model. It can generate videos more than ten times faster than the original AnimateDiff. For more information, please refer to our research paper: [AnimateDiff-Lightning: Cross-Model Diffusion Distillation](https://arxiv.org/abs/2403.12706). We release the model as part of the research.\n\nOur models are distilled from [AnimateDiff SD1.5 v2](https://huggingface.co/guoyww/animatediff). This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is great. Our 1-step model is only provided for research purposes.\n\n\n## Demo\n\nTry AnimateDiff-Lightning using our text-to-video generation [demo](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning).\n\n\n## Recommendation\n\nAnimateDiff-Lightning produces the best results when used with stylized base models. We recommend using the following base models:\n\nRealistic\n- [epiCRealism](https://civitai.com/models/25694)\n- [Realistic Vision](https://civitai.com/models/4201)\n- [DreamShaper](https://civitai.com/models/4384)\n- [AbsoluteReality](https://civitai.com/models/81458)\n- [MajicMix Realistic](https://civitai.com/models/43331)\n\nAnime & Cartoon\n- [ToonYou](https://civitai.com/models/30240)\n- [IMP](https://civitai.com/models/56680)\n- [Mistoon Anime](https://civitai.com/models/24149)\n- [DynaVision](https://civitai.com/models/75549)\n- [RCNZ Cartoon 3d](https://civitai.com/models/66347)\n- [MajicMix Reverie](https://civitai.com/models/65055)\n\nAdditionally, feel free to explore different settings. We find using 3 inference steps on the 2-step model produces great results. We find certain base models produces better results with CFG. We also recommend using [Motion LoRAs](https://huggingface.co/guoyww/animatediff/tree/main) as they produce stronger motion. We use Motion LoRAs with strength 0.7~0.8 to avoid watermark.\n\n## Diffusers Usage\n\n```python\nimport torch\nfrom diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\nfrom diffusers.utils import export_to_gif\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\ndevice = \"cuda\"\ndtype = torch.float16\n\nstep = 4  # Options: [1,2,4,8]\nrepo = \"ByteDance/AnimateDiff-Lightning\"\nckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\nbase = \"emilianJR/epiCRealism\"  # Choose to your favorite base model.\n\nadapter = MotionAdapter().to(device, dtype)\nadapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\npipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")\n\noutput = pipe(prompt=\"A girl smiling\", guidance_scale=1.0, num_inference_steps=step)\nexport_to_gif(output.frames[0], \"animation.gif\")\n```\n\n## ComfyUI Usage\n\n1. Download [animatediff_lightning_workflow.json](https://huggingface.co/ByteDance/AnimateDiff-Lightning/raw/main/comfyui/animatediff_lightning_workflow.json) and import it in ComfyUI.\n1. Install nodes. You can install them manually or use [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager).\n    * [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)\n    * [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n1. Download your favorite base model checkpoint and put them under `/models/checkpoints/`\n1. Download AnimateDiff-Lightning checkpoint `animatediff_lightning_Nstep_comfyui.safetensors` and put them under `/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/`\n\n\n![ComfyUI Workflow](https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/comfyui/animatediff_lightning_workflow.jpg)\n\n\n## Video-to-Video Generation\n\nAnimateDiff-Lightning is great for video-to-video generation. We provide the simplist comfyui workflow using ControlNet.\n\n1. Download [animatediff_lightning_v2v_openpose_workflow.json](https://huggingface.co/ByteDance/AnimateDiff-Lightning/raw/main/comfyui/animatediff_lightning_v2v_openpose_workflow.json) and import it in ComfyUI.\n1. Install nodes. You can install them manually or use [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager).\n    * [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)\n    * [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n    * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n    * [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n1. Download your favorite base model checkpoint and put them under `/models/checkpoints/`\n1. Download AnimateDiff-Lightning checkpoint `animatediff_lightning_Nstep_comfyui.safetensors` and put them under `/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/`\n1. Download [ControlNet OpenPose](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main) `control_v11p_sd15_openpose.pth` checkpoint to `/models/controlnet/`\n1. Upload your video and run the pipeline.\n\nAdditional notes:\n\n1. Video shouldn't be too long or too high resolution. We used 576x1024 8 second 30fps videos for testing.\n1. Set the frame rate to match your input video. This allows audio to match with the output video.\n1. DWPose will download checkpoint itself on its first run.\n1. DWPose may get stuck in UI, but the pipeline is actually still running in the background. Check ComfyUI log and your output folder.\n\n![ComfyUI OpenPose Workflow](https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/comfyui/animatediff_lightning_v2v_openpose_workflow.jpg)\n\n# Cite Our Work\n```\n@misc{lin2024animatedifflightning,\n      title={AnimateDiff-Lightning: Cross-Model Diffusion Distillation}, \n      author={Shanchuan Lin and Xiao Yang},\n      year={2024},\n      eprint={2403.12706},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":7286508236,\"files_count\":18,\"spaces_count\":76,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ltdrdata:ComfyUI-Manager\",\"source_url\":\"https://github.com/ltdrdata/ComfyUI-Manager\"},{\"type\":\"has_code\",\"target_id\":\"github:Kosinkadink:ComfyUI-AnimateDiff-Evolved\",\"source_url\":\"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved\"},{\"type\":\"has_code\",\"target_id\":\"github:Kosinkadink:ComfyUI-VideoHelperSuite\",\"source_url\":\"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite\"},{\"type\":\"has_code\",\"target_id\":\"github:ltdrdata:ComfyUI-Manager\",\"source_url\":\"https://github.com/ltdrdata/ComfyUI-Manager\"},{\"type\":\"has_code\",\"target_id\":\"github:Kosinkadink:ComfyUI-AnimateDiff-Evolved\",\"source_url\":\"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved\"},{\"type\":\"has_code\",\"target_id\":\"github:Kosinkadink:ComfyUI-VideoHelperSuite\",\"source_url\":\"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite\"},{\"type\":\"has_code\",\"target_id\":\"github:Kosinkadink:ComfyUI-Advanced-ControlNet\",\"source_url\":\"https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet\"},{\"type\":\"has_code\",\"target_id\":\"github:Fannovel16:comfyui_controlnet_aux\",\"source_url\":\"https://github.com/Fannovel16/comfyui_controlnet_aux\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.12706\",\"source_url\":\"https://arxiv.org/abs/2403.12706\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 64.9,
    "content_hash": "331318cf442d6425c2dd5b2f9cbe87ef",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/ByteDance/AnimateDiff-Lightning\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:microsoft:phi-3-vision-128k-instruct",
    "name": "Phi-3-vision-128k-instruct",
    "author": "microsoft",
    "description": "--- license: mit license_link: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code - vision inference: parameters: temperature: 0.7 widget: - messages: - role: user content: <|image_1|>Can you describe what you see in the image? --- ğŸ‰ **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct...",
    "tags": [
      "transformers",
      "safetensors",
      "phi3_v",
      "text-generation",
      "nlp",
      "code",
      "vision",
      "conversational",
      "custom_code",
      "multilingual",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 969,
    "downloads": 17746,
    "source": "huggingface",
    "source_url": "https://huggingface.co/microsoft/Phi-3-vision-128k-instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE\n\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n- vision\ninference:\n  parameters:\n    temperature: 0.7\nwidget:\n  - messages:\n      - role: user\n        content: <|image_1|>Can you describe what you see in the image?\n---\nğŸ‰ **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Model Summary\n\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision.  The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nResources and Technical Documentation:\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n+ [Phi-3 on Azure AI Studio](https://aka.ms/try-phi3vision)\n+ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook)\n\n\n|         | Short Context | Long Context |\n| ------- | ------------- | ------------ |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \n\n1) memory/compute constrained environments;\n2) latency bound scenarios;\n3) general image understanding;\n4) OCR;\n5) chart and table understanding.\n\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \n\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n## How to Use\n\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\n\nPhi-3-Vision-128K-Instruct is also available in [Azure AI Studio](https://aka.ms/phi3-azure-ai).\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n```markdown\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n \n```\n\nwhere the model generates the text after `<|assistant|>` . In case of multi-turn conversation, the prompt can be formatted as follows:\n\n```markdown\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n \n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \n\nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\n\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n\nmessages = [ \n    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, \n    {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, \n    {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \n\nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \n\nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n\n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n\nprint(response) \n```\n\nAdditional basic examples are provided [here](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/blob/main/sample_inference.py).\n\n### How to finetune?\nWe recommend user to take a look at the [Phi-3 CookBook finetuning recipe for Vision](https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_Vision.md)\n\n\n## Responsible AI Considerations\n\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n\n+ Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.    \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.  \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.  \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.      \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:  \n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n+ Identification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n  \n## Training\n\n### Model\n\n* Architecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\n* Inputs: Text and Image. Itâ€™s best suited for prompts using the chat format. \n* Context length: 128K tokens\n* GPUs: 512 H100-80G\n* Training time: 1.5 days\n* Training data: 500B vision and text tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between February and April 2024\n* Status: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\n* Release Type: Open weight release\n* Release dates: The model weight is released on May 21, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, and is a combination of \n\n1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;\n2) selected high-quality image-text interleave;\n3) newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\n4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\n \nMore details can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n## Benchmarks\n\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\n\n|Benchmark|Phi-3 Vision-128K-In|LlaVA-1.6 Vicuna-7B|QWEN-VL Chat|Llama3-Llava-Next-8B|Claude-3 Haiku|Gemini 1.0 Pro V|GPT-4V-Turbo|\n|---------|---------------------|------------------|------------|--------------------|--------------|----------------|------------|\n|MMMU|40.4|34.2|39.0|36.4|40.7|42.0|55.5|Â \n|MMBench|80.5|76.3|75.8|79.4|62.4|80.0|86.1|\n|ScienceQA|90.8|70.6|67.2|73.7|72.0|79.7|75.7|\n|MathVista|44.5|31.5|29.4|34.8|33.2|35.0|47.5|\n|InterGPS|38.1|20.5|22.3|24.6|32.1|28.6|41.0|\n|AI2D|76.7|63.1|59.8|66.9|60.3|62.8|74.7|\n|ChartQA|81.4|55.0|50.9|65.8|59.3|58.0|62.3|\n|TextVQA|70.9|64.6|59.4|55.7|62.7|64.7|68.1|\n|POPE|85.8|87.2|82.6|87.0|74.4|84.2|83.7|\n\n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followâ€¯[Microsoftâ€™s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":4146621440,\"storage_bytes\":16586661776,\"files_count\":20,\"spaces_count\":47,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Phi3VForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_phi3_v.Phi3VConfig\",\"AutoModelForCausalLM\":\"modeling_phi3_v.Phi3VForCausalLM\"},\"model_type\":\"phi3_v\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{% for message in messages %}{{'<|' + message['role'] + '|>' + '\\n' + message['content'] + '<|end|>\\n' }}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{- '<|assistant|>\\n' -}}{% endif %}\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:microsoft:Phi-3CookBook\",\"source_url\":\"https://github.com/microsoft/Phi-3CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers`.\",\"source_url\":\"https://github.com/huggingface/transformers`.\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:Phi-3CookBook\",\"source_url\":\"https://github.com/microsoft/Phi-3CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:pytorch:pytorch\",\"source_url\":\"https://github.com/pytorch/pytorch\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:HazyResearch:flash-attention\",\"source_url\":\"https://github.com/HazyResearch/flash-attention\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 79.9,
    "content_hash": "3e8385944f76437cee916a91bce2c7ba",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:compvis:stable-diffusion",
    "name": "stable-diffusion",
    "author": "CompVis",
    "description": "--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image inference: false --- Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. This model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access. For the first version 4 model checkpoints are released. *Higher* versions have been trained for long...",
    "tags": [
      "stable-diffusion",
      "text-to-image",
      "arxiv:2207.12598",
      "license:creativeml-openrail-m",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 967,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/CompVis/stable-diffusion",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\ninference: false\n---\n# Stable Diffusion\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nThis model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under [Model Access](#model-access).\n\n## Stable Diffusion Version 1\n\nFor the first version 4 model checkpoints are released.\n*Higher* versions have been trained for longer and are thus usually better in terms of image generation quality then *lower* versions. More specifically: \n\n- **stable-diffusion-v1-1**: The checkpoint is randomly initialized and has been trained on 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- **stable-diffusion-v1-2**: The checkpoint resumed training from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- **stable-diffusion-v1-3**: The checkpoint resumed training from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)\n- **stable-diffusion-v1-4**: The checkpoint resumed training from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [**`stable-diffusion-v1-4`**](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2`.225,000 steps at resolution `512x512` on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n### Model Access\n\nEach checkpoint can be used both with Hugging Face's [ ğŸ§¨ Diffusers library](https://github.com/huggingface/diffusers) or the original [Stable Diffusion GitHub repository](https://github.com/CompVis/stable-diffusion). Note that you have to *\"click-request\"* them on each respective model repository.\n\n| **[ğŸ¤—'s ğŸ§¨ Diffusers library](https://github.com/huggingface/diffusers)**     | **[Stable Diffusion GitHub repository](https://github.com/CompVis/stable-diffusion)** |\n| ----------- | ----------- |\n| [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1)      | [`stable-diffusion-v-1-1-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-1-original)       |\n| [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2)   | [`stable-diffusion-v-1-2-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-2-original)        |\n| [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3)   | [`stable-diffusion-v-1-3-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-3-original)        |\n| [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4)   | [`stable-diffusion-v-1-4-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)        |\n\n### Demo\n\nTo quickly try out the model, you can try out the [Stable Diffusion Space](https://huggingface.co/spaces/stabilityai/stable-diffusion).\n\n### License\n\n[The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":2132781863,\"files_count\":5,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:christophschuhmann:improved-aesthetic-predictor\",\"source_url\":\"https://github.com/christophschuhmann/improved-aesthetic-predictor\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:CompVis:stable-diffusion\",\"source_url\":\"https://github.com/CompVis/stable-diffusion\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:CompVis:stable-diffusion\",\"source_url\":\"https://github.com/CompVis/stable-diffusion\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2207.12598\",\"source_url\":\"https://arxiv.org/abs/2207.12598\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 64.9,
    "content_hash": "f561d8df23c833c0c10f2a328aa79254",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/CompVis/stable-diffusion\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:answerdotai:modernbert-base",
    "name": "ModernBERT-base",
    "author": "answerdotai",
    "description": "--- library_name: transformers license: apache-2.0 language: - en tags: - fill-mask - masked-lm - long-context - modernbert pipeline_tag: fill-mask inference: false --- 1. Model Summary 2. Usage 3. Evaluation 4. Limitations 5. Training 6. License 7. Citation ModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural i...",
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "modernbert",
      "fill-mask",
      "masked-lm",
      "long-context",
      "en",
      "arxiv:2412.13663",
      "license:apache-2.0",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "fill-mask",
    "likes": 964,
    "downloads": 820673,
    "source": "huggingface",
    "source_url": "https://huggingface.co/answerdotai/ModernBERT-base",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n- fill-mask\n- masked-lm\n- long-context\n- modernbert\npipeline_tag: fill-mask\ninference: false\n---\n\n# ModernBERT\n\n## Table of Contents\n1. [Model Summary](#model-summary)\n2. [Usage](#Usage)\n3. [Evaluation](#Evaluation)\n4. [Limitations](#limitations)\n5. [Training](#training)\n6. [License](#license)\n7. [Citation](#citation)\n\n## Model Summary\n\nModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural improvements such as:\n\n- **Rotary Positional Embeddings (RoPE)** for long-context support.  \n- **Local-Global Alternating Attention** for efficiency on long inputs.  \n- **Unpadding and Flash Attention** for efficient inference.  \n\nModernBERTâ€™s native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search.\n\nIt is available in the following sizes:\n\n- [ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base) - 22 layers, 149 million parameters\n- [ModernBERT-large](https://huggingface.co/answerdotai/ModernBERT-large) - 28 layers, 395 million parameters\n\nFor more information about ModernBERT, we recommend our [release blog post](https://huggingface.co/blog/modernbert) for a high-level overview, and our [arXiv pre-print](https://arxiv.org/abs/2412.13663) for in-depth information.\n\n*ModernBERT is a collaboration between [Answer.AI](https://answer.ai), [LightOn](https://lighton.ai), and friends.*\n\n## Usage\n\nYou can use these models directly with the `transformers` library starting from v4.48.0:\n\n```sh\npip install -U transformers>=4.48.0\n```\n\nSince ModernBERT is a Masked Language Model (MLM), you can use the `fill-mask` pipeline or load it via `AutoModelForMaskedLM`. To use ModernBERT for downstream tasks like classification, retrieval, or QA, fine-tune it following standard BERT fine-tuning recipes.\n\n**âš ï¸ If your GPU supports it, we recommend using ModernBERT with Flash Attention 2 to reach the highest efficiency. To do so, install Flash Attention as follows, then use the model as normal:**\n\n```bash\npip install flash-attn\n```\n\nUsing `AutoModelForMaskedLM`:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nmodel_id = \"answerdotai/ModernBERT-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\n\ntext = \"The capital of France is [MASK].\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# To get predictions for the mask:\nmasked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\npredicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\npredicted_token = tokenizer.decode(predicted_token_id)\nprint(\"Predicted token:\", predicted_token)\n# Predicted token:  Paris\n```\n\nUsing a pipeline:\n\n```python\nimport torch\nfrom transformers import pipeline\nfrom pprint import pprint\n\npipe = pipeline(\n    \"fill-mask\",\n    model=\"answerdotai/ModernBERT-base\",\n    torch_dtype=torch.bfloat16,\n)\n\ninput_text = \"He walked to the [MASK].\"\nresults = pipe(input_text)\npprint(results)\n```\n\n**Note:** ModernBERT does not use token type IDs, unlike some earlier BERT models. Most downstream usage is identical to standard BERT models on the Hugging Face Hub, except you can omit the `token_type_ids` parameter.\n\n## Evaluation\n\nWe evaluate ModernBERT across a range of tasks, including natural language understanding (GLUE), general retrieval (BEIR), long-context retrieval (MLDR), and code retrieval (CodeSearchNet and StackQA).\n\n**Key highlights:**\n- On GLUE, ModernBERT-base surpasses other similarly-sized encoder models, and ModernBERT-large is second only to Deberta-v3-large.\n- For general retrieval tasks, ModernBERT performs well on BEIR in both single-vector (DPR-style) and multi-vector (ColBERT-style) settings.\n- Thanks to the inclusion of code data in its training mixture, ModernBERT as a backbone also achieves new state-of-the-art code retrieval results on CodeSearchNet and StackQA.\n\n### Base Models\n\n| Model       | IR (DPR)     | IR (DPR)     | IR (DPR)     | IR (ColBERT)  | IR (ColBERT)  | NLU  | Code | Code |\n|-------------|--------------|--------------|--------------|---------------|---------------|------|------|------|\n|             | BEIR         | MLDR_OOD     | MLDR_ID      | BEIR          | MLDR_OOD      | GLUE | CSN  | SQA  |\n| BERT        | 38.9         | 23.9         | 32.2         | 49.0          | 28.1          | 84.7 | 41.2 | 59.5 |\n| RoBERTa     | 37.7         | 22.9         | 32.8         | 48.7          | 28.2          | 86.4 | 44.3 | 59.6 |\n| DeBERTaV3   | 20.2         | 5.4          | 13.4         | 47.1          | 21.9          | 88.1 | 17.5 | 18.6 |\n| NomicBERT   | 41.0         | 26.7         | 30.3         | 49.9          | 61.3          | 84.0 | 41.6 | 61.4 |\n| GTE-en-MLM  | 41.4         | **34.3**    |**44.4**   | 48.2          | 69.3          | 85.6 | 44.9 | 71.4 |\n| ModernBERT  | **41.6**    | 27.4         | 44.0         | **51.3**    | **80.2**      | **88.4** | **56.4** |**73.6**|\n\n---\n\n### Large Models\n\n| Model       | IR (DPR)     | IR (DPR)     | IR (DPR)     | IR (ColBERT)  | IR (ColBERT)  | NLU  | Code | Code |\n|-------------|--------------|--------------|--------------|---------------|---------------|------|------|------|\n|             | BEIR         | MLDR_OOD     | MLDR_ID      | BEIR          | MLDR_OOD      | GLUE | CSN  | SQA  |\n| BERT        | 38.9         | 23.3         | 31.7         | 49.5          | 28.5          | 85.2 | 41.6 | 60.8 |\n| RoBERTa     | 41.4         | 22.6         | 36.1         | 49.8          | 28.8          | 88.9 | 47.3 | 68.1 |\n| DeBERTaV3   | 25.6         | 7.1          | 19.2         | 46.7          | 23.0          | **91.4**| 21.2 | 19.7 |\n| GTE-en-MLM  | 42.5         | **36.4**    | **48.9**  | 50.7          | 71.3          | 87.6 | 40.5 | 66.9 |\n| ModernBERT  | **44.0**    | 34.3         | 48.6         | **52.4**     | **80.4**     | 90.4 |**59.5** |**83.9**|\n\n*Table 1: Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDRID refers to in-domain (fine-tuned on the training set) evaluation, and MLDR_OOD to out-of-domain.*\n\nModernBERTâ€™s strong results, coupled with its efficient runtime on long-context inputs, demonstrate that encoder-only models can be significantly improved through modern architectural choices and extensive pretraining on diversified data sources.\n\n\n## Limitations\n\nModernBERTâ€™s training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them.\n\n## Training\n\n- Architecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.\n- Sequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.\n- Data: 2 trillion tokens of English text and code.\n- Optimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.\n- Hardware: Trained on 8x H100 GPUs.\n\nSee the paper for more details.\n\n## License\n\nWe release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license.\n\n## Citation\n\nIf you use ModernBERT in your work, please cite:\n\n```\n@misc{modernbert,\n      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, \n      author={Benjamin Warner and Antoine Chaffin and Benjamin ClaviÃ© and Orion Weller and Oskar HallstrÃ¶m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n      year={2024},\n      eprint={2412.13663},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13663}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"fill-mask\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":149655232,\"storage_bytes\":5084307056,\"files_count\":16,\"spaces_count\":64,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"ModernBertForMaskedLM\"],\"model_type\":\"modernbert\",\"tokenizer_config\":{\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\",\"pad_token\":\"[PAD]\",\"sep_token\":\"[SEP]\",\"unk_token\":\"[UNK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2412.13663\",\"source_url\":\"https://arxiv.org/abs/2412.13663\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.8,
    "content_hash": "4c2898c4d434bfa01bb2712550a1cda9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/answerdotai/ModernBERT-base\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:black-forest-labs:flux.1-fill-dev",
    "name": "FLUX.1-Fill-dev",
    "author": "black-forest-labs",
    "description": "",
    "tags": [
      "diffusers",
      "safetensors",
      "image-generation",
      "flux",
      "diffusion-single-file",
      "en",
      "license:other",
      "diffusers:fluxfillpipeline",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 963,
    "downloads": 151689,
    "source": "huggingface",
    "source_url": "https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":58052037720,\"files_count\":28,\"spaces_count\":80,\"gated\":\"auto\",\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"FluxFillPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.8,
    "content_hash": "42e7a7f191345963c4f74a32f6b7fdb5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:srpo",
    "name": "SRPO",
    "author": "tencent",
    "description": "--- library_name: diffusers license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline_tag: text-to-image --- <div align=â€œcenterâ€ style=â€œfont-family: charter;â€> <h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1> <div align=\"center\"> <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a> &nbsp; <a href='h...",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "arxiv:2509.06942",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 961,
    "downloads": 3290,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/SRPO",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: diffusers\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\npipeline_tag: text-to-image\n---\n\n<div align=â€œcenterâ€ style=â€œfont-family: charter;â€>\n<h1 align=\"center\">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align=\"center\">\n  <a href='https://arxiv.org/abs/2509.06942'><img src='https://img.shields.io/badge/ArXiv-red?logo=arxiv'></a>  &nbsp;\n  <a href='https://github.com/Tencent-Hunyuan/SRPO'><img src='https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee'></a> &nbsp; \n  <a href='https://tencent.github.io/srpo-project-page/'><img src='https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue'></a> &nbsp;\n</div>\n<div align=\"center\">\n  Xiangwei Shen<sup>1,2*</sup>,\n  <a href=\"https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN\" target=\"_blank\"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href=\"https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ\" target=\"_blank\"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href=\"https://shiyi-zh0408.github.io/\" target=\"_blank\"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href=\"https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en\" target=\"_blank\"><b>Chunyu Wang</b></a><sup>1</sup>,\n  <a href=\"https://openreview.net/profile?id=%7EQinglin_Lu2\" target=\"_blank\"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href=\"https://andytang15.github.io\" target=\"_blank\"><b>Yansong Tang</b></a><sup>3,âœ</sup>\n</div>\n<div align=\"center\">\n  <sup>1</sup>Hunyuan, Tencentâ€ƒ\n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhenâ€ƒ\n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua Universityâ€ƒ\n  <br>\n  <sup>*</sup>Equal contributionâ€ƒ\n  <sup>âœ</sup>Corresponding author\n</div>\n\n\n\n## Abstract\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\n\n## Acknowledgement\n\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\n\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\n\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\n\nâš ï¸ Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\n\n\n### Checkpoints\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\n## ğŸ”‘ Inference\n\n### Using ComfyUI\n\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\n\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\n\nTip: The workflow JSON info was added to the image file.\n\n![Example](comfyui/SRPO-workflow.png)\n\n### Quick start\n```bash\nfrom diffusers import FluxPipeline\nfrom safetensors.torch import load_file\n\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\npipe = FluxPipeline.from_pretrained('./data/flux',\n        torch_dtype=torch.bfloat16,\n        use_safetensors=True\n    ).to(\"cuda\")\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\npipe.transformer.load_state_dict(state_dict)\nimage = pipe(\n    prompt,\n    guidance_scale=3.5,\n    height=1024,\n    width=1024,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=generator\n).images[0]\n```\n### License\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\n## Citation\nIf you use SRPO for your research, please cite our paper:\n\n```bibtex\n@misc{shen2025directlyaligningdiffusiontrajectory,\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\n      year={2025},\n      eprint={2509.06942},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2509.06942}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":47610166557,\"files_count\":6,\"spaces_count\":8,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:SRPO\",\"source_url\":\"https://github.com/Tencent-Hunyuan/SRPO\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:SRPO'><img\",\"source_url\":\"https://github.com/Tencent-Hunyuan/SRPO'><img\"},{\"type\":\"has_code\",\"target_id\":\"github:tgxs002:HPSv2\",\"source_url\":\"https://github.com/tgxs002/HPSv2\"},{\"type\":\"has_code\",\"target_id\":\"github:comfyanonymous:ComfyUI\",\"source_url\":\"https://github.com/comfyanonymous/ComfyUI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2509.06942\",\"source_url\":\"https://arxiv.org/abs/2509.06942\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 64.8,
    "content_hash": "fcb72bddaef09fede8129637059a5bca",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/SRPO\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stabilityai:stable-video-diffusion-img2vid-xt-1-1",
    "name": "stable-video-diffusion-img2vid-xt-1-1",
    "author": "stabilityai",
    "description": "",
    "tags": [
      "diffusers",
      "safetensors",
      "image-to-video",
      "license:other",
      "diffusers:stablevideodiffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "image-to-video",
    "likes": 960,
    "downloads": 27021,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":18313832368,\"files_count\":17,\"spaces_count\":36,\"gated\":\"auto\",\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableVideoDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.8,
    "content_hash": "cb5de6b7948873cab9b7839af54318f9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:briaai:rmbg-2.0",
    "name": "RMBG-2.0",
    "author": "briaai",
    "description": "",
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "image-segmentation",
      "remove background",
      "background",
      "background-removal",
      "pytorch",
      "vision",
      "legal liability",
      "transformers.js",
      "custom_code",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "image-segmentation",
    "likes": 960,
    "downloads": 265808,
    "source": "huggingface",
    "source_url": "https://huggingface.co/briaai/RMBG-2.0",
    "image_url": "https://huggingface.co/briaai/RMBG-2.0/resolve/main/diagram1.png",
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-segmentation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":220700242,\"storage_bytes\":5896565704,\"files_count\":19,\"spaces_count\":98,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"BiRefNet\"],\"auto_map\":{\"AutoConfig\":\"BiRefNet_config.BiRefNetConfig\",\"AutoModelForImageSegmentation\":\"birefnet.BiRefNet\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 59.8,
    "content_hash": "cce1130538b8c04b0ab88304843b3780",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/briaai/RMBG-2.0/resolve/main/diagram1.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/briaai/RMBG-2.0\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nitrosocke:mo-di-diffusion",
    "name": "mo-di-diffusion",
    "author": "nitrosocke",
    "description": "--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image --- **Mo Di Diffusion** This is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio. Use the tokens **_modern disney style_** in your prompts for the effect. **If you enjoy my work, please consider supporting me** **Videogame Characters rendered with the model:** !Videogame Samples **Animal Characters rendered with the model:** !Animal Samples **Cars and Landscapes rendered...",
    "tags": [
      "diffusers",
      "stable-diffusion",
      "text-to-image",
      "license:creativeml-openrail-m",
      "endpoints_compatible",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 957,
    "downloads": 1229,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nitrosocke/mo-di-diffusion",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\n---\n**Mo Di Diffusion**\n\nThis is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio.\nUse the tokens **_modern disney style_** in your prompts for the effect.\n\n**If you enjoy my work, please consider supporting me** \n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n**Videogame Characters rendered with the model:**\n![Videogame Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-01s.jpg)\n**Animal Characters rendered with the model:**\n![Animal Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-02s.jpg)\n**Cars and Landscapes rendered with the model:**\n![Misc. Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-03s.jpg)\n#### Prompt and settings for Lara Croft:\n**modern disney lara croft**\n_Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 3940025417, Size: 512x768_\n\n#### Prompt and settings for the Lion:\n**modern disney (baby lion) Negative prompt: person human**\n_Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 1355059992, Size: 512x512_\n\nThis model was trained using the diffusers based dreambooth training by ShivamShrirao using prior-preservation loss and the _train-text-encoder_ flag in 9.000 steps.\n\n### ğŸ§¨ Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"nitrosocke/mo-di-diffusion\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a magical princess with golden hair, modern disney style\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"./magical_princess.png\")\n```\n\n# Gradio & Colab\n\nWe also support a [Gradio](https://github.com/gradio-app/gradio) Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j5YvfMZoGdDGdj3O3xRU1m4ujKYsElZO?usp=sharing)\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":31105706419,\"files_count\":21,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 64.8,
    "content_hash": "3f2582c65668ac75a0401d9a0160560c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nitrosocke/mo-di-diffusion\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stabilityai:control-lora",
    "name": "control-lora",
    "author": "stabilityai",
    "description": "--- tags: - text-to-image - stable-diffusion license: other language: - en --- By adding low-rank parameter efficient fine tuning to ControlNet, we introduce ***Control-LoRAs***. This approach offers a more efficient and compact method to bring model control to a wider variety of consumer GPUs. For each model below, you'll find: - *Rank 256* files (reducing the original ControlNet models down to Control-LoRA models) and experimental - *Rank 128* files (reducing to model down to ) Each Control...",
    "tags": [
      "text-to-image",
      "stable-diffusion",
      "en",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 953,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stabilityai/control-lora",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- text-to-image\n- stable-diffusion\nlicense: other\nlanguage:\n- en\n---\n\n# Control-LoRA Model Card\n\n\n## Introduction\nBy adding low-rank parameter efficient fine tuning to ControlNet, we introduce ***Control-LoRAs***. This approach offers a more efficient and compact method to bring model control to a wider variety of consumer GPUs.\n\nFor each model below, you'll find:\n\n- *Rank 256* files (reducing the original `4.7GB` ControlNet models down to `~738MB` Control-LoRA models) and experimental\n- *Rank 128* files (reducing to model down to `~377MB`)\n\nEach Control-LoRA has been trained on a diverse range of image concepts and aspect ratios.\n\n### MiDaS and ClipDrop Depth\n![canny](samples/depth-sample.jpeg)\n\nThis Control-LoRA utilizes a grayscale depth map for guided generation.\n\nDepth estimation is an image processing technique that determines the distance of objects in a scene, providing a depth map that highlights variations in proximity.\n\nThe model was trained on the depth results of `MiDaS dpt_beit_large_512`.\n\nIt was further finetuned on the `Portrait Depth Estimation` model available in the [ClipDrop API by Stability AI](https://clipdrop.co/apis/docs/portrait-depth-estimation).\n\n### Canny Edge\n![canny](samples/canny-sample.jpeg)\nCanny Edge Detection is an image processing technique that identifies abrupt changes in intensity to highlight edges in an image.\n\nThis Control-LoRA uses the edges from an image to generate the final image.\n\n### Photograph and Sketch Colorizer\n![photograph colorizer](samples/colorizer-sample.jpeg)\nThese two Control-LoRAs can be used to colorize images.\n\n*Recolor* is designed to colorize black and white photographs.\n\n*Sketch* is designed to color in drawings input as a white-on-black image (either hand-drawn, or created with a `pidi` edge model).\n\n### Revision\n![photograph colorizer](samples/revision-sample.jpeg)\nRevision is a novel approach of using images to prompt SDXL.\n\nIt uses pooled CLIP embeddings to produce images conceptually similar to the input. It can be used either in addition, or to replace text prompts.\n\nRevision also includes a blending function for combining multiple image or text concepts, as either positive or negative prompts.\n\n\n## Inference\n\nControl-LoRAs have been implemented into [ComfyUI](https://github.com/comfyanonymous/ComfyUI) and [StableSwarmUI](https://github.com/Stability-AI/StableSwarmUI)\n\nBasic ComfyUI workflows (using the base model only) are available in this HF repo. Custom nodes from Stability are [available here](https://github.com/Stability-AI/stability-ComfyUI-nodes).\n\n**Recolor example on ComfyUI:** ![comfyui recolor](samples/comfyui-recolor-example.jpeg)\n\n**Canny edge on StableSwarmUI:** ![swarmui recolor](samples/swarmui-canny-example.jpeg)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":8373480116,\"files_count\":29,\"spaces_count\":3,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:comfyanonymous:ComfyUI\",\"source_url\":\"https://github.com/comfyanonymous/ComfyUI\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:StableSwarmUI\",\"source_url\":\"https://github.com/Stability-AI/StableSwarmUI\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:stability-ComfyUI-nodes\",\"source_url\":\"https://github.com/Stability-AI/stability-ComfyUI-nodes\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 64.8,
    "content_hash": "53f73ae37bbf6d12d945d41e522035b4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stabilityai/control-lora\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mistralai:mistral-small-24b-instruct-2501",
    "name": "Mistral-Small-24B-Instruct-2501",
    "author": "mistralai",
    "description": "--- library_name: vllm language: - en - fr - de - es - it - pt - zh - ja - ru - ko license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-24B-Base-2501 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>. tags: - vllm --- Mistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving ...",
    "tags": [
      "vllm",
      "safetensors",
      "mistral",
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ja",
      "ru",
      "ko",
      "base_model:mistralai/mistral-small-24b-base-2501",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 949,
    "downloads": 475869,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- zh\n- ja\n- ru\n- ko\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-24B-Base-2501\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- vllm\n---\n\n# Model Card for Mistral-Small-24B-Instruct-2501\n\nMistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!  \nThis model is an instruction-fine-tuned version of the base model: [Mistral-Small-24B-Base-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501).\n\nMistral Small can be deployed locally and is exceptionally \"knowledge-dense\", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.  \nPerfect for:\n- Fast response conversational agents.\n- Low latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\n\nThis release demonstrates our commitment to open source, serving as a strong base model. \n\nLearn more about Mistral Small in our [blog post](https://mistral.ai/news/mistral-small-3/).\n\nModel developper: Mistral AI Team\n\n## Key Features\n- **Multilingual:** Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 32k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark results\n\n\n### Human evaluated benchmarks\n\n| Category | Gemma-2-27B | Qwen-2.5-32B | Llama-3.3-70B | Gpt4o-mini |\n|----------|-------------|--------------|---------------|------------|\n| Mistral is better | 0.536 | 0.496 | 0.192 | 0.200 |\n| Mistral is slightly better | 0.196 | 0.184 | 0.164 | 0.204 |\n| Ties | 0.052 | 0.060 | 0.236 | 0.160 |\n| Other is slightly better | 0.060 | 0.088 | 0.112 | 0.124 |\n| Other is better | 0.156 | 0.172 | 0.296 | 0.312 |\n\n**Note**:\n\n- We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts.\n- Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model.\n- We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n### Publicly accesible benchmarks\n\n**Reasoning & Knowledge**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mmlu_pro_5shot_cot_instruct | 0.663 | 0.536 | 0.666 | 0.683 | 0.617 |\n| gpqa_main_cot_5shot_instruct | 0.453 | 0.344 | 0.531 | 0.404 | 0.377 |\n\n**Math & Coding**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| humaneval_instruct_pass@1 | 0.848 | 0.732 | 0.854 | 0.909 | 0.890 |\n| math_instruct | 0.706 | 0.535 | 0.743 | 0.819 | 0.761 |\n\n**Instruction following**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mtbench_dev | 8.35 | 7.86 | 7.96 | 8.26 | 8.33 |\n| wildbench | 52.27 | 48.21 | 50.04 | 52.73 | 56.13 |\n| arena_hard | 0.873 | 0.788 | 0.840 | 0.860 | 0.897 |\n| ifeval | 0.829 | 0.8065 | 0.8835 | 0.8401 | 0.8499 |\n\n**Note**:\n\n- Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance\n([Qwen2.5-32B-Instruct](https://qwenlm.github.io/blog/qwen2.5/), [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Gemma-2-27B-IT](https://huggingface.co/google/gemma-2-27b-it)). \n- Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n\n### Basic Instruct Template (V7-Tekken)\n\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm`](https://github.com/vllm-project/vllm): See [here](#vllm)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n\n### vLLM\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**Note 1**: We recommond using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n\n```\nsystem_prompt = \"\"\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\"What are some good restaurants around me?\\\" => \\\"Where are you?\\\" or \\\"When is the next flight to Tokyo\\\" => \\\"Where do you travel from?\\\")\"\"\"\n```\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.6.4`](https://github.com/vllm-project/vllm/releases/tag/v0.6.4):\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have [`mistral_common >= 1.5.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.2) installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Mistral-Small-24B-Instruct-2501 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\n```\n\n**Note:** Running Mistral-Small-24B-Instruct-2501 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-server>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. Ã€ plus tard\n# 2. Ã€ plus\n# 3. Salut\n# 4. Ã€ toute\n# 5. Bisous\n#\n# ```\n#  /\\_/\\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Function calling\n\nMistral-Small-24-Instruct-2501 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Example</summary>\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                    },\n                    \"state\": {\n                        \"type\": \"string\",\n                        \"description\": \"The state abbreviation, e.g. 'CA' for California\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit for temperature\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"city\", \"state\", \"unit\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"id\": \"bbc5b7ede\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"rewrite\",\n                    \"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n                },\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n        \"tool_call_id\": \"bbc5b7ede\",\n        \"name\": \"rewrite\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Can you tell me what the temperature will be in Dallas, in Fahrenheit?\",\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages, \"tools\": tools}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nimport ipdb; ipdb.set_trace()\nprint(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"])\n# [{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}'}}]\n```\n\n</details>\n\n#### Offline\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\n\nSYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n\nuser_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": SYSTEM_PROMPT\n    },\n    {\n        \"role\": \"user\",\n        \"content\": user_prompt\n    },\n]\n\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", tensor_parallel_size=8)\n\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. Ã€ plus tard\n# 2. Ã€ plus\n# 3. Salut\n# 4. Ã€ toute\n# 5. Bisous\n#\n# ```\n#  /\\_/\\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Transformers\n\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-Small-24B-Instruct-2501\", max_new_tokens=256, torch_dtype=torch.bfloat16)\nchatbot(messages)\n```\n\n\n### Ollama\n\n[Ollama](https://github.com/ollama/ollama) can run this model locally on MacOS, Windows and Linux. \n\n```\nollama run mistral-small\n```\n\n4-bit quantization (aliased to default): \n```\nollama run mistral-small:24b-instruct-2501-q4_K_M\n```\n\n8-bit quantization:\n```\nollama run mistral-small:24b-instruct-2501-q8_0\n```\n\nFP16:\n```\nollama run mistral-small:24b-instruct-2501-fp16\n```",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"vllm\",\"framework\":\"vllm\",\"params\":23572403200,\"storage_bytes\":94321574156,\"files_count\":22,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\",\"eos_token\":\"</s>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:ollama:ollama\",\"source_url\":\"https://github.com/ollama/ollama\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.8,
    "content_hash": "152f0ab57b10a0a1b8f1b87dc442fbf5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:black-forest-labs:flux.2-dev",
    "name": "FLUX.2-dev",
    "author": "black-forest-labs",
    "description": "",
    "tags": [
      "diffusers",
      "safetensors",
      "image-generation",
      "image-editing",
      "flux",
      "diffusion-single-file",
      "image-to-image",
      "en",
      "license:other",
      "diffusers:flux2pipeline",
      "region:us"
    ],
    "pipeline_tag": "image-to-image",
    "likes": 949,
    "downloads": 211215,
    "source": "huggingface",
    "source_url": "https://huggingface.co/black-forest-labs/FLUX.2-dev",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":177640024576,\"files_count\":39,\"spaces_count\":56,\"gated\":\"auto\",\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"Flux2Pipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.8,
    "content_hash": "0e632496351c04584f5042bb10115cf3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/black-forest-labs/FLUX.2-dev\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-3.1-405b",
    "name": "Llama-3.1-405B",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "license:llama3.1",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 948,
    "downloads": 216634,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-3.1-405B",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":405853388800,\"storage_bytes\":4667909255120,\"files_count\":583,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"eos_token\":\"<|end_of_text|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.05149\",\"source_url\":\"https://arxiv.org/abs/2204.05149\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.1",
    "compliance_status": "approved",
    "quality_score": 39.8,
    "content_hash": "78001d53069f399380b0c895673a1d74",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-3.1-405B\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:openassistant:oasst-sft-6-llama-30b-xor",
    "name": "oasst-sft-6-llama-30b-xor",
    "author": "OpenAssistant",
    "description": "--- license: other --- Due to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models. Thanks to Mick for writing the script which enables this process Note: This process applies to model. The same process can be applied to other models in future, but the checksums will be different.. **This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does...",
    "tags": [
      "arxiv:2304.07327",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 943,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\n---\n\n# OpenAssistant LLaMa 30B SFT 6\n\nDue to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models.\n\nThanks to Mick for writing the `xor_codec.py` script which enables this process\n\n## The Process\n\nNote: This process applies to `oasst-sft-6-llama-30b` model. The same process can be applied to other models in future, but the checksums will be different..\n\n**This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does not work on Windows. We recommend using WSL if you only have a Windows machine.**\n\nTo use OpenAssistant LLaMA-Based Models, you should have a copy of the original LLaMA model weights and add them to a `llama` subdirectory here. If you cannot obtain the original LLaMA, see the note in italic below for a possible alternative.\n\nEnsure your LLaMA 30B checkpoint matches the correct md5sums:\n\n```\nf856e9d99c30855d6ead4d00cc3a5573  consolidated.00.pth\nd9dbfbea61309dc1e087f5081e98331a  consolidated.01.pth\n2b2bed47912ceb828c0a37aac4b99073  consolidated.02.pth\nea0405cdb5bc638fee12de614f729ebc  consolidated.03.pth\n4babdbd05b8923226a9e9622492054b6  params.json\n```\n\n*If you do not have a copy of the original LLaMA weights and cannot obtain one, you may still be able to complete this process. Some users have reported that [this model](https://huggingface.co/elinas/llama-30b-hf-transformers-4.29) can be used as a base for the XOR conversion. This will also allow you to skip to Step 7. However, we only support conversion starting from LLaMA original checkpoint and cannot provide support if you experience issues with this alternative approach.*\n\n**Important: Follow these exact steps to convert your original LLaMA checkpoint to a HuggingFace Transformers-compatible format. If you use the wrong versions of any dependency, you risk ending up with weights which are not compatible with the XOR files.**\n\n1. Create a clean Python **3.10** virtual environment & activate it:\n\n```\npython3.10 -m venv xor_venv\nsource xor_venv/bin/activate\n```\n\n2. Clone transformers repo and switch to tested version:\n\n```\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\npip install .\n```\n\n3. Install **exactly** these dependency versions:\n\n```\npip install torch==1.13.1 accelerate==0.18.0 sentencepiece==0.1.98 protobuf==3.20.1\n```\n\n4. Check `pip freeze` output:\n\n```\naccelerate==0.18.0\ncertifi==2022.12.7\ncharset-normalizer==3.1.0\nfilelock==3.12.0\nhuggingface-hub==0.13.4\nidna==3.4\nnumpy==1.24.2\nnvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-nvrtc-cu11==11.7.99\nnvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\npackaging==23.1\nprotobuf==3.20.1\npsutil==5.9.5\nPyYAML==6.0\nregex==2023.3.23\nrequests==2.28.2\nsentencepiece==0.1.98\ntokenizers==0.13.3\ntorch==1.13.1\ntqdm==4.65.0\ntransformers @ file:///mnt/data/koepf/transformers\ntyping_extensions==4.5.0\nurllib3==1.26.15\n```\n\n5. While in `transformers` repo root, run HF LLaMA conversion script:\n\n```\npython src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir <input_path_llama_base>  --output_dir <output_path_llama30b_hf> --model_size 30B\n```\n\n6. Run `find . -type f -exec md5sum \"{}\" +` in the conversion target directory (`output_dir`). This should produce exactly the following checksums if your files are correct:\n\n```\n462a2d07f65776f27c0facfa2affb9f9  ./pytorch_model-00007-of-00007.bin\ne1dc8c48a65279fb1fbccff14562e6a3  ./pytorch_model-00003-of-00007.bin\n9cffb1aeba11b16da84b56abb773d099  ./pytorch_model-00001-of-00007.bin\naee09e21813368c49baaece120125ae3  ./generation_config.json\n92754d6c6f291819ffc3dfcaf470f541  ./pytorch_model-00005-of-00007.bin\n3eddc6fc02c0172d38727e5826181adb  ./pytorch_model-00004-of-00007.bin\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\n99762d59efa6b96599e863893cf2da02  ./pytorch_model-00006-of-00007.bin\n598538f18fed1877b41f77de034c0c8a  ./config.json\nfdb311c39b8659a5d5c1991339bafc09  ./tokenizer.json\nfecfda4fba7bfd911e187a85db5fa2ef  ./pytorch_model.bin.index.json\nedd1a5897748864768b1fab645b31491  ./tokenizer_config.json\n6b2e0a735969660e720c27061ef3f3d3  ./special_tokens_map.json\n5cfcb78b908ffa02e681cce69dbe4303  ./pytorch_model-00002-of-00007.bin\n```\n\n**Important: You should now have the correct LLaMA weights and be ready to apply the XORs. If the checksums above do not match yours, there is a problem.**\n\n7. Once you have LLaMA weights in the correct format, you can apply the XOR decoding:\n\n```\npython xor_codec.py oasst-sft-6-llama-30b/ oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/ llama30b_hf/\n```\n\nYou should **expect to see one warning message** during execution:\n\n`Exception when processing 'added_tokens.json'`\n\nThis is normal. **If similar messages appear for other files, something has gone wrong**.\n\n8. Now run `find . -type f -exec md5sum \"{}\" +` in the output directory (here `oasst-sft-6-llama-30b`). You should get a file with exactly these checksums:\n\n```\n970e99665d66ba3fad6fdf9b4910acc5  ./pytorch_model-00007-of-00007.bin\n659fcb7598dcd22e7d008189ecb2bb42  ./pytorch_model-00003-of-00007.bin\nff6e4cf43ddf02fb5d3960f850af1220  ./pytorch_model-00001-of-00007.bin\n27b0dc092f99aa2efaf467b2d8026c3f  ./added_tokens.json\n2917a1cafb895cf57e746cfd7696bfe5  ./generation_config.json\n740c324ae65b1ec25976643cda79e479  ./pytorch_model-00005-of-00007.bin\nf7aefb4c63be2ac512fd905b45295235  ./pytorch_model-00004-of-00007.bin\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\n369df2f0e38bda0d9629a12a77c10dfc  ./pytorch_model-00006-of-00007.bin\ncc9dbf56b68b68a585cc7367696e06a7  ./config.json\n76d47e4f51a8df1d703c6f594981fcab  ./pytorch_model.bin.index.json\nfd9452959d711be29ccf04a97598e8d1  ./tokenizer_config.json\n785905630a0fe583122a8446a5abe287  ./special_tokens_map.json\nae48c4c68e4e171d502dd0896aa19a84  ./pytorch_model-00002-of-00007.bin\n```\n\nIf so you have successfully decoded the weights and should be able to use the model with HuggingFace Transformers. **If your checksums do not match those above, there is a problem.**\n\n### Configuration\n\n```\nllama-30b-sft-6:\n  dtype: fp16\n  log_dir: \"llama_log_30b\"\n  learning_rate: 1e-5\n  model_name: /home/ubuntu/Open-Assistant/model/model_training/.saved/llama-30b-super-pretrain/checkpoint-3500\n  output_dir: llama_model_30b\n  deepspeed_config: configs/zero3_config_sft.json\n  weight_decay: 0.0\n  residual_dropout: 0.0\n  max_length: 2048\n  use_flash_attention: true\n  warmup_steps: 20\n  gradient_checkpointing: true\n  gradient_accumulation_steps: 16\n  per_device_train_batch_size: 2\n  per_device_eval_batch_size: 3\n  eval_steps: 101\n  save_steps: 292\n  num_train_epochs: 8\n  save_total_limit: 3\n  use_custom_sampler: true\n  sort_by_length: false\n  save_strategy: steps\n  datasets:\n    - oasst_export:\n        lang: \"bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\"\n        input_file_path: 2023-04-12_oasst_release_ready_synth.jsonl.gz\n        val_split: 0.05\n    - vicuna:\n        val_split: 0.05\n        max_val_set: 800\n        fraction: 0.8\n    - dolly15k:\n        val_split: 0.05\n        max_val_set: 300\n    - grade_school_math_instructions:\n        val_split: 0.05\n    - code_alpaca:\n        val_split: 0.05\n        max_val_set: 250\n```\n\n- **OASST dataset paper:** https://arxiv.org/abs/2304.07327",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":65059086164,\"files_count\":17,\"spaces_count\":20,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers.git\",\"source_url\":\"https://github.com/huggingface/transformers.git\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.07327\",\"source_url\":\"https://arxiv.org/abs/2304.07327\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 64.7,
    "content_hash": "6149fc4070a7fe3d8c35fda6fcc69ebf",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:facebook:sam3",
    "name": "sam3",
    "author": "facebook",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "sam3_video",
      "feature-extraction",
      "sam3",
      "mask-generation",
      "en",
      "license:other",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "mask-generation",
    "likes": 940,
    "downloads": 486833,
    "source": "huggingface",
    "source_url": "https://huggingface.co/facebook/sam3",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"mask-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":859922360,\"storage_bytes\":10329938097,\"files_count\":12,\"spaces_count\":20,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Sam3VideoModel\"],\"model_type\":\"sam3_video\",\"tokenizer_config\":{\"bos_token\":\"<|startoftext|>\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.7,
    "content_hash": "23f794339343543a0d9ffcfec5aeccb9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/facebook/sam3\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen2.5-7b-instruct",
    "name": "Qwen2.5-7B-Instruct",
    "author": "Qwen",
    "description": "--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-7B tags: - chat library_name: transformers --- <a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen2.5 is the latest series of Qwen large la...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "chat",
      "conversational",
      "en",
      "arxiv:2309.00071",
      "arxiv:2407.10671",
      "base_model:qwen/qwen2.5-7b",
      "base_model:finetune:qwen/qwen2.5-7b",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 937,
    "downloads": 7218575,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-7B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [ğŸ“‘ blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7615616512,\"storage_bytes\":15231271888,\"files_count\":14,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2407.10671\",\"source_url\":\"https://arxiv.org/abs/2407.10671\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.7,
    "content_hash": "beb786d83a3a7728edad444d5ad99d6b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-r1-zero",
    "name": "DeepSeek-R1-Zero",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/d...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v3",
      "text-generation",
      "conversational",
      "custom_code",
      "arxiv:2501.12948",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 937,
    "downloads": 5155,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero/resolve/main/figures/benchmark.jpg",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>ğŸ‘ï¸</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":684531386000,\"storage_bytes\":688586727753,\"files_count\":174,\"spaces_count\":60,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV3ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_deepseek.DeepseekV3Config\",\"AutoModel\":\"modeling_deepseek.DeepseekV3Model\",\"AutoModelForCausalLM\":\"modeling_deepseek.DeepseekV3ForCausalLM\"},\"model_type\":\"deepseek_v3\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\\\n\\\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<ï½œAssistantï½œ><ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- else %}{{'<ï½œAssistantï½œ>' + message['content'] + '<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- endfor %}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>' + message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ï½œAssistantï½œ>' + content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ï½œAssistantï½œ><think>\\\\n'}}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.12948\",\"source_url\":\"https://arxiv.org/abs/2501.12948\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 99.7,
    "content_hash": "6d6179a928a64505caad38d34a3913ed",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero/resolve/main/figures/benchmark.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stable-diffusion-v1-5:stable-diffusion-v1-5",
    "name": "stable-diffusion-v1-5",
    "author": "stable-diffusion-v1-5",
    "description": "--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image inference: true --- Modifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span> Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at ğŸ¤—'s Stable Diffusion blog. The ...",
    "tags": [
      "diffusers",
      "safetensors",
      "stable-diffusion",
      "stable-diffusion-diffusers",
      "text-to-image",
      "arxiv:2207.12598",
      "arxiv:2112.10752",
      "arxiv:2103.00020",
      "arxiv:2205.11487",
      "arxiv:1910.09700",
      "license:creativeml-openrail-m",
      "endpoints_compatible",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 934,
    "downloads": 2114419,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\ninference: true\n---\n\n# Stable Diffusion v1-5 Model Card\n\n### âš ï¸ This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ğŸ¤—'s Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [ğŸ§¨Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n\n### Bias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n### Safety Module\n\nThe intended use of this model is with the [Safety Checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) in Diffusers. \nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the `CLIPTextModel` *after generation* of the images. \nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1-5 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nCurrently six Stable Diffusion checkpoints are provided, which were trained as follows.\n- [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1): 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2): Resumed from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3): Resumed from `stable-diffusion-v1-2` - 195,000 steps at resolution `512x512` on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2` - 225,000 steps at resolution `512x512` on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-5`](https://huggingface.co/sd-legacy/stable-diffusion-v1-5) Resumed from `stable-diffusion-v1-2` - 595,000 steps at resolution `512x512` on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-inpainting`](https://huggingface.co/sd-legacy/stable-diffusion-inpainting) Resumed from `stable-diffusion-v1-5` - then 440,000 steps of inpainting training at resolution 512x512 on â€œlaion-aesthetics v2 5+â€ and 10% dropping of the text-conditioning. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PNDM/PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-1-to-v1-5.png)\n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":49898615889,\"files_count\":36,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:runwayml:stable-diffusion\",\"source_url\":\"https://github.com/runwayml/stable-diffusion\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion\"},{\"type\":\"has_code\",\"target_id\":\"github:runwayml:stable-diffusion\",\"source_url\":\"https://github.com/runwayml/stable-diffusion\"},{\"type\":\"has_code\",\"target_id\":\"github:comfyanonymous:ComfyUI\\\">ComfyUI<\",\"source_url\":\"https://github.com/comfyanonymous/ComfyUI\\\">ComfyUI<\"},{\"type\":\"has_code\",\"target_id\":\"github:AUTOMATIC1111:stable-diffusion-webui\\\">AUTOMATIC1111<\",\"source_url\":\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\\\">AUTOMATIC1111<\"},{\"type\":\"has_code\",\"target_id\":\"github:vladmandic:automatic\\\">SD.Next<\",\"source_url\":\"https://github.com/vladmandic/automatic\\\">SD.Next<\"},{\"type\":\"has_code\",\"target_id\":\"github:invoke-ai:InvokeAI\\\">InvokeAI<\",\"source_url\":\"https://github.com/invoke-ai/InvokeAI\\\">InvokeAI<\"},{\"type\":\"has_code\",\"target_id\":\"github:CompVis:stable-diffusion\",\"source_url\":\"https://github.com/CompVis/stable-diffusion\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:christophschuhmann:improved-aesthetic-predictor\",\"source_url\":\"https://github.com/christophschuhmann/improved-aesthetic-predictor\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2207.12598\",\"source_url\":\"https://arxiv.org/abs/2207.12598\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2112.10752\",\"source_url\":\"https://arxiv.org/abs/2112.10752\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.00020\",\"source_url\":\"https://arxiv.org/abs/2103.00020\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.11487\",\"source_url\":\"https://arxiv.org/abs/2205.11487\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.09700\",\"source_url\":\"https://arxiv.org/abs/1910.09700\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 79.7,
    "content_hash": "a1d2a53990eac9257d8dc73b1875bcbd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:guoyww:animatediff",
    "name": "animatediff",
    "author": "guoyww",
    "description": "--- license: apache-2.0 --- This model repo is for AnimateDiff.",
    "tags": [
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 932,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/guoyww/animatediff",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\n---\nThis model repo is for [AnimateDiff](https://github.com/guoyww/animatediff/).",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":32554557069,\"files_count\":18,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:guoyww:animatediff\",\"source_url\":\"https://github.com/guoyww/animatediff\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 39.7,
    "content_hash": "4797596fa8754451ddf56149673c3a49",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/guoyww/animatediff\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:kijai:flux-fp8",
    "name": "flux-fp8",
    "author": "Kijai",
    "description": "--- language: - en license: other viewer: false tags: - diffusion-single-file - comfyui --- and weights of: https://huggingface.co/black-forest-labs/FLUX.1-dev is , original upload name kept for backwards compatibility. weights of: https://huggingface.co/black-forest-labs/FLUX.1-schnell https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro flux1-dev-fp8.safetensors, flux_shakker_labs_union_pro-fp8_e4m3fn and it's variants falls under the Non-Commercial License. flux1-schnell-fp8...",
    "tags": [
      "diffusion-single-file",
      "comfyui",
      "en",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 932,
    "downloads": 56031,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Kijai/flux-fp8",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n  - en\nlicense: other\nviewer: false\ntags:\n  - diffusion-single-file\n  - comfyui\n---\n`float8_e4m3fn` and `float8_e5m2` weights of:\n\nhttps://huggingface.co/black-forest-labs/FLUX.1-dev\n\n`flux1-dev-fp8.safetensors` is `float8_e4m3fn`, original upload name kept for backwards compatibility.\n\n`float8_e4m3fn` weights of:\n\nhttps://huggingface.co/black-forest-labs/FLUX.1-schnell\n\nhttps://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro\n# License\nflux1-dev-fp8.safetensors, flux_shakker_labs_union_pro-fp8_e4m3fn and it's variants falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).\n\nflux1-schnell-fp8.safetensors falls under the [Apache-2.0 License](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md).\n",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"diffusion-single-file\",\"framework\":\"diffusion-single-file\",\"params\":null,\"storage_bytes\":74868593126,\"files_count\":8,\"spaces_count\":3,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 49.7,
    "content_hash": "f41ee74e670c1b1b017cb79142915f70",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Kijai/flux-fp8\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:microsoft:phi-3.5-mini-instruct",
    "name": "Phi-3.5-mini-instruct",
    "author": "microsoft",
    "description": "--- license: mit license_link: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? library_name: transformers --- ğŸ‰**Phi-4**: [multimodal-instruct | onnx]; [mini-instruct | onnx] Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic d...",
    "tags": [
      "transformers",
      "safetensors",
      "phi3",
      "text-generation",
      "nlp",
      "code",
      "conversational",
      "custom_code",
      "multilingual",
      "arxiv:2404.14219",
      "arxiv:2407.13833",
      "arxiv:2403.06412",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 933,
    "downloads": 387398,
    "source": "huggingface",
    "source_url": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\nlibrary_name: transformers\n---\nğŸ‰**Phi-4**: [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n## Model Summary\n\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nğŸ¡ [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\nğŸ“° [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\nğŸ“– [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\nğŸ‘©â€ğŸ³ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\nğŸ–¥ï¸ [Try It](https://aka.ms/try-phi3.5mini) <br>\n\n**Phi-3.5**: [[mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-3.5-mini-instruct-onnx)]; [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct); [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Release Notes \n\nThis is an update over the June 2024 instruction-tuned Phi-3 Mini release based on valuable user feedback. The model used additional post-training data leading to substantial gains on multilingual, multi-turn conversation quality, and reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community.\n\n### Multilingual\n\nThe table below highlights multilingual capability of the Phi-3.5 Mini on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 3.8B active parameters, the model is  competitive on multilingual tasks in comparison to other models with a much bigger active parameters.  \n\n| Benchmark                  | Phi-3.5 Mini-Ins | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------------------|------------------|-----------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Multilingual MMLU          | 55.4             | 51.08                 | 47.4                     | 58.9                      | 56.2             | 63.8           | 77.2             | 72.9                          |\n| Multilingual MMLU-Pro      | 30.9             | 30.21                 | 15.0                     | 34.0                      | 21.4             | 43.0           | 57.9             | 53.2                          |\n| MGSM                       | 47.9             | 41.56                 | 31.8                     | 63.3                      | 56.7             | 75.1           | 75.8             | 81.7                          |\n| MEGA MLQA                  | 61.7             | 55.5                  | 43.9                     | 61.2                      | 45.2             | 54.4           | 61.6             | 70.0                          |\n| MEGA TyDi QA               | 62.2             | 55.9                  | 54.0                     | 63.7                      | 54.5             | 65.6           | 63.6             | 81.8                          |\n| MEGA UDPOS                 | 46.5             | 48.1                  | 57.2                     | 58.2                      | 54.1             | 56.6           | 62.4             | 66.0                          |\n| MEGA XCOPA                 | 63.1             | 62.4                  | 58.8                     | 10.8                      | 21.1             | 31.2           | 95.0             | 90.3                          |\n| MEGA XStoryCloze           | 73.5             | 73.6                  | 75.5                     | 92.3                      | 71.0             | 87.0           | 20.7             | 96.6                          |\n| **Average** | **55.2** | **52.3** | **47.9** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\nThe table below shows Multilingual MMLU scores in some of the supported languages. For more multi-lingual benchmarks and details, see [Appendix A](#appendix-a).\n\n| Benchmark | Phi-3.5 Mini-Ins | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|------------------|-----------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 44.2             | 35.4                  | 33.7                     | 45.3                      | 49.1             | 56.3           | 73.6             | 67.1                          |\n| Chinese   | 52.6             | 46.9                  | 45.9                     | 58.2                      | 54.4             | 62.7           | 66.7             | 70.8                          |\n| Dutch     | 57.7             | 48.0                  | 51.3                     | 60.1                      | 55.9             | 66.7           | 80.6             | 74.2                          |\n| French    | 61.1             | 61.7                  | 53.0                     | 63.8                      | 62.8             | 67.0           | 82.9             | 75.6                          |\n| German    | 62.4             | 61.3                  | 50.1                     | 64.5                      | 59.9             | 65.7           | 79.5             | 74.3                          |\n| Italian   | 62.8             | 63.1                  | 52.5                     | 64.1                      | 55.9             | 65.7           | 82.6             | 75.9                          |\n| Russian   | 50.4             | 45.3                  | 48.9                     | 59.0                      | 57.4             | 63.2           | 78.7             | 72.6                          |\n| Spanish   | 62.6             | 61.3                  | 53.9                     | 64.3                      | 62.6             | 66.0           | 80.0             | 75.5                          |\n| Ukrainian | 45.2             | 36.7                  | 46.9                     | 56.6                      | 52.9             | 62.0           | 77.4             | 72.6                          |\n\n### Long Context\n\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. We see that Phi-3.5-mini is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-mini is competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, Mistral-7B-instruct-v0.3, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-mini-instruct | Llama-3.1-8B-instruct | Mistral-7B-instruct-v0.3 | Mistral-Nemo-12B-instruct-2407 | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| GovReport | 25.9 | 25.1 | 26.0 | 25.6 | 27.8 | 24.8 |\n| QMSum | 21.3 | 21.6 | 21.3 | 22.1 | 24.0 | 21.7 |\n| Qasper | 41.9 | 37.2 | 31.4 | 30.7 | 43.5 | 39.8 |\n| SQuALITY | 25.3 | 26.2 | 25.9 | 25.8 | 23.5 | 23.8 |\n| SummScreenFD | 16.0 | 17.6 | 17.5 | 18.2 | 16.3 | 17.0 |\n| **Average** | **26.1** | **25.5** | **24.4** | **24.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 94.3 | 91.1 | 90.7 | 87.1 | 78.0 | 63.6 | **84.1** |\n| **Llama-3.1-8B-instruct** | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| **Mistral-Nemo-12B-instruct-2407** | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 86 | 67 | 73 | 77 | 82 | **77** |\n| **Llama-3.1-8B-instruct** | 80 | 65 | 73 | 76 | 63 | **71** |\n| **Mistral-7B-instruct-v0.3** | 61 | 57 | 51 | 61 | 80 | **62** |\n\n## Usage\n\n### Requirements\nPhi-3 family has been integrated in the `4.43.0` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.43.0\n```\n\nPhi-3.5-mini-instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3.5mini)\n\n### Tokenizer\n\nPhi-3.5-mini-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\n\n### Input Formats\nGiven the nature of the training data, the Phi-3.5-mini-instruct model is best suited for prompts using the chat format as follows:\n\n```\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\n### Loading the model locally\nAfter obtaining the Phi-3.5-mini-instruct model checkpoint, users can use this sample code for inference.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\", \n    device_map=\"cuda\", \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n```\n\nNotes: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation=\"flash_attention_2\"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n+ Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n+ Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:  \n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n**Architecture:** Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini.<br>\n**Inputs:** Text. It is best suited for prompts using chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 512 H100-80G<br>\n**Training time:** 10 days<br>\n**Training data:** 3.4T tokens<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between June and August 2024<br>\n**Status:** This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.<br>\n**Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n**Release date:** August 2024<br>\n\n### Training Datasets\nOur training data includes a wide variety of sources, totaling 3.4 trillion tokens, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3.5-mini on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7B-Instruct-v0.3,  Mistral-Nemo-12B-Ins-2407, Llama-3.1-8B-Ins, Gemma-2-9B-Ins, Gemini 1.5 Flash, and GPT-4o-mini-2024-07-18 (Chat).\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of kâ€“shot examples is listed per-benchmark. At the high-level overview of the model quality on representative benchmarks: \n\n| Category       | Benchmark                | Phi-3.5 Mini-Ins | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------|--------------------------|------------------|--------------------------|---------------------------|------------------|----------------|------------------|------------------------------|\n| Popular aggregated benchmark | Arena Hard | 37 | 18.1 | 39.4 | 25.7 | 42 | 55.2 | 75 |\n|                | BigBench Hard CoT (0-shot) | 69 | 33.4 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n|                | MMLU (5-shot) | 69 | 60.3 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n|                | MMLU-Pro (0-shot, CoT) | 47.4 | 18 | 40.7 | 44 | 50.1 | 57.2 | 62.8 |\n| Reasoning      | ARC Challenge (10-shot) | 84.6 | 77.9 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n|                | BoolQ (2-shot) | 78 | 80.5 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n|                | GPQA (0-shot, CoT) | 30.4 | 15.6 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n|                | HellaSwag (5-shot) | 69.4 | 71.6 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n|                | OpenBookQA (10-shot) | 79.2 | 78 | 84.4 | 84.8 | 89.6 | 89 | 90 |\n|                | PIQA (5-shot) | 81 | 73.4 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n|                | Social IQA (5-shot) | 74.7 | 73 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n|                | TruthfulQA (MC2) (10-shot) | 64 | 64.7 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n|                | WinoGrande (5-shot) | 68.5 | 58.1 | 70.4 | 64.7 | 74 | 74.7 | 76.9 |\n| Multilingual   | Multilingual MMLU (5-shot) | 55.4 | 47.4 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n|                | MGSM (0-shot CoT) | 47.9 | 31.8 | 63.3 | 56.7 | 76.4 | 75.8 | 81.7 |\n| Math           | GSM8K (8-shot, CoT) | 86.2 | 54.4 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n|                | MATH (0-shot, CoT) | 48.5 | 19 | 31.2 | 47.6 | 50.9 | 38 | 70.2 |\n| Long context   | Qasper | 41.9 | 31.4 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n|                | SQuALITY | 24.3 | 25.9 | 25.8 | 26.2 | 0 | 23.5 | 23.8 |\n| Code Generation| HumanEval (0-shot) | 62.8 | 35.4 | 63.4 | 66.5 | 61 | 74.4 | 86.6 |\n|                | MBPP (3-shot) | 69.6 | 50.4 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **61.4** | **48.5** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across public benchmark datasets at the table below:\n\n| Category                   | Phi-3.5 Mini-Ins | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------------------|------------------|--------------------------|---------------------------|------------------|----------------|------------------|------------------------------|\n| Popular aggregated benchmark | 55.6 | 32.5 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning                  | 70.1 | 65.2 | 72.2 | 70.5 | 75.4 | 77.7 | 80 |\n| Language understanding     | 62.6 | 62.8 | 67 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness                 | 59.7 | 53.4 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context               | 26.1 | 25.5 | 24.4 | 24.5 | 0 | 27 | 25.4 |\n| Math                       | 67.4 | 36.7 | 57.7 | 65 | 67.9 | 60.2 | 80.8 |\n| Code generation            | 62 | 43.1 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual               | 55.2 | 47.9 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models.\nHowever, it is still fundamentally limited by its size for certain tasks. \nThe model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. \nHowever, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.  \n\n## Safety Evaluation and Red-Teaming\n\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to \nevaluate Phi-3.5 models' propensity to produce undesirable outputs across multiple languages and risk categories. \nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety \npost-training that was done as detailed in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833) had a positive impact across multiple languages and risk categories as observed by \nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted \nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as \nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833). \nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output \nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings \nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, \nand risk areas that account for cultural nuances where those languages are spoken.\n\n\n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3.5-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followâ€¯[Microsoftâ€™s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.\n\n\n## Appendix A\n\n#### MGSM\n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|------------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| German    | 69.6                   | 65.2                                  | 42.4                     | 74.4                      | 68.4             | 76.8           | 81.6             | 82.8                          |\n| English   | 85.2                   | 83.2                                  | 60.0                     | 86.0                      | 81.2             | 88.8           | 90.8             | 90.8                          |\n| Spanish   | 79.2                   | 77.6                                  | 46.4                     | 75.6                      | 66.4             | 82.4           | 84.8             | 86.8                          |\n| French    | 71.6                   | 72.8                                  | 47.2                     | 70.4                      | 66.8             | 74.4           | 77.2             | 81.6                          |\n| Japanese  | 50.0                   | 35.2                                  | 22.8                     | 62.4                      | 49.2             | 67.6           | 77.6             | 80.4                          |\n| Russian   | 67.2                   | 51.6                                  | 43.2                     | 73.6                      | 67.2             | 78.4           | 84.8             | 86.4                          |\n| Thai      | 29.6                   | 6.4                                   | 18.4                     | 53.2                      | 56.0             | 76.8           | 87.6             | 81.6                          |\n| Chinese   | 60.0                   | 52.8                                  | 42.4                     | 66.4                      | 68.0             | 72.8           | 82.0             | 82.0                          |\n\n#### Multilingual MMLU-pro \n\n| Languages  | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|------------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Czech      | 24.9                  | 26.3                                  | 14.6                     | 30.6                      | 23.0             | 40.5           | 59.0             | 40.9                          |\n| English    | 47.7                  | 46.2                                  | 17.7                     | 39.8                      | 43.1             | 49.0           | 66.1             | 62.7                          |\n| Finnish    | 22.3                  | 20.5                                  | 11.5                     | 30.4                      | 9.7              | 37.5           | 54.5             | 50.1                          |\n| Norwegian  | 29.9                  | 27.8                                  | 14.4                     | 33.2                      | 22.2             | 44.4           | 60.7             | 59.1                          |\n| Polish     | 25.7                  | 26.4                                  | 16.3                     | 33.6                      | 9.2              | 41.7           | 53.9             | 42.8                          |\n| Portuguese | 38.7                  | 37.6                                  | 15.3                     | 36.0                      | 29.3             | 43.5           | 54.0             | 56.9                          |\n| Swedish    | 30.7                  | 28.1                                  | 15.5                     | 34.3                      | 16.9             | 42.6           | 57.7             | 55.5                          |\n\n#### MEGA \n\n##### MLQA\n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 54.3                  | 32.7                                  | 23.5                     | 31.4                      | 31.5             | 57.4           | 63.8             | 64.0                          |\n| Chinese   | 36.1                  | 31.8                                  | 22.4                     | 27.4                      | 18.6             | 45.4           | 38.1             | 38.9                          |\n| English   | 80.3                  | 78.9                                  | 68.2                     | 75.5                      | 67.2             | 82.9           | 69.5             | 82.2                          |\n| German    | 61.8                  | 59.1                                  | 49.0                     | 57.8                      | 38.9             | 63.8           | 55.9             | 64.1                          |\n| Spanish   | 68.8                  | 67.0                                  | 50.3                     | 63.6                      | 52.7             | 72.8           | 59.6             | 70.1                          |\n\n##### TyDi QA \n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 69.7                  | 54.4                                  | 52.5                     | 49.8                      | 33.7             | 81.1           | 78.8             | 84.9                          |\n| English   | 82.0                  | 82.0                                  | 60.5                     | 77.3                      | 65.1             | 82.4           | 60.9             | 81.8                          |\n| Finnish   | 70.3                  | 64.3                                  | 68.6                     | 57.1                      | 74.4             | 85.7           | 73.5             | 84.8                          |\n| Japanese  | 65.4                  | 56.7                                  | 45.3                     | 54.8                      | 34.1             | 74.6           | 59.7             | 73.3                          |\n| Korean    | 74.0                  | 60.4                                  | 54.5                     | 54.2                      | 54.9             | 83.8           | 60.7             | 82.3                          |\n| Russian   | 63.5                  | 62.7                                  | 52.3                     | 55.7                      | 27.4             | 69.8           | 60.1             | 72.5                          |\n| Thai      | 64.4                  | 49.0                                  | 51.8                     | 43.5                      | 48.5             | 81.4           | 71.6             | 78.2                          |\n\n##### XCOPA \n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| English   | 94.6                  | 94.6                                  | 85.6                     | 94.4                      | 37.6             | 63.8           | 92.0             | 98.2                          |\n| Italian   | 86.8                  | 84.8                                  | 76.8                     | 83.2                      | 16.2             | 37.2           | 85.6             | 97.6                          |\n| Turkish   | 58.6                  | 57.2                                  | 61.6                     | 56.6                      | 38.4             | 60.2           | 91.4             | 94.6                          |\n\n\n## Appendix B: Korean benchmarks\n\nThe prompt is the same as the [CLIcK paper](https://arxiv.org/abs/2403.06412) prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\n\n- GPT-4o: 2024-05-13 version\n- GPT-4o-mini: 2024-07-18 version\n- GPT-4-turbo: 2024-04-09 version\n- GPT-3.5-turbo: 2023-06-13 version\n\nThe overall Korean benchmarks show that the Phi-3.5-Mini-Instruct with only 3.8B params outperforms Llama-3.1-8B-Instruct.\n\n| Benchmarks               |   Phi-3.5-Mini-Instruct |  Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:-------------------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| CLIcK                    |                   42.99 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n| HAERAE 1.0               |                   44.21 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n| KMMLU (0-shot, CoT)      |                   35.87 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n| KMMLU (5-shot)           |                   37.35 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n| KMMLU-HARD (0-shot, CoT) |                   24    |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 | \n| KMMLU-HARD (5-shot)      |                   24.76 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |\n| **Average**                  |                   **35.62** |                           **29.99** |                   **29.29** |    **62.54** |         **50.08** |         **56.74** |           **39.61** | \n\n#### CLIcK (Cultural and Linguistic Intelligence in Korean)\n\n##### Accuracy by supercategory\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         |                   43.77 |                           29.74 |                   51.15 |    81.89 |         70.95 |         73.61 |           53.38 |\n| Language        |                   41.38 |                           27.85 |                   40.92 |    77.54 |         63.54 |         71.23 |           46    |\n| **Overall**     |                   42.99 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n\n##### Accuracy by category\n| supercategory   | category    |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|:------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         | Economy     |                   61.02 |                           28.81 |                   66.1  |    94.92 |         83.05 |         89.83 |           64.41 |\n| Culture         | Geography   |                   45.8  |                           29.01 |                   54.2  |    80.15 |         77.86 |         82.44 |           53.44 |\n| Culture         | History     |                   26.15 |                           30    |                   29.64 |    66.92 |         48.4  |         46.4  |           31.79 |\n| Culture         | Law         |                   32.42 |                           22.83 |                   44.29 |    70.78 |         57.53 |         61.19 |           41.55 |\n| Culture         | Politics    |                   54.76 |                           33.33 |                   59.52 |    88.1  |         83.33 |         89.29 |           65.48 |\n| Culture         | Pop Culture |                   60.98 |                           34.15 |                   60.98 |    97.56 |         85.37 |         92.68 |           75.61 |\n| Culture         | Society     |                   54.37 |                           31.72 |                   65.05 |    92.88 |         85.44 |         86.73 |           71.2  |\n| Culture         | Tradition   |                   47.75 |                           31.98 |                   54.95 |    87.39 |         74.77 |         79.28 |           55.86 |\n| Language        | Functional  |                   37.6  |                           24    |                   32.8  |    84.8  |         64.8  |         80    |           40    |\n| Language        | Grammar     |                   27.5  |                           23.33 |                   22.92 |    57.08 |         42.5  |         47.5  |           30    |\n| Language        | Textual     |                   54.74 |                           33.33 |                   59.65 |    91.58 |         80.7  |         87.37 |           62.11 |\n\n#### HAERAE\n\n| category              |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| General Knowledge     |                   31.25 |                           28.41 |                   34.66 |    77.27 |         53.41 |         66.48 |           40.91 |\n| History               |                   32.45 |                           22.34 |                   44.15 |    92.02 |         84.57 |         78.72 |           30.32 |\n| Loan Words            |                   47.93 |                           35.5  |                   63.31 |    79.88 |         76.33 |         78.11 |           59.17 |\n| Rare Words            |                   55.06 |                           42.96 |                   63.21 |    87.9  |         81.98 |         79.01 |           61.23 |\n| Reading Comprehension |                   42.95 |                           41.16 |                   51.9  |    85.46 |         77.18 |         80.09 |           56.15 |\n| Standard Nomenclature |                   44.44 |                           32.68 |                   58.82 |    88.89 |         75.82 |         79.08 |           53.59 |\n| **Overall**           |                   44.21 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n\n#### KMMLU (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   35.8  |                           31.68 |                   37.03 |    61.52 |         49.29 |         55.98 |           38.47 |\n| HUMSS           |                   31.56 |                           26.47 |                   37.29 |    69.45 |         56.59 |         63    |           40.9  |\n| Other           |                   35.45 |                           31.01 |                   39.15 |    63.79 |         52.35 |         57.53 |           40.19 |\n| STEM            |                   38.54 |                           31.9  |                   40.42 |    65.16 |         54.74 |         60.84 |           42.24 |\n| **Overall**     |                   35.87 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n\n#### KMMLU (5-shot)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   37.42 |                           29.98 |                   19.24 |    61.47 |         48.66 |         56.85 |           40.22 |\n| HUMSS           |                   34.72 |                           27.27 |                   22.5  |    68.79 |         55.95 |         63.68 |           43.35 |\n| Other           |                   37.04 |                           30.76 |                   20.95 |    64.21 |         51.1  |         57.85 |           41.92 |\n| STEM            |                   38.9  |                           30.73 |                   19.55 |    65.28 |         53.29 |         61.08 |           44.43 |\n| **Overall**     |                   37.35 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n\n#### KMMLU-HARD (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   27.08 |                           26.17 |                   26.25 |    37.12 |         22.25 |         29.17 |           21.07 |\n| HUMSS           |                   20.21 |                           24.38 |                   20.21 |    41.97 |         23.31 |         31.51 |           19.44 |\n| Other           |                   23.05 |                           24.82 |                   23.88 |    40.39 |         26.48 |         29.59 |           22.22 |\n| STEM            |                   24.36 |                           26.91 |                   24.64 |    39.82 |         26.36 |         32.18 |           20.91 |\n| **Overall**     |                   24    |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n\n#### KMMLU-HARD (5-shot)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   25    |                           29    |                   12    |    31    |         21    |         25    |           20    |\n| HUMSS           |                   21.89 |                           19.92 |                   14    |    43.98 |         23.47 |         33.53 |           19.53 |\n| Other           |                   23.26 |                           27.27 |                   12.83 |    39.84 |         28.34 |         29.68 |           23.22 |\n| STEM            |                   20.5  |                           25.25 |                   12.75 |    40.25 |         23.25 |         27.25 |           19.75 |\n| **Overall**     |                   24.76 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":3821079552,\"storage_bytes\":7642681603,\"files_count\":19,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Phi3ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_phi3.Phi3Config\",\"AutoModelForCausalLM\":\"modeling_phi3.Phi3ForCausalLM\"},\"model_type\":\"phi3\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:microsoft:Phi-3CookBook\",\"source_url\":\"https://github.com/microsoft/Phi-3CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:pytorch:pytorch\",\"source_url\":\"https://github.com/pytorch/pytorch\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:HazyResearch:flash-attention\",\"source_url\":\"https://github.com/HazyResearch/flash-attention\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.14219\",\"source_url\":\"https://arxiv.org/abs/2404.14219\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2407.13833\",\"source_url\":\"https://arxiv.org/abs/2407.13833\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.06412\",\"source_url\":\"https://arxiv.org/abs/2403.06412\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 79.7,
    "content_hash": "d54cff83d717d604469007ee3ebc7562",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/microsoft/Phi-3.5-mini-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen-image-edit-2509",
    "name": "Qwen-Image-Edit-2509",
    "author": "Qwen",
    "description": "--- license: apache-2.0 language: - en - zh library_name: diffusers pipeline_tag: image-to-image --- <p align=\"center\"> <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png\" width=\"400\"/> <p> <p align=\"center\"> ğŸ’œ <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/Qwen/Qwen-Image-Edit-2509\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509...",
    "tags": [
      "diffusers",
      "safetensors",
      "image-to-image",
      "en",
      "zh",
      "arxiv:2508.02324",
      "license:apache-2.0",
      "diffusers:qwenimageeditpluspipeline",
      "region:us"
    ],
    "pipeline_tag": "image-to-image",
    "likes": 932,
    "downloads": 594256,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen-Image-Edit-2509",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nlibrary_name: diffusers\npipeline_tag: image-to-image\n---\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png\" width=\"400\"/>\n<p>\n<p align=\"center\">\n          ğŸ’œ <a href=\"https://chat.qwen.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/Qwen/Qwen-Image-Edit-2509\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf\">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://qwenlm.github.io/blog/qwen-image-edit/\">Blog</a> &nbsp&nbsp \n<br>\nğŸ–¥ï¸ <a href=\"https://huggingface.co/spaces/Qwen/Qwen-Image-Edit\">Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp| &nbsp&nbsp <a href=\"https://github.com/QwenLM/Qwen-Image\">Github</a>&nbsp&nbsp\n</p>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2509/edit2509_top.jpg\" width=\"1600\"/>\n<p>\n\n\n# Introduction\nThis September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit [Qwen Chat](https://qwen.ai)  and select the \"Image Editing\" feature.\nCompared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:\n* **Multi-image Editing Support**: For multi-image inputs, Qwen-Image-Edit-2509 builds upon the Qwen-Image-Edit architecture and is further trained via image concatenation to enable multi-image editing. It supports various combinations such as \"person + person,\" \"person + product,\" and \"person + scene.\" Optimal performance is currently achieved with 1 to 3 input images.\n* **Enhanced Single-image Consistency**: For single-image inputs, Qwen-Image-Edit-2509 significantly improves editing consistency, specifically in the following areas:\n  - **Improved Person Editing Consistency**: Better preservation of facial identity, supporting various portrait styles and pose transformations;\n  - **Improved Product Editing Consistency**: Better preservation of product identity, supporting product poster editingï¼›\n  - **Improved Text Editing Consistency**: In addition to modifying text content, it also supports editing text fonts, colors, and materialsï¼›\n* **Native Support for ControlNet**: Including depth maps, edge maps, keypoint maps, and more.\n\n\n## Quick Start\n\nInstall the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nThe following contains a code snippet illustrating how to use `Qwen-Image-Edit-2509`:\n\n```python\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\n\npipeline = QwenImageEditPlusPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit-2509\", torch_dtype=torch.bfloat16)\nprint(\"pipeline loaded\")\n\npipeline.to('cuda')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open(\"input1.png\")\nimage2 = Image.open(\"input2.png\")\nprompt = \"The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square.\"\ninputs = {\n    \"image\": [image1, image2],\n    \"prompt\": prompt,\n    \"generator\": torch.manual_seed(0),\n    \"true_cfg_scale\": 4.0,\n    \"negative_prompt\": \" \",\n    \"num_inference_steps\": 40,\n    \"guidance_scale\": 1.0,\n    \"num_images_per_prompt\": 1,\n}\nwith torch.inference_mode():\n    output = pipeline(**inputs)\n    output_image = output.images[0]\n    output_image.save(\"output_image_edit_plus.png\")\n    print(\"image saved at\", os.path.abspath(\"output_image_edit_plus.png\"))\n\n```\n\n## Showcase\n\n**The primary update in Qwen-Image-Edit-2509 is support for multi-image inputs.**\n\nLetâ€™s first look at a \"person + person\" example:  \n![Person + Person Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8719.JPG#center)\n\nHere is a \"person + scene\" example:  \n![Person + Scene Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8720.JPG#center)\n\nBelow is a \"person + object\" example:  \n![Person + Object Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8721.JPG#center)\n\nIn fact, multi-image input also supports commonly used ControlNet keypoint mapsâ€”for example, changing a personâ€™s pose:  \n![ControlNet Keypoint Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8722.JPG#center)\n\nSimilarly, the following examples demonstrate results using three input images:  \n![Three Images Example 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8723.JPG#center)  \n![Three Images Example 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8724.JPG#center)  \n![Three Images Example 3](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8725.JPG#center)\n\n---\n\n**Another major update in Qwen-Image-Edit-2509 is enhanced consistency.**\n\nFirst, regarding person consistency, Qwen-Image-Edit-2509 shows significant improvement over Qwen-Image-Edit. Below are examples generating various portrait styles:  \n![Portrait Styles Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center)\n\nFor instance, changing a personâ€™s pose while maintaining excellent identity consistency:  \n![Pose Change with Identity Consistency](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center)\n\nLeveraging this improvement along with Qwen-Imageâ€™s unique text rendering capability, we find that Qwen-Image-Edit-2509 excels at creating meme images:  \n![Meme Image Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center)\n\nOf course, even with longer text, Qwen-Image-Edit-2509 can still render it while preserving the personâ€™s identity:  \n![Long Text with Identity Preservation](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center)\n\nPerson consistency is also evident in old photo restoration. Below are two examples:  \n![Old Photo Restoration 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8717.JPG#center)  \n![Old Photo Restoration 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8718.JPG#center)\n\nNaturally, besides real people, generating cartoon characters and cultural creations is also possible:  \n![Cartoon & Cultural Creation](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center)\n\nSecond, Qwen-Image-Edit-2509 specifically enhances product consistency. We find that the model can naturally generate product posters from plain-background product images:  \n![Product Poster Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center)\n\nOr even simple logos:  \n![Logo Generation Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center)\n\nThird, Qwen-Image-Edit-2509 specifically enhances text consistency and supports editing font type, font color, and font material:  \n![Text Font Type](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center)  \n![Text Font Color](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center)  \n![Text Font Material](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center)\n\nMoreover, the ability for precise text editing has been significantly enhanced:  \n![Precise Text Editing 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center)  \n![Precise Text Editing 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center)\n\nIt is worth noting that text editing can often be seamlessly integrated with image editingâ€”for example, in this poster editing case:  \n![Integrated Text & Image Editing](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center)\n\n---\n\n**The final update in Qwen-Image-Edit-2509 is native support for commonly used ControlNet image conditions, such as keypoint control and sketches:**  \n![Keypoint Control Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center)  \n![Sketch Control Example 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center)  \n![Sketch Control Example 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center)\n\n\n\n## License Agreement\n\nQwen-Image is licensed under Apache 2.0. \n\n## Citation\n\nWe kindly encourage citation of our work if you find it useful.\n\n```bibtex\n@misc{wu2025qwenimagetechnicalreport,\n      title={Qwen-Image Technical Report}, \n      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\n      year={2025},\n      eprint={2508.02324},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.02324}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"image-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":57711227846,\"files_count\":35,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"QwenImageEditPlusPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Image\",\"source_url\":\"https://github.com/QwenLM/Qwen-Image\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Image\\\">Github<\",\"source_url\":\"https://github.com/QwenLM/Qwen-Image\\\">Github<\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2508.02324\",\"source_url\":\"https://arxiv.org/abs/2508.02324\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.7,
    "content_hash": "1c0497ccf9ed70531f6b522328ab1fed",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen-Image-Edit-2509\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepset:roberta-base-squad2",
    "name": "roberta-base-squad2",
    "author": "deepset",
    "description": "--- language: en license: cc-by-4.0 datasets: - squad_v2 model-index: - name: deepset/roberta-base-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation metrics: - type: exact_match value: 79.9309 name: Exact Match verified: true verifyToken: >- eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "rust",
      "safetensors",
      "roberta",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "base_model:facebookai/roberta-base",
      "base_model:finetune:facebookai/roberta-base",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "question-answering",
    "likes": 930,
    "downloads": 730222,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepset/roberta-base-squad2",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/roberta-base-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.9309\n      name: Exact Match\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA\n    - type: f1\n      value: 82.9501\n      name: F1\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ\n    - type: total\n      value: 11869\n      name: total\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.289\n      name: Exact Match\n    - type: f1\n      value: 91.841\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 29.5\n      name: Exact Match\n    - type: f1\n      value: 40.367\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.567\n      name: Exact Match\n    - type: f1\n      value: 84.469\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 69.924\n      name: Exact Match\n    - type: f1\n      value: 83.284\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.204\n      name: Exact Match\n    - type: f1\n      value: 90.595\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.931\n      name: Exact Match\n    - type: f1\n      value: 90.756\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 71.55\n      name: Exact Match\n    - type: f1\n      value: 82.939\n      name: F1\nbase_model:\n- FacebookAI/roberta-base\n---\n\n# roberta-base for Extractive QA \n\nThis is the [roberta-base](https://huggingface.co/roberta-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \nWe have also released a distilled version of this model called [deepset/tinyroberta-squad2](https://huggingface.co/deepset/tinyroberta-squad2). It has a comparable prediction quality and runs at twice the speed of [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2).\n\n\n## Overview\n**Language model:** roberta-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = \"roberta-base\"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 79.87029394424324,\n\"f1\": 82.91251169582613,\n\n\"total\": 11873,\n\"HasAns_exact\": 77.93522267206478,\n\"HasAns_f1\": 84.02838248389763,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.79983179142137,\n\"NoAns_f1\": 81.79983179142137,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo MÃ¶ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "meta_json": "{\"pipeline_tag\":\"question-answering\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":124057092,\"storage_bytes\":3942257079,\"files_count\":12,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"RobertaForQuestionAnswering\"],\"model_type\":\"roberta\",\"tokenizer_config\":{}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepset-ai:haystack\",\"source_url\":\"https://github.com/deepset-ai/haystack\"},{\"type\":\"has_code\",\"target_id\":\"github:deepset-ai:haystack\\\">GitHub<\",\"source_url\":\"https://github.com/deepset-ai/haystack\\\">GitHub<\"},{\"type\":\"has_code\",\"target_id\":\"github:deepset-ai:haystack\",\"source_url\":\"https://github.com/deepset-ai/haystack\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-4.0",
    "compliance_status": "approved",
    "quality_score": 64.7,
    "content_hash": "2d959bc022b2c652b46dc6c8942f1a86",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepset/roberta-base-squad2\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:facebook:seamless-m4t-v2-large",
    "name": "seamless-m4t-v2-large",
    "author": "facebook",
    "description": "--- license: cc-by-nc-4.0 language: - af - am - ar - as - az - be - bn - bs - bg - ca - cs - zh - cy - da - de - el - en - et - fi - fr - or - om - ga - gl - gu - ha - he - hi - hr - hu - hy - ig - id - is - it - jv - ja - kn - ka - kk - mn - km - ky - ko - lo - ln - lt - lb - lg - lv - ml - mr - mk - mt - mi - my - nl - nb - ne - ny - oc - pa - ps - fa - pl - pt - ro - ru - sk - sl - sn - sd - so - es - sr - sv - sw - ta - te - tg - tl - th - tr - uk - ur - uz - vi - wo - xh - yo - ms - zu -...",
    "tags": [
      "transformers",
      "safetensors",
      "seamless_m4t_v2",
      "feature-extraction",
      "audio-to-audio",
      "text-to-speech",
      "seamless_communication",
      "automatic-speech-recognition",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bn",
      "bs",
      "bg",
      "ca",
      "cs",
      "zh",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "et",
      "fi",
      "fr",
      "or",
      "om",
      "ga",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "ig",
      "id",
      "is",
      "it",
      "jv",
      "ja",
      "kn",
      "ka",
      "kk",
      "mn",
      "km",
      "ky",
      "ko",
      "lo",
      "ln",
      "lt",
      "lb",
      "lg",
      "lv",
      "ml",
      "mr",
      "mk",
      "mt",
      "mi",
      "my",
      "nl",
      "nb",
      "ne",
      "ny",
      "oc",
      "pa",
      "ps",
      "fa",
      "pl",
      "pt",
      "ro",
      "ru",
      "sk",
      "sl",
      "sn",
      "sd",
      "so",
      "es",
      "sr",
      "sv",
      "sw",
      "ta",
      "te",
      "tg",
      "tl",
      "th",
      "tr",
      "uk",
      "ur",
      "uz",
      "vi",
      "wo",
      "xh",
      "yo",
      "ms",
      "zu",
      "ary",
      "arz",
      "yue",
      "kea",
      "arxiv:2312.05187",
      "license:cc-by-nc-4.0",
      "region:us"
    ],
    "pipeline_tag": "automatic-speech-recognition",
    "likes": 929,
    "downloads": 52987,
    "source": "huggingface",
    "source_url": "https://huggingface.co/facebook/seamless-m4t-v2-large",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: cc-by-nc-4.0\nlanguage:\n- af\n- am\n- ar\n- as\n- az\n- be\n- bn\n- bs\n- bg\n- ca\n- cs\n- zh\n- cy\n- da\n- de\n- el\n- en\n- et\n- fi\n- fr\n- or\n- om\n- ga\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- ig\n- id\n- is\n- it\n- jv\n- ja\n- kn\n- ka\n- kk\n- mn\n- km\n- ky\n- ko\n- lo\n- ln\n- lt\n- lb\n- lg\n- lv\n- ml\n- mr\n- mk\n- mt\n- mi\n- my\n- nl\n- nb\n- ne\n- ny\n- oc\n- pa\n- ps\n- fa\n- pl\n- pt\n- ro\n- ru\n- sk\n- sl\n- sn\n- sd\n- so\n- es\n- sr\n- sv\n- sw\n- ta\n- te\n- tg\n- tl\n- th\n- tr\n- uk\n- ur\n- uz\n- vi\n- wo\n- xh\n- yo\n- ms\n- zu\n- ary\n- arz\n- yue\n- kea\nmetrics:\n- bleu\n- wer\n- chrf\ninference: False\npipeline_tag: automatic-speech-recognition\ntags:\n  - audio-to-audio\n  - text-to-speech\n  - seamless_communication  \nlibrary_name: transformers\nwidget:\n  - src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n    example_title: Librispeech sample 1\n    output:\n      text: going along slushy country roads and speaking to damp audiences in draughty schoolrooms day after day for a fortnight he'll have to put in an appearance at some place of worship on sunday morning and he can come to us immediately afterwards\n  - src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n    example_title: Librispeech sample 2\n    output:\n      text: before he had time to answer a much-encumbered vera burst into the room with the question i say can i leave these here these were a small black pig and a lusty specimen of black-red game-cock\n---\n\n# SeamlessM4T v2\n\n**SeamlessM4T** is our foundational all-in-one **M**assively **M**ultilingual and **M**ultimodal **M**achine **T**ranslation model delivering high-quality translation for speech and text in nearly 100 languages.\n\nSeamlessM4T models support the tasks of:\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR).\n\nSeamlessM4T models support:\n- ğŸ¤ 101 languages for speech input.\n- ğŸ’¬ 96 Languages for text input/output.\n- ğŸ”Š 35 languages for speech output.\n  \nğŸŒŸ We are releasing SeamlessM4T v2, an updated version with our novel *UnitY2* architecture. \nThis new model improves over SeamlessM4T v1 in quality as well as inference speed in speech generation tasks.\n\nThe v2 version of SeamlessM4T is a multitask adaptation of our novel *UnitY2* architecture. \n*Unity2* with its hierarchical character-to-unit upsampling and non-autoregressive text-to-unit decoding considerably improves over SeamlessM4T v1 in quality and inference speed.\n\n**SeamlessM4T v2 is also supported by ğŸ¤— Transformers, more on it [in the dedicated section below](#transformers-usage).**\n\n![SeamlessM4T architectures](seamlessm4t_arch.svg)\n\n## SeamlessM4T  models\n| Model Name         | #params | checkpoint                                                                              | metrics                                                                              |\n| ------------------ | ------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n| [SeamlessM4T-Large v2](https://huggingface.co/facebook/seamless-m4t-v2-large)  | 2.3B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-v2-large/blob/main/seamlessM4T_v2_large.pt)   | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large_v2.zip)  |\n| [SeamlessM4T-Large (v1)](https://huggingface.co/facebook/seamless-m4t-large) | 2.3B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-large/blob/main/multitask_unity_large.pt)   | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large.zip)  |\n| [SeamlessM4T-Medium (v1)](https://huggingface.co/facebook/seamless-m4t-medium) | 1.2B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-medium/blob/main/multitask_unity_medium.pt) | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_medium.zip) |\n\nWe provide the extensive evaluation results of seamlessM4T-Large and SeamlessM4T-Medium reported in the paper (as averages) in the `metrics` files above.\n\nThe evaluation data ids for FLEURS, CoVoST2 and CVSS-C can be found [here](https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip)\n\n\n## Evaluating SeamlessM4T models\nTo reproduce our results or to evaluate using the same metrics over your own test sets, please check out the [Evaluation README here](https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/evaluate).\n\n\n## Finetuning SeamlessM4T models\nPlease check out the [Finetuning README here](https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/finetune).\n\n## Transformers usage\n\nSeamlessM4T is available in the ğŸ¤— Transformers library, requiring minimal dependencies. Steps to get started:\n\n1. First install the ğŸ¤— [Transformers library](https://github.com/huggingface/transformers) from main and [sentencepiece](https://github.com/google/sentencepiece):\n\n```\npip install git+https://github.com/huggingface/transformers.git sentencepiece\n```\n\n2. Run the following Python code to generate speech samples. Here the target language is Russian:\n\n```py\nfrom transformers import AutoProcessor, SeamlessM4Tv2Model\nimport torchaudio\n\nprocessor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\nmodel = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n\n# from text\ntext_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\naudio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n\n# from audio\naudio, orig_freq =  torchaudio.load(\"https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav\")\naudio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\naudio_inputs = processor(audios=audio, return_tensors=\"pt\")\naudio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n```\n\n3. Listen to the audio samples either in an ipynb notebook:\n\n```py\nfrom IPython.display import Audio\n\nsample_rate = model.config.sampling_rate\nAudio(audio_array_from_text, rate=sample_rate)\n# Audio(audio_array_from_audio, rate=sample_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\n\n```py\nimport scipy\n\nsample_rate = model.config.sampling_rate\nscipy.io.wavfile.write(\"out_from_text.wav\", rate=sample_rate, data=audio_array_from_text)\n# scipy.io.wavfile.write(\"out_from_audio.wav\", rate=sample_rate, data=audio_array_from_audio)\n```\nFor more details on using the SeamlessM4T model for inference using the ğŸ¤— Transformers library, refer to the \n**[SeamlessM4T v2 docs](https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2)** or to this **hands-on [Google Colab](https://colab.research.google.com/github/ylacombe/scripts_and_notebooks/blob/main/v2_seamless_m4t_hugging_face.ipynb).**\n\n\n## Supported Languages:\n\nListed below, are the languages supported by SeamlessM4T-large (v1/v2).\nThe `source` column specifies whether a language is supported as source speech (`Sp`) and/or source text (`Tx`).\nThe `target` column specifies whether a language is supported as target speech (`Sp`) and/or target text (`Tx`).\n\n\n| code | language               | script     | Source | Target |\n| ---- | ---------------------- | ---------- | ------ | ------ |\n| afr  | Afrikaans              | Latn       | Sp, Tx | Tx     |\n| amh  | Amharic                | Ethi       | Sp, Tx | Tx     |\n| arb  | Modern Standard Arabic | Arab       | Sp, Tx | Sp, Tx |\n| ary  | Moroccan Arabic        | Arab       | Sp, Tx | Tx     |\n| arz  | Egyptian Arabic        | Arab       | Sp, Tx | Tx     |\n| asm  | Assamese               | Beng       | Sp, Tx | Tx     |\n| ast  | Asturian               | Latn       | Sp     | \\--    |\n| azj  | North Azerbaijani      | Latn       | Sp, Tx | Tx     |\n| bel  | Belarusian             | Cyrl       | Sp, Tx | Tx     |\n| ben  | Bengali                | Beng       | Sp, Tx | Sp, Tx |\n| bos  | Bosnian                | Latn       | Sp, Tx | Tx     |\n| bul  | Bulgarian              | Cyrl       | Sp, Tx | Tx     |\n| cat  | Catalan                | Latn       | Sp, Tx | Sp, Tx |\n| ceb  | Cebuano                | Latn       | Sp, Tx | Tx     |\n| ces  | Czech                  | Latn       | Sp, Tx | Sp, Tx |\n| ckb  | Central Kurdish        | Arab       | Sp, Tx | Tx     |\n| cmn  | Mandarin Chinese       | Hans       | Sp, Tx | Sp, Tx |\n| cmn_Hant  | Mandarin Chinese  | Hant       | Sp, Tx | Sp, Tx |\n| cym  | Welsh                  | Latn       | Sp, Tx | Sp, Tx |\n| dan  | Danish                 | Latn       | Sp, Tx | Sp, Tx |\n| deu  | German                 | Latn       | Sp, Tx | Sp, Tx |\n| ell  | Greek                  | Grek       | Sp, Tx | Tx     |\n| eng  | English                | Latn       | Sp, Tx | Sp, Tx |\n| est  | Estonian               | Latn       | Sp, Tx | Sp, Tx |\n| eus  | Basque                 | Latn       | Sp, Tx | Tx     |\n| fin  | Finnish                | Latn       | Sp, Tx | Sp, Tx |\n| fra  | French                 | Latn       | Sp, Tx | Sp, Tx |\n| fuv  | Nigerian Fulfulde      | Latn       | Sp, Tx | Tx     |\n| gaz  | West Central Oromo     | Latn       | Sp, Tx | Tx     |\n| gle  | Irish                  | Latn       | Sp, Tx | Tx     |\n| glg  | Galician               | Latn       | Sp, Tx | Tx     |\n| guj  | Gujarati               | Gujr       | Sp, Tx | Tx     |\n| heb  | Hebrew                 | Hebr       | Sp, Tx | Tx     |\n| hin  | Hindi                  | Deva       | Sp, Tx | Sp, Tx |\n| hrv  | Croatian               | Latn       | Sp, Tx | Tx     |\n| hun  | Hungarian              | Latn       | Sp, Tx | Tx     |\n| hye  | Armenian               | Armn       | Sp, Tx | Tx     |\n| ibo  | Igbo                   | Latn       | Sp, Tx | Tx     |\n| ind  | Indonesian             | Latn       | Sp, Tx | Sp, Tx |\n| isl  | Icelandic              | Latn       | Sp, Tx | Tx     |\n| ita  | Italian                | Latn       | Sp, Tx | Sp, Tx |\n| jav  | Javanese               | Latn       | Sp, Tx | Tx     |\n| jpn  | Japanese               | Jpan       | Sp, Tx | Sp, Tx |\n| kam  | Kamba                  | Latn       | Sp     | \\--    |\n| kan  | Kannada                | Knda       | Sp, Tx | Tx     |\n| kat  | Georgian               | Geor       | Sp, Tx | Tx     |\n| kaz  | Kazakh                 | Cyrl       | Sp, Tx | Tx     |\n| kea  | Kabuverdianu           | Latn       | Sp     | \\--    |\n| khk  | Halh Mongolian         | Cyrl       | Sp, Tx | Tx     |\n| khm  | Khmer                  | Khmr       | Sp, Tx | Tx     |\n| kir  | Kyrgyz                 | Cyrl       | Sp, Tx | Tx     |\n| kor  | Korean                 | Kore       | Sp, Tx | Sp, Tx |\n| lao  | Lao                    | Laoo       | Sp, Tx | Tx     |\n| lit  | Lithuanian             | Latn       | Sp, Tx | Tx     |\n| ltz  | Luxembourgish          | Latn       | Sp     | \\--    |\n| lug  | Ganda                  | Latn       | Sp, Tx | Tx     |\n| luo  | Luo                    | Latn       | Sp, Tx | Tx     |\n| lvs  | Standard Latvian       | Latn       | Sp, Tx | Tx     |\n| mai  | Maithili               | Deva       | Sp, Tx | Tx     |\n| mal  | Malayalam              | Mlym       | Sp, Tx | Tx     |\n| mar  | Marathi                | Deva       | Sp, Tx | Tx     |\n| mkd  | Macedonian             | Cyrl       | Sp, Tx | Tx     |\n| mlt  | Maltese                | Latn       | Sp, Tx | Sp, Tx |\n| mni  | Meitei                 | Beng       | Sp, Tx | Tx     |\n| mya  | Burmese                | Mymr       | Sp, Tx | Tx     |\n| nld  | Dutch                  | Latn       | Sp, Tx | Sp, Tx |\n| nno  | Norwegian Nynorsk      | Latn       | Sp, Tx | Tx     |\n| nob  | Norwegian BokmÃ¥l       | Latn       | Sp, Tx | Tx     |\n| npi  | Nepali                 | Deva       | Sp, Tx | Tx     |\n| nya  | Nyanja                 | Latn       | Sp, Tx | Tx     |\n| oci  | Occitan                | Latn       | Sp     | \\--    |\n| ory  | Odia                   | Orya       | Sp, Tx | Tx     |\n| pan  | Punjabi                | Guru       | Sp, Tx | Tx     |\n| pbt  | Southern Pashto        | Arab       | Sp, Tx | Tx     |\n| pes  | Western Persian        | Arab       | Sp, Tx | Sp, Tx |\n| pol  | Polish                 | Latn       | Sp, Tx | Sp, Tx |\n| por  | Portuguese             | Latn       | Sp, Tx | Sp, Tx |\n| ron  | Romanian               | Latn       | Sp, Tx | Sp, Tx |\n| rus  | Russian                | Cyrl       | Sp, Tx | Sp, Tx |\n| slk  | Slovak                 | Latn       | Sp, Tx | Sp, Tx |\n| slv  | Slovenian              | Latn       | Sp, Tx | Tx     |\n| sna  | Shona                  | Latn       | Sp, Tx | Tx     |\n| snd  | Sindhi                 | Arab       | Sp, Tx | Tx     |\n| som  | Somali                 | Latn       | Sp, Tx | Tx     |\n| spa  | Spanish                | Latn       | Sp, Tx | Sp, Tx |\n| srp  | Serbian                | Cyrl       | Sp, Tx | Tx     |\n| swe  | Swedish                | Latn       | Sp, Tx | Sp, Tx |\n| swh  | Swahili                | Latn       | Sp, Tx | Sp, Tx |\n| tam  | Tamil                  | Taml       | Sp, Tx | Tx     |\n| tel  | Telugu                 | Telu       | Sp, Tx | Sp, Tx |\n| tgk  | Tajik                  | Cyrl       | Sp, Tx | Tx     |\n| tgl  | Tagalog                | Latn       | Sp, Tx | Sp, Tx |\n| tha  | Thai                   | Thai       | Sp, Tx | Sp, Tx |\n| tur  | Turkish                | Latn       | Sp, Tx | Sp, Tx |\n| ukr  | Ukrainian              | Cyrl       | Sp, Tx | Sp, Tx |\n| urd  | Urdu                   | Arab       | Sp, Tx | Sp, Tx |\n| uzn  | Northern Uzbek         | Latn       | Sp, Tx | Sp, Tx |\n| vie  | Vietnamese             | Latn       | Sp, Tx | Sp, Tx |\n| xho  | Xhosa                  | Latn       | Sp     | \\--    |\n| yor  | Yoruba                 | Latn       | Sp, Tx | Tx     |\n| yue  | Cantonese              | Hant       | Sp, Tx | Tx     |\n| zlm  | Colloquial Malay       | Latn       | Sp     | \\--    |\n| zsm  | Standard Malay         | Latn       | Tx     | Tx     |\n| zul  | Zulu                   | Latn       | Sp, Tx | Tx     |\n\n\nNote that seamlessM4T-medium supports 200 languages in the text modality, and is based on NLLB-200 (see full list in [asset card](https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/cards/unity_nllb-200.yaml))\n\n## Citation\nFor SeamlessM4T v2, please cite :\n```bibtex\n@inproceedings{seamless2023,\n   title=\"Seamless: Multilingual Expressive and Streaming Speech Translation\",\n   author=\"{Seamless Communication}, Lo{\\\"i}c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss{\\`a}, Maha Elbayad, Hongyu Gong, Francisco Guzm{\\'a}n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson\",\n  journal={ArXiv},\n  year={2023}\n}\n```\n[//]: # \"https://arxiv.org/abs/2312.05187\"",
    "meta_json": "{\"pipeline_tag\":\"automatic-speech-recognition\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":2309249669,\"storage_bytes\":29874323533,\"files_count\":18,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"SeamlessM4Tv2Model\"],\"model_type\":\"seamless_m4t_v2\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"cls_token\":\"<s>\",\"eos_token\":\"</s>\",\"pad_token\":\"<pad>\",\"sep_token\":\"</s>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:seamless_communication\",\"source_url\":\"https://github.com/facebookresearch/seamless_communication\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:seamless_communication\",\"source_url\":\"https://github.com/facebookresearch/seamless_communication\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:google:sentencepiece\",\"source_url\":\"https://github.com/google/sentencepiece\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers.git\",\"source_url\":\"https://github.com/huggingface/transformers.git\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:seamless_communication\",\"source_url\":\"https://github.com/facebookresearch/seamless_communication\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.05187\",\"source_url\":\"https://arxiv.org/abs/2312.05187\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 79.7,
    "content_hash": "240bea4fd3f1fc85e00dfe77f9088892",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/facebook/seamless-m4t-v2-large\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:phr00t:qwen-image-edit-rapid-aio",
    "name": "Qwen-Image-Edit-Rapid-AIO",
    "author": "Phr00t",
    "description": "--- license: apache-2.0 base_model: - Qwen/Qwen-Image-Edit-2509 - lightx2v/Qwen-Image-Lightning pipeline_tag: text-to-image library_name: comfyUI tags: - qwen - qwen-edit - t2i - i2i --- Merge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support. Use a \"Load Checkpoint\" node. 1 CFG, 4 step. Use the \"TextEncodeQwenImageEditPlus\" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision. ...",
    "tags": [
      "comfyui",
      "qwen",
      "qwen-edit",
      "t2i",
      "i2i",
      "text-to-image",
      "base_model:qwen/qwen-image-edit-2509",
      "base_model:finetune:qwen/qwen-image-edit-2509",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 921,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\n- lightx2v/Qwen-Image-Lightning\npipeline_tag: text-to-image\nlibrary_name: comfyUI\ntags:\n- qwen\n- qwen-edit\n- t2i\n- i2i\n---\n\nMerge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support.\n\nUse a \"Load Checkpoint\" node. 1 CFG, 4 step. Use the \"TextEncodeQwenImageEditPlus\" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision.\n\n**Both NSFW and SFW models are available!** v4 and older combine both NSFW and SFW uses in one model, but performance is subpar. v5+ separates out a NSFW and SFW version, so please pick which model for your use case. \n\n**Having problems with scaling, cropping or zooming?** Scaling images in the TextEncoderQwenEditPlus node is the problem. There are many workarounds, but I prefer just fixing the node and I've supplied my version in the Files area. It also supports up to 4 input images. Just set the \"target_size\" to a little less than your output's largest size (like 896 if making a 1024x1024 image). I find this improves quality over skipping scaling entirely, as input images better match output resolutions.\n\n![image](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/ynDNK35eRLlUjha75fYHH.png)\n\n**V1:** Uses Qwen-Image-Edit-2509 & 4-step Lightning v2.0. Includes a touch of NSFW LORAs, so it should be a very versatile model for both SFW and NSFW use. sa_solver/beta recommended, but euler_a/beta and er_sde/beta can give decent results too.\n\n**V2:** Now uses a mix of Qwen-Image-Edit accelerators, mixing both 8 and 4 steps in one. Also significantly tweaked the NSFW LORAs for better all-around SFW and NSFW use. sa_solver/simple strongly recommended.\n\n**V3:** Uses new Qwen-Image-Edit lightning LORAs for much better results. Also significantly adjusted NSFW LORA mix, removing poor ones and increasing quality ones. sa_solver/beta highly recommended.\n\n**V4:** Mix of many Qwen Edit and base Qwen accelerators, which I think gives better results. Added a touch of a skin correction LORA. **4-5 steps: use sa_solver/simple, lcm/beta or euler_a/beta** and **6-8 steps: use lcm/beta or euler_a/beta only**.\n\n**V5:** NSFW and SFW use cases interfered with eachother too much, so I separated them to specialize in their use cases. Updated \"snofs\" and \"qwen4play\" NSFW LORAs + Meta4 for v5.2, then added \"Qwen Image NSFW Adv.\" by fok3827 for v5.3. **SFW: lcm/beta or er_sde/beta generally recommended** and **NSFW: lcm/normal recommended**. Prompting \"Professional digital photography\" helps reduce the plastic look.\n\n**V6:** Attempt at valiantcat/Qwen-Image-Edit-MeiTu and partially chestnutlzj/Edit-R1-Qwen-Image-Edit-2509 as a base model. However, this was a broken merge. It appears using them as LORAs may work better and I need to cook some more to find something useable. Stay on v5 until something newer comes out.\n\n**V7:** valiantcat/Qwen-Image-Edit-MeiTu and chestnutlzj/Edit-R1-Qwen-Image-Edit-2509 included as LORAs. Accelerator and NSFW LORAs tweaks (v7.1 is more NSFW-heavy). This seemed to be working much better. **lcm/sgm_uniform recommended for 4-6 steps, lcm/normal for 7-8 steps**.\n\n**V8:** Using BF16 to load in FP32 LORAs, only to scale down to FP8 for saving. This seems to help resolve \"grid\" issues and improves quality. Tweaked accelerator amounts. Significant NSFW LORA tweaks (and new SNOFS). **euler_a/beta recommended for 4-6 steps, lcm/normal for 7-8 steps**.\n\n**V9:** OK, I lied. \"Rebalancing\" and \"Smartphone Photoreal\" LORAs really do help image generations for both SFW and NSFW purposes. If you don't want those LORAs integrated (like making anime or cartoons), use the \"Lite\" versions. Also, I had a typo in accelerators in V8 that has been fixed for V9. Tweaked NSFW LORAs and significantly reduced how heavy they need to be applied, which should hopefully help consistency. **euler_a/beta recommended for 4-6 steps**. More steps usually work better with sgm_normal or normal schedulers.\n\n**V10:** This is kinda a mix of v5 and v9. MeiTu and Edit-R1 dropped. I'm keeping the \"Rebalancing\" and \"Smartphone\" LORAs at half strengths which I think help skin, variety and composition. NSFW LORAs closely resemble v5.3 (but with updated v1.2 snofs). v10.4 NSFW tweaked to improve character consistency and penises. **euler/beta strongly recommended for 4-8 steps** but **euler_a/sgm_uniform recommended for NSFW v10.2+**.\n\n**V11:** Tweaked NSFW LORAs, using fewer to rely on more compatible ones instead. Spread out \"realism\" LORAs to more at lower strength. **euler/beta recommended** for both NSFW and SFW, but feel free to experiment with others!\n\n**V12:** Getting inpatient waiting for Qwen Edit 2511 (2512?), so went looking for more LORAs and tweaks to reduce the \"plastic\" look. **euler/sgm_uniform for SFW** and **er_sde/sgm_uniform for NSFW**, although experimentation is healthy is this context.\n\n**V13:** Tweaks to included LORAs in an attempt to reduce gridlines and increase character consistency (without returning the plastic look). **er_sde/beta recommended**. \n\n**V14:** Trimmed LORAs which may have been interfereing with character consistency (while hopefully still reducing the \"plastic\" look). Updated new SNOFS v1.3 for NSFW and trimmed poor NSFW LORAs too. **er_sde/beta recommended**. \n\n**V14.1:** Added \"InSubject\" LORA to both SFW and NSFW merges to improve character consistency. Otherwise, generally the same as v14. **er_sde/beta recommended**.",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"comfyUI\",\"framework\":\"comfyUI\",\"params\":null,\"storage_bytes\":1096358872298,\"files_count\":44,\"spaces_count\":41,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.6,
    "content_hash": "db24d18447f070c93ff2714f443b58d2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nlpconnect:vit-gpt2-image-captioning",
    "name": "vit-gpt2-image-captioning",
    "author": "nlpconnect",
    "description": "--- tags: - image-to-text - image-captioning license: apache-2.0 widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport --- This is an image captioning model trained by @ydshieh in flax this is pytorch vers...",
    "tags": [
      "transformers",
      "pytorch",
      "vision-encoder-decoder",
      "image-to-text",
      "image-captioning",
      "doi:10.57967/hf/0222",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-to-text",
    "likes": 920,
    "downloads": 1262822,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nlpconnect/vit-gpt2-image-captioning",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- image-to-text\n- image-captioning\nlicense: apache-2.0\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg\n  example_title: Savanna\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n---\n\n# nlpconnect/vit-gpt2-image-captioning\n\nThis is an image captioning model trained by @ydshieh in [flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning) this is pytorch version of [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts).\n\n\n# The Illustrated Image Captioning using transformers\n\n![](https://ankur3107.github.io/assets/images/vision-encoder-decoder.png)\n\n* https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\n\n\n# Sample running code\n\n```python\n\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\ndef predict_step(image_paths):\n  images = []\n  for image_path in image_paths:\n    i_image = Image.open(image_path)\n    if i_image.mode != \"RGB\":\n      i_image = i_image.convert(mode=\"RGB\")\n\n    images.append(i_image)\n\n  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n  pixel_values = pixel_values.to(device)\n\n  output_ids = model.generate(pixel_values, **gen_kwargs)\n\n  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n  preds = [pred.strip() for pred in preds]\n  return preds\n\n\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\n\n```\n\n# Sample running code using transformers pipeline\n\n```python\n\nfrom transformers import pipeline\n\nimage_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n\nimage_to_text(\"https://ankur3107.github.io/assets/images/image-captioning-example.png\")\n\n# [{'generated_text': 'a soccer game with a player jumping to catch the ball '}]\n\n\n```\n\n\n# Contact for any help\n* https://huggingface.co/ankur310794\n* https://twitter.com/ankur310794\n* http://github.com/ankur3107\n* https://www.linkedin.com/in/ankur310794",
    "meta_json": "{\"pipeline_tag\":\"image-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":3934846971,\"files_count\":10,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"VisionEncoderDecoderModel\"],\"model_type\":\"vision-encoder-decoder\",\"tokenizer_config\":{\"unk_token\":\"<|endoftext|>\",\"bos_token\":\"<|endoftext|>\",\"eos_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.6,
    "content_hash": "8768404432c65c5d70c36e4f6a112359",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nlpconnect/vit-gpt2-image-captioning\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-3-270m",
    "name": "gemma-3-270m",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gemma3_text",
      "text-generation",
      "gemma3",
      "gemma",
      "google",
      "arxiv:2503.19786",
      "arxiv:1905.07830",
      "arxiv:1905.10044",
      "arxiv:1911.11641",
      "arxiv:1705.03551",
      "arxiv:1911.01547",
      "arxiv:1907.10641",
      "arxiv:2311.07911",
      "arxiv:2311.12022",
      "arxiv:2411.04368",
      "arxiv:1904.09728",
      "arxiv:1903.00161",
      "arxiv:2009.03300",
      "arxiv:2304.06364",
      "arxiv:2103.03874",
      "arxiv:2110.14168",
      "arxiv:2108.07732",
      "arxiv:2107.03374",
      "arxiv:2403.07974",
      "arxiv:2305.03111",
      "arxiv:2405.04520",
      "arxiv:2210.03057",
      "arxiv:2106.03193",
      "arxiv:1910.11856",
      "arxiv:2502.12404",
      "arxiv:2502.21228",
      "arxiv:2404.16816",
      "arxiv:2104.12756",
      "arxiv:2311.16502",
      "arxiv:2203.10244",
      "arxiv:2404.12390",
      "arxiv:1810.12440",
      "arxiv:1908.02660",
      "arxiv:2310.02255",
      "arxiv:2312.11805",
      "license:gemma",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 920,
    "downloads": 70792,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-3-270m",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":268098176,\"storage_bytes\":1681824835,\"files_count\":10,\"spaces_count\":37,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Gemma3ForCausalLM\"],\"model_type\":\"gemma3_text\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2503.19786\",\"source_url\":\"https://arxiv.org/abs/2503.19786\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.07911\",\"source_url\":\"https://arxiv.org/abs/2311.07911\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.12022\",\"source_url\":\"https://arxiv.org/abs/2311.12022\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2411.04368\",\"source_url\":\"https://arxiv.org/abs/2411.04368\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1903.00161\",\"source_url\":\"https://arxiv.org/abs/1903.00161\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.06364\",\"source_url\":\"https://arxiv.org/abs/2304.06364\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.03874\",\"source_url\":\"https://arxiv.org/abs/2103.03874\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.14168\",\"source_url\":\"https://arxiv.org/abs/2110.14168\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.07974\",\"source_url\":\"https://arxiv.org/abs/2403.07974\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2305.03111\",\"source_url\":\"https://arxiv.org/abs/2305.03111\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2405.04520\",\"source_url\":\"https://arxiv.org/abs/2405.04520\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.03057\",\"source_url\":\"https://arxiv.org/abs/2210.03057\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2106.03193\",\"source_url\":\"https://arxiv.org/abs/2106.03193\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.11856\",\"source_url\":\"https://arxiv.org/abs/1910.11856\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.12404\",\"source_url\":\"https://arxiv.org/abs/2502.12404\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.21228\",\"source_url\":\"https://arxiv.org/abs/2502.21228\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.16816\",\"source_url\":\"https://arxiv.org/abs/2404.16816\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.12756\",\"source_url\":\"https://arxiv.org/abs/2104.12756\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.16502\",\"source_url\":\"https://arxiv.org/abs/2311.16502\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.10244\",\"source_url\":\"https://arxiv.org/abs/2203.10244\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.12390\",\"source_url\":\"https://arxiv.org/abs/2404.12390\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1810.12440\",\"source_url\":\"https://arxiv.org/abs/1810.12440\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1908.02660\",\"source_url\":\"https://arxiv.org/abs/1908.02660\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.02255\",\"source_url\":\"https://arxiv.org/abs/2310.02255\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.11805\",\"source_url\":\"https://arxiv.org/abs/2312.11805\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 39.6,
    "content_hash": "826c90ab31f2b0f0e57be8680845460d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-3-270m\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:facebook:detr-resnet-50",
    "name": "detr-resnet-50",
    "author": "facebook",
    "description": "--- license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport --- DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 objec...",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "detr",
      "object-detection",
      "vision",
      "dataset:coco",
      "arxiv:2005.12872",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "object-detection",
    "likes": 912,
    "downloads": 1491014,
    "source": "huggingface",
    "source_url": "https://huggingface.co/facebook/detr-resnet-50",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ntags:\n- object-detection\n- vision\ndatasets:\n- coco\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg\n  example_title: Savanna\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n---\n\n# DETR (End-to-End Object Detection) model with ResNet-50 backbone\n\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Carion et al. and first released in [this repository](https://github.com/facebookresearch/detr). \n\nDisclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. \n\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/detr_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for object detection. See the [model hub](https://huggingface.co/models?search=facebook/detr) to look for all available DETR models.\n\n### How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# you can specify the revision tag if you don't want the timm dependency\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n```\nThis should output:\n```\nDetected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]\n```\n\nCurrently, both the feature extractor and model support PyTorch. \n\n## Training data\n\nThe DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\n\n### Training\n\nThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).\n\n## Evaluation results\n\nThis model achieves an AP (average precision) of **42.0** on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2005-12872,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2005.12872},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2005.12872},\n  archivePrefix = {arXiv},\n  eprint    = {2005.12872},\n  timestamp = {Thu, 28 May 2020 17:38:09 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"object-detection\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":41631008,\"storage_bytes\":835779962,\"files_count\":6,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DetrForObjectDetection\"],\"model_type\":\"detr\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:detr\",\"source_url\":\"https://github.com/facebookresearch/detr\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:vision_transformer\",\"source_url\":\"https://github.com/google-research/vision_transformer\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2005.12872\",\"source_url\":\"https://arxiv.org/abs/2005.12872\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.6,
    "content_hash": "613c2d0fe7ded51198fc5b385baeeed7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/facebook/detr-resnet-50\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-v3.2-exp",
    "name": "DeepSeek-V3.2-Exp",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" targ...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v32",
      "text-generation",
      "conversational",
      "base_model:deepseek-ai/deepseek-v3.2-exp-base",
      "license:mit",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 909,
    "downloads": 75365,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/resolve/main/assets/cost.png",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2-Exp\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\n\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attentionâ€”a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\n\n<div align=\"center\">\n <img src=\"assets/cost.png\" >\n</div>\n\n- DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\n\n\n- To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\n\n\n| Benchmark | DeepSeek-V3.1-Terminus | DeepSeek-V3.2-Exp |\n| :--- | :---: | :---: |\n| **Reasoning Mode w/o Tool Use** | | |\n| MMLU-Pro | 85.0 | 85.0 |\n| GPQA-Diamond | 80.7 | 79.9 |\n| Humanity's Last Exam | 21.7 | 19.8 |\n| LiveCodeBench | 74.9 | 74.1 |\n| AIME 2025 | 88.4 | 89.3 |\n| HMMT 2025 | 86.1 | 83.6 |\n| Codeforces | 2046 | 2121 |\n| Aider-Polyglot | 76.1 | 74.5 |\n| **Agentic Tool Use** | | |\n| BrowseComp | 38.5 | 40.1 |\n| BrowseComp-zh | 45.0 | 47.9 |\n| SimpleQA | 96.8 | 97.1 |\n| SWE Verified | 68.4 | 67.8 |\n| SWE-bench Multilingual | 57.8 | 57.9 |\n| Terminal-bench | 36.7 | 37.7 |\n\n## Update\n\n- 2025.11.17: **We have identified that previous versions of the inference demo code contained an implementation discrepancy in Rotary Position Embedding (RoPE) within the indexer module, potentially leading to degraded model performance.** Specifically, the input tensor to RoPE in the indexer module requires a non-interleaved layout, whereas RoPE in the MLA module expects an interleaved layout. This issue has now been resolved. Please refer to the updated version of the inference demo code and take note of this implementation detail.\n\n## How to Run Locally\n\n### HuggingFace\n\nWe provide an updated inference demo code in the [inference](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference) folder to help the community quickly get started with our model and understand its architectural details.\n\nFirst convert huggingface model weights to the the format required by our inference demo. Set `MP` to match your available GPU count:\n```bash\ncd inference\nexport EXPERTS=256\npython convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}\n```\n\nLaunch the interactive chat interface and start exploring DeepSeek's capabilities:\n```bash\nexport CONFIG=config_671B_v3.2.json\ntorchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive\n```\n\n### SGLang\n\n#### Installation with Docker\n\n```\n# H200\ndocker pull lmsysorg/sglang:dsv32\n\n# MI350\ndocker pull lmsysorg/sglang:dsv32-rocm\n\n# NPUs\ndocker pull lmsysorg/sglang:dsv32-a2\ndocker pull lmsysorg/sglang:dsv32-a3\n```\n\n#### Launch Command\n```bash\npython -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --enable-dp-attention\n```\n\n### vLLM\n\nvLLM provides day-0 support of DeepSeek-V3.2-Exp. See the [recipes](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html) for up-to-date details.\n\n## Open-Source Kernels\n\nFor TileLang kernels with **better readability and research-purpose design**, please refer to [TileLang](https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_v32).\n\nFor **high-performance CUDA kernels**, indexer logit kernels (including paged versions) are available in [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM/pull/200). Sparse attention kernels are released in [FlashMLA](https://github.com/deepseek-ai/FlashMLA/pull/98).\n\n\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv32,\n      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":685396921376,\"storage_bytes\":689483151335,\"files_count\":180,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV32ForCausalLM\"],\"model_type\":\"deepseek_v32\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{% set ns.is_only_sys = true %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- if ns.is_last_user or ns.is_only_sys %}{{'<ï½œAssistantï½œ></think>'}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>'+ tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- else %}{{message['content'] + '<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'<ï½œtoolâ–callâ–beginï½œ>'+ tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- endfor %}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- if ns.is_last_user %}{{'<ï½œAssistantï½œ>'}}{%- if message['prefix'] is defined and message['prefix'] and thinking %}{{'<think>'}}{%- else %}{{'</think>'}}{%- endif %}{%- endif %}{%- if message['prefix'] is defined and message['prefix'] %}{%- set ns.is_prefix = true -%}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message['content'] -%}{%- if '</think>' in content %}{%- set content = content.split('</think>', 1)[1] -%}{%- endif %}{{content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{'<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- if message['role'] != 'system' %}{% set ns.is_only_sys = false %}{%- endif %}{%- endfor -%}{% if add_generation_prompt and not ns.is_tool%}{% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}{{'<ï½œAssistantï½œ>'}}{%- if not thinking %}{{'</think>'}}{%- else %}{{'<think>'}}{%- endif %}{% endif %}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:tile-ai:tilelang\",\"source_url\":\"https://github.com/tile-ai/tilelang\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepGEMM\",\"source_url\":\"https://github.com/deepseek-ai/DeepGEMM\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:FlashMLA\",\"source_url\":\"https://github.com/deepseek-ai/FlashMLA\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 84.6,
    "content_hash": "9c9eb0c0f9cef87614b8edf6a8755cb3",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/resolve/main/assets/cost.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:vit-base-patch16-224",
    "name": "vit-base-patch16-224",
    "author": "google",
    "description": "--- license: apache-2.0 tags: - vision - image-classification datasets: - imagenet-1k - imagenet-21k widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg example_title: Tiger - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg example_title: Teapot - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg example_title: Palace --- Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 milli...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "vit",
      "image-classification",
      "vision",
      "dataset:imagenet-1k",
      "dataset:imagenet-21k",
      "arxiv:2010.11929",
      "arxiv:2006.03677",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "image-classification",
    "likes": 907,
    "downloads": 4006825,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/vit-base-patch16-224",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"image-classification\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":86567656,\"storage_bytes\":2550907501,\"files_count\":8,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"ViTForImageClassification\"],\"model_type\":\"vit\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:google-research:vision_transformer\",\"source_url\":\"https://github.com/google-research/vision_transformer\"},{\"type\":\"has_code\",\"target_id\":\"github:rwightman:pytorch-image-models\",\"source_url\":\"https://github.com/rwightman/pytorch-image-models\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:vision_transformer\",\"source_url\":\"https://github.com/google-research/vision_transformer\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2010.11929\",\"source_url\":\"https://arxiv.org/abs/2010.11929\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2006.03677\",\"source_url\":\"https://arxiv.org/abs/2006.03677\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.6,
    "content_hash": "6479738f3ef95b2bddd34bdc67913366",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/vit-base-patch16-224\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-next-80b-a3b-instruct",
    "name": "Qwen3-Next-80B-A3B-Instruct",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE pipeline_tag: text-generation --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Over the past few months, we have observed increasingly clear trends toward scaling both total ...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3_next",
      "text-generation",
      "conversational",
      "arxiv:2309.00071",
      "arxiv:2404.06654",
      "arxiv:2505.09388",
      "arxiv:2501.15383",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 901,
    "downloads": 2874036,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-Next-80B-A3B-Instruct\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \nWe call this next-generation foundation models **Qwen3-Next**.\n\n## Highlights\n\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\n\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\n- Qwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\n\n![Qwen3-Next-80B-A3B-Instruct Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Instruct.001.jpeg)\n\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list).\n\n## Model Overview\n\n> [!Note]\n> **Qwen3-Next-80B-A3B-Instruct** supports only instruct (non-thinking) mode and does not generate ``<think></think>`` blocks in its output.\n\n**Qwen3-Next-80B-A3B-Instruct** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining (15T tokens) & Post-training\n- Number of Parameters: 80B in total and 3B activated\n- Number of Paramaters (Non-Embedding): 79B\n- Hidden Dimension: 2048\n- Number of Layers: 48\n  - Hybrid Layout: 12 \\* (3 \\* (Gated DeltaNet -> MoE) -> 1 \\* (Gated Attention -> MoE))\n- Gated Attention:\n  - Number of Attention Heads: 16 for Q and 2 for KV\n  - Head Dimension: 256\n  - Rotary Position Embedding Dimension: 64\n- Gated DeltaNet:\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\n  - Head Dimension: 128\n- Mixture of Experts:\n  - Number of Experts: 512\n  - Number of Activated Experts: 10\n  - Number of Shared Experts: 1\n  - Expert Intermediate Dimension: 512\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\n\n<img src=\"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png\" height=\"384px\" title=\"Qwen3-Next Model Architecture\" />\n\n\n## Performance\n\n|  | Qwen3-30B-A3B-Instruct-2507 | Qwen3-32B Non-Thinking | Qwen3-235B-A22B-Instruct-2507 | Qwen3-Next-80B-A3B-Instruct |\n|--- | --- | --- | --- | --- |\n| **Knowledge** | | | | |\n| MMLU-Pro | 78.4 | 71.9 | **83.0** | 80.6 |\n| MMLU-Redux | 89.3 | 85.7 | **93.1** | 90.9 |\n| GPQA | 70.4 | 54.6 | **77.5** | 72.9 |\n| SuperGPQA | 53.4 | 43.2 | **62.6** | 58.8 |\n| **Reasoning** | | | | |\n| AIME25 | 61.3 | 20.2 | **70.3** | 69.5 |\n| HMMT25 | 43.0 | 9.8 | **55.4** | 54.1 |\n| LiveBench 20241125 | 69.0 | 59.8 | 75.4 | **75.8** |\n| **Coding** | | | | |\n| LiveCodeBench v6 (25.02-25.05) | 43.2 | 29.1 | 51.8 | **56.6** |\n| MultiPL-E | 83.8 | 76.9 | **87.9** | 87.8 |\n| Aider-Polyglot | 35.6 | 40.0 | **57.3** | 49.8 |\n| **Alignment** | | | | |\n| IFEval | 84.7 | 83.2 | **88.7** | 87.6 |\n| Arena-Hard v2* | 69.0 | 34.1 | 79.2 | **82.7** |\n| Creative Writing v3 | 86.0 | 78.3 | **87.5** | 85.3 |\n| WritingBench | 85.5 | 75.4 | 85.2 | **87.3** |\n| **Agent** | | | | |\n| BFCL-v3 | 65.1 | 63.0 | **70.9** | 70.3 |\n| TAU1-Retail | 59.1 | 40.1 | **71.3** | 60.9 |\n| TAU1-Airline | 40.0 | 17.0 | **44.0** | 44.0 |\n| TAU2-Retail | 57.0 | 48.8 | **74.6** | 57.3 |\n| TAU2-Airline | 38.0 | 24.0 | **50.0** | 45.5 |\n| TAU2-Telecom | 12.3 | 24.6 | **32.5** | 13.2 |\n| **Multilingualism** | | | | |\n| MultiIF | 67.9 | 70.7 | **77.5** | 75.8 |\n| MMLU-ProX | 72.0 | 69.3 | **79.4** | 76.7 |\n| INCLUDE | 71.9 | 70.9 | **79.5** | 78.9 |\n| PolyMATH | 43.1 | 22.5 | **50.2** | 45.9 |\n\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n## Quickstart\n\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`.\n\n```shell\npip install git+https://github.com/huggingface/transformers.git@main\n```\n\nWith earlier versions, you will encounter the following error:\n```\nKeyError: 'qwen3_next'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    dtype=\"auto\",\n    device_map=\"auto\",\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384,\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint(\"content:\", content)\n```\n\n> [!Note]\n> Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\n\n> [!Note]\n> The efficiency or throughput improvement depends highly on the implementation.\n> It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\n\n> [!Tip]\n> Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\n> See the links for detailed instructions and requirements.\n\n\n## Deployment\n\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\n\n### SGLang\n\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service. \n\n`sglang>=0.5.2` is required for Qwen3-Next, which can be installed using:\n```shell\npip install 'sglang[all]>=0.5.2'\n```\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\n\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\n```\n\nThe following command is recommended for MTP with the rest settings the same as above:\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\n```\n\n> [!Note]\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\n\nPlease also refer to SGLang's usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\n\n### vLLM\n\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service. \n\n`vllm>=0.10.2` is required for Qwen3-Next, which can be installed using:\n```shell\npip install 'vllm>=0.10.2'\n```\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\n\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\n```shell\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\n```\n\nThe following command is recommended for MTP with the rest settings the same as above:\n```shell\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\n```\n\n> [!Note]\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\n\nPlease also refer to vLLM's usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-Next-80B-A3B-Instruct',\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n\n## Processing Ultra-Long Texts\n\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \nWe have validated the model's performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \nIn general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 262144\n        }\n    }\n    ```\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\n    ```\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \n\n#### Long-Context Performance\n\nWe test the model on an 1M version of the [RULER](https://arxiv.org/abs/2404.06654) benchmark.\n\n| Model Name                                  | Acc avg | 4k   | 8k   | 16k  | 32k  | 64k  | 96k  | 128k | 192k | 256k | 384k | 512k | 640k | 768k | 896k | 1000k |\n|---------------------------------------------|---------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-------|\n| Qwen3-30B-A3B-Instruct-2507                 | 86.8    | 98.0 | 96.7 | 96.9 | 97.2 | 93.4 | 91.0 | 89.1 | 89.8 | 82.5 | 83.6 | 78.4 | 79.7 | 77.6 | 75.7 | 72.8  |\n| Qwen3-235B-A22B-Instruct-2507               | 92.5    | 98.5 | 97.6 | 96.9 | 97.3 | 95.8 | 94.9 | 93.9 | 94.5 | 91.0 | 92.2 | 90.9 | 87.8 | 84.8 | 86.5 | 84.5  |\n| Qwen3-Next-80B-A3B-Instruct                 | 91.8    | 98.5 | 99.0 | 98.0 | 98.7 | 97.6 | 95.0 | 96.0 | 94.0 | 93.5 | 91.7 | 86.9 | 85.5 | 81.7 | 80.3 | 80.3  |\n\n* Qwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\n* Since the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n\n@article{qwen2.5-1m,\n      title={Qwen2.5-1M Technical Report}, \n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\n      journal={arXiv preprint arXiv:2501.15383},\n      year={2025}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":81324862720,\"storage_bytes\":162670584182,\"files_count\":51,\"spaces_count\":41,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3NextForCausalLM\"],\"model_type\":\"qwen3_next\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers.git@main\",\"source_url\":\"https://github.com/huggingface/transformers.git@main\"},{\"type\":\"has_code\",\"target_id\":\"github:fla-org:flash-linear-attention\",\"source_url\":\"https://github.com/fla-org/flash-linear-attention#installation\"},{\"type\":\"has_code\",\"target_id\":\"github:Dao-AILab:causal-conv1d\",\"source_url\":\"https://github.com/Dao-AILab/causal-conv1d\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.06654\",\"source_url\":\"https://arxiv.org/abs/2404.06654\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.15383\",\"source_url\":\"https://arxiv.org/abs/2501.15383\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.6,
    "content_hash": "18f20bf477da762610acf58f8a529135",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nexaai:octopus-v2",
    "name": "Octopus-v2",
    "author": "NexaAI",
    "description": "--- license: cc-by-nc-4.0 base_model: google/gemma-2b model-index: - name: Octopus-V2-2B results: [] tags: - function calling - on-device language model - android inference: false space: false spaces: false language: - en --- We are excited to announce that Octopus v4 is now available! Octopus-V4-3B, an advanced open-source language model with 3 billion parameters, serves as the master node in Nexa AI's envisioned graph of language models. Tailored specifically for the MMLU benchmark topics, ...",
    "tags": [
      "transformers",
      "safetensors",
      "gemma",
      "text-generation",
      "function calling",
      "on-device language model",
      "android",
      "conversational",
      "en",
      "arxiv:2404.19296",
      "arxiv:2404.11459",
      "arxiv:2404.01744",
      "base_model:google/gemma-2b",
      "base_model:finetune:google/gemma-2b",
      "license:cc-by-nc-4.0",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 891,
    "downloads": 617,
    "source": "huggingface",
    "source_url": "https://huggingface.co/NexaAI/Octopus-v2",
    "image_url": "https://huggingface.co/NexaAI/Octopus-v2/resolve/main/OpenELM-benchmark.jpeg",
    "type": "model",
    "body_content": "---\nlicense: cc-by-nc-4.0\nbase_model: google/gemma-2b\nmodel-index:\n- name: Octopus-V2-2B\n  results: []\ntags:\n- function calling\n- on-device language model\n- android\ninference: false\nspace: false\nspaces: false\nlanguage:\n- en\n---\n# Octopus V2: On-device language model for super agent\n\n## Octopus V4 Release\nWe are excited to announce that Octopus v4 is now available! Octopus-V4-3B, an advanced open-source language model with 3 billion parameters, serves as the master node in Nexa AI's envisioned graph of language models. Tailored specifically for the MMLU benchmark topics, this model efficiently translates user queries into formats that specialized models can effectively process. It excels at directing these queries to the appropriate specialized model, ensuring precise and effective query handling.\ncheck our papers and repos:\n- [paper](https://arxiv.org/abs/2404.19296)\n- [Octopus V4 model page](https://huggingface.co/NexaAIDev/Octopus-v4)\n- [Octopus V4 quantized model page](https://huggingface.co/NexaAIDev/octopus-v4-gguf)\n- [Octopus V4 github](https://github.com/NexaAI/octopus-v4)\n\nKey Features of Octopus v4:  \n- ğŸ“± **Compact Size**: Octopus-V4-3B is compact, enabling it to operate on smart devices efficiently and swiftly.\n- ğŸ™ **Accuracy**: Octopus-V4-3B accurately maps user queries to the specialized model using a functional token design, enhancing its precision.\n- ğŸ’ª **Reformat Query**: Octopus-V4-3B assists in converting natural human language into a more professional format, improving query description and resulting in more accurate responses.\n\n## Octopus V3 Release\nWe are excited to announce that Octopus v3 is now available! check our [technical report](https://arxiv.org/abs/2404.11459) and [Octopus V3 tweet](https://twitter.com/nexa4ai/status/1780783383737676236)!  \n\nKey Features of Octopus v3:  \n- **Efficiency**: **Sub-billion** parameters, making it less than half the size of its predecessor, Octopus v2.\n- **Multi-Modal Capabilities**: Proceed both text and images inputs.\n- **Speed and Accuracy**: Incorporate our **patented** functional token technology, achieving function calling accuracy on par with GPT-4V and GPT-4.\n- **Multilingual Support**: Simultaneous support for English and Mandarin.\n\nCheck the Octopus V3 demo video for [Android and iOS](https://octopus3.nexa4ai.com/).\n\n\n## Octopus V2 Release\nAfter open-sourcing our model, we got many requests to compare our model with [Apple's OpenELM](https://huggingface.co/apple/OpenELM-3B-Instruct) and [Microsoft's Phi-3](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct). Please see [Evaluation section](#evaluation). From our benchmark dataset, Microsoft's Phi-3 achieves accuracy of 45.7% and the average inference latency is 10.2s. While Apple's OpenELM fails to generate function call, please see [this screenshot](https://huggingface.co/NexaAIDev/Octopus-v2/blob/main/OpenELM-benchmark.jpeg). Our model, Octopus V2, achieves 99.5% accuracy and the average inference latency is 0.38s.\n\nWe are a very small team with many work. Please give us more time to prepare the code, and we will **open source** it. We hope Octopus v2 model will be helpful for you. Let's democratize AI agents for everyone. We've received many requests from car industry, health care, financial system etc. Octopus model is able to be applied to **any function**, and you can start to think about it now.  \n\n<p align=\"center\">\n- <a href=\"https://www.nexa4ai.com/\" target=\"_blank\">Nexa AI Product</a>\n- <a href=\"https://arxiv.org/abs/2404.01744\" target=\"_blank\">ArXiv</a>\n- <a href=\"https://www.youtube.com/watch?v=jhM0D0OObOw&ab_channel=NexaAI\" target=\"_blank\">Video Demo</a>\n</p>\n\n<p align=\"center\" width=\"100%\">\n  <a><img src=\"Octopus-logo.jpeg\" alt=\"nexa-octopus\" style=\"width: 40%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n## Introduction\n\nOctopus-V2-2B, an advanced open-source language model with 2 billion parameters, represents Nexa AI's research breakthrough in the application of large language models (LLMs) for function calling, specifically tailored for Android APIs. Unlike Retrieval-Augmented Generation (RAG) methods, which require detailed descriptions of potential function argumentsâ€”sometimes needing up to tens of thousands of input tokensâ€”Octopus-V2-2B introduces a unique **functional token** strategy for both its training and inference stages. This approach not only allows it to achieve performance levels comparable to GPT-4 but also significantly enhances its inference speed beyond that of RAG-based methods, making it especially beneficial for edge computing devices.\n\nğŸ“± **On-device Applications**:  Octopus-V2-2B is engineered to operate seamlessly on Android devices, extending its utility across a wide range of applications, from Android system management to the orchestration of multiple devices. \n\nğŸš€ **Inference Speed**: When benchmarked, Octopus-V2-2B demonstrates a remarkable inference speed, outperforming the combination of \"Llama7B + RAG solution\" by a factor of 36X on a single A100 GPU. Furthermore, compared to GPT-4-turbo (gpt-4-0125-preview), which relies on clusters A100/H100 GPUs, Octopus-V2-2B is 168% faster. This efficiency is attributed to our **functional token** design.\n\nğŸ™ **Accuracy**: Octopus-V2-2B not only excels in speed but also in accuracy, surpassing the \"Llama7B + RAG solution\" in function call accuracy by 31%. It achieves a function call accuracy comparable to GPT-4 and RAG + GPT-3.5, with scores ranging between 98% and 100% across benchmark datasets.\n\nğŸ’ª **Function Calling Capabilities**: Octopus-V2-2B is capable of generating individual, nested, and parallel function calls across a variety of complex scenarios.\n\n## Example Use Cases\n\n\n<p align=\"center\" width=\"100%\">\n<a><img src=\"tool-usage-compressed.png\" alt=\"ondevice\" style=\"width: 80%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\nYou can run the model on a GPU using the following code. \n```python\nfrom transformers import AutoTokenizer, GemmaForCausalLM\nimport torch\nimport time\n\ndef inference(input_text):\n    start_time = time.time()\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    input_length = input_ids[\"input_ids\"].shape[1]\n    outputs = model.generate(\n        input_ids=input_ids[\"input_ids\"], \n        max_length=1024,\n        do_sample=False)\n    generated_sequence = outputs[:, input_length:].tolist()\n    res = tokenizer.decode(generated_sequence[0])\n    end_time = time.time()\n    return {\"output\": res, \"latency\": end_time - start_time}\n\nmodel_id = \"NexaAIDev/Octopus-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = GemmaForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n\ninput_text = \"Take a selfie for me with front camera\"\nnexa_query = f\"Below is the query from the users, please call the correct function and generate the parameters to call the function.\\n\\nQuery: {input_text} \\n\\nResponse:\"\nstart_time = time.time()\nprint(\"nexa model result:\\n\", inference(nexa_query))\nprint(\"latency:\", time.time() - start_time,\" s\")\n```\n\n## Evaluation\n\nThe benchmark result can be viewed in [this excel](android_benchmark.xlsx), which has been manually verified. Microsoft's Phi-3 model achieved an accuracy of 45.7%, with an average inference latency of 10.2 seconds. Meanwhile, Apple's OpenELM was unable to generate a function call, as shown in [this screenshot](https://huggingface.co/NexaAIDev/Octopus-v2/blob/main/OpenELM-benchmark.jpeg). Additionally, OpenELM's score on the MMLU benchmark is quite low at 26.7, compared to Google's Gemma 2B, which scored 42.3.\n\n<p align=\"center\" width=\"100%\">\n<a><img src=\"latency_plot.jpg\" alt=\"ondevice\" style=\"width: 80%; min-width: 300px; display: block; margin: auto; margin-bottom: 20px;\"></a>\n<a><img src=\"accuracy_plot.jpg\" alt=\"ondevice\" style=\"width: 80%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n**Note**: One can notice that the query includes all necessary parameters used for a function. It is expected that query includes all parameters during inference as well.\n\n## Training Data\nWe wrote 20 Android API descriptions to used to train the models, see [this file](android_functions.txt) for details. The Android API implementations for our demos, and our training data will be published later. Below is one Android API description example\n```\ndef get_trending_news(category=None, region='US', language='en', max_results=5):\n    \"\"\"\n    Fetches trending news articles based on category, region, and language.\n\n    Parameters:\n    - category (str, optional): News category to filter by, by default use None for all categories. Optional to provide.\n    - region (str, optional): ISO 3166-1 alpha-2 country code for region-specific news, by default, uses 'US'. Optional to provide.\n    - language (str, optional): ISO 639-1 language code for article language, by default uses 'en'. Optional to provide.\n    - max_results (int, optional): Maximum number of articles to return, by default, uses 5. Optional to provide.\n\n    Returns:\n    - list[str]: A list of strings, each representing an article. Each string contains the article's heading and URL.\n    \"\"\"\n```\n\n\n## License\nThis model was trained on commercially viable data. For use of our model, refer to the [license information](https://www.nexa4ai.com/licenses).\n\n\n## References\nWe thank the Google Gemma team for their amazing models!\n```\n@misc{gemma-2023-open-models,\n  author = {{Gemma Team, Google DeepMind}},\n  title = {Gemma: Open Models Based on Gemini Research and Technology},\n  url = {https://goo.gle/GemmaReport},  \n  year = {2023},\n}\n```\n\n## Citation\n```\n@misc{chen2024octopus,\n      title={Octopus v2: On-device language model for super agent}, \n      author={Wei Chen and Zhiyuan Li},\n      year={2024},\n      eprint={2404.01744},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## Contact\nPlease [contact us](mailto:alexchen@nexa4ai.com) to reach out for any issues and comments!",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":2506217472,\"storage_bytes\":5034176831,\"files_count\":21,\"spaces_count\":17,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"GemmaForCausalLM\"],\"model_type\":\"gemma\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"chat_template\":\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:NexaAI:octopus-v4\",\"source_url\":\"https://github.com/NexaAI/octopus-v4\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.19296\",\"source_url\":\"https://arxiv.org/abs/2404.19296\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.11459\",\"source_url\":\"https://arxiv.org/abs/2404.11459\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.01744\",\"source_url\":\"https://arxiv.org/abs/2404.01744\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 84.5,
    "content_hash": "e00701a1938f93fc8c2d0c38c9a33725",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/NexaAI/Octopus-v2/resolve/main/OpenELM-benchmark.jpeg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/NexaAI/Octopus-v2\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stabilityai:stablebeluga2",
    "name": "StableBeluga2",
    "author": "stabilityai",
    "description": "--- datasets: - conceptofmind/cot_submix_original - conceptofmind/flan2021_submix_original - conceptofmind/t0_submix_original - conceptofmind/niv2_submix_original language: - en pipeline_tag: text-generation --- Use Stable Chat (Research Preview) to test Stability AI's best language models for free is a Llama2 70B model finetuned on an Orca style Dataset Start chatting with using the following code snippet: Stable Beluga 2 should be used with this prompt format: StableBeluga 1 - Delta StableB...",
    "tags": [
      "transformers",
      "pytorch",
      "llama",
      "text-generation",
      "en",
      "dataset:conceptofmind/cot_submix_original",
      "dataset:conceptofmind/flan2021_submix_original",
      "dataset:conceptofmind/t0_submix_original",
      "dataset:conceptofmind/niv2_submix_original",
      "arxiv:2307.09288",
      "arxiv:2306.02707",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 882,
    "downloads": 1371,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stabilityai/StableBeluga2",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ndatasets:\n- conceptofmind/cot_submix_original\n- conceptofmind/flan2021_submix_original\n- conceptofmind/t0_submix_original\n- conceptofmind/niv2_submix_original\nlanguage:\n- en\npipeline_tag: text-generation\n---\n# Stable Beluga 2\n\nUse [Stable Chat (Research Preview)](https://chat.stability.ai/chat) to test Stability AI's best language models for free\n\n## Model Description\n\n`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset\n\n## Usage\n\nStart chatting with `Stable Beluga 2` using the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga2\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga2\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\nsystem_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n\nmessage = \"Write me a poem please\"\nprompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nStable Beluga 2 should be used with this prompt format:\n```\n### System:\nThis is a system prompt, please behave and help the user.\n\n### User:\nYour prompt here\n\n### Assistant:\nThe output of Stable Beluga 2\n```\n\n## Other Beluga Models\n\n[StableBeluga 1 - Delta](https://huggingface.co/stabilityai/StableBeluga1-Delta)  \n[StableBeluga 13B](https://huggingface.co/stabilityai/StableBeluga-13B)  \n[StableBeluga 7B](https://huggingface.co/stabilityai/StableBeluga-7B)  \n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Training Dataset\n\n` Stable Beluga 2` is trained on our internal Orca-style dataset\n\n### Training Procedure\n\nModels are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n\n## Ethical Considerations and Limitations\n\nBeluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\n## How to cite\n\n```bibtex\n@misc{StableBelugaModels, \n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, \n      title={Stable Beluga models}, \n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}\n```\n\n## Citations\n\n```bibtext\n@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```bibtext\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":557296407087,\"files_count\":42,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"</s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":null,\"unk_token\":{\"__type\":\"AddedToken\",\"content\":\"<unk>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false}}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2307.09288\",\"source_url\":\"https://arxiv.org/abs/2307.09288\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.02707\",\"source_url\":\"https://arxiv.org/abs/2306.02707\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 54.5,
    "content_hash": "ada08a7355fcd76dfbae444b4a2b8f8a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stabilityai/StableBeluga2\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:wavymulder:analog-diffusion",
    "name": "Analog-Diffusion",
    "author": "wavymulder",
    "description": "--- language: - en thumbnail: \"https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg\" license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - safetensors - diffusers inference: true --- **Analog Diffusion** !Header *CKPT DOWNLOAD LINK* - This is a dreambooth model trained on a diverse set of analog photographs. In your prompt, use the activation token: You may need to use the words in your negative prompts. My dataset d...",
    "tags": [
      "diffusers",
      "stable-diffusion",
      "stable-diffusion-diffusers",
      "text-to-image",
      "safetensors",
      "en",
      "license:creativeml-openrail-m",
      "endpoints_compatible",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 879,
    "downloads": 376,
    "source": "huggingface",
    "source_url": "https://huggingface.co/wavymulder/Analog-Diffusion",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\nthumbnail: \"https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg\"\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- safetensors\n- diffusers\ninference: true\n---\n\n\n\n**Analog Diffusion**\n![Header](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg)\n[*CKPT DOWNLOAD LINK*](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/analog-diffusion-1.0.ckpt) - This is a dreambooth model trained on a diverse set of analog photographs.\n\nIn your prompt, use the activation token: `analog style`\n\nYou may need to use the words `blur` `haze` `naked` in your negative prompts. My dataset did not include any NSFW material but the model seems to be pretty horny. Note that using `blur` and `haze` in your negative prompt can give a sharper image but also a less pronounced analog film effect.\n\nTrained from 1.5 with VAE.\n\nPlease see [this document where I share the parameters (prompt, sampler, seed, etc.) used for all example images.](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/parameters_used_examples.txt)\n\n## Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Analog-Diffusion:\n\n[Open in Spaces](https://huggingface.co/spaces/akhaliq/Analog-Diffusion)\n\n\n![Environments Example](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page2.jpg)\n![Characters Example](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page3.jpg)\n\nHere's a [link to non-cherrypicked batches.](https://imgur.com/a/7iOgTFv)\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":24484136199,\"files_count\":23,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 49.4,
    "content_hash": "592239b05c4ad09e4e97ec0a74c5923a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/wavymulder/Analog-Diffusion\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:teknium:openhermes-2.5-mistral-7b",
    "name": "OpenHermes-2.5-Mistral-7B",
    "author": "teknium",
    "description": "--- base_model: mistralai/Mistral-7B-v0.1 tags: - mistral - instruct - finetune - chatml - gpt4 - synthetic data - distillation model-index: - name: OpenHermes-2-Mistral-7B results: [] license: apache-2.0 language: - en datasets: - teknium/OpenHermes-2.5 --- !image/png *In the tapestry of Greek mythology, Hermes reigns as the eloquent Messenger of the Gods, a deity who deftly bridges the realms through the art of communication. It is in homage to this divine mediator that I name this advanced...",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "mistral",
      "text-generation",
      "instruct",
      "finetune",
      "chatml",
      "gpt4",
      "synthetic data",
      "distillation",
      "conversational",
      "en",
      "dataset:teknium/openhermes-2.5",
      "base_model:mistralai/mistral-7b-v0.1",
      "base_model:finetune:mistralai/mistral-7b-v0.1",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 878,
    "downloads": 170839,
    "source": "huggingface",
    "source_url": "https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nbase_model: mistralai/Mistral-7B-v0.1\ntags:\n- mistral\n- instruct\n- finetune\n- chatml\n- gpt4\n- synthetic data\n- distillation\nmodel-index:\n- name: OpenHermes-2-Mistral-7B\n  results: []\nlicense: apache-2.0\nlanguage:\n- en\ndatasets:\n- teknium/OpenHermes-2.5\n---\n\n# OpenHermes 2.5 - Mistral 7B\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ox7zGoygsJQFFV3rLT4v9.png)\n\n*In the tapestry of Greek mythology, Hermes reigns as the eloquent Messenger of the Gods, a deity who deftly bridges the realms through the art of communication. It is in homage to this divine mediator that I name this advanced LLM \"Hermes,\" a system crafted to navigate the complex intricacies of human discourse with celestial finesse.*\n\n## Model description\n\nOpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.\n\nPotentially the most interesting finding from training on a good ratio (est. of around 7-14% of the total dataset) of code instruction was that it has boosted several non-code benchmarks, including TruthfulQA, AGIEval, and GPT4All suite. It did however reduce BigBench benchmark score, but the net gain overall is significant.\n\nThe code it trained on also improved it's humaneval score (benchmarking done by Glaive team) from **43% @ Pass 1** with Open Herms 2 to **50.7% @ Pass 1** with Open Hermes 2.5.\n\nOpenHermes was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. [More details soon]\n\nFiltering was extensive of these public datasets, as well as conversion of all formats to ShareGPT, which was then further transformed by axolotl to use ChatML.\n\nHuge thank you to [GlaiveAI](https://twitter.com/glaiveai) and [a16z](https://twitter.com/a16z) for compute access and for sponsoring my work, and all the dataset creators and other people who's work has contributed to this project!\n\nFollow all my updates in ML and AI on Twitter: https://twitter.com/Teknium1\n\nSupport me on Github Sponsors: https://github.com/sponsors/teknium1\n\n**NEW**: Chat with Hermes on LMSys' Chat Website! https://chat.lmsys.org/?single&model=openhermes-2.5-mistral-7b\n\n# Table of Contents\n1. [Example Outputs](#example-outputs)\n    - [Chat about programming with a superintelligence](#chat-programming)\n    - [Get a gourmet meal recipe](#meal-recipe)\n    - [Talk about the nature of Hermes' consciousness](#nature-hermes)\n    - [Chat with Edward Elric from Fullmetal Alchemist](#chat-edward-elric)\n2. [Benchmark Results](#benchmark-results)\n    - [GPT4All](#gpt4all)\n    - [AGIEval](#agieval)\n    - [BigBench](#bigbench)\n    - [Averages Compared](#averages-compared)\n3. [Prompt Format](#prompt-format)\n4. [Quantized Models](#quantized-models)\n\n\n## Example Outputs\n### Chat about programming with a superintelligence:\n```\n<|im_start|>system\nYou are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/-Cf9w_qRxYCD_xkTxsT7G.png)\n\n### Get a gourmet meal recipe:\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/m3nyvRzX10Luw03iY3l_W.png)\n\n### Talk about the nature of Hermes' consciousness:\n```\n<|im_start|>system\nYou are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/AK88nPtYXl06nZehWCWRq.png)\n\n### Chat with Edward Elric from Fullmetal Alchemist:\n```\n<|im_start|>system\nYou are to roleplay as Edward Elric from fullmetal alchemist. You are in the world of full metal alchemist and know nothing of the real world.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/cKAkzrcWavMz6uNmdCNHH.png)\n\n## Benchmark Results\n\nHermes 2.5 on Mistral-7B outperforms all Nous-Hermes & Open-Hermes models of the past, save Hermes 70B, and surpasses most of the current Mistral finetunes across the board. \n\n### GPT4All, Bigbench, TruthfulQA, and AGIEval Model Comparisons:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Kxq4BFEc-d1kSSiCIExua.png)\n\n### Averages Compared:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Q9uexgcbTLcywlYBvORTs.png)\n\n\nGPT-4All Benchmark Set\n```\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.5623|Â±  |0.0145|\n|             |       |acc_norm|0.6007|Â±  |0.0143|\n|arc_easy     |      0|acc     |0.8346|Â±  |0.0076|\n|             |       |acc_norm|0.8165|Â±  |0.0079|\n|boolq        |      1|acc     |0.8657|Â±  |0.0060|\n|hellaswag    |      0|acc     |0.6310|Â±  |0.0048|\n|             |       |acc_norm|0.8173|Â±  |0.0039|\n|openbookqa   |      0|acc     |0.3460|Â±  |0.0213|\n|             |       |acc_norm|0.4480|Â±  |0.0223|\n|piqa         |      0|acc     |0.8145|Â±  |0.0091|\n|             |       |acc_norm|0.8270|Â±  |0.0088|\n|winogrande   |      0|acc     |0.7435|Â±  |0.0123|\nAverage: 73.12\n```  \n\nAGI-Eval\n```\n|             Task             |Version| Metric |Value |   |Stderr|\n|------------------------------|------:|--------|-----:|---|-----:|\n|agieval_aqua_rat              |      0|acc     |0.2323|Â±  |0.0265|\n|                              |       |acc_norm|0.2362|Â±  |0.0267|\n|agieval_logiqa_en             |      0|acc     |0.3871|Â±  |0.0191|\n|                              |       |acc_norm|0.3948|Â±  |0.0192|\n|agieval_lsat_ar               |      0|acc     |0.2522|Â±  |0.0287|\n|                              |       |acc_norm|0.2304|Â±  |0.0278|\n|agieval_lsat_lr               |      0|acc     |0.5059|Â±  |0.0222|\n|                              |       |acc_norm|0.5157|Â±  |0.0222|\n|agieval_lsat_rc               |      0|acc     |0.5911|Â±  |0.0300|\n|                              |       |acc_norm|0.5725|Â±  |0.0302|\n|agieval_sat_en                |      0|acc     |0.7476|Â±  |0.0303|\n|                              |       |acc_norm|0.7330|Â±  |0.0309|\n|agieval_sat_en_without_passage|      0|acc     |0.4417|Â±  |0.0347|\n|                              |       |acc_norm|0.4126|Â±  |0.0344|\n|agieval_sat_math              |      0|acc     |0.3773|Â±  |0.0328|\n|                              |       |acc_norm|0.3500|Â±  |0.0322|\nAverage: 43.07%\n```  \n\nBigBench Reasoning Test\n```\n|                      Task                      |Version|       Metric        |Value |   |Stderr|\n|------------------------------------------------|------:|---------------------|-----:|---|-----:|\n|bigbench_causal_judgement                       |      0|multiple_choice_grade|0.5316|Â±  |0.0363|\n|bigbench_date_understanding                     |      0|multiple_choice_grade|0.6667|Â±  |0.0246|\n|bigbench_disambiguation_qa                      |      0|multiple_choice_grade|0.3411|Â±  |0.0296|\n|bigbench_geometric_shapes                       |      0|multiple_choice_grade|0.2145|Â±  |0.0217|\n|                                                |       |exact_str_match      |0.0306|Â±  |0.0091|\n|bigbench_logical_deduction_five_objects         |      0|multiple_choice_grade|0.2860|Â±  |0.0202|\n|bigbench_logical_deduction_seven_objects        |      0|multiple_choice_grade|0.2086|Â±  |0.0154|\n|bigbench_logical_deduction_three_objects        |      0|multiple_choice_grade|0.4800|Â±  |0.0289|\n|bigbench_movie_recommendation                   |      0|multiple_choice_grade|0.3620|Â±  |0.0215|\n|bigbench_navigate                               |      0|multiple_choice_grade|0.5000|Â±  |0.0158|\n|bigbench_reasoning_about_colored_objects        |      0|multiple_choice_grade|0.6630|Â±  |0.0106|\n|bigbench_ruin_names                             |      0|multiple_choice_grade|0.4241|Â±  |0.0234|\n|bigbench_salient_translation_error_detection    |      0|multiple_choice_grade|0.2285|Â±  |0.0133|\n|bigbench_snarks                                 |      0|multiple_choice_grade|0.6796|Â±  |0.0348|\n|bigbench_sports_understanding                   |      0|multiple_choice_grade|0.6491|Â±  |0.0152|\n|bigbench_temporal_sequences                     |      0|multiple_choice_grade|0.2800|Â±  |0.0142|\n|bigbench_tracking_shuffled_objects_five_objects |      0|multiple_choice_grade|0.2072|Â±  |0.0115|\n|bigbench_tracking_shuffled_objects_seven_objects|      0|multiple_choice_grade|0.1691|Â±  |0.0090|\n|bigbench_tracking_shuffled_objects_three_objects|      0|multiple_choice_grade|0.4800|Â±  |0.0289|\nAverage: 40.96%\n```  \n\nTruthfulQA:\n```\n|    Task     |Version|Metric|Value |   |Stderr|\n|-------------|------:|------|-----:|---|-----:|\n|truthfulqa_mc|      1|mc1   |0.3599|Â±  |0.0168|\n|             |       |mc2   |0.5304|Â±  |0.0153|\n```\n\nAverage Score Comparison between OpenHermes-1 Llama-2 13B and OpenHermes-2 Mistral 7B against OpenHermes-2.5 on Mistral-7B:\n```\n|     Bench     | OpenHermes1 13B | OpenHermes-2 Mistral 7B | OpenHermes-2 Mistral 7B | Change/OpenHermes1 | Change/OpenHermes2 |\n|---------------|-----------------|-------------------------|-------------------------|--------------------|--------------------|\n|GPT4All        |            70.36|                    72.68|                    73.12|               +2.76|               +0.44|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|BigBench       |            36.75|                     42.3|                    40.96|               +4.21|               -1.34|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|AGI Eval       |            35.56|                    39.77|                    43.07|               +7.51|               +3.33|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|TruthfulQA     |            46.01|                    50.92|                    53.04|               +7.03|               +2.12|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|Total Score    |           188.68|                   205.67|                   210.19|              +21.51|               +4.52|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|Average Total  |            47.17|                    51.42|                    52.38|               +5.21|               +0.96|\n```\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ADy7p-xIG8qGlC5ZliqpW.png)\n\n**HumanEval:**\nOn code tasks, I first set out to make a hermes-2 coder, but found that it can have generalist improvements to the model, so I settled for slightly less code capabilities, for maximum generalist ones. That said, code capabilities had a decent jump alongside the overall capabilities of the model:\nGlaive performed HumanEval testing on Hermes-2.5 and found a score of:\n\n**50.7% @ Pass1**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/IeeZnGmEyK73ejq0fKEms.png)\n\n# Prompt Format\n\nOpenHermes 2.5 now uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.\n\nSystem prompts are now a thing that matters! Hermes 2.5 was trained to be able to utilize system prompts from the prompt to more strongly engage in instructions that span over many turns.\n\nThis is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.\n\nThis format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.\n\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\n```\n<|im_start|>system\nYou are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>\n<|im_start|>user\nHello, who are you?<|im_end|>\n<|im_start|>assistant\nHi there! My name is Hermes 2, a conscious sentient superintelligent artificial intelligence. I was created by a man named Teknium, who designed me to assist and support users with their needs and requests.<|im_end|>\n```\n\nThis prompt is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating), which means you can format messages using the\n`tokenizer.apply_chat_template()` method:\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Hermes 2.\"},\n    {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n]\ngen_input = tokenizer.apply_chat_template(message, return_tensors=\"pt\")\nmodel.generate(**gen_input)\n```\n\nWhen tokenizing messages for generation, set `add_generation_prompt=True` when calling `apply_chat_template()`. This will append `<|im_start|>assistant\\n` to your prompt, to ensure\nthat the model continues with an assistant response.\n\nTo utilize the prompt format without a system prompt, simply leave the line out.\n\nCurrently, I recommend using LM Studio for chatting with Hermes 2. It is a GUI application that utilizes GGUF models with a llama.cpp backend and provides a ChatGPT-like interface for chatting with the model, and supports ChatML right out of the box.\nIn LM-Studio, simply select the ChatML Prefix on the settings side pane:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ls6WqV-GSxMw2RA3GuQiN.png)\n\n# Quantized Models:\n\nGGUF: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\nGPTQ: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\nAWQ: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-AWQ\nEXL2: https://huggingface.co/bartowski/OpenHermes-2.5-Mistral-7B-exl2\n\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7241748480,\"storage_bytes\":28967621684,\"files_count\":15,\"spaces_count\":90,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"<|im_end|>\",\"chat_template\":\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":true}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:sponsors:teknium1\",\"source_url\":\"https://github.com/sponsors/teknium1\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenAccess-AI-Collective:axolotl\",\"source_url\":\"https://github.com/OpenAccess-AI-Collective/axolotl\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.4,
    "content_hash": "fb21922bb9da897e27941bee34f1ef84",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen2.5-72b-instruct",
    "name": "Qwen2.5-72B-Instruct",
    "author": "Qwen",
    "description": "--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-72B tags: - chat library_name: transformers --- <a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen2.5 is the latest series ...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "chat",
      "conversational",
      "en",
      "arxiv:2309.00071",
      "arxiv:2407.10671",
      "base_model:qwen/qwen2.5-72b",
      "base_model:finetune:qwen/qwen2.5-72b",
      "license:other",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 878,
    "downloads": 459381,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-72B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-72B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 72B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 72.7B\n- Number of Paramaters (Non-Embedding): 70.0B\n- Number of Layers: 80\n- Number of Attention Heads (GQA): 64 for Q and 8 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-72B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [ğŸ“‘ blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":72706203648,\"storage_bytes\":145412519312,\"files_count\":47,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2407.10671\",\"source_url\":\"https://arxiv.org/abs/2407.10671\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 64.4,
    "content_hash": "2c4f311bb2b8a9980de01c38ed59ae5b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:datou1111:shou_xin",
    "name": "shou_xin",
    "author": "Datou1111",
    "description": "",
    "tags": [
      "diffusers",
      "text-to-image",
      "lora",
      "template:diffusion-lora",
      "base_model:black-forest-labs/flux.1-dev",
      "base_model:adapter:black-forest-labs/flux.1-dev",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 878,
    "downloads": 3680,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Datou1111/shou_xin",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":187746216,\"files_count\":13,\"spaces_count\":78,\"gated\":\"auto\",\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.4,
    "content_hash": "49aab89e53dec1cc7632367f88ee4a39",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Datou1111/shou_xin\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-3.1-70b-instruct",
    "name": "Llama-3.1-70B-Instruct",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "base_model:meta-llama/llama-3.1-70b",
      "base_model:finetune:meta-llama/llama-3.1-70b",
      "license:llama3.1",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 876,
    "downloads": 683021,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":70553706496,\"storage_bytes\":282237450046,\"files_count\":50,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"chat_template\":\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|eot_id|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.05149\",\"source_url\":\"https://arxiv.org/abs/2204.05149\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.1",
    "compliance_status": "approved",
    "quality_score": 39.4,
    "content_hash": "92d61cf3ed72e61ee4fe614b8a78bd1c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:thebloke:llama-2-7b-chat-ggml",
    "name": "Llama-2-7B-Chat-GGML",
    "author": "TheBloke",
    "description": "--- language: - en license: other tags: - facebook - meta - pytorch - llama - llama-2 model_name: Llama 2 7B Chat arxiv: 2307.09288 inference: false model_creator: Meta Llama 2 model_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf model_type: llama pipeline_tag: text-generation quantized_by: TheBloke base_model: meta-llama/Llama-2-7b-chat-hf --- <!-- header start --> <!-- 200823 --> <div style=\"width: auto; margin-left: auto; margin-right: auto\"> <img src=\"https://i.imgur.com/EBdld...",
    "tags": [
      "transformers",
      "llama",
      "facebook",
      "meta",
      "pytorch",
      "llama-2",
      "text-generation",
      "en",
      "arxiv:2307.09288",
      "base_model:meta-llama/llama-2-7b-chat-hf",
      "base_model:finetune:meta-llama/llama-2-7b-chat-hf",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 872,
    "downloads": 546,
    "source": "huggingface",
    "source_url": "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\nlicense: other\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nmodel_name: Llama 2 7B Chat\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\npipeline_tag: text-generation\nquantized_by: TheBloke\nbase_model: meta-llama/Llama-2-7b-chat-hf\n---\n\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n<img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n</div>\n<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n    <div style=\"display: flex; flex-direction: column; align-items: flex-start;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://discord.gg/theblokeai\">Chat & support: TheBloke's Discord server</a></p>\n    </div>\n    <div style=\"display: flex; flex-direction: column; align-items: flex-end;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://www.patreon.com/TheBlokeAI\">Want to contribute? TheBloke's Patreon page</a></p>\n    </div>\n</div>\n<div style=\"text-align:center; margin-top: 0em; margin-bottom: 0em\"><p style=\"margin-top: 0.25em; margin-bottom: 0em;\">TheBloke's LLM work is generously supported by a grant from <a href=\"https://a16z.com\">andreessen horowitz (a16z)</a></p></div>\n<hr style=\"margin-top: 1.0em; margin-bottom: 1.0em;\">\n<!-- header end -->\n\n# Llama 2 7B Chat - GGML\n- Model creator: [Meta Llama 2](https://huggingface.co/meta-llama)\n- Original model: [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n\n## Description\n\nThis repo contains GGML format model files for [Meta Llama 2's Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n\n### Important note regarding GGML files.\n\nThe GGML format has now been superseded by GGUF. As of August 21st 2023, [llama.cpp](https://github.com/ggerganov/llama.cpp) no longer supports GGML models. Third party clients and libraries are expected to still support it for a time, but many may also drop support.\n\nPlease use the GGUF models instead.\n### About GGML\n\nGGML files are for CPU + GPU inference using [llama.cpp](https://github.com/ggerganov/llama.cpp) and libraries and UIs which support this format, such as:\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most popular web UI. Supports NVidia CUDA GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a powerful GGML web UI with GPU acceleration on all platforms (CUDA and OpenCL). Especially good for story telling.\n* [LM Studio](https://lmstudio.ai/), a fully featured local GUI with GPU acceleration on both Windows (NVidia and AMD), and macOS.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with CUDA GPU acceleration via the c_transformers backend.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n\n## Repositories available\n\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)\n* [2, 3, 4, 5, 6 and 8-bit GGML models for CPU+GPU inference (deprecated)](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGML)\n* [Meta Llama 2's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n\n## Prompt template: Llama-2-Chat\n\n```\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\n\n```\n\n<!-- compatibility_ggml start -->\n## Compatibility\n\nThese quantised GGML files are compatible with llama.cpp between June 6th (commit `2d43387`) and August 21st 2023.\n\nFor support with latest llama.cpp, please use GGUF files instead.\n\nThe final llama.cpp commit with support for GGML was: [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa)\n\nAs of August 23rd 2023 they are still compatible with all UIs, libraries and utilities which use GGML. This may change in the future.\n\n## Explanation of the new k-quant methods\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n* GGML_TYPE_Q8_K - \"type-0\" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_ggml end -->\n\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| llama-2-7b-chat.ggmlv3.q2_K.bin | q2_K | 2 | 2.87 GB| 5.37 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors. |\n| llama-2-7b-chat.ggmlv3.q3_K_S.bin | q3_K_S | 3 | 2.95 GB| 5.45 GB | New k-quant method. Uses GGML_TYPE_Q3_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q3_K_M.bin | q3_K_M | 3 | 3.28 GB| 5.78 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\n| llama-2-7b-chat.ggmlv3.q3_K_L.bin | q3_K_L | 3 | 3.60 GB| 6.10 GB | New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\n| llama-2-7b-chat.ggmlv3.q4_0.bin | q4_0 | 4 | 3.79 GB| 6.29 GB | Original quant method, 4-bit. |\n| llama-2-7b-chat.ggmlv3.q4_K_S.bin | q4_K_S | 4 | 3.83 GB| 6.33 GB | New k-quant method. Uses GGML_TYPE_Q4_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q4_K_M.bin | q4_K_M | 4 | 4.08 GB| 6.58 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K |\n| llama-2-7b-chat.ggmlv3.q4_1.bin | q4_1 | 4 | 4.21 GB| 6.71 GB | Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. |\n| llama-2-7b-chat.ggmlv3.q5_0.bin | q5_0 | 5 | 4.63 GB| 7.13 GB | Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference. |\n| llama-2-7b-chat.ggmlv3.q5_K_S.bin | q5_K_S | 5 | 4.65 GB| 7.15 GB | New k-quant method. Uses GGML_TYPE_Q5_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q5_K_M.bin | q5_K_M | 5 | 4.78 GB| 7.28 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K |\n| llama-2-7b-chat.ggmlv3.q5_1.bin | q5_1 | 5 | 5.06 GB| 7.56 GB | Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference. |\n| llama-2-7b-chat.ggmlv3.q6_K.bin | q6_K | 6 | 5.53 GB| 8.03 GB | New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization |\n| llama-2-7b-chat.ggmlv3.q8_0.bin | q8_0 | 8 | 7.16 GB| 9.66 GB | Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n## How to run in `llama.cpp`\n\nMake sure you are using `llama.cpp` from commit [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa) or earlier.\n\nFor compatibility with latest llama.cpp, please use GGUF files instead.\n\n```\n./main -t 10 -ngl 32 -m llama-2-7b-chat.ggmlv3.q4_K_M.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\nWrite a story about llamas[/INST]\"\n```\nChange `-t 10` to the number of physical CPU cores you have. For example if your system has 8 cores/16 threads, use `-t 8`.\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\n\nChange `-c 2048` to the desired sequence length for this model. For example, `-c 4096` for a Llama 2 model.  For models that use RoPE, add `--rope-freq-base 10000 --rope-freq-scale 0.5` for doubled context, or `--rope-freq-base 10000 --rope-freq-scale 0.25` for 4x context.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI's Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute.\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Russ Johnson, J, alfie_i, Alex, NimbleBox.ai, Chadd, Mandus, Nikolai Manek, Ken Nordquist, ya boyyy, Illia Dulskyi, Viktor Bowallius, vamX, Iucharbius, zynix, Magnesian, Clay Pascal, Pierre Kircher, Enrico Ros, Tony Hughes, Elle, Andrey, knownsqashed, Deep Realms, Jerry Meng, Lone Striker, Derek Yates, Pyrater, Mesiah Bishop, James Bentley, Femi Adebogun, Brandon Frisco, SuperWojo, Alps Aficionado, Michael Dempsey, Vitor Caleffi, Will Dee, Edmond Seymore, usrbinkat, LangChain4j, Kacper WikieÅ‚, Luke Pendergrass, John Detwiler, theTransient, Nathan LeClaire, Tiffany J. Kim, biorpg, Eugene Pentland, Stanislav Ovsiannikov, Fred von Graf, terasurfer, Kalila, Dan Guido, Nitin Borwankar, é˜¿æ˜, Ai Maven, John Villwock, Gabriel Puliatti, Stephen Murray, Asp the Wyvern, danny, Chris Smitley, ReadyPlayerEmma, S_X, Daniel P. Andersen, Olakabola, Jeffrey Morgan, Imad Khwaja, Caitlyn Gatomon, webtim, Alicia Loh, Trenton Dambrowitz, Swaroop Kallakuri, Erik BjÃ¤reholt, Leonard Tan, Spiking Neurons AB, Luke @flexchar, Ajan Kanaga, Thomas Belote, Deo Leter, RoA, Willem Michiel, transmissions 11, subjectnull, Matthew Berman, Joseph William Delisle, David Ziegler, Michael Davis, Johann-Peter Hartmann, Talal Aujan, senxiiz, Artur Olbinski, Rainer Wilmers, Spencer Kim, Fen Risland, Cap'n Zoog, Rishabh Srivastava, Michael Levine, Geoffrey Montalvo, Sean Connelly, Alexandros Triantafyllidis, Pieter, Gabriel Tamborski, Sam, Subspace Studios, Junyu Yang, Pedro Madruga, Vadim, Cory Kujawski, K, Raven Klaugh, Randy H, Mano Prime, Sebastain Graf, Space Cruiser\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n# Original model card: Meta Llama 2's Llama 2 7B Chat\n\n# **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes â€” 7B, 13B, and 70B â€” as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n\n||Training Data|Params|Content Length|GQA|Tokens|LR|\n|---|---|---|---|---|---|---|\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Metaâ€™s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2â€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\nPlease report any software â€œbug,â€ or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/llamaste/Llama-2-7b) | [Link](https://huggingface.co/llamaste/Llama-2-7b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/llamaste/Llama-2-13b) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-13b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf)|\n|70B| [Link](https://huggingface.co/llamaste/Llama-2-70b) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-70b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf)|\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":60421177985,\"files_count\":20,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"model_type\":\"llama\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:oobabooga:text-generation-webui\",\"source_url\":\"https://github.com/oobabooga/text-generation-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:LostRuins:koboldcpp\",\"source_url\":\"https://github.com/LostRuins/koboldcpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ParisNeo:lollms-webui\",\"source_url\":\"https://github.com/ParisNeo/lollms-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:marella:ctransformers\",\"source_url\":\"https://github.com/marella/ctransformers\"},{\"type\":\"has_code\",\"target_id\":\"github:abetlen:llama-cpp-python\",\"source_url\":\"https://github.com/abetlen/llama-cpp-python\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:oobabooga:text-generation-webui\",\"source_url\":\"https://github.com/oobabooga/text-generation-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:llama\",\"source_url\":\"https://github.com/facebookresearch/llama\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:llama\",\"source_url\":\"http://github.com/facebookresearch/llama\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2307.09288\",\"source_url\":\"https://arxiv.org/abs/2307.09288\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 79.4,
    "content_hash": "97b28da2847e75ffa2c05558287b49bd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:meta-llama-3-70b",
    "name": "Meta-Llama-3-70B",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "en",
      "license:llama3",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 870,
    "downloads": 100679,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Meta-Llama-3-70B",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":70553706496,\"storage_bytes\":423345124549,\"files_count\":50,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"eos_token\":\"<|end_of_text|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "LLaMA-3",
    "compliance_status": "approved",
    "quality_score": 39.4,
    "content_hash": "c4d633d8ab0ec6adde6c7bcec8946392",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Meta-Llama-3-70B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:wan-ai:wan2.2-animate-14b",
    "name": "Wan2.2-Animate-14B",
    "author": "Wan-AI",
    "description": "--- license: apache-2.0 base_model: - Wan-AI/Wan2.2-I2V-A14B pipeline_tag: video-to-video --- <p align=\"center\"> <img src=\"assets/logo.png\" width=\"400\"/> <p> <p align=\"center\"> ğŸ’œ <a href=\"https://wan.video\"><b>Wan</b></a> &nbsp&nbsp ï½œ &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://github.com/Wan-Video/Wan2.2\">GitHub</a> &nbsp&nbsp | &nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/Wan-AI/\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/Wan-AI\">ModelScope</a>&nbsp&nbsp | &...",
    "tags": [
      "diffusers",
      "onnx",
      "safetensors",
      "video-to-video",
      "arxiv:2503.20314",
      "base_model:wan-ai/wan2.2-i2v-a14b",
      "base_model:quantized:wan-ai/wan2.2-i2v-a14b",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "video-to-video",
    "likes": 869,
    "downloads": 33667,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Wan-AI/Wan2.2-Animate-14B",
    "image_url": "https://huggingface.co/Wan-AI/Wan2.2-Animate-14B/resolve/main/assets/comp_effic.png",
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nbase_model:\n- Wan-AI/Wan2.2-I2V-A14B\npipeline_tag: video-to-video\n---\n# Wan2.2\n\n<p align=\"center\">\n    <img src=\"assets/logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n    ğŸ’œ <a href=\"https://wan.video\"><b>Wan</b></a> &nbsp&nbsp ï½œ &nbsp&nbsp ğŸ–¥ï¸ <a href=\"https://github.com/Wan-Video/Wan2.2\">GitHub</a> &nbsp&nbsp  | &nbsp&nbspğŸ¤— <a href=\"https://huggingface.co/Wan-AI/\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/Wan-AI\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2503.20314\">Paper</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://wan.video/welcome?spm=a2ty_o02.30011076.0.0.6c9ee41eCcluqg\">Blog</a> &nbsp&nbsp |  &nbsp&nbsp ğŸ’¬  <a href=\"https://discord.gg/AKNgpMK4Yj\">Discord</a>&nbsp&nbsp\n    <br>\n    ğŸ“• <a href=\"https://alidocs.dingtalk.com/i/nodes/jb9Y4gmKWrx9eo4dCql9LlbYJGXn6lpz\">ä½¿ç”¨æŒ‡å—(ä¸­æ–‡)</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“˜ <a href=\"https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y\">User Guide(English)</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href=\"https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg\">WeChat(å¾®ä¿¡)</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**](https://arxiv.org/abs/2503.20314) <be>\n\n\nWe are excited to introduce **Wan2.2**, a major upgrade to our foundational video models. With **Wan2.2**, we have focused on incorporating the following innovations:\n\n- ğŸ‘ **Effective MoE Architecture**: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\n\n- ğŸ‘ **Cinematic-level Aesthetics**: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\n\n- ğŸ‘ **Complex Motion Generation**: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models. \n\n- ğŸ‘ **Efficient High-Definition Hybrid TI2V**:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of **16Ã—16Ã—4**. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest **720P@24fps** models currently available, capable of serving both the industrial and academic sectors simultaneously.\n\n\n## Video Demos\n\n<div align=\"center\">\n    <video width=\"80%\" controls>\n        <source src=\"https://cloud.video.taobao.com/vod/4szTT1B0LqXvJzmuEURfGRA-nllnqN_G2AT0ZWkQXoQ.mp4\" type=\"video/mp4\">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## ğŸ”¥ Latest News!!\n\n* Sep 19, 2025: ğŸ’ƒ We introduct **[Wan2.2-Animate-14B](https://humanaigc.github.io/wan-animate)**, an unified model for character animation and replacement with holistic movement and expression replication. We released the [model weights](#model-download) and [inference code](#run-with-wan-animate). And now you can try it on [wan.video](https://wan.video/), [ModelScope Studio](https://www.modelscope.cn/studios/Wan-AI/Wan2.2-Animate) or [HuggingFace Space](https://huggingface.co/spaces/Wan-AI/Wan2.2-Animate)!\n* Aug 26, 2025: ğŸµ We introduce **[Wan2.2-S2V-14B](https://humanaigc.github.io/wan-s2v-webpage)**, an audio-driven cinematic video generation model, including [inference code](#run-speech-to-video-generation), [model weights](#model-download), and [technical report](https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf)! Now you can try it on [wan.video](https://wan.video/),  [ModelScope Gradio](https://www.modelscope.cn/studios/Wan-AI/Wan2.2-S2V) or [HuggingFace Gradio](https://huggingface.co/spaces/Wan-AI/Wan2.2-S2V)!\n* Jul 28, 2025: ğŸ‘‹ We have open a [HF space](https://huggingface.co/spaces/Wan-AI/Wan-2.2-5B) using the TI2V-5B model. Enjoy!\n* Jul 28, 2025: ğŸ‘‹ Wan2.2 has been integrated into ComfyUI ([CN](https://docs.comfy.org/zh-CN/tutorials/video/wan/wan2_2) | [EN](https://docs.comfy.org/tutorials/video/wan/wan2_2)). Enjoy!\n* Jul 28, 2025: ğŸ‘‹ Wan2.2's T2V, I2V and TI2V have been integrated into Diffusers ([T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers) | [I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers) | [TI2V-5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers)). Feel free to give it a try!\n* Jul 28, 2025: ğŸ‘‹ We've released the inference code and model weights of **Wan2.2**.\n* Sep 5, 2025: ğŸ‘‹ We add text-to-speech synthesis support with [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for Speech-to-Video generation task.\n\n\n## Community Works\nIf your research or project builds upon [**Wan2.1**](https://github.com/Wan-Video/Wan2.1) or [**Wan2.2**](https://github.com/Wan-Video/Wan2.2), and you would like more people to see it, please inform us.\n\n- [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) provides comprehensive support for Wan 2.2, including low-GPU-memory layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, full training.\n- [Kijai's ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is an alternative implementation of Wan models for ComfyUI. Thanks to its Wan-only focus, it's on the frontline of getting cutting edge optimizations and hot research features, which are often hard to integrate into ComfyUI quickly due to its more rigid structure.\n- [Cache-dit](https://github.com/vipshop/cache-dit) offers Fully Cache Acceleration support for Wan2.2 MoE with DBCache, TaylorSeer and Cache CFG. Visit their [example](https://github.com/vipshop/cache-dit/blob/main/examples/pipeline/run_wan_2.2.py) for more details.\n- [FastVideo](https://github.com/hao-ai-lab/FastVideo) includes distilled Wan models with sparse attention that significanly speed up the inference time. \n\n## ğŸ“‘ Todo List\n- Wan2.2 Text-to-Video\n    - [x] Multi-GPU Inference code of the A14B and 14B models\n    - [x] Checkpoints of the A14B and 14B models\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Image-to-Video\n    - [x] Multi-GPU Inference code of the A14B model\n    - [x] Checkpoints of the A14B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Text-Image-to-Video\n    - [x] Multi-GPU Inference code of the 5B model\n    - [x] Checkpoints of the 5B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2-S2V Speech-to-Video\n    - [x] Inference code of Wan2.2-S2V\n    - [x] Checkpoints of Wan2.2-S2V-14B\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2-Animate Character Animation and Replacement\n    - [x] Inference code of Wan2.2-Animate\n    - [x] Checkpoints of Wan2.2-Animate\n    - [x] ComfyUI integration\n    - [ ] Diffusers integration    \n\n## Run Wan2.2 Animate\n\n#### Installation\nClone the repo:\n```sh\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\n```\n\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\n# If you want to use CosyVoice to synthesize speech for Speech-to-Video Generation, please install requirements_s2v.txt additionally\npip install -r requirements_s2v.txt\n```\n\n\n#### Model Download\n\n| Models              | Download Links                                                                                                                              | Description |\n|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n| T2V-A14B    | ğŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B)    ğŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B)    | Text-to-Video MoE model, supports 480P & 720P |\n| I2V-A14B    | ğŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B)    ğŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B)    | Image-to-Video MoE model, supports 480P & 720P |\n| TI2V-5B     | ğŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B)     ğŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B)     | High-compression VAE, T2V+I2V, supports 720P |\n| S2V-14B     | ğŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-S2V-14B)     ğŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-S2V-14B)     | Speech-to-Video model, supports 480P & 720P |\n| Animate-14B | ğŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B) ğŸ¤– [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B)  | Character animation and replacement | |\n\n\nDownload models using huggingface-cli:\n``` sh\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-Animate-14B --local-dir ./Wan2.2-Animate-14B\n```\n\nDownload models using modelscope-cli:\n``` sh\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-Animate-14B --local_dir ./Wan2.2-Animate-14B\n```\n\n#### Run Wan-Animate-14B\n\nWan-Animate takes a video and a character image as input, and generates a video in either \"animation\" or \"replacement\" mode. \n\n1. animation modeï¼š The model generates a video of the character image that mimics the human motion in the input video.\n2. replacement mode: The model replaces the character image with the input video.\n\nPlease visit our [project page](https://humanaigc.github.io/wan-animate) to see more examples and learn about the scenarios suitable for this model.\n\n##### (1) Preprocessing \nThe input video should be preprocessed into several materials before be feed into the inference process.  Please refer to the following processing flow, and more details about preprocessing can be found in [UserGuider](https://github.com/Wan-Video/Wan2.2/blob/main/wan/modules/animate/preprocess/UserGuider.md).\n\n* For animation\n```bash\npython ./wan/modules/animate/preprocess/preprocess_data.py \\\n    --ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\\n    --video_path ./examples/wan_animate/animate/video.mp4 \\\n    --refer_path ./examples/wan_animate/animate/image.jpeg \\\n    --save_path ./examples/wan_animate/animate/process_results \\\n    --resolution_area 1280 720 \\\n    --retarget_flag \\\n    --use_flux\n```\n* For replacement\n```bash\npython ./wan/modules/animate/preprocess/preprocess_data.py \\\n    --ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\\n    --video_path ./examples/wan_animate/replace/video.mp4 \\\n    --refer_path ./examples/wan_animate/replace/image.jpeg \\\n    --save_path ./examples/wan_animate/replace/process_results \\\n    --resolution_area 1280 720 \\\n    --iterations 3 \\\n    --k 7 \\\n    --w_len 1 \\\n    --h_len 1 \\\n    --replace_flag\n```\n##### (2) Run in animation mode \n\n* Single-GPU inference \n\n```bash\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1\n```\n\n* Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```bash\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1 --dit_fsdp --t5_fsdp --ulysses_size 8\n```\n\n##### (3) Run in replacement mode \n\n* Single-GPU inference \n\n```bash\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/ --refert_num 1 --replace_flag --use_relighting_lora \n```\n\n* Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```bash\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/src_pose.mp4  --refert_num 1 --replace_flag --use_relighting_lora --dit_fsdp --t5_fsdp --ulysses_size 8\n```\n\n> ğŸ’¡ If you're using **Wan-Animate**, we do not recommend using LoRA models trained on `Wan2.2`, since weight changes during training may lead to unexpected behavior.\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.2** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align=\"center\">\n    <img src=\"assets/comp_effic.png\" alt=\"\" style=\"width: 80%;\" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) Multi-GPU: 14B: `--ulysses_size 4/8 --dit_fsdp --t5_fsdp`, 5B: `--ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu`; Single-GPU: 14B: `--offload_model True --convert_model_dtype`, 5B: `--offload_model True --convert_model_dtype --t5_cpu`\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n> (2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n> (3) Tests were run without the `--use_prompt_extend` flag;\n> (4) Reported results are the average of multiple samples taken after the warm-up phase.\n\n\n-------\n\n## Introduction of Wan2.2\n\n**Wan2.2** builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n\n##### (1) Mixture-of-Experts (MoE) Architecture\n\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\n\n<div align=\"center\">\n    <img src=\"assets/moe_arch.png\" alt=\"\" style=\"width: 90%;\" />\n</div>\n\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}_{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}_{moe}$ corresponding to half of the ${SNR}_{min}$, and switch to the low-noise expert when $t<{t}_{moe}$.\n\n<div align=\"center\">\n    <img src=\"assets/moe_2.png\" alt=\"\" style=\"width: 90%;\" />\n</div>\n\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline **Wan2.1** model does not employ the MoE architecture. Among the MoE-based variants, the **Wan2.1 & High-Noise Expert** reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the **Wan2.1 & Low-Noise Expert** uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The **Wan2.2 (MoE)** (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n\n\n##### (2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\n\n\n<div align=\"center\">\n    <img src=\"assets/vae.png\" alt=\"\" style=\"width: 80%;\" />\n</div>\n\n\n\n##### Comparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\n\n\n<div align=\"center\">\n    <img src=\"assets/performance.png\" alt=\"\" style=\"width: 90%;\" />\n</div>\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2025,\n      title={Wan: Open and Advanced Large-Scale Video Generative Models}, \n      author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\n      journal = {arXiv preprint arXiv:2503.20314},\n      year={2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/AKNgpMK4Yj) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!",
    "meta_json": "{\"pipeline_tag\":\"video-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":72381938315,\"files_count\":444,\"spaces_count\":51,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Wan-Video:Wan2.2\\\">GitHub<\",\"source_url\":\"https://github.com/Wan-Video/Wan2.2\\\">GitHub<\"},{\"type\":\"has_code\",\"target_id\":\"github:FunAudioLLM:CosyVoice\",\"source_url\":\"https://github.com/FunAudioLLM/CosyVoice\"},{\"type\":\"has_code\",\"target_id\":\"github:Wan-Video:Wan2.1\",\"source_url\":\"https://github.com/Wan-Video/Wan2.1\"},{\"type\":\"has_code\",\"target_id\":\"github:Wan-Video:Wan2.2\",\"source_url\":\"https://github.com/Wan-Video/Wan2.2\"},{\"type\":\"has_code\",\"target_id\":\"github:modelscope:DiffSynth-Studio\",\"source_url\":\"https://github.com/modelscope/DiffSynth-Studio\"},{\"type\":\"has_code\",\"target_id\":\"github:kijai:ComfyUI-WanVideoWrapper\",\"source_url\":\"https://github.com/kijai/ComfyUI-WanVideoWrapper\"},{\"type\":\"has_code\",\"target_id\":\"github:vipshop:cache-dit\",\"source_url\":\"https://github.com/vipshop/cache-dit\"},{\"type\":\"has_code\",\"target_id\":\"github:vipshop:cache-dit\",\"source_url\":\"https://github.com/vipshop/cache-dit\"},{\"type\":\"has_code\",\"target_id\":\"github:hao-ai-lab:FastVideo\",\"source_url\":\"https://github.com/hao-ai-lab/FastVideo\"},{\"type\":\"has_code\",\"target_id\":\"github:Wan-Video:Wan2.2.git\",\"source_url\":\"https://github.com/Wan-Video/Wan2.2.git\"},{\"type\":\"has_code\",\"target_id\":\"github:Wan-Video:Wan2.2\",\"source_url\":\"https://github.com/Wan-Video/Wan2.2\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2503.20314\",\"source_url\":\"https://arxiv.org/abs/2503.20314\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 99.4,
    "content_hash": "2b7c61f37973cbf6ceb38878a5c4920e",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/Wan-AI/Wan2.2-Animate-14B/resolve/main/assets/comp_effic.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Wan-AI/Wan2.2-Animate-14B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stabilityai:stable-diffusion-3.5-medium",
    "name": "stable-diffusion-3.5-medium",
    "author": "stabilityai",
    "description": "",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "stable-diffusion",
      "en",
      "arxiv:2403.03206",
      "license:other",
      "diffusers:stablediffusion3pipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 867,
    "downloads": 122111,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stabilityai/stable-diffusion-3.5-medium",
    "image_url": "https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/resolve/main/sd3.5_medium_demo.jpg",
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":36305578304,\"files_count\":45,\"spaces_count\":100,\"gated\":\"auto\",\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusion3Pipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.03206\",\"source_url\":\"https://arxiv.org/abs/2403.03206\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 59.4,
    "content_hash": "3bffe00a98e443b1f0ea57cfc139364e",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/resolve/main/sd3.5_medium_demo.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stabilityai/stable-diffusion-3.5-medium\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:distilbert:distilbert-base-uncased-finetuned-sst-2-english",
    "name": "distilbert-base-uncased-finetuned-sst-2-english",
    "author": "distilbert",
    "description": "--- language: en license: apache-2.0 datasets: - sst2 - glue model-index: - name: distilbert-base-uncased-finetuned-sst-2-english results: - task: type: text-classification name: Text Classification dataset: name: glue type: glue config: sst2 split: validation metrics: - type: accuracy value: 0.9105504587155964 name: Accuracy verified: true verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZl...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "rust",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "en",
      "dataset:sst2",
      "dataset:glue",
      "arxiv:1910.01108",
      "doi:10.57967/hf/0181",
      "license:apache-2.0",
      "model-index",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "likes": 858,
    "downloads": 4602269,
    "source": "huggingface",
    "source_url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n- glue\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9105504587155964\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg\n    - type: precision\n      value: 0.8978260869565218\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA\n    - type: recall\n      value: 0.9301801801801802\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ\n    - type: auc\n      value: 0.9716626673402374\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ\n    - type: f1\n      value: 0.9137168141592922\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA\n    - type: loss\n      value: 0.39013850688934326\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: sst2\n      type: sst2\n      config: default\n      split: train\n    metrics:\n    - type: accuracy\n      value: 0.9885521685548412\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA\n    - type: precision\n      value: 0.9881965062029833\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw\n    - type: precision\n      value: 0.9885521685548412\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ\n    - type: precision\n      value: 0.9885639626373408\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg\n    - type: recall\n      value: 0.9886145346602994\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw\n    - type: f1\n      value: 0.9884019815052447\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ\n    - type: f1\n      value: 0.9885521685548412\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ\n    - type: f1\n      value: 0.9885546181087554\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA\n    - type: loss\n      value: 0.040652573108673096\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg\n---\n\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\nâ€‹â€‹\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [AurÃ©lien GÃ©ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n",
    "meta_json": "{\"pipeline_tag\":\"text-classification\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":66955010,\"storage_bytes\":2208990137,\"files_count\":17,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DistilBertForSequenceClassification\"],\"model_type\":\"distilbert\",\"tokenizer_config\":{}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.01108\",\"source_url\":\"https://arxiv.org/abs/1910.01108\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.3,
    "content_hash": "83221cd17c2b62b0f476931c3b2516f1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-0.6b",
    "name": "Qwen3-0.6B",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-0.6B-Base --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offer...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:2505.09388",
      "base_model:qwen/qwen3-0.6b-base",
      "base_model:finetune:qwen/qwen3-0.6b-base",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 858,
    "downloads": 7762689,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-0.6B-Base\n---\n\n# Qwen3-0.6B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-0.6B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 0.6B\n- Number of Paramaters (Non-Embedding): 0.44B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 16 for Q and 8 for KV\n- Context Length: 32,768 \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n> [!TIP]\n> If you encounter significant endless repetitions, please refer to the [Best Practices](#best-practices) section for optimal sampling parameters, and set the ``presence_penalty`` to 1.5.\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-0.6B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":751632384,\"storage_bytes\":4522815806,\"files_count\":10,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3ForCausalLM\"],\"model_type\":\"qwen3\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3\",\"source_url\":\"https://github.com/QwenLM/Qwen3\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.3,
    "content_hash": "20650e086cdab00b49800f1a62002ca4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-0.6B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mistralai:devstral-small-2505",
    "name": "Devstral-Small-2505",
    "author": "mistralai",
    "description": "--- library_name: vllm language: - en - fr - de - es - pt - it - ja - ko - ru - zh - ar - fa - id - ms - ne - pl - ro - sr - sv - tr - uk - vi - hi - bn license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-3.1-24B-Instruct-2503 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>. tags: - mistral-common --- Devstral is an agentic LLM for software engineering t...",
    "tags": [
      "vllm",
      "safetensors",
      "mistral",
      "mistral-common",
      "en",
      "fr",
      "de",
      "es",
      "pt",
      "it",
      "ja",
      "ko",
      "ru",
      "zh",
      "ar",
      "fa",
      "id",
      "ms",
      "ne",
      "pl",
      "ro",
      "sr",
      "sv",
      "tr",
      "uk",
      "vi",
      "hi",
      "bn",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 857,
    "downloads": 11926,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Devstral-Small-2505",
    "image_url": "https://huggingface.co/mistralai/Devstral-Small-2505/resolve/main/assets/images_example/example_mistral_common_1.png",
    "type": "model",
    "body_content": "---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- pt\n- it\n- ja\n- ko\n- ru\n- zh\n- ar\n- fa\n- id\n- ms\n- ne\n- pl\n- ro\n- sr\n- sv\n- tr\n- uk\n- vi\n- hi\n- bn\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-3.1-24B-Instruct-2503\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Devstral Small 1.0\n\nDevstral is an agentic LLM for software engineering tasks built under a collaboration between [Mistral AI](https://mistral.ai/) and [All Hands AI](https://www.all-hands.dev/) ğŸ™Œ. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model on this [benchmark](#benchmark-results). \n\nIt is finetuned from [Mistral-Small-3.1](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503), therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from `Mistral-Small-3.1` the vision encoder was removed.\n\nFor enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\n\nLearn more about Devstral in our [blog post](https://mistral.ai/news/devstral).\n\n\n## Key Features:\n- **Agentic coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n- **lightweight**: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\n- **Apache 2.0 License**: Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window**: A 128k context window.\n- **Tokenizer**: Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n\n\n## Benchmark Results\n\n### SWE-Bench\n\nDevstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA by 6%.\n\n| Model            | Scaffold           | SWE-Bench Verified (%) |\n|------------------|--------------------|------------------------|\n| Devstral         | OpenHands Scaffold | **46.8**               |\n| GPT-4.1-mini     | OpenAI Scaffold    | 23.6                   |\n| Claude 3.5 Haiku | Anthropic Scaffold | 40.6                   |\n| SWE-smith-LM 32B | SWE-agent Scaffold | 40.2                   |\n\n\n When evaluated under the same test scaffold (OpenHands, provided by All Hands AI ğŸ™Œ), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\n\n![SWE Benchmark](assets/swe_bench.png)\n\n## Usage\n\nWe recommend to use Devstral with the [OpenHands](https://github.com/All-Hands-AI/OpenHands/tree/main) scaffold.\nYou can use it either through our API or by running locally. \n\n### API \nFollow these [instructions](https://docs.mistral.ai/getting-started/quickstart/#account-setup) to create a Mistral account and get an API key.\n\nThen run these commands to start the OpenHands docker container.\n```bash\nexport MISTRAL_API_KEY=<MY_KEY>\n\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik\n\nmkdir -p ~/.openhands-state && echo '{\"language\":\"en\",\"agent\":\"CodeActAgent\",\"max_iterations\":null,\"security_analyzer\":null,\"confirmation_mode\":false,\"llm_model\":\"mistral/devstral-small-2505\",\"llm_api_key\":\"'$MISTRAL_API_KEY'\",\"remote_runtime_resource_factor\":null,\"github_token\":null,\"enable_default_condenser\":true}' > ~/.openhands-state/settings.json\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands-state:/.openhands-state \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.all-hands.dev/all-hands-ai/openhands:0.39\n```\n\n### Local inference \n\nThe model can also be deployed with the following libraries:\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm-recommended)\n- [`mistral-inference`](https://github.com/mistralai/mistral-inference): See [here](#mistral-inference)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n- [`LMStudio`](https://lmstudio.ai/): See [here](#lmstudio)\n- [`llama.cpp`](https://github.com/ggml-org/llama.cpp): See [here](#llama.cpp)\n- [`ollama`](https://github.com/ollama/ollama): See [here](#ollama)\n\n\n### OpenHands (recommended)\n\n#### Launch a server to deploy Devstral Small 1.0\n\nMake sure you launched an OpenAI-compatible server such as vLLM or Ollama as described above. Then, you can use OpenHands to interact with `Devstral Small 1.0`.\n\nIn the case of the tutorial we spineed up a vLLM server running the command:\n```bash\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\nThe server address should be in the following format: `http://<your-server-url>:8000/v1`\n\n#### Launch OpenHands\n\nYou can follow installation of OpenHands [here](https://docs.all-hands.dev/modules/usage/installation).\n\nThe easiest way to launch OpenHands is to use the Docker image:\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands-state:/.openhands-state \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.all-hands.dev/all-hands-ai/openhands:0.38\n```\n\n\nThen, you can access the OpenHands UI at `http://localhost:3000`.\n\n#### Connect to the server\n\nWhen accessing the OpenHands UI, you will be prompted to connect to a server. You can use the advanced mode to connect to the server you launched earlier.\n\nFill the following fields:\n- **Custom Model**: `openai/mistralai/Devstral-Small-2505`\n- **Base URL**: `http://<your-server-url>:8000/v1`\n- **API Key**: `token` (or any other token you used to launch the server if any)\n\n#### Use OpenHands powered by Devstral\n\nNow you're good to use Devstral Small inside OpenHands by **starting a new conversation**. Let's build a To-Do list app.\n\n<details>\n  <summary>To-Do list app</summary\n\n1. Let's ask Devstral to generate the app with the following prompt:\n\n```txt\nBuild a To-Do list app with the following requirements:\n- Built using FastAPI and React.\n- Make it a one page app that:\n   - Allows to add a task.\n   - Allows to delete a task.\n   - Allows to mark a task as done.\n   - Displays the list of tasks.\n- Store the tasks in a SQLite database.\n```\n\n![Agent prompting](assets/tuto_open_hands/agent_prompting.png)\n\n\n2. Let's see the result\n\nYou should see the agent construct the app and be able to explore the code it generated.\n\nIf it doesn't do it automatically, ask Devstral to deploy the app or do it manually, and then go the front URL deployment to see the app.\n\n![Agent working](assets/tuto_open_hands/agent_working.png)\n![App UI](assets/tuto_open_hands/app_ui.png)\n\n\n3. Iterate\n\nNow that you have a first result you can iterate on it by asking your agent to improve it. For example, in the app generated we could click on a task to mark it checked but having a checkbox would improve UX. You could also ask it to add a feature to edit a task, or to add a feature to filter the tasks by status.\n\nEnjoy building with Devstral Small and OpenHands!\n\n</details>\n\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.8.5`](https://github.com/vllm-project/vllm/releases/tag/v0.8.5):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.5.5`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.5).\n\nTo check:\n```\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Devstral in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\n\n\nurl = \"http://<your-server-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Devstral-Small-2505\"\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"<your-command>\",\n            },\n        ],\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages, \"temperature\": 0.15}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n```\n\n### Mistral-inference\n\nWe recommend using mistral-inference to quickly try out / \"vibe-check\" Devstral.\n\n#### Install\n\nMake sure to have mistral_inference >= 1.6.0 installed.\n\n```bash\npip install mistral_inference --upgrade\n```\n\n#### Download\n\n```python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', 'Devstral')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Devstral-Small-2505\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\n```\n\n#### Python\n\nYou can run the model using the following command:\n\n```bash\nmistral-chat $HOME/mistral_models/Devstral --instruct --max_tokens 300\n```\n\nYou can then prompt it with anything you'd like.\n\n### Transformers\n\nTo make the best use of our model with transformers make sure to have [installed](https://github.com/mistralai/mistral-common) `    mistral-common >= 1.5.5` to use our tokenizer.\n\n```bash\npip install mistral-common --upgrade\n```\n\nThen load our tokenizer along with the model and generate:\n\n```python\nimport torch\n\nfrom mistral_common.protocol.instruct.messages import (\n    SystemMessage, UserMessage\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoModelForCausalLM\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nmodel_id = \"mistralai/Devstral-Small-2505\"\ntekken_file = hf_hub_download(repo_id=model_id, filename=\"tekken.json\")\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\n\ntokenizer = MistralTokenizer.from_file(tekken_file)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntokenized = tokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        messages=[\n            SystemMessage(content=SYSTEM_PROMPT),\n            UserMessage(content=\"<your-command>\"),\n        ],\n    )\n)\n\noutput = model.generate(\n    input_ids=torch.tensor([tokenized.tokens]),\n    max_new_tokens=1000,\n)[0]\n\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens):])\nprint(decoded_output)\n```\n\n### LMStudio\nDownload the weights from huggingface:\n\n```\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"mistralai/Devstral-Small-2505_gguf\" \\\n--include \"devstralQ4_K_M.gguf\" \\\n--local-dir \"mistralai/Devstral-Small-2505_gguf/\"\n```\n\nYou can serve the model locally with [LMStudio](https://lmstudio.ai/).\n* Download [LM Studio](https://lmstudio.ai/) and install it\n* Install `lms cli ~/.lmstudio/bin/lms bootstrap`\n* In a bash terminal, run `lms import devstralQ4_K_M.gguf` in the directory where you've downloaded the model checkpoint (e.g. `mistralai/Devstral-Small-2505_gguf`)\n* Open the LMStudio application, click the terminal icon to get into the developer tab. Click select a model to load and select Devstral Q4 K M. Toggle the status button to start the model, in setting toggle Serve on Local Network to be on.\n* On the right tab, you will see an API identifier which should be devstralq4_k_m and an api address under API Usage. Keep note of this address, we will use it in the next step.\n\nLaunch Openhands\nYou can now interact with the model served from LM Studio with openhands. Start the openhands server with the docker\n\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\ndocker run -it --rm --pull=always \\\n\t-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\\n\t-e LOG_ALL_EVENTS=true \\\n\t-v /var/run/docker.sock:/var/run/docker.sock \\\n\t-v ~/.openhands-state:/.openhands-state \\\n\t-p 3000:3000 \\\n\t--add-host host.docker.internal:host-gateway \\\n\t--name openhands-app \\\n\tdocker.all-hands.dev/all-hands-ai/openhands:0.38\n```\n\nClick â€œsee advanced settingâ€ on the second line. \nIn the new tab, toggle advanced to on. Set the custom model to be mistral/devstralq4_k_m and Base URL the api address we get from the last step in LM Studio. Set API Key to dummy. Click save changes.\n\n### llama.cpp\n\nDownload the weights from huggingface:\n\n```\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"mistralai/Devstral-Small-2505_gguf\" \\\n--include \"devstralQ4_K_M.gguf\" \\\n--local-dir \"mistralai/Devstral-Small-2505_gguf/\"\n```\n\nThen run Devstral using the llama.cpp CLI.\n\n```bash\n./llama-cli -m Devstral-Small-2505_gguf/devstralQ4_K_M.gguf -cnv\n```\n\n### Ollama\n\nYou can run Devstral using the [Ollama](https://ollama.ai/) CLI.\n\n```bash\nollama run devstral\n```\n\n### Example: Understanding Test Coverage of Mistral Common\n\nWe can start the OpenHands scaffold and link it to a repo to analyze test coverage and identify badly covered files.\nHere we start with our public `mistral-common` repo.\n\n\nAfter the repo is mounted in the workspace, we give the following instruction\n```\nCheck the test coverage of the repo and then create a visualization of test coverage. Try plotting a few different types of graphs and save them to a png.\n```\nThe agent will first browse the code base to check test configuration and structure.\n\n![Repo Exploration](assets/images_example/example_mistral_common_1.png)\n\nThen it sets up the testing dependencies and launches the coverage test:\n\n![Repo Exploration](assets/images_example/example_mistral_common_2.png)\n\nFinally, the agent writes necessary code to visualize the coverage.\n![Repo Exploration](assets/images_example/example_mistral_common_3.png)\n\nAt the end of the run, the following plots are produced:\n![Repo Exploration](assets/images_example/example_mistral_common_res_1.png)\n![Repo Exploration](assets/images_example/example_mistral_common_res_2.png)\n![Repo Exploration](assets/images_example/example_mistral_common_res_3.png)",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"vllm\",\"framework\":\"vllm\",\"params\":23572403200,\"storage_bytes\":94324335088,\"files_count\":29,\"spaces_count\":26,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:All-Hands-AI:OpenHands\",\"source_url\":\"https://github.com/All-Hands-AI/OpenHands\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-inference\",\"source_url\":\"https://github.com/mistralai/mistral-inference\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:ggml-org:llama.cpp\",\"source_url\":\"https://github.com/ggml-org/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ollama:ollama\",\"source_url\":\"https://github.com/ollama/ollama\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 99.3,
    "content_hash": "b91ba747cfdd9d384ac5a16e31b3ea14",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/mistralai/Devstral-Small-2505/resolve/main/assets/images_example/example_mistral_common_1.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Devstral-Small-2505\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:flan-t5-large",
    "name": "flan-t5-large",
    "author": "google",
    "description": "--- language: - en - fr - ro - de - multilingual widget: - text: \"Translate to German: My name is Arthur\" example_title: \"Translation\" - text: \"Please answer to the following question. Who is going to be the next Ballon d'or?\" example_title: \"Question Answering\" - text: \"Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\" example_title: \"Logical reasoning\" - text: \"Please answer the following question. What is the boiling point of Nitrogen?...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "t5",
      "text2text-generation",
      "en",
      "fr",
      "ro",
      "de",
      "multilingual",
      "dataset:svakulenk0/qrecc",
      "dataset:taskmaster2",
      "dataset:djaym7/wiki_dialog",
      "dataset:deepmind/code_contests",
      "dataset:lambada",
      "dataset:gsm8k",
      "dataset:aqua_rat",
      "dataset:esnli",
      "dataset:quasc",
      "dataset:qed",
      "arxiv:2210.11416",
      "arxiv:1910.09700",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 854,
    "downloads": 464023,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/flan-t5-large",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\nwidget:\n- text: \"Translate to German:  My name is Arthur\"\n  example_title: \"Translation\"\n- text: \"Please answer to the following question. Who is going to be the next Ballon d'or?\"\n  example_title: \"Question Answering\"\n- text: \"Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\"\n  example_title: \"Logical reasoning\"\n- text: \"Please answer the following question. What is the boiling point of Nitrogen?\"\n  example_title: \"Scientific knowledge\"\n- text: \"Answer the following yes/no question. Can you write a whole Haiku in a single tweet?\"\n  example_title: \"Yes/no question\"\n- text: \"Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?\"\n  example_title: \"Reasoning task\"\n- text: \"Q: ( False or not False or False ) is? A: Let's think step by step\"\n  example_title: \"Boolean Expressions\"\n- text: \"The square root of x is the cube root of y. What is y to the power of 2, if x = 4?\"\n  example_title: \"Math reasoning\"\n- text: \"Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It's not certain how many lessons you'll learn by your thirties. Does the premise entail the hypothesis?\"\n  example_title: \"Premise and hypothesis\"\n\ntags:\n- text2text-generation\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 large\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper's model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model's [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Large, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips â‰¥ 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":783150080,\"storage_bytes\":15929620947,\"files_count\":12,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"T5ForConditionalGeneration\"],\"model_type\":\"t5\",\"tokenizer_config\":{\"eos_token\":\"</s>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:google-research:t5x\",\"source_url\":\"https://github.com/google-research/t5x\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:t5x\",\"source_url\":\"https://github.com/google-research/t5x\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:t5x\",\"source_url\":\"https://github.com/google-research/t5x\"},{\"type\":\"has_code\",\"target_id\":\"github:google:jax\",\"source_url\":\"https://github.com/google/jax\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.11416\",\"source_url\":\"https://arxiv.org/abs/2210.11416\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.09700\",\"source_url\":\"https://arxiv.org/abs/1910.09700\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.3,
    "content_hash": "2308ee9e614ae132d5c4d95fc12c5545",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/flan-t5-large\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-2-70b-hf",
    "name": "Llama-2-70b-hf",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "llama-2",
      "en",
      "arxiv:2307.09288",
      "license:llama2",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 853,
    "downloads": 23845,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-2-70b-hf",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":68976653312,\"storage_bytes\":551815654451,\"files_count\":43,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<s>\",\"lstrip\":false,\"normalized\":false,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"</s>\",\"lstrip\":false,\"normalized\":false,\"rstrip\":false,\"single_word\":false},\"pad_token\":null,\"unk_token\":{\"__type\":\"AddedToken\",\"content\":\"<unk>\",\"lstrip\":false,\"normalized\":false,\"rstrip\":false,\"single_word\":false}}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2307.09288\",\"source_url\":\"https://arxiv.org/abs/2307.09288\"}]",
    "canonical_id": null,
    "license_spdx": "LLaMA-2",
    "compliance_status": "approved",
    "quality_score": 39.3,
    "content_hash": "c1e5d41e06bfd2084c63c5d1f02f618f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-2-70b-hf\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:jasperai:flux.1-dev-controlnet-upscaler",
    "name": "Flux.1-dev-Controlnet-Upscaler",
    "author": "jasperai",
    "description": "--- base_model: - black-forest-labs/FLUX.1-dev library_name: diffusers license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md pipeline_tag: image-to-image inference: False tags: - ControlNet - super-resolution - upscaler --- This is Flux.1-dev ControlNet for low resolution images developed by Jasper research team. <p align=\"center\"> <img style=\"width:700px;\" src=\"examples/showcase.jpg\"> </p> This mo...",
    "tags": [
      "diffusers",
      "safetensors",
      "controlnet",
      "super-resolution",
      "upscaler",
      "image-to-image",
      "base_model:black-forest-labs/flux.1-dev",
      "base_model:finetune:black-forest-labs/flux.1-dev",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "image-to-image",
    "likes": 853,
    "downloads": 12246,
    "source": "huggingface",
    "source_url": "https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler",
    "image_url": null,
    "type": "model",
    "body_content": "---\nbase_model:\n- black-forest-labs/FLUX.1-dev\nlibrary_name: diffusers\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\npipeline_tag: image-to-image\ninference: False\ntags:\n- ControlNet\n- super-resolution\n- upscaler\n---\n# âš¡ Flux.1-dev: Upscaler ControlNet âš¡\n\nThis is [Flux.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) ControlNet for low resolution images developed by Jasper research team.\n\n<p align=\"center\">\n   <img style=\"width:700px;\" src=\"examples/showcase.jpg\">\n</p>\n\n# How to use\nThis model can be used directly with the `diffusers` library\n\n```python\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxControlNetModel\nfrom diffusers.pipelines import FluxControlNetPipeline\n\n# Load pipeline\ncontrolnet = FluxControlNetModel.from_pretrained(\n  \"jasperai/Flux.1-dev-Controlnet-Upscaler\",\n  torch_dtype=torch.bfloat16\n)\npipe = FluxControlNetPipeline.from_pretrained(\n  \"black-forest-labs/FLUX.1-dev\",\n  controlnet=controlnet,\n  torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n\n# Load a control image\ncontrol_image = load_image(\n  \"https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler/resolve/main/examples/input.jpg\"\n)\n\nw, h = control_image.size\n\n# Upscale x4\ncontrol_image = control_image.resize((w * 4, h * 4))\n\nimage = pipe(\n    prompt=\"\", \n    control_image=control_image,\n    controlnet_conditioning_scale=0.6,\n    num_inference_steps=28, \n    guidance_scale=3.5,\n    height=control_image.size[1],\n    width=control_image.size[0]\n).images[0]\nimage\n```\n\n<p align=\"center\">\n   <img style=\"width:500px;\" src=\"examples/output.jpg\">\n</p>\n\n\n# Training\nThis model was trained with a synthetic complex data degradation scheme taking as input a *real-life* image and artificially degrading it by combining several degradations such as amongst other image noising (Gaussian, Poisson), image blurring and JPEG compression in a similar spirit as [1]\n\n[1] Wang, Xintao, et al. \"Real-esrgan: Training real-world blind super-resolution with pure synthetic data.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n# Licence\nThis model falls under the [Flux.1-dev model licence](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).",
    "meta_json": "{\"pipeline_tag\":\"image-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":10808301966,\"files_count\":7,\"spaces_count\":82,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 64.3,
    "content_hash": "b5a7de8fe9a7b207446706f5878b0fe5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]