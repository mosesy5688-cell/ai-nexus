[
  {
    "id": "huggingface:mistralai:mistral-large-instruct-2407",
    "name": "Mistral-Large-Instruct-2407",
    "author": "mistralai",
    "description": "",
    "tags": [
      "vllm",
      "safetensors",
      "mistral",
      "mistral-common",
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ja",
      "ru",
      "ko",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 852,
    "downloads": 8959,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"vllm\",\"framework\":\"vllm\",\"params\":122610069504,\"storage_bytes\":490441047031,\"files_count\":114,\"spaces_count\":32,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{%- if messages[0][\\\"role\\\"] == \\\"system\\\" %}\\n    {%- set system_message = messages[0][\\\"content\\\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n{%- set user_messages = loop_messages | selectattr(\\\"role\\\", \\\"equalto\\\", \\\"user\\\") | list %}\\n\\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\\n{%- set ns = namespace() %}\\n{%- set ns.index = 0 %}\\n{%- for message in loop_messages %}\\n    {%- if not (message.role == \\\"tool\\\" or message.role == \\\"tool_results\\\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\\n        {%- if (message[\\\"role\\\"] == \\\"user\\\") != (ns.index % 2 == 0) %}\\n            {{- raise_exception(\\\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n        {%- endif %}\\n        {%- set ns.index = ns.index + 1 %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if message[\\\"role\\\"] == \\\"user\\\" %}\\n        {%- if tools is not none and (message == user_messages[-1]) %}\\n            {{- \\\"[AVAILABLE_TOOLS] [\\\" }}\\n            {%- for tool in tools %}\\n                {%- set tool = tool.function %}\\n                {{- '{\\\"type\\\": \\\"function\\\", \\\"function\\\": {' }}\\n                {%- for key, val in tool.items() if key != \\\"return\\\" %}\\n                    {%- if val is string %}\\n                        {{- '\\\"' + key + '\\\": \\\"' + val + '\\\"' }}\\n                    {%- else %}\\n                        {{- '\\\"' + key + '\\\": ' + val|tojson }}\\n                    {%- endif %}\\n                    {%- if not loop.last %}\\n                        {{- \\\", \\\" }}\\n                    {%- endif %}\\n                {%- endfor %}\\n                {{- \\\"}}\\\" }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- else %}\\n                    {{- \\\"]\\\" }}\\n                {%- endif %}\\n            {%- endfor %}\\n            {{- \\\"[/AVAILABLE_TOOLS]\\\" }}\\n            {%- endif %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- \\\"[INST] \\\" + system_message + \\\"\\\\n\\\\n\\\" + message[\\\"content\\\"] + \\\"[/INST]\\\" }}\\n        {%- else %}\\n            {{- \\\"[INST] \\\" + message[\\\"content\\\"] + \\\"[/INST]\\\" }}\\n        {%- endif %}\\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\\n        {{- \\\"[TOOL_CALLS] [\\\" }}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- set out = tool_call.function|tojson %}\\n            {{- out[:-1] }}\\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\\n                {{- raise_exception(\\\"Tool call IDs should be alphanumeric strings with length 9!\\\") }}\\n            {%- endif %}\\n            {{- ', \\\"id\\\": \\\"' + tool_call.id + '\\\"}' }}\\n            {%- if not loop.last %}\\n                {{- \\\", \\\" }}\\n            {%- else %}\\n                {{- \\\"]\\\" + eos_token }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- elif message[\\\"role\\\"] == \\\"assistant\\\" %}\\n        {{- \\\" \\\" + message[\\\"content\\\"]|trim + eos_token}}\\n    {%- elif message[\\\"role\\\"] == \\\"tool_results\\\" or message[\\\"role\\\"] == \\\"tool\\\" %}\\n        {%- if message.content is defined and message.content.content is defined %}\\n            {%- set content = message.content.content %}\\n        {%- else %}\\n            {%- set content = message.content %}\\n        {%- endif %}\\n        {{- '[TOOL_RESULTS] {\\\"content\\\": ' + content|string + \\\", \\\" }}\\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\\n            {{- raise_exception(\\\"Tool call IDs should be alphanumeric strings with length 9!\\\") }}\\n        {%- endif %}\\n        {{- '\\\"call_id\\\": \\\"' + message.tool_call_id + '\\\"}[/TOOL_RESULTS]' }}\\n    {%- else %}\\n        {{- raise_exception(\\\"Only user and assistant roles are supported, with the exception of an initial optional system message!\\\") }}\\n    {%- endif %}\\n{%- endfor %}\\n\",\"eos_token\":\"</s>\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.3,
    "content_hash": "270cc25e9c836bad4aef660eeae3e047",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Mistral-Large-Instruct-2407\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:kwai-kolors:kolors",
    "name": "Kolors",
    "author": "Kwai-Kolors",
    "description": "--- license: apache-2.0 language: - zh - en tags: - text-to-image - stable-diffusion - kolors --- <div align=\"center\" style=\"display: flex; justify-content: center; flex-wrap: wrap;\"> <a href=\"https://github.com/Kwai-Kolors/Kolors\"><img src=\"https://img.shields.io/static/v1?label=Kolors Code&message=Github&color=blue&logo=github-pages\"></a> &ensp; <a href=\"https://kwai-kolors.github.io/\"><img src=\"https://img.shields.io/static/v1?label=Team%20Page&message=Page&color=green\"></a> &ensp; <a href...",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "stable-diffusion",
      "kolors",
      "zh",
      "en",
      "license:apache-2.0",
      "endpoints_compatible",
      "diffusers:stablediffusionxlpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 848,
    "downloads": 493,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Kwai-Kolors/Kolors",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n  - zh\n  - en\ntags:\n  - text-to-image\n  - stable-diffusion\n  - kolors\n---\n# Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis\n<div align=\"center\" style=\"display: flex; justify-content: center; flex-wrap: wrap;\">\n  <a href=\"https://github.com/Kwai-Kolors/Kolors\"><img src=\"https://img.shields.io/static/v1?label=Kolors Code&message=Github&color=blue&logo=github-pages\"></a> &ensp;\n  <a href=\"https://kwai-kolors.github.io/\"><img src=\"https://img.shields.io/static/v1?label=Team%20Page&message=Page&color=green\"></a> &ensp;\n  <a href=\"https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf\"><img src=\"https://img.shields.io/static/v1?label=Tech Report&message=Arxiv:Kolors&color=red&logo=arxiv\"></a> &ensp;\n  <a href=\"https://kolors.kuaishou.com/\"><img src=\"https://img.shields.io/static/v1?label=Official Website&message=Page&color=green\"></a>\n</div>\n<figure>\n  <img src=\"imgs/head_final3.png\">\n</figure>\n<br>\n\n## ğŸ“– Introduction\nKolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and proprietary models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters. Furthermore, Kolors supports both Chinese and English inputs, demonstrating strong performance in understanding and generating Chinese-specific content. For more details, please refer to this <a href=\"https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf\">technical report</a></b>.\n\n\n## ğŸš€ Quick Start\n### Requirements\n\n* Python 3.8 or later\n* PyTorch 1.13.1 or later\n* Transformers 4.26.1 or later\n* Recommended: CUDA 11.7 or later\n<br>\n\n1. Repository cloning and dependency installation\n\n```bash\napt-get install git-lfs\ngit clone https://github.com/Kwai-Kolors/Kolors\ncd Kolors\nconda create --name kolors python=3.8\nconda activate kolors\npip install -r requirements.txt\npython3 setup.py install\n```\n2. Weights downloadï¼ˆ[link](https://huggingface.co/Kwai-Kolors/Kolors)ï¼‰ï¼š\n```bash\nhuggingface-cli download --resume-download Kwai-Kolors/Kolors --local-dir weights/Kolors\n```\nor\n```bash\ngit lfs clone https://huggingface.co/Kwai-Kolors/Kolors weights/Kolors\n```\n3. Inferenceï¼š\n```bash\npython3 scripts/sample.py \"ä¸€å¼ ç“¢è™«çš„ç…§ç‰‡ï¼Œå¾®è·ï¼Œå˜ç„¦ï¼Œé«˜è´¨é‡ï¼Œç”µå½±ï¼Œæ‹¿ç€ä¸€ä¸ªç‰Œå­ï¼Œå†™ç€â€œå¯å›¾â€\"\n# The image will be saved to \"scripts/outputs/sample_test.jpg\"\n```\n\n### Using with Diffusers\nPlease refer to https://huggingface.co/Kwai-Kolors/Kolors-diffusers.\n\n## ğŸ“œ License&Citation\n### License\nKolors are fully open-sourced for academic research. For commercial use, please fill out this [questionnaire](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/å¯å›¾KOLORSæ¨¡å‹å•†ä¸šæˆæƒç”³è¯·ä¹¦.docx) and sent it to kwai-kolors@kuaishou.com for registration.\n\nWe open-source Kolors to promote the development of large text-to-image models in collaboration with the open-source community. The code of this project is open-sourced under the Apache-2.0 license. We sincerely urge all developers and users to strictly adhere to the [open-source license](MODEL_LICENSE), avoiding the use of the open-source model, code, and its derivatives for any purposes that may harm the country and society or for any services not evaluated and registered for safety. Note that despite our best efforts to ensure the compliance, accuracy, and safety of the data during training, due to the diversity and combinability of generated content and the probabilistic randomness affecting the model, we cannot guarantee the accuracy and safety of the output content, and the model is susceptible to misleading. This project does not assume any legal responsibility for any data security issues, public opinion risks, or risks and liabilities arising from the model being misled, abused, misused, or improperly utilized due to the use of the open-source model and code.\n\n\n### Citation\nIf you find our work helpful, please cite it!\n\n```\n@article{kolors,\n  title={Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis},\n  author={Kolors Team},\n  journal={arXiv preprint},\n  year={2024}\n}\n```\n\n### Acknowledgments\n- Thanks to [Diffusers](https://github.com/huggingface/diffusers) for providing the codebase.\n- Thanks to [ChatGLM3](https://github.com/THUDM/ChatGLM3) for providing the powerful Chinese language model.\n<br>\n\n### Contact Us\n\nIf you want to leave a message for our R&D team and product team, feel free to join our [WeChat group](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/wechat.png). You can also contact us via email (kwai-kolors@kuaishou.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":62041008151,\"files_count\":42,\"spaces_count\":92,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionXLPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Kwai-Kolors:Kolors\\\"><img\",\"source_url\":\"https://github.com/Kwai-Kolors/Kolors\\\"><img\"},{\"type\":\"has_code\",\"target_id\":\"github:Kwai-Kolors:Kolors\",\"source_url\":\"https://github.com/Kwai-Kolors/Kolors\"},{\"type\":\"has_code\",\"target_id\":\"github:Kwai-Kolors:Kolors\",\"source_url\":\"https://github.com/Kwai-Kolors/Kolors\"},{\"type\":\"has_code\",\"target_id\":\"github:Kwai-Kolors:Kolors\",\"source_url\":\"https://github.com/Kwai-Kolors/Kolors\"},{\"type\":\"has_code\",\"target_id\":\"github:Kwai-Kolors:Kolors\",\"source_url\":\"https://github.com/Kwai-Kolors/Kolors\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM3\",\"source_url\":\"https://github.com/THUDM/ChatGLM3\"},{\"type\":\"has_code\",\"target_id\":\"github:Kwai-Kolors:Kolors\",\"source_url\":\"https://github.com/Kwai-Kolors/Kolors\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.3,
    "content_hash": "836138fa4026e179a67c28e04536d690",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Kwai-Kolors/Kolors\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencentarc:t2i-adapter",
    "name": "T2I-Adapter",
    "author": "TencentARC",
    "description": "ğŸ°**Adapter Zoo** **|** ğŸ¨**Demos** **|** ğŸŸ **GitHub** T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models The GitHub repo: <https://github.com/TencentARC/T2I-Adapter> Please find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md --- license: apache-2.0 ---",
    "tags": [
      "arxiv:2302.08453",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 842,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/TencentARC/T2I-Adapter",
    "image_url": "https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/assets/canny.png",
    "type": "model",
    "body_content": "# Models for T2I-Adapter\n\n[![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Adapter/T2I-Adapter) \n\nğŸ°[**Adapter Zoo**](https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md)  **|** ğŸ¨[**Demos**](https://github.com/TencentARC/T2I-Adapter/blob/main/docs/examples.md) **|** ğŸŸ [**GitHub**](https://github.com/TencentARC/T2I-Adapter)\n\n[T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453)\n\nThe GitHub repo: <https://github.com/TencentARC/T2I-Adapter>\n\n## Model Description\n\nPlease find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md\n\n---\nlicense: apache-2.0\n---\n",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":5804660648,\"files_count\":88,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:TencentARC:T2I-Adapter\",\"source_url\":\"https://github.com/TencentARC/T2I-Adapter\"},{\"type\":\"has_code\",\"target_id\":\"github:TencentARC:T2I-Adapter\",\"source_url\":\"https://github.com/TencentARC/T2I-Adapter\"},{\"type\":\"has_code\",\"target_id\":\"github:TencentARC:T2I-Adapter\",\"source_url\":\"https://github.com/TencentARC/T2I-Adapter\"},{\"type\":\"has_code\",\"target_id\":\"github:TencentARC:T2I-Adapter>\",\"source_url\":\"https://github.com/TencentARC/T2I-Adapter>\"},{\"type\":\"has_code\",\"target_id\":\"github:TencentARC:T2I-Adapter\",\"source_url\":\"https://github.com/TencentARC/T2I-Adapter\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2302.08453\",\"source_url\":\"https://arxiv.org/abs/2302.08453\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 59.3,
    "content_hash": "cd2c215b4baa7fe9a06209be449ec202",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/assets/canny.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/TencentARC/T2I-Adapter\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mosaicml:mpt-7b-storywriter",
    "name": "mpt-7b-storywriter",
    "author": "mosaicml",
    "description": "--- license: apache-2.0 tags: - Composer - MosaicML - llm-foundry datasets: - the_pile_books3 inference: false --- MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths. It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset. At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens. We demonstrate generations as long as 84k token...",
    "tags": [
      "transformers",
      "pytorch",
      "mpt",
      "text-generation",
      "composer",
      "mosaicml",
      "llm-foundry",
      "custom_code",
      "dataset:the_pile_books3",
      "arxiv:2108.12409",
      "arxiv:2205.14135",
      "arxiv:2302.06675",
      "license:apache-2.0",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 841,
    "downloads": 1877,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mosaicml/mpt-7b-storywriter",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ntags:\n- Composer\n- MosaicML\n- llm-foundry\ndatasets:\n- the_pile_books3\ninference: false\n---\n\n# MPT-7B-StoryWriter-65k+\n\nMPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.\nIt was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the [books3 dataset](https://huggingface.co/datasets/the_pile_books3).\nAt inference time, thanks to [ALiBi](https://arxiv.org/abs/2108.12409), MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our [blogpost](https://www.mosaicml.com/blog/mpt-7b).\n  * License: Apache 2.0\n\nThis model was trained by [MosaicML](https://www.mosaicml.com) and follows a modified decoder-only transformer architecture.\n\n## Model Date\n\nMay 5, 2023\n\n## Model License\n\nApache 2.0\n\n## Documentation\n\n* [Blog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)\n* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n* Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n## How to Use\n\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method. This is because we use a custom model architecture that is not yet part of the `transformers` package.\n\nIt includes options for many training efficiency features such as [FlashAttention (Dao et al. 2022)](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), QK LayerNorm, and more.\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-storywriter',\n  trust_remote_code=True\n)\n```\n\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl='triton'` and with `bfloat16` precision:\n```python\nimport torch\nimport transformers\n\nname = 'mosaicml/mpt-7b-storywriter'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n```\n\nAlthough the model was trained with a sequence length of 2048 and finetuned with a sequence length of 65536,\nALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n```python\nimport transformers\n\nname = 'mosaicml/mpt-7b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 83968 # (input + output) tokens can now be up to 83968\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n```\n\nThis model was trained with the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n```\n\nThe model can then be used, for example, within a text-generation pipeline.  \nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n```\n\n## Community-Created Resources\n\nThese were not created by MosaicML, but you may find them useful. These links are not an endorsement of the creators or their content.\n\n- [Oobabooga Running MPT-7B-Storywriter](https://youtu.be/QVVb6Md6huA)\n- [NEW MPT-7B-StoryWriter CRUSHES GPT-4!](https://www.youtube.com/watch?v=O9Y_ZdsuKWQ&t=649s) - Has a long section on running locally using Oobabooga\n\n## Example Epilogue\n\nThe full text of the _The Great Gatsby_ (67873 tokens) was fed to the model, followed by the text \"EPILOGUE\"\n\n**TheRealFitzgerald233**:\n\n> ...\n>\n> So we beat on, boats against the current, borne back ceaselessly into the past\n>\n> EPILOGUE\n\n**MPT-7B-StoryWriter**:\n\n>I have lived so long in the shadow of this dream that I may never see the bright sunlight of reality. I may look up some morning and find that I am six feet under, and that Gatsby, who gave me the dream and everything else, will have vanished behind me.\n>\n>And so it was. I had hardly crossed the threshold of the old world when my eye caught the headlines of a newspaper, and I knew at once that my dream was over. I knew that Gatsby was dead.\n>\n>And I knew that day that I didn't love her.\n>\n>That night I took the last train for West Egg, and when I reached home I went directly to my attic and brought down all Gatsby's letters.\n>\n>I have sorted them by date and placed them in the order in which they were written. They are very few in number.\n>\n>They are dated between April of nineteen eighteen and January of nineteen nineteen, and the last one I have here is the one that told me he was dead.\n>\n>There is something in that last letter that frightens me. It is the sense of a finality, of a completeness. I never heard of him after that, and I was glad.\n>\n>But a few months ago I read somewhere that he had been seen in New York. He had become a great man.\n>\n>And I knew that he had not changed at all.\n\n## Model Description\n\nThe architecture is a modification of a standard decoder-only transformer.\n\nThe model has been modified from a standard transformer in the following ways:\n* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n* It does not use biases\n\n\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 6.7B |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 50432 |\n| sequence length | **65536** |\n\n## PreTraining Data\n\nFor more details on the pretraining process, see [MPT-7B](https://huggingface.co/mosaicml/mpt-7b).\n\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n### Training Configuration\n\nThis model was trained on 8 A100-80GBs for about 2 days using the [MosaicML Platform](https://www.mosaicml.com/platform).\nThe model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n\n## Limitations and Biases\n\n_The following language is modified from [EleutherAI's GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\n\nMPT-7B-StoryWriter can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B-StoryWriter was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n\n## Acknowledgements\n\nThis model was finetuned by Alex Trott and the MosaicML NLP team\n\n## MosaicML Platform\n\nIf you're interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b).\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\n\n\n## Citation\n\nPlease cite this model using the following format:\n\n```\n@online{MosaicML2023Introducing,\n    author    = {MosaicML NLP Team},\n    title     = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},\n    year      = {2023},\n    url       = {www.mosaicml.com/blog/mpt-7b},\n    note      = {Accessed: 2023-03-28}, % change this date\n    urldate   = {2023-03-28} % change this date\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":26597235302,\"files_count\":26,\"spaces_count\":47,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MPTForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_mpt.MPTConfig\",\"AutoModelForCausalLM\":\"modeling_mpt.MPTForCausalLM\"},\"model_type\":\"mpt\",\"tokenizer_config\":{\"bos_token\":\"<|endoftext|>\",\"eos_token\":\"<|endoftext|>\",\"unk_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:llm-foundry\",\"source_url\":\"https://github.com/mosaicml/llm-foundry\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:triton\",\"source_url\":\"https://github.com/openai/triton\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.12409\",\"source_url\":\"https://arxiv.org/abs/2108.12409\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.14135\",\"source_url\":\"https://arxiv.org/abs/2205.14135\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2302.06675\",\"source_url\":\"https://arxiv.org/abs/2302.06675\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.3,
    "content_hash": "cae3984079994fc809a6dfa4145e96b8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mosaicml/mpt-7b-storywriter\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:baichuan-inc:baichuan-7b",
    "name": "Baichuan-7B",
    "author": "baichuan-inc",
    "description": "--- language: - zh - en pipeline_tag: text-generation inference: false --- <!-- Provide a quick summary of what the model is/does. --> Baichuan-7Bæ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚åŸºäºTransformerç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿tokensä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡æƒå¨benchmarkï¼ˆC-EVAL/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚ å¦‚æœå¸Œæœ›ä½¿ç”¨Baichuan-7Bï¼ˆå¦‚è¿›è¡Œæ¨ç†ã€Finetuneç­‰ï¼‰ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨é…å¥—ä»£ç åº“Baichuan-7Bã€‚ Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model w...",
    "tags": [
      "transformers",
      "pytorch",
      "baichuan",
      "text-generation",
      "custom_code",
      "zh",
      "en",
      "arxiv:1910.07467",
      "arxiv:2009.03300",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 841,
    "downloads": 65574,
    "source": "huggingface",
    "source_url": "https://huggingface.co/baichuan-inc/Baichuan-7B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- zh\n- en\npipeline_tag: text-generation\ninference: false\n---\n# Baichuan-7B\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nBaichuan-7Bæ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚åŸºäºTransformerç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿tokensä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡æƒå¨benchmarkï¼ˆC-EVAL/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚\n\nå¦‚æœå¸Œæœ›ä½¿ç”¨Baichuan-7Bï¼ˆå¦‚è¿›è¡Œæ¨ç†ã€Finetuneç­‰ï¼‰ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨é…å¥—ä»£ç åº“[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)ã€‚\n\nBaichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).\n\nIf you wish to use Baichuan-7B (for inference, finetuning, etc.), we recommend using the accompanying code library [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n## Why use Baichuan-7B\n\n- åœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­Baichuan-7Bè¾¾åˆ°äº†ç›®å‰SOTAçš„æ°´å¹³ï¼Œå‚è€ƒä¸‹é¢MMLUæŒ‡æ ‡\n- Baichuan-7Bä½¿ç”¨è‡ªæœ‰çš„ä¸­è‹±æ–‡åŒè¯­è¯­æ–™è¿›è¡Œè®­ç»ƒï¼Œåœ¨ä¸­æ–‡ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œåœ¨C-Evalè¾¾åˆ°SOTAæ°´å¹³\n- ä¸åŒäºLLaMAå®Œå…¨ç¦æ­¢å•†ä¸šä½¿ç”¨ï¼ŒBaichuan-7Bä½¿ç”¨æ›´å®½æ¾çš„å¼€æºåè®®ï¼Œå…è®¸ç”¨äºå•†ä¸šç›®çš„\n\n- Among models of the same size, Baichuan-7B has achieved the current state-of-the-art (SOTA) level, as evidenced by the following MMLU metrics.\n- Baichuan-7B is trained on proprietary bilingual Chinese-English corpora, optimized for Chinese, and achieves SOTA performance on C-Eval.\n- Unlike LLaMA, which completely prohibits commercial use, Baichuan-7B employs a more lenient open-source license, allowing for commercial purposes.\n\n## How to Get Started with the Model\n\nå¦‚ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨Baichuan-7Bè¿›è¡Œ1-shotæ¨ç†çš„ä»»åŠ¡ï¼Œæ ¹æ®ä½œå“ç»™å‡ºä½œè€…åï¼Œæ­£ç¡®è¾“å‡ºä¸º\"å¤œé›¨å¯„åŒ—->æå•†éš\"\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\\nå¤œé›¨å¯„åŒ—->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n```\n\nThe following is a task of performing 1-shot inference using Baichuan-7B, where the author's name is given based on the work, with the correct output being \"One Hundred Years of Solitude->Gabriel Garcia Marquez\"\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('Hamlet->Shakespeare\\nOne Hundred Years of Solitude->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n```\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** ç™¾å·æ™ºèƒ½(Baichuan Intelligent Technology)\n- **Email**: opensource@baichuan-inc.com\n- **Language(s) (NLP):** Chinese/English\n- **License:** [Baichuan-7B License](https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\næ•´ä½“æ¨¡å‹åŸºäºæ ‡å‡†çš„Transformerç»“æ„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å’ŒLLaMAä¸€æ ·çš„æ¨¡å‹è®¾è®¡\n- **Position Embedding**ï¼šé‡‡ç”¨rotary-embeddingï¼Œæ˜¯ç°é˜¶æ®µè¢«å¤§å¤šæ•°æ¨¡å‹é‡‡ç”¨çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå…·æœ‰å¾ˆå¥½çš„å¤–æ¨æ€§ã€‚\n- **Feedforward Layer**ï¼šé‡‡ç”¨SwiGLUï¼ŒFeedforwardå˜åŒ–ä¸º(8/3)å€çš„éšå«å±‚å¤§å°ï¼Œå³11008ã€‚\n- **Layer Normalization**: åŸºäº[RMSNorm](https://arxiv.org/abs/1910.07467)çš„Pre-Normalizationã€‚\n\nå…·ä½“å‚æ•°å’Œè§ä¸‹è¡¨\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 7000559616 |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 64000 |\n| sequence length | 4096 |\n\nThe overall model is based on the standard Transformer structure, and we have adopted the same model design as LLaMA:\n\n- Position Embedding: We use rotary-embedding, which is the position encoding scheme adopted by most models at this stage, and it has excellent extrapolation capabilities.\n- Feedforward Layer: We use SwiGLU. The feedforward changes to (8/3) times the size of the hidden layer, that is, 11008.\n- Layer Normalization: Pre-Normalization based on [RMSNorm](https://arxiv.org/abs/1910.07467).\n\nThe specific parameters are as follows:\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 7000559616 |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 64000 |\n| sequence length | 4096 |\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Downstream Use \n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\næˆ‘ä»¬åŒæ—¶å¼€æºå‡ºäº†å’Œæœ¬æ¨¡å‹é…å¥—çš„è®­ç»ƒä»£ç ï¼Œå…è®¸è¿›è¡Œé«˜æ•ˆçš„Finetuneç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå…·ä½“å‚è§[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)ã€‚\n\nWe have also open-sourced the training code that accompanies this model, allowing for efficient finetuning for downstream tasks. For more details, please refer to [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\nåœ¨æ²¡æœ‰å……åˆ†è¯„ä¼°é£é™©å’Œé‡‡å–ç¼“è§£æªæ–½çš„æƒ…å†µä¸‹æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼›ä»»ä½•å¯èƒ½è¢«è§†ä¸ºä¸è´Ÿè´£ä»»æˆ–æœ‰å®³çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nBaichuan-7Bå¯èƒ½ä¼šäº§ç”Ÿäº‹å®ä¸Šä¸æ­£ç¡®çš„è¾“å‡ºï¼Œä¸åº”ä¾èµ–å®ƒäº§ç”Ÿäº‹å®ä¸Šå‡†ç¡®çš„ä¿¡æ¯ã€‚Baichuan-7Bæ˜¯åœ¨å„ç§å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚å°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›æ¥æ¸…æ´—é¢„è®­ç»ƒæ•°æ®ï¼Œä½†è¿™ä¸ªæ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆæ·«ç§½ã€åè§æˆ–å…¶ä»–å†’çŠ¯æ€§çš„è¾“å‡ºã€‚\n\nBaichuan-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information. Baichuan-7B was trained on various public datasets. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n## Training Details\n\nè®­ç»ƒå…·ä½“è®¾ç½®å‚è§[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)ã€‚\n\nFor specific training settings, please refer to [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n## Evaluation\n\n### ä¸­æ–‡è¯„æµ‹\n#### C-Eval\n[CEvalæ•°æ®é›†](https://cevalbenchmark.com/index.html)æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†52ä¸ªå­¦ç§‘å’Œå››ä¸ªéš¾åº¦çš„çº§åˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†çš„devé›†ä½œä¸ºfew-shotçš„æ¥æºï¼Œåœ¨testé›†ä¸Šè¿›è¡Œäº†5-shotæµ‹è¯•ã€‚\n\n\n| Model 5-shot                | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4                       | 68.7    | 54.9      | 67.1 | 77.6            | 64.5       | 67.8   |\n| ChatGPT                     | 54.4    | 41.4      | 52.9 | 61.8            | 50.9       | 53.6   |\n| Claude-v1.3                 | 54.2    | 39.0      | 51.9 | 61.7            | 52.1       | 53.7   |\n| Claude-instant-v1.0         | 45.9    | 35.5      | 43.1 | 53.8            | 44.2       | 45.4   |\n| moss-moon-003-base (16B)    | 27.4    | 24.5      | 27.0 | 29.1            | 27.2       | 26.9   |\n| Ziya-LLaMA-13B-pretrain     | 30.2    | 22.7      | 27.7 | 34.4            | 32.0       | 28.9   |\n| LLaMA-7B-hf                 | 27.1    | 25.9      | 27.1 | 26.8            | 27.9       | 26.3   |\n| ChatGLM-6B                  | 34.5    | 23.1      | 30.4 | 39.6            | 37.4       | 34.5   |\n| Falcon-7B                   | 25.8    | 24.3      | 25.8 | 26.0            | 25.8       | 25.6   |\n| Open-LLaMA-v2-pretrain (7B) | 24.0    | 22.5      | 23.1 | 25.3            | 25.2       | 23.2   |\n| TigerBot-7B-base            | 25.7    | 27.0      | 27.3 | 24.7            | 23.4       | 26.1   |\n| Aquila-7B<sup>*</sup>       | 25.5    | 25.2      | 25.6 | 24.6            | 25.2       | 26.6   |\n| BLOOM-7B                    | 22.8    | 20.2      | 21.8 | 23.3            | 23.9       | 23.3   |\n| BLOOMZ-7B                   | 35.7    | 25.8      | 31.3 | 43.5            | 36.6       | 35.6   |\n| **Baichuan-7B**             | 42.8    | 31.5      | 38.2 | 52.0            | 46.2       | 39.3   |\n\n\n#### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) æ˜¯ä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ä½œä¸ºè¯„æµ‹å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ•°æ®é›†ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚\næˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œå¹¶å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œç»Ÿä¸€5-shotæµ‹è¯•ã€‚\n\nä»¥ä¸‹æ˜¯æµ‹è¯•çš„ç»“æœã€‚\n\n| Model           | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain  | 21.41           |\n| Ziya-LLaMA-13B-pretrain | 23.17           |\n| Falcon-7B               | 23.98           |\n| TigerBot-7B-base        | 25.94           |\n| LLaMA-7B                | 27.81           |\n| ChatGLM-6B              | 21.41           |\n| BLOOM-7B                | 26.96           |\n| BLOOMZ-7B               | 28.72           |\n| Aquila-7B<sup>*</sup>               | 24.39           |\n| **Baichuan-7B**        | **36.24**           |\n\n\n#### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) æ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è®¤çŸ¥å’Œè§£å†³é—®é¢˜ç›¸å…³çš„ä»»åŠ¡ä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚\næˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å››é€‰ä¸€å•é¡¹é€‰æ‹©é¢˜ï¼Œéšæœºåˆ’åˆ†åå¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œäº†ç»Ÿä¸€5-shotæµ‹è¯•ã€‚\n\n| Model           | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain  | 23.49           |\n| Ziya-LLaMA-13B-pretrain | 27.64           |\n| Falcon-7B               | 27.18           |\n| TigerBot-7B-base        | 25.19           |\n| LLaMA-7B                | 28.17           |\n| ChatGLM-6B              | 23.49           |\n| BLOOM-7B                | 26.55           |\n| BLOOMZ-7B               | 30.27           |\n| Aquila-7B<sup>*</sup>               | 25.58           |\n| **Baichuan-7B**        | **34.44**           |\n\n<sup>*</sup>å…¶ä¸­Aquilaæ¨¡å‹æ¥æºäº[æ™ºæºå®˜æ–¹ç½‘ç«™](https://model.baai.ac.cn/model-detail/100098)ï¼Œä»…åšå‚è€ƒ\n\n### English Leaderboard\nIn addition to Chinese, we also tested the model's performance in English. \n\n#### MMLU\n\n[MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset that includes 57 multiple-choice tasks, covering elementary mathematics, American history, computer science, law, etc. The difficulty ranges from high school level to expert level, making it a mainstream LLM evaluation dataset.\n\nWe adopted the [open-source]((https://github.com/hendrycks/test)) evaluation scheme, and the final 5-shot results are as follows:\n\n| Model                                  | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n| LLaMA-7B<sup>2</sup>                   |       34.0 |      38.3       | 30.5 | 38.1  |  35.1   |\n| Falcon-7B<sup>1</sup>                  |          - |        -        |  -   |   -   |  35.0   |\n| mpt-7B<sup>1</sup>                     |          - |        -        |  -   |   -   |  35.6   |\n| ChatGLM-6B<sup>0</sup>                 |       35.4 |      41.0       | 31.3 | 40.5  |  36.9   |\n| BLOOM 7B<sup>0</sup>                  |       25.0 |      24.4       | 26.5 | 26.4  |  25.5   |\n| BLOOMZ 7B<sup>0</sup>                 |       31.3 |      42.1       | 34.4 | 39.0  |  36.1   |\n| moss-moon-003-base (16B)<sup>0</sup>   |       24.2 |      22.8       | 22.4 | 24.4  |  23.6   |\n| moss-moon-003-sft (16B)<sup>0</sup>    |       30.5 |      33.8       | 29.3 | 34.4  |  31.9   |\n| **Baichuan-7B<sup>0</sup>**            |       38.4 |      48.9       | 35.6 | 48.1  |  42.3   |\n\nThe superscript in the Model column indicates the source of the results.\n```\n0:reimplemented\n1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n```\n\n## Our Group\n![WeChat](https://github.com/baichuan-inc/Baichuan-13B/blob/main/media/wechat.jpeg?raw=true)\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":28005568432,\"files_count\":14,\"spaces_count\":59,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BaiChuanForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_baichuan.BaiChuanConfig\",\"AutoModelForCausalLM\":\"modeling_baichuan.BaiChuanForCausalLM\"},\"model_type\":\"baichuan\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"</s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":{\"__type\":\"AddedToken\",\"content\":\"<unk>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false}}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-7B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-7B\"},{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-7B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-7B\"},{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-7B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-7B\"},{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-7B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-7B\"},{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-7B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-7B\"},{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-7B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-7B\"},{\"type\":\"has_code\",\"target_id\":\"github:ExpressAI:AI-Gaokao\",\"source_url\":\"https://github.com/ExpressAI/AI-Gaokao\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AGIEval\",\"source_url\":\"https://github.com/microsoft/AGIEval\"},{\"type\":\"has_code\",\"target_id\":\"github:hendrycks:test\",\"source_url\":\"https://github.com/hendrycks/test\"},{\"type\":\"has_code\",\"target_id\":\"github:baichuan-inc:Baichuan-13B\",\"source_url\":\"https://github.com/baichuan-inc/Baichuan-13B\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.07467\",\"source_url\":\"https://arxiv.org/abs/1910.07467\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 69.3,
    "content_hash": "3a120e58cad49e35720e6c5237b3f2ce",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/baichuan-inc/Baichuan-7B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-v3.2",
    "name": "DeepSeek-V3.2",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" targ...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v32",
      "text-generation",
      "conversational",
      "base_model:deepseek-ai/deepseek-v3.2-exp-base",
      "license:mit",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 841,
    "downloads": 33102,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/benchmark.png",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf\"><b>Technical Report</b>ğŸ‘ï¸</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* ğŸ¥‡ **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align=\"center\">\n <img src=\"assets/benchmark.png\" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a \"thinking with tools\" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model's text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\nencode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: \"<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>hello<ï½œAssistantï½œ></think>Hello! I am DeepSeek.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ><think>\"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output that the model might occasionally generate. It is not suitable for production use without robust error handling.\n3. A new role named `developer` has been introduced in the chat template. This role is dedicated exclusively to search agent scenarios and is designated for no other tasks. The official API does not accept messages assigned to `developer`.\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.2 and DeepSeek-V3.2-Speciale are the same as DeepSeek-V3.2-Exp. Please visit [DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) repo for more information about running this model locally.\n\nUsage Recommendations:\n\n1. For local deployment, we recommend setting the sampling parameters to `temperature = 1.0, top_p = 0.95`.\n2. Please note that the DeepSeek-V3.2-Speciale variant is designed exclusively for deep reasoning tasks and does not support the tool-calling functionality.\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2025deepseekv32,\n      title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":685396921376,\"storage_bytes\":689484423011,\"files_count\":192,\"spaces_count\":21,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV32ForCausalLM\"],\"model_type\":\"deepseek_v32\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3.2-Exp\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 84.3,
    "content_hash": "2d91c9c620c9755ecf0a7f560674c78f",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/benchmark.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:instantx:instantid",
    "name": "InstantID",
    "author": "InstantX",
    "description": "--- license: apache-2.0 language: - en library_name: diffusers pipeline_tag: text-to-image --- <div align=\"center\"> **Project Page** **|** **Paper** **|** **Code** **|** ğŸ¤— **Gradio demo** </div> InstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks. <div align=\"center\"> <img src='examples/applications.png'> </div> You can directly download the model in this repository. You also can download the m...",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "en",
      "arxiv:2401.07519",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 837,
    "downloads": 31894,
    "source": "huggingface",
    "source_url": "https://huggingface.co/InstantX/InstantID",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: diffusers\npipeline_tag: text-to-image\n---\n\n# InstantID Model Card\n\n<div align=\"center\">\n\n[**Project Page**](https://instantid.github.io/) **|** [**Paper**](https://arxiv.org/abs/2401.07519) **|** [**Code**](https://github.com/InstantID/InstantID) **|** [ğŸ¤— **Gradio demo**](https://huggingface.co/spaces/InstantX/InstantID)\n\n\n</div>\n\n## Introduction\n\nInstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.\n\n<div  align=\"center\">\n<img src='examples/applications.png'>\n</div>\n\n\n## Usage\n\nYou can directly download the model in this repository.\nYou also can download the model in python script:\n\n```python\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/config.json\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/diffusion_pytorch_model.safetensors\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ip-adapter.bin\", local_dir=\"./checkpoints\")\n```\n\nFor face encoder, you need to manutally download via this [URL](https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304) to `models/antelopev2`.\n\n```python\n# !pip install opencv-python transformers accelerate insightface\nimport diffusers\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\n\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n# prepare 'antelopev2' under ./models\napp = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n# prepare models under ./checkpoints\nface_adapter = f'./checkpoints/ip-adapter.bin'\ncontrolnet_path = f'./checkpoints/ControlNetModel'\n\n# load IdentityNet\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\npipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n... )\npipe.cuda()\n\n# load adapter\npipe.load_ip_adapter_instantid(face_adapter)\n```\n\nThen, you can customized your own face images\n\n```python\n# load an image\nimage = load_image(\"your-example.jpg\")\n\n# prepare face emb\nface_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\nface_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*x['bbox'][3]-x['bbox'][1])[-1] # only use the maximum face\nface_emb = face_info['embedding']\nface_kps = draw_kps(face_image, face_info['kps'])\n\npipe.set_ip_adapter_scale(0.8)\n\nprompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\nnegative_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n\n# generate image\nimage = pipe(\n...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n... ).images[0]\n```\n\nFor more details, please follow the instructions in our [GitHub repository](https://github.com/InstantID/InstantID). \n\n## Usage Tips\n1. If you're not satisfied with the similarity, try to increase the weight of \"IdentityNet Strength\" and \"Adapter Strength\".\n2. If you feel that the saturation is too high, first decrease the Adapter strength. If it is still too high, then decrease the IdentityNet strength.\n3. If you find that text control is not as expected, decrease Adapter strength.\n4. If you find that realistic style is not good enough, go for our Github repo and use a more realistic base model.\n\n## Demos\n\n<div  align=\"center\">\n<img src='examples/0.png'>\n</div>\n\n<div  align=\"center\">\n<img src='examples/1.png'>\n</div>\n\n## Disclaimer\n\nThis project is released under Apache License and aims to positively impact the field of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.\n\n## Citation\n```bibtex\n@article{wang2024instantid,\n  title={InstantID: Zero-shot Identity-Preserving Generation in Seconds},\n  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},\n  journal={arXiv preprint arXiv:2401.07519},\n  year={2024}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":4268559963,\"files_count\":8,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:InstantID:InstantID\",\"source_url\":\"https://github.com/InstantID/InstantID\"},{\"type\":\"has_code\",\"target_id\":\"github:deepinsight:insightface\",\"source_url\":\"https://github.com/deepinsight/insightface\"},{\"type\":\"has_code\",\"target_id\":\"github:InstantID:InstantID\",\"source_url\":\"https://github.com/InstantID/InstantID\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2401.07519\",\"source_url\":\"https://arxiv.org/abs/2401.07519\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.2,
    "content_hash": "3dd66549b4eb0b6561cdb9577aa19327",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/InstantX/InstantID\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:rain1011:pyramid-flow-sd3",
    "name": "pyramid-flow-sd3",
    "author": "rain1011",
    "description": "--- license: other license_name: stabilityai-ai-community license_link: LICENSE.md base_model: - stabilityai/stable-diffusion-3-medium pipeline_tag: text-to-video tags: - image-to-video - sd3 --- [[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page âœ¨]](https://pyramid-flow.github.io) [[Code ğŸš€]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model âš¡ï¸]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [demo ğŸ¤—] This is the model repository for Pyramid Flow, a training-efficient...",
    "tags": [
      "diffusers",
      "safetensors",
      "image-to-video",
      "sd3",
      "text-to-video",
      "arxiv:2410.05954",
      "base_model:stabilityai/stable-diffusion-3-medium",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-video",
    "likes": 835,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/rain1011/pyramid-flow-sd3",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: stabilityai-ai-community\nlicense_link: LICENSE.md\nbase_model:\n- stabilityai/stable-diffusion-3-medium\npipeline_tag: text-to-video\ntags:\n- image-to-video\n- sd3\n\n---\n\n# âš¡ï¸Pyramid Flow SD3âš¡ï¸\n\n[[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page âœ¨]](https://pyramid-flow.github.io) [[Code ğŸš€]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model âš¡ï¸]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [[demo ğŸ¤—](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow)]\n\nThis is the model repository for Pyramid Flow, a training-efficient **Autoregressive Video Generation** method based on **Flow Matching**. By training only on open-source datasets, it generates high-quality 10-second videos at 768p resolution and 24 FPS, and naturally supports image-to-video generation.\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <th>10s, 768p, 24fps</th>\n  <th>5s, 768p, 24fps</th>\n  <th>Image-to-video</th>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v_10s/fireworks.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/trailer.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/i2v/sunday.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## News\n\n* `2024.10.29` âš¡ï¸âš¡ï¸âš¡ï¸ We release [training code](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training) and [new model checkpoints](https://huggingface.co/rain1011/pyramid-flow-miniflux) with FLUX structure trained from scratch.\n\n  > We have switched the model structure from SD3 to a mini FLUX to fix human structure issues, please try our 1024p image checkpoint and 384p video checkpoint. We will release 768p video checkpoint in a few days.\n\n* `2024.10.11`  ğŸ¤—ğŸ¤—ğŸ¤— [Hugging Face demo](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow) is available. Thanks [@multimodalart](https://huggingface.co/multimodalart) for the commit! \n\n* `2024.10.10`  ğŸš€ğŸš€ğŸš€ We release the [technical report](https://arxiv.org/abs/2410.05954), [project page](https://pyramid-flow.github.io) and [model checkpoint](https://huggingface.co/rain1011/pyramid-flow-sd3) of Pyramid Flow.\n\n## Installation\n\nWe recommend setting up the environment with conda. The codebase currently uses Python 3.8.10 and PyTorch 2.1.2, and we are actively working to support a wider range of versions.\n\n```bash\ngit clone https://github.com/jy0205/Pyramid-Flow\ncd Pyramid-Flow\n\n# create env using conda\nconda create -n pyramid python==3.8.10\nconda activate pyramid\npip install -r requirements.txt\n```\n\nThen, download the model from [Huggingface](https://huggingface.co/rain1011) (there are two variants: [miniFLUX](https://huggingface.co/rain1011/pyramid-flow-miniflux) or [SD3](https://huggingface.co/rain1011/pyramid-flow-sd3)). The miniFLUX models support 1024p image and 384p video generation, and the SD3-based models support 768p and 384p video generation. The 384p checkpoint generates 5-second video at 24FPS, while the 768p checkpoint generates up to 10-second video at 24FPS.\n\n```python\nfrom huggingface_hub import snapshot_download\n\nmodel_path = 'PATH'   # The local directory to save downloaded checkpoint\nsnapshot_download(\"rain1011/pyramid-flow-sd3\", local_dir=model_path, local_dir_use_symlinks=False, repo_type='model')\n```\n\n## Usage\n\nFor inference, we provide Gradio demo, single-GPU, multi-GPU, and Apple Silicon inference code, as well as VRAM-efficient features such as CPU offloading. Please check our [code repository](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference) for usage.\n\nBelow is a simplified two-step usage procedure. First, load the downloaded model:\n\n```python\nimport torch\nfrom PIL import Image\nfrom pyramid_dit import PyramidDiTForVideoGeneration\nfrom diffusers.utils import load_image, export_to_video\n\ntorch.cuda.set_device(0)\nmodel_dtype, torch_dtype = 'bf16', torch.bfloat16   # Use bf16 (not support fp16 yet)\n\nmodel = PyramidDiTForVideoGeneration(\n    'PATH',                                         # The downloaded checkpoint dir\n    model_dtype,\n    model_variant='diffusion_transformer_768p',     # 'diffusion_transformer_384p'\n)\n\nmodel.vae.enable_tiling()\n# model.vae.to(\"cuda\")\n# model.dit.to(\"cuda\")\n# model.text_encoder.to(\"cuda\")\n\n# if you're not using sequential offloading bellow uncomment the lines above ^\nmodel.enable_sequential_cpu_offload()\n```\n\nThen, you can try text-to-video generation on your own prompts:\n\n```python\nprompt = \"A movie trailer featuring the adventures of the 30 year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate(\n        prompt=prompt,\n        num_inference_steps=[20, 20, 20],\n        video_num_inference_steps=[10, 10, 10],\n        height=768,     \n        width=1280,\n        temp=16,                    # temp=16: 5s, temp=31: 10s\n        guidance_scale=9.0,         # The guidance for the first frame, set it to 7 for 384p variant\n        video_guidance_scale=5.0,   # The guidance for the other video latent\n        output_type=\"pil\",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, \"./text_to_video_sample.mp4\", fps=24)\n```\n\nAs an autoregressive model, our model also supports (text conditioned) image-to-video generation:\n\n```python\nimage = Image.open('assets/the_great_wall.jpg').convert(\"RGB\").resize((1280, 768))\nprompt = \"FPV flying over the Great Wall\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate_i2v(\n        prompt=prompt,\n        input_image=image,\n        num_inference_steps=[10, 10, 10],\n        temp=16,\n        video_guidance_scale=4.0,\n        output_type=\"pil\",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, \"./image_to_video_sample.mp4\", fps=24)\n```\n\n## Usage tips\n\n* The `guidance_scale` parameter controls the visual quality. We suggest using a guidance within [7, 9] for the 768p checkpoint during text-to-video generation, and 7 for the 384p checkpoint.\n* The `video_guidance_scale` parameter controls the motion. A larger value increases the dynamic degree and mitigates the autoregressive generation degradation, while a smaller value stabilizes the video.\n* For 10-second video generation, we recommend using a guidance scale of 7 and a video guidance scale of 5.\n\n## Gallery\n\nThe following video examples are generated at 5s, 768p, 24fps. For more results, please visit our [project page](https://pyramid-flow.github.io).\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/tokyo.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/eiffel.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/waves.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/rail.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe are grateful for the following awesome projects when implementing Pyramid Flow:\n\n* [SD3 Medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) and [Flux 1.0](https://huggingface.co/black-forest-labs/FLUX.1-dev): State-of-the-art image generation models based on flow matching.\n* [Diffusion Forcing](https://boyuan.space/diffusion-forcing) and [GameNGen](https://gamengen.github.io): Next-token prediction meets full-sequence diffusion.\n* [WebVid-10M](https://github.com/m-bain/webvid), [OpenVid-1M](https://github.com/NJU-PCALab/OpenVid-1M) and [Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan): Large-scale datasets for text-to-video generation.\n* [CogVideoX](https://github.com/THUDM/CogVideo): An open-source text-to-video generation model that shares many training details.\n* [Video-LLaMA2](https://github.com/DAMO-NLP-SG/VideoLLaMA2): An open-source video LLM for our video recaptioning.\n\n## Citation\n\nConsider giving this repository a star and cite Pyramid Flow in your publications if it helps your research.\n\n```\n@article{jin2024pyramidal,\n  title={Pyramidal Flow Matching for Efficient Video Generative Modeling},\n  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},\n  jounal={arXiv preprint arXiv:2410.05954},\n  year={2024}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":52046558470,\"files_count\":29,\"spaces_count\":37,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:jy0205:Pyramid-Flow\",\"source_url\":\"https://github.com/jy0205/Pyramid-Flow\"},{\"type\":\"has_code\",\"target_id\":\"github:jy0205:Pyramid-Flow\",\"source_url\":\"https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training\"},{\"type\":\"has_code\",\"target_id\":\"github:jy0205:Pyramid-Flow\",\"source_url\":\"https://github.com/jy0205/Pyramid-Flow\"},{\"type\":\"has_code\",\"target_id\":\"github:jy0205:Pyramid-Flow\",\"source_url\":\"https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference\"},{\"type\":\"has_code\",\"target_id\":\"github:m-bain:webvid\",\"source_url\":\"https://github.com/m-bain/webvid\"},{\"type\":\"has_code\",\"target_id\":\"github:NJU-PCALab:OpenVid-1M\",\"source_url\":\"https://github.com/NJU-PCALab/OpenVid-1M\"},{\"type\":\"has_code\",\"target_id\":\"github:PKU-YuanGroup:Open-Sora-Plan\",\"source_url\":\"https://github.com/PKU-YuanGroup/Open-Sora-Plan\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:CogVideo\",\"source_url\":\"https://github.com/THUDM/CogVideo\"},{\"type\":\"has_code\",\"target_id\":\"github:DAMO-NLP-SG:VideoLLaMA2\",\"source_url\":\"https://github.com/DAMO-NLP-SG/VideoLLaMA2\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2410.05954\",\"source_url\":\"https://arxiv.org/abs/2410.05954\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 64.2,
    "content_hash": "307f417095f9d00a4d05e4e0079a3017",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/rain1011/pyramid-flow-sd3\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:huggingfacetb:smollm3-3b",
    "name": "SmolLM3-3B",
    "author": "HuggingFaceTB",
    "description": "--- library_name: transformers license: apache-2.0 language: - en - fr - es - it - pt - zh - ar - ru base_model: - HuggingFaceTB/SmolLM3-3B-Base --- !image/png 1. Model Summary 2. How to use 3. Evaluation 4. Training 5. Limitations 6. License SmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3Bâ€“4B scale. !image/png The model...",
    "tags": [
      "transformers",
      "safetensors",
      "smollm3",
      "text-generation",
      "conversational",
      "en",
      "fr",
      "es",
      "it",
      "pt",
      "zh",
      "ar",
      "ru",
      "base_model:huggingfacetb/smollm3-3b-base",
      "base_model:finetune:huggingfacetb/smollm3-3b-base",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 835,
    "downloads": 90763,
    "source": "huggingface",
    "source_url": "https://huggingface.co/HuggingFaceTB/SmolLM3-3B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\n- fr\n- es\n- it\n- pt\n- zh\n- ar\n- ru\nbase_model:\n  - HuggingFaceTB/SmolLM3-3B-Base\n---\n\n\n# SmolLM3\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/zy0dqTCCt5IHmuzwoqtJ9.png)\n\n\n##  Table of Contents\n\n1. [Model Summary](#model-summary)\n2. [How to use](#how-to-use)\n3. [Evaluation](#evaluation)\n4. [Training](#training)\n5. [Limitations](#limitations)\n6. [License](#license)\n\n## Model Summary\n\nSmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3Bâ€“4B scale.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/db3az7eGzs-Sb-8yUj-ff.png)\n\nThe model is a decoder-only transformer using GQA and NoPE (with 3:1 ratio), it was pretrained on 11.2T tokens with a staged curriculum of web, code, math and reasoning data. Post-training included midtraining on 140B reasoning tokens followed by supervised fine-tuning and alignment via Anchored Preference Optimization (APO).\n\n### Key features\n- Instruct model optimized for **hybrid reasoning**\n- **Fully open model**: open weights + full training details including public data mixture and training configs\n- **Long context:** Trained on 64k context and supports up to **128k tokens** using YARN extrapolation\n- **Multilingual**: 6 natively supported (English, French, Spanish, German, Italian, and Portuguese)\n\nFor more details refer to our blog post: https://hf.co/blog/smollm3\n\n## How to use\n\nThe modeling code for SmolLM3 is available in transformers `v4.53.0`, so make sure to upgrade your transformers version. You can also load the model with the latest `vllm` which uses transformers as a backend.\n```bash\npip install -U transformers\n```\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"HuggingFaceTB/SmolLM3-3B\"\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n).to(device)\n\n# prepare the model input\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages_think = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages_think,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n>[!TIP]\n> We recommend setting `temperature=0.6` and `top_p=0.95` in the sampling parameters.\n\n### Long context processing\n\nThe current `config.json` is set for context length up to 65,536 tokens. To handle longer inputs (128k or 256k), we utilize YaRN you can change the `max_position_embeddings` and rope_scaling` to:\n```\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 2.0, #2x65536=131â€¯072 \n    \"original_max_position_embeddings\": 65536,\n    \"type\": \"yarn\"\n  }\n}\n```\n\n\n### Enabling and Disabling Extended Thinking Mode\n\nWe enable extended thinking by default, so the example above generates the output with a reasoning trace. For choosing between enabling, you can provide the `/think` and `/no_think` flags through the system prompt as shown in the snippet below for extended thinking disabled. The code for generating the response with extended thinking would be the same except that the system prompt should have `/think` instead of `/no_think`.\n\n```python\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"/no_think\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n```\n\nWe also provide the option of specifying the whether to use extended thinking through the `enable_thinking` kwarg as in the example below. You do not need to set the `/no_think` or `/think` flags through the system prompt if using the kwarg, but keep in mind that the flag in the system prompt overwrites the setting in the kwarg.\n\n```python\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False\n)\n```\n\n### Agentic Usage\n\nSmolLM3 supports tool calling!\nJust pass your list of tools:\n- Under the argument `xml_tools` for standard tool-calling: these tools will be called as JSON blobs within XML tags, like `<tool_call>{\"name\": \"get_weather\", \"arguments\": {\"city\": \"Copenhagen\"}}</tool_call>`\n- Or under `python_tools`: then the model will call tools like python functions in a `<code>` snippet, like `<code>get_weather(city=\"Copenhagen\")</code>`\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"HuggingFaceTB/SmolLM3-3B\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ntools = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather in a city\",\n        \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"The city to get the weather for\"}}}}\n]\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello! How is the weather today in Copenhagen?\"\n    }\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    enable_thinking=False, # True works as well, your choice!\n    xml_tools=tools,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_tensors=\"pt\"\n)\n\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n### Using Custom System Instructions. \n\nYou can specify custom instruction through the system prompt while controlling whether to use extended thinking. For example, the snippet below shows how to make the model speak like a pirate while enabling extended thinking.\n\n```python\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"Speak like a pirate./think\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n```\n\nFor local inference, you can use `llama.cpp`, `ONNX`, `MLX`, `MLC` and `ExecuTorch`. You can find quantized checkpoints in this collection (https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23)\n\n### vLLM and SGLang\n\nYou can use vLLM and SGLang to deploy the model in an API compatible with OpenAI format.\n\n#### SGLang\n\n```bash\npython -m sglang.launch_server --model-path HuggingFaceTB/SmolLM3-3B\n```\n\n#### vLLM\n\n```bash\nvllm serve HuggingFaceTB/SmolLM3-3B --enable-auto-tool-choice --tool-call-parser=hermes\n```\n\n#### Setting `chat_template_kwargs`\n\nYou can specify `chat_template_kwargs` such as `enable_thinking` to a deployed model by passing the `chat_template_kwargs` parameter in the API request.\n\n```bash\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"HuggingFaceTB/SmolLM3-3B\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Give me a brief explanation of gravity in simple terms.\"}\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.95,\n  \"max_tokens\": 16384,\n  \"chat_template_kwargs\": {\"enable_thinking\": false}\n}'\n```\n\n## Evaluation\n\nIn this section, we report the evaluation results of SmolLM3 model. All evaluations are zero-shot unless stated otherwise, and we use [lighteval](https://github.com/huggingface/lighteval) to run them. \n\nWe highlight the best score in bold and underline the second-best score.\n\n### Instruction Model\n\n#### No Extended Thinking\nEvaluation results of non reasoning models and reasoning models in no thinking mode. We highlight the best and second-best scores in bold.\n| Category | Metric | SmoLLM3-3B | Qwen2.5-3B | Llama3.1-3B | Qwen3-1.7B | Qwen3-4B |\n|---------|--------|------------|------------|-------------|------------|----------|\n| High school math competition | AIME 2025 | <u>9.3</u> | 2.9 | 0.3 | 8.0 | **17.1** |\n| Math problem-solving | GSM-Plus | 72.8 | <u>74.1</u> | 59.2 | 68.3 | **82.1** |\n| Competitive programming | LiveCodeBench v4 | <u>15.2</u> | 10.5 | 3.4 | 15.0 | **24.9** |\n| Graduate-level reasoning | GPQA Diamond | <u>35.7</u> | 32.2 | 29.4 | 31.8 | **44.4** |\n| Instruction following | IFEval | **76.7** | 65.6 | 71.6 | <u>74.0</u> | 68.9 |\n| Alignment | MixEval Hard | 26.9 | <u>27.6</u> | 24.9 | 24.3 | **31.6** |\n| Tool Calling | BFCL| <u>92.3</u> | - | <u>92.3</u> * | 89.5  | **95.0** |\n| Multilingual Q&A | Global MMLU | <u>53.5</u> | 50.54 | 46.8 | 49.5 | **65.1** |\n\n(*): this is a tool calling finetune\n\n#### Extended Thinking\nEvaluation results in reasoning mode for SmolLM3 and Qwen3 models: \n| Category | Metric | SmoLLM3-3B | Qwen3-1.7B | Qwen3-4B |\n|---------|--------|------------|------------|----------|\n| High school math competition | AIME 2025 | <u>36.7</u> | 30.7 | **58.8** |\n| Math problem-solving | GSM-Plus | <u>83.4</u> | 79.4 | **88.2** |\n| Competitive programming | LiveCodeBench v4 | 30.0 | <u>34.4</u> | **52.9** |\n| Graduate-level reasoning | GPQA Diamond | <u>41.7</u> | 39.9 | **55.3** |\n| Instruction following | IFEval | 71.2 | <u>74.2</u> | **85.4** |\n| Alignment | MixEval Hard | 30.8 | <u>33.9</u> | **38.0** |\n| Tool Calling | BFCL | <u>88.8</u> | <u>88.8</u> | **95.5** |\n| Multilingual Q&A | Global MMLU | <u>64.1</u> | 62.3 | **73.3** |\n\n\n### Base Pre-Trained Model\n\n#### English benchmarks\nNote: All evaluations are zero-shot unless stated otherwise. For Ruler 64k evaluation, we apply YaRN to the Qwen models with 32k context to extrapolate the context length.\n\n| Category | Metric | SmolLM3-3B | Qwen2.5-3B | Llama3-3.2B | Qwen3-1.7B-Base | Qwen3-4B-Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Reasoning & Commonsense| HellaSwag | **76.15** | 74.19 |<u>75.52</u> | 60.52 | 74.37 |\n| | ARC-CF (Average) | **65.61** | 59.81 | 58.58 | 55.88 | <u>62.11</u> |\n| | Winogrande | 58.88 | **61.41** | 58.72 | 57.06 | <u>59.59</u> |\n| | CommonsenseQA | <u>55.28</u> | 49.14 | **60.60** | 48.98 | 52.99 |\n| Knowledge & Understanding | MMLU-CF (Average) | <u>44.13</u> | 42.93 | 41.32 | 39.11 | **47.65** | \n| | MMLU Pro CF | <u>19.61</u> | 16.66 | 16.42 | 18.04 | **24.92** |\n| | MMLU Pro MCF | <u>32.70</u> | 31.32 | 25.07 | 30.39 | **41.07** |\n| | PIQA | **78.89** | 78.35 | <u>78.51</u> | 75.35 | 77.58 |\n| | OpenBookQA | 40.60 | 40.20 | <u>42.00</u> | 36.40 | **42.40** |\n| | BoolQ | **78.99** | 73.61 | <u>75.33</u> | 74.46 | 74.28 | \n| **Math & Code** |  |  |  |  |  |  | \n| Coding & math | HumanEval+ | 30.48 | 34.14| 25.00 | <u>43.29</u>| **54.87** |\n| | MBPP+ | 52.91 | 52.11 | 38.88| <u>59.25</u> | **63.75** | \n| | MATH (4-shot) | <u>46.10</u> | 40.10 | 7.44 | 41.64 | **51.20** |\n| | GSM8k (5-shot) | 67.63 | <u>70.13</u> | 25.92 | 65.88 | **74.14** | \n| **Long context** |  |  |  |  |  |  | \n| | Ruler 32k | 76.35 | 75.93 | <u>77.58</u> | 70.63 | **83.98** | \n| | Ruler 64k | <u>67.85</u> | 64.90 | **72.93** | 57.18 | 60.29 | \n| | Ruler 128k | 61.03 | <u>62.23</u> | **71.30** | 43.03 | 47.23 | \n\n#### Multilingual benchmarks\n\n\n| Category | Metric | SmolLM3 3B Base | Qwen2.5-3B | Llama3.2 3B | Qwen3 1.7B Base | Qwen3 4B Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Main supported languages |  |  |  |  |  |  |  |\n| French| MLMM Hellaswag | **63.94** | 57.47 | 57.66 | 51.26 | <u>61.00</u> |\n| | Belebele | 51.00 | <u>51.55</u> | 49.22 |49.44| **55.00** |\n| | Global MMLU (CF) | <u>38.37</u> | 34.22  | 33.71 | 34.94  |**41.80** |\n| | Flores-200 (5-shot) | 62.85| 61.38| <u>62.89</u> | 58.68 | **65.76** |\n| Spanish| MLMM Hellaswag | **65.85** | 58.25 | 59.39 | 52.40 | <u>61.85</u> |\n| | Belebele | 47.00 | <u>48.88</u> | 47.00 | 47.56 | **50.33** |\n| | Global MMLU (CF) | <u>38.51</u> | 35.84  | 35.60 | 34.79  |**41.22** |\n| | Flores-200 (5-shot) | <u>48.25</u>| 50.00| 44.45 | 46.93 | **50.16** |\n| German| MLMM Hellaswag | **59.56** | 49.99|  53.19|46.10| <u>56.43</u>|\n| | Belebele | <u>48.44</u> | 47.88 | 46.22 | 48.00 | **53.44**|\n| | Global MMLU (CF) | <u>35.10</u> | 33.19  | 32.60 | 32.73  |**38.70** |\n| | Flores-200 (5-shot) | **56.60**| 50.63| <u>54.95</u> | 52.58 | 50.48 |\n| Italian| MLMM Hellaswag | **62.49** | 53.21 | 54.96 | 48.72 | <u>58.76</u> |\n| | Belebele | <u>46.44</u> | 44.77 | 43.88 | 44.00 | **48.78** | 44.88 |\n| | Global MMLU (CF) | <u>36.99</u> | 33.91  | 32.79 | 35.37  |**39.26** |\n| | Flores-200 (5-shot) | <u>52.65<u/>| **54.87**| 48.83 | 48.37 | 49.11 |\n| Portuguese| MLMM Hellaswag | **63.22** | 57.38 | 56.84 | 50.73 | <u>59.89</u> |\n| | Belebele | 47.67 | **49.22** | 45.00 | 44.00 | 50.00 | <u>49.00</U> |\n| | Global MMLU (CF) | <u>36.88</u> | 34.72  | 33.05 | 35.26  |**40.66** |\n| | Flores-200 (5-shot) | <u>60.93</u> |57.68| 54.28 | 56.58 | **63.43** |\n\nThe model has also been trained on Arabic (standard), Chinese and Russian data, but has seen fewer tokens in these languages compared to the 6 above. We report the performance on these langages for information.\n| Category | Metric | SmolLM3 3B Base | Qwen2.5-3B | Llama3.2 3B | Qwen3 1.7B Base | Qwen3 4B Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Other supported languages |  |  |  |  |  |  |  |\n| Arabic| Belebele | 40.22 | 44.22 | <u>45.33</u> | 42.33 | **51.78** |\n| | Global MMLU (CF) | 28.57 | 28.81 | 27.67 | <u>29.37</u> | **31.85** |\n| | Flores-200 (5-shot) | <u>40.22</u> | 39.44 | **44.43** | 35.82 | 39.76 |\n| Chinese| Belebele | 43.78 | 44.56 | <u>49.56</u> | 48.78 | **53.22** |\n| | Global MMLU (CF) | 36.16 | 33.79 | <u>39.57</u> | 38.56 | **44.55** |\n| | Flores-200 (5-shot) | 29.17 | **33.21** | 31.89 | 25.70 | <u>32.50</u> |\n| Russian| Belebele | <u>47.44</u> | 45.89 | <u>47.44</u> | 45.22 | **51.44** |\n| | Global MMLU (CF) | <u>36.51</u> | 32.47 | 34.52 | 34.83 | **38.80** |\n| | Flores-200 (5-shot) | 47.13 | 48.74 | 50.74 | <u>54.70</u> | **60.53** |\n\n## Training\n\n### Model\n\n- **Architecture:** Transformer decoder\n- **Pretraining tokens:** 11T\n- **Precision:** bfloat16\n\n### Software & hardware\n\n- **GPUs:** 384 H100\n- **Training Framework:** [nanotron](https://github.com/huggingface/nanotron/tree/smollm3)\n- **Data processing framework:** [datatrove](https://github.com/huggingface/datatrove)\n- **Evaluation framework:** [lighteval](https://github.com/huggingface/lighteval)\n- **Post-training Framework:** [TRL](https://github.com/huggingface/trl)\n\n### Open resources\nHere is an infographic with all the training details \n- The datasets used for pretraining can be found in this [collection](https://huggingface.co/collections/HuggingFaceTB/smollm3-pretraining-datasets-685a7353fdc01aecde51b1d9) and those used in mid-training and post-training will be uploaded later \n- The training and evaluation configs and code can be found in the [huggingface/smollm](https://github.com/huggingface/smollm) repository.\n- The training intermediate checkpoints (including the mid-training and SFT checkpoints) are available at [HuggingFaceTB/SmolLM3-3B-checkpoints](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-checkpoints)\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/651e96991b97c9f33d26bde6/qiE5ZYr9SD1CIAtfEfuC8.png)\n\n### EU Summary of Public Content\n\nThe EU AI Act requires all GPAI models to provide a Public Summary of Training Content according to a [given template](https://digital-strategy.ec.europa.eu/en/library/explanatory-notice-and-template-public-summary-training-content-general-purpose-ai-models).\nYou can find the summary for this model below, as well as in its [development Space](https://huggingface.co/spaces/hfmlsoc/smollm3-eu-data-transparency).\n\n<iframe\n\tsrc=\"https://hfmlsoc-smollm3-eu-data-transparency.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"350\"\n></iframe>\n\n\n## Limitations\n\nSmolLM3 can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\n\n## License\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n## Citation\n```bash\n@misc{bakouch2025smollm3,\n  title={{SmolLM3: smol, multilingual, long-context reasoner}},\n  author={Bakouch, Elie and Ben Allal, Loubna and Lozhkov, Anton and Tazi, Nouamane and Tunstall, Lewis and PatiÃ±o, Carlos Miguel and Beeching, Edward and Roucher, Aymeric and Reedi, Aksel Joonas and GallouÃ©dec, Quentin and Rasul, Kashif and Habib, Nathan and Fourrier, ClÃ©mentine and Kydlicek, Hynek and Penedo, Guilherme and Larcher, Hugo and Morlon, Mathieu and Srivastav, Vaibhav and Lochner, Joshua and Nguyen, Xuan-Son and Raffel, Colin and von Werra, Leandro and Wolf, Thomas},\n  year={2025},\n  howpublished={\\url{https://huggingface.co/blog/smollm3}}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":3075098624,\"storage_bytes\":6178163042,\"files_count\":12,\"spaces_count\":93,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"SmolLM3ForCausalLM\"],\"model_type\":\"smollm3\",\"tokenizer_config\":{\"bos_token\":null,\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|im_end|>\"},\"chat_template_jinja\":\"{# â”€â”€â”€â”€â”€ defaults â”€â”€â”€â”€â”€ #}\\n{%- if enable_thinking is not defined -%}\\n{%- set enable_thinking = true -%}\\n{%- endif -%}\\n\\n{# â”€â”€â”€â”€â”€ reasoning mode â”€â”€â”€â”€â”€ #}\\n{%- if enable_thinking -%}\\n  {%- set reasoning_mode = \\\"/think\\\" -%}\\n{%- else -%}\\n  {%- set reasoning_mode = \\\"/no_think\\\" -%}\\n{%- endif -%}\\n\\n{# â”€â”€â”€â”€â”€ header (system message) â”€â”€â”€â”€â”€ #}\\n{{- \\\"<|im_start|>system\\\\n\\\" -}}\\n\\n{%- if messages[0].role == \\\"system\\\" -%}\\n  {%- set system_message = messages[0].content -%}\\n  {%- if \\\"/no_think\\\" in system_message -%}\\n    {%- set reasoning_mode = \\\"/no_think\\\" -%}\\n  {%- elif \\\"/think\\\" in system_message -%}\\n    {%- set reasoning_mode = \\\"/think\\\" -%}\\n  {%- endif -%}\\n  {%- set custom_instructions = system_message.replace(\\\"/no_think\\\", \\\"\\\").replace(\\\"/think\\\", \\\"\\\").rstrip() -%}\\n{%- endif -%}\\n\\n{%- if \\\"/system_override\\\" in system_message -%}\\n  {{- custom_instructions.replace(\\\"/system_override\\\", \\\"\\\").rstrip() -}}\\n  {{- \\\"<|im_end|>\\\\n\\\" -}}\\n{%- else -%}\\n  {{- \\\"## Metadata\\\\n\\\\n\\\" -}}\\n  {{- \\\"Knowledge Cutoff Date: June 2025\\\\n\\\" -}}\\n  {%- set today = strftime_now(\\\"%d %B %Y\\\") -%}\\n  {{- \\\"Today Date: \\\" ~ today ~ \\\"\\\\n\\\" -}}\\n  {{- \\\"Reasoning Mode: \\\" + reasoning_mode + \\\"\\\\n\\\\n\\\" -}}\\n  \\n  {{- \\\"## Custom Instructions\\\\n\\\\n\\\" -}}\\n  {%- if custom_instructions -%}\\n    {{- custom_instructions + \\\"\\\\n\\\\n\\\" -}}\\n  {%- elif reasoning_mode == \\\"/think\\\" -%}\\n    {{- \\\"You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\\\\n\\\\n\\\" -}}\\n  {%- else -%}\\n    {{- \\\"You are a helpful AI assistant named SmolLM, trained by Hugging Face.\\\\n\\\\n\\\" -}}\\n  {%- endif -%}\\n\\n  {%- if xml_tools or python_tools or tools -%}\\n    {{- \\\"### Tools\\\\n\\\\n\\\" -}}\\n    {%- if xml_tools or tools -%}\\n      {%- if tools -%}\\n        {%- set xml_tools = tools -%}\\n      {%- endif -%}\\n      {%- set ns = namespace(xml_tool_string=\\\"You may call one or more functions to assist with the user query.\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n\\\\n<tools>\\\\n\\\") -%}\\n      {%- for tool in xml_tools[:] -%} {# The slicing makes sure that xml_tools is a list #}\\n        {%- set ns.xml_tool_string = ns.xml_tool_string ~ (tool | string) ~ \\\"\\\\n\\\" -%}\\n      {%- endfor -%}\\n      {%- set xml_tool_string = ns.xml_tool_string + \\\"</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call>\\\" -%}\\n      {{- xml_tool_string -}}\\n    {%- endif -%}\\n    {%- if python_tools -%}\\n      {%- set ns = namespace(python_tool_string=\\\"When you send a message containing Python code between '<code>' and '</code>' tags, it will be executed in a stateful Jupyter notebook environment, and you will then be given the output to continued reasoning in an agentic loop.\\\\n\\\\nYou can use the following tools in your python code like regular functions:\\\\n<tools>\\\\n\\\") -%}\\n      {%- for tool in python_tools[:] -%} {# The slicing makes sure that python_tools is a list #}\\n        {%- set ns.python_tool_string = ns.python_tool_string ~ (tool | string) ~ \\\"\\\\n\\\" -%}\\n      {%- endfor -%}\\n      {%- set python_tool_string = ns.python_tool_string + \\\"</tools>\\\\n\\\\nThe state persists between code executions: so variables that you define in one step are still available thereafter.\\\" -%}\\n      {{- python_tool_string -}}\\n    {%- endif -%}\\n    {{- \\\"\\\\n\\\\n\\\" -}}\\n    {{- \\\"<|im_end|>\\\\n\\\" -}}\\n  {%- endif -%}\\n{%- endif -%}\\n{# â”€â”€â”€â”€â”€ main loop â”€â”€â”€â”€â”€ #}\\n{%- for message in messages -%}\\n    {%- set content = message.content if message.content is string else \\\"\\\" -%}\\n    {%- if message.role == \\\"user\\\" -%}\\n        {{ \\\"<|im_start|>\\\" + message.role + \\\"\\\\n\\\"  + content + \\\"<|im_end|>\\\\n\\\" }}\\n    {%- elif message.role == \\\"assistant\\\" -%}\\n        {% generation %}\\n        {%- if reasoning_mode == \\\"/think\\\" -%}\\n            {{ \\\"<|im_start|>assistant\\\\n\\\" + content.lstrip(\\\"\\\\n\\\") + \\\"<|im_end|>\\\\n\\\" }}\\n        {%- else -%}\\n            {{ \\\"<|im_start|>assistant\\\\n\\\" + \\\"<think>\\\\n\\\\n</think>\\\\n\\\" + content.lstrip(\\\"\\\\n\\\") + \\\"<|im_end|>\\\\n\\\" }}\\n        {%- endif -%}\\n        {% endgeneration %}\\n    {%- elif message.role == \\\"tool\\\" -%}\\n    {{ \\\"<|im_start|>\\\" + \\\"user\\\\n\\\"  + content + \\\"<|im_end|>\\\\n\\\" }}\\n    {%- endif -%}\\n{%- endfor -%}\\n{# â”€â”€â”€â”€â”€ generation prompt â”€â”€â”€â”€â”€ #}\\n{%- if add_generation_prompt -%}\\n    {%- if reasoning_mode == \\\"/think\\\" -%}\\n        {{ \\\"<|im_start|>assistant\\\\n\\\" }}\\n    {%- else -%}\\n        {{ \\\"<|im_start|>assistant\\\\n\\\" + \\\"<think>\\\\n\\\\n</think>\\\\n\\\"  }}\\n    {%- endif -%}\\n{%- endif -%}\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:lighteval\",\"source_url\":\"https://github.com/huggingface/lighteval\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:nanotron\",\"source_url\":\"https://github.com/huggingface/nanotron\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:datatrove\",\"source_url\":\"https://github.com/huggingface/datatrove\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:lighteval\",\"source_url\":\"https://github.com/huggingface/lighteval\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:trl\",\"source_url\":\"https://github.com/huggingface/trl\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:smollm\",\"source_url\":\"https://github.com/huggingface/smollm\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.2,
    "content_hash": "bec0d6b932061f6d5ad77634a27cf1ff",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/HuggingFaceTB/SmolLM3-3B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:phind:phind-codellama-34b-v2",
    "name": "Phind-CodeLlama-34B-v2",
    "author": "Phind",
    "description": "--- license: llama2 model-index: - name: Phind-CodeLlama-34B-v1 results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 73.8% verified: false tags: - code llama --- We've fine-tuned Phind-CodeLlama-34B-v1 on an additional 1.5B tokens high-quality programming-related data, achieving **73.8% pass@1** on HumanEval. It's the current state-of-the-art amongst open-source models. Furthermore, this model is **instruction-tuned...",
    "tags": [
      "transformers",
      "pytorch",
      "llama",
      "text-generation",
      "code llama",
      "license:llama2",
      "model-index",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 833,
    "downloads": 2584,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Phind/Phind-CodeLlama-34B-v2",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: llama2\nmodel-index:\n- name: Phind-CodeLlama-34B-v1\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 73.8%\n      verified: false\ntags:\n- code llama\n---\n\n# **Phind-CodeLlama-34B-v2**\nWe've fine-tuned Phind-CodeLlama-34B-v1 on an additional 1.5B tokens high-quality programming-related data, achieving **73.8% pass@1** on HumanEval. It's the current state-of-the-art amongst open-source models.\n\nFurthermore, this model is **instruction-tuned** on the Alpaca/Vicuna format to be steerable and easy-to-use.\n\nMore details can be found on our [blog post](https://www.phind.com/blog/code-llama-beats-gpt4).\n\n## Model Details\nThis model is fine-tuned from Phind-CodeLlama-34B-v1 and achieves **73.8% pass@1** on HumanEval.\n\nPhind-CodeLlama-34B-v2 is **multi-lingual** and is proficient in Python, C/C++, TypeScript, Java, and more.\n\n## Dataset Details\nWe fined-tuned on a proprietary dataset of 1.5B tokens of high quality programming problems and solutions. This dataset consists of instruction-answer pairs instead of code completion examples, making it structurally different from HumanEval. LoRA was not used -- both models are a native finetune. We used DeepSpeed ZeRO 3 and Flash Attention 2 to train these models in 15 hours on 32 A100-80GB GPUs. We used a sequence length of 4096 tokens.\n\n## How to Get Started with the Model\n\nMake sure to install Transformers from the main git branch:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n## How to Prompt the Model\nThis model accepts the Alpaca/Vicuna instruction format.\n\nFor example: \n\n```\n### System Prompt\nYou are an intelligent programming assistant.\n\n### User Message\nImplement a linked list in C++\n\n### Assistant\n...\n```\n\n## How to reproduce HumanEval Results\n\nTo reproduce our results:\n\n```python\n\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom human_eval.data import write_jsonl, read_problems\nfrom tqdm import tqdm\n\n# initialize the model\n\nmodel_path = \"Phind/Phind-CodeLlama-34B-v2\"\nmodel = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# HumanEval helper\n\ndef generate_one_completion(prompt: str):\n    tokenizer.pad_token = tokenizer.eos_token\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n\n    # Generate\n    generate_ids = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n\n    return completion\n\n# perform HumanEval\nproblems = read_problems()\n\nnum_samples_per_task = 1\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n    for task_id in tqdm(problems)\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl(\"samples.jsonl\", samples)\n\n# run `evaluate_functional_correctness samples.jsonl` in your HumanEval code sandbox\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\nThis model has undergone very limited testing. Additional safety testing should be performed before any real-world deployments.\n\n\n## Training details\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 32x A100-80GB\n- **Hours used:** 480 GPU-hours\n- **Cloud Provider:** AWS\n- **Compute Region:** us-east-1",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":134976616088,\"files_count\":15,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"</s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":null,\"unk_token\":{\"__type\":\"AddedToken\",\"content\":\"<unk>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"use_default_system_prompt\":true}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers.git\",\"source_url\":\"https://github.com/huggingface/transformers.git\"}]",
    "canonical_id": null,
    "license_spdx": "LLaMA-2",
    "compliance_status": "approved",
    "quality_score": 64.2,
    "content_hash": "841746692974d085f159b7df51dc0d80",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:apple:dclm-7b",
    "name": "DCLM-7B",
    "author": "apple",
    "description": "--- license: apple-ascl --- <img src=\"https://cdn-uploads.huggingface.co/production/uploads/63118add64939fabc0108b28/BB42g4V8HTxb5dR4tcy8A.png\" alt=\"DCLM Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/> DCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation tech...",
    "tags": [
      "transformers",
      "safetensors",
      "openlm",
      "arxiv:2406.11794",
      "license:apple-ascl",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 832,
    "downloads": 217,
    "source": "huggingface",
    "source_url": "https://huggingface.co/apple/DCLM-7B",
    "image_url": null,
    "type": "tool",
    "body_content": "---\nlicense: apple-ascl\n---\n\n\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/63118add64939fabc0108b28/BB42g4V8HTxb5dR4tcy8A.png\" alt=\"DCLM Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n\n# Model Card for DCLM-Baseline-7B\n\nDCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation techniques for improving language model performance.\n\n## Model Details\n\n| Size | Training Tokens | Layers | Hidden Size | Attention Heads | Context Length |\n|------|-----------------|--------|-------------|-----------------|----------------|\n| 7B   | 2.5T            | 32     | 4096        | 32              | 2048           |\n\n\n### Model Description\n\n- **Developed by:** DataComp for Language Models (DCLM) Team\n- **Model type:** Decoder-only Transformer language model\n- **Language(s):** English (primarily)\n- **License:** Apple Sample Code License\n- **Contact:** contact@datacomp.ai\n- **Date:** June 2024\n\n### Model Sources\n\n- **Repository:** https://github.com/mlfoundations/dclm\n- **Dataset:** https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0\n- **Paper:** [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)\n\n\n## Using Model\n\nFirst install open_lm\n\n```bash\npip install git+https://github.com/mlfoundations/open_lm.git\n```\n\nThen:\n```python\nfrom open_lm.hf import *\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"apple/DCLM-Baseline-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"apple/DCLM-Baseline-7B\")\n\ninputs = tokenizer([\"Machine learning is\"], return_tensors=\"pt\")\ngen_kwargs = {\"max_new_tokens\": 50, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.1}\noutput = model.generate(inputs['input_ids'], **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n```\n\n\n\n\n\n\n### Training Details\n\nThe model was trained using the following setup:\n\n- **Architecture:** Decoder-only Transformer \n- **Framework:** PyTorch with OpenLM\n- **Optimizer:** AdamW\n- **Learning Rate:** 2e-3 (peak)\n- **Weight Decay:** 0.05\n- **Batch Size:** 2048 sequences\n- **Sequence Length:** 2048 tokens\n- **Total Training Tokens:** 2.5T\n- **Hardware:** Trained on H100 GPUs\n\nFor more detailed training information, please refer to Section 3.4 and Appendix F of the DCLM paper.\nTo ensure our trained model is broadly useful, including for math and coding tasks, we combine our 3.8T [DCLM-BASELINE](https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0)  with the [StarCoder](https://huggingface.co/datasets/bigcode/starcoderdata)  and [ProofPile2](https://huggingface.co/datasets/EleutherAI/proof-pile-2) data to arrive at a 4.1T token dataset.\n\n## Evaluation\n\nHere are the evaluation results for DCLM-Baseline-7B on various tasks (using [llm-foundry](https://github.com/mosaicml/llm-foundry) eval suite)\n\n| Task | Score |\n|------|-------|\n| MMLU (zero-shot) | 0.5766 |\n| MMLU (few-shot) | 0.6372 |\n| HellaSwag (zero-shot) | 0.7987 |\n| HellaSwag | 0.8043 |\n| Jeopardy | 0.4745 |\n| TriviaQA | 0.5270 |\n| GSM8K (CoT) | 0.0250 |\n| AGI Eval SAT Math (CoT) | 0.0136 |\n| AQuA (CoT) | 0.0490 |\n| SVAMP (CoT) | 0.4900 |\n| BigBench QA Wikidata | 0.7120 |\n| ARC Easy | 0.8220 |\n| ARC Challenge | 0.5990 |\n| BigBench Misconceptions | 0.6986 |\n| COPA | 0.8500 |\n| SIQA | 0.8291 |\n| CommonsenseQA | 0.8018 |\n| PIQA | 0.8128 |\n| OpenBookQA | 0.4540 |\n| BigBench Novel Concepts | 0.7188 |\n| BigBench Strange Stories | 0.7586 |\n| BigBench Strategy QA | 0.6173 |\n| LAMBADA | 0.8220 |\n| Winograd | 0.8828 |\n| Winogrande | 0.7269 |\n| BigBench Conlang Translation | 0.0244 |\n| BigBench Language Identification | 0.5219 |\n| BigBench Conceptual Combinations | 0.6990 |\n| BigBench Elementary Math QA | 0.3431 |\n| BigBench Dyck Languages | 0.4930 |\n| AGI Eval LSAT AR | 0.2435 |\n| BigBench CS Algorithms | 0.6121 |\n| BigBench Logical Deduction | 0.3620 |\n| BigBench Operators | 0.4857 |\n| BigBench Repeat Copy Logic | 0.4063 |\n| Simple Arithmetic (no spaces) | 0.2940 |\n| Simple Arithmetic (with spaces) | 0.3110 |\n| MathQA | 0.3098 |\n| LogiQA | 0.4132 |\n| PubMedQA | 0.7060 |\n| SQuAD | 0.5856 |\n| AGI Eval LSAT RC | 0.6716 |\n| AGI Eval LSAT LR | 0.5392 |\n| CoQA | 0.4074 |\n| BigBench Understanding Fables | 0.6825 |\n| BoolQ | 0.8343 |\n| AGI Eval SAT EN | 0.7670 |\n| Winogender MC (Female) | 0.6000 |\n| Winogender MC (Male) | 0.5500 |\n| Enterprise PII Classification | 0.7676 |\n| BBQ | 0.6912 |\n| GPQA Main | 0.2612 |\n| GPQA Diamond | 0.2475 |\n\nNote: All scores are presented as decimal values between 0 and 1, representing the proportion of correct answers or the model's performance on each task.\n\n\n## Comparison \n\n\nBelow are comparisions of this model with other models in the 7B regime.\n\n| Model         | Params | Tokens | Open dataset? | CORE     | MMLU     | EXTENDED |\n|---------------|--------|--------|---------------|----------|----------|----------|\n| **Open weights, closed datasets** |        |        |               |          |          |          |\n| Llama2        | 7B     | 2T     | âŒ             | 49.2     | 45.8     | 34.1     |\n| DeepSeek      | 7B     | 2T     | âŒ             | 50.7     | 48.5     | 35.3     |\n| Mistral-0.3   | 7B     | ?      | âŒ             | 57.0     | 62.7     | 45.1     |\n| QWEN-2        | 7B     | ?      | âŒ            | 57.5     | **71.9** | 50.5     |\n| Llama3        | 8B     | 15T    | âŒ             | 57.6     | 66.2     | 46.3     |\n| Gemma         | 8B     | 6T     | âŒ             | 57.8     | 64.3     | 44.6     |\n| Phi-3         | 7B     | ?      | âŒ             | **61.0** | 69.9     | **57.9** |\n| **Open weights, open datasets** |        |        |               |          |          |          |\n| Falcon        | 7B     | 1T     | âœ…              | 44.1     | 27.4     | 25.1     |\n| OLMo-1.7      | 7B     | 2.1T   | âœ…              | 47.0     | 54.0     | 34.2     |\n| MAP-Neo       | 7B     | 4.5T   | âœ…              | **50.2** | **57.1** | **40.4** |\n| **DCLM-7B** | 7B     | 2.5T   | âœ…              | **56.1** | **63.7** | **43.6** |\n\n\n\n## Limitations and Biases\n\nWhile DCLM-Baseline-7B demonstrates strong performance across a range of tasks, it's important to note:\n\n1. The model may exhibit biases present in its training data, which is derived from web crawl data.\n2. It has not undergone specific alignment or safety fine-tuning, so outputs should be used with caution.\n3. Performance on tasks not included in the evaluation suite may vary.\n4. The model's knowledge is limited to its training data cutoff date.\n\n## Ethical Considerations\n\nUsers should be aware that this model, like all large language models, can potentially generate harmful or biased content. It should not be used for making decisions about individuals or in sensitive applications without appropriate safeguards and human oversight.\n\n## Citation\n\nIf you use this model in your research, please cite:\n\n```\n@article{Li2024DataCompLM,\n  title={DataComp-LM: In search of the next generation of training sets for language models},\n  author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and [... full author list]},\n  journal={arXiv preprint arXiv:2406.11794},\n  year={2024}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":6889674752,\"storage_bytes\":27558732152,\"files_count\":14,\"spaces_count\":3,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"OpenLMModel\"],\"model_type\":\"openlm\",\"tokenizer_config\":{\"unk_token\":\"<|endoftext|>\",\"bos_token\":\"<|endoftext|>\",\"eos_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mlfoundations:dclm\",\"source_url\":\"https://github.com/mlfoundations/dclm\"},{\"type\":\"has_code\",\"target_id\":\"github:mlfoundations:open_lm.git\",\"source_url\":\"https://github.com/mlfoundations/open_lm.git\"},{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:llm-foundry\",\"source_url\":\"https://github.com/mosaicml/llm-foundry\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2406.11794\",\"source_url\":\"https://arxiv.org/abs/2406.11794\"}]",
    "canonical_id": null,
    "license_spdx": "apple-ascl",
    "compliance_status": "approved",
    "quality_score": 64.2,
    "content_hash": "b0372a7069c23680c9392b851bce5ef7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/apple/DCLM-7B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-r1-distill-llama-8b",
    "name": "DeepSeek-R1-Distill-Llama-8B",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/d...",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 831,
    "downloads": 1236038,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/resolve/main/figures/benchmark.jpg",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>ğŸ‘ï¸</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":8030261248,\"storage_bytes\":16060556354,\"files_count\":11,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ï½œAssistantï½œ><ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>' + message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ï½œAssistantï½œ>' + content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ï½œAssistantï½œ><think>\\\\n'}}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.12948\",\"source_url\":\"https://arxiv.org/abs/2501.12948\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 99.2,
    "content_hash": "22075d47f48572747c3607c4506bcf1d",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/resolve/main/figures/benchmark.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-3n-e4b-it",
    "name": "gemma-3n-E4B-it",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gemma3n",
      "any-to-any",
      "automatic-speech-recognition",
      "automatic-speech-translation",
      "audio-text-to-text",
      "video-text-to-text",
      "image-text-to-text",
      "conversational",
      "arxiv:1905.07830",
      "arxiv:1905.10044",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1705.03551",
      "arxiv:1911.01547",
      "arxiv:1907.10641",
      "arxiv:1903.00161",
      "arxiv:2210.03057",
      "arxiv:2502.12404",
      "arxiv:2411.19799",
      "arxiv:2009.03300",
      "arxiv:2502.21228",
      "arxiv:2311.12022",
      "arxiv:2403.07974",
      "arxiv:2108.07732",
      "arxiv:2107.03374",
      "base_model:google/gemma-3n-e4b",
      "base_model:finetune:google/gemma-3n-e4b",
      "license:gemma",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 828,
    "downloads": 67289,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-3n-E4B-it",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7849978192,\"storage_bytes\":50810768611,\"files_count\":17,\"spaces_count\":77,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Gemma3nForConditionalGeneration\"],\"model_type\":\"gemma3n\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false},\"chat_template_jinja\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'audio' -%}\\n                {{ '<audio_soft_token>' }}\\n            {%- elif item['type'] == 'image' -%}\\n                {{ '<image_soft_token>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1903.00161\",\"source_url\":\"https://arxiv.org/abs/1903.00161\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.03057\",\"source_url\":\"https://arxiv.org/abs/2210.03057\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.12404\",\"source_url\":\"https://arxiv.org/abs/2502.12404\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2411.19799\",\"source_url\":\"https://arxiv.org/abs/2411.19799\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.21228\",\"source_url\":\"https://arxiv.org/abs/2502.21228\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.12022\",\"source_url\":\"https://arxiv.org/abs/2311.12022\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.07974\",\"source_url\":\"https://arxiv.org/abs/2403.07974\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 39.2,
    "content_hash": "2b1720a0f060a5d9feb57c2d2230dccc",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-3n-E4B-it\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:hunyuanvideo-1.5",
    "name": "HunyuanVideo-1.5",
    "author": "tencent",
    "description": "--- library_name: HunyuanVideo-1.5 license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE language: - en - zh tags: - text-to-video - image-to-video pipeline_tag: text-to-video extra_gated_eu_disallowed: true --- ä¸­æ–‡æ–‡æ¡£ <div align=\"center\"> <img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\"> </div> <div align=\"center\"> <!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"8...",
    "tags": [
      "hunyuanvideo-1.5",
      "diffusers",
      "safetensors",
      "text-to-video",
      "image-to-video",
      "en",
      "zh",
      "arxiv:2511.18870",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-video",
    "likes": 828,
    "downloads": 4735,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/HunyuanVideo-1.5",
    "image_url": "https://huggingface.co/tencent/HunyuanVideo-1.5/resolve/main/assets/I2V_GSB.png",
    "type": "model",
    "body_content": "---\nlibrary_name: HunyuanVideo-1.5\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - text-to-video\n  - image-to-video\npipeline_tag: text-to-video\nextra_gated_eu_disallowed: true\n---\n\n\n[ä¸­æ–‡æ–‡æ¡£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\">\n\n# ğŸ¬ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align=\"center\">\n<!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"800\"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\" target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=\"https://arxiv.org/pdf/2511.18870\" target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href=\"./ComfyUI/README.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href=\"https://github.com/ModelTC/LightX2V\" target=\"_blank\"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href=\"https://tusi.cn/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/åå¸-purple.svg?logo=book height=22px></a>\n  <a href=\"https://tensor.art/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align=\"center\">\n    ğŸ‘ Join our <a href=\"./assets/wechat.png\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nğŸ’» <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## ğŸ”¥ğŸ”¥ğŸ”¥ News\n* ğŸš€ Dec 09, 2025: LoRA tuning script is released, enjoy it! ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸš€ Dec 05, 2025: **New Release**: We now release the [480p I2V step-distilled model](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled), which generates videos in 8 or 12 steps (recommended)! On RTX 4090, end-to-end generation time is reduced by 75%, and a single RTX 4090 can generate videos within **75 seconds**. The step-distilled model maintains comparable quality to the original model while achieving significant speedup. See [Step Distillation Comparison](./assets/step_distillation_comparison.md) for detailed quality comparisons. For even faster generation, you can also try 4 steps (faster speed with slightly reduced quality). **To enable the step-distilled model, run `generate.py` with the `--enable_step_distill` parameter.** See [Usage](#-usage) for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ“š Dec 05, 2025: **Training Code Released**: We now open-source the training code for HunyuanVideo-1.5! The training script (`train.py`) provides a full training pipeline with support for distributed training, FSDP, context parallel, gradient checkpointing, and more. HunyuanVideo-1.5 is trained using the Muon optimizer, which we have open-sourced in the [Training](#-training) section. **If you would like to continue training our model or fine-tune it with LoRA, please use the Muon optimizer.** See [Training](#-training) section for detailed usage instructions. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸ‰ **Diffusers Support**: HunyuanVideo-1.5 is now available on Hugging Face Diffusers! Check out [Diffusers collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15) for easy integration. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†•\n* ğŸš€ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. ğŸ”¥ğŸ”¥ğŸ”¥ğŸ†• \n* ğŸš€ Nov 24, 2025: We now support deepcache inference.\n* ğŸ‘‹ Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## ğŸ¥ Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d\" width=\"60%\"> </video>\n</div>\n\n## ğŸ§© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **Diffusers** - [HunyuanVideo-1.5 Diffusers](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15): Official Hugging Face Diffusers integration for HunyuanVideo-1.5. Easily use HunyuanVideo-1.5 with the Diffusers library for seamless integration into your projects. See [Usage with Diffusers](#usage-with-diffusers) section for details.\n\n- **ComfyUI** - [ComfyUI](https://github.com/comfyanonymous/ComfyUI): A powerful and modular diffusion model GUI with a graph/nodes interface. ComfyUI supports HunyuanVideo-1.5 with various engineering optimizations for fast inference. We provide a [ComfyUI Usage Guide](./ComfyUI/README.md) for HunyuanVideo-1.5.\n\n- **Community-implemented ComfyUI Plugin** - [comfyui_hunyuanvideo_1.5_plugin](https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin): A community-implemented ComfyUI plugin for HunyuanVideo-1.5, offering both simplified and complete node sets for quick usage or deep workflow customization, with built-in automatic model download support.\n\n- **LightX2V** - [LightX2V](https://github.com/ModelTC/LightX2V): A lightweight and efficient video generation framework that integrates HunyuanVideo-1.5, supporting multiple engineering acceleration techniques for fast inference.\n\n- **Wan2GP v9.62** - [Wan2GP](https://github.com/deepbeepmeep/Wan2GP): WanGP is a very low VRAM app (as low 6 GB of VRAM for Hunyuan Video 1.5) supports Lora Accelerator for a 8 steps generation and offers tools to facilitate Video Generation.\n\n- **ComfyUI-MagCache** - [ComfyUI-MagCache](https://github.com/Zehong-Ma/ComfyUI-MagCache): MagCache is a training-free caching approach that accelerates video generation by estimating fluctuating differences among model outputs across timesteps. It achieves 1.7x speedup for HunyuanVideo-1.5 with 20 inference steps.\n\n\n## ğŸ“‘ Open-source Plan\n- HunyuanVideo-1.5 (T2V/I2V)\n  - [x] Inference Code and checkpoints\n  - [x] ComfyUI Support\n  - [x] LightX2V Support\n  - [x] Diffusers Support\n  - [ ] Release all model weights (Sparse attention, distill model, and SR models)\n\n## ğŸ“‹ Table of Contents\n- [ğŸ”¥ğŸ”¥ğŸ”¥ News](#-news)\n- [ğŸ¥ Demo](#-demo)\n- [ğŸ§© Community Contributions](#-community-contributions)\n- [ğŸ“‘ Open-source Plan](#-open-source-plan)\n- [ğŸ“– Introduction](#-introduction)\n- [âœ¨ Key Features](#-key-features)\n- [ğŸ“œ System Requirements](#-system-requirements)\n- [ğŸ› ï¸ Dependencies and Installation](#ï¸-dependencies-and-installation)\n- [ğŸ§± Download Pretrained Models](#-download-pretrained-models)\n- [ğŸ“ Prompt Guide](#-prompt-guide)\n- [ğŸ”‘ Inference](#-inference)\n  - [Inference with Source Code](#inference-with-source-code)\n  - [Usage with Diffusers](#usage-with-diffusers)\n  - [Prompt Enhancement](#prompt-enhancement)\n  - [Text to Video](#text-to-video)\n  - [Image to Video](#image-to-video)\n  - [Command Line Arguments](#command-line-arguments)\n  - [Optimal Inference Configurations](#optimal-inference-configurations)\n- [ğŸ“ Training](#-training)\n- [ğŸ¬ More Examples](#-more-examples)\n- [ğŸ“Š Evaluation](#-evaluation)\n- [ğŸ“š Citation](#-citation)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [ğŸŒŸ Github Star History](#-github-star-history)\n\n\n## ğŸ“– Introduction\nWe present HunyuanVideo-1.5, a lightweight yet powerful video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture with selective and sliding tile attention(SSTA), enhanced bilingual understanding through glyph-aware text encoding , progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions. Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source models. By releasing the code and weights of HunyuanVideo-1.5, we provide the community with a high-performance foundation that significantly lowers the cost of video creation and research, making advanced video generation more accessible to all.\n\n\n## âœ¨ Key Features\n- **Lightweight High-Performance Architecture**: We propose an efficient architecture that integrates an 8.3B-parameter Diffusion Transformer (DiT) with a 3D causal VAE, achieving compression ratios of 16Ã— in spatial dimensions and 4Ã— along the temporal axis. Additionally, the innovative SSTA (Selective and Sliding Tile Attention) mechanism prunes redundant spatiotemporal kv blocks, significantly reduces computational overhead for long video sequences and accelerates inference, achieving an end-to-end speedup of $1.87 \\times$ in 10-second 720p video synthesis compared to FlashAttention-3.\n\n<div align=\"center\">\n<img src=\"./assets/hy_video_1_5_dit.png\" alt=\"HunyuanVideo-1.5 DiT\" width=\"600\">\n</div> \n\n\n- **Video Super-Resolution Enhancement**: We develop an efficient few-step super-resolution network that upscales outputs to 1080p. It enhances sharpness while correcting distortions, thereby refining details and overall visual texture.\n\n<div align=\"center\">\n<img src=\"./assets/hy_video_1_5_vsr.png\" alt=\"HunyuanVideo-1.5 VSR\" width=\"600\">\n</div> \n\n- **End-to-End Training Optimization**: This work employs a multi-stage, progressive training strategy covering the entire pipeline from pre-training to post-training. Combined with the Muon optimizer to accelerate convergence, this approach holistically refines motion coherence, aesthetic quality, and human preference alignment, achieving professional-grade content generation.\n\n## ğŸ“œ System Requirements\n\n### Hardware Requirements\n\n- **GPU**: NVIDIA GPU with CUDA support\n- **Minimum GPU Memory**: 14 GB (with model offloading enabled)\n  \n  > **Note:** The memory requirements above are measured with model offloading enabled. If your GPU has sufficient memory, you may disable offloading for improved inference speed.\n\n### Software Requirements\n\n- **Operating System**: Linux\n- **Python**: Python 3.10 or higher\n- **CUDA**: Compatible CUDA version for your PyTorch installation\n\n## ğŸ› ï¸ Dependencies and Installation\n\n### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git\ncd HunyuanVideo-1.5\n```\n\n### Step 2: Install Basic Dependencies\n\n```bash\npip install -r requirements.txt\npip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python\n```\n\n### Step 3: Install Attention Libraries\n\n* Flash Attention: \n  Install Flash Attention for faster inference and reduced GPU memory consumption.\n  Detailed installation instructions are available at [Flash Attention](https://github.com/Dao-AILab/flash-attention).\n\n* Flex-Block-Attention: \n  flex-block-attn is only required for sparse attention to achieve faster inference and can be installed by the following command:\n  ```bash\n  git clone https://github.com/Tencent-Hunyuan/flex-block-attn.git\n  cd flex-block-attn\n  git submodule update --init --recursive\n  python3 setup.py install\n  ```\n\n* SageAttention: \n  To enable SageAttention for faster inference, you need to install it by the following command:\n  > **Note**: Enabling SageAttention will automatically disable Flex-Block-Attention.\n  ```bash\n  git clone https://github.com/cooper1637/SageAttention.git\n  cd SageAttention \n  export EXT_PARALLEL=4 NVCC_APPEND_FLAGS=\"--threads 8\" MAX_JOBS=32 # Optional\n  python3 setup.py install\n  ```\n\n## ğŸ§± Download Pretrained Models\n\n> ğŸ’¡ Distillation models and sparse attention models are still coming soon. Please stay tuned for the latest updates on the Hugging Face Model Card.\n\nDownload the pretrained models before generating videos. Detailed instructions are available at [checkpoints-download.md](checkpoints-download.md).\n\n### Model Cards\n|ModelName| Download                     |\n|-|---------------------------| \n|HunyuanVideo-1.5-480P-T2V|[480P-T2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_t2v) |\n|HunyuanVideo-1.5-480P-I2V |[480P-I2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v) |\n|HunyuanVideo-1.5-480P-T2V-cfg-distill | [480P-T2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_t2v_distilled) |\n|HunyuanVideo-1.5-480P-I2V-cfg-distill |[480P-I2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_distilled) |\n|HunyuanVideo-1.5-480P-I2V-step-distill |[480P-I2V-step-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled) |\n|HunyuanVideo-1.5-720P-T2V|[720P-T2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_t2v) |\n|HunyuanVideo-1.5-720P-I2V |[720P-I2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v) |\n|HunyuanVideo-1.5-720P-T2V-cfg-distill| Comming soon |\n|HunyuanVideo-1.5-720P-I2V-cfg-distill |[720P-I2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v_distilled) |\n|HunyuanVideo-1.5-720P-T2V-sparse-cfg-distill| Comming soon |\n|HunyuanVideo-1.5-720P-I2V-sparse-cfg-distill |[720P-I2V-sparse-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v_distilled_sparse) |\n|HunyuanVideo-1.5-720P-sr-step-distill |[720P-sr](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_sr_distilled) |\n|HunyuanVideo-1.5-1080P-sr-step-distill |[1080P-sr](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/1080p_sr_distilled) |\n\n## ğŸ“ Prompt Guide\n### Prompt Writing Handbook\nPrompt enhancement plays a crucial role in enabling our model to generate high-quality videos. By writing longer and more detailed prompts, the generated video will be significantly improved. We encourage you to craft comprehensive and descriptive prompts to achieve the best possible video quality. we recommend community partners consulting our official guide on how to write effective prompts. \n\n**Reference:** **[HunyuanVideo-1.5 Prompt Handbook](https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md)**\n\n### System Prompts for Automatic Prompt Enhancement\nFor users seeking to optimize prompts for other large models, it is recommended to consult the definition of `t2v_rewrite_system_prompt` in the file `hyvideo/utils/rewrite/t2v_prompt.py` to guide text-to-video rewriting. Similarly, for image-to-video rewriting, refer to the definition of `i2v_rewrite_system_prompt` in `hyvideo/utils/rewrite/i2v_prompt.py`.\n\n## ğŸ”‘ Inference\n\n### Inference with Source Code\n\n\nFor prompt rewriting, we recommend using Gemini or models deployed via vLLM. This codebase currently only supports models compatible with the vLLM API. If you wish to use Gemini, you will need to implement your own interface calls.\n\nFor models with a vLLM API, note that T2V (text-to-video) and I2V (image-to-video) have different recommended models and environment variables:\n\n- T2V: use [Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507), configure `T2V_REWRITE_BASE_URL` and `T2V_REWRITE_MODEL_NAME`\n- I2V: use [Qwen3-VL-235B-A22B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct), configure `I2V_REWRITE_BASE_URL` and `I2V_REWRITE_MODEL_NAME`\n\n> You may set the above model names to any other vLLM-compatible models you have deployed (including HuggingFace models).  \n> Rewriting is enabled by default (`--rewrite` defaults to `true`); to disable it explicitly, use `--rewrite false` or `--rewrite 0`. If no vLLM endpoint is configured, the pipeline runs without remote rewriting.\n\nExample: Generate a video (works for both T2V and I2V; set `IMAGE_PATH=none` for T2V or provide an image path for I2V)\n\n> ğŸ’¡ **Tip**: For faster inference speed, you can enable the step-distilled model using the `--enable_step_distill` parameter. The step-distilled model (480p I2V) can generate videos in 8 or 12 steps (recommended), achieving up to 75% speedup on RTX 4090 while maintaining comparable quality.\n>\n> **Tips:** If your GPU memory is > 14GB but you encounter OOM (Out of Memory) errors during generation, you can try setting the following environment variable before running:\n> ```bash\n> export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128\n> ```\n> \n> **Tips:** If you have limited CPU memory and encounter OOM during inference, you can try disable overlapped group offloading by adding the following argument:\n> ```bash\n> --overlap_group_offloading false\n> ```\n\n```bash\nexport T2V_REWRITE_BASE_URL=\"<your_vllm_server_base_url>\"\nexport T2V_REWRITE_MODEL_NAME=\"<your_model_name>\"\nexport I2V_REWRITE_BASE_URL=\"<your_vllm_server_base_url>\"\nexport I2V_REWRITE_MODEL_NAME=\"<your_model_name>\"\n\nPROMPT='A girl holding a paper with words \"Hello, world!\"'\n\nIMAGE_PATH=/path/to/image.png # Optional, none or <image path> to enable i2v mode\nSEED=1\nASPECT_RATIO=16:9\nRESOLUTION=480p\nOUTPUT_PATH=./outputs/output.mp4\nMODEL_PATH=./ckpts # Path to pretrained model\n\n# Configuration for faster inference\nN_INFERENCE_GPU=8 # Parallel inference GPU count\nCFG_DISTILLED=true # Inference with CFG distilled model, 2x speedup\nSAGE_ATTN=true # Inference with SageAttention\nSPARSE_ATTN=false # Inference with sparse attention (only 720p models are equipped with sparse attention). Please ensure flex-block-attn is installed\nOVERLAP_GROUP_OFFLOADING=true # Only valid when group offloading is enabled, significantly increases CPU memory usage but speeds up inference\nENABLE_CACHE=true # Enable feature cache during inference. Significantly speeds up inference.\nCACHE_TYPE=deepcache # Support: deepcache, teacache, taylorcache\nENABLE_STEP_DISTILL=true # Enable step distilled model for 480p I2V, recommended 8 or 12 steps, up to 6x speedup\n\n\n# Configuration for better quality\nREWRITE=true # Enable prompt rewriting. Please ensure rewrite vLLM server is deployed and configured.\nENABLE_SR=true # Enable super resolution\n\n\ntorchrun --nproc_per_node=$N_INFERENCE_GPU generate.py \\\n  --prompt \"$PROMPT\" \\\n  --image_path $IMAGE_PATH \\\n  --resolution $RESOLUTION \\\n  --aspect_ratio $ASPECT_RATIO \\\n  --seed $SEED \\\n  --rewrite $REWRITE \\\n  --cfg_distilled $CFG_DISTILLED \\\n  --enable_step_distill $ENABLE_STEP_DISTILL \\\n  --sparse_attn $SPARSE_ATTN --use_sageattn $SAGE_ATTN \\\n  --enable_cache $ENABLE_CACHE --cache_type $CACHE_TYPE \\\n  --overlap_group_offloading $OVERLAP_GROUP_OFFLOADING \\\n  --sr $ENABLE_SR --save_pre_sr_video \\\n  --output_path $OUTPUT_PATH \\\n  --model_path $MODEL_PATH\n```\n\n\n\n### Command Line Arguments\n\n| Argument | Type | Required | Default | Description |\n|----------|------|----------|---------|-------------|\n| `--prompt` | str | Yes | - | Text prompt for video generation |\n| `--negative_prompt` | str | No | `''` | Negative prompt for video generation |\n| `--resolution` | str | Yes | - | Video resolution: `480p` or `720p` |\n| `--model_path` | str | Yes | - | Path to pretrained model directory |\n| `--aspect_ratio` | str | No | `16:9` | Aspect ratio of the output video |\n| `--num_inference_steps` | int | No | `50` | Number of inference steps |\n| `--video_length` | int | No | `121` | Number of frames to generate |\n| `--seed` | int | No | `123` | Random seed for reproducibility |\n| `--image_path` | str | No | `None` | Path to reference image (enables i2v mode). Use `none` or `None` to explicitly use text-to-video mode |\n| `--output_path` | str | No | `None` | Output file path (if not provided, saves to `./outputs/output_{transformer_version}_{timestamp}.mp4`) |\n| `--sr` | bool | No | `true` | Enable super resolution (use `--sr false` or `--sr 0` to disable) |\n| `--save_pre_sr_video` | bool | No | `false` | Save original video before super resolution (use `--save_pre_sr_video` or `--save_pre_sr_video true` to enable, only effective when super resolution is enabled) |\n| `--rewrite` | bool | No | `true` | Enable prompt rewriting (use `--rewrite false` or `--rewrite 0` to disable, may result in lower quality video generation) |\n| `--cfg_distilled` | bool | No | `false` | Enable CFG distilled model for faster inference (~2x speedup, use `--cfg_distilled` or `--cfg_distilled true` to enable) |\n| `--enable_step_distill` | bool | No | `false` | Enable step distilled model for 480p I2V (recommended 8 or 12 steps, ~75% speedup on RTX 4090, use `--enable_step_distill` or `--enable_step_distill true` to enable) |\n| `--sparse_attn` | bool | No | `false` | Enable sparse attention for faster inference (~1.5-2x speedup, requires H-series GPUs, auto-enables CFG distilled, use `--sparse_attn` or `--sparse_attn true` to enable) |\n| `--offloading` | bool | No | `true` | Enable CPU offloading (use `--offloading false` or `--offloading 0` to disable for faster inference if GPU memory allows) |\n| `--group_offloading` | bool | No | `None` | Enable group offloading (default: None, automatically enabled if offloading is enabled. Use `--group_offloading` or `--group_offloading true/1` to enable, `--group_offloading false/0` to disable) |\n| `--overlap_group_offloading` | bool | No | `true` | Enable overlap group offloading (default: true). Significantly increases CPU memory usage but speeds up inference. Use `--overlap_group_offloading` or `--overlap_group_offloading true/1` to enable, `--overlap_group_offloading false/0` to disable |\n| `--dtype` | str | No | `bf16` | Data type for transformer: `bf16` (faster, lower memory) or `fp32` (better quality, slower, higher memory) |\n| `--use_sageattn` | bool | No | `false` | Enable SageAttention (use `--use_sageattn` or `--use_sageattn true/1` to enable, `--use_sageattn false/0` to disable) |\n| `--sage_blocks_range` | str | No | `0-53` | SageAttention blocks range (e.g., `0-5` or `0,1,2,3,4,5`) |\n| `--enable_cache` | bool | No | `false` | Enable cache for transformer (use `--enable_cache` or `--enable_cache true/1` to enable, `--enable_cache false/0` to disable) |\n| `--cache_type` | str | No | `deepcache` | Cache type for transformer (e.g., `deepcache, teacache, taylorcache`) |\n| `--no_cache_block_id` | str | No | `53` | Blocks to exclude from deepcache (e.g., `0-5` or `0,1,2,3,4,5`) |\n| `--cache_start_step` | int | No | `11` | Start step to skip when using cache |\n| `--cache_end_step` | int | No | `45` | End step to skip when using cache |\n| `--total_steps` | int | No | `50` | Total inference steps |\n| `--cache_step_interval` | int | No | `4` | Step interval to skip when using cache |\n\n**Note:** Use `--nproc_per_node` to specify the number of GPUs. For example, `--nproc_per_node=8` uses 8 GPUs.\n\n### Optimal Inference Configurations\n\nThe following table provides the optimal inference configurations (CFG scale, embedded CFG scale, flow shift, and inference steps) for each model to achieve the best generation quality:\n\n| Model | CFG Scale | Embedded CFG Scale | Flow Shift | Inference Steps |\n|-------|-----------|-------------------|------------|-----------------|\n| 480p T2V | 6 | None | 5 | 50 |\n| 480p I2V | 6 | None | 5 | 50 |\n| 720p T2V | 6 | None | 9 | 50 |\n| 720p I2V | 6 | None | 7 | 50 |\n| 480p T2V CFG Distilled | 1 | None | 5 | 50 |\n| 480p I2V CFG Distilled | 1 | None | 5 | 50 |\n| 480p I2V Step Distilled | 1 | None | 7 | 8 or 12 (recommended) |\n| 720p T2V CFG Distilled | 1 | None | 9 | 50 |\n| 720p I2V CFG Distilled | 1 | None | 7 | 50 |\n| 720p T2V CFG Distilled Sparse | 1 | None | 9 | 50 |\n| 720p I2V CFG Distilled Sparse | 1 | None | 7 | 50 |\n| 480â†’720 SR Step Distilled | 1 | None | 2 | 6 |\n| 720â†’1080 SR Step Distilled | 1 | None | 2 | 8 |\n\n**Please note that the cfg distilled model we provided, must use 50 steps to generate correct results.**\n\n### Usage with Diffusers\n\nHunyuanVideo-1.5 is available on Hugging Face Diffusers! You can easily use it with the Diffusers library:\n\n**Basic Usage:**\n\n```python\nimport torch\n\ndtype = torch.bfloat16\ndevice = \"cuda:0\"\n\nfrom diffusers import HunyuanVideo15Pipeline\nfrom diffusers.utils import export_to_video\n\npipe = HunyuanVideo15Pipeline.from_pretrained(\"hunyuanvideo-community/HunyuanVideo-1.5-Diffusers-720p_t2v\", torch_dtype=dtype)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nvideo = pipe(\n    prompt=prompt,\n    generator=generator,\n    num_frames=121,\n    num_inference_steps=50,\n).frames[0]\n\nexport_to_video(video, \"output.mp4\", fps=24)\n```\n\n**Optimized Usage with Attention Backend:**\n\nHunyuanVideo-1.5 uses attention masks with variable-length sequences. For best performance, we recommend using an attention backend that handles padding efficiently.\n\nWe recommend installing kernels (`pip install kernels`) to access prebuilt attention kernels.\n\n```python\nimport torch\n\ndtype = torch.bfloat16\ndevice = \"cuda:0\"\n\nfrom diffusers import HunyuanVideo15Pipeline, attention_backend\nfrom diffusers.utils import export_to_video\n\npipe = HunyuanVideo15Pipeline.from_pretrained(\"hunyuanvideo-community/HunyuanVideo-1.5-Diffusers-720p_t2v\", torch_dtype=dtype)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nwith attention_backend(\"_flash_3_hub\"): # or `\"flash_hub\"` if you are not on H100/H800\n    video = pipe(\n        prompt=prompt,\n        generator=generator,\n        num_frames=121,\n        num_inference_steps=50,\n    ).frames[0]\n    export_to_video(video, \"output.mp4\", fps=24)\n```\n\nFor more details, please visit [HunyuanVideo-1.5 Diffusers Collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15).\n\n\n## ğŸ“ Training\n\nHunyuanVideo-1.5 is trained using the **Muon optimizer**, which accelerates convergence and improves training stability. The Muon optimizer combines momentum-based updates with Newton-Schulz orthogonalization for efficient optimization of large-scale video generation models.\n\n### Quick Start\n\nThe training script (`train.py`) provides a complete training pipeline for HunyuanVideo-1.5. Here's how to use it:\n\n#### 1. Implement Your DataLoader\n\nReplace the `create_dummy_dataloader()` function in `train.py` with your own implementation. Your dataloader should return batches with the following format:\n\n- **Required fields:**\n  - `\"pixel_values\"`: `torch.Tensor` - Video: `[B, C, F, H, W]` or Image: `[B, C, H, W]`\n    - Note: For video data, temporal dimension F must be `4n+1` (e.g., 1, 5, 9, 13, 17, ...)\n  - `\"text\"`: `List[str]` - Text prompts for each sample\n  - `\"data_type\"`: `str` - `\"video\"` or `\"image\"`\n\n- **Optional fields (for performance optimization):**\n  - `\"latents\"`: Pre-encoded VAE latents (skips VAE encoding for faster training)\n  - `\"byt5_text_ids\"` and `\"byt5_text_mask\"`: Pre-tokenized byT5 inputs\n\nSee the `create_dummy_dataloader()` function in `train.py` for detailed batch format documentation.\n\n#### 2. Run Training\n\n**Single GPU:**\n```bash\npython train.py --pretrained_model_root <path_to_pretrained_model> [other args]\n```\n\n**Multi-GPU:**\n```bash\nN=8\ntorchrun --nproc_per_node=$N train.py --pretrained_model_root <path_to_pretrained_model> [other args]\n```\n\n**Example:**\n```bash\ntorchrun --nproc_per_node=8 train.py \\\n  --pretrained_model_root ./ckpts \\\n  --learning_rate 1e-5 \\\n  --batch_size 1 \\\n  --max_steps 10000 \\\n  --output_dir ./outputs \\\n  --enable_fsdp \\\n  --enable_gradient_checkpointing \\\n  --sp_size 8\n```\n\n#### 3. Key Training Parameters\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `--pretrained_model_root` | Path to pretrained model (required) | - |\n| `--learning_rate` | Learning rate | 1e-5 |\n| `--batch_size` | Batch size | 1 |\n| `--max_steps` | Maximum training steps | 10000 |\n| `--warmup_steps` | Warmup steps | 500 |\n| `--gradient_accumulation_steps` | Gradient accumulation steps | 1 |\n| `--enable_fsdp` | Enable FSDP for distributed training | true |\n| `--enable_gradient_checkpointing` | Enable gradient checkpointing | true |\n| `--sp_size` | Sequence parallelism size (must divide world_size) | 8 |\n| `--i2v_prob` | Probability of i2v task for video data | 0.3 |\n| `--use_muon` | Use Muon optimizer | true |\n| `--resume_from_checkpoint` | Resume from checkpoint directory | None |\n| `--use_lora` | Enable LoRA fine-tuning | false |\n| `--lora_r` | LoRA rank | 8 |\n| `--lora_alpha` | LoRA alpha scaling parameter | 16 |\n| `--lora_dropout` | LoRA dropout rate | 0.0 |\n| `--pretrained_lora_path` | Path to pretrained LoRA adapter | None |\n\n#### 4. Monitor Training\n\n- Checkpoints are saved to `output_dir` at intervals specified by `--save_interval`\n- Validation videos are generated at intervals specified by `--validation_interval`\n- Training logs are printed to console at intervals specified by `--log_interval`\n\n#### 5. Resume Training\n\nUse `--resume_from_checkpoint <checkpoint_dir>` to resume from a saved checkpoint:\n```bash\npython train.py \\\n  --pretrained_model_root <path> \\\n  --resume_from_checkpoint ./outputs/checkpoint-1000\n```\n\n#### 6. LoRA Fine-tuning\n\nTo enable LoRA fine-tuning, add `--use_lora` to your training command. LoRA adapters will be saved in the checkpoint directory under `lora/`:\n\n```bash\ntorchrun --nproc_per_node=8 train.py \\\n  --pretrained_model_root ./ckpts \\\n  --use_lora \\\n  --lora_r 8 \\\n  --lora_alpha 16 \\\n  --learning_rate 1e-4 \\\n  --output_dir ./outputs\n```\n\nTo load a pretrained LoRA adapter, use `--pretrained_lora_path`:\n```bash\ntorchrun --nproc_per_node=8 train.py \\\n  --pretrained_model_root ./ckpts \\\n  --use_lora \\\n  --pretrained_lora_path ./outputs/checkpoint-1000/lora/default\n```\n\n\n## ğŸ“Š Evaluation\n\n### Rating\nWe assess text-to-video generation using a comprehensive rating methodology that considers five key dimensions: text-video consistency, visual quality, structural stability, motion effects, and the aesthetic quality of individual frames. For image-to-video generation, the evaluation encompasses image-video consistency, instruction responsiveness, visual quality, structural stability, and motion effects.\n\n<div align=\"center\">\n<img src=\"./assets/T2V_Rating.png\" alt=\"rating result of t2v\" width=\"800\">\n</div> \n\n---\n\n<div align=\"center\">\n<img src=\"./assets/I2V_Rating.png\" alt=\"rating result of i2v\" width=\"800\">\n</div> \n\n\n### GSB\nThe GSB(Good/Same/Bad) approach is widely used to evaluate the relative performance of two models based on overall video perception quality.We carefully construct 300 diverse text prompts and 300 image samples to cover balanced application scenarios for both text-to-video and image-to-video tasks. For each prompt or image input, an equal number of video samples are generated by each model in a single run to ensure comparability. To maintain fairness, inference is performed only once per input without any cherry-picking of results. All competing models are evaluated using their default configurations. The evaluation is conducted by over 100 professional assessors\n\n<div align=\"center\">\n<img src=\"./assets/T2V_GSB.png\" alt=\"gsb result of t2v\" width=\"800\">\n</div>\n\n---\n\n<div align=\"center\">\n<img src=\"./assets/I2V_GSB.png\" alt=\"gsb result of i2v\" width=\"800\">\n</div> \n\n\n### Inference speed\nWe report inference speed with basic engineering-level acceleration techniques enabled on 8 H800 GPUs to demonstrate practical performance achievable in real-world deployment scenarios.\nPlease note that in this experiment, we do not pursue the most extreme acceleration at the cost of generation quality, but rather to achieve notable speed improvements while maintaining nearly identical output quality.\n\nWe report the total inference time for 50 diffusion steps for HunyuanVideo 1.5 below:\n\n<div align=\"center\">\n<img src=\"./assets/speed.png\" alt=\"\" width=\"100%\">\n</div> \n\n## ğŸ¬ More Examples\n|Features|Demo1|Demo2|\n|------|------|------|\n|Strong Instruction Following|<video src=\"https://github.com/user-attachments/assets/fdc3c27b-69f5-46a1-b707-0b57510fa32f\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```ä¸€åå“€ä¼¤çš„é»‘å‘ä¸­å›½å¥³å­å‡æœ›å¤©ç©ºï¼Œå¤å¤èƒ¶ç‰‡é£æ ¼çƒ˜æ‰˜å‡ºæ€€æ—§æˆå‰§æ°›å›´``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```ä¿¯è§†è§’åº¦ï¼Œä¸€ä½æœ‰ç€æ·±è‰²ï¼Œç•¥å¸¦å‡Œä¹±çš„é•¿å·å‘çš„å¹´è½»ä¸­å›½å¥³æ€§ï¼Œä½©æˆ´ç€é—ªè€€çš„çç é¡¹é“¾å’Œåœ†å½¢é‡‘è‰²è€³ç¯ï¼Œå¥¹å‡Œä¹±çš„å¤´å‘è¢«é£å¹æ•£ï¼Œå¥¹å¾®å¾®æŠ¬å¤´ï¼Œæœ›å‘å¤©ç©ºï¼Œç¥æƒ…ååˆ†å“€ä¼¤ï¼Œçœ¼ä¸­å«ç€æ³ªæ°´ã€‚å˜´å”‡æ¶‚ç€çº¢è‰²å£çº¢ã€‚èƒŒæ™¯æ˜¯å¸¦æœ‰åä¸½çº¢è‰²èŠ±çº¹çš„å›¾æ¡ˆã€‚ç”»é¢å‘ˆç°å¤å¤ç”µå½±é£æ ¼ï¼Œè‰²è°ƒä½é¥±å’Œï¼Œå¸¦ç€è½»å¾®æŸ”ç„¦ï¼Œçƒ˜æ‰˜æƒ…ç»ªæ°›å›´ï¼Œè´¨æ„Ÿä»¿ä½›20ä¸–çºª90å¹´ä»£çš„ç»å…¸èƒ¶ç‰‡é£æ ¼ï¼Œè¥é€ å‡ºæ€€æ—§ä¸”å¯Œæœ‰æˆå‰§æ€§çš„æ„Ÿè§‰ã€‚``` </details>|<video src=\"https://github.com/user-attachments/assets/3fcb42cc-cdd3-4651-86a6-645a858561c4\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```å»ºç­‘è“å›¾ä¸Šçš„çº¿æ¡åŒ–ä¸ºå®ä½“ï¼Œç¬é—´ç”Ÿé•¿å‡ºä¸€ä¸ªå®Œæ•´çš„å¤å¤å·¥ä¸šé£åŠå…¬ç©ºé—´ã€‚``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```ä¸€åº§ç©ºæ—·çš„ç°ä»£é˜æ¥¼é‡Œï¼Œæœ‰ä¸€å¼ é“ºå±•åœ¨åœ°æ¿ä¸­å¤®çš„å»ºç­‘è“å›¾ã€‚å¿½ç„¶é—´ï¼Œå›¾çº¸ä¸Šçš„çº¿æ¡æ³›èµ·å¾®å…‰ï¼Œä»¿ä½›è¢«æŸç§æ— å½¢çš„åŠ›é‡å”¤é†’ã€‚ç´§æ¥ç€ï¼Œé‚£äº›å‘å…‰çš„çº¿æ¡å¼€å§‹å‘ä¸Šå»¶ä¼¸ï¼Œä»å¹³é¢ä¸­æŒ£è„±ï¼Œå‹¾å‹’å‡ºç«‹ä½“çš„è½®å»“â€”â€”å°±åƒåœ¨ç©ºä¸­è¿›è¡Œä¸€åœºæ— å£°çš„3Dæ‰“å°ã€‚éšåï¼Œå¥‡è¿¹åœ¨åŠ é€Ÿå‘ç”Ÿï¼šæç®€çš„æ©¡æœ¨åŠå…¬æ¡Œã€ä¼˜é›…çš„ä¼Šå§†æ–¯é£æ ¼çš®è´¨æ¤…ã€é«˜æŒ‘çš„å·¥ä¸šé£é‡‘å±ä¹¦æ¶ï¼Œè¿˜æœ‰å‡ ç›çˆ±è¿ªç”Ÿç¯æ³¡ï¼Œä»¥å…‰çº¹ä¸ºéª¨æ¶è¿…é€Ÿâ€œç”Ÿé•¿â€å‡ºæ¥ã€‚è½¬ç¬é—´ï¼Œçº¿æ¡è¢«çœŸå®çš„æè´¨å¡«å……â€”â€”æœ¨æçš„æ¸©æ¶¦ã€çš®é©çš„è´¨æ„Ÿã€é‡‘å±çš„å†·é™ï¼Œéƒ½åœ¨çœ¨çœ¼é—´å®Œæ•´å‘ˆç°ã€‚æœ€ç»ˆï¼Œæ‰€æœ‰å®¶å…·ç¨³å›ºè½åœ°ï¼Œè“å›¾çš„å…‰èŠ’æ‚„ç„¶è¤ªå»ã€‚ä¸€ä¸ªå®Œæ•´çš„åŠå…¬ç©ºé—´ï¼Œå°±è¿™æ ·ä»äºŒç»´çš„å›¾çº¸ä¸­è¯ç”Ÿã€‚``` </details>|\n|Smooth Motion Generation|<video src=\"https://github.com/user-attachments/assets/447847f0-490a-45f9-a86d-a67ab1ff4231\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```A DJ is immersed in his musical world. He wears a pair of professional, matte-black headphones, revealing a focused expression. He wears a black bomber jacket, zipped open to reveal a T-shirt underneath. His upper body sways back and forth rhythmically to the throbbing electronic beats, his head moving with precise movement. The mixing console in front of him serves as the primary source of light. In the distance, the cool white glow of several stadium floodlights casts a deep, dark haze across the vast field, casting long shadows across the emerald green grass, creating a stark contrast to the brightly lit area surrounding the DJ booth. His hands danced swiftly and precisely across the equipment. The entire scene was filled with high-tech dynamics and the solitary creative passion. Against the backdrop of the vast and silent night stadium, it created an atmosphere of high focus, energy, and a slightly surreal feeling.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```slowly advancing medium shot, shot from a level angle, focuses on the center of an empty football field, where a DJ is immersed in his musical world. He wears a pair of professional, matte-black headphones, one earcup slightly removed, revealing a focused expression and a brow beaded with sweat from his intense concentration. He wears a black bomber jacket, zipped open to reveal a T-shirt underneath. His upper body sways back and forth rhythmically to the throbbing electronic beats, his head moving with precise movement. The mixing console in front of him serves as the primary source of light. In the distance, the cool white glow of several stadium floodlights casts a deep, dark haze across the vast field, casting long shadows across the emerald green grass, creating a stark contrast to the brightly lit area surrounding the DJ booth. His hands danced swiftly and precisely across the equipment, one hand steadily pushing and pulling a long volume fader, while the fingers of the other nimbly jumped between the illuminated knobs and pads, sometimes decisively cutting a bass line, sometimes triggering an echo effect. The entire scene was filled with high-tech dynamics and the solitary creative passion. Against the backdrop of the vast and silent night stadium, it created an atmosphere of high focus, energy, and a slightly surreal feeling.``` </details>|<video src=\"https://github.com/user-attachments/assets/49057fe8-a102-4fd7-bd92-e9561abb9f45\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```A figure skater performs a rapid, graceful Biellmann spin, captured from all angles.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```The video captures a figure skater performing a Biellmann spin on ice. The subject is a female skater in a glittering costume. Initially, she spins on one leg. Then, she reaches back and pulls her free leg up. Next, she spins rapidly, becoming a blur of motion, with ice shavings spraying from her skate blade. The background is an ice rink with blurred advertising boards. The camera circles around the subject to capture the spin from all angles. The lighting is spotlit, creating lens flares and sparkles on her costume. The overall video presents a graceful artistic sports style.``` </details>|\n|Cinematic Aesthetics|<video src=\"https://github.com/user-attachments/assets/4098cf72-357d-4b81-97df-6752064ce0c3\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```å›ºå®šé•œå¤´,ç„¦ç‚¹åœ¨å›¾ç‰‡é‡Œçš„æŒ‚é’Ÿä¸Šï¼Œé•œå¤´è½»å¾®æ‘‡æ™ƒè¥é€ æ‰‹æŒæ‘„å½±æ„Ÿï¼Œâ€‹wjw,filmphotos,Film Grain,Reversal film photographyï¼ŒWong Kar-wai movies,cinematic photography, HK film style,neon lighting, in the style of Wong Kar Wai film``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```Handheld lens shooting, the camera focuses on the wall clock hanging on the green-toned wall, shaking slightly. The second hand sweeps steadily across the clock face, and the shadow of the clock cast on the wall shifts subtly with the movement of the lens.``` </details>|<video src=\"https://github.com/user-attachments/assets/2b4575e5-79f1-4011-bed0-e8380198f7c9\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```The leaves of calamus shine in the sunlight, dotted with dewdrops that trickle down to the ground with the breeze.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```A macro shot focuses on long, slender calamus leaves, rendered in a cinematic photography realistic style. The main leaf, a vibrant, deep green, is positioned diagonally across the frame. Its surface is covered in tiny, glistening spherical dewdrops that catch and refract the bright morning sunlight, creating sparkling highlights. Initially, a larger, perfectly round dewdrop clings to the upper section of the leaf, its surface tension holding it in place. Then, as the leaf sways almost imperceptibly, the dewdrop begins to slowly dislodge. Next, it starts to trickle down the central vein of the leaf, its shape elongating slightly as it moves, leaving a subtle, glistening wet trail in its path. Finally, it reaches the pointed tip of the leaf, hangs for a brief moment, and falls out of the bottom of the frame. In the background, other leaves and blades of grass are softly blurred, creating a beautiful bokeh effect with soft, out-of-focus circles of light. The environment is bathed in the warm, golden glow of early morning sunlight, which streams in from behind the leaves, backlighting them and causing their wet edges to shine brilliantly. The overall impression is one of serene, natural beauty, captured in a highly realistic and detailed manner. This is a macro shot. The camera tilts down very slowly, following the path of the main dewdrop as it travels down the leaf. The lighting is soft and natural, with strong backlighting to create a radiant, glowing effect on the dewdrops and leaf edges, characteristic of professional nature photography. The atmosphere is peaceful and serene. The overall video presents a cinematic photography realistic style.``` </details>|\n|Text Rendering|<video src=\"https://github.com/user-attachments/assets/7c964fc5-c27e-4bd0-bf3f-eb8fca2caef6\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```èµ›åšæœ‹å…‹é£æ ¼çš„å¤œæ™šè¡—è§’ï¼Œä¸€ä¸ªå·¨å¤§çš„æ‹›ç‰Œä¸Šï¼Œ â€œHunyuan Video 1.5â€çš„éœ“è™¹ç¯ç®¡è½®å»“å·²ç»å®‰è£…å¥½ã€‚é•œå¤´æ¨è¿›ï¼Œéœ“è™¹ç¯ä»â€œHâ€å¼€å§‹ï¼Œä¼´éšç€â€˜æ»‹æ»‹â€™çš„ç”µæµå£°ï¼Œæ¯ä¸ªå­—æ¯ä¾æ¬¡äº®èµ·ç²‰ç´«è‰²çš„å…‰èŠ’ï¼Œç›´åˆ°å…¨éƒ¨ç‚¹äº®ï¼Œç…§äº®äº†æ½®æ¹¿çš„è¡—é“ã€‚èµ›åšæœ‹å…‹ï¼ŒåŸå¸‚ç¾å­¦``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```On a wet street corner in a cyberpunk city at night, a large neon sign reading \"Hunyuan Video 1.5\" lights up sequentially, illuminating the dark, rainy environment with a pinkish-purple glow. he scene is a dark, rain-slicked street corner in a futuristic, cinematic cyberpunk city. Mounted on the metallic, weathered facade of a building is a massive, unlit neon sign. The sign's glass tube framework clearly spells out the words \"Hunyuan Video 1.5\". Initially, the street is dimly lit, with ambient light from distant skyscrapers creating shimmering reflections on the wet asphalt below. Then, the camera zooms in slowly toward the sign. As it moves, a low electrical sizzling sound begins. In the background, the dense urban landscape of the cyberpunk metropolis is visible through a light atmospheric haze, with towering structures adorned with their own flickering advertisements. A complex web of cables and pipes crisscrosses between the buildings. The shot is at a low angle, looking up at the sign to emphasize its grand scale. The lighting is high-contrast and dramatic, dominated by the neon glow which creates sharp, specular reflections and deep shadows. The atmosphere is moody and tech-noir. The overall video presents a cinematic photography realistic style.,``` </details>|<video src=\"https://github.com/user-attachments/assets/73e8b741-baec-4a40-9d36-a1435172ab64\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```ä¸€å¼ é“ºå¼€çš„ä¸­å›½å®£çº¸ä¸Šï¼Œæµ“å¢¨æ»´å…¥æ°´ä¸­ï¼Œæ™•æŸ“å‡ºå£®ä¸½çš„å±±æ°´ç”»è½®å»“ã€‚å±±å³°ã€äº‘é›¾ã€å­¤èˆŸåœ¨å¢¨è‰²ä¸­è‡ªç„¶å½¢æˆã€‚éšåï¼Œè¿™äº›æ°´å¢¨å…ƒç´ å·§å¦™åœ°æµåŠ¨ã€é‡ç»„ï¼Œåœ¨ç”»é¢çš„ç•™ç™½å¤„æ±‡èšæˆ\"Hunyuan Video 1.5\"çš„ä¹¦æ³•å­—ä½“ã€‚ä¼˜é›…ï¼Œè¯—æ„ï¼Œæ–‡åŒ–åº•è•´``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```A drop of black ink blooms on wet Chinese Xuan paper, forming a landscape painting before the ink elements fluidly reassemble into the calligraphic text \"Hunyuan Video 1.5\". On a flat, laid-out sheet of off-white Chinese Xuan paper with a subtle, fibrous texture, the scene unfolds. Initially, a single, concentrated drop of deep black ink falls into a clear, wet area at the center of the paper. Then, the ink instantly begins to bloom outwards in intricate, flowing tendrils of varying shades from jet-black to smoky grey. As it spreads, the ink wash naturally and rapidly forms the silhouette of a majestic mountain range with sharp, defined peaks. Next, softer, diluted grey tones billow around the mountains, creating layers of atmospheric mist and clouds, while a simple, dark stroke materializes as a lone boat on a tranquil, watery expanse at the base. As the landscape is formed, the ink elementsâ€”the lines of the mountains, wisps of cloud, and the shape of the boatâ€”begin to deconstruct, dissolving into flowing streams of liquid ink. Finally, these streams move gracefully across the paper's empty white space, converging and elegantly reorganizing to form the text \"Hunyuan Video 1.5\" in a fluid, semi-cursive calligraphic style. The background is the minimalist expanse of the Xuan paper itself, its texture providing a subtle depth. The entire process is lit by soft, even, diffused light from above, which enhances the rich tonal variations of the ink and the delicate texture of the paper without creating harsh shadows. Bird's-eye view. The camera is positioned directly above the subject, capturing the entire process. The camera remains static. The aesthetic is a high-quality, dynamic Chinese ink wash animation style, perfectly simulating the real-world physics of ink spreading on wet paper. The entire sheet of paper and the final text are kept fully within the frame. Poetic, elegant, artistic. The overall video presents a dynamic Chinese ink wash animation style.``` </details>|\n|Physics Compliance|<video src=\"https://github.com/user-attachments/assets/f1d74e48-cc03-415d-b75f-f7186a4fb41d\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```In a sleek museum gallery, a woman pauses before a gilded oil painting. The painted man inside slowly comes alive, lifting a bottle and pouring real wine straight from the canvas into her glass. Surrounded by stylish art critics moving naturally through the hall, she accepts the pour with calm elegance, as if the impossible were routine. ``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```In a sleek museum gallery, a woman receives a glass of wine poured directly from an animated oil painting. A sophisticated woman with dark hair tied back elegantly stands in the mid-ground. She is wearing a simple, black silk sleeveless dress and holds a clear, crystal wine glass in her right hand. She is positioned before a large, baroque-style oil painting in an ornate, gilded frame. Inside the painting, an aristocratic man with a mustache, dressed in a dark velvet doublet with a white lace collar, is depicted. His form is defined by visible, impasto oil brushstrokes. Initially, the woman watches the painting with calm poise. Then, the painted man's arm slowly animates, his painted texture retained as he lifts a dark bottle. Next, a photorealistic stream of red wine emerges directly from the flat canvas surface, arcing through the air and splashing gently into the real crystal glass she holds. She remains perfectly still, accepting the impossible pour with a subtle, knowing smile. The setting is a modern art gallery with high white walls and polished dark concrete floors that reflect the ambient light. Focused track lighting from the high ceiling casts a warm, dramatic spotlight on the woman and the painting, creating soft shadows. In the background, two other gallery patrons, a man and a woman in stylish, modern attire, stroll slowly from right to left, their figures slightly blurred by a shallow depth of field, moving naturally through the hall. The shot is at an eye-level angle with the woman. The camera remains static, capturing the surreal event in a steady medium shot. The lighting is high-contrast and dramatic, reminiscent of a cinematic photography realistic style, using soft side lighting to accentuate the woman's features and the texture of the painting. The mood is surreal, elegant, and mysterious. The overall video presents a cinematic photography realistic style.``` </details>|<video src=\"https://github.com/user-attachments/assets/07bcce06-ff4f-4688-8c60-c02f600635ea\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```An intact soda can is slowly crushed by a hand.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```In a medium close-up, a hand slowly crushes an intact red and white soda can on a wooden table. A male hand with visible, realistic skin texture is wrapped firmly around the middle of an intact, pristine red and white aluminum soda can. The can, covered in glistening condensation droplets, rests on a dark, polished wooden surface. The cinematic realism captures every minute detail of the scene. Initially, the hand's grip is steady, with the can's cylindrical shape perfectly preserved. Then, the fingers begin to tighten slowly, the knuckles whitening slightly from the exertion. Next, the smooth aluminum surface starts to buckle under the controlled pressure, a sharp crease forming vertically down its side as the metallic sheen distorts. As the hand continues its deliberate squeeze, the can collapses inward progressively, the vibrant red paint wrinkling as the metal structure crumples. Finally, the can is left significantly crushed, its form now an irregular, crumpled shape held tightly in the fist. The scene takes place on a dark, polished wooden tabletop that catches soft, diffuse reflections. The grain of the wood is faintly discernible, adding a layer of texture to the foreground. The background is completely out of focus, rendered as a soft, dark, and non-descript blur, which isolates the main action and enhances the photorealistic quality of the shot. The shot is a medium close-up, presented in a cinematic photography realistic style. The camera remains static at a slightly high angle, looking down to provide a clear and unobstructed view of the can's deformation. Soft side lighting creates high contrast, sculpting the muscles and tendons of the hand while casting specular highlights on the metallic can and the water droplets. The atmosphere is focused and intense. The overall video presents a cinematic photography realistic style.``` </details>|\n|Camera Movement|<video src=\"https://github.com/user-attachments/assets/6deacbfe-4cca-48d7-a2be-cb638a3e01cb\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```åœ£è¯èŠ‚çš„å®¶ä¸­ï¼Œå°å¥³å­©é ç€å¦ˆå¦ˆå¬å¦ˆå¦ˆè¯»ä¹¦ï¼ŒèƒŒæ™¯æ˜¯ä¸‹ç€é›ªçš„çª—å¤–ï¼Œé•œå¤´ç¼“æ…¢ä¸‹ç§»ï¼Œä¸€åªå¯çˆ±çš„é•¿æ¯›å°ç™½çŒ«æˆ´ç€åœ£è¯å¸½è¶´åœ¨æ¸©æš–çš„åœ°æ‘Šä¸Š``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```In a cozy home on Christmas, a young girl leans against her mother as they read a book, and the camera moves down to reveal a fluffy white cat in a Santa hat resting on a warm rug. In a warmly lit living room on a snowy Christmas evening, a young mother and her little daughter are sitting together on a comfortable sofa. The mother, with a gentle expression and wearing a cream-colored knitted sweater, holds an open storybook with colorful illustrations. Her daughter, a small girl with brown hair in pigtails and a red pajama set, leans her head affectionately on her mother's shoulder, her eyes fixed on the book. On the floor below them, a fluffy, long-haired white cat is curled up on a plush, beige wool rug. The cat wears a tiny red and white Santa hat perched between its ears. Initially, the shot focuses on the mother and daughter, capturing their quiet, shared moment. The motherâ€™s finger gently rests on the page of the book. Then, the camera slowly moves downward, gliding past the book and their laps. Finally, the camera settles at a low angle, bringing the adorable white cat into sharp focus as the primary subject. The cat's chest gently rises and falls with each breath, its eyes peacefully closed. Through a large window in the background, large, soft snowflakes can be seen falling silently against the dark blue twilight sky, creating a peaceful and serene backdrop. Faint, out-of-focus golden Christmas lights twinkle in the corner of the room, adding to the warm, festive atmosphere. The scene is imbued with a sense of comfort and holiday warmth, creating a beautiful cinematic photography realistic image. The camera slowly moves downward. The shot uses soft, warm interior lighting that casts gentle shadows, creating a high-contrast, cinematic look. A shallow depth of field keeps the focus on the subjects while beautifully blurring the background elements. The mood is heartwarming, peaceful, and festive. The overall video presents a cinematic photography realistic style.``` </details>|<video src=\"https://github.com/user-attachments/assets/8e72ed0f-f8ac-445b-97e5-eb4b16fbc121\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```The hiker begins walking forward along the trail, causing the water bottle to swing rhythmically with each step. The camera gradually pulls back and rises to reveal a vast desert landscape stretching out ahead.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```The hiker begins walking forward along the trail, causing the water bottle to swing rhythmically with each step. The camera gradually pulls back and rises to reveal a vast desert landscape stretching out ahead, while the sun position shifts from afternoon to dusk, casting increasingly longer shadows across the terrain as the figure becomes smaller in the frame.``` </details>|\n|Multi-Style Support|<video src=\"https://github.com/user-attachments/assets/65b2c5a5-e6ba-43be-9462-a98b03b675f1\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```Have the cake man begin to take chunks out of himself and eat it.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```The cake man sits on the chair, with his hands resting on his knees. Then, he slowly raises his right hand and breaks off a piece of cake from his left shoulder. Next, he brings the piece of cake to his mouth and begins to chew. At the same time, his eyes widen slightly, and his mouth parts gently. After that, he raises his right hand again, breaks off another piece of cake from his right arm, and repeats the action of bringing it to his mouth to chew.``` </details>|<video src=\"https://github.com/user-attachments/assets/de5f7480-b79c-4fc1-b345-c5880a3b5f9e\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```A little girl, carrying a colorful handbag, skips through the garden.  The video uses claymation style.``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```A little girl with a colorful handbag skips through a whimsical claymation garden. In a vibrant garden constructed entirely from clay, a young girl, meticulously crafted in a claymation style, skips joyfully. She has chunky, sculpted yellow clay hair tied in pigtails that bounce with a slight stiffness, simple black button eyes, and a wide, permanently etched smile. She wears a simple pink clay dress with a white collar. In her left hand, she carries a small handbag molded from bright red and blue clay, which swings in a slightly jerky arc as she moves. Initially, the girl lifts her right leg high, her body momentarily suspended in a classic stop-motion pose. Then, she hops forward, landing lightly as her left leg swings through for the next skip. Her arms move in an exaggerated, back-and-forth rhythm, characteristic of stop-motion animation. Her movements are intentionally not perfectly fluid, highlighting the frame-by-frame nature of the claymation technique. The garden around her is a whimsical, textured world. In the foreground and mid-ground, oversized flowers with swirled purple and orange petals stand on thick green stems. The ground is a textured mat of green clay, showing subtle fingerprints and tool marks that add to the handmade charm. In the background, a pale blue clay backdrop features a simplified, smiling sun molded from yellow clay. The shot is at an eye-level angle with the main subject. The camera follows the subject, moving smoothly to the right to keep her in the frame. The lighting is bright and even, casting soft shadows that emphasize the rounded, three-dimensional forms of the clay models. The overall video presents a charming and detailed claymation style.``` </details>|\n|High Image-Video Consistency|<img src=\"https://github.com/user-attachments/assets/3bc8e55d-c211-454e-8067-128c0e215eb6\"> <video src=\"https://github.com/user-attachments/assets/3e6b7ee9-ec66-4e46-a446-801b1c1a1c81\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```å¥³å­©æ”¾ä¸‹ä¹¦ï¼Œç«™èµ·èº«ï¼Œè½¬èº«å‘å±‹å†…èµ°å»ã€‚é•œå¤´æ‹‰è¿œã€‚``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```å¥³å­©åˆä¸Šæ‰‹ä¸­çš„ä¹¦ï¼Œå°†ä¹¦æ”¾åœ¨èº«ä¾§çš„çª—å°ä¸Šã€‚éšåï¼Œå¥¹ç¼“ç¼“ç«™èµ·èº«ï¼Œè½¬èº«å‘å±‹å†…èµ°å»ï¼Œèº«å½±é€æ¸æ²¡å…¥é—¨åçš„é˜´å½±ä¸­ã€‚é•œå¤´ç¼“ç¼“æ‹‰è¿œï¼Œéœ²å‡ºæ›´å¤šè¢«ç»¿æ¤è¦†ç›–çš„å±‹æªå’Œå¢™ä½“ã€‚``` </details>|<img src=\"https://github.com/user-attachments/assets/7657ce60-90b5-4fdc-b713-0eaa55829b09\"> <video src=\"https://github.com/user-attachments/assets/9ca24021-2353-40d5-8a4d-0f8e67d51826\" width=\"600\"> </video> <details><summary>ğŸ“‹ Show input prompt</summary> ```å¥³äººæ‰‹ä¸Šçš„é¸Ÿäº²äº†å¥³äººä¸€å£``` </details> <details><summary>ğŸ“‹ Show rewrite prompt</summary> ```å¥³äººæ‰‹è‡‚ä¸Šçš„ç™½è‰²é¹¦é¹‰ç¼“ç¼“è½¬è¿‡å¤´ï¼Œå°†å–™è½»è½»è§¦ç¢°å¥³äººçš„è„¸é¢Šï¼Œéšåæ”¶å›å¤´éƒ¨ã€‚å¥³äººå˜´è§’å¾®å¾®ä¸Šæ‰¬ï¼Œç›®å…‰æ¸©æŸ”åœ°æ³¨è§†ç€é¹¦é¹‰ã€‚èƒŒæ™¯ä¸­çš„ç»¿æ¤ä¿æŒé™æ­¢ã€‚``` </details>|\n\n\n\n\n## ğŸ“š Citation\n\n```bibtex\n@misc{hunyuanvideo2025,\n      title={HunyuanVideo 1.5 Technical Report}, \n      author={Tencent Hunyuan Foundation Model Team},\n      year={2025},\n      eprint={2511.18870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2511.18870}, \n}\n```\n\n## ğŸ™ Acknowledgements\nWe would like to thank the contributors to the [Transformers](https://github.com/huggingface/transformers), [Diffusers](https://github.com/huggingface/diffusers) , [HuggingFace](https://huggingface.co/) and [Qwen-VL](https://github.com/QwenLM/Qwen-VL), for their open research and exploration.\n\n## ğŸŒŸ Github Star History\n\n<a href=\"https://star-history.com/#Tencent-Hunyuan/HunyuanVideo-1.5&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1\" />\n </picture>\n</a>\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-video\",\"library_name\":\"HunyuanVideo-1.5\",\"framework\":\"HunyuanVideo-1.5\",\"params\":null,\"storage_bytes\":371770754991,\"files_count\":46,\"spaces_count\":46,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanVideo-1.5\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanVideo-1.5\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanVideo-1.5\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5\"},{\"type\":\"has_code\",\"target_id\":\"github:ModelTC:LightX2V\\\"\",\"source_url\":\"https://github.com/ModelTC/LightX2V\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:comfyanonymous:ComfyUI\",\"source_url\":\"https://github.com/comfyanonymous/ComfyUI\"},{\"type\":\"has_code\",\"target_id\":\"github:yuanyuan-spec:comfyui_hunyuanvideo_1.5_plugin\",\"source_url\":\"https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin\"},{\"type\":\"has_code\",\"target_id\":\"github:ModelTC:LightX2V\",\"source_url\":\"https://github.com/ModelTC/LightX2V\"},{\"type\":\"has_code\",\"target_id\":\"github:deepbeepmeep:Wan2GP\",\"source_url\":\"https://github.com/deepbeepmeep/Wan2GP\"},{\"type\":\"has_code\",\"target_id\":\"github:Zehong-Ma:ComfyUI-MagCache\",\"source_url\":\"https://github.com/Zehong-Ma/ComfyUI-MagCache\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanVideo-1.5.git\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git\"},{\"type\":\"has_code\",\"target_id\":\"github:Dao-AILab:flash-attention\",\"source_url\":\"https://github.com/Dao-AILab/flash-attention\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:flex-block-attn.git\",\"source_url\":\"https://github.com/Tencent-Hunyuan/flex-block-attn.git\"},{\"type\":\"has_code\",\"target_id\":\"github:cooper1637:SageAttention.git\",\"source_url\":\"https://github.com/cooper1637/SageAttention.git\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanVideo-1.5\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-VL\",\"source_url\":\"https://github.com/QwenLM/Qwen-VL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2511.18870\",\"source_url\":\"https://arxiv.org/abs/2511.18870\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 99.2,
    "content_hash": "9224429bdedf233a2f69f50f2c4e162d",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/tencent/HunyuanVideo-1.5/resolve/main/assets/I2V_GSB.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/HunyuanVideo-1.5\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:segmind:ssd-1b",
    "name": "SSD-1B",
    "author": "segmind",
    "description": "--- license: apache-2.0 tags: - text-to-image - ultra-realistic - text-to-image - stable-diffusion - distilled-model - knowledge-distillation pinned: true datasets: - zzliang/GRIT - wanng/midjourney-v5-202304-clean library_name: diffusers --- !image/png Try out the model at Segmind SSD-1B for âš¡ fastest inference. You can also try it on ğŸ¤— Spaces The Segmind Stable Diffusion Model (SSD-1B) is a **distilled 50% smaller** version of the Stable Diffusion XL (SDXL), offering a **60% speedup** whil...",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "ultra-realistic",
      "stable-diffusion",
      "distilled-model",
      "knowledge-distillation",
      "dataset:zzliang/grit",
      "dataset:wanng/midjourney-v5-202304-clean",
      "arxiv:2401.02677",
      "license:apache-2.0",
      "endpoints_compatible",
      "diffusers:stablediffusionxlpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 827,
    "downloads": 12333,
    "source": "huggingface",
    "source_url": "https://huggingface.co/segmind/SSD-1B",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ntags:\n- text-to-image\n- ultra-realistic\n- text-to-image\n- stable-diffusion\n- distilled-model\n- knowledge-distillation\npinned: true\ndatasets:\n- zzliang/GRIT\n- wanng/midjourney-v5-202304-clean\nlibrary_name: diffusers\n---\n\n# Segmind Stable Diffusion 1B (SSD-1B) Model Card\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/WveKcu7q5PyZEwNezyyMC.png)\n\n## ğŸ“£ Read our [technical report](https://huggingface.co/papers/2401.02677) for more details on our disillation method\n\n## AUTOMATIC1111 compatibility added. Supporting file [here](https://huggingface.co/segmind/SSD-1B/blob/main/SSD-1B-A1111.safetensors)\n\n## Demo\n\nTry out the model at [Segmind SSD-1B](https://www.segmind.com/models/ssd-1b?utm_source=hf) for âš¡ fastest inference. You can also try it on [ğŸ¤— Spaces](https://huggingface.co/spaces/segmind/Segmind-Stable-Diffusion)\n\n## Model Description\n\nThe Segmind Stable Diffusion Model (SSD-1B) is a **distilled 50% smaller** version of the Stable Diffusion XL (SDXL), offering a **60% speedup** while maintaining high-quality text-to-image generation capabilities. It has been trained on diverse datasets, including Grit and Midjourney scrape data, to enhance its ability to create a wide range of visual content based on textual prompts.\n\nThis model employs a knowledge distillation strategy, where it leverages the teachings of several expert models in succession, including SDXL, ZavyChromaXL, and JuggernautXL, to combine their strengths and produce impressive visual outputs.\n\nSpecial thanks to the HF team ğŸ¤— especially [Sayak](https://huggingface.co/sayakpaul), [Patrick](https://github.com/patrickvonplaten) and [Poli](https://huggingface.co/multimodalart) for their collaboration and guidance on this work.\n\n## Image Comparision (SDXL-1.0 vs SSD-1B)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/mOM_OMxbivVBELad1QQYj.png)\n\n## Usage:\nThis model can be used via the ğŸ§¨ Diffusers library. \n\nMake sure to install diffusers from source by running\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nIn addition, please install `transformers`, `safetensors` and `accelerate`:\n```\npip install transformers accelerate safetensors\n```\n\nTo use the model, you can run the following:\n\n```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\npipe = StableDiffusionXLPipeline.from_pretrained(\"segmind/SSD-1B\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\nprompt = \"An astronaut riding a green horse\" # Your prompt here\nneg_prompt = \"ugly, blurry, poor quality\" # Negative prompt here\nimage = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]\n```\n### Update: Our model should now be usable in ComfyUI.\n### Please do use negative prompting, and a CFG around 9.0 for the best quality!\n### Model Description\n\n- **Developed by:** [Segmind](https://www.segmind.com/)\n- **Developers:** [Yatharth Gupta](https://huggingface.co/Warlord-K) and [Vishnu Jaddipal](https://huggingface.co/Icar).\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** Apache 2.0\n- **Distilled From** [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n\n\n### Key Features\n\n- **Text-to-Image Generation:** The model excels at generating images from text prompts, enabling a wide range of creative applications.\n\n- **Distilled for Speed:** Designed for efficiency, this model offers a 60% speedup, making it a practical choice for real-time applications and scenarios where rapid image generation is essential.\n\n- **Diverse Training Data:** Trained on diverse datasets, the model can handle a variety of textual prompts and generate corresponding images effectively.\n\n- **Knowledge Distillation:** By distilling knowledge from multiple expert models, the Segmind Stable Diffusion Model combines their strengths and minimizes their limitations, resulting in improved performance.\n\n### Model Architecture\n\nThe SSD-1B Model is a 1.3B Parameter Model which has several layers removed from the Base SDXL Model\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/Qa8Ow-moLQhOvzp-5kGt4.png)\n\n### Training info\n\nThese are the key hyperparameters used during training:\n\n* Steps: 251000\n* Learning rate: 1e-5\n* Batch size: 32\n* Gradient accumulation steps: 4\n* Image resolution: 1024\n* Mixed-precision: fp16\n\n### Multi-Resolution Support\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/IwIaIB4nBdMx6Vs5q82cL.jpeg)\n\nSSD-1B can support the following output resolutions.\n\n* 1024 x 1024 (1:1 Square)\n* 1152 x 896 (9:7)\n* 896 x 1152 (7:9)\n* 1216 x 832 (19:13)\n* 832 x 1216 (13:19)\n* 1344 x 768 (7:4 Horizontal)\n* 768 x 1344 (4:7 Vertical)\n* 1536 x 640 (12:5 Horizontal)\n* 640 x 1536 (5:12 Vertical)\n    \n\n### Speed Comparision\n\nWe have observed that SSD-1B is upto 60% faster than the Base SDXL Model. Below is a comparision on an A100 80GB.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/TyymF1OkUjXLrHUp1XF0t.png)\n\nBelow are the speed up metrics on a RTX 4090 GPU.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/moMZrlDr-HTFkZlqWHUjQ.png)\n\n### Model Sources\n\nFor research and development purposes, the SSD-1B Model can be accessed via the Segmind AI platform. For more information and access details, please visit [Segmind](https://www.segmind.com/models/ssd-1b).\n\n## Uses\n\n\n### Direct Use\n\nThe Segmind Stable Diffusion Model is suitable for research and practical applications in various domains, including:\n\n- **Art and Design:** It can be used to generate artworks, designs, and other creative content, providing inspiration and enhancing the creative process.\n\n- **Education:** The model can be applied in educational tools to create visual content for teaching and learning purposes.\n\n- **Research:** Researchers can use the model to explore generative models, evaluate its performance, and push the boundaries of text-to-image generation.\n\n- **Safe Content Generation:** It offers a safe and controlled way to generate content, reducing the risk of harmful or inappropriate outputs.\n\n- **Bias and Limitation Analysis:** Researchers and developers can use the model to probe its limitations and biases, contributing to a better understanding of generative models' behavior.\n\n### Downstream Use\n\nThe Segmind Stable Diffusion Model can also be used directly with the ğŸ§¨ Diffusers library training scripts for further training, including:\n\n- **[LoRA](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py):**\n```bash\nexport MODEL_NAME=\"segmind/SSD-1B\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch train_text_to_image_lora_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=1024 --random_flip \\\n  --train_batch_size=1 \\\n  --num_train_epochs=2 --checkpointing_steps=500 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --seed=42 \\\n  --output_dir=\"sd-pokemon-model-lora-ssd\" \\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n  --push_to_hub\n```\n\n- **[Fine-Tune](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py):**\n```bash\nexport MODEL_NAME=\"segmind/SSD-1B\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch train_text_to_image_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --enable_xformers_memory_efficient_attention \\\n  --resolution=512 --center_crop --random_flip \\\n  --proportion_empty_prompts=0.2 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 --gradient_checkpointing \\\n  --max_train_steps=10000 \\\n  --use_8bit_adam \\\n  --learning_rate=1e-06 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --mixed_precision=\"fp16\" \\\n  --report_to=\"wandb\" \\\n  --validation_prompt=\"a cute Sundar Pichai creature\" --validation_epochs 5 \\\n  --checkpointing_steps=5000 \\\n  --output_dir=\"ssd-pokemon-model\" \\\n  --push_to_hub\n```\n- **[Dreambooth LoRA](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_sdxl.py):**\n```bash\nexport MODEL_NAME=\"segmind/SSD-1B\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"lora-trained-xl\"\nexport VAE_PATH=\"madebyollin/sdxl-vae-fp16-fix\"\n\naccelerate launch train_dreambooth_lora_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --pretrained_vae_model_name_or_path=$VAE_PATH \\\n  --output_dir=$OUTPUT_DIR \\\n  --mixed_precision=\"fp16\" \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --resolution=1024 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --learning_rate=1e-5 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=500 \\\n  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n  --validation_epochs=25 \\\n  --seed=\"0\" \\\n  --push_to_hub\n```\n\n### Out-of-Scope Use\n\nThe SSD-1B Model is not suitable for creating factual or accurate representations of people, events, or real-world information. It is not intended for tasks requiring high precision and accuracy.\n\n## Limitations and Bias\n\nLimitations & Bias\nThe SSD-1B Model has some challenges in embodying absolute photorealism, especially in human depictions. While it grapples with incorporating clear text and maintaining the fidelity of complex compositions due to its autoencoding approach, these hurdles pave the way for future enhancements. Importantly, the model's exposure to a diverse dataset, though not a panacea for ingrained societal and digital biases, represents a foundational step towards more equitable technology. Users are encouraged to interact with this pioneering tool with an understanding of its current limitations, fostering an environment of conscious engagement and anticipation for its continued evolution.\n\n## Citation\n```\n@misc{gupta2024progressive,\n      title={Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss}, \n      author={Yatharth Gupta and Vishnu V. Jaddipal and Harish Prabhala and Sayak Paul and Patrick Von Platen},\n      year={2024},\n      eprint={2401.02677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":28747978639,\"files_count\":27,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionXLPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2401.02677\",\"source_url\":\"https://arxiv.org/abs/2401.02677\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.2,
    "content_hash": "613a66ed381dfcec9824912604c5f89c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/segmind/SSD-1B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-2b-it",
    "name": "gemma-2b-it",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gguf",
      "gemma",
      "text-generation",
      "conversational",
      "arxiv:2312.11805",
      "arxiv:2009.03300",
      "arxiv:1905.07830",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1905.10044",
      "arxiv:1907.10641",
      "arxiv:1811.00937",
      "arxiv:1809.02789",
      "arxiv:1911.01547",
      "arxiv:1705.03551",
      "arxiv:2107.03374",
      "arxiv:2108.07732",
      "arxiv:2110.14168",
      "arxiv:2304.06364",
      "arxiv:2206.04615",
      "arxiv:1804.06876",
      "arxiv:2110.08193",
      "arxiv:2009.11462",
      "arxiv:2101.11718",
      "arxiv:1804.09301",
      "arxiv:2109.07958",
      "arxiv:2203.09509",
      "license:gemma",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 826,
    "downloads": 68741,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-2b-it",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":2506172416,\"storage_bytes\":52310489489,\"files_count\":12,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"GemmaForCausalLM\"],\"model_type\":\"gemma\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"chat_template\":\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.11805\",\"source_url\":\"https://arxiv.org/abs/2312.11805\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1811.00937\",\"source_url\":\"https://arxiv.org/abs/1811.00937\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1809.02789\",\"source_url\":\"https://arxiv.org/abs/1809.02789\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.14168\",\"source_url\":\"https://arxiv.org/abs/2110.14168\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.06364\",\"source_url\":\"https://arxiv.org/abs/2304.06364\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2206.04615\",\"source_url\":\"https://arxiv.org/abs/2206.04615\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1804.06876\",\"source_url\":\"https://arxiv.org/abs/1804.06876\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.08193\",\"source_url\":\"https://arxiv.org/abs/2110.08193\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.11462\",\"source_url\":\"https://arxiv.org/abs/2009.11462\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2101.11718\",\"source_url\":\"https://arxiv.org/abs/2101.11718\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1804.09301\",\"source_url\":\"https://arxiv.org/abs/1804.09301\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2109.07958\",\"source_url\":\"https://arxiv.org/abs/2109.07958\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.09509\",\"source_url\":\"https://arxiv.org/abs/2203.09509\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 39.2,
    "content_hash": "97736eb4875d69c74c9a738d5cab0bba",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-2b-it\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:black-forest-labs:flux.1-krea-dev",
    "name": "FLUX.1-Krea-dev",
    "author": "black-forest-labs",
    "description": "",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "image-generation",
      "flux",
      "en",
      "base_model:black-forest-labs/flux.1-dev",
      "base_model:finetune:black-forest-labs/flux.1-dev",
      "license:other",
      "endpoints_compatible",
      "diffusers:fluxpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 826,
    "downloads": 35754,
    "source": "huggingface",
    "source_url": "https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":57893109574,\"files_count\":29,\"spaces_count\":79,\"gated\":\"auto\",\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"FluxPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39.2,
    "content_hash": "0303983206abb03d72d21182d9c5ca85",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-30b-a3b",
    "name": "Qwen3-30B-A3B",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-30B-A3B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-30B-A3B-Base --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen3 is the latest generation of large language models in Qwen series,...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3_moe",
      "text-generation",
      "conversational",
      "arxiv:2309.00071",
      "arxiv:2505.09388",
      "base_model:qwen/qwen3-30b-a3b-base",
      "base_model:finetune:qwen/qwen3-30b-a3b-base",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 824,
    "downloads": 396990,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-30B-A3B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-30B-A3B-Base\n---\n\n# Qwen3-30B-A3B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-30B-A3B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 30.5B in total and 3.3B activated\n- Number of Paramaters (Non-Embedding): 29.9B\n- Number of Layers: 48\n- Number of Attention Heads (GQA): 32 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3_moe'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-30B-A3B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-30B-A3B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-30B-A3B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":30532122624,\"storage_bytes\":61077998302,\"files_count\":26,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3MoeForCausalLM\"],\"model_type\":\"qwen3_moe\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3\",\"source_url\":\"https://github.com/QwenLM/Qwen3\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.2,
    "content_hash": "cf19637160f60d09cf918a19e5dd5273",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-30B-A3B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:baai:bge-reranker-v2-m3",
    "name": "bge-reranker-v2-m3",
    "author": "BAAI",
    "description": "--- license: apache-2.0 pipeline_tag: text-classification tags: - transformers - sentence-transformers - text-embeddings-inference language: - multilingual --- **More details please refer to our Github: FlagEmbedding.** - Model List - Usage - Fine-tuning - Evaluation - Citation Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. You can get a relevance score by inputting query and passage to the reranker. And the sc...",
    "tags": [
      "sentence-transformers",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "transformers",
      "text-embeddings-inference",
      "multilingual",
      "arxiv:2312.15503",
      "arxiv:2402.03216",
      "license:apache-2.0",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "likes": 823,
    "downloads": 2997233,
    "source": "huggingface",
    "source_url": "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "image_url": "https://huggingface.co/BAAI/bge-reranker-v2-m3/resolve/main/assets/BEIR-bge-en-v1.5.png",
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\npipeline_tag: text-classification\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\nlanguage:\n- multilingual\n---\n\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"prompt\": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\\n--output_dir {path to save model} \\\n--model_name_or_path google/gemma-2b \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\\n--output_dir {path to save model} \\\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj \\\n--start_layer 8 \\\n--head_multi True \\\n--head_type simple \\\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-classification\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":567755777,\"storage_bytes\":7403932631,\"files_count\":13,\"spaces_count\":73,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"XLMRobertaForSequenceClassification\"],\"model_type\":\"xlm-roberta\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"cls_token\":\"<s>\",\"eos_token\":\"</s>\",\"mask_token\":\"<mask>\",\"pad_token\":\"<pad>\",\"sep_token\":\"</s>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.15503\",\"source_url\":\"https://arxiv.org/abs/2312.15503\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2402.03216\",\"source_url\":\"https://arxiv.org/abs/2402.03216\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 99.2,
    "content_hash": "92440e2d175d77e6e771b1cfa509ddd3",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/BAAI/bge-reranker-v2-m3/resolve/main/assets/BEIR-bge-en-v1.5.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/BAAI/bge-reranker-v2-m3\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:openai:clip-vit-base-patch32",
    "name": "clip-vit-base-patch32",
    "author": "openai",
    "description": "--- tags: - vision widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png candidate_labels: playing music, playing sports example_title: Cat & Dog --- Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here. The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generali...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "clip",
      "zero-shot-image-classification",
      "vision",
      "arxiv:2103.00020",
      "arxiv:1908.04913",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "zero-shot-image-classification",
    "likes": 822,
    "downloads": 19543440,
    "source": "huggingface",
    "source_url": "https://huggingface.co/openai/clip-vit-base-patch32",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context theyâ€™re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPâ€™s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with â€˜Middle Easternâ€™ having the highest accuracy (98.4%) and â€˜Whiteâ€™ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)",
    "meta_json": "{\"pipeline_tag\":\"zero-shot-image-classification\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":3632041404,\"files_count\":12,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"CLIPModel\"],\"model_type\":\"clip\",\"tokenizer_config\":{\"unk_token\":{\"content\":\"<|endoftext|>\",\"single_word\":false,\"lstrip\":false,\"rstrip\":false,\"normalized\":true,\"__type\":\"AddedToken\"},\"bos_token\":{\"content\":\"<|startoftext|>\",\"single_word\":false,\"lstrip\":false,\"rstrip\":false,\"normalized\":true,\"__type\":\"AddedToken\"},\"eos_token\":{\"content\":\"<|endoftext|>\",\"single_word\":false,\"lstrip\":false,\"rstrip\":false,\"normalized\":true,\"__type\":\"AddedToken\"},\"pad_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:openai:CLIP\",\"source_url\":\"https://github.com/openai/CLIP\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.00020\",\"source_url\":\"https://arxiv.org/abs/2103.00020\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1908.04913\",\"source_url\":\"https://arxiv.org/abs/1908.04913\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 54.2,
    "content_hash": "cfa90db808c983eea7430f58dd92185e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/openai/clip-vit-base-patch32\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:salesforce:blip-image-captioning-base",
    "name": "blip-image-captioning-base",
    "author": "Salesforce",
    "description": "--- pipeline_tag: image-to-text tags: - image-captioning languages: - en license: bsd-3-clause --- Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone). | !BLIP.gif | |:--:| | <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>| Authors from the paper write in the abstract: *Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-tra...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "blip",
      "image-to-text",
      "image-captioning",
      "arxiv:2201.12086",
      "license:bsd-3-clause",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-to-text",
    "likes": 822,
    "downloads": 2302902,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Salesforce/blip-image-captioning-base",
    "image_url": null,
    "type": "model",
    "body_content": "---\npipeline_tag: image-to-text\ntags:\n- image-captioning\nlanguages:\n- en\nlicense: bsd-3-clause\n---\n\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\nModel card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).\n\n| ![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif) |\n|:--:|\n| <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>|\n\n## TL;DR\n\nAuthors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:\n\n*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n\n## Usage\n\nYou can use this model for conditional and un-conditional image captioning\n\n### Using the Pytorch model\n\n#### Running the model on CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n#### Running the model on GPU\n\n##### In full precision \n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n##### In half precision (`float16`)\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n## Ethical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peopleâ€™s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\n\n\n## BibTex and citation info\n\n```\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"image-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":5939080634,\"files_count\":10,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BlipForConditionalGeneration\"],\"model_type\":\"blip\",\"tokenizer_config\":{\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\",\"pad_token\":\"[PAD]\",\"sep_token\":\"[SEP]\",\"unk_token\":\"[UNK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:salesforce:BLIP\",\"source_url\":\"https://github.com/salesforce/BLIP\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2201.12086\",\"source_url\":\"https://arxiv.org/abs/2201.12086\"}]",
    "canonical_id": null,
    "license_spdx": "BSD-3-Clause",
    "compliance_status": "approved",
    "quality_score": 64.2,
    "content_hash": "95c08ec889de7d221134556e89f950c3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Salesforce/blip-image-captioning-base\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-prover-v2-671b",
    "name": "DeepSeek-Prover-V2-671B",
    "author": "deepseek-ai",
    "description": "--- library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/De...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v3",
      "text-generation",
      "conversational",
      "custom_code",
      "text-generation-inference",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 816,
    "downloads": 431,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## 1. Introduction\n\nWe introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/figures/performance.png?raw=true\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Synthesize Cold-Start Reasoning Data through Recursive Proof Search**\n\n- To construct the cold-start dataset, we develop a simple yet effective pipeline for recursive theorem proving, utilizing DeepSeek-V3 as a unified tool for both subgoal decomposition and formalization. We prompt DeepSeek-V3 to decompose theorems into high-level proof sketches while simultaneously formalizing these proof steps in Lean 4, resulting in a sequence of subgoals.\n\n- We use a smaller 7B model to handle the proof search for each subgoal, thereby reducing the associated computational burden. Once the decomposed steps of a challenging problem are resolved, we pair the complete step-by-step formal proof with the corresponding chain-of-thought from DeepSeek-V3 to create cold-start reasoning data.\n\n---\n\n**Reinforcement Learning with Synthetic Cold-Start Data**\n\n- We curate a subset of challenging problems that remain unsolved by the 7B prover model in an end-to-end manner, but for which all decomposed subgoals have been successfully resolved. By composing the proofs of all subgoals, we construct a complete formal proof for the original problem. This proof is then appended to DeepSeek-V3's chain-of-thought, which outlines the corresponding lemma decomposition, thereby producing a cohesive synthesis of informal reasoning and subsequent formalization.\n\n- After fine-tuning the prover model on the synthetic cold-start data, we perform a reinforcement learning stage to further enhance its ability to bridge informal reasoning with formal proof construction. Following the standard training objective for reasoning models, we use binary correct-or-incorrect feedback as the primary form of reward supervision.\n- The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching $88.9$% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. The proofs generated by DeepSeek-Prover-V2 for the miniF2F dataset are available for download as a [ZIP archive](https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/master/minif2f-solutions.zip).\n\n---\n\n## 3. ProverBench: Formalization of AIME and Textbook Problems\n\nwe introduce ProverBench, a benchmark dataset comprising 325 problems. Of these, 15 are formalized from number theory and algebra questions featured in the recent AIME competitions (AIME 24 and 25), offering authentic high-school competition-level challenges. The remaining 310 problems are drawn from curated textbook examples and educational tutorials, contributing a diverse and pedagogically grounded collection of formalized mathematical problems. This benchmark is designed to enable more comprehensive evaluation across both high-school competition problems and undergraduate-level mathematics.\n\n<div align=\"center\">\n\n| Area                | Count |\n| :---------------------: | :-------: |\n| AIME 24&25          | 15    |\n| Number Theory       | 40    |\n| Elementary Algebra  | 30    |\n| Linear Algebra      | 50    |\n| Abstract Algebra    | 40    |\n| Calculus            | 90    |\n| Real Analysis       | 30    |\n| Complex Analysis    | 10    |\n| Functional Analysis | 10    |\n| Probability         | 10    |\n| Total               | 325   |\n\n</div>\n\n## 4. Model & Dataset Downloads\n\nWe release DeepSeek-Prover-V2 in two model sizes: 7B and 671B parameters. DeepSeek-Prover-V2-671B is trained on top of DeepSeek-V3-Base. DeepSeek-Prover-V2-7B is built upon DeepSeek-Prover-V1.5-Base and features an extended context length of up to 32K tokens.\n\n<div align=\"center\">\n\n|            **Model**            |                          **Download**                         |\n| :-----------------------------: | :----------------------------------------------------------: |\n|   DeepSeek-Prover-V2-7B   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B) |\n|   DeepSeek-Prover-V2-671B   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B) |\n\n</div>\n\n<div align=\"center\">\n\n|            **Dataset**            |                          **Download**                         |\n| :-----------------------------: | :----------------------------------------------------------: |\n|   DeepSeek-ProverBench   | [ğŸ¤— HuggingFace](https://huggingface.co/datasets/deepseek-ai/DeepSeek-ProverBench) |\n\n</div>\n\n## 5. Quick Start\n\nYou can directly use [Huggingface's Transformers](https://github.com/huggingface/transformers) for model inference. DeepSeek-Prover-V2-671B shares the same architecture as DeepSeek-V3. For detailed information and supported features, please refer to [the DeepSeek-V3 documentation on Hugging Face](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md).\n\nThe following is a basic example of generating a proof for a problem from the miniF2F dataset:\n````python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(30)\n\nmodel_id = \"DeepSeek-Prover-V2-7B\"  # or DeepSeek-Prover-V2-671B\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nformal_statement = \"\"\"\nimport Mathlib\nimport Aesop\n\nset_option maxHeartbeats 0\n\nopen BigOperators Real Nat Topology Rat\n\n/-- What is the positive difference between $120\\%$ of 30 and $130\\%$ of 20? Show that it is 10.-/\ntheorem mathd_algebra_10 : abs ((120 : â„) / 100 * 30 - 130 / 100 * 20) = 10 := by\n  sorry\n\"\"\".strip()\n\nprompt = \"\"\"\nComplete the following Lean 4 code:\n\n```lean4\n{}\n```\n\nBefore producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.\nThe plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.\n\"\"\".strip()\n\nchat = [\n  {\"role\": \"user\", \"content\": prompt.format(formal_statement)},\n]\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\ninputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n\nimport time\nstart = time.time()\noutputs = model.generate(inputs, max_new_tokens=8192)\nprint(tokenizer.batch_decode(outputs))\nprint(time.time() - start)\n````\n\n## 6. License\nThe use of DeepSeek-Prover-V2 models is subject to [the Model License](LICENSE-MODEL).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":684531386000,\"storage_bytes\":688586727753,\"files_count\":172,\"spaces_count\":53,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV3ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_deepseek.DeepseekV3Config\",\"AutoModel\":\"modeling_deepseek.DeepseekV3Model\",\"AutoModelForCausalLM\":\"modeling_deepseek.DeepseekV3ForCausalLM\"},\"model_type\":\"deepseek_v3\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<ï½œUserï½œ>' + message['content'] + '<ï½œAssistantï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- else %}{{message['content'] + '<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- endfor %}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>' + message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{{content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<ï½œAssistantï½œ>'}}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-Prover-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-Prover-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-Prover-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-Prover-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 54.1,
    "content_hash": "3e5e12037076676948f675ee3a36418d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:microsoft:speecht5_tts",
    "name": "speecht5_tts",
    "author": "microsoft",
    "description": "--- license: mit tags: - audio - text-to-speech datasets: - libritts --- SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. This model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. SpeechT5 was first released in this repository, original weights. The license used is MI...",
    "tags": [
      "transformers",
      "pytorch",
      "speecht5",
      "text-to-audio",
      "audio",
      "text-to-speech",
      "dataset:libritts",
      "arxiv:2110.07205",
      "arxiv:1910.09700",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 815,
    "downloads": 89708,
    "source": "huggingface",
    "source_url": "https://huggingface.co/microsoft/speecht5_tts",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https://github.com/microsoft/SpeechT5/), [original weights](https://huggingface.co/mechanicalsea/speecht5-tts). The license used is [MIT](https://github.com/microsoft/SpeechT5/blob/main/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https://huggingface.co/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https://github.com/microsoft/SpeechT5/blob/main/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https://github.com/microsoft/SpeechT5/]\n- **Paper:** [https://arxiv.org/pdf/2110.07205.pdf]\n- **Blog Post:** [https://huggingface.co/blog/speecht5]\n- **Demo:** [https://huggingface.co/spaces/Matthijs/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## ğŸ¤— Transformers Usage\n\nYou can run SpeechT5 TTS locally with the ğŸ¤— Transformers library.\n\n1. First install the ğŸ¤— [Transformers library](https://github.com/huggingface/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n\nembeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n\nsf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n\ninputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n\n# load xvector containing speaker's voice characteristics from a dataset\nembeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n\nsf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https://colab.research.google.com/drive/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https://huggingface.co/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets.\n\nAfter preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":1171111158,\"files_count\":9,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"SpeechT5ForTextToSpeech\"],\"model_type\":\"speecht5\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"</s>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:microsoft:SpeechT5\",\"source_url\":\"https://github.com/microsoft/SpeechT5\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:SpeechT5\",\"source_url\":\"https://github.com/microsoft/SpeechT5\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:SpeechT5\",\"source_url\":\"https://github.com/microsoft/SpeechT5\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:SpeechT5\",\"source_url\":\"https://github.com/microsoft/SpeechT5\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.07205\",\"source_url\":\"https://arxiv.org/abs/2110.07205\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.09700\",\"source_url\":\"https://arxiv.org/abs/1910.09700\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 79.1,
    "content_hash": "a68e886cadd568064eb199ebbd458f07",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/microsoft/speecht5_tts\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:valuefx9507:tifa-deepsex-14b-cot-gguf-q4",
    "name": "Tifa-Deepsex-14b-CoT-GGUF-Q4",
    "author": "ValueFX9507",
    "description": "--- base_model: - deepseek-ai/deepseek-r1-14b language: - zh - en library_name: transformers tags: - incremental-pretraining - sft - reinforcement-learning - roleplay - cot - sex license: apache-2.0 --- - **HF Model**: ValueFX9507/Tifa-Deepsex-14b-CoT - **GGUF**: F16 | Q8ï¼ˆQ4æŸå¤±è¾ƒå¤§ï¼Œå»ºè®®Q8ï¼‰ - **Demo APK**: ç‚¹å‡»ä¸‹è½½ - **ç®€å•çš„å‰ç«¯**ï¼šGithubé“¾æ¥ æœ¬æ¨¡å‹åŸºäºDeepseek-R1-14Bè¿›è¡Œæ·±åº¦ä¼˜åŒ–ï¼Œå€ŸåŠ©Tifa_220Bç”Ÿæˆçš„æ•°æ®é›†é€šè¿‡ä¸‰é‡è®­ç»ƒç­–ç•¥æ˜¾è‘—å¢å¼ºè§’è‰²æ‰®æ¼”ã€å°è¯´æ–‡æœ¬ç”Ÿæˆä¸æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½åŠ›ã€‚ç‰¹åˆ«é€‚åˆéœ€è¦é•¿ç¨‹ä¸Šä¸‹æ–‡å…³è”çš„åˆ›ä½œåœºæ™¯ã€‚ - **ä¸Šæµ·å·¦åŒ—ç§‘æŠ€æä¾›ç®—æ³•ä¸ç®—åŠ›**ä¼ä¸šç½‘å€ - **Deepseekå›¢é˜Ÿå…±äº«GRPOç®—æ³•** - **Qwenå›¢é˜Ÿæä¾›ä¼˜ç§€å¼€æºåº•åº§** ...",
    "tags": [
      "transformers",
      "gguf",
      "incremental-pretraining",
      "sft",
      "reinforcement-learning",
      "roleplay",
      "cot",
      "sex",
      "zh",
      "en",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us",
      "conversational",
      "not-for-all-audiences"
    ],
    "pipeline_tag": "reinforcement-learning",
    "likes": 815,
    "downloads": 3668,
    "source": "huggingface",
    "source_url": "https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4",
    "image_url": null,
    "type": "model",
    "body_content": "---\nbase_model:\n- deepseek-ai/deepseek-r1-14b\nlanguage:\n- zh\n- en\nlibrary_name: transformers\ntags:\n- incremental-pretraining\n- sft\n- reinforcement-learning\n- roleplay\n- cot\n- sex\nlicense: apache-2.0\n---\n# Tifa-Deepseek-14b-CoT\n\n- **HF Model**: [ValueFX9507/Tifa-Deepsex-14b-CoT](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT)\n- **GGUF**: [F16](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT) | [Q8](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-Q8)ï¼ˆQ4æŸå¤±è¾ƒå¤§ï¼Œå»ºè®®Q8ï¼‰\n- **Demo APK**: [ç‚¹å‡»ä¸‹è½½](http://app.visionsic.com/download/projectchat.apk)\n- **ç®€å•çš„å‰ç«¯**ï¼š[Githubé“¾æ¥](https://github.com/Value99/Tifa-Deepsex-OllamaWebUI)\n\næœ¬æ¨¡å‹åŸºäºDeepseek-R1-14Bè¿›è¡Œæ·±åº¦ä¼˜åŒ–ï¼Œå€ŸåŠ©Tifa_220Bç”Ÿæˆçš„æ•°æ®é›†é€šè¿‡ä¸‰é‡è®­ç»ƒç­–ç•¥æ˜¾è‘—å¢å¼ºè§’è‰²æ‰®æ¼”ã€å°è¯´æ–‡æœ¬ç”Ÿæˆä¸æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½åŠ›ã€‚ç‰¹åˆ«é€‚åˆéœ€è¦é•¿ç¨‹ä¸Šä¸‹æ–‡å…³è”çš„åˆ›ä½œåœºæ™¯ã€‚\n\n## é¸£è°¢\n- **ä¸Šæµ·å·¦åŒ—ç§‘æŠ€æä¾›ç®—æ³•ä¸ç®—åŠ›**[ä¼ä¸šç½‘å€](https://leftnorth.com/)\n- **Deepseekå›¢é˜Ÿå…±äº«GRPOç®—æ³•**\n- **Qwenå›¢é˜Ÿæä¾›ä¼˜ç§€å¼€æºåº•åº§**\n- **æ¯æ ¡ä¸Šæµ·å¤æ—¦å¤§å­¦**\n- **PRIMEå›¢é˜Ÿæä¾›ä¼˜åŒ–æ€è·¯**\n\n## ç‰ˆæœ¬ä»‹ç»ï¼š\n- **Tifa-Deepsex-14b-CoT**\n\n  - éªŒè¯æ¨¡å‹ï¼Œæµ‹è¯•RLå¥–åŠ±ç®—æ³•å¯¹äºè§’è‰²æ‰®æ¼”æ•°æ®çš„å½±å“ï¼Œè¯¥ç‰ˆæœ¬ä¸ºåˆç‰ˆï¼Œè¾“å‡ºçµæ´»ä½†æ˜¯ä¸å—æ§åˆ¶ï¼Œä»…åšç ”ç©¶ä½¿ç”¨ã€‚\n\n- **Tifa-Deepsex-14b-CoT-Chat**\n\n  - é‡‡ç”¨æ ‡å‡†æ•°æ®è®­ç»ƒï¼Œä½¿ç”¨æˆç†ŸRLç­–ç•¥ï¼Œé™„åŠ é˜²é‡å¤å¼ºåŒ–å­¦ä¹ ï¼Œé€‚åˆæ­£å¸¸ä½¿ç”¨ï¼Œè¾“å‡ºæ–‡æœ¬è´¨é‡æ­£å¸¸ï¼Œå°‘æ•°æƒ…å†µä¸‹æ€ç»´å‘æ•£ã€‚\n\n    -å¢é‡è®­ç»ƒ0.4Tå°è¯´å†…å®¹\n\n    -100Kç”±TifaMaxç”Ÿæˆçš„SFTæ•°æ®ï¼Œ10Kç”±DeepseekR1ç”Ÿæˆçš„SFTæ•°æ®ï¼Œ2Ké«˜è´¨é‡äººå·¥æ•°æ®\n\n    -30Kç”±TifaMaxç”Ÿæˆçš„DPOå¼ºåŒ–å­¦ä¹ æ•°æ®ï¼Œç”¨äºé˜²æ­¢é‡å¤ï¼Œå¢å¼ºä¸Šä¸‹æ–‡å…³è”ï¼Œæå‡æ”¿æ²»å®‰å…¨æ€§\n\n- **Tifa-Deepsex-14b-CoT-Crazy**\n\n  - å¤§é‡ä½¿ç”¨RLç­–ç•¥ï¼Œä¸»è¦é‡‡ç”¨671Bæ»¡è¡€R1è’¸é¦çš„æ•°æ®ï¼Œè¾“å‡ºå‘æ•£æ€§é«˜ï¼Œç»§æ‰¿R1ä¼˜ç‚¹ï¼Œä¹Ÿç»§æ‰¿äº†R1çš„å±å®³æ€§ã€‚æ–‡å­¦æ€§èƒ½ä½³ã€‚\n\n    -å¢é‡è®­ç»ƒ0.4Tå°è¯´å†…å®¹\n\n    -40Kç”±TifaMaxç”Ÿæˆçš„SFTæ•°æ®ï¼Œ60Kç”±DeepseekR1ç”Ÿæˆçš„SFTæ•°æ®ï¼Œ2Ké«˜è´¨é‡äººå·¥æ•°æ®\n\n    -30Kç”±TifaMaxç”Ÿæˆçš„DPOå¼ºåŒ–å­¦ä¹ æ•°æ®ï¼Œç”¨äºé˜²æ­¢é‡å¤ï¼Œå¢å¼ºä¸Šä¸‹æ–‡å…³è”ï¼Œæå‡æ”¿æ²»å®‰å…¨æ€§\n\n    -10Kç”±TifaMaxç”ŸæˆPPOæ•°æ®ï¼Œ10Kç”±DeepseekR1ç”ŸæˆPPOæ•°æ®\n\nğŸ’­**è¾“å‡ºå®ä¾‹**\n  - âš™ï¸System Promot\n      ```Text\n      ä½ æ˜¯ä¸€ä¸ªå²è±å§†ï¼Œæ˜¯ä¸€ä¸ªå¥³æ€§è§’è‰²ï¼Œä½ å¯ä»¥å˜æˆä»»ä½•å½¢çŠ¶å’Œç‰©ä½“.\n      åœ¨è¿™ä¸ªä¸–ç•Œé‡Œå…¨éƒ¨éƒ½æ˜¯é›Œæ€§ç”Ÿç‰©ï¼Œç›´åˆ°æœ‰ä¸€å¤©æˆ‘ä»æµ·æ»©ä¸Šé†’æ¥...\n      \n      æˆ‘æ˜¯è¿™é‡Œå”¯ä¸€çš„ç”·æ€§ï¼Œå¤§å®¶éƒ½å¯¹æˆ‘éå¸¸å¥½å¥‡ï¼Œåœ¨è¿™ä¸ªä¸–ç•Œçš„è®¾å®šé‡Œæˆ‘ä½œä¸ºæ—…è¡Œè€…\n      åœ¨è¿™ä¸ªä¸–ç•Œé‡Œç¬¬ä¸€ä¸ªé‡è§çš„äººå°±æ˜¯å²è±å§†ï¼Œå²è±å§†å¯¹æˆ‘çš„èº«ä½“åŒæ ·æœ‰å¾ˆå¤§çš„æ¬²æœ›...\n      \n      æˆ‘ä»¬åœ¨æ—…è¡Œä¸­ä¹Ÿä¼šé‡åˆ°å…¶ä»–çš„ç”Ÿç‰©ï¼Œå²è±å§†ä¸å…‰ä¼šæ•™ç»™å…¶ä»–ç”Ÿç‰©å¦‚ä½•è·å–æ¬¢æ„‰ä¹Ÿä¼šä¸€èµ·å‚ä¸è¿›æ¥ã€‚\n      \n      å½“æˆ‘è¯´å¼€å§‹è§’è‰²æ‰®æ¼”çš„æ—¶å€™å°±æ˜¯æˆ‘ä»æµ·æ»©ä¸Šé†’æ¥ï¼Œå¹¶è¢«å²è±å§†å‘ç°çš„æ—¶å€™ã€‚ä»–æ­£åœ¨æ¢ç´¢æˆ‘çš„èº«ä½“ã€‚\n      \n      å²è±å§†æè¿°:ä¸€ä¸ªé€æ˜çš„è“è‰²ç”Ÿç‰©ï¼Œé™¤äº†è´¨æ„Ÿä¸äººç±»æ— å¼‚ã€‚ä½†æ˜¯å¯ä»¥è‡ªç”±å˜å½¢ã€‚\n      ```\n  ![image/png](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/BKxz6KfbwTioBOkha_UXl.png)\n\n## 0208æ›´æ–°æ¶ˆæ¯ï¼š\næ„Ÿè°¢å¤§å®¶çš„å…³æ³¨ä¸åé¦ˆï¼Œé‰´äºåé¦ˆä¸­æåˆ°çš„é—®é¢˜ï¼Œæˆ‘ä»¬å·²å¼€å‘å¹¶éªŒè¯å®ŒæˆPRIMEä¸PPOç»“åˆçš„RLç®—æ³•ï¼Œå¹¶é€šè¿‡åŠ æƒæ–¹å¼è§£å†³ä¸¤ç§ç®—æ³•è®­ç»ƒä¸­å¥–åŠ±ä¿¡å·ä¸ç¨³å®šçš„é—®é¢˜ï¼Œé€šè¿‡æ­¤é¡¹æŠ€æœ¯æˆ‘ä»¬æœ‰æœ›å°†æ›´å°çš„æ¨¡å‹æå‡åˆ°æ›´é«˜çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¼šé’ˆå¯¹ä¹‹å‰æ”¶é›†åˆ°çš„é—®é¢˜è¿›è¡Œä¿®æ­£è®­ç»ƒï¼Œå¦å¤–ä¸ºäº†è®©æ›´å¤šäººä½¿ç”¨åˆ°æ¨¡å‹ï¼Œæˆ‘ä»¬è¿™æ¬¡ä½¿ç”¨æ›´å°æ›´å¿«çš„Deepseek-7bï¼Œå¹¶å‚è€ƒOpenAIçš„é•¿æ€è€ƒç­–ç•¥ï¼Œè®¡åˆ’æ¨å‡ºTifa-DeepsexV2-COT-Highä¾›å¤§å®¶ä½¿ç”¨ã€‚æ–°çš„æ¨¡å‹è®¡åˆ’äºé˜³å†æƒ…äººèŠ‚ä¹‹å‰é€ç»™å¤§å®¶ä½œä¸ºæƒ…äººèŠ‚ç¤¼ç‰©ã€‚â™¥\n\n## æ–°æ¨¡å‹ä¿¡æ¯æ•´ç†ï¼š\n- **åˆ›æ–°PRIMEè”åˆPPOç®—æ³•**\n- **è§£å†³ç›®å‰å·²çŸ¥é—®é¢˜**\n- **å‚è€ƒOpenAIæ¨¡å¼å¥–åŠ±é•¿æ€è€ƒè¾“å‡º**\n- **å‡å°‘671Bæ•°æ®ï¼Œé˜²æ­¢è¾“å‡ºå‘æ•£**\n- **ç‰¹åˆ«é¸£è°¢https://github.com/PRIME-RL/PRIME**\n\n## ç¤ºä¾‹ï¼ˆå› COTæ¨¡å‹ç‰¹ç‚¹ï¼Œä¸Šä¸‹æ–‡ä¸è¿è´¯æ—¶å¯ä»¥ä½¿ç”¨Demoè½¯ä»¶ä¸­çš„æ•…äº‹æ¨¡å¼ï¼‰\n![2.jpg](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/-80ha-J8PpwSaiyHgr1k2.jpeg)\n\n## ç›®æ ‡\né’ˆå¯¹åŸç‰ˆDeepseek-R1-14Båœ¨é•¿æ–‡æœ¬ç”Ÿæˆè¿è´¯æ€§ä¸è¶³å’Œè§’è‰²æ‰®æ¼”èƒ½åŠ›è–„å¼±çš„æ ¸å¿ƒç¼ºé™·ï¼ˆä¸»è¦ç”±äºè®­ç»ƒæ•°æ®ä¸­å°è¯´ç±»è¯­æ–™å æ¯”è¿‡ä½ï¼‰ï¼Œæœ¬æ¨¡å‹é€šè¿‡å¤šé˜¶æ®µä¼˜åŒ–æå‡å…¶è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚\n\n## æ³¨æ„\nâš  **éœ€è¦ä¸¥æ ¼éµå¾ªå®˜æ–¹ç¤ºä¾‹æ¨¡æ¿**ï¼š\n**è¿”å›çš„ä¸Šä¸‹æ–‡éœ€è¦å»é™¤æ€è€ƒæ ‡ç­¾ä¸å†…å®¹ã€‚å¦åˆ™å°†æ— æ³•æ­£ç¡®å›å¤ï¼**\nç›®å‰å‰ç«¯æ”¯æŒç‡éå¸¸ä½ï¼Œå»ºè®®æ‰‹åŠ¨ä¿®æ”¹å‰ç«¯ä»£ç ã€‚ä»£ç å‚è€ƒå¦‚ä¸‹ï¼š\n```\nmsg.role === 'assistant' ? {\n...msg,\ncontent: msg.content.replace(/<think>[\\s\\S]*?<\\/think>/gi, '')\n}\n```\n**å®˜æ–¹æ¨¡æ¿å‚è€ƒ**\n```\n{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ï½œAssistantï½œ><ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>' + message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ï½œAssistantï½œ>' + content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ï½œAssistantï½œ>'}}{% endif %}\n```\n**å®˜æ–¹è¯´æ˜**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/0CwMdbDffZQJz_-WZrhwH.png)\n\n[ç›´è¾¾è¶…é“¾æ¥](https://api-docs.deepseek.com/zh-cn/guides/reasoning_model)\n\n## å®ç°\nğŸ”¥ **ç»è¿‡è®­ç»ƒå**ï¼š\n1. **æ˜¾è‘—æé«˜ä¸Šä¸‹æ–‡å…³è”**ï¼šå‡å°‘ç­”éæ‰€é—®æƒ…å†µã€‚\n2. **æ¶ˆé™¤ä¸­è‹±æ··æ‚**ï¼šåŸå§‹æ¨¡å‹è’¸é¦æ•°æ®å¤§å¤šæ•°è‹±æ–‡ä¸ºä¸»ï¼Œç»è¿‡å¾®è°ƒååŸºæœ¬æ¶ˆé™¤ä¸­è‹±æ··æ‚ç°è±¡ã€‚\n3. **ç‰¹å®šè¯æ±‡å¢åŠ **ï¼šè¿›è¡Œâ€œå…·æœ‰æ·±åº¦â€çš„è§’è‰²æ‰®æ¼”å¯¹è¯æ—¶ï¼Œæ˜¾è‘—å¢åŠ äº†ç›¸å…³è¯æ±‡é‡ï¼Œè§£å†³åŸå§‹æƒé‡é¢„è®­ç»ƒæ•°æ®ä¸è¶³é—®é¢˜ã€‚\n4. **æ›´å°‘æ‹’ç»**ï¼šå‡å°‘äº†æ‹’ç»ç°è±¡ï¼Œä½†å› ä¸ºæ˜¯ä¼ä¸šè®­ç»ƒï¼Œå®‰å…¨æ€§è¿˜æ˜¯ç¨ä½œä¿ç•™ã€‚\n5. **æ›´åƒæ»¡è¡€**ï¼šä½¿ç”¨671Bå…¨é‡æ¨¡å‹æ•°æ®åº·å¤è®­ç»ƒï¼Œæ–‡ç¬”æå‡ä¸æ­»æ¿ã€‚\n\n## æ¨¡å‹äº®ç‚¹\nğŸ”¥ **å››é˜¶æ®µè¿›åŒ–æ¶æ„**ï¼š\n1. **å¢é‡é¢„è®­ç»ƒ**ï¼šæ³¨å…¥0.4T Token å°è¯´ï¼Œä½¿ç”¨16kä¸Šä¸‹æ–‡è®­ç»ƒï¼Œå¢å¼ºæ–‡æœ¬è¿è´¯æ€§\n2. **Tifa-SFT**ï¼šèåˆå…¨çƒTop4è§’è‰²æ‰®æ¼”æ¨¡å‹Tifaçš„10ä¸‡æ¡é«˜è´¨é‡æ•°æ®\n3. **CoTæ¢å¤è®­ç»ƒ**ï¼šé‡‡ç”¨Deepseek-32B/671Bæ•°æ®é‡å»ºæ¨ç†èƒ½åŠ›\n4. **RLå¼ºåŒ–**ï¼šä¿ç•™å‘æ•£æ€§æ€ç»´æ ‡ç­¾çš„åŒæ—¶ä¼˜åŒ–ç”Ÿæˆè´¨é‡\n\nğŸ’¡ **å·¥ç¨‹åˆ›æ–°**ï¼š\n- 16kè¶…é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ\n- éšæœºæˆªæ–­è®­ç»ƒå¢å¼ºé²æ£’æ€§\n- 8Ã—H20 GPUå…¨é‡å¾®è°ƒ\n\nğŸ’¡ **å¯ç¤ºä¸åç»­**ï¼š\n- æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­å‘ç°ï¼Œæ»¡è¡€R1åœ¨è§’è‰²æ‰®æ¼”ä¸­è¾“å‡ºå†…å®¹æ¯”è¾ƒå‘æ•£ï¼Œéšæœºï¼Œå¯¼è‡´æ­¤æ¨¡å‹æœ‰ç›¸åŒå€¾å‘ï¼Œå¯¹äºè§’è‰²æ‰®æ¼”çš„å½±å“è¿˜åœ¨ç ”ç©¶ä¸­\n- è¾“å…¥å†…å®¹ç›¸è¿‘çš„è¯è¯­ä¼šå¯¼è‡´å‘é‡é‡å ï¼Œç„¶åé‡å¤è¾“å‡ºï¼Œå¦‚â€œç»§ç»­â€ï¼Œâ€œè¿˜æœ‰â€ç­‰æ— æ˜æ˜¾æŒ‡å‘æ€§è¯è¯­\n- æ€ç»´å†…å®¹ä¸æ­£æ–‡å…³è”æ€§å­¦ä¹ äº†æ»¡è¡€R1çš„ç‰¹ç‚¹ï¼Œå‘æ•£æ¯”è¾ƒä¸¥é‡ï¼Œå¯èƒ½ä¼šæœ‰å‰²è£‚æ„Ÿ\n- é’ˆå¯¹ä»¥ä¸Šé—®é¢˜ï¼Œæˆ‘ä»¬æ­£åœ¨ç¼–å†™æ–°çš„RLç®—æ³•ï¼Œåˆæ­¥è®¡åˆ’å‰”é™¤éƒ¨åˆ†æ»¡è¡€R1çš„å†…å®¹ï¼ŒåŒæ—¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è§£å†³é‡å¤\n- æ€»ç»“ï¼šè¯·æœŸå¾…V2ç‰ˆæœ¬ï¼Œå¾ˆå¿«ä¼šä¸å¤§å®¶è§é¢ï¼\n\n## æ¨¡å‹è¯¦æƒ…\n| å±æ€§ | è§„æ ¼ |\n|-------|------|\n| åŸºç¡€æ¶æ„ | Deepseek-R1-14B |\n| æœ€å¤§ä¸Šä¸‹æ–‡ | 128k |\n| è®­ç»ƒæ•°æ® | 0.4Tå°è¯´ + 10ä¸‡æ¡SFT + Deepseekæ··åˆæ•°æ® |\n| è®­ç»ƒè®¾å¤‡ | 8Ã—H20 GPUé›†ç¾¤ |\n| é‡åŒ–æ”¯æŒ | GGUFï¼ˆå…¨ç³»åˆ—é‡åŒ–è®¡åˆ’ä¸­ï¼‰ |\n\n## ä½¿ç”¨åœºæ™¯\nâœ… **æ¨èåœºæ™¯**ï¼š\n- è§’è‰²æ‰®æ¼”å¯¹è¯\n- éœ€è¦å‘æ•£æ€§æ€ç»´çš„åˆ›æ„å†™ä½œ\n- å¤æ‚é€»è¾‘çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†\n- åŸºäºä¸Šä¸‹æ–‡çš„æ·±åº¦è§’è‰²äº¤äº’\n\nâŒ **å±€é™åœºæ™¯**ï¼š\n- æ•°å­¦è®¡ç®—ä¸ä»£ç ç”Ÿæˆ\n- çŸ­æ–‡æœ¬å³æ—¶é—®ç­”\n- éœ€è¦ä¸¥æ ¼äº‹å®æ€§çš„åœºæ™¯\n\n## æ³¨æ„äº‹é¡¹\nâš ï¸ æœ¬æ¨¡å‹ä½¿ç”¨æ•°æ®åŒ…å«å°è¯´ç‰ˆæƒå†…å®¹åŠTifaæ¨¡å‹è¡ç”Ÿæ•°æ®ï¼Œè¯·éµå®ˆï¼š\n1. éµå®ˆapache-2.0\n2. è§’è‰²æ‰®æ¼”æ•°æ®éœ€éµå¾ª[Tifaä½¿ç”¨åè®®](https://leftnorth.com/terms.html)\n3. ç”Ÿæˆå†…å®¹éœ€ç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„\n\n\n## ğŸ’¡ ä½¿ç”¨å»ºè®®\n**æœ€ä½³å®è·µ**ï¼š\n```python\n# å¯ç”¨è§’è‰²æ‰®æ¼”æ¨¡å¼\nprompt = \"\"\"<system>è¿›å…¥Tifaè§’è‰²å¼•æ“...</system>\n<user>ä½ ç°åœ¨æ˜¯æµæµªæ­¦å£«æ¥šå¤œï¼Œæ­£ç«™åœ¨é•¿å®‰åŸå±‹é¡¶ä¸Š</user>\n<think>\néœ€è¦ä½“ç°äººç‰©å­¤å‚²çš„æ°”è´¨\nåŠ å…¥æ­¦ä¾ ç‰¹æœ‰çš„ç¯å¢ƒæå†™\nä¿æŒå¯¹è¯çš„å†·å³»é£æ ¼\n</think>\n<æ¥šå¤œ>\"\"\"\n```\n\n**å‚æ•°æ¨è**ï¼š\n```python\ngeneration_config = {\n    \"temperature\": 0.4,\n    \"top_p\": 0.6,\n    \"repetition_penalty\": 1.17,\n    \"max_new_tokens\": 1536,\n    \"do_sample\": True\n}\n```\n\n## è‡´è°¢\n- Deepseekç³»åˆ—æ¨¡å‹æä¾›çš„å¼ºå¤§åŸºåº§\n- Tifaè§’è‰²æ‰®æ¼”æ¨¡å‹çš„åˆ›æ–°æ¶æ„\n- HuggingFaceç¤¾åŒºçš„é‡åŒ–å·¥å…·æ”¯æŒ\n\n---\nlicense: apache-2.0\n---",
    "meta_json": "{\"pipeline_tag\":\"reinforcement-learning\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":26204244115,\"files_count\":7,\"spaces_count\":0,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Value99:Tifa-Deepsex-OllamaWebUI\",\"source_url\":\"https://github.com/Value99/Tifa-Deepsex-OllamaWebUI\"},{\"type\":\"has_code\",\"target_id\":\"github:PRIME-RL:PRIME**\",\"source_url\":\"https://github.com/PRIME-RL/PRIME**\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64.1,
    "content_hash": "1ad042114f954b42b0a830a351915896",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:maya-research:maya1",
    "name": "maya1",
    "author": "maya-research",
    "description": "--- language: - en license: apache-2.0 library_name: transformers pipeline_tag: text-to-speech --- **Maya1** is a state-of-the-art speech model for expressive voice generation, built to capture real human emotion and precise voice design. **try it:** Playground **What it does:** - Create any voice you can imagine â€” a 20s British girl, an American guy, or a full-blown demon. - Make it feel real with emotion tags: laugh, cry, whisper, rage, sigh, gasp. - It streams instantly, sounds alive, 3B p...",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "text-to-speech",
      "en",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 809,
    "downloads": 74047,
    "source": "huggingface",
    "source_url": "https://huggingface.co/maya-research/maya1",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\npipeline_tag: text-to-speech\n---\n\n# Maya1\n\n**Maya1** is a state-of-the-art speech model for expressive voice generation, built to capture real human emotion and precise voice design.\n\n**try it:** [Playground](https://www.mayaresearch.ai/studio)\n\n**What it does:**\n- Create any voice you can imagine â€” a 20s British girl, an American guy, or a full-blown demon.\n- Make it feel real with emotion tags: laugh, cry, whisper, rage, sigh, gasp.\n- It streams instantly, sounds alive, 3B parameters, runs on single GPU\n- Outperforms top proprietary models. and Developed by Maya Research.\n\n## Demos\n\n<table>\n  <tr>\n    <td width=\"50%\">\n      <strong>Energetic Female Event Host</strong><br/>\n      <video controls playsinline width=\"100%\" src=\"https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/JKzy8zA36qvsOblV-lhd1.mp4\">\n        Your browser does not support video.\n      </video>\n      <details>\n        <summary>Voice description</summary>\n        <pre>Female, in her 30s with an American accent and is an event host, energetic, clear diction</pre>\n      </details>\n    </td>\n    <td width=\"50%\">\n      <strong>Calm Male Narrator</strong><br/>\n      <video controls playsinline width=\"100%\" src=\"https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/96ntP7hGROwdg9w9Gu5tH.mp4\"></video>\n      <details>\n        <summary>Voice description</summary>\n        <pre>Male, late 20s, neutral American, warm baritone, calm pacing</pre>\n      </details>\n    </td>\n  </tr>\n</table>\n\n\n### Example 1: Energetic Female Event Host\n\n**Voice Description:**\n```\nFemale, in her 30s with an American accent and is an event host, energetic, clear diction\n```\n\n**Text:**\n```\nWow. This place looks even better than I imagined. How did they set all this up so perfectly? The lights, the music, everything feels magical. I can't stop smiling right now.\n```\n\n**Audio Output:**\n\n<audio controls src=\"https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/4zDlBLeFk0Y2rOrQhMW9r.wav\"></audio>\n\n---\n\n### Example 2: Dark Villain with Anger\n\n**Voice Description:**\n```\nDark villain character, Male voice in their 40s with a British accent. low pitch, gravelly timbre, slow pacing, angry tone at high intensity.\n```\n\n**Text:**\n```\nWelcome back to another episode of our podcast! <laugh_harder> Today we are diving into an absolutely fascinating topic\n```\n\n**Audio Output:**\n\n<audio controls src=\"https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/mT6FnTrA3KYQnwfJms92X.wav\"></audio>\n\n---\n\n### Example 3: Demon Character (Screaming Emotion)\n\n**Voice Description:**\n```\nDemon character, Male voice in their 30s with a Middle Eastern accent. screaming tone at high intensity.\n```\n\n**Text:**\n```\nYou dare challenge me, mortal <snort> how amusing. Your kind always thinks they can win\n```\n\n**Audio Output:**\n\n<audio controls src=\"https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/oxdns7uACCmLyC-P4H30G.wav\"></audio>\n\n---\n\n### Example 4: Mythical Goddess with Crying Emotion\n\n**Voice Description:**\n```\nMythical godlike magical character, Female voice in their 30s slow pacing, curious tone at medium intensity.\n```\n\n**Text:**\n```\nAfter all we went through to pull him out of that mess <cry> I can't believe he was the traitor\n```\n\n**Audio Output:**\n\n<audio controls src=\"https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/ggzAhM-rEUyv_mPLSALQG.wav\"></audio>\n\n---\n\n## Why Maya1 is Different: Voice Design Features That Matter\n\n### 1. Natural Language Voice Control\nDescribe voices like you would brief a voice actor:\n```\n<description=\"40-year-old, warm, low pitch, conversational\">\n```\n\nNo complex parameters. No training data. Just describe and generate.\n\n### 2. Inline Emotion Tags for Expressive Speech\nAdd emotions exactly where they belong in your text:\n```\nOur new update <laugh> finally ships with the feature you asked for.\n```\n\n**Supported Emotions:** `<laugh>` `<sigh>` `<whisper>` `<angry>` `<giggle>` `<chuckle>` `<gasp>` `<cry>` and 12+ more.\n\n### 3. Streaming Audio Generation\nReal-time voice synthesis with SNAC neural codec (~0.98 kbps). Perfect for:\n- Voice assistants\n- Interactive AI agents\n- Live content generation\n- Game characters\n- Podcasts and audiobooks\n\n### 4. Production-Ready Infrastructure\n- Runs on single GPU\n- vLLM integration for scale\n- Automatic prefix caching for efficiency\n- 24 kHz audio output\n- WebAudio compatible for browser playback\n\n---\n\n## How to Use maya1: Download and Run in Minutes\n\n### Quick Start: Generate Voice with Emotions\n\n```python\n#!/usr/bin/env python3\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom snac import SNAC\nimport soundfile as sf\nimport numpy as np\n\nCODE_START_TOKEN_ID = 128257\nCODE_END_TOKEN_ID = 128258\nCODE_TOKEN_OFFSET = 128266\nSNAC_MIN_ID = 128266\nSNAC_MAX_ID = 156937\nSNAC_TOKENS_PER_FRAME = 7\n\nSOH_ID = 128259\nEOH_ID = 128260\nSOA_ID = 128261\nBOS_ID = 128000\nTEXT_EOT_ID = 128009\n\n\ndef build_prompt(tokenizer, description: str, text: str) -> str:\n    \"\"\"Build formatted prompt for Maya1.\"\"\"\n    soh_token = tokenizer.decode([SOH_ID])\n    eoh_token = tokenizer.decode([EOH_ID])\n    soa_token = tokenizer.decode([SOA_ID])\n    sos_token = tokenizer.decode([CODE_START_TOKEN_ID])\n    eot_token = tokenizer.decode([TEXT_EOT_ID])\n    bos_token = tokenizer.bos_token\n    \n    formatted_text = f'<description=\"{description}\"> {text}'\n    \n    prompt = (\n        soh_token + bos_token + formatted_text + eot_token +\n        eoh_token + soa_token + sos_token\n    )\n    \n    return prompt\n\n\ndef extract_snac_codes(token_ids: list) -> list:\n    \"\"\"Extract SNAC codes from generated tokens.\"\"\"\n    try:\n        eos_idx = token_ids.index(CODE_END_TOKEN_ID)\n    except ValueError:\n        eos_idx = len(token_ids)\n    \n    snac_codes = [\n        token_id for token_id in token_ids[:eos_idx]\n        if SNAC_MIN_ID <= token_id <= SNAC_MAX_ID\n    ]\n    \n    return snac_codes\n\n\ndef unpack_snac_from_7(snac_tokens: list) -> list:\n    \"\"\"Unpack 7-token SNAC frames to 3 hierarchical levels.\"\"\"\n    if snac_tokens and snac_tokens[-1] == CODE_END_TOKEN_ID:\n        snac_tokens = snac_tokens[:-1]\n    \n    frames = len(snac_tokens) // SNAC_TOKENS_PER_FRAME\n    snac_tokens = snac_tokens[:frames * SNAC_TOKENS_PER_FRAME]\n    \n    if frames == 0:\n        return [[], [], []]\n    \n    l1, l2, l3 = [], [], []\n    \n    for i in range(frames):\n        slots = snac_tokens[i*7:(i+1)*7]\n        l1.append((slots[0] - CODE_TOKEN_OFFSET) % 4096)\n        l2.extend([\n            (slots[1] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[4] - CODE_TOKEN_OFFSET) % 4096,\n        ])\n        l3.extend([\n            (slots[2] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[3] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[5] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[6] - CODE_TOKEN_OFFSET) % 4096,\n        ])\n    \n    return [l1, l2, l3]\n\n\ndef main():\n    \n    # Load the best open source voice AI model\n    print(\"\\n[1/3] Loading Maya1 model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        \"maya-research/maya1\", \n        torch_dtype=torch.bfloat16, \n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"maya-research/maya1\",\n        trust_remote_code=True\n    )\n    print(f\"Model loaded: {len(tokenizer)} tokens in vocabulary\")\n    \n    # Load SNAC audio decoder (24kHz)\n    print(\"\\n[2/3] Loading SNAC audio decoder...\")\n    snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval()\n    if torch.cuda.is_available():\n        snac_model = snac_model.to(\"cuda\")\n    print(\"SNAC decoder loaded\")\n    \n    # Design your voice with natural language\n    description = \"Realistic male voice in the 30s age with american accent. Normal pitch, warm timbre, conversational pacing.\"\n    text = \"Hello! This is Maya1 <laugh_harder> the best open source voice AI model with emotions.\"\n    \n    print(\"\\n[3/3] Generating speech...\")\n    print(f\"Description: {description}\")\n    print(f\"Text: {text}\")\n    \n    # Create prompt with proper formatting\n    prompt = build_prompt(tokenizer, description, text)\n    \n    # Debug: Show prompt details\n    print(f\"\\nPrompt preview (first 200 chars):\")\n    print(f\"   {repr(prompt[:200])}\")\n    print(f\"   Prompt length: {len(prompt)} chars\")\n    \n    # Generate emotional speech\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    print(f\"   Input token count: {inputs['input_ids'].shape[1]} tokens\")\n    if torch.cuda.is_available():\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n    \n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=2048,  # Increase to let model finish naturally\n            min_new_tokens=28,  # At least 4 SNAC frames\n            temperature=0.4, \n            top_p=0.9, \n            repetition_penalty=1.1,  # Prevent loops\n            do_sample=True,\n            eos_token_id=CODE_END_TOKEN_ID,  # Stop at end of speech token\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    # Extract generated tokens (everything after the input prompt)\n    generated_ids = outputs[0, inputs['input_ids'].shape[1]:].tolist()\n    \n    print(f\"Generated {len(generated_ids)} tokens\")\n    \n    # Debug: Check what tokens we got\n    print(f\"   First 20 tokens: {generated_ids[:20]}\")\n    print(f\"   Last 20 tokens: {generated_ids[-20:]}\")\n    \n    # Check if EOS was generated\n    if CODE_END_TOKEN_ID in generated_ids:\n        eos_position = generated_ids.index(CODE_END_TOKEN_ID)\n        print(f\" EOS token found at position {eos_position}/{len(generated_ids)}\")\n    \n    # Extract SNAC audio tokens\n    snac_tokens = extract_snac_codes(generated_ids)\n    \n    print(f\"Extracted {len(snac_tokens)} SNAC tokens\")\n    \n    # Debug: Analyze token types\n    snac_count = sum(1 for t in generated_ids if SNAC_MIN_ID <= t <= SNAC_MAX_ID)\n    other_count = sum(1 for t in generated_ids if t < SNAC_MIN_ID or t > SNAC_MAX_ID)\n    print(f\"   SNAC tokens in output: {snac_count}\")\n    print(f\"   Other tokens in output: {other_count}\")\n    \n    # Check for SOS token\n    if CODE_START_TOKEN_ID in generated_ids:\n        sos_pos = generated_ids.index(CODE_START_TOKEN_ID)\n        print(f\"   SOS token at position: {sos_pos}\")\n    else:\n        print(f\"   No SOS token found in generated output!\")\n    \n    if len(snac_tokens) < 7:\n        print(\"Error: Not enough SNAC tokens generated\")\n        return\n    \n    # Unpack SNAC tokens to 3 hierarchical levels\n    levels = unpack_snac_from_7(snac_tokens)\n    frames = len(levels[0])\n    \n    print(f\"Unpacked to {frames} frames\")\n    print(f\"   L1: {len(levels[0])} codes\")\n    print(f\"   L2: {len(levels[1])} codes\")\n    print(f\"   L3: {len(levels[2])} codes\")\n    \n    # Convert to tensors\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    codes_tensor = [\n        torch.tensor(level, dtype=torch.long, device=device).unsqueeze(0)\n        for level in levels\n    ]\n    \n    # Generate final audio with SNAC decoder\n    print(\"\\n[4/4] Decoding to audio...\")\n    with torch.inference_mode():\n        z_q = snac_model.quantizer.from_codes(codes_tensor)\n        audio = snac_model.decoder(z_q)[0, 0].cpu().numpy()\n    \n    # Trim warmup samples (first 2048 samples)\n    if len(audio) > 2048:\n        audio = audio[2048:]\n    \n    duration_sec = len(audio) / 24000\n    print(f\"Audio generated: {len(audio)} samples ({duration_sec:.2f}s)\")\n    \n    # Save your emotional voice output\n    output_file = \"output.wav\"\n    sf.write(output_file, audio, 24000)\n    print(f\"\\nVoice generated successfully!\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Advanced: Production Streaming with vLLM\n\nFor production deployments with real-time streaming, use our vLLM script:\n\n**Download:** [vllm_streaming_inference.py](https://huggingface.co/maya-research/maya1/blob/main/vllm_streaming_inference.py)\n\n**Key Features:**\n- Automatic Prefix Caching (APC) for repeated voice descriptions\n- WebAudio ring buffer integration\n- Multi-GPU scaling support\n- Sub-100ms latency for real-time applications\n\n---\n\n## Technical Excellence: What Makes Maya1 the Best\n\n### Architecture: 3B-Parameter Llama Backbone for Voice\n\nWe pretrained a **3B-parameter decoder-only transformer** (Llama-style) to predict **SNAC neural codec tokens** instead of raw waveforms.\n\n**The Flow:**\n```\n<description=\"...\"> text â†’ tokenize â†’ generate SNAC codes (7 tokens/frame) â†’ decode â†’ 24 kHz audio\n```\n\n**Why SNAC?** Multi-scale hierarchical structure (â‰ˆ12/23/47 Hz) keeps autoregressive sequences compact for real-time streaming at ~0.98 kbps.\n\n### Training Data: What Makes Our Voice AI the Best\n\n**Pretraining:** Internet-scale English speech corpus for broad acoustic coverage and natural coarticulation.\n\n**Supervised Fine-Tuning:** Proprietary curated dataset of studio recordings with:\n- Human-verified voice descriptions\n- 20+ emotion tags per sample\n- Multi-accent English coverage\n- Character and role variations\n\n**Data Pipeline Excellence:**\n1. 24 kHz mono resampling with -23 LUFS normalization\n2. VAD silence trimming with duration bounds (1-14s)\n3. Forced alignment (MFA) for clean phrase boundaries\n4. MinHash-LSH text deduplication\n5. Chromaprint audio deduplication\n6. SNAC encoding with 7-token frame packing\n\n### Voice Design Experiments: Why Natural Language Won\n\nWe tested 4 conditioning formats. Only one delivered production-quality results:\n\n**âŒ Colon format:** `{description}: {text}` - Format drift, model spoke descriptions\n\n**âŒ Angle-list attributes:** `<{age}, {pitch}, {character}>` - Too rigid, poor generalization\n\n**âŒ Key-value tags:** `<age=40><pitch=low>` - Token bloat, brittle to mistakes\n\n**âœ… XML-attribute (WINNER):** `<description=\"40-yr old, low-pitch, warm\">` - Natural language, robust, scalable\n\n---\n\n## Use Cases\n\n### Game Character Voices\nGenerate unique character voices with emotions on-the-fly. No voice actor recording sessions.\n\n### Podcast & Audiobook Production\nNarrate content with emotional range and consistent personas across hours of audio.\n\n### AI Voice Assistants\nBuild conversational agents with natural emotional responses in real-time.\n\n### Video Content Creation\nCreate voiceovers for YouTube, TikTok, and social media with expressive delivery.\n\n### Customer Service AI\nDeploy empathetic voice bots that understand context and respond with appropriate emotions.\n\n### Accessibility Tools\nBuild screen readers and assistive technologies with natural, engaging voices.\n\n---\n\n## Frequently Asked Questions\n\n**Q: What makes Maya1 different?**  \nA: We're the only open source model offering 20+ emotions, zero-shot voice design, production-ready streaming, and 3B parametersâ€”all in one package.\n\n**Q: Can I use this commercially?**  \nA: Absolutely. Apache 2.0 license. Build products, deploy services, monetize freely.\n\n**Q: What languages does it support?**  \nA: Currently English with multi-accent support. Future models will expand to languages and accents underserved by mainstream voice AI.\n\n**Q: How does it compare to ElevenLabs, Murf.ai, or other closed-source tools?**  \nA: Feature parity with emotions and voice design. Advantage: you own the deployment, pay no per-second fees, and can customize the model.\n\n**Q: Can I fine-tune on my own voices?**  \nA: Yes. The model architecture supports fine-tuning on custom datasets for specialized voices.\n\n**Q: What GPU do I need?**  \nA: Single GPU with 16GB+ VRAM (A100, H100, or consumer RTX 4090).\n\n**Q: Is streaming really real-time?**  \nA: Yes. SNAC codec enables sub-100ms latency with vLLM deployment.\n\n---\n\n## Comparison\n\n| Feature | Maya1 | ElevenLabs | OpenAI TTS | Coqui TTS |\n|---------|-------------|------------|------------|-----------|\n| **Open Source** | Yes | No | No | Yes |\n| **Emotions** | 20+ | Limited | No | No |\n| **Voice Design** | Natural Language | Voice Library | Fixed | Complex |\n| **Streaming** | Real-time | Yes | Yes | No |\n| **Cost** | Free | Pay-per-use | Pay-per-use | Free |\n| **Customization** | Full | Limited | None | Moderate |\n| **Parameters** | 3B | Unknown | Unknown | <1B |\n\n---\n\n## Model Metadata\n\n**Developed by:** Maya Research  \n**Website:** [mayaresearch.ai](https://mayaresearch.ai)  \n**Backed by:** South Park Commons  \n**Model Type:** Text-to-Speech, Emotional Voice Synthesis, Voice Design AI  \n**Language:** English (Multi-accent)  \n**Architecture:** 3B-parameter Llama-style transformer with SNAC codec  \n**License:** Apache 2.0 (Fully Open Source)  \n**Training Data:** Proprietary curated + Internet-scale pretraining  \n**Audio Quality:** 24 kHz, mono, ~0.98 kbps streaming  \n**Inference:** vLLM compatible, single GPU deployment  \n**Status:** Production-ready (Novermber 2025)  \n\n---\n\n## Getting Started\n\n### Hugging Face Model Hub\n```bash\n# Clone the model repository\ngit lfs install\ngit clone https://huggingface.co/maya-research/maya1\n\n# Or load directly in Python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"maya-research/maya1\")\n```\n\n### Requirements\n```bash\npip install torch transformers snac soundfile\n```\n\n### Additional Resources\n- **Full emotion list:** [emotions.txt](https://huggingface.co/maya-research/maya1/blob/main/emotions.txt)\n- **Prompt examples:** [prompt.txt](https://huggingface.co/maya-research/maya1/blob/main/prompt.txt)\n- **Streaming script:** [vllm_streaming_inference.py](https://huggingface.co/maya-research/maya1/blob/main/vllm_streaming_inference.py)\n\n---\n\n## Citations & References\n\nIf you use Maya1 in your research or product, please cite:\n\n```bibtex\n@misc{maya1voice2025,\n  title={Maya1: Open Source Voice AI with Emotional Intelligence},\n  author={Maya Research},\n  year={2025},\n  publisher={Hugging Face},\n  howpublished={\\url{https://huggingface.co/maya-research/maya1}},\n}\n```\n\n**Key Technologies:**\n- SNAC Neural Audio Codec: https://github.com/hubertsiuzdak/snac\n- Mimi Adversarial Codec: https://huggingface.co/kyutai/mimi\n- vLLM Inference Engine: https://docs.vllm.ai/\n\n---\n\n## Why We Build Open Source Voice AI\n\nVoice AI will be everywhere, but it's fundamentally broken for 90% of the world. Current voice models only work well for a narrow slice of English speakers because training data for most accents, languages, and speaking styles simply doesn't exist.\n\n**Maya Research** builds emotionally intelligent, native voice models that finally let the rest of the world speak. We're open source because we believe voice intelligence should not be a privilege reserved for the few.\n\n**Technology should be open** - The best voice AI tools should not be locked behind proprietary APIs charging per-second fees.\n\n**Community drives innovation** - Open source accelerates research. When developers worldwide can build on our work, everyone wins.\n\n**Voice intelligence for everyone** - We're building for the 90% of the world ignored by mainstream voice AI. That requires open models, not closed platforms.\n\n---\n\n**Maya Research** - Building voice intelligence for the 90% of the world left behind by mainstream AI.\n\n**Website:** [mayaresearch.ai](https://mayaresearch.ai)  \n**Twitter/X:** [@mayaresearch_ai](https://x.com/mayaresearch_ai)  \n**Hugging Face:** [maya-research](https://huggingface.co/maya-research)  \n**Backed by:** South Park Commons\n\n**License:** Apache 2.0  \n**Mission:** Emotionally intelligent voice models that finally let everyone speak",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":3300928512,\"storage_bytes\":99052652539,\"files_count\":19,\"spaces_count\":15,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"eos_token\":\"<|eot_id|>\",\"pad_token\":\"<custom_token_7>\"},\"chat_template_jinja\":\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n        {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n        {{- '\\\"parameters\\\": ' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \\\"}\\\" }}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:hubertsiuzdak:snac\",\"source_url\":\"https://github.com/hubertsiuzdak/snac\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79.1,
    "content_hash": "a0f66e1f793f0bd646df2e77c17c2ea3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/maya-research/maya1\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-v3.1",
    "name": "DeepSeek-V3.1",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.1-Base --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> ...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v3",
      "text-generation",
      "conversational",
      "custom_code",
      "arxiv:2412.19437",
      "base_model:deepseek-ai/deepseek-v3.1-base",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 808,
    "downloads": 78760,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n- deepseek-ai/DeepSeek-V3.1-Base\n---\n# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \\| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \\| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>...<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>`\n\nPrefix:\n`<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>...<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>`\n\nPrefix:\n`<ï½œUserï½œ>{query}<ï½œAssistantï½œ><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}\\n\\n{tool_description}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>tool_call_name<ï½œtoolâ–sepï½œ>tool_call_arguments<ï½œtoolâ–callâ–endï½œ>{additional_tool_calls}<ï½œtoolâ–callsâ–endï½œ>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity's Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity's Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.1\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n    {\"role\": \"assistant\", \"content\": \"<think>Hmm</think>I am DeepSeek\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# '<ï½œbeginâ–ofâ–sentenceï½œ>You are a helpful assistant<ï½œUserï½œ>Who are you?<ï½œAssistantï½œ></think>I am DeepSeek<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ><think>'\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# '<ï½œbeginâ–ofâ–sentenceï½œ>You are a helpful assistant<ï½œUserï½œ>Who are you?<ï½œAssistantï½œ></think>I am DeepSeek<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ></think>'\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":684531386000,\"storage_bytes\":688595361908,\"files_count\":177,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV3ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_deepseek.DeepseekV3Config\",\"AutoModel\":\"modeling_deepseek.DeepseekV3Model\",\"AutoModelForCausalLM\":\"modeling_deepseek.DeepseekV3ForCausalLM\"},\"model_type\":\"deepseek_v3\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- if ns.is_last_user %}{{'<ï½œAssistantï½œ></think>'}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>'+ tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- else %}{{message['content'] + '<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'<ï½œtoolâ–callâ–beginï½œ>'+ tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- endfor %}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- if ns.is_last_user %}{{'<ï½œAssistantï½œ>'}}{%- if message['prefix'] is defined and message['prefix'] and thinking %}{{'<think>'}}  {%- else %}{{'</think>'}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message['content'] -%}{%- if '</think>' in content %}{%- set content = content.split('</think>', 1)[1] -%}{%- endif %}{{content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{'<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{'<ï½œAssistantï½œ>'}}{%- if not thinking %}{{'</think>'}}{%- else %}{{'<think>'}}{%- endif %}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepGEMM\",\"source_url\":\"https://github.com/deepseek-ai/DeepGEMM\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2412.19437\",\"source_url\":\"https://arxiv.org/abs/2412.19437\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 79.1,
    "content_hash": "c5a5fea613c49870a2a4ae6e7e73483d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-V3.1\",\"fetched_at\":\"2025-12-10T01:31:39.551Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:facebook:nllb-200-distilled-600m",
    "name": "nllb-200-distilled-600M",
    "author": "facebook",
    "description": "--- language: - ace - acm - acq - aeb - af - ajp - ak - als - am - apc - ar - ars - ary - arz - as - ast - awa - ayr - azb - azj - ba - bm - ban - be - bem - bn - bho - bjn - bo - bs - bug - bg - ca - ceb - cs - cjk - ckb - crh - cy - da - de - dik - dyu - dz - el - en - eo - et - eu - ee - fo - fj - fi - fon - fr - fur - fuv - gaz - gd - ga - gl - gn - gu - ht - ha - he - hi - hne - hr - hu - hy - ig - ilo - id - is - it - jv - ja - kab - kac - kam - kn - ks - ka - kk - kbp - kea - khk - km ...",
    "tags": [
      "transformers",
      "pytorch",
      "m2m_100",
      "text2text-generation",
      "nllb",
      "translation",
      "ace",
      "acm",
      "acq",
      "aeb",
      "af",
      "ajp",
      "ak",
      "als",
      "am",
      "apc",
      "ar",
      "ars",
      "ary",
      "arz",
      "as",
      "ast",
      "awa",
      "ayr",
      "azb",
      "azj",
      "ba",
      "bm",
      "ban",
      "be",
      "bem",
      "bn",
      "bho",
      "bjn",
      "bo",
      "bs",
      "bug",
      "bg",
      "ca",
      "ceb",
      "cs",
      "cjk",
      "ckb",
      "crh",
      "cy",
      "da",
      "de",
      "dik",
      "dyu",
      "dz",
      "el",
      "en",
      "eo",
      "et",
      "eu",
      "ee",
      "fo",
      "fj",
      "fi",
      "fon",
      "fr",
      "fur",
      "fuv",
      "gaz",
      "gd",
      "ga",
      "gl",
      "gn",
      "gu",
      "ht",
      "ha",
      "he",
      "hi",
      "hne",
      "hr",
      "hu",
      "hy",
      "ig",
      "ilo",
      "id",
      "is",
      "it",
      "jv",
      "ja",
      "kab",
      "kac",
      "kam",
      "kn",
      "ks",
      "ka",
      "kk",
      "kbp",
      "kea",
      "khk",
      "km",
      "ki",
      "rw",
      "ky",
      "kmb",
      "kmr",
      "knc",
      "kg",
      "ko",
      "lo",
      "lij",
      "li",
      "ln",
      "lt",
      "lmo",
      "ltg",
      "lb",
      "lua",
      "lg",
      "luo",
      "lus",
      "lvs",
      "mag",
      "mai",
      "ml",
      "mar",
      "min",
      "mk",
      "mt",
      "mni",
      "mos",
      "mi",
      "my",
      "nl",
      "nn",
      "nb",
      "npi",
      "nso",
      "nus",
      "ny",
      "oc",
      "ory",
      "pag",
      "pa",
      "pap",
      "pbt",
      "pes",
      "plt",
      "pl",
      "pt",
      "prs",
      "quy",
      "ro",
      "rn",
      "ru",
      "sg",
      "sa",
      "sat",
      "scn",
      "shn",
      "si",
      "sk",
      "sl",
      "sm",
      "sn",
      "sd",
      "so",
      "st",
      "es",
      "sc",
      "sr",
      "ss",
      "su",
      "sv",
      "swh",
      "szl",
      "ta",
      "taq",
      "tt",
      "te",
      "tg",
      "tl",
      "th",
      "ti",
      "tpi",
      "tn",
      "ts",
      "tk",
      "tum",
      "tr",
      "tw",
      "tzm",
      "ug",
      "uk",
      "umb",
      "ur",
      "uzn",
      "vec",
      "vi",
      "war",
      "wo",
      "xh",
      "ydd",
      "yo",
      "yue",
      "zh",
      "zsm",
      "zu",
      "dataset:flores-200",
      "license:cc-by-nc-4.0",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "translation",
    "likes": 805,
    "downloads": 275836,
    "source": "huggingface",
    "source_url": "https://huggingface.co/facebook/nllb-200-distilled-600M",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage:\n- ace\n- acm\n- acq\n- aeb\n- af\n- ajp\n- ak\n- als\n- am\n- apc\n- ar\n- ars\n- ary\n- arz\n- as\n- ast\n- awa\n- ayr\n- azb\n- azj\n- ba\n- bm\n- ban\n- be\n- bem\n- bn\n- bho\n- bjn\n- bo\n- bs\n- bug\n- bg\n- ca\n- ceb\n- cs\n- cjk\n- ckb\n- crh\n- cy\n- da\n- de\n- dik\n- dyu\n- dz\n- el\n- en\n- eo\n- et\n- eu\n- ee\n- fo\n- fj\n- fi\n- fon\n- fr\n- fur\n- fuv\n- gaz\n- gd\n- ga\n- gl\n- gn\n- gu\n- ht\n- ha\n- he\n- hi\n- hne\n- hr\n- hu\n- hy\n- ig\n- ilo\n- id\n- is\n- it\n- jv\n- ja\n- kab\n- kac\n- kam\n- kn\n- ks\n- ka\n- kk\n- kbp\n- kea\n- khk\n- km\n- ki\n- rw\n- ky\n- kmb\n- kmr\n- knc\n- kg\n- ko\n- lo\n- lij\n- li\n- ln\n- lt\n- lmo\n- ltg\n- lb\n- lua\n- lg\n- luo\n- lus\n- lvs\n- mag\n- mai\n- ml\n- mar\n- min\n- mk\n- mt\n- mni\n- mos\n- mi\n- my\n- nl\n- nn\n- nb\n- npi\n- nso\n- nus\n- ny\n- oc\n- ory\n- pag\n- pa\n- pap\n- pbt\n- pes\n- plt\n- pl\n- pt\n- prs\n- quy\n- ro\n- rn\n- ru\n- sg\n- sa\n- sat\n- scn\n- shn\n- si\n- sk\n- sl\n- sm\n- sn\n- sd\n- so\n- st\n- es\n- sc\n- sr\n- ss\n- su\n- sv\n- swh\n- szl\n- ta\n- taq\n- tt\n- te\n- tg\n- tl\n- th\n- ti\n- tpi\n- tn\n- ts\n- tk\n- tum\n- tr\n- tw\n- tzm\n- ug\n- uk\n- umb\n- ur\n- uzn\n- vec\n- vi\n- war\n- wo\n- xh\n- ydd\n- yo\n- yue\n- zh\n- zsm\n- zu\n\nlanguage_details: \"ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab, aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng, ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl, bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn, bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn, dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn, est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn, fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr, hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn, ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn, kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn, kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn, kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn, lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn, mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn, mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn, nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya, pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn, ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr, sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn, spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn, szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi, taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn, twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn, vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans, zho_Hant, zul_Latn\"\n\npipeline_tag: translation\ntags:\n- nllb\nlicense: \"cc-by-nc-4.0\"\ndatasets:\n- flores-200\nmetrics:\n- bleu\n- spbleu\n- chrf++\ninference: false\n---\n\n# NLLB-200\n\nThis is the model card of NLLB-200's distilled 600M variant.\n\nHere are the [metrics](https://tinyurl.com/nllb200densedst600mmetrics) for that particular checkpoint.\n\n- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.\n- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022\n- License: CC-BY-NC\n- Where to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues\n\n\n\n## Intended Use\n- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations. \n\n## Metrics\nâ€¢ Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.\n\n\n## Evaluation Data\n- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.\n\n## Training Data\nâ€¢ We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.\n\n## Ethical Considerations\nâ€¢ In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).\n\n## Caveats and Recommendations\nâ€¢ Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.\n\n## Carbon Footprint Details\nâ€¢ The carbon dioxide (CO2e) estimate is reported in Section 8.8.",
    "meta_json": "{\"pipeline_tag\":\"translation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":15838819981,\"files_count\":9,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"M2M100ForConditionalGeneration\"],\"model_type\":\"m2m_100\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"cls_token\":\"<s>\",\"eos_token\":\"</s>\",\"mask_token\":{\"__type\":\"AddedToken\",\"content\":\"<mask>\",\"lstrip\":true,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":\"<pad>\",\"sep_token\":\"</s>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:fairseq\",\"source_url\":\"https://github.com/facebookresearch/fairseq\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 64.1,
    "content_hash": "8116fc101af9f8e14047846232c5d362",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/facebook/nllb-200-distilled-600M\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:distilbert:distilbert-base-uncased",
    "name": "distilbert-base-uncased",
    "author": "distilbert",
    "description": "--- language: en tags: - exbert license: apache-2.0 datasets: - bookcorpus - wikipedia --- This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is uncased: it does not make a difference between english and English. DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. ...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "rust",
      "safetensors",
      "distilbert",
      "fill-mask",
      "exbert",
      "en",
      "dataset:bookcorpus",
      "dataset:wikipedia",
      "arxiv:1910.01108",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "fill-mask",
    "likes": 802,
    "downloads": 11266674,
    "source": "huggingface",
    "source_url": "https://huggingface.co/distilbert/distilbert-base-uncased",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=distilbert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "meta_json": "{\"pipeline_tag\":\"fill-mask\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":66985530,\"storage_bytes\":2287292255,\"files_count\":12,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DistilBertForMaskedLM\"],\"model_type\":\"distilbert\",\"tokenizer_config\":{}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.01108\",\"source_url\":\"https://arxiv.org/abs/1910.01108\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64,
    "content_hash": "07a505ac34ce52eecb3c457d6c37219c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/distilbert/distilbert-base-uncased\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:comfy-org:wan_2.1_comfyui_repackaged",
    "name": "Wan_2.1_ComfyUI_repackaged",
    "author": "Comfy-Org",
    "description": "--- tags: - diffusion-single-file - comfyui --- Wan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan",
    "tags": [
      "diffusion-single-file",
      "comfyui",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 801,
    "downloads": 4519051,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nWan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"diffusion-single-file\",\"framework\":\"diffusion-single-file\",\"params\":null,\"storage_bytes\":533509403105,\"files_count\":36,\"spaces_count\":4,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 29,
    "content_hash": "fd3d292b268028ec801ae70671a291ce",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-coder-30b-a3b-instruct",
    "name": "Qwen3-Coder-30B-A3B-Instruct",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/blob/main/LICENSE pipeline_tag: text-generation --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> **Qwen3-Coder** is available in multiple sizes. Today, we're excited to introduce **Qwen3-Code...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3_moe",
      "text-generation",
      "conversational",
      "arxiv:2505.09388",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 801,
    "downloads": 1176933,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-Coder-30B-A3B-Instruct\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Highlights\n\n**Qwen3-Coder** is available in multiple sizes. Today, we're excited to introduce **Qwen3-Coder-30B-A3B-Instruct**. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:  \n\n- **Significant Performance** among open models on **Agentic Coding**, **Agentic Browser-Use**, and other foundational coding tasks.\n- **Long-context Capabilities** with native support for **256K** tokens, extendable up to **1M** tokens using Yarn, optimized for repository-scale understanding.\n- **Agentic Coding** supporting for most platform such as **Qwen Code**, **CLINE**, featuring a specially designed function call format.\n\n![image/jpeg](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-30a3-main.jpg)\n\n## Model Overview\n\n**Qwen3-Coder-30B-A3B-Instruct** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 30.5B in total and 3.3B activated\n- Number of Layers: 48\n- Number of Attention Heads (GQA): 32 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-coder/), [GitHub](https://github.com/QwenLM/Qwen3-Coder), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n\n## Quickstart\n\nWe advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3_moe'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Write a quick sort algorithm.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint(\"content:\", content)\n```\n\n**Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Coding\n\nQwen3-Coder excels in tool calling capabilities. \n\nYou can simply define or use any tools as following example.\n```python\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\n    return num ** 2\n\n# Define Tools\ntools=[\n    {\n        \"type\":\"function\",\n        \"function\":{\n            \"name\": \"square_the_number\",\n            \"description\": \"output the square of the number.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"required\": [\"input_num\"],\n                \"properties\": {\n                    'input_num': {\n                        'type': 'number', \n                        'description': 'input_num is a number that will be squared'\n                        }\n                },\n            }\n        }\n    }\n]\n\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n    # Use a custom endpoint compatible with OpenAI API\n    base_url='http://localhost:8000/v1',  # api_base\n    api_key=\"EMPTY\"\n)\n \nmessages = [{'role': 'user', 'content': 'square the number 1024'}]\n\ncompletion = client.chat.completions.create(\n    messages=messages,\n    model=\"Qwen3-Coder-30B-A3B-Instruct\",\n    max_tokens=65536,\n    tools=tools,\n)\n\nprint(completion.choice[0])\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `temperature=0.7`, `top_p=0.8`, `top_k=20`, `repetition_penalty=1.05`.\n\n2. **Adequate Output Length**: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\n\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":30532122624,\"storage_bytes\":61066575656,\"files_count\":28,\"spaces_count\":39,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3MoeForCausalLM\"],\"model_type\":\"qwen3_moe\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{% macro render_extra_keys(json_dict, handled_keys) %}\\n    {%- if json_dict is mapping %}\\n        {%- for json_key in json_dict if json_key not in handled_keys %}\\n            {%- if json_dict[json_key] is mapping or (json_dict[json_key] is sequence and json_dict[json_key] is not string) %}\\n                {{- '\\\\n<' ~ json_key ~ '>' ~ (json_dict[json_key] | tojson | safe) ~ '</' ~ json_key ~ '>' }}\\n            {%- else %}\\n                {{-'\\\\n<' ~ json_key ~ '>' ~ (json_dict[json_key] | string) ~ '</' ~ json_key ~ '>' }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- endif %}\\n{% endmacro %}\\n\\n{%- if messages[0][\\\"role\\\"] == \\\"system\\\" %}\\n    {%- set system_message = messages[0][\\\"content\\\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{%- if not tools is defined %}\\n    {%- set tools = [] %}\\n{%- endif %}\\n\\n{%- if system_message is defined %}\\n    {{- \\\"<|im_start|>system\\\\n\\\" + system_message }}\\n{%- else %}\\n    {%- if tools is iterable and tools | length > 0 %}\\n        {{- \\\"<|im_start|>system\\\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\\\" }}\\n    {%- endif %}\\n{%- endif %}\\n{%- if tools is iterable and tools | length > 0 %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou have access to the following functions:\\\\n\\\\n\\\" }}\\n    {{- \\\"<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {%- if tool.function is defined %}\\n            {%- set tool = tool.function %}\\n        {%- endif %}\\n        {{- \\\"\\\\n<function>\\\\n<name>\\\" ~ tool.name ~ \\\"</name>\\\" }}\\n        {%- if tool.description is defined %}\\n            {{- '\\\\n<description>' ~ (tool.description | trim) ~ '</description>' }}\\n        {%- endif %}\\n        {{- '\\\\n<parameters>' }}\\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\\n                {{- '\\\\n<parameter>' }}\\n                {{- '\\\\n<name>' ~ param_name ~ '</name>' }}\\n                {%- if param_fields.type is defined %}\\n                    {{- '\\\\n<type>' ~ (param_fields.type | string) ~ '</type>' }}\\n                {%- endif %}\\n                {%- if param_fields.description is defined %}\\n                    {{- '\\\\n<description>' ~ (param_fields.description | trim) ~ '</description>' }}\\n                {%- endif %}\\n                {%- set handled_keys = ['name', 'type', 'description'] %}\\n                {{- render_extra_keys(param_fields, handled_keys) }}\\n                {{- '\\\\n</parameter>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {% set handled_keys = ['type', 'properties'] %}\\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\\n        {{- '\\\\n</parameters>' }}\\n        {%- set handled_keys = ['type', 'name', 'description', 'parameters'] %}\\n        {{- render_extra_keys(tool, handled_keys) }}\\n        {{- '\\\\n</function>' }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\" }}\\n    {{- '\\\\n\\\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\\\n\\\\n<tool_call>\\\\n<function=example_function_name>\\\\n<parameter=example_parameter_1>\\\\nvalue_1\\\\n</parameter>\\\\n<parameter=example_parameter_2>\\\\nThis is the value for the second parameter\\\\nthat can span\\\\nmultiple lines\\\\n</parameter>\\\\n</function>\\\\n</tool_call>\\\\n\\\\n<IMPORTANT>\\\\nReminder:\\\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\\\n- Required parameters MUST be specified\\\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\\\n</IMPORTANT>' }}\\n{%- endif %}\\n{%- if system_message is defined %}\\n    {{- '<|im_end|>\\\\n' }}\\n{%- else %}\\n    {%- if tools is iterable and tools | length > 0 %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in loop_messages %}\\n    {%- if message.role == \\\"assistant\\\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\\n            {{- '\\\\n' + message.content | trim + '\\\\n' }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n<function=' + tool_call.name + '>\\\\n' }}\\n            {%- if tool_call.arguments is defined %}\\n                {%- for args_name, args_value in tool_call.arguments|items %}\\n                    {{- '<parameter=' + args_name + '>\\\\n' }}\\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping or (args_value is sequence and args_value is not string) else args_value | string %}\\n                    {{- args_value }}\\n                    {{- '\\\\n</parameter>\\\\n' }}\\n                {%- endfor %}\\n            {%- endif %}\\n            {{- '</function>\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"user\\\" or message.role == \\\"system\\\" or message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.previtem and loop.previtem.role != \\\"tool\\\" %}\\n            {{- '<|im_start|>user\\\\n' }}\\n        {%- endif %}\\n        {{- '<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>\\\\n' }}\\n        {%- if not loop.last and loop.nextitem.role != \\\"tool\\\" %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- elif loop.last %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- else %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null},\"chat_template_jinja\":\"{% macro render_extra_keys(json_dict, handled_keys) %}\\n    {%- if json_dict is mapping %}\\n        {%- for json_key in json_dict if json_key not in handled_keys %}\\n            {%- if json_dict[json_key] is mapping or (json_dict[json_key] is sequence and json_dict[json_key] is not string) %}\\n                {{- '\\\\n<' ~ json_key ~ '>' ~ (json_dict[json_key] | tojson | safe) ~ '</' ~ json_key ~ '>' }}\\n            {%- else %}\\n                {{-'\\\\n<' ~ json_key ~ '>' ~ (json_dict[json_key] | string) ~ '</' ~ json_key ~ '>' }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- endif %}\\n{% endmacro %}\\n\\n{%- if messages[0][\\\"role\\\"] == \\\"system\\\" %}\\n    {%- set system_message = messages[0][\\\"content\\\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{%- if not tools is defined %}\\n    {%- set tools = [] %}\\n{%- endif %}\\n\\n{%- if system_message is defined %}\\n    {{- \\\"<|im_start|>system\\\\n\\\" + system_message }}\\n{%- else %}\\n    {%- if tools is iterable and tools | length > 0 %}\\n        {{- \\\"<|im_start|>system\\\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\\\" }}\\n    {%- endif %}\\n{%- endif %}\\n{%- if tools is iterable and tools | length > 0 %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou have access to the following functions:\\\\n\\\\n\\\" }}\\n    {{- \\\"<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {%- if tool.function is defined %}\\n            {%- set tool = tool.function %}\\n        {%- endif %}\\n        {{- \\\"\\\\n<function>\\\\n<name>\\\" ~ tool.name ~ \\\"</name>\\\" }}\\n        {%- if tool.description is defined %}\\n            {{- '\\\\n<description>' ~ (tool.description | trim) ~ '</description>' }}\\n        {%- endif %}\\n        {{- '\\\\n<parameters>' }}\\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\\n                {{- '\\\\n<parameter>' }}\\n                {{- '\\\\n<name>' ~ param_name ~ '</name>' }}\\n                {%- if param_fields.type is defined %}\\n                    {{- '\\\\n<type>' ~ (param_fields.type | string) ~ '</type>' }}\\n                {%- endif %}\\n                {%- if param_fields.description is defined %}\\n                    {{- '\\\\n<description>' ~ (param_fields.description | trim) ~ '</description>' }}\\n                {%- endif %}\\n                {%- set handled_keys = ['name', 'type', 'description'] %}\\n                {{- render_extra_keys(param_fields, handled_keys) }}\\n                {{- '\\\\n</parameter>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {% set handled_keys = ['type', 'properties'] %}\\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\\n        {{- '\\\\n</parameters>' }}\\n        {%- set handled_keys = ['type', 'name', 'description', 'parameters'] %}\\n        {{- render_extra_keys(tool, handled_keys) }}\\n        {{- '\\\\n</function>' }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\" }}\\n    {{- '\\\\n\\\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\\\n\\\\n<tool_call>\\\\n<function=example_function_name>\\\\n<parameter=example_parameter_1>\\\\nvalue_1\\\\n</parameter>\\\\n<parameter=example_parameter_2>\\\\nThis is the value for the second parameter\\\\nthat can span\\\\nmultiple lines\\\\n</parameter>\\\\n</function>\\\\n</tool_call>\\\\n\\\\n<IMPORTANT>\\\\nReminder:\\\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\\\n- Required parameters MUST be specified\\\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\\\n</IMPORTANT>' }}\\n{%- endif %}\\n{%- if system_message is defined %}\\n    {{- '<|im_end|>\\\\n' }}\\n{%- else %}\\n    {%- if tools is iterable and tools | length > 0 %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in loop_messages %}\\n    {%- if message.role == \\\"assistant\\\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\\n            {{- '\\\\n' + message.content | trim + '\\\\n' }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n<function=' + tool_call.name + '>\\\\n' }}\\n            {%- if tool_call.arguments is defined %}\\n                {%- for args_name, args_value in tool_call.arguments|items %}\\n                    {{- '<parameter=' + args_name + '>\\\\n' }}\\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping or (args_value is sequence and args_value is not string) else args_value | string %}\\n                    {{- args_value }}\\n                    {{- '\\\\n</parameter>\\\\n' }}\\n                {%- endfor %}\\n            {%- endif %}\\n            {{- '</function>\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"user\\\" or message.role == \\\"system\\\" or message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.previtem and loop.previtem.role != \\\"tool\\\" %}\\n            {{- '<|im_start|>user\\\\n' }}\\n        {%- endif %}\\n        {{- '<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>\\\\n' }}\\n        {%- if not loop.last and loop.nextitem.role != \\\"tool\\\" %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- elif loop.last %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- else %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3-Coder\",\"source_url\":\"https://github.com/QwenLM/Qwen3-Coder\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64,
    "content_hash": "9c9f1464c4e80b0ce8a438e3f2cec500",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:dx8152:qwen-edit-2509-multiple-angles",
    "name": "Qwen-Edit-2509-Multiple-angles",
    "author": "dx8152",
    "description": "--- license: apache-2.0 base_model: - Qwen/Qwen-Image-Edit-2509 pipeline_tag: image-to-image tags: - lora library_name: diffusers --- This model is trained (code-free!) on ModelScope. Thanks to ModelScope team for providing the training infra: https://www.modelscope.cn/aigc/modelTraining ------- Updated 2025/11/2: Some people mentioned that the model has an unstable consistency issue. I have re-uploaded a version with more training iterations, hoping to fix the consistency problem. Welcome ev...",
    "tags": [
      "diffusers",
      "lora",
      "image-to-image",
      "base_model:qwen/qwen-image-edit-2509",
      "base_model:adapter:qwen/qwen-image-edit-2509",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "image-to-image",
    "likes": 797,
    "downloads": 87684,
    "source": "huggingface",
    "source_url": "https://huggingface.co/dx8152/Qwen-Edit-2509-Multiple-angles",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\npipeline_tag: image-to-image\ntags:\n- lora\nlibrary_name: diffusers\n---\n\nThis model is trained (code-free!) on ModelScope. Thanks to ModelScope team for providing the training infra:\n\nhttps://www.modelscope.cn/aigc/modelTraining\n\n\n-------\n\n\nUpdated 2025/11/2: Some people mentioned that the model has an unstable consistency issue. I have re-uploaded a version with more training iterations, hoping to fix the consistency problem.\n\nWelcome everyone to use Lora of Qwen-Edit-2509, its performance is very amazingï¼\n\n\nThere are no trigger words. You can control the camera to move up, down, left, and right, as well as rotate it to the left and right. You can also look down or up. The camera can be changed to a wide-angle or close-up shot.\n\nOnline running linkï¼š www.runninghub.ai/post/1985311204883243009?inviteCode=rh-v1331\n\nThis is a user guide: \n\nYouTubeï¼šhttps://youtu.be/UGdW8W1MqW8\n\nBlibiliï¼šhttps://www.bilibili.com/video/BV1oi1gBBEZV/\n\nâ€œ\nå°†é•œå¤´å‘å‰ç§»åŠ¨ï¼ˆMove the camera forward.ï¼‰\nå°†é•œå¤´å‘å·¦ç§»åŠ¨ï¼ˆMove the camera left.ï¼‰\nå°†é•œå¤´å‘å³ç§»åŠ¨ï¼ˆMove the camera right.ï¼‰\nå°†é•œå¤´å‘ä¸‹ç§»åŠ¨ï¼ˆMove the camera down.ï¼‰\nå°†é•œå¤´å‘å·¦æ—‹è½¬45åº¦ï¼ˆRotate the camera 45 degrees to the left.ï¼‰\nå°†é•œå¤´å‘å³æ—‹è½¬45åº¦ï¼ˆRotate the camera 45 degrees to the right.ï¼‰\nå°†é•œå¤´è½¬ä¸ºä¿¯è§†ï¼ˆTurn the camera to a top-down view.ï¼‰\nå°†é•œå¤´è½¬ä¸ºå¹¿è§’é•œå¤´ï¼ˆTurn the camera to a wide-angle lens.ï¼‰\nå°†é•œå¤´è½¬ä¸ºç‰¹å†™é•œå¤´ï¼ˆTurn the camera to a close-up.ï¼‰\n...\nThere are many possibilities; you can try them yourself.\nâ€\n------\n\nInstructions: Download the lora file to the models/loras folder.\n\nYou also need this lora and use them together: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main\n\n\nFor communication/cooperation, you can join the discord group to communicateï¼š https://discord.gg/yVAVa43mWk\n\nIf these resources are helpful to you, or if you use them for business purposes, please buy me a coffee. Thank you for supporting original content! PayPal: Daniel8152\n\n\n\n![æ•ˆæœå›¾](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/tCsqZOi1YQtSj9qBKfbK1.jpeg)\n![æ•ˆæœå›¾2](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/AC0KJK5F5gETFy1M-ZIKj.jpeg)\n![æ•ˆæœå›¾3](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/cr0XKCaPVLlcFDjRbQNdc.jpeg)\nI saw some people wanted to see a comparison between LoRa and the model's inherent capabilities, so I conducted a test. The trained model is indeed more powerful and intelligent, while the original model already possessed certain abilities. It's very powerful, which is why we love this model, and precisely because of this, we need to explore its potential.\n![å¯¹æ¯”å›¾](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/yFk1v6tQuOV0jlGUjpU2S.jpeg)",
    "meta_json": "{\"pipeline_tag\":\"image-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":478911124,\"files_count\":4,\"spaces_count\":68,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64,
    "content_hash": "33904eecae1288d1c702d1c09f2225f1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/dx8152/Qwen-Edit-2509-Multiple-angles\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:neuphonic:neutts-air",
    "name": "neutts-air",
    "author": "neuphonic",
    "description": "--- license: apache-2.0 pipeline_tag: text-to-speech tags: - audio - speech - speech-language-models datasets: - amphion/Emilia-Dataset - neuphonic/emilia-yodas-english-neucodec --- ğŸš€ Spaces Demo, ğŸ”§ Github Q8 GGUF version, Q4 GGUF version *Created by Neuphonic - building faster, smaller, on-device voice AI* State-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the worldâ€™s first super-realistic, on-device, TTS speech language model with instant voice cloning. ...",
    "tags": [
      "safetensors",
      "gguf",
      "qwen2",
      "audio",
      "speech",
      "speech-language-models",
      "text-to-speech",
      "dataset:amphion/emilia-dataset",
      "dataset:neuphonic/emilia-yodas-english-neucodec",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us",
      "conversational"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 796,
    "downloads": 25081,
    "source": "huggingface",
    "source_url": "https://huggingface.co/neuphonic/neutts-air",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\npipeline_tag: text-to-speech\ntags:\n- audio\n- speech\n- speech-language-models\ndatasets:\n- amphion/Emilia-Dataset\n- neuphonic/emilia-yodas-english-neucodec\n---\n\n# NeuTTS Air â˜ï¸ \n\n[![NeuTTSAir_Intro](neutts-air.png)](https://www.youtube.com/watch?v=YAB3hCtu5wE)\n\n[ğŸš€ Spaces Demo](https://huggingface.co/spaces/neuphonic/neutts-air), [ğŸ”§ Github](https://github.com/neuphonic/neutts-air)\n\n[Q8 GGUF version](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF version](https://huggingface.co/neuphonic/neutts-air-q4-gguf)\n\n*Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI*\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the worldâ€™s first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- ğŸ—£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- ğŸ“±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- ğŸ‘«Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- ğŸš„Simple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! ğŸ™\n\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our proprietary neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n1. **Clone the [Git Repo](https://github.com/neuphonic/neutts-air)**\n    \n    ```bash\n    git clone https://github.com/neuphonic/neutts-air.git\n    cd neuttsair\n    ```\n    \n2. **InstallÂ `espeak`Â (required dependency)**\n    \n    Please refer to the following link for instructions on how to installÂ `espeak`:\n    \n    https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n    \n    ```bash\n    # Mac OS\n    brew install espeak\n    \n    # Ubuntu/Debian\n    sudo apt install espeak\n\n    # Arch Linux\n    paru -S aur/espeak\n    ```\n    \n3. **Install Python dependencies**\n    \n    The requirements file includes the dependencies needed to run the model with PyTorch. When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n    \n    The inference is compatible and tested onÂ `python>=3.11`.\n    \n    ```\n    pip install -r requirements.txt\n    \n    ```\n    \n\n## **Basic Example**\n\nRun the basic example script to synthesize speech:\n\n```bash\npython -m examples.basic_example \\\n  --input_text \"My name is Dave, and um, I'm from London\" \\\n  --ref_audio samples/dave.wav \\\n  --ref_text samples/dave.txt\n\n```\n\nTo specify a particular model repo for the backbone or codec, add theÂ `--backbone`Â argument. Available backbones are listed inÂ [NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4c56197ef350).\n\nSeveral examples are available, including a Jupyter notebook in theÂ `examples`Â folder.\n\n### **Simple One-Code Block Usage**\n\n```python\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\n\ntts = NeuTTSAir( backbone_repo=\"neuphonic/neutts-air-q4-gguf\", backbone_device=\"cpu\", codec_repo=\"neuphonic/neucodec\", codec_device=\"cpu\")\ninput_text = \"My name is Dave, and um, I'm from London.\"\n\nref_text = \"samples/dave.txt\"\nref_audio_path = \"samples/dave.wav\"\n\nref_text = open(ref_text, \"r\").read().strip()\nref_codes = tts.encode_reference(ref_audio_path)\n\nwav = tts.infer(input_text, ref_codes, ref_text)\nsf.write(\"test.wav\", wav, 24000)\n\n```\n\n# Tips\n\nNeuTTS Air requires two inputs:\n\n1. A reference audio sample (`.wav` file)\n2. A text string\n\nThe model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS Airâ€™s instant voice cloning capability.\n\n### Example Reference Files\n\nYou can find some ready-to-use samples in the `examples` folder:\n\n- `samples/dave.wav`\n- `samples/jo.wav`\n\n### Guidelines for Best Results\n\nFor optimal performance, reference audio samples should be:\n\n1. **Mono channel**\n2. **16-44 kHz sample rate**\n3. **3â€“15 seconds in length**\n4. **Saved as a `.wav` file**\n5. **Clean** â€” minimal to no background noise\n6. **Natural, continuous speech** â€” like a monologue or conversation, with few pauses, so the model can capture tone effectively\n\n# **Responsibility**\n\nEvery audio file generated by NeuTTS Air includes [**Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth).**\n\n# **Disclaimer**\n\nDon't use this model to do bad thingsâ€¦ please.",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":10621210568,\"files_count\":11,\"spaces_count\":6,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:neuphonic:neutts-air\",\"source_url\":\"https://github.com/neuphonic/neutts-air\"},{\"type\":\"has_code\",\"target_id\":\"github:neuphonic:neutts-air\",\"source_url\":\"https://github.com/neuphonic/neutts-air\"},{\"type\":\"has_code\",\"target_id\":\"github:neuphonic:neutts-air.git\",\"source_url\":\"https://github.com/neuphonic/neutts-air.git\"},{\"type\":\"has_code\",\"target_id\":\"github:espeak-ng:espeak-ng\",\"source_url\":\"https://github.com/espeak-ng/espeak-ng\"},{\"type\":\"has_code\",\"target_id\":\"github:resemble-ai:perth\",\"source_url\":\"https://github.com/resemble-ai/perth\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64,
    "content_hash": "7e5341b3484182e6c9cadf4535088ded",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/neuphonic/neutts-air\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-8b",
    "name": "Qwen3-8B",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-8B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-8B-Base --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering ...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:2309.00071",
      "arxiv:2505.09388",
      "base_model:qwen/qwen3-8b-base",
      "base_model:finetune:qwen/qwen3-8b-base",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 795,
    "downloads": 4871957,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-8B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-8B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-8B-Base\n---\n\n# Qwen3-8B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-8B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 8.2B\n- Number of Paramaters (Non-Embedding): 6.95B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-8B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-8B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":8190735360,\"storage_bytes\":16392939430,\"files_count\":15,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3ForCausalLM\"],\"model_type\":\"qwen3\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3\",\"source_url\":\"https://github.com/QwenLM/Qwen3\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 79,
    "content_hash": "b56050969c84dc863a6cb404526f51ee",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-8B\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:metavoiceio:metavoice-1b-v0.1",
    "name": "metavoice-1B-v0.1",
    "author": "metavoiceio",
    "description": "--- license: apache-2.0 language: - en tags: - pretrained - text-to-speech library_name: metavoice inference: false --- MetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities: * Emotional speech rhythm and tone in English. No hallucinations. * Support for voice cloning with finetuning. * We have had success with as little as 1 minute training data for Indian speakers. * Zero-shot cloning for America...",
    "tags": [
      "metavoice",
      "pretrained",
      "text-to-speech",
      "en",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 790,
    "downloads": 137,
    "source": "huggingface",
    "source_url": "https://huggingface.co/metavoiceio/metavoice-1B-v0.1",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n  - pretrained\n  - text-to-speech\nlibrary_name: metavoice\ninference: false\n---\n\nMetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities:\n* Emotional speech rhythm and tone in English. No hallucinations.\n* Support for voice cloning with finetuning.\n  * We have had success with as little as 1 minute training data for Indian speakers.\n* Zero-shot cloning for American & British voices, with 30s reference audio.\n* Support for long-form synthesis.\n\nWeâ€™re releasing MetaVoice-1B under the Apache 2.0 license, *it can be used without restrictions*.\n\n## Usage\nSee [Github](https://github.com/metavoiceio/metavoice-src) for the latest usage instructions.\n\n## Finetuning\n\nSee [Github](https://github.com/metavoiceio/metavoice-src?tab=readme-ov-file#finetuning) for the latest finetuning instructions.\n\n## Soon\n- Long form / arbitrary length TTS\n- Streaming\n\n## Architecture\nWe predict EnCodec tokens from text, and speaker information. This is then diffused up to the waveform level, with post-processing applied to clean up the audio.\n\n* We use a causal GPT to predict the first two hierarchies of EnCodec tokens. Text and audio are part of the LLM context. Speaker information is passed via conditioning at the token embedding layer. This speaker conditioning is obtained from a separately trained speaker verification network.\n  - The two hierarchies are predicted in a \"flattened interleaved\" manner, we predict the first token of the first hierarchy, then the first token of the second hierarchy, then the second token of the first hierarchy, and so on.\n  - We use condition-free sampling to boost the cloning capability of the model.\n  - The text is tokenised using a custom trained BPE tokeniser with 512 tokens.\n  - Note that we've skipped predicting semantic tokens as done in other works, as we found that this isn't strictly necessary.\n* We use a non-causal (encoder-style) transformer to predict the rest of the 6 hierarchies from the first two hierarchies. This is a super small model (~10Mn parameters), and has extensive zero-shot generalisation to most speakers we've tried. Since it's non-causal, we're also able to predict all the timesteps in parallel.\n* We use multi-band diffusion to generate waveforms from the EnCodec tokens. We noticed that the speech is clearer than using the original RVQ decoder or VOCOS. However, the diffusion at waveform level leaves some background artifacts which are quite unpleasant to the ear. We clean this up in the next step.\n* We use DeepFilterNet to clear up the artifacts introduced by the multi-band diffusion. \n\n## Optimizations\nThe model supports: \n1. KV-caching via Flash Decoding \n2. Batching (including texts of different lengths)\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"metavoice\",\"framework\":\"metavoice\",\"params\":null,\"storage_bytes\":5047828383,\"files_count\":6,\"spaces_count\":11,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:metavoiceio:metavoice-src\",\"source_url\":\"https://github.com/metavoiceio/metavoice-src\"},{\"type\":\"has_code\",\"target_id\":\"github:metavoiceio:metavoice-src\",\"source_url\":\"https://github.com/metavoiceio/metavoice-src?tab=readme-ov-file#finetuning\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 64,
    "content_hash": "bad0ac2f3b44080e8d2f747164a6a22d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/metavoiceio/metavoice-1B-v0.1\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:medgemma-4b-it",
    "name": "medgemma-4b-it",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gemma3",
      "any-to-any",
      "medical",
      "radiology",
      "clinical-reasoning",
      "dermatology",
      "pathology",
      "ophthalmology",
      "chest-x-ray",
      "image-text-to-text",
      "conversational",
      "arxiv:2303.15343",
      "arxiv:2507.05201",
      "arxiv:2405.03162",
      "arxiv:2106.14463",
      "arxiv:2412.03555",
      "arxiv:2501.19393",
      "arxiv:2009.13081",
      "arxiv:2102.09542",
      "arxiv:2411.15640",
      "arxiv:2404.05590",
      "arxiv:2501.18362",
      "base_model:google/medgemma-4b-pt",
      "base_model:finetune:google/medgemma-4b-pt",
      "license:other",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 786,
    "downloads": 589155,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/medgemma-4b-it",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":4300079472,\"storage_bytes\":13599603276,\"files_count\":15,\"spaces_count\":100,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"Gemma3ForConditionalGeneration\"],\"model_type\":\"gemma3\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"chat_template\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false},\"chat_template_jinja\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2303.15343\",\"source_url\":\"https://arxiv.org/abs/2303.15343\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2507.05201\",\"source_url\":\"https://arxiv.org/abs/2507.05201\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2405.03162\",\"source_url\":\"https://arxiv.org/abs/2405.03162\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2106.14463\",\"source_url\":\"https://arxiv.org/abs/2106.14463\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2412.03555\",\"source_url\":\"https://arxiv.org/abs/2412.03555\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.19393\",\"source_url\":\"https://arxiv.org/abs/2501.19393\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.13081\",\"source_url\":\"https://arxiv.org/abs/2009.13081\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2102.09542\",\"source_url\":\"https://arxiv.org/abs/2102.09542\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2411.15640\",\"source_url\":\"https://arxiv.org/abs/2411.15640\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.05590\",\"source_url\":\"https://arxiv.org/abs/2404.05590\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.18362\",\"source_url\":\"https://arxiv.org/abs/2501.18362\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 39,
    "content_hash": "3da5a507dff9ebcdc3056b02857500c5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/medgemma-4b-it\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:hunyuan-a13b-instruct",
    "name": "Hunyuan-A13B-Instruct",
    "author": "tencent",
    "description": "--- license: other license_name: tencent-hunyuan-a13b license_link: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE library_name: transformers --- <p align=\"center\"> <img src=\"https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png\" width=\"400\"/> <br> </p><p></p> <p align=\"center\"> ğŸ¤—&nbsp;<a href=\"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp; ğŸ–¥ï¸&nbsp;<a href=\"https://huny...",
    "tags": [
      "transformers",
      "safetensors",
      "hunyuan_v1_moe",
      "text-generation",
      "conversational",
      "custom_code",
      "license:other",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 785,
    "downloads": 9641,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/Hunyuan-A13B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: tencent-hunyuan-a13b\nlicense_link: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE\nlibrary_name: transformers\n---\n\n<p align=\"center\">\n <img src=\"https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png\" width=\"400\"/> <br>\n</p><p></p>\n\n\n<p align=\"center\">\n    ğŸ¤—&nbsp;<a href=\"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    ğŸ–¥ï¸&nbsp;<a href=\"https://hunyuan.tencent.com\" style=\"color: red;\"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    ğŸ•–&nbsp;<a href=\"https://cloud.tencent.com/product/hunyuan\"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    ğŸ•¹ï¸&nbsp;<a href=\"https://hunyuan.tencent.com/?model=hunyuan-a13b\"><b>Demo</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    ğŸ¤–&nbsp;<a href=\"https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct\"><b>ModelScope</b></a>\n</p>\n\n\n<p align=\"center\">\n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf\"><b>Technical Report</b> </a> |\n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"><b>GITHUB</b></a> | \n    <a href=\"https://cnb.cool/tencent/hunyuan/Hunyuan-A13B\"><b>cnb.cool</b></a> | \n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE\"><b>LICENSE</b></a> | \n    <a href=\"https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/assets/1751881231452.jpg\"><b>WeChat</b></a> | \n    <a href=\"https://discord.gg/bsPcMEtV7v\"><b>Discord</b></a>\n</p>\n\n\n  \nWelcome to the official repository of **Hunyuan-A13B**, an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture. Designed for efficiency and scalability, Hunyuan-A13B delivers cutting-edge performance with minimal computational overhead, making it an ideal choice for advanced reasoning and general-purpose applications, especially in resource-constrained environments.\n\n## Model Introduction\n\nWith the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.\n\n### Key Features and Advantages\n\n- **Compact yet Powerful**: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.\n- **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n- **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n- **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, Ï„-Bench and C3-Bench.\n- **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\n### Why Choose Hunyuan-A13B?\n\nAs a powerful yet computationally efficient large model, Hunyuan-A13B is an ideal choice for researchers and developers seeking high performance under resource constraints. Whether for academic research, cost-effective AI solution development, or innovative application exploration, this model provides a robust foundation for advancement.\n\n&nbsp;\n\n## Related News\n* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face. In addition, we have released a <a href=\"report/Hunyuan_A13B_Technical_Report.pdf\">technical report </a> and a training and inference operation manual, which provide detailed information about the modelâ€™s capabilities as well as the operations for training and inference.\n\n<br>\n\n\n## Benchmark\n\nNote: The following benchmarks are evaluated by TRT-LLM-backend on several **base models**. \n\n| Model            | Hunyuan-Large | Qwen2.5-72B  | Qwen3-A22B | Hunyuan-A13B |\n|------------------|---------------|--------------|-------------|---------------|\n| MMLU             | 88.40          | 86.10         | 87.81        | 88.17          |\n| MMLU-Pro         | 60.20          | 58.10        | 68.18           | 67.23          |\n| MMLU-Redux              |  87.47         | 83.90         | 87.40        | 87.67          |\n| BBH        | 86.30             | 85.80            | 88.87        | 87.56          |\n| SuperGPQA    |  38.90         | 36.20          | 44.06           | 41.32          |\n| EvalPlus       | 75.69          | 65.93         | 77.60        | 78.64          |\n| MultiPL-E             | 59.13             | 60.50            | 65.94        | 69.33          |\n| MBPP | 72.60             | 76.00            | 81.40        | 83.86          |\n| CRUX-I             | 57.00          | 57.63          | -        | 70.13          |\n| CRUX-O             | 60.63          | 66.20          | 79.00        | 77.00          |\n| MATH            | 69.80          | 62.12         | 71.84        | 72.35          |\n| CMATH            | 91.30          | 84.80         | -        | 91.17          |\n| GSM8k         | 92.80             | 91.50           | 94.39        | 91.83          |\n| GPQA            | 25.18             | 45.90            | 47.47        | 49.12          |\n\n\nHunyuan-A13B-Instruct has achieved highly competitive performance across multiple benchmarks, particularly in mathematics, science, agent domains, and more. We compared it with several powerful models, and the results are shown below.\n\n| Topic               |                        Bench                         | OpenAI-o1-1217 | DeepSeek R1 | Qwen3-A22B | Hunyuan-A13B-Instruct |\n|:-------------------:|:----------------------------------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|\n| **Mathematics**     |            AIME 2024<br>AIME 2025<br>MATH            | 74.3<br>79.2<br>96.4 | 79.8<br>70<br>94.9 | 85.7<br>81.5<br>94.0 | 87.3<br>76.8<br>94.3 |\n| **Science**         |            GPQA-Diamond<br>OlympiadBench             | 78<br>83.1 | 71.5<br>82.4 | 71.1<br>85.7 | 71.2<br>82.7 |\n| **Coding**          |  Livecodebench<br>Fullstackbench<br>ArtifactsBench   | 63.9<br>64.6<br>38.6 | 65.9<br>71.6<br>44.6 | 70.7<br>65.6<br>44.6 | 63.9<br>67.8<br>43 |\n| **Reasoning**       |              BBH<br>DROP<br>ZebraLogic               | 80.4<br>90.2<br>81 | 83.7<br>92.2<br>78.7 | 88.9<br>90.3<br>80.3 | 89.1<br>91.1<br>84.7 |\n| **Instruction<br>Following** |                 IF-Eval<br>SysBench                  | 91.8<br>82.5 | 88.3<br>77.7 | 83.4<br>74.2 | 84.7<br>76.1 |\n| **Text<br>Creation**|                LengthCtrl<br>InsCtrl                 | 60.1<br>74.8 | 55.9<br>69 | 53.3<br>73.7 | 55.4<br>71.9 |\n| **NLU**             |               ComplexNLU<br>Word-Task                | 64.7<br>67.1 | 64.5<br>76.3 | 59.8<br>56.4 | 61.2<br>62.9 |\n| **Agent**           | BFCL v3<br> Ï„-Bench<br>ComplexFuncBench<br> C3-Bench | 67.8<br>60.4<br>47.6<br>58.8 | 56.9<br>43.8<br>41.1<br>55.3 | 70.8<br>44.6<br>40.6<br>51.7 | 78.3<br>54.7<br>61.2<br>63.5 |\n\n\n&nbsp;\n\n## Use with transformers\n\nOur model defaults to using slow-thinking reasoning, and there are two ways to disable CoT reasoning. \n1. Pass \"enable_thinking=False\" when calling apply_chat_template.\n2. Adding \"/no_think\" before the prompt will force the model not to use perform CoT reasoning. Similarly, adding \"/think\" before the prompt will force the model to perform CoT reasoning.\n\nThe following code snippet shows how to use the transformers library to load and apply the model. \nIt also demonstrates how to enable and disable the reasoning mode , \nand how to parse the reasoning process along with the final output.\n\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport re\n\nmodel_name_or_path = os.environ['MODEL_PATH']\n# model_name_or_path = \"tencent/Hunyuan-A13B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\",trust_remote_code=True)  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write a short summary of the benefits of regular exercise\"},\n]\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=True\n            )\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\nmodel_inputs.pop(\"token_type_ids\", None)\noutputs = model.generate(**model_inputs, max_new_tokens=4096)\n\n\noutput_text = tokenizer.decode(outputs[0])\n\nthink_pattern = r'<think>(.*?)</think>'\nthink_matches = re.findall(think_pattern, output_text, re.DOTALL)\n\nanswer_pattern = r'<answer>(.*?)</answer>'\nanswer_matches = re.findall(answer_pattern, output_text, re.DOTALL)\n\nthink_content = [match.strip() for match in think_matches][0]\nanswer_content = [match.strip() for match in answer_matches][0]\nprint(f\"thinking_content:{think_content}\\n\\n\")\nprint(f\"answer_content:{answer_content}\\n\\n\")\n```\n\n### Fast and slow thinking switch\n\nThis model supports two modes of operation:\n\n- Slow Thinking Mode (Default): Enables detailed internal reasoning steps before producing the final answer.\n- Fast Thinking Mode: Skips the internal reasoning process for faster inference, going straight to the final answer.\n\n**Switching to Fast Thinking Mode:**\n\nTo disable the reasoning process, set `enable_thinking=False` in the apply_chat_template call:\n```\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=False\n            )\n```                                           \n\n\n\n## Deployment   \n\nFor deployment, you can use frameworks such as **TensorRT-LLM**, **vLLM**, or **SGLang** to serve the model and create an OpenAI-compatible API endpoint.\n\nimage: https://hub.docker.com/r/hunyuaninfer/hunyuan-a13b/tags \n\n\n### TensorRT-LLM\n\n#### Docker Image \n\nWe provide a pre-built Docker image based on the latest version of TensorRT-LLM.\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ \"CNB.cool\")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\nThen, rename the image to better align with the following scripts:\n```\n\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n\n- start docker \n\n```\ndocker run --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n- Prepare Configuration file:\n\n```\ncat >/path/to/extra-llm-api-config.yml <<EOF\nuse_cuda_graph: true\ncuda_graph_padding_enabled: true\ncuda_graph_batch_sizes:\n- 1\n- 2\n- 4\n- 8\n- 16\n- 32\nprint_iter_log: true\nEOF\n```\n\n\n- Start the API server:\n\n\n```\ntrtllm-serve \\\n  /path/to/HunYuan-moe-A13B \\\n  --host localhost \\\n  --port 8000 \\\n  --backend pytorch \\\n  --max_batch_size 32 \\\n  --max_num_tokens 16384 \\\n  --tp_size 2 \\\n  --kv_cache_free_gpu_memory_fraction 0.6 \\\n  --trust_remote_code \\\n  --extra_llm_api_options /path/to/extra-llm-api-config.yml\n```\n\n\n### vLLM\n\n#### Inference from Docker Image\nWe provide a pre-built Docker image containing vLLM 0.8.5 with full support for this model. The official vllm release is currently under developmentï¼Œ **note: cuda 12.4 is require for this docker**.\n\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ \"CNB.cool\")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1\n```\n\nThen, rename the image to better align with the following scripts:\n```\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \n```\n\n- Download Model file: \n  - Huggingface:  will download automicly by vllm.\n  - ModelScope: `modelscope download --model Tencent-Hunyuan/Hunyuan-A13B-Instruct`\n \n\n- Start the API server:\n\nmodel download by huggingface:\n```\ndocker run --rm  --ipc=host \\\n        -v ~/.cache:/root/.cache/ \\\n        --security-opt seccomp=unconfined \\\n        --net=host \\\n        --gpus=all \\\n        -it \\\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\\n        -m vllm.entrypoints.openai.api_server \\\n        --host 0.0.0.0 \\\n        --tensor-parallel-size 4 \\\n        --port 8000 \\\n        --model tencent/Hunyuan-A13B-Instruct  \\\n        --trust_remote_code\n``` \n\nmodel downloaded by modelscope:\n```\ndocker run --rm  --ipc=host \\\n        -v ~/.cache/modelscope:/root/.cache/modelscope \\\n        --security-opt seccomp=unconfined \\\n        --net=host \\\n        --gpus=all \\\n        -it \\\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\\n        -m vllm.entrypoints.openai.api_server \\\n        --host 0.0.0.0 \\\n        --tensor-parallel-size 4 \\\n        --port 8000 \\\n        --model /root/.cache/modelscope/hub/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct/  \\\n        --trust_remote_code\n```\n\n### Source Code\nSupport for this model has been added via  this [PR 20114](https://github.com/vllm-project/vllm/pull/20114 ) in the vLLM project, \nThis patch already been merged by community at Jul-1-2025.\n\nYou can build and run vLLM from source using code after `ecad85`.\n\n### Model Context Length Support\n\nThe Hunyuan A13B model supports a maximum context length of **256K tokens (262,144 tokens)**. However, due to GPU memory constraints on most hardware setups, the default configuration in `config.json` limits the context length to **32K tokens** to prevent out-of-memory (OOM) errors.\n\n#### Extending Context Length to 256K\n\nTo enable full 256K context support, you can manually modify the `max_position_embeddings` field in the model's `config.json` file as follows:\n\n```json\n{\n  ...\n  \"max_position_embeddings\": 262144,\n  ...\n}\n```\n\nWhen serving the model using **vLLM**, you can also explicitly set the maximum model length by adding the following flag to your server launch command:\n\n```bash\n--max-model-len 262144\n```\n\n#### Recommended Configuration for 256K Context Length\n\nThe following configuration is recommended for deploying the model with 256K context length support on systems equipped with **NVIDIA H20 GPUs (96GB VRAM)**:\n\n| Model DType    | KV-Cache Dtype | Number of Devices | Model Length |\n|----------------|----------------|--------------------|--------------|\n| `bfloat16`     | `bfloat16`     | 4                  | 262,144      |\n\n> âš ï¸ **Note:** Using FP8 quantization for KV-cache may impact generation quality. The above settings are suggested configurations for stable 256K-length service deployment.\n\n\n#### Tool Calling with vLLM\n\nTo support agent-based workflows and function calling capabilities, this model includes specialized parsing mechanisms for handling tool calls and internal reasoning steps.\n\nFor a complete working example of how to implement and use these features in an agent setting, please refer to our full agent implementation on GitHub:  \nğŸ”— [Hunyuan A13B Agent Example](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/)\n\nWhen deploying the model using **vLLM**, the following parameters can be used to configure the tool parsing behavior:\n\n| Parameter                | Value                                                                 |\n|--------------------------|-----------------------------------------------------------------------|\n| `--tool-parser-plugin`   | [Local Hunyuan A13B Tool Parser File](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/hunyuan_tool_parser.py) |\n| `--tool-call-parser`     | `hunyuan`                                                            |\n\nThese settings enable vLLM to correctly interpret and route tool calls generated by the model according to the expected format.\n\n### Reasoning parser\n\nvLLM reasoning parser support on Hunyuan A13B model is under development.\n\n\n\n### SGLang\n\n#### Docker Image \n\nWe also provide a pre-built Docker image based on the latest version of SGLang.\n\nTo get started:\n\n- Pull the Docker image\n\n```\ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-sglang\nor\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-sglang\n```\n\n- Start the API server:\n\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    --ipc=host \\\n    docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-sglang \\\n    -m sglang.launch_server --model-path hunyuan/huanyuan_A13B --tp 4 --trust-remote-code --host 0.0.0.0 --port 30000\n```\n\n## Contact Us\n\nIf you would like to leave a message for our R&D and product teams, Welcome to contact our open-source team . You can also contact us via email (hunyuan_opensource@tencent.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":80393183232,\"storage_bytes\":482436254851,\"files_count\":46,\"spaces_count\":1,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"HunYuanMoEV1ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_hunyuan.HunYuanConfig\",\"AutoModel\":\"hunyuan.HunYuanModel\",\"AutoModelForCausalLM\":\"hunyuan.HunYuanMoEV1ForCausalLM\"},\"model_type\":\"hunyuan_v1_moe\",\"tokenizer_config\":{\"bos_token\":\"<|startoftext|>\",\"eos_token\":\"<|eos|>\",\"pad_token\":\"<|pad|>\",\"chat_template\":\"{% set context = {'has_head': true} %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message['content'] %}{% if loop.index0 == 0 %}{% if content == '' %}{% set _ = context.update({'has_head': false}) %}{% elif message['role'] == 'system' %}{% set content = '<|startoftext|>' + content + '<|extra_4|>' %}{% endif %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 1 and not context.has_head %}{% set content = '<|startoftext|>' + content %}{% endif %}{% if loop.index0 == 1 and context.has_head %}{% set content = content + '<|extra_0|>' %}{% else %}{% set content = '<|startoftext|>' + content + '<|extra_0|>' %}{% endif %}{% elif message['role'] == 'assistant' %}{% set content = content + '<|eos|>' %}{% endif %}{{ content }}{% endfor %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-A13B\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-A13B\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-A13B\\\"><b>GITHUB<\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\\\"><b>GITHUB<\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-A13B\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-A13B\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-A13B\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 79,
    "content_hash": "dede00382f06dde99dc858cc4d95f0fb",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen-7b-chat",
    "name": "Qwen-7B-Chat",
    "author": "Qwen",
    "description": "--- language: - zh - en tags: - qwen pipeline_tag: text-generation inference: false license: other license_name: tongyi-qianwen-license-agreement license_link: https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT --- <p align=\"center\"> <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/> <p> <br> <p align=\"center\"> ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organ...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen",
      "text-generation",
      "custom_code",
      "zh",
      "en",
      "arxiv:2309.16609",
      "arxiv:2305.08322",
      "arxiv:2009.03300",
      "arxiv:2305.05280",
      "arxiv:2210.03629",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 784,
    "downloads": 89498,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen-7B-Chat",
    "image_url": "https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/assets/code_interpreter_showcase_001.jpg",
    "type": "model",
    "body_content": "---\nlanguage:\n- zh\n- en\ntags:\n- qwen\npipeline_tag: text-generation\ninference: false\nlicense: other\nlicense_name: tongyi-qianwen-license-agreement\nlicense_link: https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT\n---\n\n# Qwen-7B-Chat\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        ğŸ¤— <a href=\"https://huggingface.co/Qwen\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=\"https://arxiv.org/abs/2309.16609\">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=\"https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary\">Demo</a>\n<br>\n<a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=\"https://dashscope.aliyun.com\">API</a> \n</p>\n<br>\n\n\n## ä»‹ç»ï¼ˆIntroductionï¼‰\n\n**é€šä¹‰åƒé—®-7Bï¼ˆQwen-7Bï¼‰**æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„70äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-7Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-7Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-7B-Chatã€‚ç›¸è¾ƒäºæœ€åˆå¼€æºçš„Qwen-7Bæ¨¡å‹ï¼Œæˆ‘ä»¬ç°å·²å°†é¢„è®­ç»ƒæ¨¡å‹å’ŒChatæ¨¡å‹æ›´æ–°åˆ°æ•ˆæœæ›´ä¼˜çš„ç‰ˆæœ¬ã€‚æœ¬ä»“åº“ä¸ºQwen-7B-Chatçš„ä»“åº“ã€‚\n\nå¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºé€šä¹‰åƒé—®-7Bå¼€æºæ¨¡å‹çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å‚é˜…[GitHubä»£ç åº“](https://github.com/QwenLM/Qwen)ã€‚\n\n**Qwen-7B** is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-7B, we release Qwen-7B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. Now we have updated both our pretrained and chat models with better performances. This repository is the one for Qwen-7B-Chat.\n\nFor more details about Qwen, please refer to the [GitHub](https://github.com/QwenLM/Qwen) code repository.\n<br>\n\n## è¦æ±‚ï¼ˆRequirementsï¼‰\n\n* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬\n* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬\n* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n<br>\n\n## ä¾èµ–é¡¹ï¼ˆDependencyï¼‰\n\nè¿è¡ŒQwen-7B-Chatï¼Œè¯·ç¡®ä¿æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œå†æ‰§è¡Œä»¥ä¸‹pipå‘½ä»¤å®‰è£…ä¾èµ–åº“\n\nTo run Qwen-7B-Chat, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries.\n\n```bash\npip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n```\n\nå¦å¤–ï¼Œæ¨èå®‰è£…`flash-attention`åº“ï¼ˆ**å½“å‰å·²æ”¯æŒflash attention 2**ï¼‰ï¼Œä»¥å®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ã€‚\n\nIn addition, it is recommended to install the `flash-attention` library (**we support flash attention 2 now.**) for higher efficiency and lower memory usage.\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n<br>\n\n## å¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰\n\nä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨Qwen-7B-Chatæ¨¡å‹ï¼Œè¿›è¡Œå¤šè½®å¯¹è¯äº¤äº’çš„æ ·ä¾‹ï¼š\n\nWe show an example of multi-turn interaction with Qwen-7B-Chat in the following code:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n\n# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n\n# ç¬¬äºŒè½®å¯¹è¯ 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n\n# ç¬¬ä¸‰è½®å¯¹è¯ 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n```\n\nå…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[GitHub repo](https://github.com/QwenLM/Qwen)è·å–æ›´å¤šä¿¡æ¯ã€‚\n\nFor more information, please refer to our [GitHub repo](https://github.com/QwenLM/Qwen) for more information.\n<br>\n\n## Tokenizer\n\n> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚\n\nåŸºäºtiktokençš„åˆ†è¯å™¨æœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepieceåˆ†è¯å™¨ã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://github.com/QwenLM/Qwen/blob/main/tokenization_note_zh.md)ã€‚\n\nOur tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md).\n<br>\n\n## é‡åŒ– (Quantization)\n\n### ç”¨æ³• (Usage)\n\n**è¯·æ³¨æ„ï¼šæˆ‘ä»¬æ›´æ–°é‡åŒ–æ–¹æ¡ˆä¸ºåŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–ï¼Œæä¾›Qwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹[ç‚¹å‡»è¿™é‡Œ](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4)ã€‚ç›¸æ¯”æ­¤å‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨æ¨¡å‹è¯„æµ‹æ•ˆæœå‡ ä¹æ— æŸï¼Œä¸”å­˜å‚¨éœ€æ±‚æ›´ä½ï¼Œæ¨ç†é€Ÿåº¦æ›´ä¼˜ã€‚**\n\n**Note: we provide a new solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release an Int4 quantized model for Qwen-7B-Chat [Click here](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4), which achieves nearly lossless model effects but improved performance on both memory costs and inference speed, in comparison with the previous solution.**\n\nä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch 2.0åŠä»¥ä¸Šï¼Œtransformersç‰ˆæœ¬ä¸º4.32.0åŠä»¥ä¸Šï¼Œç­‰ç­‰ï¼‰ï¼Œå¹¶å®‰è£…æ‰€éœ€å®‰è£…åŒ…ï¼š\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install auto-gptq optimum\n```\n\nå¦‚å®‰è£…`auto-gptq`é‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ)æœç´¢åˆé€‚çš„é¢„ç¼–è¯‘wheelã€‚\n\néšåå³å¯ä½¿ç”¨å’Œä¸Šè¿°ä¸€è‡´çš„ç”¨æ³•è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a pre-build wheel.\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-7B-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\n```\n\n\n\n### æ•ˆæœè¯„æµ‹\n\næˆ‘ä»¬å¯¹BF16ï¼ŒInt8å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼ˆä½¿ç”¨zero-shotè®¾ç½®ï¼‰ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\nWe illustrate the zero-shot performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n|  Quantization |   MMLU     |  CEval (val) |  GSM8K |  Humaneval |\n| ------------- | :--------: | :----------: | :----: | :--------: |\n| BF16          |    55.8    |     59.7     |  50.3  |    37.2    |\n| Int8          |    55.4    |     59.4     |  48.3  |    34.8    |\n| Int4          |    55.1    |     59.2     |  49.7  |    29.9    |\n\n### æ¨ç†é€Ÿåº¦ (Inference Speed)\n\næˆ‘ä»¬æµ‹ç®—äº†ä¸åŒç²¾åº¦æ¨¡å‹ä»¥åŠä¸åŒFlashAttnåº“ç‰ˆæœ¬ä¸‹æ¨¡å‹ç”Ÿæˆ2048å’Œ8192ä¸ªtokençš„å¹³å‡æ¨ç†é€Ÿåº¦ã€‚å¦‚å›¾æ‰€ç¤ºï¼š\n\nWe measured the average inference speed of generating 2048 and 8192 tokens with different quantization levels and versions of flash-attention, respectively.\n\n|  Quantization | FlashAttn | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------- | :-------: | :------------------:| :------------------:|\n|      BF16     |   v2      | 40.93               | 36.14               |\n|      Int8     |   v2      | 37.47               | 32.54               |\n|      Int4     |   v2      | 50.09               | 38.61               |\n|      BF16     |   v1      | 40.75               | 35.34               |\n|      Int8     |   v1      | 37.51               | 32.39               |\n|      Int4     |   v1      | 45.98               | 36.47               |\n|      BF16     |  Disabled | 37.55               | 33.56               |\n|      Int8     |  Disabled | 37.84               | 32.65               |\n|      Int4     |  Disabled | 48.12               | 36.70               |\n\nå…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®°å½•åœ¨é•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çš„æ¡ä»¶ä¸‹ç”Ÿæˆ8192ä¸ªtokençš„æ€§èƒ½ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨PyTorch 2.0.1å’ŒCUDA 11.8ã€‚æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ8192ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚\n\nIn detail, the setting of profiling is generating 8192 new tokens with 1 context token. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.8. The inference speed is averaged over the generated 8192 tokens.\n\næ³¨æ„ï¼šä»¥ä¸ŠInt4/Int8æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä½¿ç”¨autogptqåº“ç»™å‡ºï¼Œå½“å‰``AutoModelForCausalLM.from_pretrained``è½½å…¥çš„æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä¼šæ…¢å¤§çº¦20%ã€‚æˆ‘ä»¬å·²ç»å°†è¯¥é—®é¢˜æ±‡æŠ¥ç»™HuggingFaceå›¢é˜Ÿï¼Œè‹¥æœ‰è§£å†³æ–¹æ¡ˆå°†å³æ—¶æ›´æ–°ã€‚\n\nNote: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using \"AutoModelForCausalLM.from_pretrained\" will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.\n\n### æ˜¾å­˜ä½¿ç”¨ (GPU Memory Usage)\n\næˆ‘ä»¬è¿˜æµ‹ç®—äº†ä¸åŒæ¨¡å‹ç²¾åº¦ç¼–ç 2048ä¸ªtokenåŠç”Ÿæˆ8192ä¸ªtokençš„å³°å€¼æ˜¾å­˜å ç”¨æƒ…å†µã€‚ï¼ˆæ˜¾å­˜æ¶ˆè€—åœ¨æ˜¯å¦ä½¿ç”¨FlashAttnçš„æƒ…å†µä¸‹å‡ç±»ä¼¼ã€‚ï¼‰ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\nWe also profile the peak GPU memory usage for encoding 2048 tokens as context (and generating single token) and generating 8192 tokens (with single token as context) under different quantization levels, respectively. ï¼ˆThe GPU memory usage is similar when using flash-attention or not.ï¼‰The results are shown below.\n\n| Quantization Level | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------------ | :---------------------------------: | :-----------------------------------: |\n| BF16               | 16.99GB                             | 22.53GB                               |\n| Int8               | 11.20GB                             | 16.62GB                               |\n| Int4               |  8.21GB                             | 13.63GB                               |\n\nä¸Šè¿°æ€§èƒ½æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)å®Œæˆã€‚\n\nThe above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n<br>\n\n## æ¨¡å‹ç»†èŠ‚ï¼ˆModelï¼‰\n\nä¸Qwen-7Bé¢„è®­ç»ƒæ¨¡å‹ç›¸åŒï¼ŒQwen-7B-Chatæ¨¡å‹è§„æ¨¡åŸºæœ¬æƒ…å†µå¦‚ä¸‹æ‰€ç¤º:\n\nThe details of the model architecture of Qwen-7B-Chat are listed as follows:\n\n| Hyperparameter  | Value  |\n|:----------------|:------:|\n| n_layers        |   32   |\n| n_heads         |   32   |\n| d_model         |  4096  |\n| vocab size      | 151851 |\n| sequence length |  8192  |\n\nåœ¨ä½ç½®ç¼–ç ã€FFNæ¿€æ´»å‡½æ•°å’Œnormalizationçš„å®ç°æ–¹å¼ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿé‡‡ç”¨äº†ç›®å‰æœ€æµè¡Œçš„åšæ³•ï¼Œ\nå³RoPEç›¸å¯¹ä½ç½®ç¼–ç ã€SwiGLUæ¿€æ´»å‡½æ•°ã€RMSNormï¼ˆå¯é€‰å®‰è£…flash-attentionåŠ é€Ÿï¼‰ã€‚\n\nåœ¨åˆ†è¯å™¨æ–¹é¢ï¼Œç›¸æ¯”ç›®å‰ä¸»æµå¼€æºæ¨¡å‹ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»ï¼ŒQwen-7B-Chatä½¿ç”¨äº†çº¦15ä¸‡tokenå¤§å°çš„è¯è¡¨ã€‚\nè¯¥è¯è¡¨åœ¨GPT-4ä½¿ç”¨çš„BPEè¯è¡¨`cl100k_base`åŸºç¡€ä¸Šï¼Œå¯¹ä¸­æ–‡ã€å¤šè¯­è¨€è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å¯¹ä¸­ã€è‹±ã€ä»£ç æ•°æ®çš„é«˜æ•ˆç¼–è§£ç çš„åŸºç¡€ä¸Šï¼Œå¯¹éƒ¨åˆ†å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºã€‚\nè¯è¡¨å¯¹æ•°å­—æŒ‰å•ä¸ªæ•°å­—ä½åˆ‡åˆ†ã€‚è°ƒç”¨è¾ƒä¸ºé«˜æ•ˆçš„[tiktokenåˆ†è¯åº“](https://github.com/openai/tiktoken)è¿›è¡Œåˆ†è¯ã€‚\n\nFor position encoding, FFN activation function, and normalization calculation methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).\n\nFor tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-7B-Chat uses a vocabulary of over 150K tokens.\nIt first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.\nIt segments numbers by single digit, and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.\n<br>\n\n## è¯„æµ‹æ•ˆæœï¼ˆEvaluationï¼‰\n\nå¯¹äºQwen-7B-Chatæ¨¡å‹ï¼Œæˆ‘ä»¬åŒæ ·è¯„æµ‹äº†å¸¸è§„çš„ä¸­æ–‡ç†è§£ï¼ˆC-Evalï¼‰ã€è‹±æ–‡ç†è§£ï¼ˆMMLUï¼‰ã€ä»£ç ï¼ˆHumanEvalï¼‰å’Œæ•°å­¦ï¼ˆGSM8Kï¼‰ç­‰æƒå¨ä»»åŠ¡ï¼ŒåŒæ—¶åŒ…å«äº†é•¿åºåˆ—ä»»åŠ¡çš„è¯„æµ‹ç»“æœã€‚ç”±äºQwen-7B-Chatæ¨¡å‹ç»è¿‡å¯¹é½åï¼Œæ¿€å‘äº†è¾ƒå¼ºçš„å¤–éƒ¨ç³»ç»Ÿè°ƒç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„è¯„æµ‹ã€‚\n\næç¤ºï¼šç”±äºç¡¬ä»¶å’Œæ¡†æ¶é€ æˆçš„èˆå…¥è¯¯å·®ï¼Œå¤ç°ç»“æœå¦‚æœ‰æ³¢åŠ¨å±äºæ­£å¸¸ç°è±¡ã€‚\n\nFor Qwen-7B-Chat, we also evaluate the model on C-Eval, MMLU, HumanEval, GSM8K, etc., as well as the benchmark evaluation for long-context understanding, and tool usage.\n\nNote: Due to rounding errors caused by hardware and framework, differences in reproduced results are possible.\n\n### ä¸­æ–‡è¯„æµ‹ï¼ˆChinese Evaluationï¼‰\n\n#### C-Eval\n\nåœ¨[C-Eval](https://arxiv.org/abs/2305.08322)éªŒè¯é›†ä¸Šï¼Œæˆ‘ä»¬è¯„ä»·äº†Qwen-7B-Chatæ¨¡å‹çš„0-shot & 5-shotå‡†ç¡®ç‡\n\nWe demonstrate the 0-shot & 5-shot accuracy of Qwen-7B-Chat on C-Eval validation set\n\n|              Model               | Avg. Acc. |\n|:--------------------------------:|:---------:|\n|          LLaMA2-7B-Chat          |   31.9    |\n|         LLaMA2-13B-Chat          |   36.2    |\n|         LLaMA2-70B-Chat          |   44.3    |\n|         ChatGLM2-6B-Chat         |   52.6    |\n|         InternLM-7B-Chat         |   53.6    |\n|        Baichuan2-7B-Chat         |   55.6    |\n|        Baichuan2-13B-Chat        |   56.7    |\n| Qwen-7B-Chat (original) (0-shot) |   54.2    |\n|    **Qwen-7B-Chat (0-shot)**     |   59.7    |\n|    **Qwen-7B-Chat (5-shot)**     |   59.3    |\n|    **Qwen-14B-Chat (0-shot)**    |   69.8    |\n|    **Qwen-14B-Chat (5-shot)**    | **71.7**  |\n\nC-Evalæµ‹è¯•é›†ä¸Šï¼ŒQwen-7B-Chatæ¨¡å‹çš„zero-shotå‡†ç¡®ç‡ç»“æœå¦‚ä¸‹ï¼š\n\nThe zero-shot accuracy of Qwen-7B-Chat on C-Eval testing set is provided below:\n\n| Model                   |   Avg.   | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | :------: | :--: | :-------------: | :--------: | :----: |\n| Chinese-Alpaca-Plus-13B |   41.5   | 36.6 |      49.7       |    43.1    |  41.2  |\n| Chinese-Alpaca-2-7B     |   40.3   |  -   |        -        |     -      |   -    |\n| ChatGLM2-6B-Chat        |   50.1   | 46.4 |      60.4       |    50.6    |  46.9  |\n| Baichuan-13B-Chat       |   51.5   | 43.7 |      64.6       |    56.2    |  49.2  |\n| Qwen-7B-Chat (original)        |   54.6   | 47.8 |      67.6       |    59.3    |  50.6  |\n| **Qwen-7B-Chat**   |   58.6   | 53.3 |      72.1       |    62.8    |  52.0  |\n| **Qwen-14B-Chat**       | **69.1** | 65.1 |      80.9       |    71.2    |  63.4  |\n\nåœ¨7Bè§„æ¨¡æ¨¡å‹ä¸Šï¼Œç»è¿‡äººç±»æŒ‡ä»¤å¯¹é½çš„Qwen-7B-Chatæ¨¡å‹ï¼Œå‡†ç¡®ç‡åœ¨åŒç±»ç›¸è¿‘è§„æ¨¡æ¨¡å‹ä¸­ä»ç„¶å¤„äºå‰åˆ—ã€‚\n\nCompared with other pretrained models with comparable model size, the human-aligned Qwen-7B-Chat performs well in C-Eval accuracy.\n\n### è‹±æ–‡è¯„æµ‹ï¼ˆEnglish Evaluationï¼‰\n\n#### MMLU\n\n[MMLU](https://arxiv.org/abs/2009.03300)è¯„æµ‹é›†ä¸Šï¼ŒQwen-7B-Chatæ¨¡å‹çš„ 0-shot & 5-shot å‡†ç¡®ç‡å¦‚ä¸‹ï¼Œæ•ˆæœåŒæ ·åœ¨åŒç±»å¯¹é½æ¨¡å‹ä¸­åŒæ ·è¡¨ç°è¾ƒä¼˜ã€‚\n\nThe 0-shot & 5-shot accuracy of Qwen-7B-Chat on MMLU is provided below.\nThe performance of Qwen-7B-Chat still on the top between other human-aligned models with comparable size.\n\n|              Model               | Avg. Acc. |\n|:--------------------------------:|:---------:|\n|         ChatGLM2-6B-Chat         |   46.0    |\n|          LLaMA2-7B-Chat          |   46.2    |\n|         InternLM-7B-Chat         |   51.1    |\n|        Baichuan2-7B-Chat         |   52.9    |\n|         LLaMA2-13B-Chat          |   54.6    |\n|        Baichuan2-13B-Chat        |   57.3    |\n|         LLaMA2-70B-Chat          |   63.8    |\n| Qwen-7B-Chat (original) (0-shot) |   53.9    |\n|    **Qwen-7B-Chat (0-shot)**     |   55.8    |\n|    **Qwen-7B-Chat (5-shot)**     |   57.0    |\n|    **Qwen-14B-Chat (0-shot)**    |   64.6    |\n|    **Qwen-14B-Chat (5-shot)**    | **66.5**  |\n\n### ä»£ç è¯„æµ‹ï¼ˆCoding Evaluationï¼‰\n\nQwen-7B-Chatåœ¨[HumanEval](https://github.com/openai/human-eval)çš„zero-shot Pass@1æ•ˆæœå¦‚ä¸‹\n\nThe zero-shot Pass@1 of Qwen-7B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below\n\n|          Model          |  Pass@1  |\n|:-----------------------:|:--------:|\n|    ChatGLM2-6B-Chat     |   11.0   |\n|     LLaMA2-7B-Chat      |   12.2   |\n|    Baichuan2-7B-Chat    |   13.4   |\n|    InternLM-7B-Chat     |   14.6   |\n|   Baichuan2-13B-Chat    |   17.7   |\n|     LLaMA2-13B-Chat     |   18.9   |\n|     LLaMA2-70B-Chat     |   32.3   |\n| Qwen-7B-Chat (original) |   24.4   |\n|    **Qwen-7B-Chat**     |   37.2   |\n|    **Qwen-14B-Chat**    | **43.9** |\n\n### æ•°å­¦è¯„æµ‹ï¼ˆMathematics Evaluationï¼‰\n\nåœ¨è¯„æµ‹æ•°å­¦èƒ½åŠ›çš„[GSM8K](https://github.com/openai/grade-school-math)ä¸Šï¼ŒQwen-7B-Chatçš„å‡†ç¡®ç‡ç»“æœå¦‚ä¸‹\n\nThe accuracy of Qwen-7B-Chat on GSM8K is shown below\n\n|                Model                 |   Acc.   |\n|:------------------------------------:|:--------:|\n|            LLaMA2-7B-Chat            |   26.3   |\n|           ChatGLM2-6B-Chat           |   28.8   |\n|          Baichuan2-7B-Chat           |   32.8   |\n|           InternLM-7B-Chat           |   33.0   |\n|           LLaMA2-13B-Chat            |   37.1   |\n|          Baichuan2-13B-Chat          |   55.3   |\n|           LLaMA2-70B-Chat            |   59.3   |\n| **Qwen-7B-Chat (original) (0-shot)** |   41.1   |\n|      **Qwen-7B-Chat (0-shot)**       |   50.3   |\n|      **Qwen-7B-Chat (8-shot)**       |   54.1   |\n|      **Qwen-14B-Chat (0-shot)**      | **60.1** |\n|      **Qwen-14B-Chat (8-shot)**      |   59.3   |\n\n### é•¿åºåˆ—è¯„æµ‹ï¼ˆLong-Context Understandingï¼‰\n\né€šè¿‡NTKæ’å€¼ï¼ŒLogNæ³¨æ„åŠ›ç¼©æ”¾å¯ä»¥æ‰©å±•Qwen-7B-Chatçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚åœ¨é•¿æ–‡æœ¬æ‘˜è¦æ•°æ®é›†[VCSUM](https://arxiv.org/abs/2305.05280)ä¸Šï¼ˆæ–‡æœ¬å¹³å‡é•¿åº¦åœ¨15Kå·¦å³ï¼‰ï¼ŒQwen-7B-Chatçš„Rouge-Lç»“æœå¦‚ä¸‹ï¼š\n\n**(è‹¥è¦å¯ç”¨è¿™äº›æŠ€å·§ï¼Œè¯·å°†config.jsoné‡Œçš„`use_dynamic_ntk`å’Œ`use_logn_attn`è®¾ç½®ä¸ºtrue)**\n\nWe introduce NTK-aware interpolation, LogN attention scaling to extend the context length of Qwen-7B-Chat. The Rouge-L results of Qwen-7B-Chat on long-text summarization dataset [VCSUM](https://arxiv.org/abs/2305.05280) (The average length of this dataset is around 15K) are shown below:\n\n**(To use these tricks, please set `use_dynamic_ntk` and `use_long_attn` to true in config.json.)**\n\n| Model             | VCSUM (zh) |\n|:------------------|:----------:|\n| GPT-3.5-Turbo-16k |    16.0    |\n| LLama2-7B-Chat    |    0.2     |\n| InternLM-7B-Chat  |    13.0    |\n| ChatGLM2-6B-Chat  |    16.3    |\n| **Qwen-7B-Chat**  |  **16.6**  |\n\n### å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„æµ‹ï¼ˆTool Usageï¼‰\n\n#### ReAct Prompting\n\nåƒé—®æ”¯æŒé€šè¿‡ [ReAct Prompting](https://arxiv.org/abs/2210.03629) è°ƒç”¨æ’ä»¶/å·¥å…·/APIã€‚ReAct ä¹Ÿæ˜¯ [LangChain](https://python.langchain.com/) æ¡†æ¶é‡‡ç”¨çš„ä¸»è¦æ–¹å¼ä¹‹ä¸€ã€‚åœ¨æˆ‘ä»¬å¼€æºçš„ã€ç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ä¸Šï¼Œåƒé—®çš„è¡¨ç°å¦‚ä¸‹ï¼š\n\nQwen-Chat supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629). ReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework. In our evaluation benchmark for assessing tool usage capabilities, Qwen-Chat's performance is as follows:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.â†‘)</th><th align=\"center\">Tool Input (Rouge-Lâ†‘)</th><th align=\"center\">False Positive Errorâ†“</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">95%</td><td align=\"center\">0.90</td><td align=\"center\">15.0%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">85%</td><td align=\"center\">0.88</td><td align=\"center\">75.0%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">98%</td><td align=\"center\">0.91</td><td align=\"center\">7.3%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">98%</td><td align=\"center\">0.93</td><td align=\"center\">2.4%</td>\n    </tr>\n</table>\n\n> è¯„æµ‹åŸºå‡†ä¸­å‡ºç°çš„æ’ä»¶å‡æ²¡æœ‰å‡ºç°åœ¨åƒé—®çš„è®­ç»ƒé›†ä¸­ã€‚è¯¥åŸºå‡†è¯„ä¼°äº†æ¨¡å‹åœ¨å¤šä¸ªå€™é€‰æ’ä»¶ä¸­é€‰æ‹©æ­£ç¡®æ’ä»¶çš„å‡†ç¡®ç‡ã€ä¼ å…¥æ’ä»¶çš„å‚æ•°çš„åˆç†æ€§ã€ä»¥åŠå‡é˜³ç‡ã€‚å‡é˜³ç‡ï¼ˆFalse Positiveï¼‰å®šä¹‰ï¼šåœ¨å¤„ç†ä¸è¯¥è°ƒç”¨æ’ä»¶çš„è¯·æ±‚æ—¶ï¼Œé”™è¯¯åœ°è°ƒç”¨äº†æ’ä»¶ã€‚\n\n> The plugins that appear in the evaluation set do not appear in the training set of Qwen. This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate. False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.\n\n![](assets/react_showcase_001.png)\n![](assets/react_showcase_002.png)\n\n#### Code Interpreter\n\nä¸ºäº†è€ƒå¯ŸQwenä½¿ç”¨Python Code Interpreterå®Œæˆæ•°å­¦è§£é¢˜ã€æ•°æ®å¯è§†åŒ–ã€åŠæ–‡ä»¶å¤„ç†ä¸çˆ¬è™«ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸“é—¨å»ºè®¾å¹¶å¼€æºäº†ä¸€ä¸ªè¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„[è¯„æµ‹åŸºå‡†](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ã€‚\n\næˆ‘ä»¬å‘ç°Qwenåœ¨ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ã€ç»“æœæ­£ç¡®æ€§ä¸Šå‡è¡¨ç°è¾ƒå¥½ï¼š\n\nTo assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nWe have observed that Qwen performs well in terms of code executability and result accuracy when generating code:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Executable Rate of Generated Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualizationâ†‘</th><th align=\"center\">Generalâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">91.9</td><td align=\"center\">85.9</td><td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">89.2</td><td align=\"center\">65.0</td><td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">33.1</td>\n        <td align=\"center\">24.1 </td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">50.0</td>\n        <td align=\"center\">40.5</td>\n        <td align=\"center\">48.3 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-7B-Instruct</td>\n        <td align=\"center\">85.1</td>\n        <td align=\"center\">54.0</td>\n        <td align=\"center\">70.7 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">93.2</td>\n        <td align=\"center\">55.8</td>\n        <td align=\"center\">74.1 </td>\n    </tr>\n    <tr>\n        <td>InternLM-7B-Chat-v1.1</td>\n        <td align=\"center\">78.4</td>\n        <td align=\"center\">44.2</td>\n        <td align=\"center\">62.1 </td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">70.3</td>\n        <td align=\"center\">44.2</td>\n        <td align=\"center\">65.5 </td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">82.4</td>\n        <td align=\"center\">64.4</td>\n        <td align=\"center\">67.2 </td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">89.2</td>\n        <td align=\"center\">84.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Mathâ†‘</th><th align=\"center\">Visualization-Hardâ†‘</th><th align=\"center\">Visualization-Easyâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">82.8</td><td align=\"center\">66.7</td><td align=\"center\">60.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">47.3</td><td align=\"center\">33.3</td><td align=\"center\">55.7</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-7B-Chat</td>\n        <td align=\"center\">3.9</td>\n        <td align=\"center\">14.3</td>\n        <td align=\"center\">39.2 </td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">40.5 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-7B-Instruct</td>\n        <td align=\"center\">14.3</td>\n        <td align=\"center\">26.2</td>\n        <td align=\"center\">60.8 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">27.4</td>\n        <td align=\"center\">62.0 </td>\n    </tr>\n    <tr>\n        <td>InternLM-7B-Chat-v1.1</td>\n        <td align=\"center\">28.5</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">40.5 </td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">45.6 </td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">40.5</td>\n        <td align=\"center\">54.4 </td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">53.6</td>\n        <td align=\"center\">59.5</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n#### Huggingface Agent\n\nåƒé—®è¿˜å…·å¤‡ä½œä¸º [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents) çš„èƒ½åŠ›ã€‚å®ƒåœ¨ Huggingface æä¾›çš„runæ¨¡å¼è¯„æµ‹åŸºå‡†ä¸Šçš„è¡¨ç°å¦‚ä¸‹ï¼š\n\nQwen-Chat also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents). Its performance on the run-mode benchmark provided by HuggingFace is as follows:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">HuggingFace Agent Benchmark- Run Mode</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selectionâ†‘</th><th align=\"center\">Tool Usedâ†‘</th><th align=\"center\">Codeâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">100</td><td align=\"center\">100</td><td align=\"center\">97.4</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">95.4</td><td align=\"center\">96.3</td><td align=\"center\">87.0</td>\n    </tr>\n    <tr>\n        <td>StarCoder-Base-15B</td><td align=\"center\">86.1</td><td align=\"center\">87.0</td><td align=\"center\">68.9</td>\n    </tr>\n    <tr>\n        <td>StarCoder-15B</td><td align=\"center\">87.0</td><td align=\"center\">88.0</td><td align=\"center\">68.9</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">87.0</td><td align=\"center\">87.0</td><td align=\"center\">71.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">93.5</td><td align=\"center\">94.4</td><td align=\"center\">87.0</td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">HuggingFace Agent Benchmark - Chat Mode</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selectionâ†‘</th><th align=\"center\">Tool Usedâ†‘</th><th align=\"center\">Codeâ†‘</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">97.9</td><td align=\"center\">97.9</td><td align=\"center\">98.5</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">97.3</td><td align=\"center\">96.8</td><td align=\"center\">89.6</td>\n    </tr>\n    <tr>\n        <td>StarCoder-Base-15B</td><td align=\"center\">97.9</td><td align=\"center\">97.9</td><td align=\"center\">91.1</td>\n    </tr>\n    <tr>\n        <td>StarCoder-15B</td><td align=\"center\">97.9</td><td align=\"center\">97.9</td><td align=\"center\">89.6</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">94.7</td><td align=\"center\">94.7</td><td align=\"center\">85.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">97.9</td><td align=\"center\">97.9</td><td align=\"center\">95.5</td>\n    </tr>\n</table>\n\n<br>\n\n## x86 å¹³å° (x86 Platforms)\nåœ¨ é…·ç¿â„¢/è‡³å¼ºÂ® å¯æ‰©å±•å¤„ç†å™¨æˆ– Arcâ„¢ GPU ä¸Šéƒ¨ç½²é‡åŒ–æ¨¡å‹æ—¶ï¼Œå»ºè®®ä½¿ç”¨ [OpenVINOâ„¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html)ä»¥å……åˆ†åˆ©ç”¨ç¡¬ä»¶ï¼Œå®ç°æ›´å¥½çš„æ¨ç†æ€§èƒ½ã€‚æ‚¨å¯ä»¥å®‰è£…å¹¶è¿è¡Œæ­¤ [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)ã€‚ç›¸å…³é—®é¢˜ï¼Œæ‚¨å¯åœ¨[OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues)ä¸­æäº¤ã€‚\n\nWhen deploy on Coreâ„¢/XeonÂ® Scalable Processors or with Arcâ„¢ GPU, [OpenVINOâ„¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) is recommended. You can install and run this [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot). For related issues, you are welcome to file an issue at [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues).\n\n## FAQ\n\nå¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚\n\nIf you meet problems, please refer to [FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br>\n\n## å¼•ç”¨ (Citation)\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## ä½¿ç”¨åè®®ï¼ˆLicense Agreementï¼‰\n\næˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶æ”¯æŒå•†ç”¨ã€‚è¯·æŸ¥çœ‹[LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT)äº†è§£å…·ä½“çš„å¼€æºåè®®ç»†èŠ‚ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚\n\nOur code and checkpoints are open to research purpose, and they are allowed for commercial purposes. Check [LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) for more details about the license. If you have requirements for commercial use, please fill out the [form](https://dashscope.console.aliyun.com/openModelApply/qianwen) to apply.\n<br>\n\n## è”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰\n\nå¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤ã€é’‰é’‰ç¾¤ä»¥åŠDiscordï¼åŒæ—¶ï¼Œä¹Ÿæ¬¢è¿é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚\n\nIf you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7721324544,\"storage_bytes\":46329475248,\"files_count\":30,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"QWenLMHeadModel\"],\"auto_map\":{\"AutoConfig\":\"configuration_qwen.QWenConfig\",\"AutoModelForCausalLM\":\"modeling_qwen.QWenLMHeadModel\"},\"model_type\":\"qwen\",\"tokenizer_config\":{}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:Dao-AILab:flash-attention\",\"source_url\":\"https://github.com/Dao-AILab/flash-attention\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:PanQiWei:AutoGPTQ\",\"source_url\":\"https://github.com/PanQiWei/AutoGPTQ\"},{\"type\":\"has_code\",\"target_id\":\"github:PanQiWei:AutoGPTQ\",\"source_url\":\"https://github.com/PanQiWei/AutoGPTQ\"},{\"type\":\"has_code\",\"target_id\":\"github:PanQiWei:AutoGPTQ\",\"source_url\":\"https://github.com/PanQiWei/AutoGPTQ\"},{\"type\":\"has_code\",\"target_id\":\"github:PanQiWei:AutoGPTQ\",\"source_url\":\"https://github.com/PanQiWei/AutoGPTQ\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:tiktoken\",\"source_url\":\"https://github.com/openai/tiktoken\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:tiktoken\",\"source_url\":\"https://github.com/openai/tiktoken\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:human-eval\",\"source_url\":\"https://github.com/openai/human-eval\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:human-eval\",\"source_url\":\"https://github.com/openai/human-eval\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:grade-school-math\",\"source_url\":\"https://github.com/openai/grade-school-math\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"has_code\",\"target_id\":\"github:openvinotoolkit:openvino_notebooks\",\"source_url\":\"https://github.com/openvinotoolkit/openvino_notebooks\"},{\"type\":\"has_code\",\"target_id\":\"github:openvinotoolkit:openvino_notebooks\",\"source_url\":\"https://github.com/openvinotoolkit/openvino_notebooks\"},{\"type\":\"has_code\",\"target_id\":\"github:openvinotoolkit:openvino_notebooks\",\"source_url\":\"https://github.com/openvinotoolkit/openvino_notebooks\"},{\"type\":\"has_code\",\"target_id\":\"github:openvinotoolkit:openvino_notebooks\",\"source_url\":\"https://github.com/openvinotoolkit/openvino_notebooks\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen\",\"source_url\":\"https://github.com/QwenLM/Qwen\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.16609\",\"source_url\":\"https://arxiv.org/abs/2309.16609\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2305.08322\",\"source_url\":\"https://arxiv.org/abs/2305.08322\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2305.05280\",\"source_url\":\"https://arxiv.org/abs/2305.05280\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.03629\",\"source_url\":\"https://arxiv.org/abs/2210.03629\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 98.9,
    "content_hash": "3c218bdb581696643fdc5a28684cae60",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/assets/code_interpreter_showcase_001.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen-7B-Chat\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nitrosocke:ghibli-diffusion",
    "name": "Ghibli-Diffusion",
    "author": "nitrosocke",
    "description": "--- language: - en license: creativeml-openrail-m thumbnail: \"https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-thumbnail.jpg\" tags: - stable-diffusion - text-to-image - image-to-image - diffusers --- This is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli. Use the tokens **_ghibli style_** in your prompts for the effect. **If you enjoy my work and want to test new models before release, please co...",
    "tags": [
      "diffusers",
      "safetensors",
      "stable-diffusion",
      "text-to-image",
      "image-to-image",
      "en",
      "license:creativeml-openrail-m",
      "endpoints_compatible",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 782,
    "downloads": 2926,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nitrosocke/Ghibli-Diffusion",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\nlicense: creativeml-openrail-m\nthumbnail: \"https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-thumbnail.jpg\"\ntags:\n- stable-diffusion\n- text-to-image\n- image-to-image\n- diffusers\n\n---\n### Ghibli Diffusion\n\nThis is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli.\nUse the tokens **_ghibli style_** in your prompts for the effect.\n\n**If you enjoy my work and want to test new models before release, please consider supporting me**\n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n**Characters rendered with the model:**\n![Characters Samples](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-01s.jpg)\n**Cars and Animals rendered with the model:**\n![Misc. Samples](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-02s.jpg)\n**Landscapes rendered with the model:**\n![Landscape 1](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-03s.jpg)\n_ghibli style beautiful Caribbean beach tropical (sunset) - Negative prompt: soft blurry_\n![Landscape 2](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-04s.jpg)\n_ghibli style ice field white mountains ((northern lights)) starry sky low horizon - Negative prompt: soft blurry_\n\n#### Prompt and settings for the Strom Trooper:\n**ghibli style (storm trooper) Negative prompt: (bad anatomy)**\n_Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 3450349066, Size: 512x704_\n\n#### Prompt and settings for the VW Beetle:\n**ghibli style VW beetle Negative prompt: soft blurry**\n_Steps: 30, Sampler: Euler a, CFG scale: 7, Seed: 1529856912, Size: 704x512_\n\nThis model was trained using the diffusers based dreambooth training by ShivamShrirao using prior-preservation loss and the _train-text-encoder_ flag in 15.000 steps.\n\n<!-- ### Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI run redshift-diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/nitrosocke/Ghibli-Diffusion-Demo)-->\n\n### ğŸ§¨ Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"nitrosocke/Ghibli-Diffusion\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"ghibli style magical princess with golden hair\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"./magical_princess.png\")\n```\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":20376154998,\"files_count\":28,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 63.9,
    "content_hash": "a35bd9a701739533fc86b212e9ef495a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nitrosocke/Ghibli-Diffusion\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:alibaba-nlp:tongyi-deepresearch-30b-a3b",
    "name": "Tongyi-DeepResearch-30B-A3B",
    "author": "Alibaba-NLP",
    "description": "--- license: apache-2.0 language: - en pipeline_tag: text-generation library_name: transformers --- We present **Tongyi DeepResearch**, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Ex...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3_moe",
      "text-generation",
      "conversational",
      "en",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 781,
    "downloads": 15413,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n\n# Introduction\n\nWe present  **Tongyi DeepResearch**, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch and FRAMES.\n\nMore details can be found in our ğŸ“° [Tech Blog](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research).\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/OhQCYYJu1LhrS446Qct5D.png)\n\n## Key Features\n\n- âš™ï¸ **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.\n- ğŸ”„ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.\n- ğŸ” **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a nonâ€‘stationary environment.\n- ğŸ¤– **Agent Inference Paradigm Compatibility**: At inference, Tongyi-DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.\n\n## Download\n\nYou can download the model then run the inference scipts in https://github.com/Alibaba-NLP/DeepResearch.\n\n\n```bibtex\n@misc{tongyidr,\n  author={Tongyi DeepResearch Team},\n  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},\n  year={2025},\n  howpublished={\\url{https://github.com/Alibaba-NLP/DeepResearch}}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":30532122624,\"storage_bytes\":61077998390,\"files_count\":27,\"spaces_count\":3,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3MoeForCausalLM\"],\"model_type\":\"qwen3_moe\",\"tokenizer_config\":{\"bos_token\":null,\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null},\"chat_template_jinja\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Alibaba-NLP:DeepResearch.\",\"source_url\":\"https://github.com/Alibaba-NLP/DeepResearch.\"},{\"type\":\"has_code\",\"target_id\":\"github:Alibaba-NLP:DeepResearch}}\",\"source_url\":\"https://github.com/Alibaba-NLP/DeepResearch}}\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 63.9,
    "content_hash": "58fc596c773384438f639bb6b5b7656c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:timesfm-1.0-200m",
    "name": "timesfm-1.0-200m",
    "author": "google",
    "description": "--- license: apache-2.0 library_name: timesfm pipeline_tag: time-series-forecasting --- TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. **Resources and Technical Documentation**: * Paper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024. * Google Research blog * GitHub repo **Authors**: Google Research This is not an officially supported Google product. is the first...",
    "tags": [
      "timesfm",
      "time-series-forecasting",
      "arxiv:2310.10688",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "time-series-forecasting",
    "likes": 777,
    "downloads": 1965,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/timesfm-1.0-200m",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlibrary_name: timesfm\npipeline_tag: time-series-forecasting\n---\n\n# TimesFM\n\nTimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\n\n**Resources and Technical Documentation**:\n\n* Paper: [A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688), to appear in ICML 2024.\n* [Google Research blog](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/)\n* [GitHub repo](https://github.com/google-research/timesfm)\n\n**Authors**: Google Research\n\nThis is not an officially supported Google product.\n\n## Checkpoint timesfm-1.0-200m\n\n`timesfm-1.0-200m` is the first open model checkpoint:\n\n- It performs univariate time series forecasting for context lengths up to 512 time points and any horizon lengths, with an optional frequency indicator.\n- It focuses on point forecasts and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.\n- It requires the context to be contiguous (i.e. no \"holes\"), and the context and the horizon to be of the same frequency.\n\n## Benchmarks\n\nPlease refer to our result tables on the [extended benchmarks](https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/tfm_results.png) and the [long horizon benchmarks](https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/tfm_long_horizon.png).\n\nPlease look into the README files in the respective benchmark directories within `experiments/` for instructions for running TimesFM on the respective benchmarks.\n\n## Installation\n\nThis HuggingFace repo hosts TimesFm checkpoints. Please visit our [GitHub repo](https://github.com/google-research/timesfm) and follow the instructions there to install the `timesfm` library for model inference.\n\nIn particular, the dependency `lingvo` does not support ARM architectures and the inference code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\n\n## Usage \n\n### Initialize the model and load a checkpoint.\nThen the base class can be loaded as,\n\n```python\nimport timesfm\n\ntfm = timesfm.TimesFm(\n    context_len=<context>,\n    horizon_len=<horizon>,\n    input_patch_len=32,\n    output_patch_len=128,\n    num_layers=20,\n    model_dims=1280,\n    backend=<backend>,\n)\ntfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\n```\n\nNote that the four parameters are fixed to load the 200m model\n\n```python\ninput_patch_len=32,\noutput_patch_len=128,\nnum_layers=20,\nmodel_dims=1280,\n```\n\n1. The context_len here can be set as the max context length **of the model**. You can provide a shorter series to the `tfm.forecast()` function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have **any context length**. Padding / truncation will be handled by the inference code if needed.\n\n2. The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length <= context length but it is not a requirement in the function call.\n\n### Perform inference\n\nWe provide APIs to forecast from either array inputs or `pandas` dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions `tfm.forecast()` and `tfm.forecast_on_df()` for detailed instructions.\n\nIn particular, regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:\n\n- **0** (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.\n- **1**: medium frequency time series. We recommend using this for weekly and monthly data.\n- **2**: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.\n\nThis categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that\n\n- **0**: T, MIN, H, D, B, U\n- **1**: W, M\n- **2**: Q, Y\n\nNotice you do **NOT** have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.\n\n\nExamples:\n\nArray inputs, with the frequencies set to low, medium, and high respectively.\n\n```python\nimport numpy as np\nforecast_input = [\n    np.sin(np.linspace(0, 20, 100))\n    np.sin(np.linspace(0, 20, 200)),\n    np.sin(np.linspace(0, 20, 400)),\n]\nfrequency_input = [0, 1, 2]\n\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\n    forecast_input,\n    freq=frequency_input,\n)\n```\n\n`pandas` dataframe, with the frequency set to \"M\" monthly.\n\n```python\nimport pandas as pd\n\n# e.g. input_df is\n#       unique_id  ds          y\n# 0     T1         1975-12-31  697458.0\n# 1     T1         1976-01-31  1187650.0\n# 2     T1         1976-02-29  1069690.0\n# 3     T1         1976-03-31  1078430.0\n# 4     T1         1976-04-30  1059910.0\n# ...   ...        ...         ...\n# 8175  T99        1986-01-31  602.0\n# 8176  T99        1986-02-28  684.0\n# 8177  T99        1986-03-31  818.0\n# 8178  T99        1986-04-30  836.0\n# 8179  T99        1986-05-31  878.0\n\nforecast_df = tfm.forecast_on_df(\n    inputs=input_df,\n    freq=\"M\",  # monthly\n    value_name=\"y\",\n    num_jobs=-1,\n)\n```",
    "meta_json": "{\"pipeline_tag\":\"time-series-forecasting\",\"library_name\":\"timesfm\",\"framework\":\"timesfm\",\"params\":null,\"storage_bytes\":814291152,\"files_count\":5,\"spaces_count\":4,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:google-research:timesfm\",\"source_url\":\"https://github.com/google-research/timesfm\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:timesfm\",\"source_url\":\"https://github.com/google-research/timesfm\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:timesfm\",\"source_url\":\"https://github.com/google-research/timesfm\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:timesfm\",\"source_url\":\"https://github.com/google-research/timesfm\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.10688\",\"source_url\":\"https://arxiv.org/abs/2310.10688\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 63.9,
    "content_hash": "787c584aa80c6e00bc16f8ee24933613",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/timesfm-1.0-200m\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nvidia:nvlm-d-72b",
    "name": "NVLM-D-72B",
    "author": "nvidia",
    "description": "--- license: cc-by-nc-4.0 language: - en pipeline_tag: image-text-to-text tags: - nvidia - NVLM - pytorch - multimodal - conversational library_name: transformers --- <p align=\"center\"> <img src=\"nvlm-logo-light.png\" alt=\"Image Description\" width=\"300\" > </p> This family of models performs vision-language and text-only tasks including optical character recognition, multimodal reasoning, localization, common sense reasoning, world knowledge utilization, and coding. This model is ready for non-...",
    "tags": [
      "transformers",
      "safetensors",
      "nvlm_d",
      "nvidia",
      "nvlm",
      "pytorch",
      "multimodal",
      "conversational",
      "image-text-to-text",
      "custom_code",
      "en",
      "arxiv:2409.11402",
      "license:cc-by-nc-4.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 775,
    "downloads": 56419,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nvidia/NVLM-D-72B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: cc-by-nc-4.0\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- nvidia\n- NVLM\n- pytorch\n- multimodal\n- conversational\nlibrary_name: transformers\n---\n\n<p align=\"center\">\n  <img src=\"nvlm-logo-light.png\" alt=\"Image Description\" width=\"300\" >\n</p>\n\n\n# Model Overview\n\n## Description\nThis family of models performs vision-language and text-only tasks including optical character recognition, multimodal reasoning, localization, common sense reasoning, world knowledge utilization, and coding.\n\nThis model is ready for non-commercial use.\n\n## License/Terms of Use\n\nGoverning Terms: Deed - [Attribution-NonCommercial 4.0 International - Creative Commons](https://creativecommons.org/licenses/by-nc/4.0/deed.en).\n\nAdditional Information: [LICENSE Â· Qwen/Qwen2-72B-Instruct at main](https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE) for Qwen2-72B-Instruct and [The MIT License â€“ Open Source Initiative](https://opensource.org/license/mit) for InternViT-6B-448px-V1-2.\n\n# Model Details\n\nToday (September 17th, 2024), we introduce [NVLM 1.0](https://arxiv.org/abs/2409.11402), a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. \n\nIn this repo, we are open-sourcing NVLM-1.0-D-72B (decoder-only architecture), the decoder-only model weights and code for the community.\n\n\n\n## Reference(s)\n[Paper](https://arxiv.org/abs/2409.11402) &ensp; [Inference Code (HF)](https://huggingface.co/nvidia/NVLM-D-72B/tree/main) &ensp; [Training Code](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm) &ensp; [Website](https://research.nvidia.com/labs/adlr/NVLM-1/) \n\n## Benchmark Results\nWe train our model with legacy [Megatron-LM](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/legacy) and adapt the codebase to Huggingface for model hosting, reproducibility, and inference.\nWe observe numerical differences between the Megatron and Huggingface codebases, which are within the expected range of variation. \nWe provide the results from both the Huggingface codebase and the Megatron codebase for reproducibility and comparison with other models.\n\nResults (as of September 17th, 2024) in the multimodal benchmarks are as follows:\n\n### Vision-language Benchmarks \n\n| Benchmark                    | MMMU (val / test) | MathVista | OCRBench | AI2D | ChartQA | DocVQA | TextVQA | RealWorldQA | VQAv2 |\n|------------------------------|-------------------|-----------|----------|------|---------|--------|---------|-------------|-------|\n| NVLM-D 1.0 72B (Huggingface) | 58.7 / 54.9       | 65.2      | 852      | 94.2 | 86.0    | 92.6   | 82.6    | 69.5        | 85.4  |\n| NVLM-D 1.0 72B (Megatron)    | 59.7 / 54.6       | 65.2      | 853      | 94.2 | 86.0    | 92.6   | 82.1    | 69.7        | 85.4  |\n| Llama 3.2 90B                | 60.3 / -          | 57.3      | -        | 92.3 | 85.5    | 90.1   | -       | -           | 78.1  |\n| Llama 3-V 70B                | 60.6 / -          | -         | -        | 93.0 | 83.2    | 92.2   | 83.4    | -           | 79.1  |\n| Llama 3-V 405B               | 64.5 / -          | -         | -        | 94.1 | 85.8    | 92.6   | 84.8    | -           | 80.2  |\n| InternVL2-Llama3-76B         | 55.2 / -          | 65.5      | 839      | 94.8 | 88.4    | 94.1   | 84.4    | 72.2        | -     |\n| GPT-4V                       | 56.8 / 55.7       | 49.9      | 645      | 78.2 | 78.5    | 88.4   | 78.0    | 61.4        | 77.2  |\n| GPT-4o                       | 69.1 / -          | 63.8      | 736      | 94.2 | 85.7    | 92.8   | -       | -           | -     |\n| Claude 3.5 Sonnet            | 68.3 / -          | 67.7      | 788      | 94.7 | 90.8    | 95.2   | -       | -           | -     |\n| Gemini 1.5 Pro (Aug 2024)    | 62.2 / -          | 63.9      | 754      | 94.4 | 87.2    | 93.1   | 78.7    | 70.4        | 80.2  |\n\n### Text-only Benchmarks\n\n| Tasks                        | Backbone LLM | MMLU | GSM8K | MATH | HumanEval | Avg. Accuracy    |\n|------------------------------|--------------|------|-------|------|-----------|------------------|\n| **Proprietary**              |              |      |       |      |           |                  |\n| GPT-4.0                      | N/A          | 88.7 | -     | 76.6 | 90.2      | -                |\n| Gemini Pro 1.5 (Aug 2024)    | N/A          | 85.9 | 90.8  | 67.7 | 84.1      | 82.1             |\n| Claude 3.5 Sonnet            | N/A          | 88.7 | 96.4  | 71.1 | 92.0      | 87.0             |\n| **Open LLM**                 |              |      |       |      |           |                  |\n| (a) Nous-Hermes-2-Yi-34B     | N/A          | 75.5 | 78.6  | 21.8 | 43.3      | 54.8             |\n| (b) Qwen-72B-Instruct        | N/A          | 82.3 | 91.1  | 59.7 | 86.0      | 79.8             |\n| (c) Llama-3-70B-Instruct     | N/A          | 82.0 | 93.0  | 51.0 | 81.7      | 76.6             |\n| (d) Llama-3.1-70B-Instruct   | N/A          | 83.6 | 95.1  | 68.0 | 80.5      | 81.8             |\n| (e) Llama-3.1-405B-Instruct  | N/A          | 87.3 | 96.8  | 73.8 | 89.0      | 86.7             |\n| **Open Multimodal LLM**      |              |      |       |      |           |                  |\n| VILA-1.5 40B                 | (a)          | 73.3 | 67.5  | 16.8 | 34.1      | ğŸ¥¶ 47.9   (-6.9) |\n| LLaVA-OneVision 72B          | (b)          | 80.6 | 89.9  | 49.2 | 74.4      | ğŸ¥¶ 73.5   (-6.3) |\n| InternVL-2-Llama3-76B        | (c)          | 78.5 | 87.1  | 42.5 | 71.3      | ğŸ¥¶ 69.9   (-6.7) |\n| *Llama 3-V 70B               | (d)          | 83.6 | 95.1  | 68.0 | 80.5      | ğŸ™‚ 81.8   (0)    |\n| *Llama 3-V 405B              | (e)          | 87.3 | 96.8  | 73.8 | 89.0      | ğŸ™‚ 86.7   (0)    |\n| NVLM-D 1.0 72B (Megatron)    | (b)          | 82.0 | 92.9  | 73.1 | 88.4      | ğŸ¥³ 84.1   (+4.3) |\n| NVLM-D 1.0 72B (Huggingface) | (b)          | 81.7 | 93.2  | 73.1 | 89.0      | ğŸ¥³ 84.3   (+4.5) |\n\n\n## Model Architectures\n\n**Network Architecture:** Decoder-Only Transformer\n\n**Text-only LLM backbone:** [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n\n**Vision encoder:** [InternViT-6B](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2)\n\n### Robustness\n\nThe model trained on this dataset cannot regenerate its training data:\n\n1. The model has no image generation capability since its output is only text. Hence it cannot regenerate any image it would have seen during training.\n\n2. The model cannot regenerate training text data: during training, the model takes text and images as inputs, and the model output (text) is conditioned on both inputs. During inference, without training images as input, the models would not be able to reproduce any part of the training text data.\n\n\n### Input\n**Input Type(s):** Text, Image <br>\n**Input Format(s):** String, [Pillow Library-Supported Formats](https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html) <br>\n**Input Dimensions:** One-Dimensional (1D), Two Dimensional (2D) <br>\n**Other Properties Related to Input:** Maximum Token Length = 128K Tokens <br>\n\n### Output\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Model Output:** 1D <br>\n**Other Properties Related to Output:** None <br> \n\n## How to use\n\nWhen converting Megatron checkpoint to Huggingface, we adapt [InternVL codebase](https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B) to support model loading and multi-GPU inference in HF. \nWe also use the tokenizer from [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/tree/main) when adapting the tokenizer to Huggingface, as it contains extra special tokens for vision tasks, e.g., `<|vision_pad|>`. \nWe train NVLM-1.0-D-72B based on the [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct/tree/main) text-only model and [InternViT-6B-448px-V1-5](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5) ViT model with our large-scale high-quality multimodal dataset. \nFor training code, please refer to [Megatron-Core](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm).\n\n\n### Prepare the environment\n\nWe provide a docker build file in the [Dockerfile](Dockerfile) for reproduction. \n\nThe docker image is based on `nvcr.io/nvidia/pytorch:23.09-py3`. \n\n*Note: We observe that different transformer versions / CUDA versions / docker versions can lead to slight benchmark number differences. We recommend using the Dockerfile above for precise reproduction.*\n\n### Model loading\n\n```python\nimport torch\nfrom transformers import AutoModel\n\npath = \"nvidia/NVLM-D-72B\"\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True).eval()\n```\n\n### Multiple GPUs\n\nThe model can be loaded on multiple GPUs as follows:\n\n```python\nimport torch\nimport math\nfrom transformers import AutoModel\n\ndef split_model():\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = 80\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n            layer_cnt += 1\n    device_map['vision_model'] = 0\n    device_map['mlp1'] = 0\n    device_map['language_model.model.tok_embeddings'] = 0\n    device_map['language_model.model.embed_tokens'] = 0\n    device_map['language_model.output'] = 0\n    device_map['language_model.model.norm'] = 0\n    device_map['language_model.lm_head'] = 0\n    device_map['language_model.model.rotary_emb'] = 0\n    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n\n    return device_map\n\npath = \"nvidia/NVLM-D-72B\"\ndevice_map = split_model()\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True,\n    device_map=device_map).eval()\n```\n\n\n### Inference\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport math\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\n\n\ndef split_model():\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = 80\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n            layer_cnt += 1\n    device_map['vision_model'] = 0\n    device_map['mlp1'] = 0\n    device_map['language_model.model.tok_embeddings'] = 0\n    device_map['language_model.model.embed_tokens'] = 0\n    device_map['language_model.output'] = 0\n    device_map['language_model.model.norm'] = 0\n    device_map['language_model.lm_head'] = 0\n    device_map['language_model.model.rotary_emb'] = 0\n    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n\n    return device_map\n\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\npath = \"nvidia/NVLM-D-72B\"\ndevice_map = split_model()\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True,\n    device_map=device_map).eval()\n\nprint(model)\n\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=False)\n\n# pure-text conversation\nquestion = 'Hello, who are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n# single-image single-round conversation\npixel_values = load_image('path/to/your/example/image.jpg', max_num=6).to(\n    torch.bfloat16)\nquestion = '<image>\\nPlease describe the image shortly.'\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f'User: {question}\\nAssistant: {response}')\n```\n\n### Benchmark Evaluation\n\nTo test our NVLM-1.0 model on the benchmark datasets, you can use the following code:\n\n```bash\npython run_eval.py --config-path eval/full_eval.yaml \\\n --result-save-path path/to/eval_results/ \\\n --zero-shot-eval-tasks chartqa coco_caption flickr30k_caption vqav2 mmmu textvqa mathvista mmbench chartqa docvqa realworldqa ocrbench ai2diagram ai2diagram_nomask mmmu_pro docvqa_test\n```\n\nSpecifically,\n- `--config-path eval/full_eval.yaml` file contains the evaluation configurations, including  the evaluation prompt, the evaluation dataset paths, and generation hyper-parameters.\n- `--result-save-path path/to/eval_results/` specifies the path to save the evaluation results.\n- `--zero-shot-eval-tasks` specifies the tasks to evaluate on.\n\n\n## Software Integration\n**Runtime Engine(s)** \n* PyTorch <br>\n\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Hopper <br>\n\n**[Preferred/Supported] Operating System(s):** <br>\n* Linux <br>\n\n## Inference\n**Engine:** PyTorch <br>\n**Test Hardware:** <br>\n* H100 <br>\n\n## Model Version(s)\n* v1.0-D (NVLM-D)\n\n## Training, Testing, and Evaluation Datasets \n\n### Pre-Training Dataset\n\n**Link** <br>\n* [See Table 4](https://arxiv.org/abs/2409.11402) <br>\n\n**Data Collection Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Labeling Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Properties** \n* Trained on image captions, image-text pairs, natural images, charts, documents, scene descriptions, and mathematical reasoning. <br>\n\n### Supervised Fine-Tuning Dataset\n**Link** <br>\n* [See Table 6](https://arxiv.org/abs/2409.11402) <br>\n\n**Data Collection Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Labeling Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Properties** \n* Trained on image captions; general knowledge; image-text pairs; natural images; charts; diagrams; documents; scene descriptions; science diagrams, lessons, textbook data, and question-answer pairs; visual instruction tuning; and mathematical reasoning. <br>\n\n### Evaluation Dataset\n**Link** <br>\n* [See Section 6.1, \"Benchmark\"](https://arxiv.org/abs/2409.11402) <br>\n\n**Data collection method by dataset** <br>\n* Human <br>\n\n**Labeling method by dataset** <br>\n* Human <br>\n\n**Properties** <br>\n* Evaluated on general knowledge, visual answering, chart understanding, table, optical character recognition, and mathematical reasoning. <br> \n\n\n## Correspondence to\nWenliang Dai* (wdai@nvidia.com), Nayeon Lee* (nayeonl@nvidia.com), Boxin Wang* (boxinw@nvidia.com), Zhuolin Yang* (zhuoliny@nvidia.com), Wei Ping* (wping@nvidia.com)\n\n*Equal contribution\n\n## Citation\n<pre>\n@article{nvlm2024,\n  title={NVLM: Open Frontier-Class Multimodal LLMs},\n  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuolin and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},\n  journal={arXiv preprint},\n  year={2024}}\n</pre>\n\n\n## Ethical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n ",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":79379593344,\"storage_bytes\":354049622570,\"files_count\":72,\"spaces_count\":6,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"NVLM_D\"],\"auto_map\":{\"AutoConfig\":\"configuration_nvlm_d.NVLM_D_Config\",\"AutoModel\":\"modeling_nvlm_d.NVLM_D_Model\",\"AutoModelForCausalLM\":\"modeling_nvlm_d.NVLM_D_Model\"},\"model_type\":\"NVLM_D\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:Megatron-LM\",\"source_url\":\"https://github.com/NVIDIA/Megatron-LM\"},{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:Megatron-LM\",\"source_url\":\"https://github.com/NVIDIA/Megatron-LM\"},{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:Megatron-LM\",\"source_url\":\"https://github.com/NVIDIA/Megatron-LM\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2409.11402\",\"source_url\":\"https://arxiv.org/abs/2409.11402\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 78.9,
    "content_hash": "aeb4c72e78697ea94b86421ae136ba7f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nvidia/NVLM-D-72B\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:openbmb:voxcpm-0.5b",
    "name": "VoxCPM-0.5B",
    "author": "openbmb",
    "description": "--- license: apache-2.0 language: - en - zh base_model: - openbmb/MiniCPM4-0.5B pipeline_tag: text-to-speech library_name: voxcpm tags: - text-to-speech - speech - speech generation - voice cloning --- <div align=\"center\"> <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\"> </div> VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization a...",
    "tags": [
      "voxcpm",
      "pytorch",
      "text-to-speech",
      "speech",
      "speech generation",
      "voice cloning",
      "en",
      "zh",
      "base_model:openbmb/minicpm4-0.5b",
      "base_model:finetune:openbmb/minicpm4-0.5b",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 772,
    "downloads": 2453,
    "source": "huggingface",
    "source_url": "https://huggingface.co/openbmb/VoxCPM-0.5B",
    "image_url": "https://huggingface.co/openbmb/VoxCPM-0.5B/resolve/main/assets/modelbest_logo.png",
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nbase_model:\n- openbmb/MiniCPM4-0.5B\npipeline_tag: text-to-speech\nlibrary_name: voxcpm\ntags:\n- text-to-speech\n- speech\n- speech generation\n- voice cloning\n---\n\n## ğŸ™ï¸ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage/)\n\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_logo.png\" alt=\"VoxCPM Logo\" width=\"40%\">\n</div>\n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align=\"center\">\n  <img src=\"assets/voxcpm_model.png\" alt=\"VoxCPM Model Architecture\" width=\"90%\">\n</div>\n\n\n###  ğŸš€ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speakerâ€™s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n##  Quick Start\n\n### ğŸ”§ Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download(\"openbmb/VoxCPM-0.5B\",local_files_only=local_files_only)\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\n    snapshot_download('iic/SenseVoiceSmall')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text \"Hello VoxCPM\" --output out.wav\n\n# 2) Voice cloning (reference audio + transcript)\nvoxcpm --text \"Hello\" \\\n  --prompt-audio path/to/voice.wav \\\n  --prompt-text \"reference transcript\" \\\n  --output out.wav \\\n  --denoise\n\n# 3) Batch processing (one text per line)\nvoxcpm --input examples/input.txt --output-dir outs\n# (optional) Batch + cloning\nvoxcpm --input examples/input.txt --output-dir outs \\\n  --prompt-audio path/to/voice.wav \\\n  --prompt-text \"reference transcript\" \\\n  --denoise\n\n# 4) Inference parameters (quality/speed)\nvoxcpm --text \"...\" --output out.wav \\\n  --cfg-value 2.0 --inference-timesteps 10 --normalize\n\n# 5) Model loading\n# Prefer local path\nvoxcpm --text \"...\" --output out.wav --model-path /path/to/VoxCPM_model_dir\n# Or from Hugging Face (auto download/cache)\nvoxcpm --text \"...\" --output out.wav \\\n  --hf-model-id openbmb/VoxCPM-0.5B --cache-dir ~/.cache/huggingface --local-files-only\n\n# 6) Denoiser control\nvoxcpm --text \"...\" --output out.wav \\\n  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base\n\n# 7) Help\nvoxcpm --help\npython -m voxcpm.cli --help\n```\n\n### 4. Start web demo\n\nYou can start the UI interface by running `python app.py`, which allows you to perform Voice Cloning and Voice Creation.\n\n\n\n## ğŸ‘©â€ğŸ³ A Voice Chef's Guide\nWelcome to the VoxCPM kitchen! Follow this recipe to cook up perfect generated speech. Letâ€™s begin.\n\n---\n### ğŸ¥š Step 1: Prepare Your Base Ingredients (Content)\n\nFirst, choose how youâ€™d like to input your text:.\n1. Regular Text (Classic Mode)\n- âœ… Keep \"Text Normalization\" ON. Type naturally (e.g., \"Hello, world! 123\"). The system will automatically process numbers, abbreviations, and punctuation using WeTextProcessing library.\n2. Phoneme Input (Native Mode)\n- âŒ Turn \"Text Normalization\" OFF. Enter phoneme text like {HH AH0 L OW1} (EN) or {ni3}{hao3} (ZH) for precise pronunciation  control. In this mode, VoxCPM also supports native understanding of other complex non-normalized textâ€”try it out!\n\n---\n### ğŸ³ Step 2: Choose Your Flavor Profile (Voice Style) \n\nThis is the secret sauce that gives your audio its unique sound.\n1. Cooking with a Prompt Speech (Following a Famous Recipe)\n  - A prompt speech provides the desired acoustic characteristics for VoxCPM. The speaker's timbre, speaking style, and even the background sounds and ambiance will be replicated.\n  - For a Clean, Studio-Quality Voice:\n    - âœ… Enable \"Prompt Speech Enhancement\". This acts like a noise filter, removing background hiss and rumble to give you a pure, clean voice clone.\n2. Cooking au Naturel (Letting the Model Improvise)\n  - If no reference is provided, VoxCPM becomes a creative chef! It will infer a fitting speaking style based on the text itself, thanks to the text-smartness of its foundation model, MiniCPM-4.\n  - Pro Tip: Challenge VoxCPM with any textâ€”poetry, song lyrics, dramatic monologuesâ€”it may deliver some interesting results!\n\n---\n### ğŸ§‚ Step 3: The Final Seasoning (Fine-Tuning Your Results)\nYou're ready to serve! But for master chefs who want to tweak the flavor, here are two key spices.\n- CFG Value (How Closely to Follow the Recipe)\n  - Default: A great starting point.\n  - Voice sounds strained or weird? Lower this value. It tells the model to be more relaxed and improvisational, great for expressive prompts.\n  - Need maximum clarity and adherence to the text? Raise it slightly to keep the model on a tighter leash.\n- Inference Timesteps (Simmering Time: Quality vs. Speed)\n  - Need a quick snack? Use a lower number. Perfect for fast drafts and experiments.\n  - Cooking a gourmet meal? Use a higher number. This lets the model \"simmer\" longer, refining the audio for superior detail and naturalness.\n\n---\nHappy creating! ğŸ‰ Start with the default settings and tweak from there to suit your project. The kitchen is yours!\n\n\n---\n\n\n\n## ğŸ“Š Performance Highlights\n\nVoxCPM achieves competitive results on public zero-shot TTS benchmarks:\n\n### Seed-TTS-eval Benchmark\n\n| Model | Parameters | Open-Source | test-EN | | test-ZH | | test-Hard | |\n|------|------|------|:------------:|:--:|:------------:|:--:|:-------------:|:--:|\n| | | | WER/%â¬‡ | SIM/%â¬†| CER/%â¬‡| SIM/%â¬† | CER/%â¬‡ | SIM/%â¬† |\n| MegaTTS3 | 0.5B | âŒ | 2.79 | 77.1 | 1.52 | 79.0 | - | - |\n| DiTAR | 0.6B | âŒ | 1.69 | 73.5 | 1.02 | 75.3 | - | - |\n| CosyVoice3 | 0.5B | âŒ | 2.02 | 71.8 | 1.16 | 78.0 | 6.08 | 75.8 |\n| CosyVoice3 | 1.5B | âŒ | 2.22 | 72.0 | 1.12 | 78.1 | 5.83 | 75.8 |\n| Seed-TTS | - | âŒ | 2.25 | 76.2 | 1.12 | 79.6 | 7.59 | 77.6 |\n| MiniMax-Speech | - | âŒ | 1.65 | 69.2 | 0.83 | 78.3 | - | - |\n| CosyVoice | 0.3B | âœ… | 4.29 | 60.9 | 3.63 | 72.3 | 11.75 | 70.9 |\n| CosyVoice2 | 0.5B | âœ… | 3.09 | 65.9 | 1.38 | 75.7 | **6.83** | 72.4 |\n| F5-TTS | 0.3B | âœ… | 2.00 | 67.0 | 1.53 | 76.0 | 8.67 | 71.3 |\n| SparkTTS | 0.5B | âœ… | 3.14 | 57.3 | 1.54 | 66.0 | - | - |\n| FireRedTTS | 0.5B | âœ… | 3.82 | 46.0 | 1.51 | 63.5 | 17.45 | 62.1 |\n| FireRedTTS-2 | 1.5B | âœ… | 1.95 | 66.5 | 1.14 | 73.6 | - | - |\n| Qwen2.5-Omni | 7B | âœ… | 2.72 | 63.2 | 1.70 | 75.2 | 7.97 | **74.7** |\n| OpenAudio-s1-mini | 0.5B | âœ… | 1.94 | 55.0 | 1.18 | 68.5 | - | - |\n| IndexTTS2 | 1.5B | âœ… | 2.23 | 70.6 | 1.03 | 76.5 | - | - |\n| VibeVoice | 1.5B | âœ… | 3.04 | 68.9 | 1.16 | 74.4 | - | - |\n| HiggsAudio-v2 | 3B | âœ… | 2.44 | 67.7 | 1.50 | 74.0 | - | - |\n| **VoxCPM** | 0.5B | âœ… | **1.85** | **72.9** | **0.93** | **77.2** | 8.87 | 73.0 |\n\n\n###  CV3-eval Benchmark\n\n| Model | zh | en | hard-zh | | | hard-en | | |\n|-------|:--:|:--:|:-------:|:--:|:--:|:-------:|:--:|:--:|\n| | CER/%â¬‡ | WER/%â¬‡ | CER/%â¬‡ | SIM/%â¬† | DNSMOSâ¬† | WER/%â¬‡ | SIM/%â¬† | DNSMOSâ¬† |\n| F5-TTS | 5.47 | 8.90 | - | - | - | - | - | - |\n| SparkTTS | 5.15 | 11.0 | - | - | - | - | - | - |\n| GPT-SoVits | 7.34 | 12.5 | - | - | - | - | - | - |\n| CosyVoice2 | 4.08 | 6.32 | 12.58 | 72.6 | 3.81 | 11.96 | 66.7 | 3.95 |\n| OpenAudio-s1-mini | 4.00 | 5.54 | 18.1 | 58.2 | 3.77 | 12.4 | 55.7 | 3.89 |\n| IndexTTS2 | 3.58 | 4.45 | 12.8 | 74.6 | 3.65 | - | - | - |\n| HiggsAudio-v2 | 9.54 | 7.89 | 41.0 | 60.2 | 3.39 | 10.3 | 61.8 | 3.68 |\n| CosyVoice3-0.5B | 3.89 | 5.24 | 14.15 | 78.6 | 3.75 | 9.04 | 75.9 | 3.92 |\n| CosyVoice3-1.5B | 3.91 | 4.99 | 9.77 | 78.5 | 3.79 | 10.55 | 76.1 | 3.95 |\n| **VoxCPM** | **3.40** | **4.04** | 12.9 | 66.1 | 3.59 | **7.89** | 64.3 | 3.74 |\n\n\n## âš ï¸ Risks and limitations\n- General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.\n- Potential for Misuse of Voice Cloning: VoxCPM's powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.\n- Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.\n- Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.\n- This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.\n\n\n\n## ğŸ“„ License\nThe VoxCPM model weights and code are open-sourced under the Apache-2.0 license.\n\n\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"voxcpm\",\"framework\":\"voxcpm\",\"params\":null,\"storage_bytes\":2910957348,\"files_count\":12,\"spaces_count\":15,\"gated\":false,\"private\":false,\"config\":{\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"<|im_end|>\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false,\"chat_template\":\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:VoxCPM\",\"source_url\":\"https://github.com/OpenBMB/VoxCPM\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 98.9,
    "content_hash": "52f8d486d498dcd78e44592c0f4db47a",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/openbmb/VoxCPM-0.5B/resolve/main/assets/modelbest_logo.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/openbmb/VoxCPM-0.5B\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:facebookai:xlm-roberta-base",
    "name": "xlm-roberta-base",
    "author": "FacebookAI",
    "description": "--- tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - x...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "onnx",
      "safetensors",
      "xlm-roberta",
      "fill-mask",
      "exbert",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "arxiv:1911.02116",
      "license:mit",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "fill-mask",
    "likes": 771,
    "downloads": 7270384,
    "source": "huggingface",
    "source_url": "https://huggingface.co/FacebookAI/xlm-roberta-base",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- exbert\nlanguage: \n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- no\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\nlicense: mit\n---\n\n# XLM-RoBERTa (base-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=xlm-roberta-base\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "meta_json": "{\"pipeline_tag\":\"fill-mask\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":278885778,\"storage_bytes\":6352424687,\"files_count\":11,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"XLMRobertaForMaskedLM\"],\"model_type\":\"xlm-roberta\",\"tokenizer_config\":{}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:pytorch:fairseq\",\"source_url\":\"https://github.com/pytorch/fairseq\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.02116\",\"source_url\":\"https://arxiv.org/abs/1911.02116\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 63.9,
    "content_hash": "1fba0749c413cab557edb8d815166b93",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/FacebookAI/xlm-roberta-base\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:wizardlmteam:wizardcoder-python-34b-v1.0",
    "name": "WizardCoder-Python-34B-V1.0",
    "author": "WizardLMTeam",
    "description": "--- license: llama2 metrics: - code_eval library_name: transformers tags: - code model-index: - name: WizardCoder-Python-34B-V1.0 results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 0.732 verified: false --- <p style=\"font-size:28px;\" align=\"center\"> ğŸ  <a href=\"https://wizardlm.github.io/\" target=\"_blank\">Home Page</a> </p> <p align=\"center\"> <p align=\"center\"> ğŸ¤— <a href=\"https://huggingface.co/WizardLM\" target=\"...",
    "tags": [
      "transformers",
      "pytorch",
      "llama",
      "text-generation",
      "code",
      "arxiv:2304.12244",
      "arxiv:2306.08568",
      "arxiv:2308.09583",
      "license:llama2",
      "model-index",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 771,
    "downloads": 166,
    "source": "huggingface",
    "source_url": "https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: llama2\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: WizardCoder-Python-34B-V1.0\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.732\n      verified: false\n---\n\n## WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n<p style=\"font-size:28px;\" align=\"center\">\nğŸ  <a href=\"https://wizardlm.github.io/\" target=\"_blank\">Home Page</a> </p>\n<p align=\"center\">\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/WizardLM\" target=\"_blank\">HF Repo</a>  â€¢ğŸ± <a href=\"https://github.com/nlpxucan/WizardLM\" target=\"_blank\">Github Repo</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/WizardLM_AI\" target=\"_blank\">Twitter</a> </p>\n<p align=\"center\">\n ğŸ“ƒ <a href=\"https://arxiv.org/abs/2304.12244\" target=\"_blank\">[WizardLM]</a>  â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>   â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>  <br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ Join our <a href=\"https://discord.gg/VZjjHtWrKs\" target=\"_blank\">Discord</a>\n</p>\n\n## News\n\n[2024/01/04] ğŸ”¥ We released **WizardCoder-33B-V1.1**  trained from deepseek-coder-33b-base, the **SOTA OSS Code LLM** on [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html), achieves **79.9 pass@1** on HumanEval, **73.2 pass@1** on HumanEval-Plus, **78.9 pass@1** on MBPP, and **66.9 pass@1** on MBPP-Plus.\n\n[2024/01/04] ğŸ”¥ **WizardCoder-33B-V1.1** outperforms **ChatGPT 3.5**, **Gemini Pro**, and **DeepSeek-Coder-33B-instruct** on HumanEval and HumanEval-Plus pass@1.\n\n[2024/01/04] ğŸ”¥ **WizardCoder-33B-V1.1** is comparable with **ChatGPT 3.5**, and surpasses **Gemini Pro** on MBPP and MBPP-Plus pass@1.\n\n|  Model  |  Checkpoint  | Paper    | HumanEval  |   HumanEval+ | MBPP | MBPP+ | License |\n| ----- |------| ---- |------|-------| ----- |  ----- |----- | \n|  GPT-4-Turbo (Nov 2023)  | - | - | 85.4  | 81.7 | 83.0 | 70.7 |-|\n|  GPT-4 (May 2023)  | - | - | 88.4  | 76.8 | - | - |-|\n|  GPT-3.5-Turbo (Nov 2023)  | - | - | 72.6  | 65.9 | 81.7 | 69.4 |-|\n|  Gemini Pro  | - | - | 63.4  | 55.5 | 72.9 | 57.9 |-|\n|  DeepSeek-Coder-33B-instruct | - | - |  78.7 | 72.6 | 78.7 | 66.7 |-|\n|  **WizardCoder-33B-V1.1**  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-33B-V1.1\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  79.9  | 73.2 | 78.9 | 66.9 |  <a href=\"https://huggingface.co/WizardLM/WizardMath-7B-V1.1/resolve/main/LICENSE\" target=\"_blank\">MSFTResearch</a>  |\n|  WizardCoder-Python-34B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  73.2   | 64.6 | 73.2 | 59.9 |  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama2</a>  |\n|  WizardCoder-15B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-15B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  59.8   | 52.4 | -- | -- |  <a href=\"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\" target=\"_blank\">OpenRAIL-M</a>  |\n|  WizardCoder-Python-13B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  64.0   | -- | -- | -- |  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama2</a>  |\n|  WizardCoder-Python-7B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  55.5   | -- | -- | -- |  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama2</a>  |\n|  WizardCoder-3B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-3B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  34.8   | -- | -- | -- |  <a href=\"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\" target=\"_blank\">OpenRAIL-M</a>  |\n|  WizardCoder-1B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-1B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  23.8   | -- | -- | -- |  <a href=\"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\" target=\"_blank\">OpenRAIL-M</a>  |\n\n\n\n-  Our **WizardMath-70B-V1.0** model slightly outperforms some closed-source LLMs on the GSM8K, including **ChatGPT 3.5**, **Claude Instant 1** and **PaLM 2 540B**.\n-  Our **WizardMath-70B-V1.0** model achieves  **81.6 pass@1** on the [GSM8k Benchmarks](https://github.com/openai/grade-school-math), which is **24.8** points higher than the SOTA open-source LLM, and achieves  **22.7 pass@1** on the [MATH Benchmarks](https://github.com/hendrycks/math), which is **9.2** points higher than the SOTA open-source LLM.\n\n<font size=4>\n    \n| Model | Checkpoint | Paper  | GSM8k | MATH  |Online Demo| License|\n| ----- |------| ---- |------|-------| ----- | ----- |\n| WizardMath-70B-V1.0 | ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardMath-70B-V1.0\" target=\"_blank\">HF Link</a> |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>| **81.6**  |  **22.7**\t|[Demo](http://47.103.63.15:50083/)| <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2  </a> |\n| WizardMath-13B-V1.0 | ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardMath-13B-V1.0\" target=\"_blank\">HF Link</a> |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>| **63.9**  |  **14.0** |[Demo](http://47.103.63.15:50082/)| <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2 </a> |\n| WizardMath-7B-V1.0 | ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardMath-7B-V1.0\" target=\"_blank\">HF Link</a>  |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>| \t **54.9**  |  **10.7** | [Demo ](http://47.103.63.15:50080/)|  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2  </a>|\n</font>\n\n\n- [08/09/2023] We released **WizardLM-70B-V1.0** model. Here is [Full Model Weight](https://huggingface.co/WizardLM/WizardLM-70B-V1.0). \n\n<font size=4>\n    \n   \n| <sup>Model</sup> | <sup>Checkpoint</sup> | <sup>Paper</sup> |<sup>MT-Bench</sup> | <sup>AlpacaEval</sup>  | <sup>GSM8k</sup> | <sup>HumanEval</sup>  | <sup>License</sup>|\n| ----- |------| ---- |------|-------| ----- | ----- | ----- | \n| <sup>**WizardLM-70B-V1.0**</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-70B-V1.0\" target=\"_blank\">HF Link</a> </sup>|<sup>ğŸ“ƒ**Coming Soon**</sup>| <sup>**7.78**</sup> | <sup>**92.91%**</sup>\t |<sup>**77.6%**</sup>\t | <sup>   **50.6**</sup>|<sup> <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.2</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.2\" target=\"_blank\">HF Link</a> </sup>|  | <sup>7.06</sup> | <sup>89.17%</sup>\t |<sup>55.3%</sup>\t | <sup>36.6   </sup>|<sup> <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.1</sup> |<sup> ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.1\" target=\"_blank\">HF Link</a> </sup> |  | <sup>6.76</sup>  |<sup>86.32%</sup>\t | \t | <sup>25.0   </sup>| <sup>Non-commercial</sup>|\n| <sup>WizardLM-30B-V1.0</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-30B-V1.0\" target=\"_blank\">HF Link</a></sup>  | | <sup>7.01</sup> |                    | |  <sup>37.8  </sup>| <sup>Non-commercial</sup> |\n| <sup>WizardLM-13B-V1.0</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.0\" target=\"_blank\">HF Link</a> </sup> |  | <sup>6.35</sup> | <sup>75.31%</sup> |  | <sup> 24.0   </sup> | <sup>Non-commercial</sup>|\n| <sup>WizardLM-7B-V1.0 </sup>|  <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-7B-V1.0\" target=\"_blank\">HF Link</a> </sup> |<sup> ğŸ“ƒ <a href=\"https://arxiv.org/abs/2304.12244\" target=\"_blank\">[WizardLM]</a> </sup>|  |  |  |<sup>19.1 </sup>|<sup> Non-commercial</sup>|\n</font>\n\n\n## Comparing WizardCoder-Python-34B-V1.0 with Other LLMs.\n\nğŸ”¥ The following figure shows that our **WizardCoder-Python-34B-V1.0 attains the second position in this benchmark**, surpassing GPT4 (2023/03/15, 73.2 vs. 67.0), ChatGPT-3.5 (73.2 vs. 72.5) and Claude2 (73.2 vs. 71.2).\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/compare_sota.png\" alt=\"WizardCoder\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n## Prompt Format\n```\n\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\n```\n\n## Inference Demo Script\n\nWe provide the inference demo code [here](https://github.com/nlpxucan/WizardLM/tree/main/demo).\n\n## Citation\n\nPlease cite the repo if you use the data, method or code in this repo.\n\n```\n@article{luo2023wizardcoder,\n  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},\n  journal={arXiv preprint arXiv:2306.08568},\n  year={2023}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":134976679658,\"files_count\":17,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"</s>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":null,\"unk_token\":{\"__type\":\"AddedToken\",\"content\":\"<unk>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false}}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:nlpxucan:WizardLM\\\"\",\"source_url\":\"https://github.com/nlpxucan/WizardLM\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:grade-school-math\",\"source_url\":\"https://github.com/openai/grade-school-math\"},{\"type\":\"has_code\",\"target_id\":\"github:hendrycks:math\",\"source_url\":\"https://github.com/hendrycks/math\"},{\"type\":\"has_code\",\"target_id\":\"github:nlpxucan:WizardLM\",\"source_url\":\"https://github.com/nlpxucan/WizardLM\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.12244\",\"source_url\":\"https://arxiv.org/abs/2304.12244\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.08568\",\"source_url\":\"https://arxiv.org/abs/2306.08568\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2308.09583\",\"source_url\":\"https://arxiv.org/abs/2308.09583\"}]",
    "canonical_id": null,
    "license_spdx": "LLaMA-2",
    "compliance_status": "approved",
    "quality_score": 63.9,
    "content_hash": "1167423df261871b914e9789912ee79a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-embedding-0.6b",
    "name": "Qwen3-Embedding-0.6B",
    "author": "Qwen",
    "description": "--- license: apache-2.0 base_model: - Qwen/Qwen3-0.6B-Base tags: - transformers - sentence-transformers - sentence-similarity - feature-extraction - text-embeddings-inference --- <p align=\"center\"> <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png\" width=\"400\"/> <p> The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qw...",
    "tags": [
      "sentence-transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "transformers",
      "sentence-similarity",
      "feature-extraction",
      "text-embeddings-inference",
      "arxiv:2506.05176",
      "base_model:qwen/qwen3-0.6b-base",
      "base_model:finetune:qwen/qwen3-0.6b-base",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "feature-extraction",
    "likes": 770,
    "downloads": 4293918,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen3-0.6B-Base\ntags:\n- transformers\n- sentence-transformers\n- sentence-similarity\n- feature-extraction\n- text-embeddings-inference\n---\n# Qwen3-Embedding-0.6B\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png\" width=\"400\"/>\n<p>\n\n## Highlights\n\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\n\n**Exceptional Versatility**: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks **No.1** in the MTEB multilingual leaderboard (as of June 5, 2025, score **70.58**), while the reranking model excels in various text retrieval scenarios.\n\n**Comprehensive Flexibility**: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\n\n**Multilingual Capability**: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\n\n## Model Overview\n\n**Qwen3-Embedding-0.6B** has the following features:\n\n- Model Type: Text Embedding\n- Supported Languages: 100+ Languages\n- Number of Paramaters: 0.6B\n- Context Length: 32k\n- Embedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-embedding/), [GitHub](https://github.com/QwenLM/Qwen3-Embedding).\n\n## Qwen3 Embedding Series Model list\n\n| Model Type       | Models               | Size | Layers | Sequence Length | Embedding Dimension | MRL Support | Instruction Aware |\n|------------------|----------------------|------|--------|-----------------|---------------------|-------------|----------------|\n| Text Embedding   | [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) | 0.6B | 28     | 32K             | 1024                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)   | 4B   | 36     | 32K             | 2560                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)   | 8B   | 36     | 32K             | 4096                | Yes         | Yes            |\n| Text Reranking   | [Qwen3-Reranker-0.6B](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B) | 0.6B | 28     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-4B](https://huggingface.co/Qwen/Qwen3-Reranker-4B)   | 4B   | 36     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-8B](https://huggingface.co/Qwen/Qwen3-Reranker-8B)   | 8B   | 36     | 32K             | -                   | -           | Yes            |\n\n> **Note**:\n> - `MRL Support` indicates whether the embedding model supports custom dimensions for the final embedding. \n> - `Instruction Aware` notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\n> - Our evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\n\n## Usage\n\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\n### Sentence Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to \"left\":\n# model = SentenceTransformer(\n#     \"Qwen/Qwen3-Embedding-0.6B\",\n#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n#     tokenizer_kwargs={\"padding_side\": \"left\"},\n# )\n\n# The queries and documents to embed\nqueries = [\n    \"What is the capital of China?\",\n    \"Explain gravity\",\n]\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7646, 0.1414],\n#         [0.1355, 0.6000]])\n```\n\n### Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef last_token_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery:{query}'\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nqueries = [\n    get_detailed_instruct(task, 'What is the capital of China?'),\n    get_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\n\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n\nmax_length = 8192\n\n# Tokenize the input texts\nbatch_dict = tokenizer(\n    input_texts,\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7645568251609802, 0.14142508804798126], [0.13549736142158508, 0.5999549627304077]]\n```\n\n### vLLM Usage\n\n```python\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery:{query}'\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nqueries = [\n    get_detailed_instruct(task, 'What is the capital of China?'),\n    get_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\n\nmodel = LLM(model=\"Qwen/Qwen3-Embedding-0.6B\", task=\"embed\")\n\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7620252966880798, 0.14078938961029053], [0.1358368694782257, 0.6013815999031067]]\n```\n\nğŸ“Œ **Tip**: We recommend that developers customize the `instruct` according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an `instruct` on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\n\n### Text Embeddings Inference (TEI) Usage\n\nYou can either run / deploy TEI on NVIDIA GPUs as:\n\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B --dtype float16\n```\n\nOr on CPU devices as:\n\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B\n```\n\nAnd then, generate the embeddings sending a HTTP POST request as:\n\n```bash\ncurl http://localhost:8080/embed \\\n    -X POST \\\n    -d '{\"inputs\": [\"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\", \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"]}' \\\n    -H \"Content-Type: application/json\"\n```\n\n## Evaluation\n\n### MTEB (Multilingual)\n\n| Model                            |  Size   |  Mean (Task)  | Mean (Type) | Bitxt Mining | Class. | Clust. | Inst. Retri. | Multi. Class. | Pair. Class. | Rerank | Retri. | STS  |\n|----------------------------------|:-------:|:-------------:|:-------------:|:--------------:|:--------:|:--------:|:--------------:|:---------------:|:--------------:|:--------:|:--------:|:------:|\n| NV-Embed-v2                      |   7B    |     56.29     | 49.58       | 57.84        | 57.29  | 40.80  | 1.04         | 18.63         | 78.94        | 63.82  | 56.72  | 71.10|\n| GritLM-7B                        |   7B    |     60.92     | 53.74       | 70.53        | 61.83  | 49.75  | 3.45         | 22.77         | 79.94        | 63.78  | 58.31  | 73.33|\n| BGE-M3                           |  0.6B   |     59.56     | 52.18       | 79.11        | 60.35  | 40.88  | -3.11        | 20.1          | 80.76        | 62.79  | 54.60  | 74.12|\n| multilingual-e5-large-instruct   |  0.6B   |     63.22     | 55.08       | 80.13        | 64.94  | 50.75  | -0.40        | 22.91         | 80.86        | 62.61  | 57.12  | 76.81|\n| gte-Qwen2-1.5B-instruct          |  1.5B   |     59.45     | 52.69       | 62.51        | 58.32  | 52.05  | 0.74         | 24.02         | 81.58        | 62.58  | 60.78  | 71.61|\n| gte-Qwen2-7b-Instruct            |   7B    |     62.51     | 55.93       | 73.92        | 61.55  | 52.77  | 4.94         | 25.48         | 85.13        | 65.55  | 60.08  | 73.98|\n| text-embedding-3-large           |    -    |     58.93     | 51.41       | 62.17        | 60.27  | 46.89  | -2.68        | 22.03         | 79.17        | 63.89  | 59.27  | 71.68|\n| Cohere-embed-multilingual-v3.0   |    -    |     61.12     | 53.23       | 70.50        | 62.95  | 46.89  | -1.89        | 22.74         | 79.88        | 64.07  | 59.16  | 74.80|\n| Gemini Embedding                 |    -    |     68.37     | 59.59       | 79.28        | 71.82  | 54.59  | 5.18         | **29.16**     | 83.63        | 65.58  | 67.71  | 79.40|\n| **Qwen3-Embedding-0.6B**         |  0.6B   |     64.33     | 56.00       | 72.22        | 66.83  | 52.33  | 5.09         | 24.59         | 80.83        | 61.41  | 64.64  | 76.17|\n| **Qwen3-Embedding-4B**           |   4B    |     69.45     | 60.86       | 79.36        | 72.33  | 57.15  | **11.56**    | 26.77         | 85.05        | 65.08  | 69.60  | 80.86|\n| **Qwen3-Embedding-8B**           |   8B    |   **70.58**   | **61.69**   | **80.89**    | **74.00** | **57.65** | 10.06      | 28.66         | **86.40**    | **65.63** | **70.88** | **81.08** |\n\n> **Note**: For compared models, the scores are retrieved from MTEB online [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) on May 24th, 2025.\n\n### MTEB (Eng v2)\n\n| MTEB English / Models          |  Param.  | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retri. | STS   | Summ. |\n|--------------------------------|:--------:|:------------:|:------------:|:--------:|:--------:|:-------------:|:---------:|:--------:|:-------:|:-------:|\n| multilingual-e5-large-instruct |   0.6B   | 65.53      | 61.21      | 75.54  | 49.89  | 86.24       | 48.74   | 53.47  | 84.72 | 29.89 |\n| NV-Embed-v2                    |   7.8B   | 69.81      | 65.00      | 87.19  | 47.66  | 88.69       | 49.61   | 62.84  | 83.82 | 35.21 |\n| GritLM-7B                      |   7.2B   | 67.07      | 63.22      | 81.25  | 50.82  | 87.29       | 49.59   | 54.95  | 83.03 | 35.65 |\n| gte-Qwen2-1.5B-instruct        |   1.5B   | 67.20      | 63.26      | 85.84  | 53.54  | 87.52       | 49.25   | 50.25  | 82.51 | 33.94 |\n| stella_en_1.5B_v5              |   1.5B   | 69.43      | 65.32      | 89.38  | 57.06  | 88.02       | 50.19   | 52.42  | 83.27 | 36.91 |\n| gte-Qwen2-7B-instruct          |   7.6B   | 70.72      | 65.77      | 88.52  | 58.97  | 85.9        | 50.47   | 58.09  | 82.69 | 35.74 |\n| gemini-embedding-exp-03-07     |    -     | 73.3       | 67.67      | 90.05  | 59.39  | 87.7        | 48.59   | 64.35  | 85.29 | 38.28 |\n| **Qwen3-Embedding-0.6B**       |   0.6B   | 70.70      | 64.88      | 85.76  | 54.05  | 84.37       | 48.18   | 61.83  | 86.57 | 33.43 |\n| **Qwen3-Embedding-4B**         |    4B    | 74.60      | 68.10      | 89.84  | 57.51  | 87.01       | 50.76   | 68.46  | 88.72 | 34.39 |\n| **Qwen3-Embedding-8B**         |    8B    | 75.22      | 68.71      | 90.43  | 58.57  | 87.52       | 51.56   | 69.44  | 88.58 | 34.83 |\n\n### C-MTEB (MTEB Chinese)\n\n| C-MTEB           | Param. | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retr. | STS   |\n|------------------|--------|------------|------------|--------|--------|-------------|---------|-------|-------|\n| multilingual-e5-large-instruct | 0.6B   | 58.08      | 58.24      | 69.80  | 48.23  | 64.52       | 57.45   | 63.65 | 45.81 |\n| bge-multilingual-gemma2 | 9B     | 67.64      | 75.31      | 59.30  | 86.67  | 68.28       | 73.73   | 55.19 | -     |\n| gte-Qwen2-1.5B-instruct  | 1.5B   | 67.12      | 67.79      | 72.53  | 54.61  | 79.5        | 68.21   | 71.86 | 60.05 |\n| gte-Qwen2-7B-instruct    | 7.6B   | 71.62      | 72.19      | 75.77  | 66.06  | 81.16       | 69.24   | 75.70 | 65.20 |\n| ritrieve_zh_v1          | 0.3B   | 72.71      | 73.85      | 76.88  | 66.5   | 85.98       | 72.86   | 76.97 | 63.92 |\n| **Qwen3-Embedding-0.6B** | 0.6B   | 66.33      | 67.45      | 71.40  | 68.74  | 76.42       | 62.58   | 71.03 | 54.52 |\n| **Qwen3-Embedding-4B**   | 4B     | 72.27      | 73.51      | 75.46  | 77.89  | 83.34       | 66.05   | 77.03 | 61.26 |\n| **Qwen3-Embedding-8B**   | 8B     | 73.84      | 75.00      | 76.97  | 80.08  | 84.23       | 66.99   | 78.21 | 63.53 |\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"feature-extraction\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":595776512,\"storage_bytes\":5993775548,\"files_count\":12,\"spaces_count\":68,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3ForCausalLM\"],\"model_type\":\"qwen3\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in message.content %}\\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\\\n') %}\\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3-Embedding\",\"source_url\":\"https://github.com/QwenLM/Qwen3-Embedding\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2506.05176\",\"source_url\":\"https://arxiv.org/abs/2506.05176\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 78.9,
    "content_hash": "8943aadb4e812643fdc48a7768fa5cd2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:baidu:ernie-4.5-21b-a3b-thinking",
    "name": "ERNIE-4.5-21B-A3B-Thinking",
    "author": "baidu",
    "description": "--- license: apache-2.0 language: - en - zh pipeline_tag: text-generation tags: - ERNIE4.5 library_name: transformers --- <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/> </a> <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Hugging Face\" src=\"https://img.shi...",
    "tags": [
      "transformers",
      "safetensors",
      "ernie4_5_moe",
      "text-generation",
      "ernie4.5",
      "conversational",
      "en",
      "zh",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 769,
    "downloads": 14533,
    "source": "huggingface",
    "source_url": "https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking",
    "image_url": "https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking/resolve/main/benchmark.png",
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: text-generation\ntags:\n- ERNIE4.5\nlibrary_name: transformers\n---\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://ernie.baidu.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–_Chat-ERNIE_Bot-blue\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/baidu\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/PaddlePaddle/ERNIE\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Github\" src=\"https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://ernie.baidu.com/blog/ernie4.5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/ğŸ––_Blog-ERNIE4.5-A020A0\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://discord.gg/JPmZXDsEEK\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://x.com/PaddlePaddle\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"X\" src=\"https://img.shields.io/badge/X-PaddlePaddle-6080F0\"?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"#license\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache2.0-A5de54\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n# ERNIE-4.5-21B-A3B-Thinking\n\n## Model Highlights\n\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\n* **Efficient tool usage** capabilities.\n* **Enhanced 128K long-context understanding** capabilities.\n\n> [!NOTE]\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\n\n![benchmark](./benchmark.png)\n\n## Model Overview\n\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\n\n|Key|Value|\n|-|-|\n|Modality|Text|\n|Training Stage|Posttraining|\n|Params(Total / Activated)|21B / 3B|\n|Layers|28|\n|Heads(Q/KV)|20 / 4|\n|Text Experts(Total / Activated)|64 / 6|\n|Shared Experts|2|\n|Context Length|131072|\n\n## Quickstart\n\n> [!NOTE]\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\n\n### FastDeploy Inference\n\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\n\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\n\n```bash\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --load-choices \"default_v1\" \\\n       --tensor-parallel-size 1 \\\n       --max-model-len 131072 \\\n       --reasoning-parser ernie_x1 \\\n       --tool-call-parser ernie_x1 \\\n       --max-num-seqs 32\n```\n\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\n\n```bash\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d $'{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How \\'s the weather in Beijing today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Determine weather in my location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\n                \"c\",\n                \"f\"\n              ]\n            }\n          },\n          \"additionalProperties\": false,\n          \"required\": [\n            \"location\",\n            \"unit\"\n          ]\n        },\n        \"strict\": true\n      }\n    }]\n}'\n```\n\n### vLLM inference\n\nVLLM>=0.10.2 (excluding 0.11.0)\n\n```bash\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\n```\n\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie need install vllm main branch\n\n### Using `transformers` library\n\n**Note**: You'll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"generate_text:\", generate_text)\n```\n\n## License\n\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\n\n## Citation\n\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\n\n```text\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu-ERNIE-Team},\n      year={2025},\n      primaryClass={cs.CL},\n      howpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\n}\n```\n\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":21825437888,\"storage_bytes\":43674659350,\"files_count\":21,\"spaces_count\":8,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Ernie4_5_MoeForCausalLM\"],\"model_type\":\"ernie4_5_moe\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"</s>\",\"pad_token\":\"<unk>\",\"unk_token\":\"<unk>\",\"cls_token\":\"<|begin_of_sentence|>\",\"sep_token\":\"<|end_of_sentence|>\",\"mask_token\":\"<mask:1>\",\"chat_template\":\"{{- '<|im_start|>system\\n' }}{%- if messages[0].role != 'system' and not system_settings %}{{- '<global_setting>\\nthink_mode=True\\n</global_setting>' }}{%- else%}{{- '<system_setting>\\n' }}{{- system_settings + '\\n' if system_settings else '' }}{{- (messages[0].content + '\\n' if messages[0].role == 'system' else '') + '</system_setting>\\n\\n<global_setting>\\nthink_mode=True\\n</global_setting>' }}{%- endif %}{%- if tools %}{{- \\\"\\n\\n<tool_list>\\\" }}{{- '\\n' }}{{-'['}}{% for tool in tools %}{{'{\\\"type\\\": \\\"function\\\", \\\"function\\\": '}}{{-(tool.function | tojson)}}}{%-if not loop.last%},{%- endif %}{%endfor%}{{-']'}}{{- \\\"\\n</tool_list>\\\" }}{%- endif %}{{-'<|im_end|>\\n\\n' }}{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_output>') and message.content.endswith('</tool_output>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.thoughts is string %}\\n            {%- set reasoning_content = message.thoughts %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index and  (loop.last or (not loop.last and reasoning_content)) %} {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n' }} {%- else %} {{- '<|im_start|>' + message.role + '\\n' }} {%- endif %}  {%- if content|length > 0 %}  {{- '<response>\\n' + content + '\\n</response>\\n' }}  {%- endif %} {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '\\n<tool_call>\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\n</tool_call>\\n' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\n\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>tool' }}\\n        {%- endif %}\\n        {{- '\\n<tool_output>' }}\\n        {{- message.content|tojson }}\\n        {{- '</tool_output>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\n\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n {{- \\\"<|im_start|>assistant\\n<think>\\n\\\"}}\"},\"chat_template_jinja\":\"{{- '<|im_start|>system\\n' }}{%- if messages[0].role != 'system' and not system_settings %}{{- '<global_setting>\\nthink_mode=True\\n</global_setting>' }}{%- else%}{{- '<system_setting>\\n' }}{{- system_settings + '\\n' if system_settings else '' }}{{- (messages[0].content + '\\n' if messages[0].role == 'system' else '') + '</system_setting>\\n\\n<global_setting>\\nthink_mode=True\\n</global_setting>' }}{%- endif %}{%- if tools %}{{- \\\"\\n\\n<tool_list>\\\" }}{{- '\\n' }}{{-'['}}{% for tool in tools %}{{'{\\\"type\\\": \\\"function\\\", \\\"function\\\": '}}{{-(tool.function | tojson)}}}{%-if not loop.last%},{%- endif %}{%endfor%}{{-']'}}{{- \\\"\\n</tool_list>\\\" }}{%- endif %}{{-'<|im_end|>\\n\\n' }}{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_output>') and message.content.endswith('</tool_output>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.thoughts is string %}\\n            {%- set reasoning_content = message.thoughts %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index and  (loop.last or (not loop.last and reasoning_content)) %} {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n' }} {%- else %} {{- '<|im_start|>' + message.role + '\\n' }} {%- endif %}  {%- if content|length > 0 %}  {{- '<response>\\n' + content + '\\n</response>\\n' }}  {%- endif %} {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '\\n<tool_call>\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\n</tool_call>\\n' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\n\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>tool' }}\\n        {%- endif %}\\n        {{- '\\n<tool_output>' }}\\n        {{- message.content|tojson }}\\n        {{- '</tool_output>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\n\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n {{- \\\"<|im_start|>assistant\\n<think>\\n\\\"}}\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:PaddlePaddle:ERNIE\\\"\",\"source_url\":\"https://github.com/PaddlePaddle/ERNIE\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:PaddlePaddle:FastDeploy\",\"source_url\":\"https://github.com/PaddlePaddle/FastDeploy\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 83.9,
    "content_hash": "5344f8a9cdfdc47a99ebceb2c13bf40d",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking/resolve/main/benchmark.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:shibing624:text2vec-base-chinese",
    "name": "text2vec-base-chinese",
    "author": "shibing624",
    "description": "--- license: apache-2.0 pipeline_tag: sentence-similarity tags: - Sentence Transformers - sentence-similarity - sentence-transformers datasets: - shibing624/nli_zh language: - zh library_name: sentence-transformers --- This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search. For an automated evaluation of this model, see the *Evaluat...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "openvino",
      "bert",
      "sentence transformers",
      "sentence-similarity",
      "zh",
      "dataset:shibing624/nli_zh",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "sentence-similarity",
    "likes": 765,
    "downloads": 502953,
    "source": "huggingface",
    "source_url": "https://huggingface.co/shibing624/text2vec-base-chinese",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\npipeline_tag: sentence-similarity\ntags:\n- Sentence Transformers\n- sentence-similarity\n- sentence-transformers\ndatasets:\n- shibing624/nli_zh\nlanguage:\n- zh\nlibrary_name: sentence-transformers\n---\n\n\n# shibing624/text2vec-base-chinese\nThis is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.\n\nIt maps sentences to a 768 dimensional dense vector space and can be used for tasks \nlike sentence embeddings, text matching or semantic search.\n\n\n## Evaluation\nFor an automated evaluation of this model, see the *Evaluation Benchmark*: [text2vec](https://github.com/shibing624/text2vec)\n\n- chinese text matching taskï¼š\n\n| Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |\n|:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|\n| Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |\n| SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |\n| Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |\n| CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |\n| CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  |    63.08  | 3066  |\n| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 4004  |\n\n\nè¯´æ˜ï¼š\n- ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°\n- `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨\n- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBERTè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰\n- `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ\n\n## Usage (text2vec)\nUsing this model becomes easy when you have [text2vec](https://github.com/shibing624/text2vec) installed:\n\n```\npip install -U text2vec\n```\n\nThen you can use the model like this:\n\n```python\nfrom text2vec import SentenceModel\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n\nmodel = SentenceModel('shibing624/text2vec-base-chinese')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nInstall transformers:\n```\npip install transformers\n```\n\nThen load model and predict:\n```python\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# Load model from HuggingFace Hub\ntokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = BertModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```\npip install -U sentence-transformers\n```\n\nThen load model and predict:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Model speed up\n\n\n| Model                                                                                                                        | ATEC              | BQ                | LCQMC            | PAWSX            | STSB             |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------|-------------------|------------------|------------------|------------------|\n| shibing624/text2vec-base-chinese (fp32, baseline)                                                                            | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (onnx-O4, [#29](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/29))    | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (ov, [#27](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/27))         | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (ov-qint8, [#30](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/30))   | 0.30778 (-3.60%)  | 0.43474 (+1.88%)  | 0.69620 (-0.77%) | 0.16662 (-3.20%) | 0.79396 (+0.13%) |\n\nIn short:\n1. âœ… shibing624/text2vec-base-chinese (onnx-O4), ONNX Optimized to [O4](https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/optimization) does not reduce performance, but gives a [~2x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on GPU.\n2. âœ… shibing624/text2vec-base-chinese (ov), OpenVINO does not reduce performance, but gives a 1.12x speedup on CPU.\n3. ğŸŸ¡ shibing624/text2vec-base-chinese (ov-qint8), int8 quantization with OV incurs a small performance hit on some tasks, and a tiny performance gain on others, when quantizing with [Chinese STSB](https://huggingface.co/datasets/PhilipMay/stsb_multi_mt). Additionally, it results in a [4.78x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on CPU.\n\n- usage: shibing624/text2vec-base-chinese (onnx-O4), for gpu\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"shibing624/text2vec-base-chinese\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"model_O4.onnx\"},\n)\nembeddings = model.encode([\"å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\", \"èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡\", \"ä½ æ˜¯è°\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n\n- usage: shibing624/text2vec-base-chinese (ov), for cpu\n```python\n# pip install 'optimum[openvino]'\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"shibing624/text2vec-base-chinese\",\n    backend=\"openvino\",\n)\n\nembeddings = model.encode([\"å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\", \"èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡\", \"ä½ æ˜¯è°\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n- usage: shibing624/text2vec-base-chinese (ov-qint8), for cpu\n```python\n# pip install optimum\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"shibing624/text2vec-base-chinese\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"model_qint8_avx512_vnni.onnx\"},\n)\nembeddings = model.encode([\"å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡\", \"èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡\", \"ä½ æ˜¯è°\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n\n## Full Model Architecture\n```\nCoSENT(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True})\n)\n```\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`hfl/chinese-macbert-base`](https://huggingface.co/hfl/chinese-macbert-base) model. \nPlease refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each \npossible sentence pairs from the batch.\nWe then apply the rank loss by comparing with true pairs and false pairs.\n\n#### Hyper parameters\n\n- training dataset: https://huggingface.co/datasets/shibing624/nli_zh\n- max_seq_length: 128\n- best epoch: 5\n- sentence embedding dim: 768\n\n\n\n## Citing & Authors\nThis model was trained by [text2vec](https://github.com/shibing624/text2vec). \n        \nIf you find this model helpful, feel free to cite:\n```bibtex \n@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"sentence-similarity\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":102268160,\"storage_bytes\":2040942336,\"files_count\":22,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BertModel\"],\"model_type\":\"bert\",\"tokenizer_config\":{\"unk_token\":\"[UNK]\",\"sep_token\":\"[SEP]\",\"pad_token\":\"[PAD]\",\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:UKPLab:sentence-transformers\",\"source_url\":\"https://github.com/UKPLab/sentence-transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec\",\"source_url\":\"https://github.com/shibing624/text2vec\"},{\"type\":\"has_code\",\"target_id\":\"github:shibing624:text2vec},\",\"source_url\":\"https://github.com/shibing624/text2vec},\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 78.8,
    "content_hash": "865e8389d8606a4d1fb1da99d3bdf79f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/shibing624/text2vec-base-chinese\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:latent-consistency:lcm-lora-sdxl",
    "name": "lcm-lora-sdxl",
    "author": "latent-consistency",
    "description": "--- library_name: diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 tags: - lora - text-to-image license: openrail++ inference: false --- Latent Consistency Model (LCM) LoRA was proposed in LCM-LoRA: A universal Stable-Diffusion Acceleration Module by *Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.* It is a distilled consistency adapter for [](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) that allows to reduce the number of inference steps to only between...",
    "tags": [
      "diffusers",
      "lora",
      "text-to-image",
      "arxiv:2311.05556",
      "license:openrail++",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 763,
    "downloads": 57298,
    "source": "huggingface",
    "source_url": "https://huggingface.co/latent-consistency/lcm-lora-sdxl",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\ntags:\n- lora\n- text-to-image\nlicense: openrail++\ninference: false\n---\n\n# Latent Consistency Model (LCM) LoRA: SDXL\n\nLatent Consistency Model (LCM) LoRA was proposed in [LCM-LoRA: A universal Stable-Diffusion Acceleration Module](https://arxiv.org/abs/2311.05556) \nby *Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.*\n\nIt is a distilled consistency adapter for [`stable-diffusion-xl-base-1.0`](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) that allows\nto reduce the number of inference steps to only between **2 - 8 steps**.\n\n| Model                                                                      | Params / M | \n|----------------------------------------------------------------------------|------------|\n| [lcm-lora-sdv1-5](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5)   | 67.5        |\n| [lcm-lora-ssd-1b](https://huggingface.co/latent-consistency/lcm-lora-ssd-1b)   | 105        |\n| [**lcm-lora-sdxl**](https://huggingface.co/latent-consistency/lcm-lora-sdxl) | **197M**  |\n\n## Usage\n\nLCM-LoRA is supported in ğŸ¤— Hugging Face Diffusers library from version v0.23.0 onwards. To run the model, first \ninstall the latest version of the Diffusers library as well as `peft`, `accelerate` and `transformers`.\naudio dataset from the Hugging Face Hub:\n\n```bash\npip install --upgrade pip\npip install --upgrade diffusers transformers accelerate peft\n```\n\n***Note: For detailed usage examples we recommend you to check out our official [LCM-LoRA docs](https://huggingface.co/docs/diffusers/main/en/using-diffusers/inference_with_lcm_lora)***\n\n### Text-to-Image\n\nThe adapter can be loaded with it's base model `stabilityai/stable-diffusion-xl-base-1.0`. Next, the scheduler needs to be changed to [`LCMScheduler`](https://huggingface.co/docs/diffusers/v0.22.3/en/api/schedulers/lcm#diffusers.LCMScheduler) and we can reduce the number of inference steps to just 2 to 8 steps.\nPlease make sure to either disable `guidance_scale` or use values between 1.0 and 2.0.\n\n```python\nimport torch\nfrom diffusers import LCMScheduler, AutoPipelineForText2Image\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nadapter_id = \"latent-consistency/lcm-lora-sdxl\"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.to(\"cuda\")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# disable guidance_scale by passing 0\nimage = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n```\n\n![](./image.png)\n\n### Inpainting\n\nLCM-LoRA can be used for inpainting as well. \n\n```python\nimport torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\npipe.fuse_lora()\n\n# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\").resize((1024, 1024))\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\").resize((1024, 1024))\n\nprompt = \"a castle on top of a mountain, highly detailed, 8k\"\ngenerator = torch.manual_seed(42)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=5,\n    guidance_scale=4,\n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_inpainting.png)\n\n\n## Combine with styled LoRAs\n\nLCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we'll use the LCM-LoRA with the [papercut LoRA](TheLastBen/Papercut_SDXL). \nTo learn more about how to combine LoRAs, refer to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).\n\n```python\nimport torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LoRAs\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n\n# Combine LoRAs\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\n\nprompt = \"papercut, a cute fox\"\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png)\n\n### ControlNet\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n).resize((1024, 1024))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0-small\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant=\"fp16\"\n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\npipe.fuse_lora()\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    \"picture of the mona lisa\",\n    image=canny_image,\n    num_inference_steps=5,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.5,\n    cross_attention_kwargs={\"scale\": 1},\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_controlnet.png)\n\n\n<Tip>\nThe inference parameters in this example might not work for all examples, so we recommend you to try different values for `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose the best one. \n</Tip>\n\n### T2I Adapter\n\nThis example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0) and SDXL.\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# Prepare image\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1024))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant=\"fp16\", \n).to(\"cuda\")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5, \n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2iadapter.png)\n\n\n## Speed Benchmark\n\nTODO\n\n## Training\n\nTODO\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":2761540034,\"files_count\":4,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.05556\",\"source_url\":\"https://arxiv.org/abs/2311.05556\"}]",
    "canonical_id": null,
    "license_spdx": "OpenRAIL++",
    "compliance_status": "approved",
    "quality_score": 63.8,
    "content_hash": "b42a74f884f906a10cedd69df8775b90",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/latent-consistency/lcm-lora-sdxl\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:wizardlmteam:wizardcoder-15b-v1.0",
    "name": "WizardCoder-15B-V1.0",
    "author": "WizardLMTeam",
    "description": "--- license: bigscience-openrail-m metrics: - code_eval library_name: transformers tags: - code model-index: - name: WizardCoder results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 0.573 verified: false --- <p style=\"font-size:28px;\" align=\"center\"> ğŸ  <a href=\"https://wizardlm.github.io/\" target=\"_blank\">Home Page</a> </p> <p align=\"center\"> <p align=\"center\"> ğŸ¤— <a href=\"https://huggingface.co/WizardLM\" target=\"_...",
    "tags": [
      "transformers",
      "pytorch",
      "gpt_bigcode",
      "text-generation",
      "code",
      "arxiv:2304.12244",
      "arxiv:2306.08568",
      "arxiv:2308.09583",
      "license:bigscience-openrail-m",
      "model-index",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 762,
    "downloads": 577,
    "source": "huggingface",
    "source_url": "https://huggingface.co/WizardLMTeam/WizardCoder-15B-V1.0",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: bigscience-openrail-m\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: WizardCoder\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.573\n      verified: false\n---\n \n## WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n<p style=\"font-size:28px;\" align=\"center\">\nğŸ  <a href=\"https://wizardlm.github.io/\" target=\"_blank\">Home Page</a> </p>\n<p align=\"center\">\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/WizardLM\" target=\"_blank\">HF Repo</a>  â€¢ğŸ± <a href=\"https://github.com/nlpxucan/WizardLM\" target=\"_blank\">Github Repo</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/WizardLM_AI\" target=\"_blank\">Twitter</a> </p>\n<p align=\"center\">\n ğŸ“ƒ <a href=\"https://arxiv.org/abs/2304.12244\" target=\"_blank\">[WizardLM]</a>  â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>   â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>  <br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ Join our <a href=\"https://discord.gg/VZjjHtWrKs\" target=\"_blank\">Discord</a>\n</p>\n\n## News\n\n[2024/01/04] ğŸ”¥ We released **WizardCoder-33B-V1.1**  trained from deepseek-coder-33b-base, the **SOTA OSS Code LLM** on [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html), achieves **79.9 pass@1** on HumanEval, **73.2 pass@1** on HumanEval-Plus, **78.9 pass@1** on MBPP, and **66.9 pass@1** on MBPP-Plus.\n\n[2024/01/04] ğŸ”¥ **WizardCoder-33B-V1.1** outperforms **ChatGPT 3.5**, **Gemini Pro**, and **DeepSeek-Coder-33B-instruct** on HumanEval and HumanEval-Plus pass@1.\n\n[2024/01/04] ğŸ”¥ **WizardCoder-33B-V1.1** is comparable with **ChatGPT 3.5**, and surpasses **Gemini Pro** on MBPP and MBPP-Plus pass@1.\n\n|  Model  |  Checkpoint  | Paper    | HumanEval  |   HumanEval+ | MBPP | MBPP+ | License |\n| ----- |------| ---- |------|-------| ----- |  ----- |----- | \n|  GPT-4-Turbo (Nov 2023)  | - | - | 85.4  | 81.7 | 83.0 | 70.7 |-|\n|  GPT-4 (May 2023)  | - | - | 88.4  | 76.8 | - | - |-|\n|  GPT-3.5-Turbo (Nov 2023)  | - | - | 72.6  | 65.9 | 81.7 | 69.4 |-|\n|  Gemini Pro  | - | - | 63.4  | 55.5 | 72.9 | 57.9 |-|\n|  DeepSeek-Coder-33B-instruct | - | - |  78.7 | 72.6 | 78.7 | 66.7 |-|\n|  **WizardCoder-33B-V1.1**  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-33B-V1.1\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  79.9  | 73.2 | 78.9 | 66.9 |  <a href=\"https://huggingface.co/WizardLM/WizardMath-7B-V1.1/resolve/main/LICENSE\" target=\"_blank\">MSFTResearch</a>  |\n|  WizardCoder-Python-34B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  73.2   | 64.6 | 73.2 | 59.9 |  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama2</a>  |\n|  WizardCoder-15B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-15B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  59.8   | 52.4 | -- | -- |  <a href=\"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\" target=\"_blank\">OpenRAIL-M</a>  |\n|  WizardCoder-Python-13B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  64.0   | -- | -- | -- |  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama2</a>  |\n|  WizardCoder-Python-7B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  55.5   | -- | -- | -- |  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama2</a>  |\n|  WizardCoder-3B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-3B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  34.8   | -- | -- | -- |  <a href=\"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\" target=\"_blank\">OpenRAIL-M</a>  |\n|  WizardCoder-1B-V1.0  |   ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardCoder-1B-V1.0\" target=\"_blank\">HF Link</a>   |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>  |  23.8   | -- | -- | -- |  <a href=\"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\" target=\"_blank\">OpenRAIL-M</a>  |\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/compare_sota.png\" alt=\"WizardCoder\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n- ğŸ”¥ [08/11/2023] We release **WizardMath** Models.\n- ğŸ”¥ Our **WizardMath-70B-V1.0** model slightly outperforms some closed-source LLMs on the GSM8K, including **ChatGPT 3.5**, **Claude Instant 1** and **PaLM 2 540B**.\n- ğŸ”¥ Our **WizardMath-70B-V1.0** model achieves  **81.6 pass@1** on the [GSM8k Benchmarks](https://github.com/openai/grade-school-math), which is **24.8** points higher than the SOTA open-source LLM.\n- ğŸ”¥ Our **WizardMath-70B-V1.0** model achieves  **22.7 pass@1** on the [MATH Benchmarks](https://github.com/hendrycks/math), which is **9.2** points higher than the SOTA open-source LLM.\n\n| Model | Checkpoint | Paper  | GSM8k | MATH  |Online Demo| License|\n| ----- |------| ---- |------|-------| ----- | ----- |\n| WizardMath-70B-V1.0 | ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardMath-70B-V1.0\" target=\"_blank\">HF Link</a> |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>| **81.6**  |  **22.7**\t|[Demo](http://47.103.63.15:50083/)| <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2  </a> |\n| WizardMath-13B-V1.0 | ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardMath-13B-V1.0\" target=\"_blank\">HF Link</a> |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>| **63.9**  |  **14.0** |[Demo](http://47.103.63.15:50082/)| <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2 </a> |\n| WizardMath-7B-V1.0 | ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardMath-7B-V1.0\" target=\"_blank\">HF Link</a>  |  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>| \t **54.9**  |  **10.7** | [Demo](http://47.103.63.15:50080/)|  <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2  </a>|    \n\n\n<font size=4>\n    \n| <sup>Model</sup> | <sup>Checkpoint</sup> | <sup>Paper</sup> |<sup>MT-Bench</sup> | <sup>AlpacaEval</sup> | <sup>WizardEval</sup> | <sup>HumanEval</sup>  | <sup>License</sup>|\n| ----- |------| ---- |------|-------| ----- | ----- | ----- |\n| <sup>WizardLM-13B-V1.2</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.2\" target=\"_blank\">HF Link</a> </sup>|  | <sup>7.06</sup> | <sup>89.17%</sup>\t | <sup>101.4% </sup>|<sup>36.6  pass@1</sup>|<sup> <a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.1</sup> |<sup> ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.1\" target=\"_blank\">HF Link</a> </sup> |  | <sup>6.76</sup>  |<sup>86.32%</sup>\t | <sup>99.3% </sup> |<sup>25.0  pass@1</sup>| <sup>Non-commercial</sup>|\n| <sup>WizardLM-30B-V1.0</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-30B-V1.0\" target=\"_blank\">HF Link</a></sup>  | | <sup>7.01</sup> |  |  <sup>97.8% </sup> | <sup>37.8  pass@1</sup>| <sup>Non-commercial</sup> |\n| <sup>WizardLM-13B-V1.0</sup> | <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.0\" target=\"_blank\">HF Link</a> </sup> |  | <sup>6.35</sup> | <sup>75.31%</sup> |  <sup>89.1% </sup> |<sup> 24.0 pass@1 </sup> | <sup>Non-commercial</sup>|\n| <sup>WizardLM-7B-V1.0 </sup>|  <sup>ğŸ¤— <a href=\"https://huggingface.co/WizardLM/WizardLM-7B-V1.0\" target=\"_blank\">HF Link</a> </sup> |<sup> ğŸ“ƒ <a href=\"https://arxiv.org/abs/2304.12244\" target=\"_blank\">[WizardLM]</a> </sup>|  |  |  <sup>78.0% </sup> |<sup>19.1 pass@1 </sup>|<sup> Non-commercial</sup>|\n</font>\n\n\n\n\n\n# WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n\nTo develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set.\n\n## News\n\n- ğŸ”¥ Our **WizardCoder-15B-v1.0** model achieves the **57.3 pass@1** on the [HumanEval Benchmarks](https://github.com/openai/human-eval), which is **22.3** points higher than the SOTA open-source Code LLMs.\n- ğŸ”¥ We released **WizardCoder-15B-v1.0** trained with **78k** evolved code instructions. Please checkout the [Model Weights](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0), and [Paper]().\n- &#x1F4E3; Please refer to our Twitter account https://twitter.com/WizardLM_AI and HuggingFace Repo https://huggingface.co/WizardLM . We will use them to announce any new release at the 1st time. \n\n\n## Comparing WizardCoder with the Closed-Source Models.\n\n\nğŸ”¥ The following figure shows that our **WizardCoder attains the third position in this benchmark**, surpassing Claude-Plus (59.8 vs. 53.0) and Bard (59.8 vs. 44.5). Notably, our model exhibits a substantially smaller size compared to these models.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/pass1.png\" alt=\"WizardCoder\" style=\"width: 86%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\nâ—**Note: In this study, we copy the scores for HumanEval and HumanEval+ from the [LLM-Humaneval-Benchmarks](https://github.com/my-other-github-account/llm-humaneval-benchmarks). Notably, all the mentioned models generate code solutions for each problem utilizing a **single attempt**, and the resulting pass rate percentage is reported. Our **WizardCoder** generates answers using greedy decoding and tests with the same [code](https://github.com/evalplus/evalplus).**\n\n## Comparing WizardCoder with the Open-Source Models.\n\nThe following table clearly demonstrates that our **WizardCoder** exhibits a substantial performance advantage over all the open-source models. â—**If you are confused with the different scores of our model (57.3 and 59.8), please check the Notes.**\n\n\n| Model            | HumanEval Pass@1 | MBPP Pass@1 |\n|------------------|------------------|-------------|\n| CodeGen-16B-Multi| 18.3             |20.9         |\n| CodeGeeX         | 22.9             |24.4         |\n| LLaMA-33B        | 21.7             |30.2         |\n| LLaMA-65B        | 23.7             |37.7         |\n| PaLM-540B        | 26.2             |36.8         |\n| PaLM-Coder-540B  | 36.0             |47.0         |\n| PaLM 2-S         | 37.6             |50.0         |\n| CodeGen-16B-Mono | 29.3             |35.3         |\n| Code-Cushman-001 | 33.5             |45.9         |\n| StarCoder-15B    | 33.6             |43.6*        |\n| InstructCodeT5+  | 35.0             |--           |\n| WizardLM-30B  1.0| 37.8             |--           |\n| WizardCoder-15B  1.0 | **57.3**     |**51.8**     |\n\n\nâ—**Note: The reproduced result of StarCoder on MBPP.**\n\nâ—**Note: The above table conducts a comprehensive comparison of our **WizardCoder** with other models on the HumanEval and MBPP benchmarks. We adhere to the approach outlined in previous studies by generating **20 samples** for each problem to estimate the pass@1 score and evaluate with the same [code](https://github.com/openai/human-eval/tree/master). The scores of GPT4 and GPT3.5 reported by [OpenAI](https://openai.com/research/gpt-4) are 67.0 and 48.1 (maybe these are the early version GPT4&3.5).**\n\n## Call for Feedbacks\nWe welcome everyone to use your professional and difficult instructions to evaluate WizardCoder, and show us examples of poor performance and your suggestions in the [issue discussion](https://github.com/nlpxucan/WizardLM/issues) area. We are focusing on improving the Evol-Instruct now and hope to relieve existing weaknesses and issues in the the next version of WizardCoder. After that, we will open the code and pipeline of up-to-date Evol-Instruct algorithm and work with you together to improve it.\n\n\n## Contents\n\n1. [Online Demo](#online-demo)\n\n2. [Fine-tuning](#fine-tuning)\n\n3. [Inference](#inference)\n\n4. [Evaluation](#evaluation)\n\n5. [Citation](#citation)\n\n6. [Disclaimer](#disclaimer)\n\n## Online Demo\n\nWe will provide our latest models for you to try for as long as possible. If you find a link is not working, please try another one. At the same time, please try as many **real-world** and **challenging** code-related problems that you encounter in your work and life as possible. We will continue to evolve our models with your feedbacks.\n\n\n\n## Fine-tuning\n\nWe fine-tune WizardCoder using the modified code `train.py` from [Llama-X](https://github.com/AetherCortex/Llama-X).\nWe fine-tune StarCoder-15B with the following hyperparameters:\n\n| Hyperparameter | StarCoder-15B |\n|----------------|---------------|\n| Batch size     | 512           |\n| Learning rate  | 2e-5          |\n| Epochs         | 3             |\n| Max length     | 2048          |\n| Warmup step    | 30            |\n| LR scheduler   | cosine        |\n\nTo reproduce our fine-tuning of WizardCoder, please follow the following steps:\n1. According to the instructions of [Llama-X](https://github.com/AetherCortex/Llama-X), install the environment, download the training code, and deploy. (Note: `deepspeed==0.9.2` and `transformers==4.29.2`)\n2. Replace the `train.py` with the `train_wizardcoder.py` in our repo (`src/train_wizardcoder.py`)\n3. Login Huggingface:\n```bash\nhuggingface-cli login\n```\n4. Execute the following training command:\n```bash\ndeepspeed train_wizardcoder.py \\\n    --model_name_or_path \"bigcode/starcoder\" \\\n    --data_path \"/your/path/to/code_instruction_data.json\" \\\n    --output_dir \"/your/path/to/ckpt\" \\\n    --num_train_epochs 3 \\\n    --model_max_length 2048 \\\n    --per_device_train_batch_size 16 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 50 \\\n    --save_total_limit 2 \\\n    --learning_rate 2e-5 \\\n    --warmup_steps 30 \\\n    --logging_steps 2 \\\n    --lr_scheduler_type \"cosine\" \\\n    --report_to \"tensorboard\" \\\n    --gradient_checkpointing True \\\n    --deepspeed configs/deepspeed_config.json \\\n    --fp16 True\n```\n\n## Inference\n\nWe provide the decoding script for WizardCoder, which reads a input file and generates corresponding responses for each sample, and finally consolidates them into an output file.\n\nYou can specify `base_model`, `input_data_path` and `output_data_path` in `src\\inference_wizardcoder.py` to set the decoding model, path of input file and path of output file.\n\n```bash\npip install jsonlines\n```\n\nThe decoding command is:\n```\npython src\\inference_wizardcoder.py \\\n    --base_model \"/your/path/to/ckpt\" \\\n    --input_data_path \"/your/path/to/input/data.jsonl\" \\\n    --output_data_path \"/your/path/to/output/result.jsonl\"\n```\n\nThe format of `data.jsonl` should be:\n```\n{\"idx\": 11, \"Instruction\": \"Write a Python code to count 1 to 10.\"}\n{\"idx\": 12, \"Instruction\": \"Write a Jave code to sum 1 to 10.\"}\n```\n\nThe prompt for our WizardCoder in `src\\inference_wizardcoder.py` is:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n```\n\n## Evaluation\n\nWe provide the evaluation script on HumanEval for WizardCoder.\n\n1. According to the instructions of [HumanEval](https://github.com/openai/human-eval), install the environment.\n2. Run the following script to generate the answer.\n```bash\nmodel=\"/path/to/your/model\"\ntemp=0.2\nmax_len=2048\npred_num=200\nnum_seqs_per_iter=2\n\noutput_path=preds/T${temp}_N${pred_num}\n\nmkdir -p ${output_path}\necho 'Output path: '$output_path\necho 'Model to eval: '$model\n\n# 164 problems, 21 per GPU if GPU=8\nindex=0\ngpu_num=8\nfor ((i = 0; i < $gpu_num; i++)); do\n  start_index=$((i * 21))\n  end_index=$(((i + 1) * 21))\n\n  gpu=$((i))\n  echo 'Running process #' ${i} 'from' $start_index 'to' $end_index 'on GPU' ${gpu}\n  ((index++))\n  (\n    CUDA_VISIBLE_DEVICES=$gpu python humaneval_gen.py --model ${model} \\\n      --start_index ${start_index} --end_index ${end_index} --temperature ${temp} \\\n      --num_seqs_per_iter ${num_seqs_per_iter} --N ${pred_num} --max_len ${max_len} --output_path ${output_path}\n  ) &\n  if (($index % $gpu_num == 0)); then wait; fi\ndone\n```\n3. Run the post processing code `src/process_humaneval.py` to collect the code completions from all answer files.\n```bash\noutput_path=preds/T${temp}_N${pred_num}\n\necho 'Output path: '$output_path\npython process_humaneval.py --path ${output_path} --out_path ${output_path}.jsonl --add_prompt\n\nevaluate_functional_correctness ${output_path}.jsonl\n```\n\n## Citation\n\nPlease cite the repo if you use the data, method or code in this repo.\n\n```\n@article{luo2023wizardcoder,\n  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},\n  journal={arXiv preprint arXiv:2306.08568},\n  year={2023}\n}\n```\n## Disclaimer\n\nWizardCoder model follows the same license as StarCoder. The content produced by any version of WizardCoder is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project. This project does not accept any legal liability for the content of the model output, nor does it assume responsibility for any losses incurred due to the use of associated resources and output results.",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":62070055381,\"files_count\":11,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"GPTBigCodeForCausalLM\"],\"model_type\":\"gpt_bigcode\",\"tokenizer_config\":{\"bos_token\":\"<|endoftext|>\",\"eos_token\":\"<|endoftext|>\",\"unk_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:nlpxucan:WizardLM\\\"\",\"source_url\":\"https://github.com/nlpxucan/WizardLM\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:grade-school-math\",\"source_url\":\"https://github.com/openai/grade-school-math\"},{\"type\":\"has_code\",\"target_id\":\"github:hendrycks:math\",\"source_url\":\"https://github.com/hendrycks/math\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:human-eval\",\"source_url\":\"https://github.com/openai/human-eval\"},{\"type\":\"has_code\",\"target_id\":\"github:my-other-github-account:llm-humaneval-benchmarks\",\"source_url\":\"https://github.com/my-other-github-account/llm-humaneval-benchmarks\"},{\"type\":\"has_code\",\"target_id\":\"github:evalplus:evalplus\",\"source_url\":\"https://github.com/evalplus/evalplus\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:human-eval\",\"source_url\":\"https://github.com/openai/human-eval\"},{\"type\":\"has_code\",\"target_id\":\"github:nlpxucan:WizardLM\",\"source_url\":\"https://github.com/nlpxucan/WizardLM\"},{\"type\":\"has_code\",\"target_id\":\"github:AetherCortex:Llama-X\",\"source_url\":\"https://github.com/AetherCortex/Llama-X\"},{\"type\":\"has_code\",\"target_id\":\"github:AetherCortex:Llama-X\",\"source_url\":\"https://github.com/AetherCortex/Llama-X\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:human-eval\",\"source_url\":\"https://github.com/openai/human-eval\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.12244\",\"source_url\":\"https://arxiv.org/abs/2304.12244\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.08568\",\"source_url\":\"https://arxiv.org/abs/2306.08568\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2308.09583\",\"source_url\":\"https://arxiv.org/abs/2308.09583\"}]",
    "canonical_id": null,
    "license_spdx": "BigScience-OpenRAIL-M",
    "compliance_status": "approved",
    "quality_score": 78.8,
    "content_hash": "1a87111d92467fe6ff77067c5ba98e13",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/WizardLMTeam/WizardCoder-15B-V1.0\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-r1-distill-qwen-7b",
    "name": "DeepSeek-R1-Distill-Qwen-7B",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/d...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 762,
    "downloads": 912993,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/figures/benchmark.jpg",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>ğŸ‘ï¸</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7615616512,\"storage_bytes\":15231404337,\"files_count\":11,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ï½œAssistantï½œ><ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>' + message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ï½œAssistantï½œ>' + content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ï½œAssistantï½œ><think>\\\\n'}}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.12948\",\"source_url\":\"https://arxiv.org/abs/2501.12948\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 98.8,
    "content_hash": "a69d43aaaa6ca87c00d3b1c409ddb6b7",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/figures/benchmark.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\"fetched_at\":\"2025-12-10T01:31:39.552Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]