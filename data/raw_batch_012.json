[
  {
    "id": "arxiv:2512.07564v1",
    "name": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models",
    "author": "Kassoum Sanogo",
    "description": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with fro...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "arxiv:cs.CL",
      "arxiv:cs.LG",
      "vision",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07564v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.",
    "meta_json": "{\"arxiv_id\":\"2512.07564v1\",\"authors\":[\"Kassoum Sanogo\",\"Renzo Ardiccioni\"],\"categories\":[\"cs.CV\",\"cs.AI\",\"cs.CL\",\"cs.LG\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:58:46Z\",\"updated_date\":\"2025-12-08T13:58:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "4a09aa58bdd41cd40cc6f18a6542abd9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07564v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07557v1",
    "name": "On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series",
    "author": "Jitendra K. Tugnait",
    "description": "Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.LG",
      "arxiv:eess.SP"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07557v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.",
    "meta_json": "{\"arxiv_id\":\"2512.07557v1\",\"authors\":[\"Jitendra K. Tugnait\"],\"categories\":[\"stat.ML\",\"cs.LG\",\"eess.SP\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:48:25Z\",\"updated_date\":\"2025-12-08T13:48:25Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.SP\",\"source_url\":\"https://arxiv.org/abs/eess.SP\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "551e9c669822acbb8d33fa2f7268e25e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07557v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07552v1",
    "name": "Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries",
    "author": "Francois Vandenhende",
    "description": "In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. Th...",
    "tags": [
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07552v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.",
    "meta_json": "{\"arxiv_id\":\"2512.07552v1\",\"authors\":[\"Francois Vandenhende\",\"Anna Georgiou\",\"Michalis Georgiou\",\"Theodoros Psaras\",\"Ellie Karekla\",\"Elena Hadjicosta\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:32:20Z\",\"updated_date\":\"2025-12-08T13:32:20Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "172363f9fe34c28b2386794c77e916ee",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07552v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07542v1",
    "name": "RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems",
    "author": "Jad Mounayer",
    "description": "Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent...",
    "tags": [
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07542v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.",
    "meta_json": "{\"arxiv_id\":\"2512.07542v1\",\"authors\":[\"Jad Mounayer\",\"Sebastian Rodriguez\",\"Jerome Tomezyk\",\"Chady Ghnatios\",\"Francisco Chinesta\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:23:12Z\",\"updated_date\":\"2025-12-08T13:23:12Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:JadM133:RRAEDy.\",\"source_url\":\"https://github.com/JadM133/RRAEDy.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "1606769c002beedc48e2d90f285d55a1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07542v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07541v1",
    "name": "High-Dimensional Change Point Detection using Graph Spanning Ratio",
    "author": "Youngwen Sun",
    "description": "Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07541v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.",
    "meta_json": "{\"arxiv_id\":\"2512.07541v1\",\"authors\":[\"Youngwen Sun\",\"Katerina Papagiannouli\",\"Vladimir Spokoiny\"],\"categories\":[\"stat.ML\",\"cs.LG\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:22:25Z\",\"updated_date\":\"2025-12-08T13:22:25Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "002613183f941f9c877649e2cebff0d7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07541v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07540v1",
    "name": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation",
    "author": "Boxuan Lyu",
    "description": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We add...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07540v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.",
    "meta_json": "{\"arxiv_id\":\"2512.07540v1\",\"authors\":[\"Boxuan Lyu\",\"Haiyue Song\",\"Hidetaka Kamigaito\",\"Chenchen Ding\",\"Hideki Tanaka\",\"Masao Utiyama\",\"Kotaro Funakoshi\",\"Manabu Okumura\"],\"categories\":[\"cs.CL\",\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:21:44Z\",\"updated_date\":\"2025-12-08T13:21:44Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "7f08b2a924a31c7d1d9ceccd5b73c439",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07540v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07539v1",
    "name": "FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting",
    "author": "Qingyuan Yang",
    "description": "Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving ...",
    "tags": [
      "arxiv:cs.LG",
      "attention"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07539v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.",
    "meta_json": "{\"arxiv_id\":\"2512.07539v1\",\"authors\":[\"Qingyuan Yang\",\"Shizhuo\",\"Dongyue Chen\",\"Da Teng\",\"Zehua Gan\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:18:20Z\",\"updated_date\":\"2025-12-08T13:18:20Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:yangqingyuan-byte:FRWKV.\",\"source_url\":\"https://github.com/yangqingyuan-byte/FRWKV.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "fedfa5ba3a092fc12556137c5eb263f4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07539v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07538v1",
    "name": "SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents",
    "author": "Michelle Wastl",
    "description": "Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-leve...",
    "tags": [
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07538v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.",
    "meta_json": "{\"arxiv_id\":\"2512.07538v1\",\"authors\":[\"Michelle Wastl\",\"Jannis Vamvas\",\"Rico Sennrich\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:17:27Z\",\"updated_date\":\"2025-12-08T13:17:27Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "972395049d0cc4660a009e33851bf7c5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07538v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07533v1",
    "name": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection",
    "author": "Yuzhou Nie",
    "description": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with ...",
    "tags": [
      "arxiv:cs.CR",
      "arxiv:cs.AI",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07533v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.",
    "meta_json": "{\"arxiv_id\":\"2512.07533v1\",\"authors\":[\"Yuzhou Nie\",\"Hongwei Li\",\"Chengquan Guo\",\"Ruizhe Jiang\",\"Zhun Wang\",\"Bo Li\",\"Dawn Song\",\"Wenbo Guo\"],\"categories\":[\"cs.CR\",\"cs.AI\"],\"primary_category\":\"cs.CR\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:06:23Z\",\"updated_date\":\"2025-12-08T13:06:23Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ucsb-mlsec:VulnLLM-R}{github}.\",\"source_url\":\"https://github.com/ucsb-mlsec/VulnLLM-R}{github}.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CR\",\"source_url\":\"https://arxiv.org/abs/cs.CR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "8b6cc387d2f75e294bb68a06fd43e98c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07533v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07528v1",
    "name": "Model-Based Reinforcement Learning Under Confounding",
    "author": "Nishanth Venkatesh",
    "description": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy ev...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07528v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.",
    "meta_json": "{\"arxiv_id\":\"2512.07528v1\",\"authors\":[\"Nishanth Venkatesh\",\"Andreas A. Malikopoulos\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:02:00Z\",\"updated_date\":\"2025-12-08T13:02:00Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "567b24025668f6204e612b936f9986f7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07528v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07527v1",
    "name": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images",
    "author": "Fei Yu",
    "description": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail. To address this problem, we propose two design choices tailored for...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.GR"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07527v1",
    "image_url": null,
    "type": "paper",
    "body_content": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail. To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs. Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.",
    "meta_json": "{\"arxiv_id\":\"2512.07527v1\",\"authors\":[\"Fei Yu\",\"Yu Liu\",\"Luyang Tang\",\"Mingchao Sun\",\"Zengye Ge\",\"Rui Bu\",\"Yuchao Jin\",\"Haisen Zhao\",\"He Sun\",\"Yangyan Li\",\"Mu Xu\",\"Wenzheng Chen\",\"Baoquan Chen\"],\"categories\":[\"cs.CV\",\"cs.GR\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T13:01:12Z\",\"updated_date\":\"2025-12-08T13:01:12Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.GR\",\"source_url\":\"https://arxiv.org/abs/cs.GR\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "fdb3f3b27db116ea3a1c1f792e75f6ba",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07527v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07525v1",
    "name": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
    "author": "Xiaoran Liu",
    "description": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-cont...",
    "tags": [
      "arxiv:cs.CL",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07525v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.",
    "meta_json": "{\"arxiv_id\":\"2512.07525v1\",\"authors\":[\"Xiaoran Liu\",\"Yuerong Song\",\"Zhigeng Liu\",\"Zengfeng Huang\",\"Qipeng Guo\",\"Zhaoxiang Liu\",\"Shiguo Lian\",\"Ziwei He\",\"Xipeng Qiu\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:59:54Z\",\"updated_date\":\"2025-12-08T12:59:54Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:OpenMOSS:rope_pp.\",\"source_url\":\"https://github.com/OpenMOSS/rope_pp.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "580b9c09dff83208b8581600fed645aa",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07525v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07522v1",
    "name": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings",
    "author": "Sebastian Sztwiertnia",
    "description": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substanti...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07522v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.",
    "meta_json": "{\"arxiv_id\":\"2512.07522v1\",\"authors\":[\"Sebastian Sztwiertnia\",\"Felix Friedrich\",\"Kristian Kersting\",\"Patrick Schramowski\",\"Bj√∂rn Deiseroth\"],\"categories\":[\"cs.CL\",\"cs.AI\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:59:24Z\",\"updated_date\":\"2025-12-08T12:59:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "fed0744db622167fe63d4e9b7373c95d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07522v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07519v1",
    "name": "Machine Learning: Progress and Prospects",
    "author": "Alexander Gammerman",
    "description": "This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers. When did machine learning start? Maybe a good starting point is 1...",
    "tags": [
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07519v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers. When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of \"simplicity\" known as \"Ockham's razor\" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that \"we learn some things only by doing things\". The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.",
    "meta_json": "{\"arxiv_id\":\"2512.07519v1\",\"authors\":[\"Alexander Gammerman\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:57:16Z\",\"updated_date\":\"2025-12-08T12:57:16Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "b5dc0ce409b99d299713f4822befdf8d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07519v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07515v1",
    "name": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG",
    "author": "Pengqian Lu",
    "description": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, w...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07515v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance",
    "meta_json": "{\"arxiv_id\":\"2512.07515v1\",\"authors\":[\"Pengqian Lu\",\"Jie Lu\",\"Anjin Liu\",\"Guangquan Zhang\"],\"categories\":[\"cs.CL\",\"cs.AI\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:50:41Z\",\"updated_date\":\"2025-12-08T12:50:41Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "3c0742476f7b0e2ca4ccff4d2d9822b5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07515v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07514v1",
    "name": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes",
    "author": "Junkai Lin",
    "description": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on thre...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07514v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.",
    "meta_json": "{\"arxiv_id\":\"2512.07514v1\",\"authors\":[\"Junkai Lin\",\"Hang Long\",\"Huipeng Guo\",\"Jielei Zhang\",\"JiaYi Yang\",\"Tianle Guo\",\"Yang Yang\",\"Jianwen Li\",\"Wenxiao Zhang\",\"Matthias Nie√üner\",\"Wei Yang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:50:23Z\",\"updated_date\":\"2025-12-08T12:50:23Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "469974977a1e5b9868d594877f935b78",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07514v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07509v1",
    "name": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces",
    "author": "Nikita Gabdullin",
    "description": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large ...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "arxiv:cs.CV",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07509v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.",
    "meta_json": "{\"arxiv_id\":\"2512.07509v1\",\"authors\":[\"Nikita Gabdullin\"],\"categories\":[\"cs.LG\",\"cs.AI\",\"cs.CV\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:46:39Z\",\"updated_date\":\"2025-12-08T12:46:39Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "b1b29452163c56e7511945c4baf706fa",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07509v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07504v1",
    "name": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points",
    "author": "Ryota Okumura",
    "description": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framewor...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07504v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .",
    "meta_json": "{\"arxiv_id\":\"2512.07504v1\",\"authors\":[\"Ryota Okumura\",\"Kaede Shiohara\",\"Toshihiko Yamasaki\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:38:11Z\",\"updated_date\":\"2025-12-08T12:38:11Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:RyotaOkumura:ControlVP\",\"source_url\":\"https://github.com/RyotaOkumura/ControlVP\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "ea160a855847fedac998e813e35925bf",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07504v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07503v1",
    "name": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation",
    "author": "Yao Teng",
    "description": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forw...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07503v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.",
    "meta_json": "{\"arxiv_id\":\"2512.07503v1\",\"authors\":[\"Yao Teng\",\"Zhihuan Jiang\",\"Han Shi\",\"Xian Liu\",\"Xuefei Ning\",\"Guohao Dai\",\"Yu Wang\",\"Zhenguo Li\",\"Xihui Liu\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:36:43Z\",\"updated_date\":\"2025-12-08T12:36:43Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "40c1b1e1325428dc32b882269160acf8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07503v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07501v1",
    "name": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution",
    "author": "Weilin Luo",
    "description": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize impli...",
    "tags": [
      "arxiv:cs.SE",
      "arxiv:cs.AI",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07501v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.",
    "meta_json": "{\"arxiv_id\":\"2512.07501v1\",\"authors\":[\"Weilin Luo\",\"Xueyi Liang\",\"Haotian Deng\",\"Yanan Liu\",\"Hai Wan\"],\"categories\":[\"cs.SE\",\"cs.AI\"],\"primary_category\":\"cs.SE\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:35:10Z\",\"updated_date\":\"2025-12-08T12:35:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SE\",\"source_url\":\"https://arxiv.org/abs/cs.SE\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "def5c1e9474d50b1f46bc6b393ee3f5b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07501v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07497v1",
    "name": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations",
    "author": "JV Roig",
    "description": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.SE",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07497v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",
    "meta_json": "{\"arxiv_id\":\"2512.07497v1\",\"authors\":[\"JV Roig\"],\"categories\":[\"cs.AI\",\"cs.SE\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:27:15Z\",\"updated_date\":\"2025-12-08T12:27:15Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SE\",\"source_url\":\"https://arxiv.org/abs/cs.SE\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "564fbff62c14680158604521d29a9698",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07497v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07490v1",
    "name": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent",
    "author": "Zhiyu Liu",
    "description": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient d...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:math.OC"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07490v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.",
    "meta_json": "{\"arxiv_id\":\"2512.07490v1\",\"authors\":[\"Zhiyu Liu\",\"Zhi Han\",\"Yandong Tang\",\"Jun Fan\",\"Yao Wang\"],\"categories\":[\"cs.LG\",\"math.OC\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:17:40Z\",\"updated_date\":\"2025-12-08T12:17:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:math.OC\",\"source_url\":\"https://arxiv.org/abs/math.OC\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "bb2855c5a88ab9d1d1d7d3f7f66189a3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07490v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07487v1",
    "name": "Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility",
    "author": "David M. Allison",
    "description": "A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the ...",
    "tags": [
      "arxiv:cs.CY",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07487v1",
    "image_url": null,
    "type": "paper",
    "body_content": "A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.",
    "meta_json": "{\"arxiv_id\":\"2512.07487v1\",\"authors\":[\"David M. Allison\",\"Stephen Herzog\"],\"categories\":[\"cs.CY\",\"cs.AI\"],\"primary_category\":\"cs.CY\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:14:51Z\",\"updated_date\":\"2025-12-08T12:14:51Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CY\",\"source_url\":\"https://arxiv.org/abs/cs.CY\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "e33f88cf6e29b5d7ed9009940f8e5f8c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07487v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07486v1",
    "name": "Materium: An Autoregressive Approach for Material Generation",
    "author": "Niklas Dobberstein",
    "description": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cond-mat.mtrl-sci"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07486v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.",
    "meta_json": "{\"arxiv_id\":\"2512.07486v1\",\"authors\":[\"Niklas Dobberstein\",\"Jan Hamaekers\"],\"categories\":[\"cs.LG\",\"cond-mat.mtrl-sci\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:14:42Z\",\"updated_date\":\"2025-12-08T12:14:42Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cond-mat.mtrl-sci\",\"source_url\":\"https://arxiv.org/abs/cond-mat.mtrl-sci\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "5b9278174087cb2b80b52d2d90f6073c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07486v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07482v1",
    "name": "From Real-World Traffic Data to Relevant Critical Scenarios",
    "author": "Florian L√ºttner",
    "description": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the num...",
    "tags": [
      "arxiv:cs.RO",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07482v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.",
    "meta_json": "{\"arxiv_id\":\"2512.07482v1\",\"authors\":[\"Florian L√ºttner\",\"Nicole Neis\",\"Daniel Stadler\",\"Robin Moss\",\"Mirjam Fehling-Kaschek\",\"Matthias Pfriem\",\"Alexander Stolz\",\"Jens Ziehn\"],\"categories\":[\"cs.RO\",\"cs.AI\"],\"primary_category\":\"cs.RO\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:07:15Z\",\"updated_date\":\"2025-12-08T12:07:15Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "172f146c71eb8da9af0272ec46f01807",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07482v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07480v1",
    "name": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance",
    "author": "Naifu Xue",
    "description": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec...",
    "tags": [
      "arxiv:cs.CV",
      "diffusion"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07480v1",
    "image_url": null,
    "type": "paper",
    "body_content": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.",
    "meta_json": "{\"arxiv_id\":\"2512.07480v1\",\"authors\":[\"Naifu Xue\",\"Zhaoyang Jia\",\"Jiahao Li\",\"Bin Li\",\"Zihan Zheng\",\"Yuan Zhang\",\"Yan Lu\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T12:05:30Z\",\"updated_date\":\"2025-12-08T12:05:30Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "ca3c4b78b1c3f364458937a80801cf77",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07480v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07478v1",
    "name": "Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization",
    "author": "Zhuoran Zhuang",
    "description": "Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergenc...",
    "tags": [
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07478v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.",
    "meta_json": "{\"arxiv_id\":\"2512.07478v1\",\"authors\":[\"Zhuoran Zhuang\",\"Ye Chen\",\"Jianghao Su\",\"Chao Luo\",\"Luhui Liu\",\"Xia Zeng\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:59:25Z\",\"updated_date\":\"2025-12-08T11:59:25Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a0ade35a55457fd8dfee03be1ca14000",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07478v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07474v1",
    "name": "Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels",
    "author": "Yifei Huang",
    "description": "We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness ...",
    "tags": [
      "arxiv:cs.HC",
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07474v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.",
    "meta_json": "{\"arxiv_id\":\"2512.07474v1\",\"authors\":[\"Yifei Huang\",\"Tianyu Yan\",\"Sitong Gong\",\"Xiwei Gao\",\"Caixin Kang\",\"Ruicong Liu\",\"Huchuan Lu\",\"Bo Zheng\"],\"categories\":[\"cs.HC\",\"cs.CL\"],\"primary_category\":\"cs.HC\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:57:46Z\",\"updated_date\":\"2025-12-08T11:57:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.HC\",\"source_url\":\"https://arxiv.org/abs/cs.HC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "028b2ca487ea6b0eff84d13cf8df617c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07474v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07463v1",
    "name": "Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification",
    "author": "Rongmei Liang",
    "description": "In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not on...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:stat.AP",
      "arxiv:stat.CO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07463v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.",
    "meta_json": "{\"arxiv_id\":\"2512.07463v1\",\"authors\":[\"Rongmei Liang\",\"Zizheng Liu\",\"Xiaofei Wu\",\"Jingwen Tu\"],\"categories\":[\"cs.LG\",\"stat.AP\",\"stat.CO\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:41:06Z\",\"updated_date\":\"2025-12-08T11:41:06Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.AP\",\"source_url\":\"https://arxiv.org/abs/stat.AP\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.CO\",\"source_url\":\"https://arxiv.org/abs/stat.CO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "8a3c8214ebf93a5786c320670984266b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07463v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07462v1",
    "name": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics",
    "author": "Trung-Kiet Huynh",
    "description": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systema...",
    "tags": [
      "arxiv:cs.MA",
      "arxiv:cs.AI",
      "arxiv:cs.GT",
      "arxiv:cs.LG",
      "arxiv:math.DS",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07462v1",
    "image_url": null,
    "type": "paper",
    "body_content": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.",
    "meta_json": "{\"arxiv_id\":\"2512.07462v1\",\"authors\":[\"Trung-Kiet Huynh\",\"Duy-Minh Dao-Sy\",\"Thanh-Bang Cao\",\"Phong-Hao Le\",\"Hong-Dan Nguyen\",\"Phu-Quy Nguyen-Lam\",\"Minh-Luan Nguyen-Vo\",\"Hong-Phat Pham\",\"Phu-Hoa Pham\",\"Thien-Kim Than\",\"Chi-Nguyen Tran\",\"Huy Tran\",\"Gia-Thoai Tran-Le\",\"Alessio Buscemi\",\"Le Hong Trang\",\"The Anh Han\"],\"categories\":[\"cs.MA\",\"cs.AI\",\"cs.GT\",\"cs.LG\",\"math.DS\"],\"primary_category\":\"cs.MA\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:40:03Z\",\"updated_date\":\"2025-12-08T11:40:03Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.MA\",\"source_url\":\"https://arxiv.org/abs/cs.MA\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.GT\",\"source_url\":\"https://arxiv.org/abs/cs.GT\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:math.DS\",\"source_url\":\"https://arxiv.org/abs/math.DS\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "85db3b0f10e9a2a2d5c388afa89e6db6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07462v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07461v1",
    "name": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "author": "Tong Wu",
    "description": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAP...",
    "tags": [
      "arxiv:cs.CL",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07461v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.",
    "meta_json": "{\"arxiv_id\":\"2512.07461v1\",\"authors\":[\"Tong Wu\",\"Yang Liu\",\"Jun Bai\",\"Zixia Jia\",\"Shuyi Zhang\",\"Ziyong Lin\",\"Yanting Wang\",\"Song-Chun Zhu\",\"Zilong Zheng\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:39:43Z\",\"updated_date\":\"2025-12-08T11:39:43Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a0f5ba287f29d43e3d0efa91796ad8de",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07461v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07459v1",
    "name": "Human Geometry Distribution for 3D Animation Generation",
    "author": "Xiangjun Tang",
    "description": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a...",
    "tags": [
      "arxiv:cs.GR",
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07459v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \\times$ higher user study score), achieving the best results across all evaluation metrics.",
    "meta_json": "{\"arxiv_id\":\"2512.07459v1\",\"authors\":[\"Xiangjun Tang\",\"Biao Zhang\",\"Peter Wonka\"],\"categories\":[\"cs.GR\",\"cs.CV\"],\"primary_category\":\"cs.GR\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:35:16Z\",\"updated_date\":\"2025-12-08T11:35:16Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.GR\",\"source_url\":\"https://arxiv.org/abs/cs.GR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "abefb5e99916516c15155610221e71ad",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07459v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07458v1",
    "name": "Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems",
    "author": "Dmitrii Kapitan",
    "description": "This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states...",
    "tags": [
      "arxiv:physics.comp-ph",
      "arxiv:cond-mat.dis-nn",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07458v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.",
    "meta_json": "{\"arxiv_id\":\"2512.07458v1\",\"authors\":[\"Dmitrii Kapitan\",\"Pavel Ovchinnikov\",\"Konstantin Soldatov\",\"Petr Andriushchenko\",\"Vitalii Kapitan\"],\"categories\":[\"physics.comp-ph\",\"cond-mat.dis-nn\",\"cs.LG\"],\"primary_category\":\"physics.comp-ph\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:33:24Z\",\"updated_date\":\"2025-12-08T11:33:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:physics.comp-ph\",\"source_url\":\"https://arxiv.org/abs/physics.comp-ph\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cond-mat.dis-nn\",\"source_url\":\"https://arxiv.org/abs/cond-mat.dis-nn\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "036d7ad509240a2e280fdaf238f955ab",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07458v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07454v1",
    "name": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning",
    "author": "Amir Mohammad Akhlaghi",
    "description": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient ...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07454v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.",
    "meta_json": "{\"arxiv_id\":\"2512.07454v1\",\"authors\":[\"Amir Mohammad Akhlaghi\",\"Amirhossein Shabani\",\"Mostafa Abdolmaleki\",\"Saeed Reza Kheradpisheh\"],\"categories\":[\"cs.CL\",\"cs.AI\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:27:52Z\",\"updated_date\":\"2025-12-08T11:27:52Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a50c264a2cfe48c8048d2771a522a9fb",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07454v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07453v1",
    "name": "Social welfare optimisation in well-mixed and structured populations",
    "author": "Van An Nguyen",
    "description": "Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address t...",
    "tags": [
      "arxiv:physics.soc-ph",
      "arxiv:cs.AI",
      "arxiv:cs.MA",
      "arxiv:math.OC",
      "arxiv:nlin.AO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07453v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.",
    "meta_json": "{\"arxiv_id\":\"2512.07453v1\",\"authors\":[\"Van An Nguyen\",\"Vuong Khang Huynh\",\"Ho Nam Duong\",\"Huu Loi Bui\",\"Hai Anh Ha\",\"Quang Dung Le\",\"Le Quoc Dung Ngo\",\"Tan Dat Nguyen\",\"Ngoc Ngu Nguyen\",\"Hoai Thuong Nguyen\",\"Zhao Song\",\"Le Hong Trang\",\"The Anh Han\"],\"categories\":[\"physics.soc-ph\",\"cs.AI\",\"cs.MA\",\"math.OC\",\"nlin.AO\"],\"primary_category\":\"physics.soc-ph\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:27:43Z\",\"updated_date\":\"2025-12-08T11:27:43Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:physics.soc-ph\",\"source_url\":\"https://arxiv.org/abs/physics.soc-ph\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.MA\",\"source_url\":\"https://arxiv.org/abs/cs.MA\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:math.OC\",\"source_url\":\"https://arxiv.org/abs/math.OC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:nlin.AO\",\"source_url\":\"https://arxiv.org/abs/nlin.AO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "9f1dad5f48cd56a1d368a393fdea40f3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07453v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07450v1",
    "name": "Forget and Explain: Transparent Verification of GNN Unlearning",
    "author": "Imran Ahsan",
    "description": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier f...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07450v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.",
    "meta_json": "{\"arxiv_id\":\"2512.07450v1\",\"authors\":[\"Imran Ahsan\",\"Hyunwook Yu\",\"Jinsung Kim\",\"Mucheol Kim\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:25:19Z\",\"updated_date\":\"2025-12-08T11:25:19Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "77df8e37d637c0b2534c283b5384058b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07450v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07437v1",
    "name": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models",
    "author": "Chenwei Shi",
    "description": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architect...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "arxiv:cs.CV",
      "arxiv:cs.NE",
      "arxiv:cs.RO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07437v1",
    "image_url": null,
    "type": "paper",
    "body_content": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.",
    "meta_json": "{\"arxiv_id\":\"2512.07437v1\",\"authors\":[\"Chenwei Shi\",\"Xueyu Luan\"],\"categories\":[\"cs.LG\",\"cs.AI\",\"cs.CV\",\"cs.NE\",\"cs.RO\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:13:15Z\",\"updated_date\":\"2025-12-08T11:13:15Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.NE\",\"source_url\":\"https://arxiv.org/abs/cs.NE\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "8bb14297fdaf4acfd76e3869486615cf",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07437v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07436v1",
    "name": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services",
    "author": "Hang He",
    "description": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning acros...",
    "tags": [
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07436v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.",
    "meta_json": "{\"arxiv_id\":\"2512.07436v1\",\"authors\":[\"Hang He\",\"Chuhuai Yue\",\"Chengqi Dong\",\"Mingxue Tian\",\"Zhenfeng Liu\",\"Jiajun Chai\",\"Xiaohan Wang\",\"Yufei Zhang\",\"Qun Liao\",\"Guojun Yin\",\"Wei Lin\",\"Chengcheng Wan\",\"Haiying Sun\",\"Ting Su\"],\"categories\":[\"cs.AI\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:12:39Z\",\"updated_date\":\"2025-12-08T11:12:39Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "67ed1dcc5ee5c814ad56bb60b4411d0c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07436v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07433v1",
    "name": "Mitigating Bias in Graph Hyperdimensional Computing",
    "author": "Yezi Liu",
    "description": "Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and sim...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.SI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07433v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\\approx 10\\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.",
    "meta_json": "{\"arxiv_id\":\"2512.07433v1\",\"authors\":[\"Yezi Liu\",\"William Youngwoo Chung\",\"Yang Ni\",\"Hanning Chen\",\"Mohsen Imani\"],\"categories\":[\"cs.LG\",\"cs.SI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:09:24Z\",\"updated_date\":\"2025-12-08T11:09:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SI\",\"source_url\":\"https://arxiv.org/abs/cs.SI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "deb6124af621354bf1693aa5f57d50a4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07433v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07430v1",
    "name": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis",
    "author": "Yangle Li",
    "description": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, w...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "multimodal"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07430v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.",
    "meta_json": "{\"arxiv_id\":\"2512.07430v1\",\"authors\":[\"Yangle Li\",\"Danli Luo\",\"Haifeng Hu\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:04:00Z\",\"updated_date\":\"2025-12-08T11:04:00Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "f1b7a755300fdb1e38a52ae581a4a10d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07430v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07426v1",
    "name": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing",
    "author": "Karel Moens",
    "description": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to ...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07426v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.",
    "meta_json": "{\"arxiv_id\":\"2512.07426v1\",\"authors\":[\"Karel Moens\",\"Matthew B. Blaschko\",\"Tinne Tuytelaars\",\"Bart Diricx\",\"Jonas De Vylder\",\"Mustafa Yousif\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T11:01:07Z\",\"updated_date\":\"2025-12-08T11:01:07Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e9d21bf3d794ed7b5dad70811831c91b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07426v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07425v1",
    "name": "Microseismic event classification with a lightweight Fourier Neural Operator model",
    "author": "Ayrat Abdullin",
    "description": "Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classif...",
    "tags": [
      "arxiv:physics.geo-ph",
      "arxiv:cs.LG",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07425v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.",
    "meta_json": "{\"arxiv_id\":\"2512.07425v1\",\"authors\":[\"Ayrat Abdullin\",\"Umair bin Waheed\",\"Leo Eisner\",\"Abdullatif Al-Shuhail\"],\"categories\":[\"physics.geo-ph\",\"cs.LG\"],\"primary_category\":\"physics.geo-ph\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:57:36Z\",\"updated_date\":\"2025-12-08T10:57:36Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:physics.geo-ph\",\"source_url\":\"https://arxiv.org/abs/physics.geo-ph\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "6b350a566f964cf649588dbfe6ff7be4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07425v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07419v1",
    "name": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models",
    "author": "Haidong Kang",
    "description": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human exp...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.CV",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07419v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.",
    "meta_json": "{\"arxiv_id\":\"2512.07419v1\",\"authors\":[\"Haidong Kang\",\"Jun Du\",\"Lihong Lin\"],\"categories\":[\"cs.LG\",\"cs.CV\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:52:55Z\",\"updated_date\":\"2025-12-08T10:52:55Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "0b0ab2a249964324b4dc82858387e54b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07419v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07417v1",
    "name": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning",
    "author": "Giray √ñn√ºr",
    "description": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes th...",
    "tags": [
      "arxiv:cs.LG",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07417v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.",
    "meta_json": "{\"arxiv_id\":\"2512.07417v1\",\"authors\":[\"Giray √ñn√ºr\",\"Azita Dabiri\",\"Bart De Schutter\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:52:00Z\",\"updated_date\":\"2025-12-08T10:52:00Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "1b41b5e8d5dd7c4f5d4d2564a8e058c3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07417v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07415v1",
    "name": "Data-driven Exploration of Mobility Interaction Patterns",
    "author": "Gabriele Galatolo",
    "description": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07415v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.",
    "meta_json": "{\"arxiv_id\":\"2512.07415v1\",\"authors\":[\"Gabriele Galatolo\",\"Mirco Nanni\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:50:24Z\",\"updated_date\":\"2025-12-08T10:50:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "7bf4fa9bde52727d61ef4342149126bd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07415v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07410v1",
    "name": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs",
    "author": "Bin Li",
    "description": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, ...",
    "tags": [
      "arxiv:cs.CV",
      "diffusion"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07410v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.",
    "meta_json": "{\"arxiv_id\":\"2512.07410v1\",\"authors\":[\"Bin Li\",\"Ruichi Zhang\",\"Han Liang\",\"Jingyan Zhang\",\"Juze Zhang\",\"Xin Chen\",\"Lan Xu\",\"Jingyi Yu\",\"Jingya Wang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:46:01Z\",\"updated_date\":\"2025-12-08T10:46:01Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "07c12331001728e536616fe995935c95",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07410v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07407v1",
    "name": "Training Language Models to Use Prolog as a Tool",
    "author": "Niklas Mellgren",
    "description": "Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (executi...",
    "tags": [
      "arxiv:cs.CL",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07407v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference",
    "meta_json": "{\"arxiv_id\":\"2512.07407v1\",\"authors\":[\"Niklas Mellgren\",\"Peter Schneider-Kamp\",\"Lukas Galke Poech\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:39:38Z\",\"updated_date\":\"2025-12-08T10:39:38Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:niklasmellgren:grpo-prolog-inference\",\"source_url\":\"https://github.com/niklasmellgren/grpo-prolog-inference\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "41eae2183b7da5fe279d14d69e146465",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07407v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07404v1",
    "name": "Do LLMs Trust the Code They Write?",
    "author": "Francisco Ribeiro",
    "description": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the ...",
    "tags": [
      "arxiv:cs.SE",
      "arxiv:cs.AI",
      "arxiv:cs.LG",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07404v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
    "meta_json": "{\"arxiv_id\":\"2512.07404v1\",\"authors\":[\"Francisco Ribeiro\",\"Claudio Spiess\",\"Prem Devanbu\",\"Sarah Nadi\"],\"categories\":[\"cs.SE\",\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.SE\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:38:03Z\",\"updated_date\":\"2025-12-08T10:38:03Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SE\",\"source_url\":\"https://arxiv.org/abs/cs.SE\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "5a0f13f49c422fe8530edd63c1e01b19",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07404v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07400v1",
    "name": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse",
    "author": "Giulia Lanzillotta",
    "description": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger bu...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07400v1",
    "image_url": null,
    "type": "paper",
    "body_content": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.",
    "meta_json": "{\"arxiv_id\":\"2512.07400v1\",\"authors\":[\"Giulia Lanzillotta\",\"Damiano Meier\",\"Thomas Hofmann\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:35:57Z\",\"updated_date\":\"2025-12-08T10:35:57Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "d3b7733ed57b70851d322aab43db35b8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07400v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07394v1",
    "name": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video",
    "author": "Zhifan Zhu",
    "description": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07394v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.",
    "meta_json": "{\"arxiv_id\":\"2512.07394v1\",\"authors\":[\"Zhifan Zhu\",\"Siddhant Bansal\",\"Shashank Tripathi\",\"Dima Damen\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:26:36Z\",\"updated_date\":\"2025-12-08T10:26:36Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a4ee63f8692cdf40a3f71b2efc7340bd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07394v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]