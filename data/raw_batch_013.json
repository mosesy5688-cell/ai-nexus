[
  {
    "id": "arxiv:2512.07393v1",
    "name": "Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects",
    "author": "Yann Bourdin",
    "description": "This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demon...",
    "tags": [
      "arxiv:cs.LG",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07393v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.",
    "meta_json": "{\"arxiv_id\":\"2512.07393v1\",\"authors\":[\"Yann Bourdin\",\"Pierrick Legrand\",\"Fanny Roche\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:26:27Z\",\"updated_date\":\"2025-12-08T10:26:27Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "b4c0c4e001a71d681c4498993bb1bc3f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07393v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07391v1",
    "name": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring",
    "author": "Đorđe Nedeljković",
    "description": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07391v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.",
    "meta_json": "{\"arxiv_id\":\"2512.07391v1\",\"authors\":[\"Đorđe Nedeljković\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:24:19Z\",\"updated_date\":\"2025-12-08T10:24:19Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:djordjened92:gdd-cnn.\",\"source_url\":\"https://github.com/djordjened92/gdd-cnn.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "452f97c813b4b5ab7e547562a30f636a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07391v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07390v1",
    "name": "Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood",
    "author": "Gilhyun Nam",
    "description": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a frame...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07390v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.",
    "meta_json": "{\"arxiv_id\":\"2512.07390v1\",\"authors\":[\"Gilhyun Nam\",\"Taewon Kim\",\"Joonhyun Jeong\",\"Eunho Yang\"],\"categories\":[\"cs.LG\",\"cs.CV\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:23:44Z\",\"updated_date\":\"2025-12-08T10:23:44Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "06165f1743aa0a342fefbe59a8fd0f36",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07390v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07385v1",
    "name": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline",
    "author": "Chunhui Zhang",
    "description": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new mult...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07385v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.",
    "meta_json": "{\"arxiv_id\":\"2512.07385v1\",\"authors\":[\"Chunhui Zhang\",\"Li Liu\",\"Zhipeng Zhang\",\"Yong Wang\",\"Hao Wen\",\"Xi Zhou\",\"Shiming Ge\",\"Yanfeng Wang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:19:54Z\",\"updated_date\":\"2025-12-08T10:19:54Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:983632847:Awesome-Multimodal-Object-Tracking}.\",\"source_url\":\"https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "17afe9f26d8f97436a32cf09049f0d6d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07385v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07383v1",
    "name": "LogicCBMs: Logic-Enhanced Concept-Based Learning",
    "author": "Deepika SN Vemuri",
    "description": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designe...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07383v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.",
    "meta_json": "{\"arxiv_id\":\"2512.07383v1\",\"authors\":[\"Deepika SN Vemuri\",\"Gautham Bellamkonda\",\"Aditya Pola\",\"Vineeth N Balasubramanian\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:16:54Z\",\"updated_date\":\"2025-12-08T10:16:54Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "701628f1950d35d14d856c0236df50db",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07383v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07381v1",
    "name": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects",
    "author": "Shuohan Tao",
    "description": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localize...",
    "tags": [
      "arxiv:cs.CV",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07381v1",
    "image_url": null,
    "type": "paper",
    "body_content": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.",
    "meta_json": "{\"arxiv_id\":\"2512.07381v1\",\"authors\":[\"Shuohan Tao\",\"Boyao Zhou\",\"Hanzhang Tu\",\"Yuwang Wang\",\"Yebin Liu\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:16:14Z\",\"updated_date\":\"2025-12-08T10:16:14Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "d5e1d1d34a655cee576f5685ed78a1cd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07381v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07379v1",
    "name": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency",
    "author": "Mahila Moghadami",
    "description": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial a...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07379v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.",
    "meta_json": "{\"arxiv_id\":\"2512.07379v1\",\"authors\":[\"Mahila Moghadami\",\"Mohammad Ali Keyvanrad\",\"Melika Sabaghian\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:15:21Z\",\"updated_date\":\"2025-12-08T10:15:21Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "ed9db879cebdd2712c8c559625c52440",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07379v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07375v1",
    "name": "LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples",
    "author": "Yezi Liu",
    "description": "Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lig...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.CL",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07375v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.",
    "meta_json": "{\"arxiv_id\":\"2512.07375v1\",\"authors\":[\"Yezi Liu\",\"Hanning Chen\",\"Wenjun Huang\",\"Yang Ni\",\"Mohsen Imani\"],\"categories\":[\"cs.LG\",\"cs.CL\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:10:29Z\",\"updated_date\":\"2025-12-08T10:10:29Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "5dba94a589b2be05e501b728a42feb68",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07375v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07374v1",
    "name": "Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning",
    "author": "Yezi Liu",
    "description": "Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.CL",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07374v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.",
    "meta_json": "{\"arxiv_id\":\"2512.07374v1\",\"authors\":[\"Yezi Liu\",\"Hanning Chen\",\"Wenjun Huang\",\"Yang Ni\",\"Mohsen Imani\"],\"categories\":[\"cs.LG\",\"cs.CL\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:10:12Z\",\"updated_date\":\"2025-12-08T10:10:12Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "4d3cfcf1ead9a96e147d8403269ec35c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07374v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07371v1",
    "name": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning",
    "author": "Byungju Kim",
    "description": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling ...",
    "tags": [
      "arxiv:cs.RO",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07371v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.",
    "meta_json": "{\"arxiv_id\":\"2512.07371v1\",\"authors\":[\"Byungju Kim\",\"Jinu Pahk\",\"Chungwoo Lee\",\"Jaejoon Kim\",\"Jangha Lee\",\"Theo Taeyeong Kim\",\"Kyuhwan Shim\",\"Jun Ki Lee\",\"Byoung-Tak Zhang\"],\"categories\":[\"cs.RO\",\"cs.AI\"],\"primary_category\":\"cs.RO\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:08:33Z\",\"updated_date\":\"2025-12-08T10:08:33Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "785af8e21aa9493f1522e636dbceb38d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07371v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07367v1",
    "name": "Multilingual corpora for the study of new concepts in the social sciences and humanities:",
    "author": "Revekka Kyriakoglou",
    "description": "This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, form...",
    "tags": [
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07367v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.",
    "meta_json": "{\"arxiv_id\":\"2512.07367v1\",\"authors\":[\"Revekka Kyriakoglou\",\"Anna Pappa\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:04:50Z\",\"updated_date\":\"2025-12-08T10:04:50Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "fe6176d6afb53faa2861c0c71a39ba58",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07367v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07360v1",
    "name": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation",
    "author": "Qiming Huang",
    "description": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predi...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07360v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.",
    "meta_json": "{\"arxiv_id\":\"2512.07360v1\",\"authors\":[\"Qiming Huang\",\"Hao Ai\",\"Jianbo Jiao\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T10:00:36Z\",\"updated_date\":\"2025-12-08T10:00:36Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "2f028c2ba49775e0c007c402330c901b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07360v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07355v1",
    "name": "A Geometric Unification of Concept Learning with Concept Cones",
    "author": "Alexandre Rocchi--Henry",
    "description": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.CV",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07355v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.",
    "meta_json": "{\"arxiv_id\":\"2512.07355v1\",\"authors\":[\"Alexandre Rocchi--Henry\",\"Thomas Fel\",\"Gianni Franchi\"],\"categories\":[\"cs.AI\",\"cs.CV\",\"cs.LG\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:51:46Z\",\"updated_date\":\"2025-12-08T09:51:46Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "787595581b18586cd7c6e404c51c3e5e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07355v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07351v1",
    "name": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection",
    "author": "Sayeem Been Zaman",
    "description": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of d...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "arxiv:cs.SD",
      "multimodal"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07351v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.",
    "meta_json": "{\"arxiv_id\":\"2512.07351v1\",\"authors\":[\"Sayeem Been Zaman\",\"Wasimul Karim\",\"Arefin Ittesafun Abian\",\"Reem E. Mohamed\",\"Md Rafiqul Islam\",\"Asif Karim\",\"Sami Azam\"],\"categories\":[\"cs.CV\",\"cs.AI\",\"cs.SD\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:43:30Z\",\"updated_date\":\"2025-12-08T09:43:30Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SD\",\"source_url\":\"https://arxiv.org/abs/cs.SD\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "fed09f0f50af2ec65bbb28a2cf45c79c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07351v1\",\"fetched_at\":\"2025-12-10T01:31:39.559Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07348v1",
    "name": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition",
    "author": "Xinyu Wei",
    "description": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize ...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07348v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&amp;Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&amp;Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.",
    "meta_json": "{\"arxiv_id\":\"2512.07348v1\",\"authors\":[\"Xinyu Wei\",\"Kangrui Cen\",\"Hongyang Wei\",\"Zhen Guo\",\"Bairui Li\",\"Zeqing Wang\",\"Jinrui Zhang\",\"Lei Zhang\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:40:11Z\",\"updated_date\":\"2025-12-08T09:40:11Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "80e3d68a23abc152b4904f5ddb904e83",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07348v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07345v1",
    "name": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting",
    "author": "Shilong Jin",
    "description": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view con...",
    "tags": [
      "arxiv:cs.CV",
      "diffusion",
      "attention"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07345v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.",
    "meta_json": "{\"arxiv_id\":\"2512.07345v1\",\"authors\":[\"Shilong Jin\",\"Haoran Duan\",\"Litao Hua\",\"Wentao Huang\",\"Yuan Zhou\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:33:39Z\",\"updated_date\":\"2025-12-08T09:33:39Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "7a001ef26e1c5fd74d73afaf9cb5a0a5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07345v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07344v1",
    "name": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding",
    "author": "Shengyuan Ye",
    "description": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understandi...",
    "tags": [
      "arxiv:cs.DC",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07344v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.",
    "meta_json": "{\"arxiv_id\":\"2512.07344v1\",\"authors\":[\"Shengyuan Ye\",\"Bei Ouyang\",\"Tianyi Qian\",\"Liekang Zeng\",\"Mu Yuan\",\"Xiaowen Chu\",\"Weijie Hong\",\"Xu Chen\"],\"categories\":[\"cs.DC\",\"cs.AI\"],\"primary_category\":\"cs.DC\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:32:47Z\",\"updated_date\":\"2025-12-08T09:32:47Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.DC\",\"source_url\":\"https://arxiv.org/abs/cs.DC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "9d261087630f956c0a90fa7d68580fa9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07344v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07342v1",
    "name": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning",
    "author": "Chen Gong",
    "description": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns...",
    "tags": [
      "arxiv:cs.CR",
      "arxiv:cs.LG",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07342v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged. To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.",
    "meta_json": "{\"arxiv_id\":\"2512.07342v1\",\"authors\":[\"Chen Gong\",\"Zheng Liu\",\"Kecen Li\",\"Tianhao Wang\"],\"categories\":[\"cs.CR\",\"cs.LG\"],\"primary_category\":\"cs.CR\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:29:24Z\",\"updated_date\":\"2025-12-08T09:29:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CR\",\"source_url\":\"https://arxiv.org/abs/cs.CR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "3b8bdb0db47b964f21fd01a5c36ec600",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07342v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07338v1",
    "name": "Generalized Referring Expression Segmentation on Aerial Photos",
    "author": "Luís Marnoto",
    "description": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixel...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07338v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .",
    "meta_json": "{\"arxiv_id\":\"2512.07338v1\",\"authors\":[\"Luís Marnoto\",\"Alexandre Bernardino\",\"Bruno Martins\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:25:59Z\",\"updated_date\":\"2025-12-08T09:25:59Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "af4952f78244786a88b256dff978a193",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07338v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07335v1",
    "name": "Machine learning in an expectation-maximisation framework for nowcasting",
    "author": "Paul Wilsens",
    "description": "Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of ...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07335v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.",
    "meta_json": "{\"arxiv_id\":\"2512.07335v1\",\"authors\":[\"Paul Wilsens\",\"Katrien Antonio\",\"Gerda Claeskens\"],\"categories\":[\"stat.ML\",\"cs.LG\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:22:22Z\",\"updated_date\":\"2025-12-08T09:22:22Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "32bce5f456896cfdb8f1b6ee7a9051de",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07335v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07332v1",
    "name": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach",
    "author": "Zhengquan Luo",
    "description": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a prio...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07332v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.",
    "meta_json": "{\"arxiv_id\":\"2512.07332v1\",\"authors\":[\"Zhengquan Luo\",\"Guy Tadmor\",\"Or Amar\",\"David Zeevi\",\"Zhiqiang Xu\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:20:06Z\",\"updated_date\":\"2025-12-08T09:20:06Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "54a4b230838db721ea428e3a0bd9f066",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07332v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07331v1",
    "name": "The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers",
    "author": "Kanishk Awadhiya",
    "description": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact...",
    "tags": [
      "arxiv:cs.CV",
      "transformer",
      "vision"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07331v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively \"learning\" a bottleneck to isolate semantic features.",
    "meta_json": "{\"arxiv_id\":\"2512.07331v1\",\"authors\":[\"Kanishk Awadhiya\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:18:32Z\",\"updated_date\":\"2025-12-08T09:18:32Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "4eb836bf7a0f3f8567040e0a26a75995",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07331v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07329v1",
    "name": "Two-dimensional RMSD projections for reaction path visualization and validation",
    "author": "Rohit Goswami",
    "description": "Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of d...",
    "tags": [
      "arxiv:physics.chem-ph",
      "arxiv:cond-mat.mtrl-sci",
      "arxiv:cs.LG",
      "arxiv:physics.comp-ph"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07329v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.",
    "meta_json": "{\"arxiv_id\":\"2512.07329v1\",\"authors\":[\"Rohit Goswami\"],\"categories\":[\"physics.chem-ph\",\"cond-mat.mtrl-sci\",\"cs.LG\",\"physics.comp-ph\"],\"primary_category\":\"physics.chem-ph\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:15:24Z\",\"updated_date\":\"2025-12-08T09:15:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:physics.chem-ph\",\"source_url\":\"https://arxiv.org/abs/physics.chem-ph\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cond-mat.mtrl-sci\",\"source_url\":\"https://arxiv.org/abs/cond-mat.mtrl-sci\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:physics.comp-ph\",\"source_url\":\"https://arxiv.org/abs/physics.comp-ph\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "b87364af94feb2107191cb5753aa83f5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07329v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07328v1",
    "name": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
    "author": "Ziyang Mai",
    "description": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image....",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "diffusion"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07328v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.",
    "meta_json": "{\"arxiv_id\":\"2512.07328v1\",\"authors\":[\"Ziyang Mai\",\"Yu-Wing Tai\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T09:12:18Z\",\"updated_date\":\"2025-12-08T09:12:18Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ziyang1106:ContextAnyone}{https:\",\"source_url\":\"https://github.com/ziyang1106/ContextAnyone}{https:\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "26c3cd54f0d1d2538804f95c68fd1744",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07328v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07313v1",
    "name": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach",
    "author": "Bosun Kang",
    "description": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless in...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.DS"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07313v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.",
    "meta_json": "{\"arxiv_id\":\"2512.07313v1\",\"authors\":[\"Bosun Kang\",\"Hyejun Park\",\"Chenglin Fan\"],\"categories\":[\"cs.LG\",\"cs.DS\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:56:25Z\",\"updated_date\":\"2025-12-08T08:56:25Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.DS\",\"source_url\":\"https://arxiv.org/abs/cs.DS\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "c5a13e9caea51e234af934f21ddf4c34",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07313v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07312v1",
    "name": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management",
    "author": "Zhongchun Zhou",
    "description": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approac...",
    "tags": [
      "arxiv:cs.AR",
      "arxiv:cs.AI",
      "arxiv:cs.DC",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07312v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing. We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups. Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.",
    "meta_json": "{\"arxiv_id\":\"2512.07312v1\",\"authors\":[\"Zhongchun Zhou\",\"Chengtao Lai\",\"Yuhang Gu\",\"Wei Zhang\"],\"categories\":[\"cs.AR\",\"cs.AI\",\"cs.DC\"],\"primary_category\":\"cs.AR\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:56:10Z\",\"updated_date\":\"2025-12-08T08:56:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AR\",\"source_url\":\"https://arxiv.org/abs/cs.AR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.DC\",\"source_url\":\"https://arxiv.org/abs/cs.DC\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "bd891b8dbd9aa28d7a7c6ea12a140629",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07312v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07310v1",
    "name": "Towards a Relationship-Aware Transformer for Tabular Data",
    "author": "Andrei V. Konstantinov",
    "description": "Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention ...",
    "tags": [
      "arxiv:cs.LG",
      "transformer"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07310v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.",
    "meta_json": "{\"arxiv_id\":\"2512.07310v1\",\"authors\":[\"Andrei V. Konstantinov\",\"Valerii A. Zuev\",\"Lev V. Utkin\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:54:53Z\",\"updated_date\":\"2025-12-08T08:54:53Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "ea0ddb78724145d5371eee33f6241dca",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07310v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07309v1",
    "name": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals",
    "author": "Guosheng Wang",
    "description": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining ...",
    "tags": [
      "arxiv:cs.IT",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07309v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.",
    "meta_json": "{\"arxiv_id\":\"2512.07309v1\",\"authors\":[\"Guosheng Wang\",\"Shen Wang\",\"Lei Yang\"],\"categories\":[\"cs.IT\",\"cs.AI\"],\"primary_category\":\"cs.IT\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:52:08Z\",\"updated_date\":\"2025-12-08T08:52:08Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.IT\",\"source_url\":\"https://arxiv.org/abs/cs.IT\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "1683aa0a2ccbaf7c4ea09dfccddc91a0",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07309v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07306v1",
    "name": "Exact Synthetic Populations for Scalable Societal and Market Modeling",
    "author": "Thierry Petit",
    "description": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distribu...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.AI",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07306v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.",
    "meta_json": "{\"arxiv_id\":\"2512.07306v1\",\"authors\":[\"Thierry Petit\",\"Arnault Pachot\"],\"categories\":[\"stat.ML\",\"cs.AI\",\"cs.LG\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:48:21Z\",\"updated_date\":\"2025-12-08T08:48:21Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "796992f1c7561a2fafdd4df3c4f58d47",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07306v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07305v1",
    "name": "Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset",
    "author": "Tobias Abraham Haider",
    "description": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, clos...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07305v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.",
    "meta_json": "{\"arxiv_id\":\"2512.07305v1\",\"authors\":[\"Tobias Abraham Haider\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:47:16Z\",\"updated_date\":\"2025-12-08T08:47:16Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "a066738a387f7edfa1827aace07e809e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07305v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07302v1",
    "name": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts",
    "author": "Mingning Guo",
    "description": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simpli...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "vision",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07302v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.",
    "meta_json": "{\"arxiv_id\":\"2512.07302v1\",\"authors\":[\"Mingning Guo\",\"Mengwei Wu\",\"Shaoxian Li\",\"Haifeng Li\",\"Chao Tao\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:44:57Z\",\"updated_date\":\"2025-12-08T08:44:57Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:lostwolves:AerialVP.\",\"source_url\":\"https://github.com/lostwolves/AerialVP.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "1b7533cf74a7c06c8b289877a47b7dae",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07302v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07289v1",
    "name": "Equivariant Diffusion for Crystal Structure Prediction",
    "author": "Peijia Lin",
    "description": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permu...",
    "tags": [
      "arxiv:cond-mat.mtrl-sci",
      "arxiv:cs.LG",
      "diffusion"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07289v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.",
    "meta_json": "{\"arxiv_id\":\"2512.07289v1\",\"authors\":[\"Peijia Lin\",\"Pin Chen\",\"Rui Jiao\",\"Qing Mo\",\"Jianhuan Cen\",\"Wenbing Huang\",\"Yang Liu\",\"Dan Huang\",\"Yutong Lu\"],\"categories\":[\"cond-mat.mtrl-sci\",\"cs.LG\"],\"primary_category\":\"cond-mat.mtrl-sci\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:28:22Z\",\"updated_date\":\"2025-12-08T08:28:22Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cond-mat.mtrl-sci\",\"source_url\":\"https://arxiv.org/abs/cond-mat.mtrl-sci\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "b980ef17a621734f19fb3aa8f9887749",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07289v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07288v1",
    "name": "Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models",
    "author": "Tomoki Doi",
    "description": "Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improve...",
    "tags": [
      "arxiv:cs.CL",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07288v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",
    "meta_json": "{\"arxiv_id\":\"2512.07288v1\",\"authors\":[\"Tomoki Doi\",\"Masaru Isonuma\",\"Hitomi Yanaka\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:28:10Z\",\"updated_date\":\"2025-12-08T08:28:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "7e47049898f59183084adf9ff70ec337",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07288v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07287v1",
    "name": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents",
    "author": "Sijia Li",
    "description": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07287v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",
    "meta_json": "{\"arxiv_id\":\"2512.07287v1\",\"authors\":[\"Sijia Li\",\"Yuchen Huang\",\"Zifan Liu\",\"Zijian Li\",\"Jingjing fu\",\"Lei Song\",\"Jiang Bian\",\"Jun Zhang\",\"Rui Wang\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:27:24Z\",\"updated_date\":\"2025-12-08T08:27:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "feeaea5bb1c36898ba96b2263fcdadfe",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07287v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07279v1",
    "name": "Verifiable Deep Quantitative Group Testing",
    "author": "Shreyas Jayant Grampurohit",
    "description": "We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \\ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery ...",
    "tags": [
      "arxiv:eess.SP",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07279v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \\ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.",
    "meta_json": "{\"arxiv_id\":\"2512.07279v1\",\"authors\":[\"Shreyas Jayant Grampurohit\",\"Satish Mulleti\",\"Ajit Rajwade\"],\"categories\":[\"eess.SP\",\"cs.LG\"],\"primary_category\":\"eess.SP\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:18:57Z\",\"updated_date\":\"2025-12-08T08:18:57Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.SP\",\"source_url\":\"https://arxiv.org/abs/eess.SP\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "dd42dac9f8c882fad4be5bdcc20ca8c6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07279v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07277v1",
    "name": "Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data",
    "author": "Srihari Bandarupalli",
    "description": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap ...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:eess.AS",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07277v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.",
    "meta_json": "{\"arxiv_id\":\"2512.07277v1\",\"authors\":[\"Srihari Bandarupalli\",\"Bhavana Akkiraju\",\"Charan Devarakonda\",\"Vamsiraghusimha Narsinga\",\"Anil Kumar Vuppala\"],\"categories\":[\"cs.CL\",\"eess.AS\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:16:34Z\",\"updated_date\":\"2025-12-08T08:16:34Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.AS\",\"source_url\":\"https://arxiv.org/abs/eess.AS\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "0db5c987a55b031dccb5a87bb56f0d21",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07277v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07276v1",
    "name": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery",
    "author": "Mai Tsujimoto",
    "description": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive be...",
    "tags": [
      "arxiv:cs.CV",
      "vision",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07276v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.",
    "meta_json": "{\"arxiv_id\":\"2512.07276v1\",\"authors\":[\"Mai Tsujimoto\",\"Junjue Wang\",\"Weihao Xuan\",\"Naoto Yokoya\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:16:14Z\",\"updated_date\":\"2025-12-08T08:16:14Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mm1129:Geo3DVQA.\",\"source_url\":\"https://github.com/mm1129/Geo3DVQA.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "59efb6c8f7c20fd28a654e9e323d28b6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07276v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07275v1",
    "name": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation",
    "author": "Siyu Wang",
    "description": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature informatio...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "attention"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07275v1",
    "image_url": null,
    "type": "paper",
    "body_content": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.",
    "meta_json": "{\"arxiv_id\":\"2512.07275v1\",\"authors\":[\"Siyu Wang\",\"Hua Wang\",\"Huiyu Li\",\"Fan Zhang\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:15:39Z\",\"updated_date\":\"2025-12-08T08:15:39Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "36dd08943e8b203eb4d6cef1aa140bab",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07275v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07273v1",
    "name": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation",
    "author": "Zhi Rao",
    "description": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with rein...",
    "tags": [
      "arxiv:cs.CV",
      "vision",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07273v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.",
    "meta_json": "{\"arxiv_id\":\"2512.07273v1\",\"authors\":[\"Zhi Rao\",\"Yucheng Zhou\",\"Benjia Zhou\",\"Yiqing Huang\",\"Sergio Escalera\",\"Jun Wan\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:11:53Z\",\"updated_date\":\"2025-12-08T08:11:53Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e74f001695d82327706679538e241705",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07273v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07269v1",
    "name": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data",
    "author": "Mike Diessner",
    "description": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and d...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07269v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.",
    "meta_json": "{\"arxiv_id\":\"2512.07269v1\",\"authors\":[\"Mike Diessner\",\"Yannick Tarant\"],\"categories\":[\"cs.CV\",\"cs.LG\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:08:38Z\",\"updated_date\":\"2025-12-08T08:08:38Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "ea7201ce75e81ad9ac7950c947c43d0e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07269v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07267v1",
    "name": "Non-negative DAG Learning from Time-Series Data",
    "author": "Samuel Rey",
    "description": "This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the ...",
    "tags": [
      "arxiv:eess.SP",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07267v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.",
    "meta_json": "{\"arxiv_id\":\"2512.07267v1\",\"authors\":[\"Samuel Rey\",\"Gonzalo Mateos\"],\"categories\":[\"eess.SP\",\"cs.LG\"],\"primary_category\":\"eess.SP\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:07:19Z\",\"updated_date\":\"2025-12-08T08:07:19Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.SP\",\"source_url\":\"https://arxiv.org/abs/eess.SP\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "9b7a200bf6433e3c28b6624f0f48dc62",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07267v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07266v1",
    "name": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks",
    "author": "Florian Tretter",
    "description": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic ...",
    "tags": [
      "arxiv:cs.RO",
      "arxiv:cs.AI",
      "arxiv:eess.SY",
      "neural",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07266v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.",
    "meta_json": "{\"arxiv_id\":\"2512.07266v1\",\"authors\":[\"Florian Tretter\",\"Daniel Flögel\",\"Alexandru Vasilache\",\"Max Grobbel\",\"Jürgen Becker\",\"Sören Hohmann\"],\"categories\":[\"cs.RO\",\"cs.AI\",\"eess.SY\"],\"primary_category\":\"cs.RO\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:06:40Z\",\"updated_date\":\"2025-12-08T08:06:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.SY\",\"source_url\":\"https://arxiv.org/abs/eess.SY\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "b5c83ae8f8931f6b6b3bd0c11f9f4084",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07266v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07265v1",
    "name": "TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation",
    "author": "Bhavana Akkiraju",
    "description": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Tel...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:eess.AS"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07265v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.",
    "meta_json": "{\"arxiv_id\":\"2512.07265v1\",\"authors\":[\"Bhavana Akkiraju\",\"Srihari Bandarupalli\",\"Swathi Sambangi\",\"Vasavi Ravuri\",\"R Vijaya Saraswathi\",\"Anil Kumar Vuppala\"],\"categories\":[\"cs.CL\",\"eess.AS\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T08:06:11Z\",\"updated_date\":\"2025-12-08T08:06:11Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.AS\",\"source_url\":\"https://arxiv.org/abs/eess.AS\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "bff124e52ac6ca8794c5535b6635f987",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07265v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07259v1",
    "name": "Affine Subspace Models and Clustering for Patch-Based Image Denoising",
    "author": "Tharindu Wickremasinghe",
    "description": "Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the...",
    "tags": [
      "arxiv:eess.IV",
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07259v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.",
    "meta_json": "{\"arxiv_id\":\"2512.07259v1\",\"authors\":[\"Tharindu Wickremasinghe\",\"Marco F. Duarte\"],\"categories\":[\"eess.IV\",\"cs.CV\"],\"primary_category\":\"eess.IV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:53:54Z\",\"updated_date\":\"2025-12-08T07:53:54Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:eess.IV\",\"source_url\":\"https://arxiv.org/abs/eess.IV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "aa4946c02978bdc2bc30c36bddb14483",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07259v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07253v1",
    "name": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement",
    "author": "Handing Xu",
    "description": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address ...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "gan"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07253v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.",
    "meta_json": "{\"arxiv_id\":\"2512.07253v1\",\"authors\":[\"Handing Xu\",\"Zhenguo Nie\",\"Tairan Peng\",\"Huimin Pan\",\"Xin-Jun Liu\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:49:50Z\",\"updated_date\":\"2025-12-08T07:49:50Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "730def4d11a45412ddf12dc15e7a0d43",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07253v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07251v1",
    "name": "See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement",
    "author": "Junqi Liu",
    "description": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically releva...",
    "tags": [
      "arxiv:cs.CV",
      "diffusion"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07251v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.",
    "meta_json": "{\"arxiv_id\":\"2512.07251v1\",\"authors\":[\"Junqi Liu\",\"Zejun Wu\",\"Pedro R. A. S. Bassi\",\"Xinze Zhou\",\"Wenxuan Li\",\"Ibrahim E. Hamamci\",\"Sezgin Er\",\"Tianyu Lin\",\"Yi Luo\",\"Szymon Płotka\",\"Bjoern Menze\",\"Daguang Xu\",\"Kai Ding\",\"Kang Wang\",\"Yang Yang\",\"Yucheng Tang\",\"Alan L. Yuille\",\"Zongwei Zhou\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:48:45Z\",\"updated_date\":\"2025-12-08T07:48:45Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "68d3092a98657bd106adbfc09bde3ae8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07251v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07249v1",
    "name": "IFFair: Influence Function-driven Sample Reweighting for Fair Classification",
    "author": "Jingran Yang",
    "description": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore,...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07249v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.",
    "meta_json": "{\"arxiv_id\":\"2512.07249v1\",\"authors\":[\"Jingran Yang\",\"Min Zhang\",\"Lingfeng Zhang\",\"Zhaohui Wang\",\"Yonggang Zhang\"],\"categories\":[\"cs.LG\",\"cs.AI\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:45:55Z\",\"updated_date\":\"2025-12-08T07:45:55Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "9789f4cbb81b51ff4fd20580b321e0fc",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07249v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07247v1",
    "name": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing",
    "author": "Ziming Hong",
    "description": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: v...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.CR",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07247v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.",
    "meta_json": "{\"arxiv_id\":\"2512.07247v1\",\"authors\":[\"Ziming Hong\",\"Tianyu Huang\",\"Runnan Chen\",\"Shanshan Ye\",\"Mingming Gong\",\"Bo Han\",\"Tongliang Liu\"],\"categories\":[\"cs.CV\",\"cs.CR\",\"cs.LG\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:41:23Z\",\"updated_date\":\"2025-12-08T07:41:23Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CR\",\"source_url\":\"https://arxiv.org/abs/cs.CR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e6a301ffa13ba395e97e165673c07492",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07247v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07246v1",
    "name": "Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection",
    "author": "Mengqi Wang",
    "description": "Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is hi...",
    "tags": [
      "arxiv:cs.CL",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07246v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.",
    "meta_json": "{\"arxiv_id\":\"2512.07246v1\",\"authors\":[\"Mengqi Wang\",\"Jianwei Wang\",\"Qing Liu\",\"Xiwei Xu\",\"Zhenchang Xing\",\"Liming Zhu\",\"Wenjie Zhang\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:40:48Z\",\"updated_date\":\"2025-12-08T07:40:48Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "9a4899247e5d2671fdcaad856085d95e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07246v1\",\"fetched_at\":\"2025-12-10T01:31:39.560Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07245v1",
    "name": "Zero-Shot Textual Explanations via Translating Decision-Critical Features",
    "author": "Toshinori Yamauchi",
    "description": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decisio...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07245v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.",
    "meta_json": "{\"arxiv_id\":\"2512.07245v1\",\"authors\":[\"Toshinori Yamauchi\",\"Hiroshi Kera\",\"Kazuhiko Kawamoto\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T07:39:52Z\",\"updated_date\":\"2025-12-08T07:39:52Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "9598b21865d06491ab5ec5700a45e582",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07245v1\",\"fetched_at\":\"2025-12-10T01:31:39.562Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]