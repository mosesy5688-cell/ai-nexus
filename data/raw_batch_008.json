[
  {
    "id": "huggingface:shakker-labs:flux.1-dev-controlnet-union-pro",
    "name": "FLUX.1-dev-ControlNet-Union-Pro",
    "author": "Shakker-Labs",
    "description": "--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md language: - en library_name: diffusers pipeline_tag: text-to-image tags: - Text-to-Image - ControlNet - Diffusers - Flux.1-dev - image-generation - Stable Diffusion base_model: black-forest-labs/FLUX.1-dev --- This repository contains a unified ControlNet for FLUX.1-dev model jointly released by researchers from InstantX Team and Shakker Lab...",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "controlnet",
      "diffusers",
      "flux.1-dev",
      "image-generation",
      "stable diffusion",
      "text-to-image",
      "en",
      "base_model:black-forest-labs/flux.1-dev",
      "base_model:finetune:black-forest-labs/flux.1-dev",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 611,
    "downloads": 12594,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro",
    "image_url": "https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/resolve/main/assets/blur.jpg",
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\n\nlanguage:\n  - en\nlibrary_name: diffusers\npipeline_tag: text-to-image\n\ntags:\n- Text-to-Image\n- ControlNet\n- Diffusers\n- Flux.1-dev\n- image-generation\n- Stable Diffusion\nbase_model: black-forest-labs/FLUX.1-dev\n---\n\n# FLUX.1-dev-ControlNet-Union-Pro\n\nThis repository contains a unified ControlNet for FLUX.1-dev model jointly released by researchers from [InstantX Team](https://huggingface.co/InstantX) and [Shakker Labs](https://huggingface.co/Shakker-Labs).\n\n<div class=\"container\">\n  <img src=\"./assets/poster.png\" width=\"1024\"/>\n</div>\n\n\n# Model Cards\n- This checkpoint is a Pro version of [FLUX.1-dev-Controlnet-Union](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union) trained with more steps and datasets.\n- This model supports 7 control modes, including canny (0), tile (1), depth (2), blur (3), pose (4), gray (5), low quality (6).\n- The recommended controlnet_conditioning_scale is 0.3-0.8.\n- This model can be jointly used with other ControlNets.\n\n\n# Showcases\n\n<div class=\"container\">\n  <img src=\"./assets/teaser1.png\" width=\"1024\"/>\n  <img src=\"./assets/teaser2.png\" width=\"1024\"/>\n  <img src=\"./assets/teaser3.png\" width=\"1024\"/>\n</div>\n\n\n# Inference\nPlease install `diffusers` from [the source](https://github.com/huggingface/diffusers), as [the PR](https://github.com/huggingface/diffusers/pull/9175) has not been included in currently released version yet.\n\n# Multi-Controls Inference\n```python\nimport torch\nfrom diffusers.utils import load_image\n\nfrom diffusers import FluxControlNetPipeline, FluxControlNetModel\nfrom diffusers.models import FluxMultiControlNetModel\n\nbase_model = 'black-forest-labs/FLUX.1-dev'\ncontrolnet_model_union = 'Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro'\n\ncontrolnet_union = FluxControlNetModel.from_pretrained(controlnet_model_union, torch_dtype=torch.bfloat16)\ncontrolnet = FluxMultiControlNetModel([controlnet_union]) # we always recommend loading via FluxMultiControlNetModel\n\npipe = FluxControlNetPipeline.from_pretrained(base_model, controlnet=controlnet, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n\nprompt = 'A bohemian-style female travel blogger with sun-kissed skin and messy beach waves.'\ncontrol_image_depth = load_image(\"https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/resolve/main/assets/depth.jpg\")\ncontrol_mode_depth = 2\n\ncontrol_image_canny = load_image(\"https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/resolve/main/assets/canny.jpg\")\ncontrol_mode_canny = 0\n\nwidth, height = control_image_depth.size\n\nimage = pipe(\n    prompt, \n    control_image=[control_image_depth, control_image_canny],\n    control_mode=[control_mode_depth, control_mode_canny],\n    width=width,\n    height=height,\n    controlnet_conditioning_scale=[0.2, 0.4],\n    num_inference_steps=24, \n    guidance_scale=3.5,\n    generator=torch.manual_seed(42),\n).images[0]\n```\n\nWe also support loading multiple ControlNets as before, you can load as\n```python\nfrom diffusers import FluxControlNetModel\nfrom diffusers.models import FluxMultiControlNetModel\n\ncontrolnet_model_union = 'Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro'\ncontrolnet_union = FluxControlNetModel.from_pretrained(controlnet_model_union, torch_dtype=torch.bfloat16)\n\ncontrolnet_model_depth = 'Shakker-Labs/FLUX.1-dev-Controlnet-Depth'\ncontrolnet_depth = FluxControlNetModel.from_pretrained(controlnet_model_depth, torch_dtype=torch.bfloat16)\n\ncontrolnet = FluxMultiControlNetModel([controlnet_union, controlnet_depth])\n\n# set mode to None for other ControlNets\ncontrol_mode=[2, None]\n```\n\n# Resources\n- [InstantX/FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny)\n- [Shakker-Labs/FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth)\n- [Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro)\n\n# Acknowledgements\nThis project is trained by [InstantX Team](https://huggingface.co/InstantX) and sponsored by [Shakker AI](https://www.shakker.ai/). The original idea is inspired by [xinsir/controlnet-union-sdxl-1.0](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0). All copyright reserved.\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":6606512638,\"files_count\":22,\"spaces_count\":45,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 82.9,
    "content_hash": "209369b2f36d97cce4091fcc00986f74",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/resolve/main/assets/blur.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:hakurei:waifu-diffusion-v1-3",
    "name": "waifu-diffusion-v1-3",
    "author": "hakurei",
    "description": "--- language: - en tags: - stable-diffusion - text-to-image license: creativeml-openrail-m inference: false --- Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. - Float 16 EMA Pruned - Float 32 EMA Pruned - Float 32 Full Weights - Float 32 Full Weights + Optimizer Weights (For Training) The model originally used for fine-tuning is Stable Diffusion 1.4, which is a latent image diffusion model trained on LAION2...",
    "tags": [
      "stable-diffusion",
      "text-to-image",
      "en",
      "license:creativeml-openrail-m",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 610,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/hakurei/waifu-diffusion-v1-3",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\nlicense: creativeml-openrail-m\ninference: false\n\n---\n\n# Waifu Diffusion v1.3\n\nWaifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n\n- [Float 16 EMA Pruned](https://huggingface.co/hakurei/waifu-diffusion-v1-3/blob/main/wd-v1-3-float16.ckpt)\n- [Float 32 EMA Pruned](https://huggingface.co/hakurei/waifu-diffusion-v1-3/blob/main/wd-v1-3-float32.ckpt)\n- [Float 32 Full Weights](https://huggingface.co/hakurei/waifu-diffusion-v1-3/blob/main/wd-v1-3-full.ckpt)\n- [Float 32 Full Weights + Optimizer Weights (For Training)](https://huggingface.co/hakurei/waifu-diffusion-v1-3/blob/main/wd-v1-3-full-opt.ckpt)\n\n## Model Description\n\nThe model originally used for fine-tuning is [Stable Diffusion 1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4), which is a latent image diffusion model trained on [LAION2B-en](https://huggingface.co/datasets/laion/laion2B-en). The current model has been fine-tuned with a learning rate of 5.0e-6 for 10 epochs on 680k anime-styled images.\n\n[See here for an in-depth overview of Waifu Diffusion 1.3.](https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1)\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)\n\n## Downstream Uses\n\nThis model can be used for entertainment purposes and as a generative art assistant.\n\n## Team Members and Acknowledgements\n\nThis project would not have been possible without the incredible work by the [CompVis Researchers](https://ommer-lab.com/).\n\n- [Anthony Mercurio](https://github.com/harubaru)\n- [Salt](https://github.com/sALTaccount/)\n- [Cafe](https://twitter.com/cafeai_labs)\n\nIn order to reach us, you can join our [Discord server](https://discord.gg/touhouai).\n\n[![Discord Server](https://discordapp.com/api/guilds/930499730843250783/widget.png?style=banner2)](https://discord.gg/touhouai)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":127396557201,\"files_count\":27,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 62.9,
    "content_hash": "1a3ac4bd3f4dce612bb883bb598b20d6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/hakurei/waifu-diffusion-v1-3\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-2-2b",
    "name": "gemma-2-2b",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gemma2",
      "text-generation",
      "arxiv:2009.03300",
      "arxiv:1905.07830",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1905.10044",
      "arxiv:1907.10641",
      "arxiv:1811.00937",
      "arxiv:1809.02789",
      "arxiv:1911.01547",
      "arxiv:1705.03551",
      "arxiv:2107.03374",
      "arxiv:2108.07732",
      "arxiv:2110.14168",
      "arxiv:2009.11462",
      "arxiv:2101.11718",
      "arxiv:2110.08193",
      "arxiv:1804.09301",
      "arxiv:2109.07958",
      "arxiv:1804.06876",
      "arxiv:2103.03874",
      "arxiv:2304.06364",
      "arxiv:1903.00161",
      "arxiv:2206.04615",
      "arxiv:2203.09509",
      "arxiv:2403.13793",
      "license:gemma",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 610,
    "downloads": 145630,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-2-2b",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":2614341888,\"storage_bytes\":10549261876,\"files_count\":12,\"spaces_count\":96,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Gemma2ForCausalLM\"],\"model_type\":\"gemma2\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1811.00937\",\"source_url\":\"https://arxiv.org/abs/1811.00937\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1809.02789\",\"source_url\":\"https://arxiv.org/abs/1809.02789\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.14168\",\"source_url\":\"https://arxiv.org/abs/2110.14168\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.11462\",\"source_url\":\"https://arxiv.org/abs/2009.11462\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2101.11718\",\"source_url\":\"https://arxiv.org/abs/2101.11718\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.08193\",\"source_url\":\"https://arxiv.org/abs/2110.08193\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1804.09301\",\"source_url\":\"https://arxiv.org/abs/1804.09301\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2109.07958\",\"source_url\":\"https://arxiv.org/abs/2109.07958\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1804.06876\",\"source_url\":\"https://arxiv.org/abs/1804.06876\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.03874\",\"source_url\":\"https://arxiv.org/abs/2103.03874\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.06364\",\"source_url\":\"https://arxiv.org/abs/2304.06364\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1903.00161\",\"source_url\":\"https://arxiv.org/abs/1903.00161\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2206.04615\",\"source_url\":\"https://arxiv.org/abs/2206.04615\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.09509\",\"source_url\":\"https://arxiv.org/abs/2203.09509\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.13793\",\"source_url\":\"https://arxiv.org/abs/2403.13793\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 37.9,
    "content_hash": "eb609e433dec8b53e76043c7c2b45e6c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-2-2b\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:iamcreateai:ruyi-mini-7b",
    "name": "Ruyi-Mini-7B",
    "author": "IamCreateAI",
    "description": "--- language: - \"en\" tags: - video generation - CreateAI license: apache-2.0 pipeline_tag: image-to-video --- Hugging Face | Github An image-to-video model by CreateAI. Ruyi-Mini-7B is an open-source image-to-video generation model. Starting with an input image, Ruyi produces subsequent video frames at resolutions ranging from 360p to 720p, supporting various aspect ratios and a maximum duration of 5 seconds. Enhanced with motion and camera control, Ruyi offers greater flexibility and creativ...",
    "tags": [
      "diffusers",
      "safetensors",
      "video generation",
      "createai",
      "image-to-video",
      "en",
      "license:apache-2.0",
      "diffusers:ruyiinpaintpipeline",
      "region:us"
    ],
    "pipeline_tag": "image-to-video",
    "likes": 610,
    "downloads": 239,
    "source": "huggingface",
    "source_url": "https://huggingface.co/IamCreateAI/Ruyi-Mini-7B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n - \"en\"\ntags:\n - video generation\n - CreateAI\nlicense: apache-2.0\npipeline_tag: image-to-video\n---\n\n\n# Ruyi-Mini-7B\n[Hugging Face](https://huggingface.co/IamCreateAI/Ruyi-Mini-7B) | [Github](https://github.com/IamCreateAI/Ruyi-Models)\n\nAn image-to-video model by CreateAI.\n\n## Overview\n\nRuyi-Mini-7B is an open-source image-to-video generation model. Starting with an input image, Ruyi produces subsequent video frames at resolutions ranging from 360p to 720p, supporting various aspect ratios and a maximum duration of 5 seconds. Enhanced with motion and camera control, Ruyi offers greater flexibility and creativity in video generation. We are releasing the model under the permissive Apache 2.0 license.\n\n## Update\n\nDec 24, 2024: The diffusion model is updated to fix the black lines when creating 3:4 or 4:5 videos. \n\nDec 16, 2024: Ruyi-mini-7B is released. \n\n## Installation\n\nInstall code from github:\n```bash\ngit clone https://github.com/IamCreateAI/Ruyi-Models\ncd Ruyi-Models\npip install -r requirements.txt\n```\n\n## Running\n\nWe provide two ways to run our model. The first is directly using python code.\n\n```bash\npython3 predict_i2v.py\n```\n\nOr use ComfyUI wrapper in our [github repo](https://github.com/IamCreateAI/Ruyi-Models).\n\n## Model Architecture\n\nRuyi-Mini-7B is an advanced image-to-video model with about 7.1 billion parameters. The model architecture is modified from [EasyAnimate V4 model](https://github.com/aigc-apps/EasyAnimate), whose transformer module is inherited from [HunyuanDiT](https://github.com/Tencent/HunyuanDiT). It comprises three key components:\n  1. Casual VAE Module: Handles video compression and decompression. It reduces spatial resolution to 1/8 and temporal resolution to 1/4, with each latent pixel is represented in 16 float numbers after compression.\n  2. Diffusion Transformer Module: Generates compressed video data using 3D full attention, with: \n  - 2D Normalized-RoPE for spatial dimensions;\n  - Sin-cos position embedding for temporal dimensions;\n  - DDPM (Denoising Diffusion Probabilistic Models) for model training.\n  3. Ruyi also utilizes a CLIP model to extract the semantic features from the input image to guide the whole video generation. The CLIP features are introduced into the transformer by cross-attention.\n\n## Training Data and Methodology\n  The training process is divided into four phases:\n  - Phase 1: Pre-training from scratch with ~200M video clips and ~30M images at a 256-resolution, using a batch size of 4096 for 350,000 iterations to achieve full convergence.\n  - Phase 2: Fine-tuning with ~60M video clips for multi-scale resolutions (384–512), with a batch size of 1024 for 60,000 iterations.\n  - Phase 3: High-quality fine-tuning with ~20M video clips and ~8M images for 384–1024 resolutions, with dynamic batch sizes based on memory and 10,000 iterations.\n  - Phase 4: Image-to-video training with ~10M curated high-quality video clips, with dynamic batch sizes based on memory for ~10,000 iterations.\n\n## Hardware Requirements\nThe VRAM cost of Ruyi depends on the resolution and duration of the video. Here we list the costs for some typical video size. Tested on single A100.\n|Video Size | 360x480x120 | 384x672x120 | 480x640x120 | 630x1120x120 | 720x1280x120 | \n|:--:|:--:|:--:|:--:|:--:|:--:|\n|Memory   | 21.5GB   | 25.5GB   | 27.7GB   | 44.9GB   |   54.8GB   |\n|Time     | 03:10   | 05:29   | 06:49   | 24:18   |   39:02   |\n\nFor 24GB VRAM cards such as RTX4090, we provide `low_gpu_memory_mode`, under which the model can generate 720x1280x120 videos with a longer time.\n\n## Showcase\n\n### Image to Video Effects\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n    <tr>\n        <td><video src=\"https://github.com/user-attachments/assets/4dedf40b-82f2-454c-9a67-5f4ed243f5ea\" width=\"100%\" style=\"max-height:640px; min-height: 200px\" controls autoplay loop></video></td>\n        <td><video src=\"https://github.com/user-attachments/assets/905fef17-8c5d-49b0-a49a-6ae7e212fa07\" width=\"100%\" style=\"max-height:640px; min-height: 200px\" controls autoplay loop></video></td>\n        <td><video src=\"https://github.com/user-attachments/assets/20daab12-b510-448a-9491-389d7bdbbf2e\" width=\"100%\" style=\"max-height:640px; min-height: 200px\" controls autoplay loop></video></td>\n        <td><video src=\"https://github.com/user-attachments/assets/f1bb0a91-d52a-4611-bac2-8fcf9658cac0\" width=\"100%\" style=\"max-height:640px; min-height: 200px\" controls autoplay loop></video></td>\n    </tr>\n</table>\n\n### Camera Control\n\n<table border=\"0\" style=\"width: 100%; text-align: center; \">\n    <tr>\n        <td align=center><img src=\"https://github.com/user-attachments/assets/8aedcea6-3b8e-4c8b-9fed-9ceca4d41954\" width=\"100%\" style=\"max-height:240px; min-height: 100px; margin-top: 20%;\"></img></td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/d9d027d4-0d4f-45f5-9d46-49860b562c69\" width=\"100%\" style=\"max-height:360px; min-height: 200px\" controls autoplay loop></video></td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/7716a67b-1bb8-4d44-b128-346cbc35e4ee\" width=\"100%\" style=\"max-height:360px; min-height: 200px\" controls autoplay loop></video></td>\n    </tr>\n    <tr><td>input</td><td>left</td><td>right</td></tr>\n    <tr>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/cc1f1928-cab7-4c4b-90af-928936102e66\" width=\"100%\" style=\"max-height:360px; min-height: 200px\" controls autoplay loop></video></td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/c742ea2c-503a-454f-a61a-10b539100cd9\" width=\"100%\" style=\"max-height:360px; min-height: 200px\" controls autoplay loop></video></td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/442839fa-cc53-4b75-b015-909e44c065e0\" width=\"100%\" style=\"max-height:360px; min-height: 200px\" controls autoplay loop></video></td>\n    </tr>\n    <tr><td>static</td><td>up</td><td>down</td></tr>\n</table>\n\n### Motion Amplitude Control\n\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\n    <tr>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/0020bd54-0ff6-46ad-91ee-d9f0df013772\" width=\"100%\" controls autoplay loop></video>motion 1</td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/d1c26419-54e3-4b86-8ae3-98e12de3022e\" width=\"100%\" controls autoplay loop></video>motion 2</td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/535147a2-049a-4afc-8d2a-017bc778977e\" width=\"100%\" controls autoplay loop></video>motion 3</td>\n        <td align=center><video src=\"https://github.com/user-attachments/assets/bf893d53-2e11-406f-bb9a-2aacffcecd44\" width=\"100%\" controls autoplay loop></video>motion 4</td>\n    </tr>\n</table>\n\n## Limitations\nThere are some known limitations in this experimental release. Texts, hands and crowded human faces may be distorted. The video may cut to another scene when the model does not know how to generate future frames. We are still working on these problems and will update the model as we make progress.\n\n\n## BibTeX\n```\n@misc{createai2024ruyi,\n      title={Ruyi-Mini-7B},\n      author={CreateAI Team},\n      year={2024},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished={\\url{https://github.com/IamCreateAI/Ruyi-Models}}\n}\n```\n\n## Contact Us\n\nYou are welcomed to join our [Discord](https://discord.com/invite/nueQFQwwGw) or Wechat Group (Scan QR code to add Ruyi Assistant and join the official group) for further discussion!\n\n![wechat](https://github.com/user-attachments/assets/cc5e25c6-34ab-4be1-a59b-7d5789264a9c)",
    "meta_json": "{\"pipeline_tag\":\"image-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":17334852644,\"files_count\":12,\"spaces_count\":1,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"RuyiInpaintPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:IamCreateAI:Ruyi-Models\",\"source_url\":\"https://github.com/IamCreateAI/Ruyi-Models\"},{\"type\":\"has_code\",\"target_id\":\"github:IamCreateAI:Ruyi-Models\",\"source_url\":\"https://github.com/IamCreateAI/Ruyi-Models\"},{\"type\":\"has_code\",\"target_id\":\"github:IamCreateAI:Ruyi-Models\",\"source_url\":\"https://github.com/IamCreateAI/Ruyi-Models\"},{\"type\":\"has_code\",\"target_id\":\"github:aigc-apps:EasyAnimate\",\"source_url\":\"https://github.com/aigc-apps/EasyAnimate\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent:HunyuanDiT\",\"source_url\":\"https://github.com/Tencent/HunyuanDiT\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:IamCreateAI:Ruyi-Models}}\",\"source_url\":\"https://github.com/IamCreateAI/Ruyi-Models}}\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.9,
    "content_hash": "5b6da9dadcdc5285741873f3eed0ab6c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/IamCreateAI/Ruyi-Mini-7B\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qvq-72b-preview",
    "name": "QVQ-72B-Preview",
    "author": "Qwen",
    "description": "--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/QVQ-72B-Preview/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text base_model: Qwen/Qwen2-VL-72B tags: - chat library_name: transformers --- <a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> **QVQ-72B-Preview** is an expe...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2_vl",
      "image-to-text",
      "chat",
      "image-text-to-text",
      "conversational",
      "en",
      "arxiv:2409.12191",
      "base_model:qwen/qwen2-vl-72b",
      "base_model:finetune:qwen/qwen2-vl-72b",
      "license:other",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 609,
    "downloads": 383,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/QVQ-72B-Preview",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/QVQ-72B-Preview/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\nbase_model: Qwen/Qwen2-VL-72B\ntags:\n  - chat\nlibrary_name: transformers\n---\n\n\n# QVQ-72B-Preview\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\n**QVQ-72B-Preview** is an experimental research model developed by the Qwen team, focusing on enhancing visual reasoning capabilities.\n\n## Performance\n\n|                | **QVQ-72B-Preview** | o1-2024-12-17 | gpt-4o-2024-05-13 | Claude3.5 Sonnet-20241022 | Qwen2VL-72B |\n|----------------|-----------------|---------------|-------------------|----------------------------|-------------|\n| MMMU(val)      | 70.3            | 77.3          | 69.1              | 70.4                       | 64.5        |\n| MathVista(mini) | 71.4            | 71.0          | 63.8              | 65.3                       | 70.5        |\n| MathVision(full)   | 35.9            | –             | 30.4              | 35.6                       | 25.9        |\n| OlympiadBench  | 20.4            | –             | 25.9              | –                          | 11.2        |\n\n\n**QVQ-72B-Preview** has achieved remarkable performance on various benchmarks. It scored a remarkable 70.3% on the Multimodal Massive Multi-task Understanding (MMMU) benchmark, showcasing QVQ's powerful ability in multidisciplinary understanding and reasoning. Furthermore, the significant improvements on MathVision highlight the model's progress in mathematical reasoning tasks. OlympiadBench also demonstrates the model's enhanced ability to tackle challenging problems.\n\n***But It's Not All Perfect:  Acknowledging the Limitations***\n\nWhile **QVQ-72B-Preview** exhibits promising performance that surpasses expectations, it’s important to acknowledge several limitations:\n\n1. **Language Mixing and Code-Switching:** The model might occasionally mix different languages or unexpectedly switch between them, potentially affecting the clarity of its responses.\n2. **Recursive Reasoning Loops:**  There's a risk of the model getting caught in recursive reasoning loops, leading to lengthy responses that may not even arrive at a final answer.\n3. **Safety and Ethical Considerations:** Robust safety measures are needed to ensure reliable and safe performance. Users should exercise caution when deploying this model.\n4. **Performance and Benchmark Limitations:** Despite the improvements in visual reasoning, QVQ doesn’t entirely replace the capabilities of Qwen2-VL-72B. During multi-step visual reasoning, the model might gradually lose focus on the image content, leading to hallucinations. Moreover, QVQ doesn’t show significant improvement over Qwen2-VL-72B in basic recognition tasks like identifying people, animals, or plants.\n\nNote: Currently, the model only supports single-round dialogues and image outputs. It does not support video inputs.\n## Quickstart\n\nWe offer a toolkit to help you handle various types of visual input more conveniently. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\npip install qwen-vl-utils\n```\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/QVQ-72B-Preview\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/QVQ-72B-Preview\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/QVQ-72B-Preview\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\",\n            },\n            {\"type\": \"text\", \"text\": \"What value should be filled in the blank space?\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qvq-72b-preview,\n    title = {QVQ: To See the World with Wisdom},\n    url = {https://qwenlm.github.io/blog/qvq-72b-preview/},\n    author = {Qwen Team},\n    month = {December},\n    year = {2024}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":73405560320,\"storage_bytes\":146811273776,\"files_count\":50,\"spaces_count\":34,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2VLForConditionalGeneration\"],\"model_type\":\"qwen2_vl\",\"processor_config\":{\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"},\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2409.12191\",\"source_url\":\"https://arxiv.org/abs/2409.12191\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 62.9,
    "content_hash": "9fbeb25a5d0f0bce482064aebef042ef",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/QVQ-72B-Preview\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:thebloke:mistral-7b-instruct-v0.1-gguf",
    "name": "Mistral-7B-Instruct-v0.1-GGUF",
    "author": "TheBloke",
    "description": "--- base_model: mistralai/Mistral-7B-Instruct-v0.1 inference: false license: apache-2.0 model_creator: Mistral AI model_name: Mistral 7B Instruct v0.1 model_type: mistral pipeline_tag: text-generation prompt_template: '<s>[INST]{prompt} [/INST] ' quantized_by: TheBloke tags: - finetuned --- <!-- header start --> <!-- 200823 --> <div style=\"width: auto; margin-left: auto; margin-right: auto\"> <img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; disp...",
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "finetuned",
      "text-generation",
      "base_model:mistralai/mistral-7b-instruct-v0.1",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 608,
    "downloads": 34018,
    "source": "huggingface",
    "source_url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
    "image_url": null,
    "type": "model",
    "body_content": "---\nbase_model: mistralai/Mistral-7B-Instruct-v0.1\ninference: false\nlicense: apache-2.0\nmodel_creator: Mistral AI\nmodel_name: Mistral 7B Instruct v0.1\nmodel_type: mistral\npipeline_tag: text-generation\nprompt_template: '<s>[INST]{prompt} [/INST]\n\n  '\nquantized_by: TheBloke\ntags:\n- finetuned\n---\n\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n<img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n</div>\n<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n    <div style=\"display: flex; flex-direction: column; align-items: flex-start;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://discord.gg/theblokeai\">Chat & support: TheBloke's Discord server</a></p>\n    </div>\n    <div style=\"display: flex; flex-direction: column; align-items: flex-end;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://www.patreon.com/TheBlokeAI\">Want to contribute? TheBloke's Patreon page</a></p>\n    </div>\n</div>\n<div style=\"text-align:center; margin-top: 0em; margin-bottom: 0em\"><p style=\"margin-top: 0.25em; margin-bottom: 0em;\">TheBloke's LLM work is generously supported by a grant from <a href=\"https://a16z.com\">andreessen horowitz (a16z)</a></p></div>\n<hr style=\"margin-top: 1.0em; margin-bottom: 1.0em;\">\n<!-- header end -->\n\n# Mistral 7B Instruct v0.1 - GGUF\n- Model creator: [Mistral AI](https://huggingface.co/mistralai)\n- Original model: [Mistral 7B Instruct v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n\n<!-- description start -->\n## Description\n\nThis repo contains GGUF format model files for [Mistral AI's Mistral 7B Instruct v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n\n<!-- description end -->\n<!-- README_GGUF.md-about-gguf start -->\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplate list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n\n<!-- README_GGUF.md-about-gguf end -->\n<!-- repositories-available start -->\n## Repositories available\n\n* [AWQ model(s) for GPU inference.](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ)\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)\n* [Mistral AI's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n<!-- repositories-available end -->\n\n<!-- prompt-template start -->\n## Prompt template: Mistral\n\n```\n<s>[INST] {prompt} [/INST]\n```\n\n<!-- prompt-template end -->\n\n\n<!-- compatibility_gguf start -->\n## Compatibility\n\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221)\n\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\n\nSequence length note: The model will work at sequence lengths of 4096, or lower. GGUF does not yet have support for the new sliding window sequence length mode, so longer sequence lengths are not supported.\n\n## Explanation of quantisation methods\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_gguf end -->\n\n<!-- README_GGUF.md-provided-files start -->\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| [mistral-7b-instruct-v0.1.Q2_K.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q2_K.gguf) | Q2_K | 2 | 3.08 GB| 5.58 GB | smallest, significant quality loss - not recommended for most purposes |\n| [mistral-7b-instruct-v0.1.Q3_K_S.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q3_K_S.gguf) | Q3_K_S | 3 | 3.16 GB| 5.66 GB | very small, high quality loss |\n| [mistral-7b-instruct-v0.1.Q3_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q3_K_M.gguf) | Q3_K_M | 3 | 3.52 GB| 6.02 GB | very small, high quality loss |\n| [mistral-7b-instruct-v0.1.Q3_K_L.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q3_K_L.gguf) | Q3_K_L | 3 | 3.82 GB| 6.32 GB | small, substantial quality loss |\n| [mistral-7b-instruct-v0.1.Q4_0.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q4_0.gguf) | Q4_0 | 4 | 4.11 GB| 6.61 GB | legacy; small, very high quality loss - prefer using Q3_K_M |\n| [mistral-7b-instruct-v0.1.Q4_K_S.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q4_K_S.gguf) | Q4_K_S | 4 | 4.14 GB| 6.64 GB | small, greater quality loss |\n| [mistral-7b-instruct-v0.1.Q4_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf) | Q4_K_M | 4 | 4.37 GB| 6.87 GB | medium, balanced quality - recommended |\n| [mistral-7b-instruct-v0.1.Q5_0.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q5_0.gguf) | Q5_0 | 5 | 5.00 GB| 7.50 GB | legacy; medium, balanced quality - prefer using Q4_K_M |\n| [mistral-7b-instruct-v0.1.Q5_K_S.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q5_K_S.gguf) | Q5_K_S | 5 | 5.00 GB| 7.50 GB | large, low quality loss - recommended |\n| [mistral-7b-instruct-v0.1.Q5_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf) | Q5_K_M | 5 | 5.13 GB| 7.63 GB | large, very low quality loss - recommended |\n| [mistral-7b-instruct-v0.1.Q6_K.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q6_K.gguf) | Q6_K | 6 | 5.94 GB| 8.44 GB | very large, extremely low quality loss |\n| [mistral-7b-instruct-v0.1.Q8_0.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q8_0.gguf) | Q8_0 | 8 | 7.70 GB| 10.20 GB | very large, extremely low quality loss - not recommended |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n\n\n<!-- README_GGUF.md-provided-files end -->\n\n<!-- README_GGUF.md-how-to-download start -->\n## How to download GGUF files\n\n**Note for manual downloaders:** You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\n\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\n- LM Studio\n- LoLLMS Web UI\n- Faraday.dev\n\n### In `text-generation-webui`\n\nUnder Download Model, you can enter the model repo: TheBloke/Mistral-7B-Instruct-v0.1-GGUF and below it, a specific filename to download, such as: mistral-7b-instruct-v0.1.Q4_K_M.gguf.\n\nThen click Download.\n\n### On the command line, including multiple files at once\n\nI recommend using the `huggingface-hub` Python library:\n\n```shell\npip3 install huggingface-hub\n```\n\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\n\n```shell\nhuggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\n<details>\n  <summary>More advanced huggingface-cli download usage</summary>\n\nYou can also download multiple files at once with a pattern:\n\n```shell\nhuggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\n```\n\nFor more documentation on downloading with `huggingface-cli`, please see: [HF -> Hub Python Library -> Download files -> Download from the CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).\n\nTo accelerate downloads on fast connections (1Gbit/s or higher), install `hf_transfer`:\n\n```shell\npip3 install hf_transfer\n```\n\nAnd set environment variable `HF_HUB_ENABLE_HF_TRANSFER` to `1`:\n\n```shell\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\nWindows Command Line users: You can set the environment variable by running `set HF_HUB_ENABLE_HF_TRANSFER=1` before the download command.\n</details>\n<!-- README_GGUF.md-how-to-download end -->\n\n<!-- README_GGUF.md-how-to-run start -->\n## Example `llama.cpp` command\n\nMake sure you are using `llama.cpp` from commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221) or later.\n\n```shell\n./main -ngl 32 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<s>[INST]{prompt} [/INST]\"\n```\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\n\nSequence length can be 4096 or lower. Mistral's sliding window sequence length is not yet supported in llama.cpp, so do not use sequence lengths longer than 4096.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).\n\n## How to run from Python code\n\nYou can use GGUF models from Python using the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) or [ctransformers](https://github.com/marella/ctransformers) libraries.\n\n### How to load this model in Python code, using ctransformers\n\nI have not tested ctransformers with Mistral models. It may work, but will require that you set the `model_type` to `llama` for now, until ctransformers updates with specific support.\n\n#### First install the package\n\nRun one of the following commands, according to your system:\n\n```shell\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\n```\n\n#### Simple ctransformers example code\n\n```python\nfrom ctransformers import AutoModelForCausalLM\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n```\n\n## How to use with LangChain\n\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\n\n* [LangChain + llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp)\n* [LangChain + ctransformers](https://python.langchain.com/docs/integrations/providers/ctransformers)\n\n<!-- README_GGUF.md-how-to-run end -->\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI's Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nThanks to Clay from [gpus.llm-utils.org](llm-utils)!\n\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Alicia Loh, Stephen Murray, K, Ajan Kanaga, RoA, Magnesian, Deo Leter, Olakabola, Eugene Pentland, zynix, Deep Realms, Raymond Fosdick, Elijah Stavena, Iucharbius, Erik Bjäreholt, Luis Javier Navarrete Lozano, Nicholas, theTransient, John Detwiler, alfie_i, knownsqashed, Mano Prime, Willem Michiel, Enrico Ros, LangChain4j, OG, Michael Dempsey, Pierre Kircher, Pedro Madruga, James Bentley, Thomas Belote, Luke @flexchar, Leonard Tan, Johann-Peter Hartmann, Illia Dulskyi, Fen Risland, Chadd, S_X, Jeff Scroggin, Ken Nordquist, Sean Connelly, Artur Olbinski, Swaroop Kallakuri, Jack West, Ai Maven, David Ziegler, Russ Johnson, transmissions 11, John Villwock, Alps Aficionado, Clay Pascal, Viktor Bowallius, Subspace Studios, Rainer Wilmers, Trenton Dambrowitz, vamX, Michael Levine, 준교 김, Brandon Frisco, Kalila, Trailburnt, Randy H, Talal Aujan, Nathan Dryer, Vadim, 阿明, ReadyPlayerEmma, Tiffany J. Kim, George Stoitzev, Spencer Kim, Jerry Meng, Gabriel Tamborski, Cory Kujawski, Jeffrey Morgan, Spiking Neurons AB, Edmond Seymore, Alexandros Triantafyllidis, Lone Striker, Cap'n Zoog, Nikolai Manek, danny, ya boyyy, Derek Yates, usrbinkat, Mandus, TL, Nathan LeClaire, subjectnull, Imad Khwaja, webtim, Raven Klaugh, Asp the Wyvern, Gabriel Puliatti, Caitlyn Gatomon, Joseph William Delisle, Jonathan Leane, Luke Pendergrass, SuperWojo, Sebastain Graf, Will Dee, Fred von Graf, Andrey, Dan Guido, Daniel P. Andersen, Nitin Borwankar, Elle, Vitor Caleffi, biorpg, jjj, NimbleBox.ai, Pieter, Matthew Berman, terasurfer, Michael Davis, Alex, Stanislav Ovsiannikov\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n<!-- original-model-card start -->\n# Original model card: Mistral AI's Mistral 7B Instruct v0.1\n\n\n# Model Card for Mistral-7B-Instruct-v0.1\n\nThe Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n\nFor full details of this model please read our [release blog post](https://mistral.ai/news/announcing-mistral-7b/)\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[\\INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\ntext = \"\"\"<s>[INST] What is your favourite condiment? [/INST]\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n[INST] Do you have mayonnaise recipes? [/INST]\"\"\"\n\nencodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Model Architecture\nThis instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.\n\n<!-- original-model-card end -->\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":54971166592,\"files_count\":15,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"model_type\":\"mistral\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:oobabooga:text-generation-webui\",\"source_url\":\"https://github.com/oobabooga/text-generation-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:LostRuins:koboldcpp\",\"source_url\":\"https://github.com/LostRuins/koboldcpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ParisNeo:lollms-webui\",\"source_url\":\"https://github.com/ParisNeo/lollms-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:marella:ctransformers\",\"source_url\":\"https://github.com/marella/ctransformers\"},{\"type\":\"has_code\",\"target_id\":\"github:abetlen:llama-cpp-python\",\"source_url\":\"https://github.com/abetlen/llama-cpp-python\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:candle\",\"source_url\":\"https://github.com/huggingface/candle\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:oobabooga:text-generation-webui\",\"source_url\":\"https://github.com/oobabooga/text-generation-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:abetlen:llama-cpp-python\",\"source_url\":\"https://github.com/abetlen/llama-cpp-python\"},{\"type\":\"has_code\",\"target_id\":\"github:marella:ctransformers\",\"source_url\":\"https://github.com/marella/ctransformers\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 77.8,
    "content_hash": "5336dbccbceb7870ec9ac6b2368b547c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:jinaai:reader-lm-1.5b",
    "name": "reader-lm-1.5b",
    "author": "jinaai",
    "description": "--- pipeline_tag: text-generation language: - multilingual inference: false license: cc-by-nc-4.0 library_name: transformers --- <br><br> <p align=\"center\"> <img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\"> </p> <p align=\"center\"> <b>Trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b> </p> A new version of this model has been released! ReaderLM-v2! Blog | Colab Jina Reader...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "multilingual",
      "license:cc-by-nc-4.0",
      "text-generation-inference",
      "deploy:azure",
      "region:eu"
    ],
    "pipeline_tag": "text-generation",
    "likes": 607,
    "downloads": 502,
    "source": "huggingface",
    "source_url": "https://huggingface.co/jinaai/reader-lm-1.5b",
    "image_url": null,
    "type": "model",
    "body_content": "---\npipeline_tag: text-generation\nlanguage:\n- multilingual\ninference: false\nlicense: cc-by-nc-4.0\nlibrary_name: transformers\n---\n\n<br><br>\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\">\n</p>\n\n<p align=\"center\">\n<b>Trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b>\n</p>\n\n[A new version of this model has been released! ReaderLM-v2!](https://huggingface.co/jinaai/ReaderLM-v2)\n\n[Blog](https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown) | [Colab](https://colab.research.google.com/drive/1wXWyj5hOxEHY6WeHbOwEzYAC0WB1I5uA)\n\n# Intro\n\nJina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content.\n\n# Models\n\n| Name            |  Context Length   | Download                                                                                                                                          |\n|-----------------|-------------------|-----------------------------------------------------------------------|\n| reader-lm-0.5b  |  256K             | [🤗 Hugging Face](https://huggingface.co/jinaai/reader-lm-0.5b)       |\n| reader-lm-1.5b  |  256K             | [🤗 Hugging Face](https://huggingface.co/jinaai/reader-lm-1.5b)       |\n|                 |\n\n# Get Started\n\n## On Google Colab\nThe easiest way to experience reader-lm is by running [our Colab notebook](https://colab.research.google.com/drive/1wXWyj5hOxEHY6WeHbOwEzYAC0WB1I5uA), \nwhere we demonstrate how to use reader-lm-1.5b to convert the HackerNews website into markdown. The notebook is optimized to run smoothly on Google Colab’s free T4 GPU tier. You can also load reader-lm-0.5b or change the URL to any website and explore the output. Note that the input (i.e., the prompt) to the model is the raw HTML—no prefix instruction is required.\n\n## Local\n\nTo use this model, you need to install `transformers`:\n\n```bash\npip install transformers<=4.43.4\n```\n\nThen, you can use the model as follows:\n\n```python\n# pip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"jinaai/reader-lm-1.5b\"\n\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\n# example html content\nhtml_content = \"<html><body><h1>Hello, world!</h1></body></html>\"\n\nmessages = [{\"role\": \"user\", \"content\": html_content}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\n\nprint(input_text)\n\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n## AWS Sagemaker & Azure Marketplace\n[AWS 0.5b](https://aws.amazon.com/marketplace/pp/prodview-nli7b6dueo424?sr=0-1&ref_=beagle&applicationId=AWSMPContessa)\n[AWS 1.5b](https://aws.amazon.com/marketplace/pp/prodview-ms27ixcwq3wjk?sr=0-2&ref_=beagle&applicationId=AWSMPContessa)\n[Azure 0.5b](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.reader-lm-500m)\n[Azure 1.5b](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.reader-lm-1500m)\n\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1543714304,\"storage_bytes\":3087467144,\"files_count\":11,\"spaces_count\":3,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 62.8,
    "content_hash": "49e4f9a375c778c36a06f6224d25aefc",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/jinaai/reader-lm-1.5b\",\"fetched_at\":\"2025-12-10T01:31:39.554Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:baai:bge-large-en-v1.5",
    "name": "bge-large-en-v1.5",
    "author": "BAAI",
    "description": "--- tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers - mteb model-index: - name: bge-large-en-v1.5 results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 75.8507462686567 - type: ap value: 38.566457320228245 - type: f1 value: 69.69386648043475 - task: type: Classification da...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "bert",
      "feature-extraction",
      "sentence-similarity",
      "transformers",
      "mteb",
      "en",
      "arxiv:2401.03462",
      "arxiv:2312.15503",
      "arxiv:2311.13534",
      "arxiv:2310.07554",
      "arxiv:2309.07597",
      "license:mit",
      "model-index",
      "text-embeddings-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "feature-extraction",
    "likes": 606,
    "downloads": 3988913,
    "source": "huggingface",
    "source_url": "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- mteb\nmodel-index:\n- name: bge-large-en-v1.5\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 75.8507462686567\n    - type: ap\n      value: 38.566457320228245\n    - type: f1\n      value: 69.69386648043475\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 92.416675\n    - type: ap\n      value: 89.1928861155922\n    - type: f1\n      value: 92.39477019574215\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.175999999999995\n    - type: f1\n      value: 47.80712792870253\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.184999999999995\n    - type: map_at_10\n      value: 55.654\n    - type: map_at_100\n      value: 56.25\n    - type: map_at_1000\n      value: 56.255\n    - type: map_at_3\n      value: 51.742999999999995\n    - type: map_at_5\n      value: 54.129000000000005\n    - type: mrr_at_1\n      value: 40.967\n    - type: mrr_at_10\n      value: 55.96\n    - type: mrr_at_100\n      value: 56.54900000000001\n    - type: mrr_at_1000\n      value: 56.554\n    - type: mrr_at_3\n      value: 51.980000000000004\n    - type: mrr_at_5\n      value: 54.44\n    - type: ndcg_at_1\n      value: 40.184999999999995\n    - type: ndcg_at_10\n      value: 63.542\n    - type: ndcg_at_100\n      value: 65.96499999999999\n    - type: ndcg_at_1000\n      value: 66.08699999999999\n    - type: ndcg_at_3\n      value: 55.582\n    - type: ndcg_at_5\n      value: 59.855000000000004\n    - type: precision_at_1\n      value: 40.184999999999995\n    - type: precision_at_10\n      value: 8.841000000000001\n    - type: precision_at_100\n      value: 0.987\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 22.238\n    - type: precision_at_5\n      value: 15.405\n    - type: recall_at_1\n      value: 40.184999999999995\n    - type: recall_at_10\n      value: 88.407\n    - type: recall_at_100\n      value: 98.72\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 66.714\n    - type: recall_at_5\n      value: 77.027\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 48.567077926750066\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 43.19453389182364\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 64.46555939623092\n    - type: mrr\n      value: 77.82361605768807\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.9554128814735\n    - type: cos_sim_spearman\n      value: 84.65373612172036\n    - type: euclidean_pearson\n      value: 83.2905059954138\n    - type: euclidean_spearman\n      value: 84.52240782811128\n    - type: manhattan_pearson\n      value: 82.99533802997436\n    - type: manhattan_spearman\n      value: 84.20673798475734\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 87.78896103896103\n    - type: f1\n      value: 87.77189310964883\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 39.714538337650495\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 36.90108349284447\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.795\n    - type: map_at_10\n      value: 43.669000000000004\n    - type: map_at_100\n      value: 45.151\n    - type: map_at_1000\n      value: 45.278\n    - type: map_at_3\n      value: 40.006\n    - type: map_at_5\n      value: 42.059999999999995\n    - type: mrr_at_1\n      value: 39.771\n    - type: mrr_at_10\n      value: 49.826\n    - type: mrr_at_100\n      value: 50.504000000000005\n    - type: mrr_at_1000\n      value: 50.549\n    - type: mrr_at_3\n      value: 47.115\n    - type: mrr_at_5\n      value: 48.832\n    - type: ndcg_at_1\n      value: 39.771\n    - type: ndcg_at_10\n      value: 50.217999999999996\n    - type: ndcg_at_100\n      value: 55.454\n    - type: ndcg_at_1000\n      value: 57.37\n    - type: ndcg_at_3\n      value: 44.885000000000005\n    - type: ndcg_at_5\n      value: 47.419\n    - type: precision_at_1\n      value: 39.771\n    - type: precision_at_10\n      value: 9.642000000000001\n    - type: precision_at_100\n      value: 1.538\n    - type: precision_at_1000\n      value: 0.198\n    - type: precision_at_3\n      value: 21.268\n    - type: precision_at_5\n      value: 15.536\n    - type: recall_at_1\n      value: 32.795\n    - type: recall_at_10\n      value: 62.580999999999996\n    - type: recall_at_100\n      value: 84.438\n    - type: recall_at_1000\n      value: 96.492\n    - type: recall_at_3\n      value: 47.071000000000005\n    - type: recall_at_5\n      value: 54.079\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.671\n    - type: map_at_10\n      value: 43.334\n    - type: map_at_100\n      value: 44.566\n    - type: map_at_1000\n      value: 44.702999999999996\n    - type: map_at_3\n      value: 40.343\n    - type: map_at_5\n      value: 41.983\n    - type: mrr_at_1\n      value: 40.764\n    - type: mrr_at_10\n      value: 49.382\n    - type: mrr_at_100\n      value: 49.988\n    - type: mrr_at_1000\n      value: 50.03300000000001\n    - type: mrr_at_3\n      value: 47.293\n    - type: mrr_at_5\n      value: 48.51\n    - type: ndcg_at_1\n      value: 40.764\n    - type: ndcg_at_10\n      value: 49.039\n    - type: ndcg_at_100\n      value: 53.259\n    - type: ndcg_at_1000\n      value: 55.253\n    - type: ndcg_at_3\n      value: 45.091\n    - type: ndcg_at_5\n      value: 46.839999999999996\n    - type: precision_at_1\n      value: 40.764\n    - type: precision_at_10\n      value: 9.191\n    - type: precision_at_100\n      value: 1.476\n    - type: precision_at_1000\n      value: 0.19499999999999998\n    - type: precision_at_3\n      value: 21.72\n    - type: precision_at_5\n      value: 15.299\n    - type: recall_at_1\n      value: 32.671\n    - type: recall_at_10\n      value: 58.816\n    - type: recall_at_100\n      value: 76.654\n    - type: recall_at_1000\n      value: 89.05999999999999\n    - type: recall_at_3\n      value: 46.743\n    - type: recall_at_5\n      value: 51.783\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.328\n    - type: map_at_10\n      value: 53.32599999999999\n    - type: map_at_100\n      value: 54.37499999999999\n    - type: map_at_1000\n      value: 54.429\n    - type: map_at_3\n      value: 49.902\n    - type: map_at_5\n      value: 52.002\n    - type: mrr_at_1\n      value: 46.332\n    - type: mrr_at_10\n      value: 56.858\n    - type: mrr_at_100\n      value: 57.522\n    - type: mrr_at_1000\n      value: 57.54899999999999\n    - type: mrr_at_3\n      value: 54.472\n    - type: mrr_at_5\n      value: 55.996\n    - type: ndcg_at_1\n      value: 46.332\n    - type: ndcg_at_10\n      value: 59.313\n    - type: ndcg_at_100\n      value: 63.266999999999996\n    - type: ndcg_at_1000\n      value: 64.36\n    - type: ndcg_at_3\n      value: 53.815000000000005\n    - type: ndcg_at_5\n      value: 56.814\n    - type: precision_at_1\n      value: 46.332\n    - type: precision_at_10\n      value: 9.53\n    - type: precision_at_100\n      value: 1.238\n    - type: precision_at_1000\n      value: 0.13699999999999998\n    - type: precision_at_3\n      value: 24.054000000000002\n    - type: precision_at_5\n      value: 16.589000000000002\n    - type: recall_at_1\n      value: 40.328\n    - type: recall_at_10\n      value: 73.421\n    - type: recall_at_100\n      value: 90.059\n    - type: recall_at_1000\n      value: 97.81\n    - type: recall_at_3\n      value: 59.009\n    - type: recall_at_5\n      value: 66.352\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.424\n    - type: map_at_10\n      value: 36.332\n    - type: map_at_100\n      value: 37.347\n    - type: map_at_1000\n      value: 37.422\n    - type: map_at_3\n      value: 33.743\n    - type: map_at_5\n      value: 35.176\n    - type: mrr_at_1\n      value: 29.153000000000002\n    - type: mrr_at_10\n      value: 38.233\n    - type: mrr_at_100\n      value: 39.109\n    - type: mrr_at_1000\n      value: 39.164\n    - type: mrr_at_3\n      value: 35.876000000000005\n    - type: mrr_at_5\n      value: 37.169000000000004\n    - type: ndcg_at_1\n      value: 29.153000000000002\n    - type: ndcg_at_10\n      value: 41.439\n    - type: ndcg_at_100\n      value: 46.42\n    - type: ndcg_at_1000\n      value: 48.242000000000004\n    - type: ndcg_at_3\n      value: 36.362\n    - type: ndcg_at_5\n      value: 38.743\n    - type: precision_at_1\n      value: 29.153000000000002\n    - type: precision_at_10\n      value: 6.315999999999999\n    - type: precision_at_100\n      value: 0.927\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 15.443000000000001\n    - type: precision_at_5\n      value: 10.644\n    - type: recall_at_1\n      value: 27.424\n    - type: recall_at_10\n      value: 55.364000000000004\n    - type: recall_at_100\n      value: 78.211\n    - type: recall_at_1000\n      value: 91.74600000000001\n    - type: recall_at_3\n      value: 41.379\n    - type: recall_at_5\n      value: 47.14\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 19.601\n    - type: map_at_10\n      value: 27.826\n    - type: map_at_100\n      value: 29.017\n    - type: map_at_1000\n      value: 29.137\n    - type: map_at_3\n      value: 25.125999999999998\n    - type: map_at_5\n      value: 26.765\n    - type: mrr_at_1\n      value: 24.005000000000003\n    - type: mrr_at_10\n      value: 32.716\n    - type: mrr_at_100\n      value: 33.631\n    - type: mrr_at_1000\n      value: 33.694\n    - type: mrr_at_3\n      value: 29.934\n    - type: mrr_at_5\n      value: 31.630999999999997\n    - type: ndcg_at_1\n      value: 24.005000000000003\n    - type: ndcg_at_10\n      value: 33.158\n    - type: ndcg_at_100\n      value: 38.739000000000004\n    - type: ndcg_at_1000\n      value: 41.495\n    - type: ndcg_at_3\n      value: 28.185\n    - type: ndcg_at_5\n      value: 30.796\n    - type: precision_at_1\n      value: 24.005000000000003\n    - type: precision_at_10\n      value: 5.908\n    - type: precision_at_100\n      value: 1.005\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 13.391\n    - type: precision_at_5\n      value: 9.876\n    - type: recall_at_1\n      value: 19.601\n    - type: recall_at_10\n      value: 44.746\n    - type: recall_at_100\n      value: 68.82300000000001\n    - type: recall_at_1000\n      value: 88.215\n    - type: recall_at_3\n      value: 31.239\n    - type: recall_at_5\n      value: 37.695\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.130000000000003\n    - type: map_at_10\n      value: 40.96\n    - type: map_at_100\n      value: 42.282\n    - type: map_at_1000\n      value: 42.392\n    - type: map_at_3\n      value: 37.889\n    - type: map_at_5\n      value: 39.661\n    - type: mrr_at_1\n      value: 36.958999999999996\n    - type: mrr_at_10\n      value: 46.835\n    - type: mrr_at_100\n      value: 47.644\n    - type: mrr_at_1000\n      value: 47.688\n    - type: mrr_at_3\n      value: 44.562000000000005\n    - type: mrr_at_5\n      value: 45.938\n    - type: ndcg_at_1\n      value: 36.958999999999996\n    - type: ndcg_at_10\n      value: 47.06\n    - type: ndcg_at_100\n      value: 52.345\n    - type: ndcg_at_1000\n      value: 54.35\n    - type: ndcg_at_3\n      value: 42.301\n    - type: ndcg_at_5\n      value: 44.635999999999996\n    - type: precision_at_1\n      value: 36.958999999999996\n    - type: precision_at_10\n      value: 8.479000000000001\n    - type: precision_at_100\n      value: 1.284\n    - type: precision_at_1000\n      value: 0.163\n    - type: precision_at_3\n      value: 20.244\n    - type: precision_at_5\n      value: 14.224999999999998\n    - type: recall_at_1\n      value: 30.130000000000003\n    - type: recall_at_10\n      value: 59.27\n    - type: recall_at_100\n      value: 81.195\n    - type: recall_at_1000\n      value: 94.21199999999999\n    - type: recall_at_3\n      value: 45.885\n    - type: recall_at_5\n      value: 52.016\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.169999999999998\n    - type: map_at_10\n      value: 36.451\n    - type: map_at_100\n      value: 37.791000000000004\n    - type: map_at_1000\n      value: 37.897\n    - type: map_at_3\n      value: 33.109\n    - type: map_at_5\n      value: 34.937000000000005\n    - type: mrr_at_1\n      value: 32.877\n    - type: mrr_at_10\n      value: 42.368\n    - type: mrr_at_100\n      value: 43.201\n    - type: mrr_at_1000\n      value: 43.259\n    - type: mrr_at_3\n      value: 39.763999999999996\n    - type: mrr_at_5\n      value: 41.260000000000005\n    - type: ndcg_at_1\n      value: 32.877\n    - type: ndcg_at_10\n      value: 42.659000000000006\n    - type: ndcg_at_100\n      value: 48.161\n    - type: ndcg_at_1000\n      value: 50.345\n    - type: ndcg_at_3\n      value: 37.302\n    - type: ndcg_at_5\n      value: 39.722\n    - type: precision_at_1\n      value: 32.877\n    - type: precision_at_10\n      value: 7.9\n    - type: precision_at_100\n      value: 1.236\n    - type: precision_at_1000\n      value: 0.158\n    - type: precision_at_3\n      value: 17.846\n    - type: precision_at_5\n      value: 12.9\n    - type: recall_at_1\n      value: 26.169999999999998\n    - type: recall_at_10\n      value: 55.35\n    - type: recall_at_100\n      value: 78.755\n    - type: recall_at_1000\n      value: 93.518\n    - type: recall_at_3\n      value: 40.176\n    - type: recall_at_5\n      value: 46.589000000000006\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.15516666666667\n    - type: map_at_10\n      value: 36.65741666666667\n    - type: map_at_100\n      value: 37.84991666666666\n    - type: map_at_1000\n      value: 37.96316666666667\n    - type: map_at_3\n      value: 33.74974999999999\n    - type: map_at_5\n      value: 35.3765\n    - type: mrr_at_1\n      value: 32.08233333333334\n    - type: mrr_at_10\n      value: 41.033833333333334\n    - type: mrr_at_100\n      value: 41.84524999999999\n    - type: mrr_at_1000\n      value: 41.89983333333333\n    - type: mrr_at_3\n      value: 38.62008333333333\n    - type: mrr_at_5\n      value: 40.03441666666666\n    - type: ndcg_at_1\n      value: 32.08233333333334\n    - type: ndcg_at_10\n      value: 42.229\n    - type: ndcg_at_100\n      value: 47.26716666666667\n    - type: ndcg_at_1000\n      value: 49.43466666666667\n    - type: ndcg_at_3\n      value: 37.36408333333333\n    - type: ndcg_at_5\n      value: 39.6715\n    - type: precision_at_1\n      value: 32.08233333333334\n    - type: precision_at_10\n      value: 7.382583333333334\n    - type: precision_at_100\n      value: 1.16625\n    - type: precision_at_1000\n      value: 0.15408333333333332\n    - type: precision_at_3\n      value: 17.218\n    - type: precision_at_5\n      value: 12.21875\n    - type: recall_at_1\n      value: 27.15516666666667\n    - type: recall_at_10\n      value: 54.36683333333333\n    - type: recall_at_100\n      value: 76.37183333333333\n    - type: recall_at_1000\n      value: 91.26183333333333\n    - type: recall_at_3\n      value: 40.769916666666674\n    - type: recall_at_5\n      value: 46.702333333333335\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.749\n    - type: map_at_10\n      value: 33.001999999999995\n    - type: map_at_100\n      value: 33.891\n    - type: map_at_1000\n      value: 33.993\n    - type: map_at_3\n      value: 30.703999999999997\n    - type: map_at_5\n      value: 31.959\n    - type: mrr_at_1\n      value: 28.834\n    - type: mrr_at_10\n      value: 35.955\n    - type: mrr_at_100\n      value: 36.709\n    - type: mrr_at_1000\n      value: 36.779\n    - type: mrr_at_3\n      value: 33.947\n    - type: mrr_at_5\n      value: 35.089\n    - type: ndcg_at_1\n      value: 28.834\n    - type: ndcg_at_10\n      value: 37.329\n    - type: ndcg_at_100\n      value: 41.79\n    - type: ndcg_at_1000\n      value: 44.169000000000004\n    - type: ndcg_at_3\n      value: 33.184999999999995\n    - type: ndcg_at_5\n      value: 35.107\n    - type: precision_at_1\n      value: 28.834\n    - type: precision_at_10\n      value: 5.7669999999999995\n    - type: precision_at_100\n      value: 0.876\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 14.213000000000001\n    - type: precision_at_5\n      value: 9.754999999999999\n    - type: recall_at_1\n      value: 25.749\n    - type: recall_at_10\n      value: 47.791\n    - type: recall_at_100\n      value: 68.255\n    - type: recall_at_1000\n      value: 85.749\n    - type: recall_at_3\n      value: 36.199\n    - type: recall_at_5\n      value: 41.071999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.777\n    - type: map_at_10\n      value: 25.201\n    - type: map_at_100\n      value: 26.423999999999996\n    - type: map_at_1000\n      value: 26.544\n    - type: map_at_3\n      value: 22.869\n    - type: map_at_5\n      value: 24.023\n    - type: mrr_at_1\n      value: 21.473\n    - type: mrr_at_10\n      value: 29.12\n    - type: mrr_at_100\n      value: 30.144\n    - type: mrr_at_1000\n      value: 30.215999999999998\n    - type: mrr_at_3\n      value: 26.933\n    - type: mrr_at_5\n      value: 28.051\n    - type: ndcg_at_1\n      value: 21.473\n    - type: ndcg_at_10\n      value: 30.003\n    - type: ndcg_at_100\n      value: 35.766\n    - type: ndcg_at_1000\n      value: 38.501000000000005\n    - type: ndcg_at_3\n      value: 25.773000000000003\n    - type: ndcg_at_5\n      value: 27.462999999999997\n    - type: precision_at_1\n      value: 21.473\n    - type: precision_at_10\n      value: 5.482\n    - type: precision_at_100\n      value: 0.975\n    - type: precision_at_1000\n      value: 0.13799999999999998\n    - type: precision_at_3\n      value: 12.205\n    - type: precision_at_5\n      value: 8.692\n    - type: recall_at_1\n      value: 17.777\n    - type: recall_at_10\n      value: 40.582\n    - type: recall_at_100\n      value: 66.305\n    - type: recall_at_1000\n      value: 85.636\n    - type: recall_at_3\n      value: 28.687\n    - type: recall_at_5\n      value: 33.089\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.677\n    - type: map_at_10\n      value: 36.309000000000005\n    - type: map_at_100\n      value: 37.403999999999996\n    - type: map_at_1000\n      value: 37.496\n    - type: map_at_3\n      value: 33.382\n    - type: map_at_5\n      value: 34.98\n    - type: mrr_at_1\n      value: 31.343\n    - type: mrr_at_10\n      value: 40.549\n    - type: mrr_at_100\n      value: 41.342\n    - type: mrr_at_1000\n      value: 41.397\n    - type: mrr_at_3\n      value: 38.029\n    - type: mrr_at_5\n      value: 39.451\n    - type: ndcg_at_1\n      value: 31.343\n    - type: ndcg_at_10\n      value: 42.1\n    - type: ndcg_at_100\n      value: 47.089999999999996\n    - type: ndcg_at_1000\n      value: 49.222\n    - type: ndcg_at_3\n      value: 36.836999999999996\n    - type: ndcg_at_5\n      value: 39.21\n    - type: precision_at_1\n      value: 31.343\n    - type: precision_at_10\n      value: 7.164\n    - type: precision_at_100\n      value: 1.0959999999999999\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 16.915\n    - type: precision_at_5\n      value: 11.940000000000001\n    - type: recall_at_1\n      value: 26.677\n    - type: recall_at_10\n      value: 55.54599999999999\n    - type: recall_at_100\n      value: 77.094\n    - type: recall_at_1000\n      value: 92.01\n    - type: recall_at_3\n      value: 41.191\n    - type: recall_at_5\n      value: 47.006\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.501\n    - type: map_at_10\n      value: 33.102\n    - type: map_at_100\n      value: 34.676\n    - type: map_at_1000\n      value: 34.888000000000005\n    - type: map_at_3\n      value: 29.944\n    - type: map_at_5\n      value: 31.613999999999997\n    - type: mrr_at_1\n      value: 29.447000000000003\n    - type: mrr_at_10\n      value: 37.996\n    - type: mrr_at_100\n      value: 38.946\n    - type: mrr_at_1000\n      value: 38.995000000000005\n    - type: mrr_at_3\n      value: 35.079\n    - type: mrr_at_5\n      value: 36.69\n    - type: ndcg_at_1\n      value: 29.447000000000003\n    - type: ndcg_at_10\n      value: 39.232\n    - type: ndcg_at_100\n      value: 45.247\n    - type: ndcg_at_1000\n      value: 47.613\n    - type: ndcg_at_3\n      value: 33.922999999999995\n    - type: ndcg_at_5\n      value: 36.284\n    - type: precision_at_1\n      value: 29.447000000000003\n    - type: precision_at_10\n      value: 7.648000000000001\n    - type: precision_at_100\n      value: 1.516\n    - type: precision_at_1000\n      value: 0.23900000000000002\n    - type: precision_at_3\n      value: 16.008\n    - type: precision_at_5\n      value: 11.779\n    - type: recall_at_1\n      value: 24.501\n    - type: recall_at_10\n      value: 51.18899999999999\n    - type: recall_at_100\n      value: 78.437\n    - type: recall_at_1000\n      value: 92.842\n    - type: recall_at_3\n      value: 35.808\n    - type: recall_at_5\n      value: 42.197\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.039\n    - type: map_at_10\n      value: 30.377\n    - type: map_at_100\n      value: 31.275\n    - type: map_at_1000\n      value: 31.379\n    - type: map_at_3\n      value: 27.98\n    - type: map_at_5\n      value: 29.358\n    - type: mrr_at_1\n      value: 24.03\n    - type: mrr_at_10\n      value: 32.568000000000005\n    - type: mrr_at_100\n      value: 33.403\n    - type: mrr_at_1000\n      value: 33.475\n    - type: mrr_at_3\n      value: 30.436999999999998\n    - type: mrr_at_5\n      value: 31.796000000000003\n    - type: ndcg_at_1\n      value: 24.03\n    - type: ndcg_at_10\n      value: 35.198\n    - type: ndcg_at_100\n      value: 39.668\n    - type: ndcg_at_1000\n      value: 42.296\n    - type: ndcg_at_3\n      value: 30.709999999999997\n    - type: ndcg_at_5\n      value: 33.024\n    - type: precision_at_1\n      value: 24.03\n    - type: precision_at_10\n      value: 5.564\n    - type: precision_at_100\n      value: 0.828\n    - type: precision_at_1000\n      value: 0.117\n    - type: precision_at_3\n      value: 13.309000000000001\n    - type: precision_at_5\n      value: 9.39\n    - type: recall_at_1\n      value: 22.039\n    - type: recall_at_10\n      value: 47.746\n    - type: recall_at_100\n      value: 68.23599999999999\n    - type: recall_at_1000\n      value: 87.852\n    - type: recall_at_3\n      value: 35.852000000000004\n    - type: recall_at_5\n      value: 41.410000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 15.692999999999998\n    - type: map_at_10\n      value: 26.903\n    - type: map_at_100\n      value: 28.987000000000002\n    - type: map_at_1000\n      value: 29.176999999999996\n    - type: map_at_3\n      value: 22.137\n    - type: map_at_5\n      value: 24.758\n    - type: mrr_at_1\n      value: 35.57\n    - type: mrr_at_10\n      value: 47.821999999999996\n    - type: mrr_at_100\n      value: 48.608000000000004\n    - type: mrr_at_1000\n      value: 48.638999999999996\n    - type: mrr_at_3\n      value: 44.452000000000005\n    - type: mrr_at_5\n      value: 46.546\n    - type: ndcg_at_1\n      value: 35.57\n    - type: ndcg_at_10\n      value: 36.567\n    - type: ndcg_at_100\n      value: 44.085\n    - type: ndcg_at_1000\n      value: 47.24\n    - type: ndcg_at_3\n      value: 29.964000000000002\n    - type: ndcg_at_5\n      value: 32.511\n    - type: precision_at_1\n      value: 35.57\n    - type: precision_at_10\n      value: 11.485\n    - type: precision_at_100\n      value: 1.9619999999999997\n    - type: precision_at_1000\n      value: 0.256\n    - type: precision_at_3\n      value: 22.237000000000002\n    - type: precision_at_5\n      value: 17.471999999999998\n    - type: recall_at_1\n      value: 15.692999999999998\n    - type: recall_at_10\n      value: 43.056\n    - type: recall_at_100\n      value: 68.628\n    - type: recall_at_1000\n      value: 86.075\n    - type: recall_at_3\n      value: 26.918999999999997\n    - type: recall_at_5\n      value: 34.14\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 9.53\n    - type: map_at_10\n      value: 20.951\n    - type: map_at_100\n      value: 30.136000000000003\n    - type: map_at_1000\n      value: 31.801000000000002\n    - type: map_at_3\n      value: 15.021\n    - type: map_at_5\n      value: 17.471999999999998\n    - type: mrr_at_1\n      value: 71.0\n    - type: mrr_at_10\n      value: 79.176\n    - type: mrr_at_100\n      value: 79.418\n    - type: mrr_at_1000\n      value: 79.426\n    - type: mrr_at_3\n      value: 78.125\n    - type: mrr_at_5\n      value: 78.61200000000001\n    - type: ndcg_at_1\n      value: 58.5\n    - type: ndcg_at_10\n      value: 44.106\n    - type: ndcg_at_100\n      value: 49.268\n    - type: ndcg_at_1000\n      value: 56.711999999999996\n    - type: ndcg_at_3\n      value: 48.934\n    - type: ndcg_at_5\n      value: 45.826\n    - type: precision_at_1\n      value: 71.0\n    - type: precision_at_10\n      value: 35.0\n    - type: precision_at_100\n      value: 11.360000000000001\n    - type: precision_at_1000\n      value: 2.046\n    - type: precision_at_3\n      value: 52.833\n    - type: precision_at_5\n      value: 44.15\n    - type: recall_at_1\n      value: 9.53\n    - type: recall_at_10\n      value: 26.811\n    - type: recall_at_100\n      value: 55.916999999999994\n    - type: recall_at_1000\n      value: 79.973\n    - type: recall_at_3\n      value: 16.413\n    - type: recall_at_5\n      value: 19.980999999999998\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 51.519999999999996\n    - type: f1\n      value: 46.36601294761231\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 74.413\n    - type: map_at_10\n      value: 83.414\n    - type: map_at_100\n      value: 83.621\n    - type: map_at_1000\n      value: 83.635\n    - type: map_at_3\n      value: 82.337\n    - type: map_at_5\n      value: 83.039\n    - type: mrr_at_1\n      value: 80.19800000000001\n    - type: mrr_at_10\n      value: 87.715\n    - type: mrr_at_100\n      value: 87.778\n    - type: mrr_at_1000\n      value: 87.779\n    - type: mrr_at_3\n      value: 87.106\n    - type: mrr_at_5\n      value: 87.555\n    - type: ndcg_at_1\n      value: 80.19800000000001\n    - type: ndcg_at_10\n      value: 87.182\n    - type: ndcg_at_100\n      value: 87.90299999999999\n    - type: ndcg_at_1000\n      value: 88.143\n    - type: ndcg_at_3\n      value: 85.60600000000001\n    - type: ndcg_at_5\n      value: 86.541\n    - type: precision_at_1\n      value: 80.19800000000001\n    - type: precision_at_10\n      value: 10.531\n    - type: precision_at_100\n      value: 1.113\n    - type: precision_at_1000\n      value: 0.11499999999999999\n    - type: precision_at_3\n      value: 32.933\n    - type: precision_at_5\n      value: 20.429\n    - type: recall_at_1\n      value: 74.413\n    - type: recall_at_10\n      value: 94.363\n    - type: recall_at_100\n      value: 97.165\n    - type: recall_at_1000\n      value: 98.668\n    - type: recall_at_3\n      value: 90.108\n    - type: recall_at_5\n      value: 92.52\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.701\n    - type: map_at_10\n      value: 37.122\n    - type: map_at_100\n      value: 39.178000000000004\n    - type: map_at_1000\n      value: 39.326\n    - type: map_at_3\n      value: 32.971000000000004\n    - type: map_at_5\n      value: 35.332\n    - type: mrr_at_1\n      value: 44.753\n    - type: mrr_at_10\n      value: 53.452\n    - type: mrr_at_100\n      value: 54.198\n    - type: mrr_at_1000\n      value: 54.225\n    - type: mrr_at_3\n      value: 50.952\n    - type: mrr_at_5\n      value: 52.464\n    - type: ndcg_at_1\n      value: 44.753\n    - type: ndcg_at_10\n      value: 45.021\n    - type: ndcg_at_100\n      value: 52.028\n    - type: ndcg_at_1000\n      value: 54.596000000000004\n    - type: ndcg_at_3\n      value: 41.622\n    - type: ndcg_at_5\n      value: 42.736000000000004\n    - type: precision_at_1\n      value: 44.753\n    - type: precision_at_10\n      value: 12.284\n    - type: precision_at_100\n      value: 1.955\n    - type: precision_at_1000\n      value: 0.243\n    - type: precision_at_3\n      value: 27.828999999999997\n    - type: precision_at_5\n      value: 20.061999999999998\n    - type: recall_at_1\n      value: 22.701\n    - type: recall_at_10\n      value: 51.432\n    - type: recall_at_100\n      value: 77.009\n    - type: recall_at_1000\n      value: 92.511\n    - type: recall_at_3\n      value: 37.919000000000004\n    - type: recall_at_5\n      value: 44.131\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.189\n    - type: map_at_10\n      value: 66.24600000000001\n    - type: map_at_100\n      value: 67.098\n    - type: map_at_1000\n      value: 67.149\n    - type: map_at_3\n      value: 62.684\n    - type: map_at_5\n      value: 64.974\n    - type: mrr_at_1\n      value: 80.378\n    - type: mrr_at_10\n      value: 86.127\n    - type: mrr_at_100\n      value: 86.29299999999999\n    - type: mrr_at_1000\n      value: 86.297\n    - type: mrr_at_3\n      value: 85.31400000000001\n    - type: mrr_at_5\n      value: 85.858\n    - type: ndcg_at_1\n      value: 80.378\n    - type: ndcg_at_10\n      value: 74.101\n    - type: ndcg_at_100\n      value: 76.993\n    - type: ndcg_at_1000\n      value: 77.948\n    - type: ndcg_at_3\n      value: 69.232\n    - type: ndcg_at_5\n      value: 72.04599999999999\n    - type: precision_at_1\n      value: 80.378\n    - type: precision_at_10\n      value: 15.595999999999998\n    - type: precision_at_100\n      value: 1.7840000000000003\n    - type: precision_at_1000\n      value: 0.191\n    - type: precision_at_3\n      value: 44.884\n    - type: precision_at_5\n      value: 29.145\n    - type: recall_at_1\n      value: 40.189\n    - type: recall_at_10\n      value: 77.981\n    - type: recall_at_100\n      value: 89.21\n    - type: recall_at_1000\n      value: 95.48299999999999\n    - type: recall_at_3\n      value: 67.326\n    - type: recall_at_5\n      value: 72.863\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 92.84599999999999\n    - type: ap\n      value: 89.4710787567357\n    - type: f1\n      value: 92.83752676932258\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.132\n    - type: map_at_10\n      value: 35.543\n    - type: map_at_100\n      value: 36.702\n    - type: map_at_1000\n      value: 36.748999999999995\n    - type: map_at_3\n      value: 31.737\n    - type: map_at_5\n      value: 33.927\n    - type: mrr_at_1\n      value: 23.782\n    - type: mrr_at_10\n      value: 36.204\n    - type: mrr_at_100\n      value: 37.29\n    - type: mrr_at_1000\n      value: 37.330999999999996\n    - type: mrr_at_3\n      value: 32.458999999999996\n    - type: mrr_at_5\n      value: 34.631\n    - type: ndcg_at_1\n      value: 23.782\n    - type: ndcg_at_10\n      value: 42.492999999999995\n    - type: ndcg_at_100\n      value: 47.985\n    - type: ndcg_at_1000\n      value: 49.141\n    - type: ndcg_at_3\n      value: 34.748000000000005\n    - type: ndcg_at_5\n      value: 38.651\n    - type: precision_at_1\n      value: 23.782\n    - type: precision_at_10\n      value: 6.665\n    - type: precision_at_100\n      value: 0.941\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 14.776\n    - type: precision_at_5\n      value: 10.84\n    - type: recall_at_1\n      value: 23.132\n    - type: recall_at_10\n      value: 63.794\n    - type: recall_at_100\n      value: 89.027\n    - type: recall_at_1000\n      value: 97.807\n    - type: recall_at_3\n      value: 42.765\n    - type: recall_at_5\n      value: 52.11\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 94.59188326493388\n    - type: f1\n      value: 94.3842594786827\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 79.49384404924761\n    - type: f1\n      value: 59.7580539534629\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 77.56220578345663\n    - type: f1\n      value: 75.27228165561478\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 80.53463349024884\n    - type: f1\n      value: 80.4893958236536\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 32.56100273484962\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 31.470380028839607\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 32.06102792457849\n    - type: mrr\n      value: 33.30709199672238\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 6.776999999999999\n    - type: map_at_10\n      value: 14.924000000000001\n    - type: map_at_100\n      value: 18.955\n    - type: map_at_1000\n      value: 20.538999999999998\n    - type: map_at_3\n      value: 10.982\n    - type: map_at_5\n      value: 12.679000000000002\n    - type: mrr_at_1\n      value: 47.988\n    - type: mrr_at_10\n      value: 57.232000000000006\n    - type: mrr_at_100\n      value: 57.818999999999996\n    - type: mrr_at_1000\n      value: 57.847\n    - type: mrr_at_3\n      value: 54.901999999999994\n    - type: mrr_at_5\n      value: 56.481\n    - type: ndcg_at_1\n      value: 46.594\n    - type: ndcg_at_10\n      value: 38.129000000000005\n    - type: ndcg_at_100\n      value: 35.54\n    - type: ndcg_at_1000\n      value: 44.172\n    - type: ndcg_at_3\n      value: 43.025999999999996\n    - type: ndcg_at_5\n      value: 41.052\n    - type: precision_at_1\n      value: 47.988\n    - type: precision_at_10\n      value: 28.111000000000004\n    - type: precision_at_100\n      value: 8.929\n    - type: precision_at_1000\n      value: 2.185\n    - type: precision_at_3\n      value: 40.144000000000005\n    - type: precision_at_5\n      value: 35.232\n    - type: recall_at_1\n      value: 6.776999999999999\n    - type: recall_at_10\n      value: 19.289\n    - type: recall_at_100\n      value: 36.359\n    - type: recall_at_1000\n      value: 67.54\n    - type: recall_at_3\n      value: 11.869\n    - type: recall_at_5\n      value: 14.999\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.108000000000004\n    - type: map_at_10\n      value: 47.126000000000005\n    - type: map_at_100\n      value: 48.171\n    - type: map_at_1000\n      value: 48.199\n    - type: map_at_3\n      value: 42.734\n    - type: map_at_5\n      value: 45.362\n    - type: mrr_at_1\n      value: 34.936\n    - type: mrr_at_10\n      value: 49.571\n    - type: mrr_at_100\n      value: 50.345\n    - type: mrr_at_1000\n      value: 50.363\n    - type: mrr_at_3\n      value: 45.959\n    - type: mrr_at_5\n      value: 48.165\n    - type: ndcg_at_1\n      value: 34.936\n    - type: ndcg_at_10\n      value: 55.028999999999996\n    - type: ndcg_at_100\n      value: 59.244\n    - type: ndcg_at_1000\n      value: 59.861\n    - type: ndcg_at_3\n      value: 46.872\n    - type: ndcg_at_5\n      value: 51.217999999999996\n    - type: precision_at_1\n      value: 34.936\n    - type: precision_at_10\n      value: 9.099\n    - type: precision_at_100\n      value: 1.145\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 21.456\n    - type: precision_at_5\n      value: 15.411\n    - type: recall_at_1\n      value: 31.108000000000004\n    - type: recall_at_10\n      value: 76.53999999999999\n    - type: recall_at_100\n      value: 94.39\n    - type: recall_at_1000\n      value: 98.947\n    - type: recall_at_3\n      value: 55.572\n    - type: recall_at_5\n      value: 65.525\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.56400000000001\n    - type: map_at_10\n      value: 85.482\n    - type: map_at_100\n      value: 86.114\n    - type: map_at_1000\n      value: 86.13\n    - type: map_at_3\n      value: 82.607\n    - type: map_at_5\n      value: 84.405\n    - type: mrr_at_1\n      value: 82.42\n    - type: mrr_at_10\n      value: 88.304\n    - type: mrr_at_100\n      value: 88.399\n    - type: mrr_at_1000\n      value: 88.399\n    - type: mrr_at_3\n      value: 87.37\n    - type: mrr_at_5\n      value: 88.024\n    - type: ndcg_at_1\n      value: 82.45\n    - type: ndcg_at_10\n      value: 89.06500000000001\n    - type: ndcg_at_100\n      value: 90.232\n    - type: ndcg_at_1000\n      value: 90.305\n    - type: ndcg_at_3\n      value: 86.375\n    - type: ndcg_at_5\n      value: 87.85300000000001\n    - type: precision_at_1\n      value: 82.45\n    - type: precision_at_10\n      value: 13.486999999999998\n    - type: precision_at_100\n      value: 1.534\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.813\n    - type: precision_at_5\n      value: 24.773999999999997\n    - type: recall_at_1\n      value: 71.56400000000001\n    - type: recall_at_10\n      value: 95.812\n    - type: recall_at_100\n      value: 99.7\n    - type: recall_at_1000\n      value: 99.979\n    - type: recall_at_3\n      value: 87.966\n    - type: recall_at_5\n      value: 92.268\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 57.241876648614145\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 64.66212576446223\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.308\n    - type: map_at_10\n      value: 13.803\n    - type: map_at_100\n      value: 16.176\n    - type: map_at_1000\n      value: 16.561\n    - type: map_at_3\n      value: 9.761000000000001\n    - type: map_at_5\n      value: 11.802\n    - type: mrr_at_1\n      value: 26.200000000000003\n    - type: mrr_at_10\n      value: 37.621\n    - type: mrr_at_100\n      value: 38.767\n    - type: mrr_at_1000\n      value: 38.815\n    - type: mrr_at_3\n      value: 34.117\n    - type: mrr_at_5\n      value: 36.107\n    - type: ndcg_at_1\n      value: 26.200000000000003\n    - type: ndcg_at_10\n      value: 22.64\n    - type: ndcg_at_100\n      value: 31.567\n    - type: ndcg_at_1000\n      value: 37.623\n    - type: ndcg_at_3\n      value: 21.435000000000002\n    - type: ndcg_at_5\n      value: 18.87\n    - type: precision_at_1\n      value: 26.200000000000003\n    - type: precision_at_10\n      value: 11.74\n    - type: precision_at_100\n      value: 2.465\n    - type: precision_at_1000\n      value: 0.391\n    - type: precision_at_3\n      value: 20.033\n    - type: precision_at_5\n      value: 16.64\n    - type: recall_at_1\n      value: 5.308\n    - type: recall_at_10\n      value: 23.794999999999998\n    - type: recall_at_100\n      value: 50.015\n    - type: recall_at_1000\n      value: 79.283\n    - type: recall_at_3\n      value: 12.178\n    - type: recall_at_5\n      value: 16.882\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.93231134675553\n    - type: cos_sim_spearman\n      value: 81.68319292603205\n    - type: euclidean_pearson\n      value: 81.8396814380367\n    - type: euclidean_spearman\n      value: 81.24641903349945\n    - type: manhattan_pearson\n      value: 81.84698799204274\n    - type: manhattan_spearman\n      value: 81.24269997904105\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.73241671587446\n    - type: cos_sim_spearman\n      value: 79.05091082971826\n    - type: euclidean_pearson\n      value: 83.91146869578044\n    - type: euclidean_spearman\n      value: 79.87978465370936\n    - type: manhattan_pearson\n      value: 83.90888338917678\n    - type: manhattan_spearman\n      value: 79.87482848584241\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.14970731146177\n    - type: cos_sim_spearman\n      value: 86.37363490084627\n    - type: euclidean_pearson\n      value: 83.02154218530433\n    - type: euclidean_spearman\n      value: 83.80258761957367\n    - type: manhattan_pearson\n      value: 83.01664495119347\n    - type: manhattan_spearman\n      value: 83.77567458007952\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.40474139886784\n    - type: cos_sim_spearman\n      value: 82.77768789165984\n    - type: euclidean_pearson\n      value: 80.7065877443695\n    - type: euclidean_spearman\n      value: 81.375940662505\n    - type: manhattan_pearson\n      value: 80.6507552270278\n    - type: manhattan_spearman\n      value: 81.32782179098741\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.08585968722274\n    - type: cos_sim_spearman\n      value: 88.03110031451399\n    - type: euclidean_pearson\n      value: 85.74012019602384\n    - type: euclidean_spearman\n      value: 86.13592849438209\n    - type: manhattan_pearson\n      value: 85.74404842369206\n    - type: manhattan_spearman\n      value: 86.14492318960154\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.95069052788875\n    - type: cos_sim_spearman\n      value: 86.4867991595147\n    - type: euclidean_pearson\n      value: 84.31013325754635\n    - type: euclidean_spearman\n      value: 85.01529258006482\n    - type: manhattan_pearson\n      value: 84.26995570085374\n    - type: manhattan_spearman\n      value: 84.96982104986162\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.54617647971897\n    - type: cos_sim_spearman\n      value: 87.49834181751034\n    - type: euclidean_pearson\n      value: 86.01015322577122\n    - type: euclidean_spearman\n      value: 84.63362652063199\n    - type: manhattan_pearson\n      value: 86.13807574475706\n    - type: manhattan_spearman\n      value: 84.7772370721132\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 67.20047755786615\n    - type: cos_sim_spearman\n      value: 67.05324077987636\n    - type: euclidean_pearson\n      value: 66.91930642976601\n    - type: euclidean_spearman\n      value: 65.21491856099105\n    - type: manhattan_pearson\n      value: 66.78756851976624\n    - type: manhattan_spearman\n      value: 65.12356257740728\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.19852871539686\n    - type: cos_sim_spearman\n      value: 87.5161895296395\n    - type: euclidean_pearson\n      value: 84.59848645207485\n    - type: euclidean_spearman\n      value: 85.26427328757919\n    - type: manhattan_pearson\n      value: 84.59747366996524\n    - type: manhattan_spearman\n      value: 85.24045855146915\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 87.63320317811032\n    - type: mrr\n      value: 96.26242947321379\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 60.928000000000004\n    - type: map_at_10\n      value: 70.112\n    - type: map_at_100\n      value: 70.59299999999999\n    - type: map_at_1000\n      value: 70.623\n    - type: map_at_3\n      value: 66.846\n    - type: map_at_5\n      value: 68.447\n    - type: mrr_at_1\n      value: 64.0\n    - type: mrr_at_10\n      value: 71.212\n    - type: mrr_at_100\n      value: 71.616\n    - type: mrr_at_1000\n      value: 71.64500000000001\n    - type: mrr_at_3\n      value: 68.77799999999999\n    - type: mrr_at_5\n      value: 70.094\n    - type: ndcg_at_1\n      value: 64.0\n    - type: ndcg_at_10\n      value: 74.607\n    - type: ndcg_at_100\n      value: 76.416\n    - type: ndcg_at_1000\n      value: 77.102\n    - type: ndcg_at_3\n      value: 69.126\n    - type: ndcg_at_5\n      value: 71.41300000000001\n    - type: precision_at_1\n      value: 64.0\n    - type: precision_at_10\n      value: 9.933\n    - type: precision_at_100\n      value: 1.077\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 26.556\n    - type: precision_at_5\n      value: 17.467\n    - type: recall_at_1\n      value: 60.928000000000004\n    - type: recall_at_10\n      value: 87.322\n    - type: recall_at_100\n      value: 94.833\n    - type: recall_at_1000\n      value: 100.0\n    - type: recall_at_3\n      value: 72.628\n    - type: recall_at_5\n      value: 78.428\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.86237623762376\n    - type: cos_sim_ap\n      value: 96.72586477206649\n    - type: cos_sim_f1\n      value: 93.01858362631845\n    - type: cos_sim_precision\n      value: 93.4409687184662\n    - type: cos_sim_recall\n      value: 92.60000000000001\n    - type: dot_accuracy\n      value: 99.78019801980199\n    - type: dot_ap\n      value: 93.72748205246228\n    - type: dot_f1\n      value: 89.04109589041096\n    - type: dot_precision\n      value: 87.16475095785441\n    - type: dot_recall\n      value: 91.0\n    - type: euclidean_accuracy\n      value: 99.85445544554456\n    - type: euclidean_ap\n      value: 96.6661459876145\n    - type: euclidean_f1\n      value: 92.58337481333997\n    - type: euclidean_precision\n      value: 92.17046580773042\n    - type: euclidean_recall\n      value: 93.0\n    - type: manhattan_accuracy\n      value: 99.85445544554456\n    - type: manhattan_ap\n      value: 96.6883549244056\n    - type: manhattan_f1\n      value: 92.57598405580468\n    - type: manhattan_precision\n      value: 92.25422045680239\n    - type: manhattan_recall\n      value: 92.9\n    - type: max_accuracy\n      value: 99.86237623762376\n    - type: max_ap\n      value: 96.72586477206649\n    - type: max_f1\n      value: 93.01858362631845\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 66.39930057069995\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 34.96398659903402\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 55.946944700355395\n    - type: mrr\n      value: 56.97151398438164\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 31.541657650692905\n    - type: cos_sim_spearman\n      value: 31.605804192286303\n    - type: dot_pearson\n      value: 28.26905996736398\n    - type: dot_spearman\n      value: 27.864801765851187\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.22599999999999998\n    - type: map_at_10\n      value: 1.8870000000000002\n    - type: map_at_100\n      value: 9.78\n    - type: map_at_1000\n      value: 22.514\n    - type: map_at_3\n      value: 0.6669999999999999\n    - type: map_at_5\n      value: 1.077\n    - type: mrr_at_1\n      value: 82.0\n    - type: mrr_at_10\n      value: 89.86699999999999\n    - type: mrr_at_100\n      value: 89.86699999999999\n    - type: mrr_at_1000\n      value: 89.86699999999999\n    - type: mrr_at_3\n      value: 89.667\n    - type: mrr_at_5\n      value: 89.667\n    - type: ndcg_at_1\n      value: 79.0\n    - type: ndcg_at_10\n      value: 74.818\n    - type: ndcg_at_100\n      value: 53.715999999999994\n    - type: ndcg_at_1000\n      value: 47.082\n    - type: ndcg_at_3\n      value: 82.134\n    - type: ndcg_at_5\n      value: 79.81899999999999\n    - type: precision_at_1\n      value: 82.0\n    - type: precision_at_10\n      value: 78.0\n    - type: precision_at_100\n      value: 54.48\n    - type: precision_at_1000\n      value: 20.518\n    - type: precision_at_3\n      value: 87.333\n    - type: precision_at_5\n      value: 85.2\n    - type: recall_at_1\n      value: 0.22599999999999998\n    - type: recall_at_10\n      value: 2.072\n    - type: recall_at_100\n      value: 13.013\n    - type: recall_at_1000\n      value: 43.462\n    - type: recall_at_3\n      value: 0.695\n    - type: recall_at_5\n      value: 1.139\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 2.328\n    - type: map_at_10\n      value: 9.795\n    - type: map_at_100\n      value: 15.801000000000002\n    - type: map_at_1000\n      value: 17.23\n    - type: map_at_3\n      value: 4.734\n    - type: map_at_5\n      value: 6.644\n    - type: mrr_at_1\n      value: 30.612000000000002\n    - type: mrr_at_10\n      value: 46.902\n    - type: mrr_at_100\n      value: 47.495\n    - type: mrr_at_1000\n      value: 47.495\n    - type: mrr_at_3\n      value: 41.156\n    - type: mrr_at_5\n      value: 44.218\n    - type: ndcg_at_1\n      value: 28.571\n    - type: ndcg_at_10\n      value: 24.806\n    - type: ndcg_at_100\n      value: 36.419000000000004\n    - type: ndcg_at_1000\n      value: 47.272999999999996\n    - type: ndcg_at_3\n      value: 25.666\n    - type: ndcg_at_5\n      value: 25.448999999999998\n    - type: precision_at_1\n      value: 30.612000000000002\n    - type: precision_at_10\n      value: 23.061\n    - type: precision_at_100\n      value: 7.714\n    - type: precision_at_1000\n      value: 1.484\n    - type: precision_at_3\n      value: 26.531\n    - type: precision_at_5\n      value: 26.122\n    - type: recall_at_1\n      value: 2.328\n    - type: recall_at_10\n      value: 16.524\n    - type: recall_at_100\n      value: 47.179\n    - type: recall_at_1000\n      value: 81.22200000000001\n    - type: recall_at_3\n      value: 5.745\n    - type: recall_at_5\n      value: 9.339\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 70.9142\n    - type: ap\n      value: 14.335574772555415\n    - type: f1\n      value: 54.62839595194111\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 59.94340690435768\n    - type: f1\n      value: 60.286487936731916\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 51.26597708987974\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 87.48882398521786\n    - type: cos_sim_ap\n      value: 79.04326607602204\n    - type: cos_sim_f1\n      value: 71.64566826860633\n    - type: cos_sim_precision\n      value: 70.55512918905092\n    - type: cos_sim_recall\n      value: 72.77044854881267\n    - type: dot_accuracy\n      value: 84.19264469213805\n    - type: dot_ap\n      value: 67.96360043562528\n    - type: dot_f1\n      value: 64.06418393006827\n    - type: dot_precision\n      value: 58.64941898706424\n    - type: dot_recall\n      value: 70.58047493403694\n    - type: euclidean_accuracy\n      value: 87.45902127913214\n    - type: euclidean_ap\n      value: 78.9742237648272\n    - type: euclidean_f1\n      value: 71.5553235908142\n    - type: euclidean_precision\n      value: 70.77955601445535\n    - type: euclidean_recall\n      value: 72.34828496042216\n    - type: manhattan_accuracy\n      value: 87.41729749061214\n    - type: manhattan_ap\n      value: 78.90073137580596\n    - type: manhattan_f1\n      value: 71.3942611553533\n    - type: manhattan_precision\n      value: 68.52705653967483\n    - type: manhattan_recall\n      value: 74.51187335092348\n    - type: max_accuracy\n      value: 87.48882398521786\n    - type: max_ap\n      value: 79.04326607602204\n    - type: max_f1\n      value: 71.64566826860633\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 88.68125897465751\n    - type: cos_sim_ap\n      value: 85.6003454431979\n    - type: cos_sim_f1\n      value: 77.6957163958641\n    - type: cos_sim_precision\n      value: 73.0110366307807\n    - type: cos_sim_recall\n      value: 83.02279026793964\n    - type: dot_accuracy\n      value: 87.7672992587418\n    - type: dot_ap\n      value: 82.4971301112899\n    - type: dot_f1\n      value: 75.90528233151184\n    - type: dot_precision\n      value: 72.0370626469368\n    - type: dot_recall\n      value: 80.21250384970742\n    - type: euclidean_accuracy\n      value: 88.4503434625684\n    - type: euclidean_ap\n      value: 84.91949884748384\n    - type: euclidean_f1\n      value: 76.92365018444684\n    - type: euclidean_precision\n      value: 74.53245721712759\n    - type: euclidean_recall\n      value: 79.47336002463813\n    - type: manhattan_accuracy\n      value: 88.47556952691427\n    - type: manhattan_ap\n      value: 84.8963689101517\n    - type: manhattan_f1\n      value: 76.85901249256395\n    - type: manhattan_precision\n      value: 74.31693989071039\n    - type: manhattan_recall\n      value: 79.58115183246073\n    - type: max_accuracy\n      value: 88.68125897465751\n    - type: max_ap\n      value: 85.6003454431979\n    - type: max_f1\n      value: 77.6957163958641\nlicense: mit\nlanguage:\n- en\n---\n\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#contact\">Contact</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\nFor more details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).\n\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using [bge-m3](https://huggingface.co/BAAI/bge-m3).\n\n\n[English](README.md) | [中文](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model that supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) :fire:\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) and [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size 🤗**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `为这个句子生成表示以用于检索相关文章：`  |\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"样例数据-1\", \"样例数据-2\"]\nsentences_2 = [\"样例数据-3\", \"样例数据-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"样例文档-1\", \"样例文档-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"样例数据-1\", \"样例数据-2\"]\nsentences_2 = [\"样例数据-3\", \"样例数据-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"样例文档-1\", \"样例文档-2\"]\ninstruction = \"为这个句子生成表示以用于检索相关文章：\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n)\nmodel.query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"样例数据-1\", \"样例数据-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n#### Usage of the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction  # type: ignore\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\")\nmodel_ort = ORTModelForFeatureExtraction.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\",file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\nsentences = [\"样例数据-1\", \"样例数据-2\"]\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\nmodel_output_ort = model_ort(**encoded_input)\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# model_output and model_output_ort are identical\n\n```\n\nIts also possible to deploy the onnx files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-large-en-v1.5\", device=\"cpu\", engine=\"optimum\" # or engine=\"torch\"\n))\n\nasync def main(): \n    async with engine:\n        embeddings, usage = await engine.embed(sentences=sentences)\nasyncio.run(main())\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n## Contact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn). \n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.\n\n",
    "meta_json": "{\"pipeline_tag\":\"feature-extraction\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":335142400,\"storage_bytes\":7033733981,\"files_count\":14,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BertModel\"],\"model_type\":\"bert\",\"tokenizer_config\":{\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\",\"pad_token\":\"[PAD]\",\"sep_token\":\"[SEP]\",\"unk_token\":\"[UNK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:michaelfeil:infinity\",\"source_url\":\"https://github.com/michaelfeil/infinity\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:staoxiao:RetroMAE\",\"source_url\":\"https://github.com/staoxiao/RetroMAE\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2401.03462\",\"source_url\":\"https://arxiv.org/abs/2401.03462\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.15503\",\"source_url\":\"https://arxiv.org/abs/2312.15503\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.13534\",\"source_url\":\"https://arxiv.org/abs/2311.13534\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.07554\",\"source_url\":\"https://arxiv.org/abs/2310.07554\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.07597\",\"source_url\":\"https://arxiv.org/abs/2309.07597\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 77.8,
    "content_hash": "19e6bf78f329c357d14d2a752d216d25",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/BAAI/bge-large-en-v1.5\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mistralai:mamba-codestral-7b-v0.1",
    "name": "Mamba-Codestral-7B-v0.1",
    "author": "mistralai",
    "description": "--- library_name: vllm license: apache-2.0 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>. tags: - mistral-common --- Codestral Mamba is an open code model based on the Mamba2 architecture. It performs on par with state-of-the-art Transformer-based code models. \\ You can read more in the official blog post. It is recommended to use with mistral-inference or directly with the ...",
    "tags": [
      "vllm",
      "safetensors",
      "mistral-common",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 606,
    "downloads": 140102,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: vllm\nlicense: apache-2.0\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Model Card for Mamba-Codestral-7B-v0.1\n\nCodestral Mamba is an open code model based on the Mamba2 architecture. It performs on par with state-of-the-art Transformer-based code models. \\\nYou can read more in the [official blog post](https://mistral.ai/news/codestral-mamba/).\n\n\n## Installation\n\nIt is recommended to use `mistralai/Mamba-Codestral-7B-v0.1` with [mistral-inference](https://github.com/mistralai/mistral-inference) \n\n\n```\npip install mistral_inference>=1 mamba-ssm causal-conv1d\n```\n\nor directly with the original [`mamba`](https://github.com/state-spaces/mamba) package:\n\n```\npip install mamba_ssm causal-conv1d\n```\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', 'Mamba-Codestral-7B-v0.1')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mamba-Codestral-7B-v0.1\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.\n\n```\nmistral-chat $HOME/mistral_models/Mamba-Codestral-7B-v0.1 --instruct  --max_tokens 256\n```\n\n## Evaluation\nWe evaluate Codestral Mamba, Codestral and open-weight models of similar size on industry-standard benchmarks.\n| Benchmarks | HumanEval | MBPP | Spider | CruxE | HumanEval C++ | HumanEvalJava |HumanEvalJS |HumanEval Bash |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| CodeGemma 1.1 7B | 61.0% | 67.7% | 46.3% | 50.4% | 49.1% | 41.8% | 52.2% | 9.4% |\n| CodeLlama 7B | 31.1% | 48.2% | 29.3% | 50.1% | 31.7% | 29.7% | 31.7% | 11.4% |\n| DeepSeek v1.5 7B | 65.9% | **70.8%** | **61.2%** | 55.5% | 59.0% | **62.7%** | 60.9% | **33.5%** |\n| **Codestral Mamba (7B)** | **75.0%** | 68.5% | 58.8% | **57.8%** | **59.8%** | 57.0% | **61.5%** | 31.1% |\n|\n| **Codestral (22B)** | **81.1%%** | **78.2%%** | **63.5%%** | 51.3% | **65.2%** | **63.3%** | -  | **42.4%** |\n| CodeLlama 34B | 43.3% | 75.1% | 50.8% | 55.2% | 51.6% | 57.0% | 59.0% | 29.7% |\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"vllm\",\"framework\":\"vllm\",\"params\":7285403648,\"storage_bytes\":29142332551,\"files_count\":15,\"spaces_count\":9,\"gated\":false,\"private\":false,\"config\":{\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"</s>\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-inference\",\"source_url\":\"https://github.com/mistralai/mistral-inference\"},{\"type\":\"has_code\",\"target_id\":\"github:state-spaces:mamba\",\"source_url\":\"https://github.com/state-spaces/mamba\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.8,
    "content_hash": "69f040bedb80a9f35a4aa90cabfa196a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mistralai:magistral-small-2506",
    "name": "Magistral-Small-2506",
    "author": "mistralai",
    "description": "--- library_name: vllm base_model: - mistralai/Mistral-Small-3.1-24B-Instruct-2503 language: - en - fr - de - es - pt - it - ja - ko - ru - zh - ar - fa - id - ms - ne - pl - ro - sr - sv - tr - uk - vi - hi - bn license: apache-2.0 inference: false extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>. tags: - mistral-common --- Building upon Mistral Small 3.1 (2503), **with added ...",
    "tags": [
      "vllm",
      "safetensors",
      "mistral",
      "mistral-common",
      "en",
      "fr",
      "de",
      "es",
      "pt",
      "it",
      "ja",
      "ko",
      "ru",
      "zh",
      "ar",
      "fa",
      "id",
      "ms",
      "ne",
      "pl",
      "ro",
      "sr",
      "sv",
      "tr",
      "uk",
      "vi",
      "hi",
      "bn",
      "arxiv:2506.10910",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 605,
    "downloads": 12957,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Magistral-Small-2506",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: vllm\nbase_model:\n- mistralai/Mistral-Small-3.1-24B-Instruct-2503\nlanguage:\n- en\n- fr\n- de\n- es\n- pt\n- it\n- ja\n- ko\n- ru\n- zh\n- ar\n- fa\n- id\n- ms\n- ne\n- pl\n- ro\n- sr\n- sv\n- tr\n- uk\n- vi\n- hi\n- bn\nlicense: apache-2.0\ninference: false\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Magistral Small 1.0\n\nBuilding upon Mistral Small 3.1 (2503), **with added reasoning capabilities**, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.\n\nMagistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.\n\nLearn more about Magistral in our [blog post](https://mistral.ai/news/magistral/).\n\nThe model was presented in the paper [Magistral](https://huggingface.co/papers/2506.10910).\n\n## Key Features\n- **Reasoning:** Capable of long chains of reasoning traces before providing an answer.\n- **Multilingual:** Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 128k context window, **but** performance might degrade past **40k**. Hence we recommend setting the maximum model length to 40k.\n\n## Benchmark Results\n\n| Model | AIME24 pass@1 | AIME25 pass@1 | GPQA Diamond | Livecodebench (v5) |\n|-------|-------------|-------------|--------------|-------------------|\n| Magistral Medium | 73.59% | 64.95% | 70.83% | 59.36% |\n| Magistral Small | 70.68% | 62.76% | 68.18% | 55.84% |\n\n## Sampling parameters\n\nPlease make sure to use: \n- `top_p`: 0.95\n- `temperature`: 0.7\n- `max_tokens`: 40960\n\n## Basic Chat Template\n\nWe highly recommend including the default system prompt used during RL for the best results, you can edit and customise it if needed for your specific use case.\n\n```\n<s>[SYSTEM_PROMPT]system_prompt\n\nA user will ask you to solve a task. You should first draft your thinking process (inner monologue) until you have derived the final answer. Afterwards, write a self-contained summary of your thoughts (i.e. your summary should be succinct but contain all the critical steps you needed to reach the conclusion). You should use Markdown to format your response. Write both your thoughts and summary in the same language as the task posed by the user. NEVER use \\boxed{} in your response.\n\nYour thinking process must follow the template below:\n<think>\nYour thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate a correct answer.\n</think>\n\nHere, provide a concise summary that reflects your reasoning and presents a clear final answer to the user. Don't mention that this is a summary.\n\nProblem:\n\n[/SYSTEM_PROMPT][INST]user_message[/INST]<think>\nreasoning_traces\n</think>\nassistant_response</s>[INST]user_message[/INST]\n```\n*`system_prompt`, `user_message` and `assistant_response` are placeholders.*\n\nWe invite you to choose, depending on your use case and requirements, between keeping reasoning traces during multi-turn interactions or keeping only the final assistant response.\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n\n### Inference\n\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [below](#vllm-recommended)\n\nIn addition the community has prepared quantized versions of the model that can be used with the following frameworks (*alphabetically sorted*):\n- [`llama.cpp`](https://github.com/ggml-org/llama.cpp): https://huggingface.co/mistralai/Magistral-Small-2506_gguf\n- [`lmstudio` (llama.cpp, MLX)](https://lmstudio.ai/): https://lmstudio.ai/models/mistralai/magistral-small\n- [`ollama`](https://ollama.com/): https://ollama.com/library/magistral\n- [`unsloth` (llama.cpp)](https://huggingface.co/unsloth): https://huggingface.co/unsloth/Magistral-Small-2506-GGUF\n\n### Training\n\nFine-tuning is possible with (*alphabetically sorted*):\n- [`axolotl`](https://github.com/axolotl-ai-cloud/axolotl): https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples/magistral\n- [`unsloth`](https://github.com/unslothai/unsloth): https://docs.unsloth.ai/basics/magistral\n\n### Other\n\nAlso you can use Magistral with:\n- [`kaggle`](https://www.kaggle.com/models/mistral-ai/magistral-small-2506): https://www.kaggle.com/models/mistral-ai/magistral-small-2506\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**_Installation_**\n\nMake sure you install the latest [`vLLM`](https://github.com/vllm-project/vllm/) code:\n\n```\npip install -U vllm \\\n    --pre \\\n    --extra-index-url https://wheels.vllm.ai/nightly\n```\n\nDoing so should automatically install [`mistral_common >= 1.6.0`](https://github.com/mistralai/mistral-common/releases/tag/v1.6.0).\n\nTo check:\n```\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n\nServe model as follows:\n\n```\nvllm serve mistralai/Magistral-Small-2506 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\nPing model as follows:\n\n```py\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 40_960\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\nquery = \"Write 4 sentences, each with at least 8 words. Now make absolutely sure that every sentence has exactly one word less than the previous sentence.\"\n# or try out other queries\n# query = \"Exactly how many days ago did the French Revolution start? Today is June 4th, 2025.\"\n# query = \"Think about 5 random numbers. Verify if you can combine them with addition, multiplication, subtraction or division to 133\"\n# query = \"If it takes 30 minutes to dry 12 T-shirts in the sun, how long does it take to dry 33 T-shirts?\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": query}\n]\nstream = client.chat.completions.create(\n  model=model,\n  messages=messages,\n  stream=True,\n  temperature=TEMP,\n  top_p=TOP_P,\n  max_tokens=MAX_TOK,\n)\n\nprint(\"client: Start streaming chat completions...\")\nprinted_content = False\n\nfor chunk in stream:\n  content = None\n  # Check the content is content\n  if hasattr(chunk.choices[0].delta, \"content\"):\n    content = chunk.choices[0].delta.content\n\n  if content is not None:\n    if not printed_content:\n        printed_content = True\n        print(\"\\ncontent:\", end=\"\", flush=True)\n    # Extract and print the content\n    print(content, end=\"\", flush=True)\n\n# content:<think>\n# Alright, I need to write 4 sentences where each one has at least 8 words and each subsequent sentence has one fewer word than the previous one.\n# ...\n# Final boxed answer (the four sentences):\n\n# \\[\n# \\boxed{\n# \\begin{aligned}\n# &\\text{1. The quick brown fox jumps over lazy dog and yells hello.} \\\\\n# &\\text{2. I saw the cat on the stair with my hat.} \\\\\n# &\\text{3. The man in the moon came down quickly today.} \\\\\n# &\\text{4. A cat sat on the mat today patiently.}\n# \\end{aligned}\n# }\n# \\]\n```",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"vllm\",\"framework\":\"vllm\",\"params\":23572403200,\"storage_bytes\":94309094674,\"files_count\":19,\"spaces_count\":24,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:ggml-org:llama.cpp\",\"source_url\":\"https://github.com/ggml-org/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:axolotl-ai-cloud:axolotl\",\"source_url\":\"https://github.com/axolotl-ai-cloud/axolotl\"},{\"type\":\"has_code\",\"target_id\":\"github:axolotl-ai-cloud:axolotl\",\"source_url\":\"https://github.com/axolotl-ai-cloud/axolotl\"},{\"type\":\"has_code\",\"target_id\":\"github:unslothai:unsloth\",\"source_url\":\"https://github.com/unslothai/unsloth\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2506.10910\",\"source_url\":\"https://arxiv.org/abs/2506.10910\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.8,
    "content_hash": "7c0e615cde99792fc7c3056a524b252f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Magistral-Small-2506\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:hunyuanworld-voyager",
    "name": "HunyuanWorld-Voyager",
    "author": "tencent",
    "description": "--- library_name: hunyuanworld-voyager license: other license_name: tencent-hunyuanworld-voyager-community license_link: https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager/blob/main/LICENSE language: - en - zh tags: - hunyuan3d - worldmodel - 3d-aigc - 3d-generation - 3d - scene-generation - image-to-video pipeline_tag: image-to-video extra_gated_eu_disallowed: true --- <div align=\"center\"> <a href=\"\"><img src=\"https://img.shields.io/static/v1?label=Project%20Page&message=Web&color=green...",
    "tags": [
      "hunyuanworld-voyager",
      "safetensors",
      "hunyuan3d",
      "worldmodel",
      "3d-aigc",
      "3d-generation",
      "3d",
      "scene-generation",
      "image-to-video",
      "en",
      "zh",
      "arxiv:2506.04225",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "image-to-video",
    "likes": 601,
    "downloads": 154,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/HunyuanWorld-Voyager",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: hunyuanworld-voyager\nlicense: other\nlicense_name: tencent-hunyuanworld-voyager-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager/blob/main/LICENSE\nlanguage:\n- en\n- zh\ntags:\n- hunyuan3d\n- worldmodel\n- 3d-aigc\n- 3d-generation\n- 3d\n- scene-generation\n- image-to-video\npipeline_tag: image-to-video\nextra_gated_eu_disallowed: true\n---\n\n<div align=\"center\">\n  <a href=\"\"><img src=\"https://img.shields.io/static/v1?label=Project%20Page&message=Web&color=green\"></a> &ensp;\n  <a href=\"https://3d-models.hunyuan.tencent.com/voyager/voyager_en/assets/HYWorld_Voyager.pdf\"><img src=\"https://img.shields.io/static/v1?label=Tech%20Report&message=Arxiv&color=red\"></a> &ensp;\n  <a href=\"https://huggingface.co/tencent/HunyuanWorld-Voyager\"><img src=\"https://img.shields.io/static/v1?label=HunyuanWorld-Voyager&message=HuggingFace&color=yellow\"></a>\n</div>\n\nWe introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also jointly generate aligned depth and RGB video for effective and direct 3D reconstruction.\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/62e7c26236a8e8a827ff0891/ZVq46hyyfscgR8927wsq3.jpeg)\n\n## 🔗 BibTeX\n\nIf you find [Voyager](https://arxiv.org/abs/2506.04225) useful for your research and applications, please cite using this BibTeX:\n\n```BibTeX\n@article{huang2025voyager,\n  title={Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation},\n  author={Huang, Tianyu and Zheng, Wangguandong and Wang, Tengfei and Liu, Yuhao and Wang, Zhenwei and Wu, Junta and Jiang, Jie and Li, Hui and Lau, Rynson WH and Zuo, Wangmeng and Guo, Chunchao},\n  journal={arXiv preprint arXiv:2506.04225},\n  year={2025}\n}\n```\n\n\n\n## Acknowledgements\n\nWe would like to thank [HunyuanWorld](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0), [Hunyuan3D-2](https://github.com/Tencent-Hunyuan/Hunyuan3D-2), and [HunyuanVideo-I2V](https://github.com/Tencent-Hunyuan/HunyuanVideo-I2V). We also thank [VGGT](https://github.com/facebookresearch/vggt), [MoGE](https://github.com/microsoft/MoGe), [Metric3D](https://github.com/YvanYin/Metric3D), for their open research and exploration.",
    "meta_json": "{\"pipeline_tag\":\"image-to-video\",\"library_name\":\"hunyuanworld-voyager\",\"framework\":\"hunyuanworld-voyager\",\"params\":null,\"storage_bytes\":116251654035,\"files_count\":37,\"spaces_count\":0,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanWorld-Voyager\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanWorld-1.0\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan3D-2\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan3D-2\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanVideo-I2V\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanVideo-I2V\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:vggt\",\"source_url\":\"https://github.com/facebookresearch/vggt\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:MoGe\",\"source_url\":\"https://github.com/microsoft/MoGe\"},{\"type\":\"has_code\",\"target_id\":\"github:YvanYin:Metric3D\",\"source_url\":\"https://github.com/YvanYin/Metric3D\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2506.04225\",\"source_url\":\"https://arxiv.org/abs/2506.04225\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 62.8,
    "content_hash": "f84ac71bbe6e756207aee28b3ea227c8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/HunyuanWorld-Voyager\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:sand-ai:magi-1",
    "name": "MAGI-1",
    "author": "sand-ai",
    "description": "--- license: apache-2.0 language: - en pipeline_tag: image-to-video tags: - magi-1 --- !magi-logo ----- <p align=\"center\" style=\"line-height: 1;\"> <a href=\"https://arxiv.org/abs/2505.13211\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"paper\" src=\"https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv\" style=\"display: inline-block; vertical-align: middle;\"> </a> <a href=\"https://sand.ai\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"blog\" src=\"https://img.shields.io/badge/Sand%20AI-Ho...",
    "tags": [
      "diffusers",
      "safetensors",
      "magi-1",
      "image-to-video",
      "en",
      "arxiv:2505.13211",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "image-to-video",
    "likes": 599,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/sand-ai/MAGI-1",
    "image_url": "https://huggingface.co/sand-ai/MAGI-1/resolve/main/figures/dit_architecture.png",
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: image-to-video\ntags: \n- magi-1\n---\n\n![magi-logo](figures/logo_black.png)\n\n\n-----\n\n<p align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://arxiv.org/abs/2505.13211\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"paper\" src=\"https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n  <a href=\"https://sand.ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"blog\" src=\"https://img.shields.io/badge/Sand%20AI-Homepage-333333.svg?logo=data:image/svg%2bxml;base64,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\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n  <a href=\"https://magi.sand.ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"product\" src=\"https://img.shields.io/badge/Magi-Product-logo.svg?logo=data:image/svg%2bxml;base64,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&color=DCBE7E\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n  <a href=\"https://huggingface.co/sand-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Sand AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n  <a href=\"https://x.com/SandAI_HQ\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-Sand%20AI-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n  <a href=\"https://discord.gg/hgaZ86D7Wv\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Sand%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n  <a href=\"https://github.com/SandAI-org/Magi/LICENSE\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"license\" src=\"https://img.shields.io/badge/License-Apache2.0-green?logo=Apache\" style=\"display: inline-block; vertical-align: middle;\">\n  </a>\n</p>\n\n# MAGI-1: Autoregressive Video Generation at Scale\n\nThis repository contains the [code](https://github.com/SandAI-org/MAGI-1) for the MAGI-1 model, pre-trained weights and inference code. You can find more information on our [technical report](https://static.magi.world/static/files/MAGI_1.pdf) or directly create magic with MAGI-1 [here](http://sand.ai) . 🚀✨\n\n\n## 🔥🔥🔥 Latest News\n\n- May 30, 2025: Support for ComfyUI is added 🎉 — the custom nodes for MAGI-1 are now available. Try them out in your workflows!\n- May 26, 2025: MAGI-1 4.5B distill and distill+quant models has been released 🎉 — we’ve updated the model weights - check it out!\n- May 14, 2025: Added Dify DSL for prompt enhancement 🎉 — import it into Dify to boost prompt quality!\n- Apr 30, 2025: MAGI-1 4.5B model has been released 🎉. We've updated the model weights — check it out!\n- Apr 21, 2025: MAGI-1 is here 🎉. We've released the model weights and inference code — check it out!\n\n\n## 1. About\n\nWe present MAGI-1, a world model that generates videos by ***autoregressively*** predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 further supports controllable generation via chunk-wise prompting, enabling smooth scene transitions, long-horizon synthesis, and fine-grained text-driven control. We believe MAGI-1 offers a promising direction for unifying high-fidelity video generation with flexible instruction control and real-time deployment.\n\n\n## 2. Model Summary\n\n### Transformer-based VAE\n\n- Variational autoencoder (VAE) with transformer-based architecture, 8x spatial and 4x temporal compression.\n- Fastest average decoding time and highly competitive reconstruction quality\n\n### Auto-Regressive Denoising Algorithm\n\nMAGI-1 is an autoregressive denoising video generation model generating videos chunk-by-chunk instead of as a whole. Each chunk (24 frames) is denoised holistically, and the generation of the next chunk begins as soon as the current one reaches a certain level of denoising. This pipeline design enables concurrent processing of up to four chunks for efficient video generation.\n\n![auto-regressive denosing algorithm](figures/algorithm.png)\n\n### Diffusion Model Architecture\n\nMAGI-1 is built upon the Diffusion Transformer, incorporating several key innovations to enhance training efficiency and stability at scale. These advancements include Block-Causal Attention, Parallel Attention Block, QK-Norm and GQA, Sandwich Normalization in FFN, SwiGLU, and Softcap Modulation. For more details, please refer to the [technical report.](https://static.magi.world/static/files/MAGI_1.pdf)\n<div align=\"center\">\n<img src=\"figures/dit_architecture.png\" alt=\"diffusion model architecture\" width=\"500\" />\n</div>\n\n### Distillation Algorithm\n\nWe adopt a shortcut distillation approach that trains a single velocity-based model to support variable inference budgets. By enforcing a self-consistency constraint—equating one large step with two smaller steps—the model learns to approximate flow-matching trajectories across multiple step sizes. During training, step sizes are cyclically sampled from {64, 32, 16, 8}, and classifier-free guidance distillation is incorporated to preserve conditional alignment. This enables efficient inference with minimal loss in fidelity.\n\n\n## 3. Model Zoo\n\nWe provide the pre-trained weights for MAGI-1, including the 24B and 4.5B models, as well as the corresponding distill and distill+quant models. The model weight links are shown in the table.\n\n| Model                         | Link                                                                 | Recommend Machine             |\n| ------------------------------ | -------------------------------------------------------------------- | ------------------------------- |\n| T5                             | [T5](https://huggingface.co/sand-ai/MAGI-1/tree/main/ckpt/t5)        | -                               |\n| MAGI-1-VAE                     | [MAGI-1-VAE](https://huggingface.co/sand-ai/MAGI-1/tree/main/ckpt/vae) | -                               |\n| MAGI-1-24B                     | [MAGI-1-24B](https://huggingface.co/sand-ai/MAGI-1/tree/main/ckpt/magi/24B_base) | H100/H800 × 8                   |\n| MAGI-1-24B-distill              | [MAGI-1-24B-distill](https://huggingface.co/sand-ai/MAGI-1/tree/main/ckpt/magi/24B_distill) | H100/H800 × 8                   |\n| MAGI-1-24B-distill+fp8_quant    | [MAGI-1-24B-distill+quant](https://huggingface.co/sand-ai/MAGI-1/tree/main/ckpt/magi/24B_distill_quant) | H100/H800 × 4 or RTX 4090 × 8    |\n| MAGI-1-4.5B                    | [MAGI-1-4.5B](https://huggingface.co/sand-ai/MAGI-1/tree/main/ckpt/magi/4.5B_base) | RTX 4090 × 1                    |\n| MAGI-1-4.5B-distill             | Coming soon                                                         | RTX 4090 × 1                    |\n| MAGI-1-4.5B-distill+fp8_quant   | Coming soon                                                         | RTX 4090 × 1                    |\n\n> [!NOTE]\n>\n> For 4.5B models, any machine with at least 24GB of GPU memory is sufficient.\n\n## 4. Evaluation\n\n### In-house Human Evaluation\n\nMAGI-1 achieves state-of-the-art performance among open-source models like Wan-2.1 and HunyuanVideo and closed-source model like Hailuo (i2v-01), particularly excelling in instruction following and motion quality, positioning it as a strong potential competitor to closed-source commercial models such as Kling.\n\n![inhouse human evaluation](figures/inhouse_human_evaluation.png)\n\n### Physical Evaluation\n\nThanks to the natural advantages of autoregressive architecture, Magi achieves far superior precision in predicting physical behavior on the [Physics-IQ benchmark](https://github.com/google-deepmind/physics-IQ-benchmark) through video continuation—significantly outperforming all existing models.\n\n| Model          | Phys. IQ Score ↑ | Spatial IoU ↑ | Spatio Temporal ↑ | Weighted Spatial IoU ↑ | MSE ↓  |\n|----------------|------------------|---------------|-------------------|-------------------------|--------|\n| **V2V Models** |                  |               |                   |                         |        |\n| **Magi-24B (V2V)** | **56.02**        | **0.367**     | **0.270**         | **0.304**               | **0.005** |\n| **Magi-4.5B (V2V)** | **42.44**        | **0.234**     | **0.285**         | **0.188**               | **0.007** |\n| VideoPoet (V2V)| 29.50            | 0.204         | 0.164             | 0.137                   | 0.010  |\n| **I2V Models** |                  |               |                   |                         |        |\n| **Magi-24B (I2V)** | **30.23**        | **0.203**     | **0.151**         | **0.154**               | **0.012** |\n| Kling1.6 (I2V) | 23.64            | 0.197         | 0.086             | 0.144                   | 0.025  |\n| VideoPoet (I2V)| 20.30            | 0.141         | 0.126             | 0.087                   | 0.012  |\n| Gen 3 (I2V)    | 22.80            | 0.201         | 0.115             | 0.116                   | 0.015  |\n| Wan2.1 (I2V)   | 20.89            | 0.153         | 0.100             | 0.112                   | 0.023  |\n| Sora (I2V)     | 10.00            | 0.138         | 0.047             | 0.063                   | 0.030  |\n| **GroundTruth**| **100.0**        | **0.678**     | **0.535**         | **0.577**               | **0.002** |\n\n\n## 5. How to run\n\n### Environment Preparation\n\nWe provide two ways to run MAGI-1, with the Docker environment being the recommended option.\n\n**Run with Docker Environment (Recommend)**\n\n```bash\ndocker pull sandai/magi:latest\n\ndocker run -it --gpus all --privileged --shm-size=32g --name magi --net=host --ipc=host --ulimit memlock=-1 --ulimit stack=6710886 sandai/magi:latest /bin/bash\n```\n\n**Run with Source Code**\n\n```bash\n# Create a new environment\nconda create -n magi python==3.10.12\n\n# Install pytorch\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia\n\n# Install other dependencies\npip install -r requirements.txt\n\n# Install ffmpeg\nconda install -c conda-forge ffmpeg=4.4\n\n# For GPUs based on the Hopper architecture (e.g., H100/H800), it is recommended to install MagiAttention(https://github.com/SandAI-org/MagiAttention) for acceleration. For non-Hopper GPUs, installing MagiAttention is not necessary.\ngit clone git@github.com:SandAI-org/MagiAttention.git\ncd MagiAttention\ngit submodule update --init --recursive\npip install --no-build-isolation .\n```\n\n### Inference Command\n\nTo run the `MagiPipeline`, you can control the input and output by modifying the parameters in the `example/24B/run.sh` or `example/4.5B/run.sh` script. Below is an explanation of the key parameters:\n\n#### Parameter Descriptions\n\n- `--config_file`: Specifies the path to the configuration file, which contains model configuration parameters, e.g., `example/24B/24B_config.json`.\n- `--mode`: Specifies the mode of operation. Available options are:\n  - `t2v`: Text to Video\n  - `i2v`: Image to Video\n  - `v2v`: Video to Video\n- `--prompt`: The text prompt used for video generation, e.g., `\"Good Boy\"`.\n- `--image_path`: Path to the image file, used only in `i2v` mode.\n- `--prefix_video_path`: Path to the prefix video file, used only in `v2v` mode.\n- `--output_path`: Path where the generated video file will be saved.\n\n#### Bash Script\n\n```bash\n#!/bin/bash\n# Run 24B MAGI-1 model\nbash example/24B/run.sh\n\n# Run 4.5B MAGI-1 model\nbash example/4.5B/run.sh\n```\n\n#### Customizing Parameters\n\nYou can modify the parameters in `run.sh` as needed. For example:\n\n- To use the Image to Video mode (`i2v`), set `--mode` to `i2v` and provide `--image_path`:\n  ```bash\n  --mode i2v \\\n  --image_path example/assets/image.jpeg \\\n  ```\n\n- To use the Video to Video mode (`v2v`), set `--mode` to `v2v` and provide `--prefix_video_path`:\n  ```bash\n  --mode v2v \\\n  --prefix_video_path example/assets/prefix_video.mp4 \\\n  ```\n\nBy adjusting these parameters, you can flexibly control the input and output to meet different requirements.\n\n### Some Useful Configs (for config.json)\n\n> [!NOTE]\n>\n> - If you are running 24B model with RTX 4090 \\* 8, please set `pp_size:2 cp_size: 4`.\n>\n> - Our model supports arbitrary resolutions. To accelerate inference process, the default resolution for the 4.5B model is set to 720×720 in the `4.5B_config.json`.\n\n| Config         | Help                                                         |\n| -------------- | ------------------------------------------------------------ |\n| seed           | Random seed used for video generation                        |\n| video_size_h   | Height of the video                                          |\n| video_size_w   | Width of the video                                           |\n| num_frames     | Controls the duration of generated video                     |\n| fps            | Frames per second, 4 video frames correspond to 1 latent_frame |\n| cfg_number     | Base model uses cfg_number==3, distill and quant model uses cfg_number=1 |\n| load           | Directory containing a model checkpoint.                     |\n| t5_pretrained  | Path to load pretrained T5 model                             |\n| vae_pretrained | Path to load pretrained VAE model                            |\n\n\n## 6. License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\n## 7. Citation\n\nIf you find our code or model useful in your research, please cite:\n\n```bibtex\n@misc{ai2025magi1autoregressivevideogeneration,\n      title={MAGI-1: Autoregressive Video Generation at Scale},\n      author={Sand. ai and Hansi Teng and Hongyu Jia and Lei Sun and Lingzhi Li and Maolin Li and Mingqiu Tang and Shuai Han and Tianning Zhang and W. Q. Zhang and Weifeng Luo and Xiaoyang Kang and Yuchen Sun and Yue Cao and Yunpeng Huang and Yutong Lin and Yuxin Fang and Zewei Tao and Zheng Zhang and Zhongshu Wang and Zixun Liu and Dai Shi and Guoli Su and Hanwen Sun and Hong Pan and Jie Wang and Jiexin Sheng and Min Cui and Min Hu and Ming Yan and Shucheng Yin and Siran Zhang and Tingting Liu and Xianping Yin and Xiaoyu Yang and Xin Song and Xuan Hu and Yankai Zhang and Yuqiao Li},\n      year={2025},\n      eprint={2505.13211},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2505.13211},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please feel free to raise an issue or contact us at [research@sand.ai](mailto:research@sand.ai) .",
    "meta_json": "{\"pipeline_tag\":\"image-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":180791441820,\"files_count\":41,\"spaces_count\":1,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:SandAI-org:Magi\",\"source_url\":\"https://github.com/SandAI-org/Magi\"},{\"type\":\"has_code\",\"target_id\":\"github:SandAI-org:MAGI-1\",\"source_url\":\"https://github.com/SandAI-org/MAGI-1\"},{\"type\":\"has_code\",\"target_id\":\"github:google-deepmind:physics-IQ-benchmark\",\"source_url\":\"https://github.com/google-deepmind/physics-IQ-benchmark\"},{\"type\":\"has_code\",\"target_id\":\"github:SandAI-org:MagiAttention\",\"source_url\":\"https://github.com/SandAI-org/MagiAttention\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.13211\",\"source_url\":\"https://arxiv.org/abs/2505.13211\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 97.8,
    "content_hash": "d991a6dbbcebe23282a69b1b0d2def46",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/sand-ai/MAGI-1/resolve/main/figures/dit_architecture.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/sand-ai/MAGI-1\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:distilbert:distilgpt2",
    "name": "distilgpt2",
    "author": "distilbert",
    "description": "--- language: en tags: - exbert license: apache-2.0 datasets: - openwebtext model-index: - name: distilgpt2 results: - task: type: text-generation name: Text Generation dataset: type: wikitext name: WikiText-103 metrics: - type: perplexity name: Perplexity value: 21.1 co2_eq_emissions: 149200 --- DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "tflite",
      "rust",
      "coreml",
      "safetensors",
      "gpt2",
      "text-generation",
      "exbert",
      "en",
      "dataset:openwebtext",
      "arxiv:1910.01108",
      "arxiv:2201.08542",
      "arxiv:2203.12574",
      "arxiv:1910.09700",
      "arxiv:1503.02531",
      "license:apache-2.0",
      "model-index",
      "co2_eq_emissions",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 598,
    "downloads": 2157936,
    "source": "huggingface",
    "source_url": "https://huggingface.co/distilbert/distilgpt2",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: en\ntags:\n- exbert\n\nlicense: apache-2.0\ndatasets:\n- openwebtext\n\nmodel-index:\n- name: distilgpt2\n  results:\n  - task: \n      type: text-generation\n      name: Text Generation\n    dataset:\n      type: wikitext\n      name: WikiText-103\n    metrics:\n       - type: perplexity\n         name: Perplexity\n         value: 21.1\n         \nco2_eq_emissions: 149200\n---\n\n# DistilGPT2\n\nDistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2).\n\n## Model Details\n\n- **Developed by:** Hugging Face\n- **Model type:** Transformer-based Language Model\n- **Language:** English\n- **License:** Apache 2.0\n- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.\n- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).\n\n## Uses, Limitations and Risks\n\n#### Limitations and Risks\n\n<details>\n<summary>Click to expand</summary>\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nAs the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n\nDistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.\n\nThe impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example: \n\n- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.\n- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias). \n- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2. \n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(48)\n>>> generator(\"The White man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': \"The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the\"},\n {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a \"'},\n {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]\n \n>>> set_seed(48)\n>>> generator(\"The Black man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},\n {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},\n {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]\n```\n\n</details>\n\n#### Potential Uses\n\nSince DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. \n\nThe developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: \n\n> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*\n> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*\n> - *Entertainment: Creation of games, chat bots, and amusing generations.*\n\nUsing DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.\n\n#### Out-of-scope Uses\n\nOpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): \n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.\n\n### How to Get Started with the Model \n\n<details>\n<summary>Click to expand</summary>\n\n*Be sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.*\n\nUsing DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I’m a language model\", max_length=20, num_return_sequences=5)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n[{'generated_text': \"Hello, I'm a language model, I'm a language model. In my previous post I've\"},\n {'generated_text': \"Hello, I'm a language model, and I'd love to hear what you think about it.\"},\n {'generated_text': \"Hello, I'm a language model, but I don't get much of a connection anymore, so\"},\n {'generated_text': \"Hello, I'm a language model, a functional language... It's not an example, and that\"},\n {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I\"}]\n``` \n \nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = GPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nAnd in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = TFGPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n</details>\n\n## Training Data\n\nDistilGPT2 was trained using [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the [OpenWebTextCorpus Dataset Card](https://huggingface.co/datasets/openwebtext) for additional information about OpenWebTextCorpus and [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) for additional information about WebText.\n\n## Training Procedure\n\nThe texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108). \n\n## Evaluation Results\n\nThe creators of DistilGPT2 [report](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) that, on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).\n\n## Environmental Impact\n\n*Carbon emissions were estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.*\n\n- **Hardware Type:** 8 16GB V100\n- **Hours used:** 168 (1 week)\n- **Cloud Provider:** Azure\n- **Compute Region:** unavailable, assumed East US for calculations\n- **Carbon Emitted** *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2\n\n## Citation\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\n## Glossary\n\n-\t<a name=\"knowledge-distillation\">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).\n\n<a href=\"https://huggingface.co/exbert/?model=distilgpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":88204032,\"storage_bytes\":8079459096,\"files_count\":19,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"GPT2LMHeadModel\"],\"model_type\":\"gpt2\",\"tokenizer_config\":{}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:gpt-2\",\"source_url\":\"https://github.com/openai/gpt-2\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:gpt-2\",\"source_url\":\"https://github.com/openai/gpt-2\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:gpt-2\",\"source_url\":\"https://github.com/openai/gpt-2\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.01108\",\"source_url\":\"https://arxiv.org/abs/1910.01108\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2201.08542\",\"source_url\":\"https://arxiv.org/abs/2201.08542\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.12574\",\"source_url\":\"https://arxiv.org/abs/2203.12574\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.09700\",\"source_url\":\"https://arxiv.org/abs/1910.09700\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1503.02531\",\"source_url\":\"https://arxiv.org/abs/1503.02531\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 77.8,
    "content_hash": "caa3fb1e33237aa35347eeaceef0e08c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/distilbert/distilgpt2\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:madebyollin:sdxl-vae-fp16-fix",
    "name": "sdxl-vae-fp16-fix",
    "author": "madebyollin",
    "description": "--- license: mit tags: - stable-diffusion - stable-diffusion-diffusers inference: false --- SDXL-VAE-FP16-Fix is the SDXL VAE*, but modified to run in fp16 precision without generating NaNs. | VAE | Decoding in / precision | Decoding in precision | | --------------------- | -------------------------------------------- | ------------------------------- | | SDXL-VAE | ✅ | ⚠️ | | SDXL-VAE-FP16-Fix | ✅ | ✅ | Just load this checkpoint via : 1. Download the fixed sdxl.vae.safetensors file 2. Move t...",
    "tags": [
      "diffusers",
      "safetensors",
      "stable-diffusion",
      "stable-diffusion-diffusers",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 597,
    "downloads": 306035,
    "source": "huggingface",
    "source_url": "https://huggingface.co/madebyollin/sdxl-vae-fp16-fix",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\ninference: false\n---\n# SDXL-VAE-FP16-Fix\n\nSDXL-VAE-FP16-Fix is the [SDXL VAE](https://huggingface.co/stabilityai/sdxl-vae)*, but modified to run in fp16 precision without generating NaNs.\n\n| VAE                   | Decoding in `float32` / `bfloat16` precision | Decoding in `float16` precision |\n| --------------------- | -------------------------------------------- | ------------------------------- |\n| SDXL-VAE              | ✅ ![](./images/orig-fp32.png)              | ⚠️ ![](./images/orig-fp16.png)  |\n| SDXL-VAE-FP16-Fix     | ✅ ![](./images/fix-fp32.png)               | ✅ ![](./images/fix-fp16.png)   |\n\n## 🧨 Diffusers Usage\n\nJust load this checkpoint via `AutoencoderKL`:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", vae=vae, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\npipe.to(\"cuda\")\n\nrefiner = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\", vae=vae, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\nrefiner.to(\"cuda\")\n\nn_steps = 40\nhigh_noise_frac = 0.7\n\nprompt = \"A majestic lion jumping from a big stone at night\"\n\nimage = pipe(prompt=prompt, num_inference_steps=n_steps, denoising_end=high_noise_frac, output_type=\"latent\").images\nimage = refiner(prompt=prompt, num_inference_steps=n_steps, denoising_start=high_noise_frac, image=image).images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lion_refined.png)\n\n## Automatic1111 Usage\n\n1. Download the fixed [sdxl.vae.safetensors](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/sdxl.vae.safetensors?download=true) file\n2. Move this `sdxl.vae.safetensors` file into the webui folder under `stable-diffusion-webui/models/VAE`\n3. In your webui settings, select the fixed VAE you just added\n4. If you were using the `--no-half-vae` command line arg for SDXL (in `webui-user.bat` or wherever), you can now remove it\n\n(Disclaimer - I haven't tested this, just aggregating various instructions I've seen elsewhere :P PRs to improve these instructions are welcomed!)\n\n## Details\n\nSDXL-VAE generates NaNs in fp16 because the internal activation values are too big:\n![](./images/activation-magnitudes.jpg)\n\nSDXL-VAE-FP16-Fix was created by finetuning the SDXL-VAE to:\n1. keep the final output the same, but\n2. make the internal activation values smaller, by\n3. scaling down weights and biases within the network\n\nThere are slight discrepancies between the output of SDXL-VAE-FP16-Fix and SDXL-VAE, but the decoded images should be [close enough for most purposes](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/discussions/7#64c5c0f8e2e5c94bd04eaa80).\n\n---\n\n\\* `sdxl-vae-fp16-fix` is specifically based on [SDXL-VAE (0.9)](https://huggingface.co/stabilityai/sdxl-vae/discussions/6#64acea3f7ac35b7de0554490), but it works with SDXL 1.0 too",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":3016720617,\"files_count\":12,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 62.8,
    "content_hash": "4b185f8f9d35c49dce9cd51ad7ffa4cd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/madebyollin/sdxl-vae-fp16-fix\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:baai:bge-large-zh-v1.5",
    "name": "bge-large-zh-v1.5",
    "author": "BAAI",
    "description": "--- license: mit language: - zh tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers --- <h1 align=\"center\">FlagEmbedding</h1> <h4 align=\"center\"> <p> <a href=#model-list>Model List</a> | <a href=#frequently-asked-questions>FAQ</a> | <a href=#usage>Usage</a> | <a href=\"#evaluation\">Evaluation</a> | <a href=\"#train\">Train</a> | <a href=\"#contact\">Contact</a> | <a href=\"#citation\">Citation</a> | <a href=\"#license\">License</a> <p> </h4> For more details please ...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "bert",
      "feature-extraction",
      "sentence-similarity",
      "transformers",
      "zh",
      "arxiv:2401.03462",
      "arxiv:2312.15503",
      "arxiv:2311.13534",
      "arxiv:2310.07554",
      "arxiv:2309.07597",
      "license:mit",
      "text-embeddings-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "feature-extraction",
    "likes": 597,
    "downloads": 681261,
    "source": "huggingface",
    "source_url": "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlanguage:\n- zh\ntags:\n  - sentence-transformers\n  - feature-extraction\n  - sentence-similarity\n  - transformers\n---\n\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#contact\">Contact</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\nFor more details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).\n\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using [bge-m3](https://huggingface.co/BAAI/bge-m3).\n\n\n[English](README.md) | [中文](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) :fire:\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) and [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size 🤗**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `为这个句子生成表示以用于检索相关文章：`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `为这个句子生成表示以用于检索相关文章：`  |\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"样例数据-1\", \"样例数据-2\"]\nsentences_2 = [\"样例数据-3\", \"样例数据-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"样例文档-1\", \"样例文档-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"样例数据-1\", \"样例数据-2\"]\nsentences_2 = [\"样例数据-3\", \"样例数据-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"样例文档-1\", \"样例文档-2\"]\ninstruction = \"为这个句子生成表示以用于检索相关文章：\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n)\nmodel.query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"样例数据-1\", \"样例数据-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n## Contact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn). \n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.",
    "meta_json": "{\"pipeline_tag\":\"feature-extraction\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":null,\"storage_bytes\":4879617945,\"files_count\":12,\"spaces_count\":78,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BertModel\"],\"model_type\":\"bert\",\"tokenizer_config\":{\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\",\"pad_token\":\"[PAD]\",\"sep_token\":\"[SEP]\",\"unk_token\":\"[UNK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:staoxiao:RetroMAE\",\"source_url\":\"https://github.com/staoxiao/RetroMAE\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"has_code\",\"target_id\":\"github:FlagOpen:FlagEmbedding\",\"source_url\":\"https://github.com/FlagOpen/FlagEmbedding\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2401.03462\",\"source_url\":\"https://arxiv.org/abs/2401.03462\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.15503\",\"source_url\":\"https://arxiv.org/abs/2312.15503\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.13534\",\"source_url\":\"https://arxiv.org/abs/2311.13534\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.07554\",\"source_url\":\"https://arxiv.org/abs/2310.07554\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.07597\",\"source_url\":\"https://arxiv.org/abs/2309.07597\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 77.8,
    "content_hash": "703e583164dde8920ba396a8dc342842",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/BAAI/bge-large-zh-v1.5\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:hunyuan-1.8b-instruct",
    "name": "Hunyuan-1.8B-Instruct",
    "author": "tencent",
    "description": "--- library_name: transformers --- <p align=\"center\"> <img src=\"https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png\" width=\"400\"/> <br> </p><p></p> <p align=\"center\"> 🤗&nbsp;<a href=\"https://huggingface.co/tencent/\"><b>HuggingFace</b></a>&nbsp;|&nbsp; 🤖&nbsp;<a href=\"https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-1.8B-Instruct\"><b>ModelScope</b></a>&nbsp;|&nbsp; 🪡&nbsp;<a href=\"https://github.com/Tencent/AngelSlim/tree/main\"><b>AngelS...",
    "tags": [
      "transformers",
      "safetensors",
      "hunyuan_v1_dense",
      "text-generation",
      "conversational",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 597,
    "downloads": 804,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/Hunyuan-1.8B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\n---\n\n\n\n<p align=\"center\">\n <img src=\"https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png\" width=\"400\"/> <br>\n</p><p></p>\n\n\n<p align=\"center\">\n    🤗&nbsp;<a href=\"https://huggingface.co/tencent/\"><b>HuggingFace</b></a>&nbsp;|&nbsp;\n    🤖&nbsp;<a href=\"https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-1.8B-Instruct\"><b>ModelScope</b></a>&nbsp;|&nbsp;\n    🪡&nbsp;<a href=\"https://github.com/Tencent/AngelSlim/tree/main\"><b>AngelSlim</b></a>\n</p>\n\n<p align=\"center\">\n    🖥️&nbsp;<a href=\"https://hunyuan.tencent.com\" style=\"color: red;\"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    🕖&nbsp;<a href=\"https://cloud.tencent.com/product/hunyuan\"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    🕹️&nbsp;<a href=\"https://hunyuan.tencent.com/\"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;&nbsp;\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-1.8B\"><b>GITHUB</b></a> | \n    <a href=\"https://cnb.cool/tencent/hunyuan/Hunyuan-1.8B\"><b>cnb.cool</b></a> | \n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-1.8B/blob/main/LICENSE\"><b>LICENSE</b></a> | \n    <a href=\"https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/assets/1751881231452.jpg\"><b>WeChat</b></a> | \n    <a href=\"https://discord.gg/bsPcMEtV7v\"><b>Discord</b></a>\n</p>\n\n\n## Model Introduction\n\nHunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.\n\nWe have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.\n\n### Key Features and Advantages\n\n- **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n- **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n- **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.\n- **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\n## Related News\n* 2025.7.30 We have open-sourced  **Hunyuan-0.5B-Pretrain** ,  **Hunyuan-0.5B-Instruct** , **Hunyuan-1.8B-Pretrain** ,  **Hunyuan-1.8B-Instruct** , **Hunyuan-4B-Pretrain** ,  **Hunyuan-4B-Instruct** , **Hunyuan-7B-Pretrain** ,**Hunyuan-7B-Instruct** on Hugging Face.\n<br>\n\n\n## Benchmark\n\nNote: The following benchmarks are evaluated by TRT-LLM-backend on several **base models**. \n\n| Model            | Hunyuan-0.5B-Pretrain | Hunyuan-1.8B-Pretrain | Hunyuan-4B-Pretrain | Hunyuan-7B-Pretrain|\n|:------------------:|:---------------:|:--------------:|:-------------:|:---------------:|\n| MMLU             | 54.02          | 64.62         | 74.01        | 79.82         |\n| MMLU-Redux              |  54.72         | 64.42        | 73.53       | 79         |\n| MMLU-Pro        | 31.15             | 38.65            | 51.91        | 57.79          |\n| SuperGPQA    |  17.23         | 24.98          | 27.28           | 30.47          |\n| BBH       | 45.92          | 74.32         | 75.17        | 82.95          |\n| GPQA             | 27.76             | 35.81            | 43.52        | 44.07          |\n| GSM8K | 55.64             | 77.26            | 87.49       | 88.25         |\n| MATH             | 42.95          | 62.85          | 72.25        | 74.85          |\n| EvalPlus             | 39.71          | 60.67          | 67.76        | 66.96          |\n| MultiPL-E            | 21.83          | 45.92         | 59.87        | 60.41          |\n| MBPP            | 43.38          | 66.14         | 76.46        | 76.19          |\n| CRUX-O         | 30.75             | 36.88           | 56.5        | 60.75          |\n| Chinese SimpleQA            | 12.51             | 22.31            | 30.53        | 38.86          |\n| simpleQA (5shot)            | 2.38             | 3.61            | 4.21        | 5.69          |\n\n\n| Topic               |                        Bench                         | Hunyuan-0.5B-Instruct | Hunyuan-1.8B-Instruct | Hunyuan-4B-Instruct | Hunyuan-7B-Instruct|\n|:-------------------:|:----------------------------------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|\n| **Mathematics**     |            AIME 2024<br>AIME 2025<br>MATH            | 17.2<br>20<br>48.5 | 56.7<br>53.9<br>86 | 78.3<br>66.5<br>92.6 | 81.1<br>75.3<br>93.7 |\n| **Science**         |            GPQA-Diamond<br>OlympiadBench             | 23.3<br>29.6 | 47.2<br>63.4 | 61.1<br>73.1 | 60.1<br>76.5 |\n| **Coding**          |           Livecodebench<br>Fullstackbench            | 11.1<br>20.9 | 31.5<br>42   | 49.4<br>54.6 | 57<br>56.3 |\n| **Reasoning**       |              BBH<br>DROP<br>ZebraLogic               | 40.3<br>52.8<br>34.5 | 64.6<br>76.7<br>74.6 | 83<br>78.2<br>83.5 | 87.8<br>85.9<br>85.1 |\n| **Instruction<br>Following** |        IF-Eval<br>SysBench                  | 49.7<br>28.1 | 67.6<br>55.5 | 76.6<br>68 | 79.3<br>72.7 |\n| **Agent**           | BFCL v3<br> τ-Bench<br>ComplexFuncBench<br> C3-Bench | 49.8<br>14.4<br>13.9<br>45.3 | 58.3<br>18.2<br>22.3<br>54.6 | 67.9<br>30.1<br>26.3<br>64.3 | 70.8<br>35.3<br>29.2<br>68.5 |\n| **Long<br>Context** | PenguinScrolls<br>longbench-v2<br>FRAMES          | 53.9<br>34.7<br>41.9 | 73.1<br>33.2<br>55.6 | 83.1<br>44.1<br>79.2 | 82<br>43<br>78.6 |\n\n\n&nbsp;\n\n### Use with transformers\nFirst, please install transformers. We will merge it into the main branch later.\n```SHELL\npip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca\n```\nOur model defaults to using slow-thinking reasoning, and there are two ways to disable CoT reasoning. \n1. Pass **\"enable_thinking=False\"** when calling apply_chat_template.\n2. Adding **\"/no_think\"** before the prompt will force the model not to use perform CoT reasoning. Similarly, adding **\"/think\"** before the prompt will force the model to perform CoT reasoning.\n\nThe following code snippet shows how to use the transformers library to load and apply the model. It also demonstrates how to enable and disable the reasoning mode , and how to parse the reasoning process along with the final output.\n\nwe use tencent/Hunyuan-7B-Instruct for example\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport re\n\nmodel_name_or_path = \"tencent/Hunyuan-7B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write a short summary of the benefits of regular exercise\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True,return_tensors=\"pt\",\n                                                enable_thinking=True # Toggle thinking mode (default: True)\n                                                )\n                                                \noutputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=2048)\n\noutput_text = tokenizer.decode(outputs[0])\nprint(\"output_text=\",output_text)\nthink_pattern = r'<think>(.*?)</think>'\nthink_matches = re.findall(think_pattern, output_text, re.DOTALL)\n\nanswer_pattern = r'<answer>(.*?)</answer>'\nanswer_matches = re.findall(answer_pattern, output_text, re.DOTALL)\n\nthink_content = [match.strip() for match in think_matches][0]\nanswer_content = [match.strip() for match in answer_matches][0]\nprint(f\"thinking_content:{think_content}\\n\\n\")\nprint(f\"answer_content:{answer_content}\\n\\n\")\n\n\n```\n\nWe recommend using the following set of parameters for inference. Note that our model does not have the default system_prompt.\n\n```json\n\n{\n  \"do_sample\": true,\n  \"top_k\": 20,\n  \"top_p\": 0.8,\n  \"repetition_penalty\": 1.05,\n  \"temperature\": 0.7\n}\n```\n\n&nbsp;\n\n### Training Data Format\n\nIf you need to fine-tune our Instruct model, we recommend processing the data into the following format, corresponding to both slow-thinking and fast-thinking scenarios.\n\n```python\n# think_pattern\nthink = \"\"\nanswer = \"\"\nthink_pattern = f\"<think>\\n{think}\\n</think>\\n<answer>\\n{answer}\\n</answer>\"\n\n# fast think pattern\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"/no_think海水为什么是咸的\" },\n    {\"role\": \"assistant\", \"content\": \"<think>\\n\\n</think>\\n<answer>\\n海水是咸的主要是因为其中含有许多溶解在水中的盐类和矿物质。这些盐类和矿物质来自于地球表面的岩石和土壤中的化学物质，随着时间的推移，它们被带到了海洋中。当海水蒸发时，水分蒸发掉了，但盐类和矿物质仍然留在水中，导致海水变得更加咸味。因此，海水的咸度是由其中的盐类和矿物质的含量决定的。\\n</answer>\"}\n]\n\n# slow think pattern\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"1+1=\" },\n    {\"role\": \"assistant\", \"content\": \"<think>\\n嗯，用户问的是1加1等于多少。首先，我需要确认这是一个基本的算术问题。1加1在十进制的数学体系中，通常的结果是2。不过，可能需要考虑是否有其他情况，比如二进制或者其他数制，但用户没有特别说明，所以默认应该是十进制。另外，有时候可能会有脑筋急转弯的情况，比如在某些语境下1+1可能等于1（比如1滴水加1滴水还是1滴水），但通常数学问题中都是2。所以最准确的回答应该是2。</think>\\n<answer>\\n在十进制的基本算术运算中，1加1的结果是2。这是数学中最基础的加法运算之一，遵循自然数的加法规则。因此，1 + 1 = 2。\\n</answer>\"}\n]\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"your_tokenizer_path\", trust_remote_code=True)\ntrain_ids = tokenizer.apply_chat_template(messages)\n```\n\n&nbsp;\n\n### Train with LLaMA-Factory\n\nIn the following chapter, we will introduce how to use `LLaMA-Factory` to fine-tune the `Hunyuan` model.\n\n#### Prerequisites\n\nVerify installation of the following dependencies:  \n- **LLaMA-Factory**: Follow [official installation guide](https://github.com/hiyouga/LLaMA-Factory)\n- **DeepSpeed** (optional): Follow [official installation guide](https://github.com/deepspeedai/DeepSpeed#installation)\n- **Transformer Library**: Use the companion branch (Hunyuan-submitted code is pending review)\n    ```\n    pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca\n    ```\n\n#### Data preparation\n\nWe need to prepare a custom dataset:\n1. Organize your data in `json` format and place it in the `data` directory in `LLaMA-Factory`. The current implementation uses the `sharegpt` dataset format, which requires the following structure:\n```\n[\n  {\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"System prompt (optional)\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Human instruction\"\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"Model response\"\n      }\n    ]\n  }\n]\n```\nRefer to the [Data Format](#training-data-format) section mentioned earlier for details.\n\n2. Define your dataset in the data/dataset_info.json file using the following format:\n```\n\"dataset_name\": {\n  \"file_name\": \"dataset.json\",\n  \"formatting\": \"sharegpt\",\n  \"columns\": {\n    \"messages\": \"messages\"\n  },\n  \"tags\": {\n    \"role_tag\": \"role\",\n    \"content_tag\": \"content\",\n    \"user_tag\": \"user\",\n    \"assistant_tag\": \"assistant\",\n    \"system_tag\": \"system\"\n  }\n}\n```\n\n#### Training execution\n\n1. Copy all files from the `train/llama_factory_support/example_configs` directory to the `example/hunyuan` directory in `LLaMA-Factory`.\n2. Modify the model path and dataset name in the configuration file `hunyuan_full.yaml`. Adjust other configurations as needed:\n```\n### model\nmodel_name_or_path: [!!!add the model path here!!!]\n\n### dataset\ndataset: [!!!add the dataset name here!!!]\n```\n3. Execute training commands:\n    *​​Single-node training​​\n    Note: Set the environment variable DISABLE_VERSION_CHECK to 1 to avoid version conflicts.\n    ```\n    export DISABLE_VERSION_CHECK=1\n    llamafactory-cli train examples/hunyuan/hunyuan_full.yaml\n    ```\n    *Multi-node training​​\n    Execute the following command on each node. Configure NNODES, NODE_RANK, MASTER_ADDR, and MASTER_PORT according to your environment:\n    ```\n    export DISABLE_VERSION_CHECK=1\n    FORCE_TORCHRUN=1 NNODES=${NNODES} NODE_RANK=${NODE_RANK} MASTER_ADDR=${MASTER_ADDR} MASTER_PORT=${MASTER_PORT} \\\n    llamafactory-cli train examples/hunyuan/hunyuan_full.yaml\n    ```\n\n&nbsp;\n\n\n## Quantization Compression\nWe used our own [AngleSlim](https://github.com/tencent/AngelSlim) compression tool to produce FP8 and INT4 quantization models. `AngleSlim` is a toolset dedicated to creating a more user-friendly, comprehensive and efficient model compression solution.\n\n### FP8 Quantization\nWe use FP8-static quantization, FP8 quantization adopts 8-bit floating point format, through a small amount of calibration data (without training) to pre-determine the quantization scale, the model weights and activation values will be converted to FP8 format, to improve the inference efficiency and reduce the deployment threshold. We you can use AngleSlim quantization, you can also directly download our quantization completed open source model to use [LINK](https://huggingface.co/).\n\n### Int4 Quantization\nWe use the GPTQ and AWQ algorithm to achieve W4A16 quantization.\n\nGPTQ processes the model weights layer by layer, uses a small amount of calibration data to minimize the reconfiguration error of the quantized weights, and adjusts the weights layer by layer by the optimization process of approximating the Hessian inverse matrix. The process eliminates the need to retrain the model and requires only a small amount of calibration data to quantize the weights, improving inference efficiency and lowering the deployment threshold. \nAWQ using a small amount of calibration data (without the need for training), the amplitude of the activation values is statistically calculated. For each weight channel, a scaling coefficient s is computed to expand the numerical range of important weights, allowing more information to be retained during quantization.\n\nYou can use  [AngleSlim](https://github.com/tencent/AngelSlim) quantization, you can also directly download our quantization completed open source model to use [LINK](https://huggingface.co/).\n\n\n\n#### Quantization Benchmark\nThis subsection describes the Benchmark metrics for the Hunyuan quantitative model.\n\n|     Bench     |           Quantization            |    Hunyuan-0.5B-Instruct     |     Hunyuan-1.8B-Instruct      |     Hunyuan-4B-Instruct      |     Hunyuan-7B-Instruct      |\n|:-------------:|:---------------------------------:|:----------------------------:|:------------------------------:|:----------------------------:|:----------------------------:|\n|     DROP      | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ | 52.8<br>51.6<br>50.9<br>48.9 |  76.7<br>75.1<br>73.0<br>71.7  | 78.2<br>78.3<br>78.1<br>78.2 | 85.9<br>86.0<br>85.7<br>85.9 |\n| GPQA-Diamond  | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ | 23.3<br>22.5<br>23.3<br>23.3 | 47.2<br>47.7<br>44.43<br>43.62 |  61.1<br>60.2<br>58.1<br>-   | 60.1<br>60.1<br>60.0<br>60.1 |\n| OlympiadBench | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ | 29.6<br>29.6<br>26.8<br>26.3 |  63.4<br>62.5<br>60.9<br>61.7  | 73.1<br>73.1<br>71.1<br>71.2 | 76.5<br>76.6<br>76.2<br>76.4 |\n|   AIME 2024   | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ |    17.2<br>17.2<br>-<br>-    |    56.7<br>55.17<br>-<br>-     |    78.3<br>76.6<br>-<br>-    | 81.1<br>80.9<br>81.0<br>80.9 |\n\n\n## Deployment   \n\nFor deployment, you can use frameworks such as **TensorRT-LLM**, **vLLM**, or **SGLang** to serve the model and create an OpenAI-compatible API endpoint.\n\nimage: https://hub.docker.com/r/hunyuaninfer/hunyuan-7B/tags \n\n\n### TensorRT-LLM\n\n#### Docker Image \n\nWe provide a pre-built Docker image based on the latest version of TensorRT-LLM.\n\nWe use tencent/Hunyuan-7B-Instruct for example\n- To get started:\n\nhttps://hub.docker.com/r/hunyuaninfer/hunyuan-large/tags \n\n```\ndocker pull hunyuaninfer/hunyuan-7B:hunyuan-moe-7B-trtllm\n```\n```\ndocker run --privileged --user root --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-7B:hunyuan-moe-7B-trtllm\n```\n\n- Prepare Configuration file:\n\n```\ncat >/path/to/extra-llm-api-config.yml <<EOF\nuse_cuda_graph: true\ncuda_graph_padding_enabled: true\ncuda_graph_batch_sizes:\n- 1\n- 2\n- 4\n- 8\n- 16\n- 32\nprint_iter_log: true\nEOF\n```\n\n\n- Start the API server:\n\n\n```\ntrtllm-serve \\\n  /path/to/HunYuan-moe-7B \\\n  --host localhost \\\n  --port 8000 \\\n  --backend pytorch \\\n  --max_batch_size 32 \\\n  --max_num_tokens 16384 \\\n  --tp_size 2 \\\n  --kv_cache_free_gpu_memory_fraction 0.6 \\\n  --trust_remote_code \\\n  --extra_llm_api_options /path/to/extra-llm-api-config.yml\n```\n\n\n### vllm\n\n#### Start\nPlease use vLLM version v0.10.0 or higher for inference.\n\nWe use tencent/Hunyuan-7B-Instruct for example\n- Download Model file: \n  - Huggingface:  will download automicly by vllm.\n  - ModelScope: `modelscope download --model Tencent-Hunyuan/Hunyuan-7B-Instruct`\n  \n- model download by huggingface:\n```shell\nexport MODEL_PATH=tencent/Hunyuan-7B-Instruct\n``` \n\n- model downloaded by modelscope:\n```shell\nexport MODEL_PATH=/root/.cache/modelscope/hub/models/Tencent-Hunyuan/Hunyuan-7B-Instruct/\n```\n\n- Start the API server:\n\n```shell\npython3 -m vllm.entrypoints.openai.api_server \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --trust-remote-code \\\n    --model ${MODEL_PATH} \\\n    --tensor-parallel-size 1 \\\n    --dtype bfloat16 \\\n    --quantization experts_int8 \\\n    --served-model-name hunyuan \\\n    2>&1 | tee log_server.txt\n``` \n- After running service script successfully, run the request script\n```shell\ncurl http://0.0.0.0:8000/v1/chat/completions -H 'Content-Type: application/json' -d '{\n\"model\": \"hunyuan\",\n\"messages\": [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"text\", \"text\": \"请按面积大小对四大洋进行排序，并给出面积最小的洋是哪一个？直接输出结果。\"}]\n    }\n],\n\"max_tokens\": 2048,\n\"temperature\":0.7,\n\"top_p\": 0.6,\n\"top_k\": 20,\n\"repetition_penalty\": 1.05,\n\"stop_token_ids\": [127960]\n}'\n```\n#### Quantitative model deployment\nThis section describes the process of deploying a post-quantization model using vLLM.\n\nDefault server in BF16.\n\n##### Int8 quantitative model deployment\nDeploying the Int8-weight-only version of the HunYuan-7B model only requires setting the environment variables\n\nNext we start the Int8 service. Run:\n```shell\npython3 -m vllm.entrypoints.openai.api_server \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --trust-remote-code \\\n    --model ${MODEL_PATH} \\\n    --tensor-parallel-size 1 \\\n    --dtype bfloat16 \\\n    --served-model-name hunyuan \\\n    --quantization experts_int8 \\\n    2>&1 | tee log_server.txt\n```\n\n\n##### Int4 quantitative model deployment\nDeploying the Int4-weight-only version of the HunYuan-7B model only requires setting the environment variables , using the GPTQ method\n```shell\nexport MODEL_PATH=PATH_TO_INT4_MODEL\n```\nNext we start the Int4 service. Run\n```shell\npython3 -m vllm.entrypoints.openai.api_server \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --trust-remote-code \\\n    --model ${MODEL_PATH} \\\n    --tensor-parallel-size 1 \\\n    --dtype bfloat16 \\\n    --served-model-name hunyuan \\\n    --quantization gptq_marlin \\\n    2>&1 | tee log_server.txt\n```\n\n##### FP8 quantitative model deployment\nDeploying the W8A8C8 version of the HunYuan-7B model only requires setting the environment variables\n\n\nNext we start the FP8 service. Run\n```shell\npython3 -m vllm.entrypoints.openai.api_server \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --trust-remote-code \\\n    --model ${MODEL_PATH} \\\n    --tensor-parallel-size 1 \\\n    --dtype bfloat16 \\\n    --served-model-name hunyuan \\\n    --kv-cache-dtype fp8 \\\n    2>&1 | tee log_server.txt\n```\n\n\n\n\n### SGLang\n\n#### Docker Image \n\nWe also provide a pre-built Docker image based on the latest version of SGLang.\n\nWe use tencent/Hunyuan-7B-Instruct for example\n\nTo get started:\n\n- Pull the Docker image\n\n```\ndocker pull lmsysorg/sglang:latest\n```\n\n- Start the API server:\n\n```\ndocker run --entrypoint=\"python3\" --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    --ulimit nproc=10000 \\\n    --privileged \\\n    --ipc=host \\\n     lmsysorg/sglang:latest \\\n    -m sglang.launch_server --model-path hunyuan/huanyuan_7B --tp 4 --trust-remote-code --host 0.0.0.0 --port 30000\n```\n\n\n## Contact Us\n\nIf you would like to leave a message for our R&D and product teams, Welcome to contact our open-source team . You can also contact us via email (hunyuan_opensource@tencent.com).",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1791080448,\"storage_bytes\":3582202056,\"files_count\":13,\"spaces_count\":0,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"HunYuanDenseV1ForCausalLM\"],\"model_type\":\"hunyuan_v1_dense\",\"tokenizer_config\":{\"bos_token\":\"<｜hy_begin▁of▁sentence｜>\",\"eos_token\":\"<｜hy_place▁holder▁no▁2｜>\",\"pad_token\":\"<｜hy_▁pad▁｜>\",\"chat_template\":\"{%- if not add_generation_prompt is defined %}\\n    {%- set add_generation_prompt = false %}\\n{%- endif %}\\n{%- set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_first_user=true, is_last_user=false) %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n        {%- if ns.is_first_sp %}\\n            {%- set ns.system_prompt = ns.system_prompt + message['content'] %}\\n            {%- set ns.is_first_sp = false %}\\n        {%- else %}\\n            {% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{{- bos_token }}\\n{{- ns.system_prompt }}\\n{%- if tools %}\\n    {%- if ns.system_prompt != '' %}\\n        {{- '\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.' }}\\n    {%- else %}\\n        {{- '# Tools\\n\\nYou may call one or more functions to assist with the user query.' }}\\n    {%- endif %}\\n    {{- '\\n\\nYou are provided with function signatures within <tools></tools> XML tags:' }}\\n    {{- '\\n<tools>\\n' }}\\n    {%- for tool in tools %}\\n        {%- if loop.index0 > 1 %}\\n            {{- '\\n' }}\\n        {%- endif %}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- '\\n</tools>\\n\\n' }}\\n    {{- 'For function call returns, you should first print <tool_calls>' }}\\n    {{- 'For each function call, you should return object like:\\n' }}\\n    {{- '<tool_call>function_name\\n```json\\nfunction_arguments_in_json_format\\n```</tool_call>' }}\\n    {{- 'At the end of function call returns, you should print </tool_calls>' }}\\n{%- endif %}\\n{%- if ns.system_prompt != '' or tools %}\\n    {{- '<｜hy_place▁holder▁no▁3｜>' }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'user' %}\\n        {%- set ns.is_tool = false %}\\n        {%- set ns.is_first = false %}\\n        {%- set ns.is_last_user = true %}\\n        {{- '<｜hy_User｜>' + message['content'] + '<｜hy_Assistant｜>' }}\\n    {%- endif %}\\n    {%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}\\n        {%- set ns.is_last_user = false %}\\n        {%- if ns.is_tool %}\\n            {{- '</tool_responses>' + '<｜hy_Assistant｜>' }}\\n        {%- endif %}\\n        {%- set ns.is_first = false %}\\n        {%- set ns.is_tool = false %}\\n        {%- set ns.is_output_first = true %}\\n        {%- for tool in message['tool_calls'] %}\\n            {%- set arguments = tool['function']['arguments'] %}\\n            {%- if arguments is not string %}\\n                {%- set arguments = arguments | tojson %}\\n            {%- endif %}\\n            {%- if not ns.is_first %}\\n                {%- if message['content'] is none %}\\n                    {{- '<tool_calls><tool_call>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + arguments + '\\n' + '```' + '</tool_call>' }}\\n                {%- else %}\\n                    {{- message['content'] + '<tool_calls><tool_call>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + arguments + '\\n' + '```' + '</tool_call>' }}\\n                {%- endif %}\\n            {%- set ns.is_first = true %}\\n            {%- else %}\\n                {{- '\\n' + '<tool_call>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + arguments + '\\n' + '```' + '</tool_call>' }}\\n            {%- endif %}\\n        {%- endfor %}\\n        {{- '</tool_calls>' + eos_token }}\\n    {%- endif %}\\n    {%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}\\n        {%- set content = message['content'] %}\\n        {%- if '<answer>' in content and not loop.last %}\\n            {%- set content = content.split('<answer>')[-1].strip('</answer>').strip() %}\\n        {%- endif %}\\n        {%- set ns.is_last_user = false %}\\n        {%- if ns.is_tool %}\\n            {{- '</tool_responses>' + '<｜hy_Assistant｜>' + content + eos_token }}\\n            {%- set ns.is_tool = false %}\\n        {%- else %}\\n            {{- content + eos_token }}\\n        {%- endif %}\\n    {%- endif %}\\n    {%- if message['role'] == 'tool' %}\\n        {%- set ns.is_last_user = false %}\\n        {%- set ns.is_tool = true %}\\n        {%- if ns.is_output_first %}\\n            {{- '<｜hy_User｜>' + '<tool_responses><tool_response>' + message['content'] + '</tool_response>' }}\\n            {%- set ns.is_output_first = false %}\\n        {%- else %}\\n            {{- '\\n<tool_response>' + message['content'] + '</tool_response>' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if ns.is_tool %}\\n    {{- '</tool_responses>' + '<｜hy_Assistant｜>' }}\\n{%- endif %}\\n{%- if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}\\n    {{- '<｜hy_Assistant｜>' }}\\n{%- endif %}\\n{%- if enable_thinking is defined and not enable_thinking %}\\n    {{- '<think>\\n\\n</think>\\n' }}\\n{%- endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent:AngelSlim\",\"source_url\":\"https://github.com/Tencent/AngelSlim\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-1.8B\\\"><b>GITHUB<\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-1.8B\\\"><b>GITHUB<\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:Hunyuan-1.8B\",\"source_url\":\"https://github.com/Tencent-Hunyuan/Hunyuan-1.8B\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers@4970b23cedaf745f963779b4eae68da281e8c6ca\",\"source_url\":\"https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca\"},{\"type\":\"has_code\",\"target_id\":\"github:hiyouga:LLaMA-Factory\",\"source_url\":\"https://github.com/hiyouga/LLaMA-Factory\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed#installation\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers@4970b23cedaf745f963779b4eae68da281e8c6ca\",\"source_url\":\"https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca\"},{\"type\":\"has_code\",\"target_id\":\"github:tencent:AngelSlim\",\"source_url\":\"https://github.com/tencent/AngelSlim\"},{\"type\":\"has_code\",\"target_id\":\"github:tencent:AngelSlim\",\"source_url\":\"https://github.com/tencent/AngelSlim\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 67.8,
    "content_hash": "71ac2acbd213f3440b710ee45c5af444",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/Hunyuan-1.8B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:microsoft:trellis-image-large",
    "name": "TRELLIS-image-large",
    "author": "microsoft",
    "description": "--- library_name: trellis pipeline_tag: image-to-3d license: mit language: - en --- <!-- Provide a quick summary of what the model is/does. --> The image conditioned version of TRELLIS, a large 3D genetive model. It was introduced in the paper Structured 3D Latents for Scalable and Versatile 3D Generation. Project page: https://trellis3d.github.io/ Code: https://github.com/Microsoft/TRELLIS",
    "tags": [
      "trellis",
      "image-to-3d",
      "en",
      "arxiv:2412.01506",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "image-to-3d",
    "likes": 596,
    "downloads": 2474761,
    "source": "huggingface",
    "source_url": "https://huggingface.co/microsoft/TRELLIS-image-large",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: trellis\npipeline_tag: image-to-3d\nlicense: mit\nlanguage:\n- en\n---\n# TRELLIS Image Large\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThe image conditioned version of TRELLIS, a large 3D genetive model. It was introduced in the paper [Structured 3D Latents for Scalable and Versatile 3D Generation](https://huggingface.co/papers/2412.01506).\n\nProject page: https://trellis3d.github.io/\n\nCode: https://github.com/Microsoft/TRELLIS\n",
    "meta_json": "{\"pipeline_tag\":\"image-to-3d\",\"library_name\":\"trellis\",\"framework\":\"trellis\",\"params\":null,\"storage_bytes\":3300497168,\"files_count\":19,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Microsoft:TRELLIS\",\"source_url\":\"https://github.com/Microsoft/TRELLIS\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2412.01506\",\"source_url\":\"https://arxiv.org/abs/2412.01506\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 37.8,
    "content_hash": "7469d386a9c81fa9510892869ccfee8a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/microsoft/TRELLIS-image-large\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-32b",
    "name": "Qwen3-32B",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-32B/blob/main/LICENSE pipeline_tag: text-generation --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense a...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:2309.00071",
      "arxiv:2505.09388",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 596,
    "downloads": 4240315,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-32B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-32B/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-32B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-32B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 32.8B\n- Number of Paramaters (Non-Embedding): 31.2B\n- Number of Layers: 64\n- Number of Attention Heads (GQA): 64 for Q and 8 for KV\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-32B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-32B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-32B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-32B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":32762123264,\"storage_bytes\":65535751214,\"files_count\":27,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3ForCausalLM\"],\"model_type\":\"qwen3\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3\",\"source_url\":\"https://github.com/QwenLM/Qwen3\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 77.8,
    "content_hash": "9213066e2eaf22d7fb34c7f259d9a6d8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-32B\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mistralai:voxtral-mini-3b-2507",
    "name": "Voxtral-Mini-3B-2507",
    "author": "mistralai",
    "description": "--- library_name: mistral-common language: - en - fr - de - es - it - pt - nl - hi license: apache-2.0 inference: false extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>. tags: - vllm --- Voxtral Mini is an enhancement of Ministral 3B, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, transl...",
    "tags": [
      "mistral-common",
      "safetensors",
      "voxtral",
      "vllm",
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "nl",
      "hi",
      "arxiv:2507.13264",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 596,
    "downloads": 535206,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Voxtral-Mini-3B-2507",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: mistral-common\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- nl\n- hi\nlicense: apache-2.0\ninference: false\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\ntags:\n- vllm\n---\n# Voxtral Mini 1.0 (3B) - 2507\n\nVoxtral Mini is an enhancement of [Ministral 3B](https://mistral.ai/news/ministraux), incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding.\n\nLearn more about Voxtral in our blog post [here](https://mistral.ai/news/voxtral) and our [research paper](https://arxiv.org/abs/2507.13264).\n\n## Key Features\n\nVoxtral builds upon Ministral-3B with powerful audio understanding capabilities.\n- **Dedicated transcription mode**: Voxtral can operate in a pure speech transcription mode to maximize performance. By default, Voxtral automatically predicts the source audio language and transcribes the text accordingly\n- **Long-form context**: With a 32k token context length, Voxtral handles audios up to 30 minutes for transcription, or 40 minutes for understanding\n- **Built-in Q&A and summarization**: Supports asking questions directly through audio. Analyze audio and generate structured summaries without the need for separate ASR and language models\n- **Natively multilingual**: Automatic language detection and state-of-the-art performance in the world’s most widely used languages (English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian)\n- **Function-calling straight from voice**: Enables direct triggering of backend functions, workflows, or API calls based on spoken user intents\n- **Highly capable at text**: Retains the text understanding capabilities of its language model backbone, Ministral-3B\n\n## Benchmark Results\n\n### Audio\n\nAverage word error rate (WER) over the FLEURS, Mozilla Common Voice and Multilingual LibriSpeech benchmarks:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/puASxtajF1lDeGYPrRK5y.png)\n\n### Text\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/5dfcb1aada6d0311fd3d5448/iH9V8JVtMoaGlqJd6FIri.png)\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm-recommended)\n- [`Transformers` 🤗](https://github.com/huggingface/transformers): See [here](#transformers-🤗)\n\n**Notes**:\n\n- `temperature=0.2` and `top_p=0.95` for chat completion (*e.g. Audio Understanding*) and `temperature=0.0` for transcription\n- Multiple audios per message and multiple user turns with audio are supported\n- System prompts are not yet supported\n\n### vLLM (recommended)\n\nWe recommend using this model with [vLLM](https://github.com/vllm-project/vllm).\n\n#### Installation\n\nMake sure to install vllm >= 0.10.0, we recommend using `uv`:\n\n```\nuv pip install -U \"vllm[audio]\" --system\n```\n\nDoing so should automatically install [`mistral_common >= 1.8.1`](https://github.com/mistralai/mistral-common/releases/tag/v1.8.1).\n\nTo check:\n```\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n```\n\n#### Offline\n\nYou can test that your vLLM setup works as expected by cloning the vLLM repo:\n\n```sh\ngit clone https://github.com/vllm-project/vllm && cd vllm\n```\n\nand then running:\n\n```sh\npython examples/offline_inference/audio_language.py --num-audios 2 --model-type voxtral\n```\n\n#### Serve\n\nWe recommend that you use Voxtral-Small-24B-2507 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Voxtral-Mini-3B-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n\n**Note:** Running Voxtral-Mini-3B-2507 on GPU requires ~9.5 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet. See the following examples.\n\n\n### Audio Instruct\n\nLeverage the audio capabilities of Voxtral-Mini-3B-2507 to chat.\n\nMake sure that your client has `mistral-common` with audio installed:\n\n```sh\npip install --upgrade mistral_common\\[audio\\]\n```\n\n<details>\n  <summary>Python snippet</summary>\n\n```py\nfrom mistral_common.protocol.instruct.messages import TextChunk, AudioChunk, UserMessage, AssistantMessage, RawAudio\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\n\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\nbcn_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"bcn_weather.mp3\", repo_type=\"dataset\")\n\ndef file_to_chunk(file: str) -> AudioChunk:\n    audio = Audio.from_file(file, strict=False)\n    return AudioChunk.from_audio(audio)\n\ntext_chunk = TextChunk(text=\"Which speaker is more inspiring? Why? How are they different from each other?\")\nuser_msg = UserMessage(content=[file_to_chunk(obama_file), file_to_chunk(bcn_file), text_chunk]).to_openai()\n\nprint(30 * \"=\" + \"USER 1\" + 30 * \"=\")\nprint(text_chunk.text)\nprint(\"\\n\\n\")\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=[user_msg],\n    temperature=0.2,\n    top_p=0.95,\n)\ncontent = response.choices[0].message.content\n\nprint(30 * \"=\" + \"BOT 1\" + 30 * \"=\")\nprint(content)\nprint(\"\\n\\n\")\n# The speaker who is more inspiring is the one who delivered the farewell address, as they express\n# gratitude, optimism, and a strong commitment to the nation and its citizens. They emphasize the importance of\n# self-government and active citizenship, encouraging everyone to participate in the democratic process. In contrast,\n# the other speaker provides a factual update on the weather in Barcelona, which is less inspiring as it\n# lacks the emotional and motivational content of the farewell address.\n\n# **Differences:**\n# - The farewell address speaker focuses on the values and responsibilities of citizenship, encouraging active participation in democracy.\n# - The weather update speaker provides factual information about the temperature in Barcelona, without any emotional or motivational content.\n\n\nmessages = [\n    user_msg,\n    AssistantMessage(content=content).to_openai(),\n    UserMessage(content=\"Ok, now please summarize the content of the first audio.\").to_openai()\n]\nprint(30 * \"=\" + \"USER 2\" + 30 * \"=\")\nprint(messages[-1][\"content\"])\nprint(\"\\n\\n\")\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=0.2,\n    top_p=0.95,\n)\ncontent = response.choices[0].message.content\nprint(30 * \"=\" + \"BOT 2\" + 30 * \"=\")\nprint(content)\n```\n</details>\n\n#### Transcription\n\nVoxtral-Mini-3B-2507 has powerful transcription capabilities! \n\nMake sure that your client has `mistral-common` with audio installed:\n\n```sh\npip install --upgrade mistral_common\\[audio\\]\n```\n\n<details>\n  <summary>Python snippet</summary>\n\n```python\nfrom mistral_common.protocol.transcription.request import TranscriptionRequest\nfrom mistral_common.protocol.instruct.messages import RawAudio\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\n\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\naudio = Audio.from_file(obama_file, strict=False)\n\naudio = RawAudio.from_audio(audio)\nreq = TranscriptionRequest(model=model, audio=audio, language=\"en\", temperature=0.0).to_openai(exclude=(\"top_p\", \"seed\"))\n\nresponse = client.audio.transcriptions.create(**req)\nprint(response)\n```\n</details>\n\n### Transformers 🤗\n\nStarting with `transformers >= 4.54.0` and above, you can run Voxtral natively!\n\nInstall Transformers:\n```bash\npip install -U transformers\n```\n\nMake sure to have `mistral-common >= 1.8.1` installed with audio dependencies:\n```bash\npip install --upgrade \"mistral-common[audio]\"\n```\n\n#### Audio Instruct\n\n<details>\n  <summary>➡️ multi-audio + text instruction</summary>\n\n```python\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\n\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"audio\",\n                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\",\n            },\n            {\n                \"type\": \"audio\",\n                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n            },\n            {\"type\": \"text\", \"text\": \"What sport and what nursery rhyme are referenced?\"},\n        ],\n    }\n]\n\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\n\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n```\n</details>\n\n\n<details>\n  <summary>➡️ multi-turn</summary>\n\n```python\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\n\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"audio\",\n                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n            },\n            {\n                \"type\": \"audio\",\n                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe briefly what you can hear.\"},\n        ],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"audio\",\n                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n            },\n            {\"type\": \"text\", \"text\": \"Ok, now compare this new audio with the previous one.\"},\n        ],\n    },\n]\n\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\n\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n```\n</details>\n\n\n<details>\n  <summary>➡️ text only</summary>\n\n```python\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\n\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Why should AI models be open-sourced?\",\n            },\n        ],\n    }\n]\n\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\n\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n```\n</details>\n\n\n<details>\n  <summary>➡️ audio only</summary>\n\n```python\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\n\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"audio\",\n                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n            },\n        ],\n    }\n]\n\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\n\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n```\n</details>\n\n\n<details>\n  <summary>➡️ batched inference</summary>\n\n```python\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\n\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n\nconversations = [\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"audio\",\n                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n                },\n                {\n                    \"type\": \"audio\",\n                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Who's speaking in the speach and what city's weather is being discussed?\",\n                },\n            ],\n        }\n    ],\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"audio\",\n                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n                },\n                {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n            ],\n        }\n    ],\n]\n\ninputs = processor.apply_chat_template(conversations)\ninputs = inputs.to(device, dtype=torch.bfloat16)\n\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated responses:\")\nprint(\"=\" * 80)\nfor decoded_output in decoded_outputs:\n    print(decoded_output)\n    print(\"=\" * 80)\n```\n</details>\n\n#### Transcription\n\n<details>\n  <summary>➡️ transcribe</summary>\n\n```python\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\n\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n\ninputs = processor.apply_transcription_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\", model_id=repo_id)\ninputs = inputs.to(device, dtype=torch.bfloat16)\n\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated responses:\")\nprint(\"=\" * 80)\nfor decoded_output in decoded_outputs:\n    print(decoded_output)\n    print(\"=\" * 80)\n```\n</details>",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"mistral-common\",\"framework\":\"mistral-common\",\"params\":4676271104,\"storage_bytes\":18732435234,\"files_count\":11,\"spaces_count\":13,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"VoxtralForConditionalGeneration\"],\"model_type\":\"voxtral\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2507.13264\",\"source_url\":\"https://arxiv.org/abs/2507.13264\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 77.8,
    "content_hash": "d768c4d04ad22a3aeea2a796749a349f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Voxtral-Mini-3B-2507\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:microsoft:vibevoice-realtime-0.5b",
    "name": "VibeVoice-Realtime-0.5B",
    "author": "microsoft",
    "description": "--- license: mit language: - en pipeline_tag: text-to-speech tags: - Realtime TTS - Streaming text input - Long-form speech generation library_name: transformers --- VibeVoice-Realtime is a **lightweight real‑time** text-to-speech model supporting **streaming text input** and **robust long-form speech generation**. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long bef...",
    "tags": [
      "transformers",
      "safetensors",
      "vibevoice_streaming",
      "realtime tts",
      "streaming text input",
      "long-form speech generation",
      "text-to-speech",
      "en",
      "arxiv:2508.19205",
      "arxiv:2412.08635",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 593,
    "downloads": 56958,
    "source": "huggingface",
    "source_url": "https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlanguage:\n- en\npipeline_tag: text-to-speech\ntags:\n- Realtime TTS\n- Streaming text input\n- Long-form speech generation\nlibrary_name: transformers\n---\n\n## VibeVoice: A Frontier Open-Source Text-to-Speech Model\n\nVibeVoice-Realtime is a **lightweight real‑time** text-to-speech model supporting **streaming text input** and **robust long-form speech generation**. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in **~300 ms** (hardware dependent).\n\n[▶️ Watch demo video](https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc) (Launch your own realtime demo via the websocket example in [Usage](https://github.com/microsoft/VibeVoice/blob/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo))\n\nAlthough the model is primarily built for English, we found that it still exhibits a certain level of multilingual capability—and even performs reasonably well in some languages. We provide nine additional languages (German, French, Italian, Japanese, Korean, Dutch, Polish, Portuguese, and Spanish) for users to explore and share feedback.\n\nThe model uses an interleaved, windowed design: it incrementally encodes incoming text chunks while, in parallel, continuing diffusion-based acoustic latent generation from prior context. Unlike the full multi-speaker long-form variants, this streaming model removes the semantic tokenizer and relies solely on an efficient acoustic tokenizer operating at an ultra-low frame rate (7.5 Hz).\n\nKey features:\n- Parameter size: 0.5B (deployment-friendly)\n- Realtime TTS (~300 ms first audible latency)\n- Streaming text input\n- Robust long-form speech generation\n\n<p align=\"left\">\n  <img src=\"figures/Fig1.png\" alt=\"VibeVoice Realtime Model Overview\" height=\"250px\">\n</p>\n\nThis realtime variant supports only a single speaker. For multi-speaker conversational speech generation, please use other [VibeVoice models](https://huggingface.co/collections/microsoft/vibevoice). The model is currently intended for English speech only; other languages may produce unpredictable results.\n\n➡️ **Technical Report:** [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)\n\n➡️ **Project Page:** [microsoft/VibeVoice](https://microsoft.github.io/VibeVoice)\n\n➡️ **Code:** [microsoft/VibeVoice-Code](https://github.com/microsoft/VibeVoice)\n\n➡️ **App:** [anycoderapps/VibeVoice-Realtime-0.5B](https://huggingface.co/spaces/anycoderapps/VibeVoice-Realtime-0.5B)\n\n\n\n## Training Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic tokenizer and a diffusion-based decoding head.\n- LLM: [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) for this release.\n- Tokenizers:\n    - Acoustic Tokenizer: Based on a σ-VAE variant (proposed in [LatentLM](https://arxiv.org/pdf/2412.08635)), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Decoder component is ~340M parameters.\n- Diffusion Head: Lightweight module (4 layers, ~40M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\n- Context Length: Trained with a curriculum increasing up to 8,192 tokens.\n- Training Stages:\n    - Tokenizer Pre-training: Acoustic tokenizer is pre-trained.\n    - VibeVoice Training: Pre-trained tokenizer is frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 8K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic tokenizer.\n\n\n## Models\n| Model | Context Length | Generation Length |  Weight |\n|-------|----------------|----------|----------|\n| VibeVoice-Realtime-0.5B | 8k | ~10 min | You are here. |\n| VibeVoice-1.5B | 64K | ~90 min | [HF link](https://huggingface.co/microsoft/VibeVoice-1.5B) |\n| VibeVoice-Large| 32K | ~45 min | [HF link](https://huggingface.co/microsoft/VibeVoice-Large) |\n\n\n## Results\n\nThe model achieves satisfactory performance on short-sentence benchmarks, while the model is more focused on long‑form speech generation.\n\n### Zero-shot TTS performance on LibriSpeech test-clean set\n\n| Model | WER (%) ↓ | Speaker Similarity ↑ |\n|:--------------------|:---------:|:----------------:|\n| VALL-E 2            | 2.40      | 0.643            |\n| Voicebox            | 1.90      | 0.662            |\n| MELLE               | 2.10      | 0.625            |\n| **VibeVoice-Realtime-0.5B** | 2.00 | 0.695            |\n\n### Zero-shot TTS performance on SEED test-en set\n\n| Model | WER (%) ↓ | Speaker Similarity ↑ |\n|:--------------------|:---------:|:----------------:|\n| MaskGCT             | 2.62      | 0.714            |\n| Seed-TTS            | 2.25      | 0.762            |\n| FireRedTTS          | 3.82      | 0.460            |\n| SparkTTS            | 1.98      | 0.584            |\n| CosyVoice2          | 2.57      | 0.652            |\n| **VibeVoice-Realtime-0.5B** | 2.05 | 0.633            | \n\n\n## Installation and Usage\n\nPlease refer to [GitHub README](https://github.com/microsoft/VibeVoice/blob/main/docs/vibevoice-realtime-0.5b.md#installation)\n\n\n## Responsible Usage\n### Direct intended uses\nThe VibeVoice-Realtime model is limited to research purposes exploring real-time highly realistic audio generation detailed in the [tech report](https://arxiv.org/pdf/2508.19205). \n\n### Out-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\n\n- Voice impersonation without explicit, recorded consent, including but not limited to, cloning a real individual’s voice for satire, advertising, ransom, social‑engineering, or authentication bypass. \n- Disinformation or impersonation, including but not limited to, creating audio presented as genuine recordings of real people or events. \n- Real‑time or low‑latency voice conversion, including but not limited to, telephone or video‑conference “live deep‑fake” applications.\n- Any act to circumvent, disable, or otherwise interfere with any technical or procedural safeguards implemented in this release, including but not limited to security controls, watermarking and other transparency mechanisms. Any act of reverse engineering, modification, injection of unauthorized code, or exploitation of vulnerabilities for purposes beyond the intended scope of use. \n- Unsupported language – the model is trained only on English data; outputs in other languages are unsupported and may be unintelligible or inappropriate.\n- Generation of background ambience, Foley, or music – VibeVoice is speech‑only and cannot produce coherent non‑speech audio such as music.\n\n\n## Risks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice may inherit any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 0.5b in this release). \nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish only: Transcripts in language other than English may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nCode, formulas, and special symbols – The model does not currently support reading code, mathematical formulas, or uncommon symbols. Please pre‑process input text to remove or normalize such content to avoid unpredictable results.\n\n\n## Recommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. If you use this model to generate speech, we recommend disclosing to the end user that they are listening to AI generated content. This model is intended for research and development purposes only. Please use responsibly.\n\nTo mitigate the risks of misuse, we have:\nRemoved acoustic tokenizer to avoid users creating embedding on their own. \nEmbedded an audible disclaimer (e.g. “This segment was generated by AI”) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nUsers are responsible for sourcing their datasets legally. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns. \n\n\n## Contact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations.\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1017626722,\"storage_bytes\":2035456431,\"files_count\":6,\"spaces_count\":4,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"VibeVoiceStreamingForConditionalGenerationInference\"],\"model_type\":\"vibevoice_streaming\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:VibeVoice\",\"source_url\":\"https://github.com/microsoft/VibeVoice\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:VibeVoice\",\"source_url\":\"https://github.com/microsoft/VibeVoice\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:VibeVoice\",\"source_url\":\"https://github.com/microsoft/VibeVoice\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2508.19205\",\"source_url\":\"https://arxiv.org/abs/2508.19205\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2412.08635\",\"source_url\":\"https://arxiv.org/abs/2412.08635\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 77.7,
    "content_hash": "ace1b7b0322fbfcacf40e8e8d7866625",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:janus-1.3b",
    "name": "Janus-1.3B",
    "author": "deepseek-ai",
    "description": "--- license: mit license_name: deepseek license_link: LICENSE pipeline_tag: any-to-any library_name: transformers tags: - muiltimodal - text-to-image - unified-model --- **2024.10.20**: We have uploaded the correct . The previous file was missing the , which caused poor visual generation results. Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathway...",
    "tags": [
      "transformers",
      "safetensors",
      "multi_modality",
      "muiltimodal",
      "text-to-image",
      "unified-model",
      "any-to-any",
      "arxiv:2410.13848",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "any-to-any",
    "likes": 592,
    "downloads": 7445,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/Janus-1.3B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlicense_name: deepseek\nlicense_link: LICENSE\npipeline_tag: any-to-any\nlibrary_name: transformers\ntags:\n- muiltimodal\n- text-to-image\n- unified-model\n---\n\n## 0. Update\n**2024.10.20**: We have uploaded the correct `tokenizer_config.json`. The previous file was missing the `pad_token`, which caused poor visual generation results.\n\n\n## 1. Introduction\n\nJanus is a novel autoregressive framework that unifies multimodal understanding and generation. \nIt addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder’s roles in understanding and generation, but also enhances the framework’s flexibility. \nJanus surpasses previous unified model and matches or exceeds the performance of task-specific models. \nThe simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.\n\n[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848)\n\n[**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n<div align=\"center\">\n<img alt=\"image\" src=\"teaser.png\" style=\"width:90%;\">\n</div>\n\n\n### 2. Model Summary\n\nJanus is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. \nJanus is constructed based on the DeepSeek-LLM-1.3b-base which is trained on an approximate corpus of 500B text tokens.\nFor multimodal understanding, it uses the [SigLIP-L](https://huggingface.co/timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus uses the tokenizer from [here](https://github.com/FoundationVision/LlamaGen) with a downsample rate of 16.\n\n<div align=\"center\">\n<img alt=\"image\" src=\"arch.jpg\" style=\"width:90%;\">\n</div>\n\n## 3. Quick Start\n\nPlease refer to [**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n\n## 4. License\n\nThis code repository is licensed under [the MIT License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-CODE). The use of Janus models is subject to [DeepSeek Model License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-MODEL).\n## 5. Citation\n\n```\n@misc{wu2024janus,\n      title={Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation}, \n      author={Chengyue Wu and Xiaokang Chen and Zhiyu Wu and Yiyang Ma and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan and Ping Luo},\n      year={2024},\n      eprint={2410.13848},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2410.13848}, \n}\n```\n\n## 6. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
    "meta_json": "{\"pipeline_tag\":\"any-to-any\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":2089297547,\"storage_bytes\":4178706382,\"files_count\":11,\"spaces_count\":19,\"gated\":false,\"private\":false,\"config\":{\"model_type\":\"multi_modality\",\"tokenizer_config\":{\"bos_token\":\"<｜begin▁of▁sentence｜>\",\"eos_token\":\"<｜end▁of▁sentence｜>\",\"pad_token\":\"<｜▁pad▁｜>\",\"unk_token\":null,\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:Janus\",\"source_url\":\"https://github.com/deepseek-ai/Janus\"},{\"type\":\"has_code\",\"target_id\":\"github:FoundationVision:LlamaGen\",\"source_url\":\"https://github.com/FoundationVision/LlamaGen\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:Janus\",\"source_url\":\"https://github.com/deepseek-ai/Janus\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-LLM\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-LLM\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-LLM\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-LLM\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2410.13848\",\"source_url\":\"https://arxiv.org/abs/2410.13848\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 62.7,
    "content_hash": "4b9a18cd67c138da1976f08e22ae3d68",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/Janus-1.3B\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:vrgamedevgirl84:wan14bt2vfusionix",
    "name": "Wan14BT2VFusioniX",
    "author": "vrgamedevgirl84",
    "description": "--- tags: - text-to-video - diffusion - merged-model - video-generation - wan2.1 widget: - text: >- Prompt: A gritty close-up of an elven princess kneeling in a rocky ravine, calming a wounded, desert dragon. Its scales are cracked, dry, She wears a crimson sash over bone-colored armor, her auburn hair half-tied back. The camera dollies in rapidly as she reaches for its eye ridge. Lighting comes from golden sunlight reflecting off surrounding rock, casting a warm, earthy hue with no artificia...",
    "tags": [
      "text-to-video",
      "diffusion",
      "merged-model",
      "video-generation",
      "wan2.1",
      "base_model:wan-ai/wan2.1-t2v-14b",
      "base_model:finetune:wan-ai/wan2.1-t2v-14b",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-video",
    "likes": 592,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- text-to-video\n- diffusion\n- merged-model\n- video-generation\n- wan2.1\n\nwidget:\n- text: >-\n    Prompt: A gritty close-up of an elven princess kneeling in a rocky ravine, calming a wounded, desert dragon. Its scales are cracked, dry, She wears a crimson sash over bone-colored armor, her auburn hair half-tied back. The camera dollies in rapidly as she reaches for its eye ridge. Lighting comes from golden sunlight reflecting off surrounding rock, casting a warm, earthy hue with no artificial glow.\n  output:\n    url: videos/Video_00063.mp4\n    \n- text: >-\n    Prompt: Tight close-up of her smiling lips and sparkling eyes, catching golden hour sunlight. She wears a white sundress with floral prints and a wide-brimmed straw hat. Camera pulls back in a dolly motion, revealing her twirling under a cherry blossom tree. Petals flutter in the air, casting playful shadows. Soft lens flares enhance the euphoric, dreamlike vibe. (Before vs After — Left: Wan2.1 | Right: Merged model Wan14BT2V_MasterModel)\n  output:\n    url: videos/AnimateDiff_00001.mp4\n\n- text: >-\n    Prompt: A gritty close-up of a dwarven beastmaster’s face, his grey beard braided tightly, brows furrowed as he looks just off-camera. The camera dollies out over his shoulder, revealing a perched gryphon watching him from a boulder, its feathers rustling slightly in the breeze. The moment holds stillness and mutual trust. Lighting is early daylight, clean and sharp with strong environmental clarity.\n  output:\n    url: videos/FusionX_00012.mp4\n\n- text: >-\n    Prompt: A gritty close-up of a jungle tracker crouching low, face flushed with focus as she watches a perched macaw a few feet ahead. Her cheek twitches as she shifts forward, beads of sweat visible on her brow. The camera slowly dollies in from below her line of sight, capturing the moment her eyes widen in fascination. Lighting is rich and directional from above, creating a warm glow over her face with minimal shadows.\n  output:\n    url: videos/FusionX_00005.mp4\n\n- text: >-\n    Prompt: A gritty close-up of a battle-worn ranger kneeling in a scorched clearing, calming a wounded gryphon whose wing is torn and bloodied. Its feathers are dusky bronze with streaks of ash-gray. She wears soot-covered hunter green armor, her blonde hair pulled into a loose braid. The camera dollies in as her hand brushes the creature's sharp beak. Lighting comes from late afternoon sun filtering through smoke, casting a burnt-orange haze across the frame.\n  output:\n    url: videos/Video_00069.mp4\n\n\n\nbase_model:\n- Wan-AI/Wan2.1-T2V-14B\nlicense: apache-2.0\n---\n\n# 🌀 Wan2.1_14B_FusionX\n\n**High-Performance Merged Text-to-Video Model**  \nBuilt on WAN 2.1 and fused with research-grade components for cinematic motion, detail, and speed — optimized for ComfyUI and rapid iteration in as few as 6 steps.\n\nMerged models for faster, richer motion & detail — high performance even at just 8 steps.\n\n> 📌 Important: To match the quality shown here, use the linked workflows or make sure to follow the recommended settings outlined below.\n\n---\n\n## 🚀 Overview\n\nA powerful text-to-video model built on top of **WAN 2.1 14B**, merged with several research-grade models to boost:\n\n- Motion quality\n- Scene consistency\n- Visual detail\n\nComparable with closed-source solutions, but open and optimized for **ComfyUI** workflows.\n\n---\n\n## 💡 Inside the Fusion\n\nThis model is made up of the following which is on TOP of Wan 2.1 14B 720p(FusionX would not be what it is without these Models):\n\n- **CausVid** – [Causal motion modeling for better flow and dynamics](https://github.com/tianweiy/CausVid)\n- **AccVideo** – [Better temporal alignment and speed boost](https://github.com/aejion/AccVideo)\n- **MoviiGen1.1** – [Cinematic smoothness and lighting](https://huggingface.co/ZuluVision/MoviiGen1.1)\n- **MPS Reward LoRA** – [Tuned for motion and detail](https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs)\n- **Custom LoRAs** – For texture, clarity, and small detail enhancements (Set at a very low level)\n\nAll merged models are provided for research and non-commercial use only.\nSome components are subject to licenses such as CC BY-NC-SA 4.0, and do not fall under permissive licenses like Apache 2.0 or MIT.\nPlease refer to each model’s original license for full usage terms.\n\n---\n\n## 🚨✨**Hey guys! Just a quick update!**\n\nWe finally cooked up **FusionX LoRAs**!! 🧠💥  \nThis is huge – now you can plug FusionX into your favorite workflows as a LoRA on top of the Wan base models and SkyReels models!🔌💫 \nYou can still stick with the base FusionX Model if you already use it, but if you would rather have more control over the \"FusionX\" strength and a speed boost, then this might be for you.\n\nOh, and there’s a **nice speed boost** too! ⚡  \n**Example:** *(RTX 5090)*  \n- FusionX as a full base model: **8 steps = 160s** ⏱️  \n- FusionX as a **LoRA on Wan 2.1 14B fp8 T2V**: **8 steps = 120s** 🚀\n\n**Bonus:** You can bump up the FusionX LoRA strength and lower your steps for a **huge speed boost** while testing/drafting.  \nExample: strength `2.00` with `3 steps` takes `72 seconds`.  \nOr lower the strength to experiment with a **less “FusionX” look**. ⚡🔍\n\nWe’ve got:\n- **T2V (Text to Video)** 🎬 – works perfectly with **VACE** ⚙️  \n- **I2V (Image to Video)** 🖼️➡️📽️  \n- A dedicated **Phantom LoRA** 👻  \nThe new LoRA's are [HERE](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/tree/main/FusionX_LoRa)\nNote: The LoRa's are not meant to be put on top of the FusionX main models and instead you would use them with the Wan base models.\n**New workflows**  are [HERE](https://civitai.com/models/1681541)  🛠️🚀\n\n---\n\nAfter lots of testing 🧪, the video quality with the LoRA is **just as good** (and sometimes **even better**! 💯)  \nThat’s thanks to it being trained on the **fp16 version** of FusionX 🧬💎\n\n---\n\n### 🌀 Preview Gallery  \n*These are compressed GIF previews for quick viewing — final video outputs are higher quality.*\n\n![FusionX_00020](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00020.gif)  \n![FusionX_00021](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00021.gif)  \n![FusionX_00022](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00022.gif)  \n![FusionX_00023](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00023.gif)  \n![FusionX_00024](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00024.gif)  \n![FusionX_00025](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00025.gif)  \n![FusionX_00026](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00026.gif)  \n![FusionX_00027](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00027.gif)  \n![FusionX_00028](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00028.gif)  \n![FusionX_00029](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00029.gif)  \n![FusionX_00030](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00030.gif)  \n![FusionX_00031](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00031.gif)\n\n---\n\n\n## 📂 Workflows & Model Downloads\n\n- 💡 **ComfyUI workflows** can be found here:  \n  👉 [Workflow Collection (WIP)](https://civitai.com/models/1663553)\n\n- 📦 **Model files (T2V, I2V, Phantom, VACE)**:  \n  👉 [Main Hugging Face Repo](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/tree/main)\n\n### 🧠 GGUF Variants:\n- 🖼️ [FusionX Image-to-Video (GGUF)](https://huggingface.co/QuantStack/Wan2.1_I2V_14B_FusionX-GGUF/tree/main)  \n- 🎥 [FusionX Text-to-Video (GGUF)](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_FusionX-GGUF/tree/main)  \n- 🎞️ [FusionX T2V VACE (for native)](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF/tree/main)  \n- 👻 [FusionX Phantom](https://huggingface.co/QuantStack/Phantom_Wan_14B_FusionX-GGUF/tree/main)\n\n---\n## 🎬 Example Videos\n\nWant to see what FusionX can do? Check out these real outputs generated using the latest workflows and settings:\n\n- **Text-to-Video**  \n  👉 [Watch Examples](https://civitai.com/posts/17874424)\n\n- **Image-to-Video**  \n  👉 [Watch Examples](https://civitai.com/posts/18029174)\n\n- **Phantom Mode**  \n  👉 [Watch Examples](https://civitai.com/posts/17986906)\n\n- **VACE Integration**  \n  👉 [Watch Examples](https://civitai.com/posts/18080876)\n\n---\n\n## 🔧 Usage Details\n\n### Text-to-Video\n\n- **CGF**: Must be set to `1`  \n- **Shift**:  \n  - `1024x576`: Start at `1`  \n  - `1080x720`: Start at `2`  \n  - For realism → lower values  \n  - For stylized → test `3–9`\n- **Scheduler**:  \n  - Recommended: `uni_pc`  \n  - Alternative: `flowmatch_causvid` (better for some details)\n\n### Image-to-Video\n\n- **CGF**: `1`\n- **Shift**: `2` works best in most cases\n- **Scheduler**:  \n  - Recommended: `dmp++_sde/beta`  \n- To boost motion and reduce slow-mo effect:\n  - Frame count: `121`\n  - FPS: `24`\n\n---\n\n## 🛠 Technical Notes\n\n- Works in as few as **6 steps**\n- Best quality at **8–10 steps**\n- Drop-in replacement for `Wan2.1-T2V-14B`\n- Up to **50% faster rendering**, especially with **SageAttn**\n- Works natively and with **Kaji Wan Wrapper**  \n  [Wrapper GitHub](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n- Do **not** re-add merged LoRAs (CausVid, AccVideo, MPS)\n- Feel free to add **other LoRAs** for style/variation\n- Native WAN workflows also supported (slightly slower)\n\n---\n\n## 🧪 Performance Tips\n\n- RTX 5090 → ~138 sec/video at 1024x576 / 81 frames\n- If VRAM is limited:\n  - Enable block swapping\n  - Start with `5` blocks and adjust as needed\n- Use **SageAttn** for ~30% speedup (wrapper only)\n- Do **not** use `teacache`\n- \"Enhance a video\" (tested): Adds vibrance (try values 2–4)\n- \"SLG\" not tested — feel free to explore\n\n---\n\n## 🧠 Prompt Help\n\nWant better cinematic prompts? Try the **WAN Cinematic Video Prompt Generator GPT** — it adds visual richness and makes a big difference in quality. [Download Here](https://chatgpt.com/g/g-67c3a6d6d19c81919b3247d2bfd01d0b-wan-cinematic-video-prompt-generator)\n\n---\n\n## 📣 Join The Community\n\nWe’re building a friendly space to chat, share outputs, and get help.\n\n- Motion LoRAs coming soon\n- Tips, updates, and support from other users\n\n👉 [Join the Discord](https://discord.com/invite/hxPmmXmRW3)\n\n---\n\n## ⚖️ License\n\nSome merged components use permissive licenses (Apache 2.0 / MIT),  \n**but others** — such as those from research models like *CausVid* — may be released under **non-commercial licenses** (e.g., [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)).\n\n- ✅ You **can** use, modify, and redistribute **under original license terms**  \n- ❗ You **must** retain and respect the license of each component  \n- ⚠️ **Commercial use is not permitted** for models or components under non-commercial licenses  \n- 📌 Outputs are **not automatically licensed** — do your own due diligence\n\nThis model is intended for **research, education, and personal use only**.  \nFor commercial use or monetization, please consult a legal advisor and verify all component licenses.\n\n---\n\n## 🙏 Credits\n\n- WAN Team (base model)\n- aejion (AccVideo)\n- Tianwei Yin (CausVid)\n- ZuluVision (MoviiGen)\n- Alibaba PAI (MPS LoRA)\n- Kijai (ComfyUI Wrapper)\n\nAnd thanks to the open-source community!\n\n---\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-video\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":122604036605,\"files_count\":41,\"spaces_count\":42,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:tianweiy:CausVid\",\"source_url\":\"https://github.com/tianweiy/CausVid\"},{\"type\":\"has_code\",\"target_id\":\"github:aejion:AccVideo\",\"source_url\":\"https://github.com/aejion/AccVideo\"},{\"type\":\"has_code\",\"target_id\":\"github:kijai:ComfyUI-WanVideoWrapper\",\"source_url\":\"https://github.com/kijai/ComfyUI-WanVideoWrapper\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 77.7,
    "content_hash": "38ccaf5a6343ef203638d51865cfd56e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:hunyuanworld-1",
    "name": "HunyuanWorld-1",
    "author": "tencent",
    "description": "--- library_name: diffusion-single-file license: other license_name: tencent-hunyuanworld-1.0-community license_link: https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/blob/main/LICENSE language: - en - zh tags: - hunyuan3d - worldmodel - 3d-aigc - 3d-generation - 3d - scene-generation pipeline_tag: image-to-3d extra_gated_eu_disallowed: true --- <p align=\"center\"> <img src=\"assets/teaser.png\"> </p> <div align=\"center\"> <a href=https://3d.hunyuan.tencent.com/sceneTo3D target=\"_blank\"><img s...",
    "tags": [
      "diffusion-single-file",
      "hunyuan3d",
      "worldmodel",
      "3d-aigc",
      "3d-generation",
      "3d",
      "scene-generation",
      "image-to-3d",
      "en",
      "zh",
      "arxiv:2507.21809",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "image-to-3d",
    "likes": 589,
    "downloads": 13681,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/HunyuanWorld-1",
    "image_url": "https://huggingface.co/tencent/HunyuanWorld-1/resolve/main/assets/qrcode/discord.png",
    "type": "model",
    "body_content": "---\nlibrary_name: diffusion-single-file\nlicense: other\nlicense_name: tencent-hunyuanworld-1.0-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/blob/main/LICENSE\nlanguage:\n- en\n- zh\ntags:\n- hunyuan3d\n- worldmodel\n- 3d-aigc\n- 3d-generation\n- 3d\n- scene-generation\npipeline_tag: image-to-3d\nextra_gated_eu_disallowed: true\n---\n<p align=\"center\">\n  <img src=\"assets/teaser.png\">\n</p>\n\n<div align=\"center\">\n  <a href=https://3d.hunyuan.tencent.com/sceneTo3D target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanWorld-1 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://3d-models.hunyuan.tencent.com/world/ target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/abs/2507.21809 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://discord.gg/dNBrdrGGMa target=\"_blank\"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n <a href=\"#community-resources\" target=\"_blank\"><img src=https://img.shields.io/badge/Community-lavender.svg?logo=homeassistantcommunitystore height=22px></a>\n</div>\n\n[//]: # (  <a href=# target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>)\n\n[//]: # (  <a href=# target=\"_blank\"><img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px></a>)\n\n[//]: # (  <a href=\"#\"><img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/v/mulankit?logo=pypi\"  height=22px></a>)\n\n<br>\n\n<p align=\"center\">\n  \"To see a World in a Grain of Sand, and a Heaven in a Wild Flower\"\n</p>\n\n## 🔗 BibTeX\n```\n@misc{hunyuanworld2025tencent,\n    title={HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n## Acknowledgements\nWe would like to thank the contributors to the [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers), [HuggingFace](https://huggingface.co), [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN), [ZIM](https://github.com/naver-ai/ZIM), [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO), [MoGe](https://github.com/microsoft/moge), [Worldsheet](https://worldsheet.github.io/), [WorldGen](https://github.com/ZiYang-xie/WorldGen) repositories, for their open research.",
    "meta_json": "{\"pipeline_tag\":\"image-to-3d\",\"library_name\":\"diffusion-single-file\",\"framework\":\"diffusion-single-file\",\"params\":null,\"storage_bytes\":1081660589,\"files_count\":12,\"spaces_count\":9,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanWorld-1.0\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:stablediffusion\",\"source_url\":\"https://github.com/Stability-AI/stablediffusion\"},{\"type\":\"has_code\",\"target_id\":\"github:black-forest-labs:flux\",\"source_url\":\"https://github.com/black-forest-labs/flux\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:xinntao:Real-ESRGAN\",\"source_url\":\"https://github.com/xinntao/Real-ESRGAN\"},{\"type\":\"has_code\",\"target_id\":\"github:naver-ai:ZIM\",\"source_url\":\"https://github.com/naver-ai/ZIM\"},{\"type\":\"has_code\",\"target_id\":\"github:IDEA-Research:GroundingDINO\",\"source_url\":\"https://github.com/IDEA-Research/GroundingDINO\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:moge\",\"source_url\":\"https://github.com/microsoft/moge\"},{\"type\":\"has_code\",\"target_id\":\"github:ZiYang-xie:WorldGen\",\"source_url\":\"https://github.com/ZiYang-xie/WorldGen\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2507.21809\",\"source_url\":\"https://arxiv.org/abs/2507.21809\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 82.7,
    "content_hash": "783c2f341a77cfe37ac939ce0c7cb59a",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/tencent/HunyuanWorld-1/resolve/main/assets/qrcode/discord.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/HunyuanWorld-1\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-3.1-405b-instruct",
    "name": "Llama-3.1-405B-Instruct",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "base_model:meta-llama/llama-3.1-405b",
      "base_model:finetune:meta-llama/llama-3.1-405b",
      "license:llama3.1",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 587,
    "downloads": 130351,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":405853388800,\"storage_bytes\":3035854528726,\"files_count\":584,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"chat_template\":\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|eot_id|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.05149\",\"source_url\":\"https://arxiv.org/abs/2204.05149\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.1",
    "compliance_status": "approved",
    "quality_score": 37.7,
    "content_hash": "af82932cd58a9adb6f0f2f7ef1e1ed7e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-r1-distill-qwen-14b",
    "name": "DeepSeek-R1-Distill-Qwen-14B",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/d...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 586,
    "downloads": 316093,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/resolve/main/figures/benchmark.jpg",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":14770033664,\"storage_bytes\":29540133872,\"files_count\":13,\"spaces_count\":91,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<｜begin▁of▁sentence｜>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<｜end▁of▁sentence｜>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<｜end▁of▁sentence｜>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\\\n'}}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.12948\",\"source_url\":\"https://arxiv.org/abs/2501.12948\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 97.7,
    "content_hash": "69bdf980e4334c092c4b8cb68aaa707d",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/resolve/main/figures/benchmark.jpg",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-3-12b-it",
    "name": "gemma-3-12b-it",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gemma3",
      "any-to-any",
      "image-text-to-text",
      "conversational",
      "arxiv:1905.07830",
      "arxiv:1905.10044",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1705.03551",
      "arxiv:1911.01547",
      "arxiv:1907.10641",
      "arxiv:1903.00161",
      "arxiv:2009.03300",
      "arxiv:2304.06364",
      "arxiv:2103.03874",
      "arxiv:2110.14168",
      "arxiv:2311.12022",
      "arxiv:2108.07732",
      "arxiv:2107.03374",
      "arxiv:2210.03057",
      "arxiv:2106.03193",
      "arxiv:1910.11856",
      "arxiv:2502.12404",
      "arxiv:2502.21228",
      "arxiv:2404.16816",
      "arxiv:2104.12756",
      "arxiv:2311.16502",
      "arxiv:2203.10244",
      "arxiv:2404.12390",
      "arxiv:1810.12440",
      "arxiv:1908.02660",
      "arxiv:2312.11805",
      "base_model:google/gemma-3-12b-pt",
      "base_model:finetune:google/gemma-3-12b-pt",
      "license:gemma",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 586,
    "downloads": 1583082,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-3-12b-it",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":12187325040,\"storage_bytes\":111273492196,\"files_count\":18,\"spaces_count\":76,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Gemma3ForConditionalGeneration\"],\"model_type\":\"gemma3\",\"processor_config\":{\"chat_template\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\"},\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"chat_template\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1903.00161\",\"source_url\":\"https://arxiv.org/abs/1903.00161\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.06364\",\"source_url\":\"https://arxiv.org/abs/2304.06364\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.03874\",\"source_url\":\"https://arxiv.org/abs/2103.03874\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.14168\",\"source_url\":\"https://arxiv.org/abs/2110.14168\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.12022\",\"source_url\":\"https://arxiv.org/abs/2311.12022\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.03057\",\"source_url\":\"https://arxiv.org/abs/2210.03057\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2106.03193\",\"source_url\":\"https://arxiv.org/abs/2106.03193\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.11856\",\"source_url\":\"https://arxiv.org/abs/1910.11856\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.12404\",\"source_url\":\"https://arxiv.org/abs/2502.12404\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.21228\",\"source_url\":\"https://arxiv.org/abs/2502.21228\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.16816\",\"source_url\":\"https://arxiv.org/abs/2404.16816\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.12756\",\"source_url\":\"https://arxiv.org/abs/2104.12756\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.16502\",\"source_url\":\"https://arxiv.org/abs/2311.16502\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.10244\",\"source_url\":\"https://arxiv.org/abs/2203.10244\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.12390\",\"source_url\":\"https://arxiv.org/abs/2404.12390\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1810.12440\",\"source_url\":\"https://arxiv.org/abs/1810.12440\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1908.02660\",\"source_url\":\"https://arxiv.org/abs/1908.02660\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.11805\",\"source_url\":\"https://arxiv.org/abs/2312.11805\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 37.7,
    "content_hash": "26860ff7b85e55e1fce52bbcb346e218",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-3-12b-it\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:comfy-org:flux1-dev",
    "name": "flux1-dev",
    "author": "Comfy-Org",
    "description": "--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/LICENSE.md tags: - diffusion-single-file - comfyui --- This is a smaller checkpoint for flux1-dev that will work better for ComfyUI users with less VRAM (under 24gb). The two text encoders used by Flux are already included in this one safetensor. Use it with the node in ComfyUI.",
    "tags": [
      "diffusion-single-file",
      "comfyui",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 583,
    "downloads": 368366,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Comfy-Org/flux1-dev",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/LICENSE.md\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nThis is a smaller checkpoint for flux1-dev that will work better for ComfyUI users with less VRAM (under 24gb). \n\nThe two text encoders used by Flux are already included in this one safetensor.\n\nUse it with the `Load Checkpoint` node in ComfyUI.",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"diffusion-single-file\",\"framework\":\"diffusion-single-file\",\"params\":null,\"storage_bytes\":89902171980,\"files_count\":7,\"spaces_count\":5,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 37.7,
    "content_hash": "477e732582d74ec61baf3550ceef9c78",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Comfy-Org/flux1-dev\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:intfloat:multilingual-e5-large-instruct",
    "name": "multilingual-e5-large-instruct",
    "author": "intfloat",
    "description": "--- tags: - mteb - sentence-transformers - transformers model-index: - name: multilingual-e5-large-instruct results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 76.23880597014924 - type: ap value: 39.07351965022687 - type: f1 value: 70.04836733862683 - task: type: Classification dataset: type: mteb/amazon_count...",
    "tags": [
      "sentence-transformers",
      "onnx",
      "safetensors",
      "xlm-roberta",
      "feature-extraction",
      "mteb",
      "transformers",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "arxiv:2402.05672",
      "arxiv:2401.00368",
      "arxiv:2104.08663",
      "arxiv:2210.07316",
      "license:mit",
      "model-index",
      "text-embeddings-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "feature-extraction",
    "likes": 582,
    "downloads": 1284364,
    "source": "huggingface",
    "source_url": "https://huggingface.co/intfloat/multilingual-e5-large-instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- mteb\n- sentence-transformers\n- transformers\nmodel-index:\n- name: multilingual-e5-large-instruct\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 76.23880597014924\n    - type: ap\n      value: 39.07351965022687\n    - type: f1\n      value: 70.04836733862683\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 66.71306209850107\n    - type: ap\n      value: 79.01499914759529\n    - type: f1\n      value: 64.81951817560703\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 73.85307346326837\n    - type: ap\n      value: 22.447519885878737\n    - type: f1\n      value: 61.0162730745633\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 76.04925053533191\n    - type: ap\n      value: 23.44983217128922\n    - type: f1\n      value: 62.5723230907759\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 96.28742500000001\n    - type: ap\n      value: 94.8449918887462\n    - type: f1\n      value: 96.28680923610432\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 56.716\n    - type: f1\n      value: 55.76510398266401\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 52.99999999999999\n    - type: f1\n      value: 52.00829994765178\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.806000000000004\n    - type: f1\n      value: 48.082345914983634\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.507999999999996\n    - type: f1\n      value: 47.68752844642045\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.709999999999994\n    - type: f1\n      value: 47.05870376637181\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 44.662000000000006\n    - type: f1\n      value: 43.42371965372771\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.721\n    - type: map_at_10\n      value: 49.221\n    - type: map_at_100\n      value: 49.884\n    - type: map_at_1000\n      value: 49.888\n    - type: map_at_3\n      value: 44.31\n    - type: map_at_5\n      value: 47.276\n    - type: mrr_at_1\n      value: 32.432\n    - type: mrr_at_10\n      value: 49.5\n    - type: mrr_at_100\n      value: 50.163000000000004\n    - type: mrr_at_1000\n      value: 50.166\n    - type: mrr_at_3\n      value: 44.618\n    - type: mrr_at_5\n      value: 47.541\n    - type: ndcg_at_1\n      value: 31.721\n    - type: ndcg_at_10\n      value: 58.384\n    - type: ndcg_at_100\n      value: 61.111000000000004\n    - type: ndcg_at_1000\n      value: 61.187999999999995\n    - type: ndcg_at_3\n      value: 48.386\n    - type: ndcg_at_5\n      value: 53.708999999999996\n    - type: precision_at_1\n      value: 31.721\n    - type: precision_at_10\n      value: 8.741\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 20.057\n    - type: precision_at_5\n      value: 14.609\n    - type: recall_at_1\n      value: 31.721\n    - type: recall_at_10\n      value: 87.411\n    - type: recall_at_100\n      value: 99.075\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 60.171\n    - type: recall_at_5\n      value: 73.044\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 46.40419580759799\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 40.48593255007969\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 63.889179122289995\n    - type: mrr\n      value: 77.61146286769556\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.15075203727929\n    - type: cos_sim_spearman\n      value: 86.9622224570873\n    - type: euclidean_pearson\n      value: 86.70473853624121\n    - type: euclidean_spearman\n      value: 86.9622224570873\n    - type: manhattan_pearson\n      value: 86.21089380980065\n    - type: manhattan_spearman\n      value: 86.75318154937008\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.65553235908142\n    - type: f1\n      value: 99.60681976339595\n    - type: precision\n      value: 99.58246346555325\n    - type: recall\n      value: 99.65553235908142\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26260180497468\n    - type: f1\n      value: 99.14520507740848\n    - type: precision\n      value: 99.08650671362535\n    - type: recall\n      value: 99.26260180497468\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.07412538967787\n    - type: f1\n      value: 97.86629719431936\n    - type: precision\n      value: 97.76238309664012\n    - type: recall\n      value: 98.07412538967787\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.42074776197998\n    - type: f1\n      value: 99.38564156573635\n    - type: precision\n      value: 99.36808846761454\n    - type: recall\n      value: 99.42074776197998\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 85.73376623376623\n    - type: f1\n      value: 85.68480707214599\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 40.935218072113855\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 36.276389017675264\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.764166666666668\n    - type: map_at_10\n      value: 37.298166666666674\n    - type: map_at_100\n      value: 38.530166666666666\n    - type: map_at_1000\n      value: 38.64416666666667\n    - type: map_at_3\n      value: 34.484833333333334\n    - type: map_at_5\n      value: 36.0385\n    - type: mrr_at_1\n      value: 32.93558333333333\n    - type: mrr_at_10\n      value: 41.589749999999995\n    - type: mrr_at_100\n      value: 42.425333333333334\n    - type: mrr_at_1000\n      value: 42.476333333333336\n    - type: mrr_at_3\n      value: 39.26825\n    - type: mrr_at_5\n      value: 40.567083333333336\n    - type: ndcg_at_1\n      value: 32.93558333333333\n    - type: ndcg_at_10\n      value: 42.706583333333334\n    - type: ndcg_at_100\n      value: 47.82483333333333\n    - type: ndcg_at_1000\n      value: 49.95733333333334\n    - type: ndcg_at_3\n      value: 38.064750000000004\n    - type: ndcg_at_5\n      value: 40.18158333333333\n    - type: precision_at_1\n      value: 32.93558333333333\n    - type: precision_at_10\n      value: 7.459833333333334\n    - type: precision_at_100\n      value: 1.1830833333333335\n    - type: precision_at_1000\n      value: 0.15608333333333332\n    - type: precision_at_3\n      value: 17.5235\n    - type: precision_at_5\n      value: 12.349833333333333\n    - type: recall_at_1\n      value: 27.764166666666668\n    - type: recall_at_10\n      value: 54.31775\n    - type: recall_at_100\n      value: 76.74350000000001\n    - type: recall_at_1000\n      value: 91.45208333333332\n    - type: recall_at_3\n      value: 41.23425\n    - type: recall_at_5\n      value: 46.73983333333334\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 12.969\n    - type: map_at_10\n      value: 21.584999999999997\n    - type: map_at_100\n      value: 23.3\n    - type: map_at_1000\n      value: 23.5\n    - type: map_at_3\n      value: 18.218999999999998\n    - type: map_at_5\n      value: 19.983\n    - type: mrr_at_1\n      value: 29.316\n    - type: mrr_at_10\n      value: 40.033\n    - type: mrr_at_100\n      value: 40.96\n    - type: mrr_at_1000\n      value: 41.001\n    - type: mrr_at_3\n      value: 37.123\n    - type: mrr_at_5\n      value: 38.757999999999996\n    - type: ndcg_at_1\n      value: 29.316\n    - type: ndcg_at_10\n      value: 29.858\n    - type: ndcg_at_100\n      value: 36.756\n    - type: ndcg_at_1000\n      value: 40.245999999999995\n    - type: ndcg_at_3\n      value: 24.822\n    - type: ndcg_at_5\n      value: 26.565\n    - type: precision_at_1\n      value: 29.316\n    - type: precision_at_10\n      value: 9.186\n    - type: precision_at_100\n      value: 1.6549999999999998\n    - type: precision_at_1000\n      value: 0.22999999999999998\n    - type: precision_at_3\n      value: 18.436\n    - type: precision_at_5\n      value: 13.876\n    - type: recall_at_1\n      value: 12.969\n    - type: recall_at_10\n      value: 35.142\n    - type: recall_at_100\n      value: 59.143\n    - type: recall_at_1000\n      value: 78.594\n    - type: recall_at_3\n      value: 22.604\n    - type: recall_at_5\n      value: 27.883000000000003\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.527999999999999\n    - type: map_at_10\n      value: 17.974999999999998\n    - type: map_at_100\n      value: 25.665\n    - type: map_at_1000\n      value: 27.406000000000002\n    - type: map_at_3\n      value: 13.017999999999999\n    - type: map_at_5\n      value: 15.137\n    - type: mrr_at_1\n      value: 62.5\n    - type: mrr_at_10\n      value: 71.891\n    - type: mrr_at_100\n      value: 72.294\n    - type: mrr_at_1000\n      value: 72.296\n    - type: mrr_at_3\n      value: 69.958\n    - type: mrr_at_5\n      value: 71.121\n    - type: ndcg_at_1\n      value: 50.875\n    - type: ndcg_at_10\n      value: 38.36\n    - type: ndcg_at_100\n      value: 44.235\n    - type: ndcg_at_1000\n      value: 52.154\n    - type: ndcg_at_3\n      value: 43.008\n    - type: ndcg_at_5\n      value: 40.083999999999996\n    - type: precision_at_1\n      value: 62.5\n    - type: precision_at_10\n      value: 30.0\n    - type: precision_at_100\n      value: 10.038\n    - type: precision_at_1000\n      value: 2.0869999999999997\n    - type: precision_at_3\n      value: 46.833000000000006\n    - type: precision_at_5\n      value: 38.800000000000004\n    - type: recall_at_1\n      value: 8.527999999999999\n    - type: recall_at_10\n      value: 23.828\n    - type: recall_at_100\n      value: 52.322\n    - type: recall_at_1000\n      value: 77.143\n    - type: recall_at_3\n      value: 14.136000000000001\n    - type: recall_at_5\n      value: 17.761\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 51.51\n    - type: f1\n      value: 47.632159862049896\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 60.734\n    - type: map_at_10\n      value: 72.442\n    - type: map_at_100\n      value: 72.735\n    - type: map_at_1000\n      value: 72.75\n    - type: map_at_3\n      value: 70.41199999999999\n    - type: map_at_5\n      value: 71.80499999999999\n    - type: mrr_at_1\n      value: 65.212\n    - type: mrr_at_10\n      value: 76.613\n    - type: mrr_at_100\n      value: 76.79899999999999\n    - type: mrr_at_1000\n      value: 76.801\n    - type: mrr_at_3\n      value: 74.8\n    - type: mrr_at_5\n      value: 76.12400000000001\n    - type: ndcg_at_1\n      value: 65.212\n    - type: ndcg_at_10\n      value: 77.988\n    - type: ndcg_at_100\n      value: 79.167\n    - type: ndcg_at_1000\n      value: 79.452\n    - type: ndcg_at_3\n      value: 74.362\n    - type: ndcg_at_5\n      value: 76.666\n    - type: precision_at_1\n      value: 65.212\n    - type: precision_at_10\n      value: 10.003\n    - type: precision_at_100\n      value: 1.077\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 29.518\n    - type: precision_at_5\n      value: 19.016\n    - type: recall_at_1\n      value: 60.734\n    - type: recall_at_10\n      value: 90.824\n    - type: recall_at_100\n      value: 95.71600000000001\n    - type: recall_at_1000\n      value: 97.577\n    - type: recall_at_3\n      value: 81.243\n    - type: recall_at_5\n      value: 86.90299999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.845\n    - type: map_at_10\n      value: 39.281\n    - type: map_at_100\n      value: 41.422\n    - type: map_at_1000\n      value: 41.593\n    - type: map_at_3\n      value: 34.467\n    - type: map_at_5\n      value: 37.017\n    - type: mrr_at_1\n      value: 47.531\n    - type: mrr_at_10\n      value: 56.204\n    - type: mrr_at_100\n      value: 56.928999999999995\n    - type: mrr_at_1000\n      value: 56.962999999999994\n    - type: mrr_at_3\n      value: 54.115\n    - type: mrr_at_5\n      value: 55.373000000000005\n    - type: ndcg_at_1\n      value: 47.531\n    - type: ndcg_at_10\n      value: 47.711999999999996\n    - type: ndcg_at_100\n      value: 54.510999999999996\n    - type: ndcg_at_1000\n      value: 57.103\n    - type: ndcg_at_3\n      value: 44.145\n    - type: ndcg_at_5\n      value: 45.032\n    - type: precision_at_1\n      value: 47.531\n    - type: precision_at_10\n      value: 13.194\n    - type: precision_at_100\n      value: 2.045\n    - type: precision_at_1000\n      value: 0.249\n    - type: precision_at_3\n      value: 29.424\n    - type: precision_at_5\n      value: 21.451\n    - type: recall_at_1\n      value: 23.845\n    - type: recall_at_10\n      value: 54.967\n    - type: recall_at_100\n      value: 79.11399999999999\n    - type: recall_at_1000\n      value: 94.56700000000001\n    - type: recall_at_3\n      value: 40.256\n    - type: recall_at_5\n      value: 46.215\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 37.819\n    - type: map_at_10\n      value: 60.889\n    - type: map_at_100\n      value: 61.717999999999996\n    - type: map_at_1000\n      value: 61.778\n    - type: map_at_3\n      value: 57.254000000000005\n    - type: map_at_5\n      value: 59.541\n    - type: mrr_at_1\n      value: 75.638\n    - type: mrr_at_10\n      value: 82.173\n    - type: mrr_at_100\n      value: 82.362\n    - type: mrr_at_1000\n      value: 82.37\n    - type: mrr_at_3\n      value: 81.089\n    - type: mrr_at_5\n      value: 81.827\n    - type: ndcg_at_1\n      value: 75.638\n    - type: ndcg_at_10\n      value: 69.317\n    - type: ndcg_at_100\n      value: 72.221\n    - type: ndcg_at_1000\n      value: 73.382\n    - type: ndcg_at_3\n      value: 64.14\n    - type: ndcg_at_5\n      value: 67.07600000000001\n    - type: precision_at_1\n      value: 75.638\n    - type: precision_at_10\n      value: 14.704999999999998\n    - type: precision_at_100\n      value: 1.698\n    - type: precision_at_1000\n      value: 0.185\n    - type: precision_at_3\n      value: 41.394999999999996\n    - type: precision_at_5\n      value: 27.162999999999997\n    - type: recall_at_1\n      value: 37.819\n    - type: recall_at_10\n      value: 73.52499999999999\n    - type: recall_at_100\n      value: 84.875\n    - type: recall_at_1000\n      value: 92.559\n    - type: recall_at_3\n      value: 62.092999999999996\n    - type: recall_at_5\n      value: 67.907\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 94.60079999999999\n    - type: ap\n      value: 92.67396345347356\n    - type: f1\n      value: 94.5988098167121\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 21.285\n    - type: map_at_10\n      value: 33.436\n    - type: map_at_100\n      value: 34.63\n    - type: map_at_1000\n      value: 34.681\n    - type: map_at_3\n      value: 29.412\n    - type: map_at_5\n      value: 31.715\n    - type: mrr_at_1\n      value: 21.848\n    - type: mrr_at_10\n      value: 33.979\n    - type: mrr_at_100\n      value: 35.118\n    - type: mrr_at_1000\n      value: 35.162\n    - type: mrr_at_3\n      value: 30.036\n    - type: mrr_at_5\n      value: 32.298\n    - type: ndcg_at_1\n      value: 21.862000000000002\n    - type: ndcg_at_10\n      value: 40.43\n    - type: ndcg_at_100\n      value: 46.17\n    - type: ndcg_at_1000\n      value: 47.412\n    - type: ndcg_at_3\n      value: 32.221\n    - type: ndcg_at_5\n      value: 36.332\n    - type: precision_at_1\n      value: 21.862000000000002\n    - type: precision_at_10\n      value: 6.491\n    - type: precision_at_100\n      value: 0.935\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 13.744\n    - type: precision_at_5\n      value: 10.331999999999999\n    - type: recall_at_1\n      value: 21.285\n    - type: recall_at_10\n      value: 62.083\n    - type: recall_at_100\n      value: 88.576\n    - type: recall_at_1000\n      value: 98.006\n    - type: recall_at_3\n      value: 39.729\n    - type: recall_at_5\n      value: 49.608000000000004\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.92612859097127\n    - type: f1\n      value: 93.82370333372853\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.67681036911807\n    - type: f1\n      value: 92.14191382411472\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.26817878585723\n    - type: f1\n      value: 91.92824250337878\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 89.96554963983714\n    - type: f1\n      value: 90.02859329630792\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 90.02509860164935\n    - type: f1\n      value: 89.30665159182062\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 87.55515370705244\n    - type: f1\n      value: 87.94449232331907\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 82.4623803009576\n    - type: f1\n      value: 66.06738378772725\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 79.3716539870386\n    - type: f1\n      value: 60.37614033396853\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 80.34022681787857\n    - type: f1\n      value: 58.302008026952\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 76.72095208268087\n    - type: f1\n      value: 59.64524724009049\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.87020437432773\n    - type: f1\n      value: 57.80202694670567\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.73598553345387\n    - type: f1\n      value: 58.19628250675031\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.6630800268998\n    - type: f1\n      value: 65.00996668051691\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.7128446536651\n    - type: f1\n      value: 57.95860594874963\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.61129791526563\n    - type: f1\n      value: 59.75328290206483\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.00134498991257\n    - type: f1\n      value: 67.0230483991802\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.54068594485541\n    - type: f1\n      value: 65.54604628946976\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.032952252858095\n    - type: f1\n      value: 58.715741857057104\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.80901143241427\n    - type: f1\n      value: 68.33963989243877\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.47141896435777\n    - type: f1\n      value: 69.56765020308262\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.2373907195696\n    - type: f1\n      value: 69.04529836036467\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 77.05783456624076\n    - type: f1\n      value: 74.69430584708174\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.82111634162744\n    - type: f1\n      value: 70.77228952803762\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.25353059852051\n    - type: f1\n      value: 71.05310103416411\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.28648285137861\n    - type: f1\n      value: 69.08020473732226\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.31540013449899\n    - type: f1\n      value: 70.9426355465791\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.2151983860121\n    - type: f1\n      value: 67.52541755908858\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.58372562205784\n    - type: f1\n      value: 69.49769064229827\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.9233355749832\n    - type: f1\n      value: 69.36311548259593\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.07330195023538\n    - type: f1\n      value: 64.99882022345572\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.62273032952253\n    - type: f1\n      value: 70.6394885471001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.77000672494957\n    - type: f1\n      value: 62.9368944815065\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.453261600538\n    - type: f1\n      value: 70.85069934666681\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.6906523201076\n    - type: f1\n      value: 72.03249740074217\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.03631472763953\n    - type: f1\n      value: 59.3165215571852\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.913920645595155\n    - type: f1\n      value: 57.367337711611285\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 54.42837928715535\n    - type: f1\n      value: 52.60527294970906\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.33490248823135\n    - type: f1\n      value: 63.213340969404065\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.58507061197041\n    - type: f1\n      value: 68.40256628040486\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.11230665770006\n    - type: f1\n      value: 66.44863577842305\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.70073974445192\n    - type: f1\n      value: 67.21291337273702\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.43913920645595\n    - type: f1\n      value: 64.09838087422806\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.80026899798251\n    - type: f1\n      value: 68.76986742962444\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.78816408876934\n    - type: f1\n      value: 62.18781873428972\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.6577000672495\n    - type: f1\n      value: 68.75171511133003\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.42501681237391\n    - type: f1\n      value: 71.18434963451544\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.64828513786146\n    - type: f1\n      value: 70.67741914007422\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.62811028917284\n    - type: f1\n      value: 71.36402039740959\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.88634835238736\n    - type: f1\n      value: 69.23701923480677\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.15938130464022\n    - type: f1\n      value: 71.87792218993388\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.96301277740416\n    - type: f1\n      value: 67.29584200202983\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.49562878278412\n    - type: f1\n      value: 66.91716685679431\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.6805648957633\n    - type: f1\n      value: 72.02723592594374\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.00605245460659\n    - type: f1\n      value: 60.16716669482932\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.90988567585742\n    - type: f1\n      value: 63.99405488777784\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.62273032952253\n    - type: f1\n      value: 65.17213906909481\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.50907868190988\n    - type: f1\n      value: 69.15165697194853\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.30733019502352\n    - type: f1\n      value: 66.69024007380474\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.24277067921989\n    - type: f1\n      value: 68.80515408492947\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.49831876260929\n    - type: f1\n      value: 64.83778567111116\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.28782784129119\n    - type: f1\n      value: 69.3294186700733\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.315400134499\n    - type: f1\n      value: 71.22674385243207\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.37794216543377\n    - type: f1\n      value: 68.96962492838232\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.33557498318764\n    - type: f1\n      value: 72.28949738478356\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 65.84398117014123\n    - type: f1\n      value: 64.71026362091463\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.76462676529925\n    - type: f1\n      value: 69.8229667407667\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.02420981842636\n    - type: f1\n      value: 71.76576384895898\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.7572293207801\n    - type: f1\n      value: 72.76840765295256\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.02286482851379\n    - type: f1\n      value: 66.17237947327872\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.60928043039678\n    - type: f1\n      value: 77.27094731234773\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.68325487558843\n    - type: f1\n      value: 77.97530399082261\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.13315400134498\n    - type: f1\n      value: 75.97558584796424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 80.47410894418292\n    - type: f1\n      value: 80.52244841473792\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.9670477471419\n    - type: f1\n      value: 77.37318805793146\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.09683927370544\n    - type: f1\n      value: 77.69773737430847\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.20847343644922\n    - type: f1\n      value: 75.17071738727348\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.07464694014796\n    - type: f1\n      value: 77.16136207698571\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.53396099529255\n    - type: f1\n      value: 73.58296404484122\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.75319435104237\n    - type: f1\n      value: 75.24674707850833\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.0948217888366\n    - type: f1\n      value: 76.47559490205028\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.07599193006052\n    - type: f1\n      value: 70.76028043093511\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.10490921318089\n    - type: f1\n      value: 77.01215275283272\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.25756556825824\n    - type: f1\n      value: 70.20605314648762\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.08137188971082\n    - type: f1\n      value: 77.3899269057439\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.35440484196369\n    - type: f1\n      value: 79.58964690002772\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.42299932750504\n    - type: f1\n      value: 68.07844356925413\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.15669132481507\n    - type: f1\n      value: 65.89383352608513\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 60.11432414256894\n    - type: f1\n      value: 57.69910594559806\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.24747814391392\n    - type: f1\n      value: 70.42455553830918\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.46267652992603\n    - type: f1\n      value: 76.8854559308316\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.24815063887021\n    - type: f1\n      value: 72.77805034658074\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.11566913248151\n    - type: f1\n      value: 73.86147988001356\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.0168123739072\n    - type: f1\n      value: 69.38515920054571\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.41156691324814\n    - type: f1\n      value: 73.43474953408237\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.39609952925353\n    - type: f1\n      value: 67.29731681109291\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.20914593140552\n    - type: f1\n      value: 77.07066497935367\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.52387357094821\n    - type: f1\n      value: 78.5259569473291\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.6913248150639\n    - type: f1\n      value: 76.91201656350455\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.1217215870881\n    - type: f1\n      value: 77.41179937912504\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.25891055817083\n    - type: f1\n      value: 75.8089244542887\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.70679219905851\n    - type: f1\n      value: 78.21459594517711\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.83523873570948\n    - type: f1\n      value: 74.86847028401978\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.71755211835911\n    - type: f1\n      value: 74.0214326485662\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.06523201075991\n    - type: f1\n      value: 79.10545620325138\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.91862811028918\n    - type: f1\n      value: 66.50386121217983\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.93140551445865\n    - type: f1\n      value: 70.755435928495\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.40753194351042\n    - type: f1\n      value: 71.61816115782923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.1815736381977\n    - type: f1\n      value: 75.08016717887205\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.86482851378614\n    - type: f1\n      value: 72.39521180006291\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.46940147948891\n    - type: f1\n      value: 76.70044085362349\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.89307330195024\n    - type: f1\n      value: 71.5721825332298\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.7511768661735\n    - type: f1\n      value: 75.17918654541515\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.69535978480162\n    - type: f1\n      value: 78.90019070153316\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.45729657027572\n    - type: f1\n      value: 76.19578371794672\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 36.92715354123554\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 35.53536244162518\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 33.08507884504006\n    - type: mrr\n      value: 34.32436977159129\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.935\n    - type: map_at_10\n      value: 13.297\n    - type: map_at_100\n      value: 16.907\n    - type: map_at_1000\n      value: 18.391\n    - type: map_at_3\n      value: 9.626999999999999\n    - type: map_at_5\n      value: 11.190999999999999\n    - type: mrr_at_1\n      value: 46.129999999999995\n    - type: mrr_at_10\n      value: 54.346000000000004\n    - type: mrr_at_100\n      value: 55.067\n    - type: mrr_at_1000\n      value: 55.1\n    - type: mrr_at_3\n      value: 51.961\n    - type: mrr_at_5\n      value: 53.246\n    - type: ndcg_at_1\n      value: 44.118\n    - type: ndcg_at_10\n      value: 35.534\n    - type: ndcg_at_100\n      value: 32.946999999999996\n    - type: ndcg_at_1000\n      value: 41.599000000000004\n    - type: ndcg_at_3\n      value: 40.25\n    - type: ndcg_at_5\n      value: 37.978\n    - type: precision_at_1\n      value: 46.129999999999995\n    - type: precision_at_10\n      value: 26.842\n    - type: precision_at_100\n      value: 8.427\n    - type: precision_at_1000\n      value: 2.128\n    - type: precision_at_3\n      value: 37.977\n    - type: precision_at_5\n      value: 32.879000000000005\n    - type: recall_at_1\n      value: 5.935\n    - type: recall_at_10\n      value: 17.211000000000002\n    - type: recall_at_100\n      value: 34.33\n    - type: recall_at_1000\n      value: 65.551\n    - type: recall_at_3\n      value: 10.483\n    - type: recall_at_5\n      value: 13.078999999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 35.231\n    - type: map_at_10\n      value: 50.202000000000005\n    - type: map_at_100\n      value: 51.154999999999994\n    - type: map_at_1000\n      value: 51.181\n    - type: map_at_3\n      value: 45.774\n    - type: map_at_5\n      value: 48.522\n    - type: mrr_at_1\n      value: 39.687\n    - type: mrr_at_10\n      value: 52.88\n    - type: mrr_at_100\n      value: 53.569\n    - type: mrr_at_1000\n      value: 53.58500000000001\n    - type: mrr_at_3\n      value: 49.228\n    - type: mrr_at_5\n      value: 51.525\n    - type: ndcg_at_1\n      value: 39.687\n    - type: ndcg_at_10\n      value: 57.754000000000005\n    - type: ndcg_at_100\n      value: 61.597\n    - type: ndcg_at_1000\n      value: 62.18900000000001\n    - type: ndcg_at_3\n      value: 49.55\n    - type: ndcg_at_5\n      value: 54.11899999999999\n    - type: precision_at_1\n      value: 39.687\n    - type: precision_at_10\n      value: 9.313\n    - type: precision_at_100\n      value: 1.146\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 22.229\n    - type: precision_at_5\n      value: 15.939\n    - type: recall_at_1\n      value: 35.231\n    - type: recall_at_10\n      value: 78.083\n    - type: recall_at_100\n      value: 94.42099999999999\n    - type: recall_at_1000\n      value: 98.81\n    - type: recall_at_3\n      value: 57.047000000000004\n    - type: recall_at_5\n      value: 67.637\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.241\n    - type: map_at_10\n      value: 85.462\n    - type: map_at_100\n      value: 86.083\n    - type: map_at_1000\n      value: 86.09700000000001\n    - type: map_at_3\n      value: 82.49499999999999\n    - type: map_at_5\n      value: 84.392\n    - type: mrr_at_1\n      value: 82.09\n    - type: mrr_at_10\n      value: 88.301\n    - type: mrr_at_100\n      value: 88.383\n    - type: mrr_at_1000\n      value: 88.384\n    - type: mrr_at_3\n      value: 87.37\n    - type: mrr_at_5\n      value: 88.035\n    - type: ndcg_at_1\n      value: 82.12\n    - type: ndcg_at_10\n      value: 89.149\n    - type: ndcg_at_100\n      value: 90.235\n    - type: ndcg_at_1000\n      value: 90.307\n    - type: ndcg_at_3\n      value: 86.37599999999999\n    - type: ndcg_at_5\n      value: 87.964\n    - type: precision_at_1\n      value: 82.12\n    - type: precision_at_10\n      value: 13.56\n    - type: precision_at_100\n      value: 1.539\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.88\n    - type: precision_at_5\n      value: 24.92\n    - type: recall_at_1\n      value: 71.241\n    - type: recall_at_10\n      value: 96.128\n    - type: recall_at_100\n      value: 99.696\n    - type: recall_at_1000\n      value: 99.994\n    - type: recall_at_3\n      value: 88.181\n    - type: recall_at_5\n      value: 92.694\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 56.59757799655151\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 64.27391998854624\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.243\n    - type: map_at_10\n      value: 10.965\n    - type: map_at_100\n      value: 12.934999999999999\n    - type: map_at_1000\n      value: 13.256\n    - type: map_at_3\n      value: 7.907\n    - type: map_at_5\n      value: 9.435\n    - type: mrr_at_1\n      value: 20.9\n    - type: mrr_at_10\n      value: 31.849\n    - type: mrr_at_100\n      value: 32.964\n    - type: mrr_at_1000\n      value: 33.024\n    - type: mrr_at_3\n      value: 28.517\n    - type: mrr_at_5\n      value: 30.381999999999998\n    - type: ndcg_at_1\n      value: 20.9\n    - type: ndcg_at_10\n      value: 18.723\n    - type: ndcg_at_100\n      value: 26.384999999999998\n    - type: ndcg_at_1000\n      value: 32.114\n    - type: ndcg_at_3\n      value: 17.753\n    - type: ndcg_at_5\n      value: 15.558\n    - type: precision_at_1\n      value: 20.9\n    - type: precision_at_10\n      value: 9.8\n    - type: precision_at_100\n      value: 2.078\n    - type: precision_at_1000\n      value: 0.345\n    - type: precision_at_3\n      value: 16.900000000000002\n    - type: precision_at_5\n      value: 13.88\n    - type: recall_at_1\n      value: 4.243\n    - type: recall_at_10\n      value: 19.885\n    - type: recall_at_100\n      value: 42.17\n    - type: recall_at_1000\n      value: 70.12\n    - type: recall_at_3\n      value: 10.288\n    - type: recall_at_5\n      value: 14.072000000000001\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.84209174935282\n    - type: cos_sim_spearman\n      value: 81.73248048438833\n    - type: euclidean_pearson\n      value: 83.02810070308149\n    - type: euclidean_spearman\n      value: 81.73248295679514\n    - type: manhattan_pearson\n      value: 82.95368060376002\n    - type: manhattan_spearman\n      value: 81.60277910998718\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.52628804556943\n    - type: cos_sim_spearman\n      value: 82.5713913555672\n    - type: euclidean_pearson\n      value: 85.8796774746988\n    - type: euclidean_spearman\n      value: 82.57137506803424\n    - type: manhattan_pearson\n      value: 85.79671002960058\n    - type: manhattan_spearman\n      value: 82.49445981618027\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.23682503505542\n    - type: cos_sim_spearman\n      value: 87.15008956711806\n    - type: euclidean_pearson\n      value: 86.79805401524959\n    - type: euclidean_spearman\n      value: 87.15008956711806\n    - type: manhattan_pearson\n      value: 86.65298502699244\n    - type: manhattan_spearman\n      value: 86.97677821948562\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.63370304677802\n    - type: cos_sim_spearman\n      value: 84.97105553540318\n    - type: euclidean_pearson\n      value: 85.28896108687721\n    - type: euclidean_spearman\n      value: 84.97105553540318\n    - type: manhattan_pearson\n      value: 85.09663190337331\n    - type: manhattan_spearman\n      value: 84.79126831644619\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 90.2614838800733\n    - type: cos_sim_spearman\n      value: 91.0509162991835\n    - type: euclidean_pearson\n      value: 90.33098317533373\n    - type: euclidean_spearman\n      value: 91.05091625871644\n    - type: manhattan_pearson\n      value: 90.26250435151107\n    - type: manhattan_spearman\n      value: 90.97999594417519\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.80480973335091\n    - type: cos_sim_spearman\n      value: 87.313695492969\n    - type: euclidean_pearson\n      value: 86.49267251576939\n    - type: euclidean_spearman\n      value: 87.313695492969\n    - type: manhattan_pearson\n      value: 86.44019901831935\n    - type: manhattan_spearman\n      value: 87.24205395460392\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 90.05662789380672\n    - type: cos_sim_spearman\n      value: 90.02759424426651\n    - type: euclidean_pearson\n      value: 90.4042483422981\n    - type: euclidean_spearman\n      value: 90.02759424426651\n    - type: manhattan_pearson\n      value: 90.51446975000226\n    - type: manhattan_spearman\n      value: 90.08832889933616\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 67.5975528273532\n    - type: cos_sim_spearman\n      value: 67.62969861411354\n    - type: euclidean_pearson\n      value: 69.224275734323\n    - type: euclidean_spearman\n      value: 67.62969861411354\n    - type: manhattan_pearson\n      value: 69.3761447059927\n    - type: manhattan_spearman\n      value: 67.90921005611467\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.11244327231684\n    - type: cos_sim_spearman\n      value: 88.37902438979035\n    - type: euclidean_pearson\n      value: 87.86054279847336\n    - type: euclidean_spearman\n      value: 88.37902438979035\n    - type: manhattan_pearson\n      value: 87.77257757320378\n    - type: manhattan_spearman\n      value: 88.25208966098123\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 85.87174608143563\n    - type: mrr\n      value: 96.12836872640794\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 57.760999999999996\n    - type: map_at_10\n      value: 67.258\n    - type: map_at_100\n      value: 67.757\n    - type: map_at_1000\n      value: 67.78800000000001\n    - type: map_at_3\n      value: 64.602\n    - type: map_at_5\n      value: 65.64\n    - type: mrr_at_1\n      value: 60.667\n    - type: mrr_at_10\n      value: 68.441\n    - type: mrr_at_100\n      value: 68.825\n    - type: mrr_at_1000\n      value: 68.853\n    - type: mrr_at_3\n      value: 66.444\n    - type: mrr_at_5\n      value: 67.26100000000001\n    - type: ndcg_at_1\n      value: 60.667\n    - type: ndcg_at_10\n      value: 71.852\n    - type: ndcg_at_100\n      value: 73.9\n    - type: ndcg_at_1000\n      value: 74.628\n    - type: ndcg_at_3\n      value: 67.093\n    - type: ndcg_at_5\n      value: 68.58\n    - type: precision_at_1\n      value: 60.667\n    - type: precision_at_10\n      value: 9.6\n    - type: precision_at_100\n      value: 1.0670000000000002\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 26.111\n    - type: precision_at_5\n      value: 16.733\n    - type: recall_at_1\n      value: 57.760999999999996\n    - type: recall_at_10\n      value: 84.967\n    - type: recall_at_100\n      value: 93.833\n    - type: recall_at_1000\n      value: 99.333\n    - type: recall_at_3\n      value: 71.589\n    - type: recall_at_5\n      value: 75.483\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.66633663366336\n    - type: cos_sim_ap\n      value: 91.17685358899108\n    - type: cos_sim_f1\n      value: 82.16818642350559\n    - type: cos_sim_precision\n      value: 83.26488706365504\n    - type: cos_sim_recall\n      value: 81.10000000000001\n    - type: dot_accuracy\n      value: 99.66633663366336\n    - type: dot_ap\n      value: 91.17663411119032\n    - type: dot_f1\n      value: 82.16818642350559\n    - type: dot_precision\n      value: 83.26488706365504\n    - type: dot_recall\n      value: 81.10000000000001\n    - type: euclidean_accuracy\n      value: 99.66633663366336\n    - type: euclidean_ap\n      value: 91.17685189882275\n    - type: euclidean_f1\n      value: 82.16818642350559\n    - type: euclidean_precision\n      value: 83.26488706365504\n    - type: euclidean_recall\n      value: 81.10000000000001\n    - type: manhattan_accuracy\n      value: 99.66633663366336\n    - type: manhattan_ap\n      value: 91.2241619496737\n    - type: manhattan_f1\n      value: 82.20472440944883\n    - type: manhattan_precision\n      value: 86.51933701657458\n    - type: manhattan_recall\n      value: 78.3\n    - type: max_accuracy\n      value: 99.66633663366336\n    - type: max_ap\n      value: 91.2241619496737\n    - type: max_f1\n      value: 82.20472440944883\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 66.85101268897951\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 42.461184054706905\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 51.44542568873886\n    - type: mrr\n      value: 52.33656151854681\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 30.75982974997539\n    - type: cos_sim_spearman\n      value: 30.385405026539914\n    - type: dot_pearson\n      value: 30.75982433546523\n    - type: dot_spearman\n      value: 30.385405026539914\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.22799999999999998\n    - type: map_at_10\n      value: 2.064\n    - type: map_at_100\n      value: 13.056000000000001\n    - type: map_at_1000\n      value: 31.747999999999998\n    - type: map_at_3\n      value: 0.67\n    - type: map_at_5\n      value: 1.097\n    - type: mrr_at_1\n      value: 90.0\n    - type: mrr_at_10\n      value: 94.667\n    - type: mrr_at_100\n      value: 94.667\n    - type: mrr_at_1000\n      value: 94.667\n    - type: mrr_at_3\n      value: 94.667\n    - type: mrr_at_5\n      value: 94.667\n    - type: ndcg_at_1\n      value: 86.0\n    - type: ndcg_at_10\n      value: 82.0\n    - type: ndcg_at_100\n      value: 64.307\n    - type: ndcg_at_1000\n      value: 57.023999999999994\n    - type: ndcg_at_3\n      value: 85.816\n    - type: ndcg_at_5\n      value: 84.904\n    - type: precision_at_1\n      value: 90.0\n    - type: precision_at_10\n      value: 85.8\n    - type: precision_at_100\n      value: 66.46\n    - type: precision_at_1000\n      value: 25.202\n    - type: precision_at_3\n      value: 90.0\n    - type: precision_at_5\n      value: 89.2\n    - type: recall_at_1\n      value: 0.22799999999999998\n    - type: recall_at_10\n      value: 2.235\n    - type: recall_at_100\n      value: 16.185\n    - type: recall_at_1000\n      value: 53.620999999999995\n    - type: recall_at_3\n      value: 0.7040000000000001\n    - type: recall_at_5\n      value: 1.172\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (sqi-eng)\n      config: sqi-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.39999999999999\n    - type: f1\n      value: 96.75\n    - type: precision\n      value: 96.45\n    - type: recall\n      value: 97.39999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (fry-eng)\n      config: fry-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 85.54913294797689\n    - type: f1\n      value: 82.46628131021194\n    - type: precision\n      value: 81.1175337186898\n    - type: recall\n      value: 85.54913294797689\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kur-eng)\n      config: kur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 81.21951219512195\n    - type: f1\n      value: 77.33333333333334\n    - type: precision\n      value: 75.54878048780488\n    - type: recall\n      value: 81.21951219512195\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tur-eng)\n      config: tur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 98.6\n    - type: f1\n      value: 98.26666666666665\n    - type: precision\n      value: 98.1\n    - type: recall\n      value: 98.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (deu-eng)\n      config: deu-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 99.5\n    - type: f1\n      value: 99.33333333333333\n    - type: precision\n      value: 99.25\n    - type: recall\n      value: 99.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nld-eng)\n      config: nld-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.8\n    - type: f1\n      value: 97.2\n    - type: precision\n      value: 96.89999999999999\n    - type: recall\n      value: 97.8\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ron-eng)\n      config: ron-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.8\n    - type: f1\n      value: 97.18333333333334\n    - type: precision\n      value: 96.88333333333333\n    - type: recall\n      value: 97.8\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ang-eng)\n      config: ang-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 77.61194029850746\n    - type: f1\n      value: 72.81094527363183\n    - type: precision\n      value: 70.83333333333333\n    - type: recall\n      value: 77.61194029850746\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ido-eng)\n      config: ido-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.7\n    - type: f1\n      value: 91.91666666666667\n    - type: precision\n      value: 91.08333333333334\n    - type: recall\n      value: 93.7\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (jav-eng)\n      config: jav-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 88.29268292682927\n    - type: f1\n      value: 85.27642276422765\n    - type: precision\n      value: 84.01277584204414\n    - type: recall\n      value: 88.29268292682927\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (isl-eng)\n      config: isl-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.1\n    - type: f1\n      value: 95.0\n    - type: precision\n      value: 94.46666666666668\n    - type: recall\n      value: 96.1\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (slv-eng)\n      config: slv-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.681652490887\n    - type: f1\n      value: 91.90765492102065\n    - type: precision\n      value: 91.05913325232888\n    - type: recall\n      value: 93.681652490887\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (cym-eng)\n      config: cym-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 92.17391304347827\n    - type: f1\n      value: 89.97101449275361\n    - type: precision\n      value: 88.96811594202899\n    - type: recall\n      value: 92.17391304347827\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kaz-eng)\n      config: kaz-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 90.43478260869566\n    - type: f1\n      value: 87.72173913043478\n    - type: precision\n      value: 86.42028985507245\n    - type: recall\n      value: 90.43478260869566\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (est-eng)\n      config: est-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 90.4\n    - type: f1\n      value: 88.03\n    - type: precision\n      value: 86.95\n    - type: recall\n      value: 90.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (heb-eng)\n      config: heb-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.4\n    - type: f1\n      value: 91.45666666666666\n    - type: precision\n      value: 90.525\n    - type: recall\n      value: 93.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (gla-eng)\n      config: gla-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 81.9059107358263\n    - type: f1\n      value: 78.32557872364869\n    - type: precision\n      value: 76.78260286824823\n    - type: recall\n      value: 81.9059107358263\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (mar-eng)\n      config: mar-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 94.3\n    - type: f1\n      value: 92.58333333333333\n    - type: precision\n      value: 91.73333333333332\n    - type: recall\n      value: 94.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lat-eng)\n      config: lat-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 79.10000000000001\n    - type: f1\n      value: 74.50500000000001\n    - type: precision\n      value: 72.58928571428571\n    - type: recall\n      value: 79.10000000000001\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (bel-eng)\n      config: bel-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.6\n    - type: f1\n      value: 95.55\n    - type: precision\n      value: 95.05\n    - type: recall\n      value: 96.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (pms-eng)\n      config: pms-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 82.0952380952381\n    - type: f1\n      value: 77.98458049886621\n    - type: precision\n      value: 76.1968253968254\n    - type: recall\n      value: 82.0952380952381\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (gle-eng)\n      config: gle-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 87.9\n    - type: f1\n      value: 84.99190476190476\n    - type: precision\n      value: 83.65\n    - type: recall\n      value: 87.9\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (pes-eng)\n      config: pes-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.7\n    - type: f1\n      value: 94.56666666666666\n    - type: precision\n      value: 94.01666666666667\n    - type: recall\n      value: 95.7\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nob-eng)\n      config: nob-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 98.6\n    - type: f1\n      value: 98.2\n    - type: precision\n      value: 98.0\n    - type: recall\n      value: 98.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (bul-eng)\n      config: bul-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.6\n    - type: f1\n      value: 94.38333333333334\n    - type: precision\n      value: 93.78333333333335\n    - type: recall\n      value: 95.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (cbk-eng)\n      config: cbk-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 87.4\n    - type: f1\n      value: 84.10380952380952\n    - type: precision\n      value: 82.67\n    - type: recall\n      value: 87.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (hun-eng)\n      config: hun-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.5\n    - type: f1\n      value: 94.33333333333334\n    - type: precision\n      value: 93.78333333333333\n    - type: recall\n      value: 95.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (uig-eng)\n      config: uig-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 89.4\n    - type: f1\n      value: 86.82000000000001\n    - type: precision\n      value: 85.64500000000001\n    - type: recall\n      value: 89.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (rus-eng)\n      config: rus-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.1\n    - type: f1\n      value: 93.56666666666668\n    - type: precision\n      value: 92.81666666666666\n    - type: recall\n      value: 95.1\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (spa-eng)\n      config: spa-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 98.9\n    - type: f1\n      value: 98.6\n    - type: precision\n      value: 98.45\n    - type: recall\n      value: 98.9\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (hye-eng)\n      config: hye-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.01347708894879\n    - type: f1\n      value: 93.51752021563343\n    - type: precision\n      value: 92.82794249775381\n    - type: recall\n      value: 95.01347708894879\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tel-eng)\n      config: tel-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.00854700854701\n    - type: f1\n      value: 96.08262108262107\n    - type: precision\n      value: 95.65527065527067\n    - type: recall\n      value: 97.00854700854701\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (afr-eng)\n      config: afr-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.5\n    - type: f1\n      value: 95.39999999999999\n    - type: precision\n      value: 94.88333333333333\n    - type: recall\n      value: 96.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (mon-eng)\n      config: mon-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.5909090909091\n    - type: f1\n      value: 95.49242424242425\n    - type: precision\n      value: 94.9621212121212\n    - type: recall\n      value: 96.5909090909091\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (arz-eng)\n      config: arz-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 84.90566037735849\n    - type: f1\n      value: 81.85883997204752\n    - type: precision\n      value: 80.54507337526205\n    - type: recall\n      value: 84.90566037735849\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (hrv-eng)\n      config: hrv-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.5\n    - type: f1\n      value: 96.75\n    - type: precision\n      value: 96.38333333333333\n    - type: recall\n      value: 97.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nov-eng)\n      config: nov-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 86.7704280155642\n    - type: f1\n      value: 82.99610894941635\n    - type: precision\n      value: 81.32295719844358\n    - type: recall\n      value: 86.7704280155642\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (gsw-eng)\n      config: gsw-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 67.52136752136752\n    - type: f1\n      value: 61.89662189662191\n    - type: precision\n      value: 59.68660968660969\n    - type: recall\n      value: 67.52136752136752\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nds-eng)\n      config: nds-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 89.2\n    - type: f1\n      value: 86.32\n    - type: precision\n      value: 85.015\n    - type: recall\n      value: 89.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ukr-eng)\n      config: ukr-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.0\n    - type: f1\n      value: 94.78333333333333\n    - type: precision\n      value: 94.18333333333334\n    - type: recall\n      value: 96.0\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (uzb-eng)\n      config: uzb-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 83.8785046728972\n    - type: f1\n      value: 80.54517133956385\n    - type: precision\n      value: 79.154984423676\n    - type: recall\n      value: 83.8785046728972\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lit-eng)\n      config: lit-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.60000000000001\n    - type: f1\n      value: 92.01333333333334\n    - type: precision\n      value: 91.28333333333333\n    - type: recall\n      value: 93.60000000000001\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ina-eng)\n      config: ina-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.1\n    - type: f1\n      value: 96.26666666666667\n    - type: precision\n      value: 95.85000000000001\n    - type: recall\n      value: 97.1\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lfn-eng)\n      config: lfn-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 84.3\n    - type: f1\n      value: 80.67833333333333\n    - type: precision\n      value: 79.03928571428571\n    - type: recall\n      value: 84.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (zsm-eng)\n      config: zsm-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.3\n    - type: f1\n      value: 96.48333333333332\n    - type: precision\n      value: 96.08333333333331\n    - type: recall\n      value: 97.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ita-eng)\n      config: ita-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.7\n    - type: f1\n      value: 94.66666666666667\n    - type: precision\n      value: 94.16666666666667\n    - type: recall\n      value: 95.7\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (cmn-eng)\n      config: cmn-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.2\n    - type: f1\n      value: 96.36666666666667\n    - type: precision\n      value: 95.96666666666668\n    - type: recall\n      value: 97.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lvs-eng)\n      config: lvs-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 94.3\n    - type: f1\n      value: 92.80666666666667\n    - type: precision\n      value: 92.12833333333333\n    - type: recall\n      value: 94.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (glg-eng)\n      config: glg-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.0\n    - type: f1\n      value: 96.22333333333334\n    - type: precision\n      value: 95.875\n    - type: recall\n      value: 97.0\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ceb-eng)\n\n[Content truncated...]",
    "meta_json": "{\"pipeline_tag\":\"feature-extraction\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":559890432,\"storage_bytes\":8885644361,\"files_count\":19,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"XLMRobertaModel\"],\"model_type\":\"xlm-roberta\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"cls_token\":\"<s>\",\"eos_token\":\"</s>\",\"mask_token\":\"<mask>\",\"pad_token\":\"<pad>\",\"sep_token\":\"</s>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2402.05672\",\"source_url\":\"https://arxiv.org/abs/2402.05672\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2401.00368\",\"source_url\":\"https://arxiv.org/abs/2401.00368\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.08663\",\"source_url\":\"https://arxiv.org/abs/2104.08663\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.07316\",\"source_url\":\"https://arxiv.org/abs/2210.07316\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 77.7,
    "content_hash": "fa56d9053fb6495cc5b4aec326b0ef0f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/intfloat/multilingual-e5-large-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:hkunlp:instructor-xl",
    "name": "instructor-xl",
    "author": "hkunlp",
    "description": "--- pipeline_tag: sentence-similarity tags: - text-embedding - embeddings - information-retrieval - beir - text-classification - language-model - text-clustering - text-semantic-similarity - text-evaluation - prompt-retrieval - text-reranking - sentence-transformers - feature-extraction - sentence-similarity - transformers - t5 - English - Sentence Similarity - natural_questions - ms_marco - fever - hotpot_qa - mteb language: en inference: false license: apache-2.0 model-index: - name: final_...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "t5",
      "text-embedding",
      "embeddings",
      "information-retrieval",
      "beir",
      "text-classification",
      "language-model",
      "text-clustering",
      "text-semantic-similarity",
      "text-evaluation",
      "prompt-retrieval",
      "text-reranking",
      "feature-extraction",
      "sentence-similarity",
      "transformers",
      "english",
      "sentence similarity",
      "natural_questions",
      "ms_marco",
      "fever",
      "hotpot_qa",
      "mteb",
      "en",
      "arxiv:2212.09741",
      "license:apache-2.0",
      "model-index",
      "text-generation-inference",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "sentence-similarity",
    "likes": 581,
    "downloads": 461707,
    "source": "huggingface",
    "source_url": "https://huggingface.co/hkunlp/instructor-xl",
    "image_url": null,
    "type": "model",
    "body_content": "---\npipeline_tag: sentence-similarity\ntags:\n- text-embedding\n- embeddings\n- information-retrieval\n- beir\n- text-classification\n- language-model\n- text-clustering\n- text-semantic-similarity\n- text-evaluation\n- prompt-retrieval\n- text-reranking\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- t5\n- English\n- Sentence Similarity\n- natural_questions\n- ms_marco\n- fever\n- hotpot_qa\n- mteb\nlanguage: en\ninference: false\nlicense: apache-2.0\nmodel-index:\n- name: final_xl_results\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 85.08955223880596\n    - type: ap\n      value: 52.66066378722476\n    - type: f1\n      value: 79.63340218960269\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 86.542\n    - type: ap\n      value: 81.92695193008987\n    - type: f1\n      value: 86.51466132573681\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 42.964\n    - type: f1\n      value: 41.43146249774862\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.872\n    - type: map_at_10\n      value: 46.342\n    - type: map_at_100\n      value: 47.152\n    - type: map_at_1000\n      value: 47.154\n    - type: map_at_3\n      value: 41.216\n    - type: map_at_5\n      value: 44.035999999999994\n    - type: mrr_at_1\n      value: 30.939\n    - type: mrr_at_10\n      value: 46.756\n    - type: mrr_at_100\n      value: 47.573\n    - type: mrr_at_1000\n      value: 47.575\n    - type: mrr_at_3\n      value: 41.548\n    - type: mrr_at_5\n      value: 44.425\n    - type: ndcg_at_1\n      value: 29.872\n    - type: ndcg_at_10\n      value: 55.65\n    - type: ndcg_at_100\n      value: 58.88099999999999\n    - type: ndcg_at_1000\n      value: 58.951\n    - type: ndcg_at_3\n      value: 45.0\n    - type: ndcg_at_5\n      value: 50.09\n    - type: precision_at_1\n      value: 29.872\n    - type: precision_at_10\n      value: 8.549\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 18.658\n    - type: precision_at_5\n      value: 13.669999999999998\n    - type: recall_at_1\n      value: 29.872\n    - type: recall_at_10\n      value: 85.491\n    - type: recall_at_100\n      value: 99.075\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 55.974000000000004\n    - type: recall_at_5\n      value: 68.35\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 42.452729850641276\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 32.21141846480423\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 65.34710928952622\n    - type: mrr\n      value: 77.61124301983028\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_spearman\n      value: 84.15312230525639\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 82.66233766233766\n    - type: f1\n      value: 82.04175284777669\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 37.36697339826455\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 30.551241447593092\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 36.797000000000004\n    - type: map_at_10\n      value: 48.46\n    - type: map_at_100\n      value: 49.968\n    - type: map_at_1000\n      value: 50.080000000000005\n    - type: map_at_3\n      value: 44.71\n    - type: map_at_5\n      value: 46.592\n    - type: mrr_at_1\n      value: 45.494\n    - type: mrr_at_10\n      value: 54.747\n    - type: mrr_at_100\n      value: 55.43599999999999\n    - type: mrr_at_1000\n      value: 55.464999999999996\n    - type: mrr_at_3\n      value: 52.361000000000004\n    - type: mrr_at_5\n      value: 53.727000000000004\n    - type: ndcg_at_1\n      value: 45.494\n    - type: ndcg_at_10\n      value: 54.989\n    - type: ndcg_at_100\n      value: 60.096000000000004\n    - type: ndcg_at_1000\n      value: 61.58\n    - type: ndcg_at_3\n      value: 49.977\n    - type: ndcg_at_5\n      value: 51.964999999999996\n    - type: precision_at_1\n      value: 45.494\n    - type: precision_at_10\n      value: 10.558\n    - type: precision_at_100\n      value: 1.6049999999999998\n    - type: precision_at_1000\n      value: 0.203\n    - type: precision_at_3\n      value: 23.796\n    - type: precision_at_5\n      value: 16.881\n    - type: recall_at_1\n      value: 36.797000000000004\n    - type: recall_at_10\n      value: 66.83\n    - type: recall_at_100\n      value: 88.34100000000001\n    - type: recall_at_1000\n      value: 97.202\n    - type: recall_at_3\n      value: 51.961999999999996\n    - type: recall_at_5\n      value: 57.940000000000005\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.597\n    - type: map_at_10\n      value: 43.424\n    - type: map_at_100\n      value: 44.78\n    - type: map_at_1000\n      value: 44.913\n    - type: map_at_3\n      value: 40.315\n    - type: map_at_5\n      value: 41.987\n    - type: mrr_at_1\n      value: 40.382\n    - type: mrr_at_10\n      value: 49.219\n    - type: mrr_at_100\n      value: 49.895\n    - type: mrr_at_1000\n      value: 49.936\n    - type: mrr_at_3\n      value: 46.996\n    - type: mrr_at_5\n      value: 48.231\n    - type: ndcg_at_1\n      value: 40.382\n    - type: ndcg_at_10\n      value: 49.318\n    - type: ndcg_at_100\n      value: 53.839999999999996\n    - type: ndcg_at_1000\n      value: 55.82899999999999\n    - type: ndcg_at_3\n      value: 44.914\n    - type: ndcg_at_5\n      value: 46.798\n    - type: precision_at_1\n      value: 40.382\n    - type: precision_at_10\n      value: 9.274000000000001\n    - type: precision_at_100\n      value: 1.497\n    - type: precision_at_1000\n      value: 0.198\n    - type: precision_at_3\n      value: 21.592\n    - type: precision_at_5\n      value: 15.159\n    - type: recall_at_1\n      value: 32.597\n    - type: recall_at_10\n      value: 59.882000000000005\n    - type: recall_at_100\n      value: 78.446\n    - type: recall_at_1000\n      value: 90.88000000000001\n    - type: recall_at_3\n      value: 46.9\n    - type: recall_at_5\n      value: 52.222\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 43.8\n    - type: map_at_10\n      value: 57.293000000000006\n    - type: map_at_100\n      value: 58.321\n    - type: map_at_1000\n      value: 58.361\n    - type: map_at_3\n      value: 53.839999999999996\n    - type: map_at_5\n      value: 55.838\n    - type: mrr_at_1\n      value: 49.592000000000006\n    - type: mrr_at_10\n      value: 60.643\n    - type: mrr_at_100\n      value: 61.23499999999999\n    - type: mrr_at_1000\n      value: 61.251999999999995\n    - type: mrr_at_3\n      value: 58.265\n    - type: mrr_at_5\n      value: 59.717\n    - type: ndcg_at_1\n      value: 49.592000000000006\n    - type: ndcg_at_10\n      value: 63.364\n    - type: ndcg_at_100\n      value: 67.167\n    - type: ndcg_at_1000\n      value: 67.867\n    - type: ndcg_at_3\n      value: 57.912\n    - type: ndcg_at_5\n      value: 60.697\n    - type: precision_at_1\n      value: 49.592000000000006\n    - type: precision_at_10\n      value: 10.088\n    - type: precision_at_100\n      value: 1.2930000000000001\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 25.789\n    - type: precision_at_5\n      value: 17.541999999999998\n    - type: recall_at_1\n      value: 43.8\n    - type: recall_at_10\n      value: 77.635\n    - type: recall_at_100\n      value: 93.748\n    - type: recall_at_1000\n      value: 98.468\n    - type: recall_at_3\n      value: 63.223\n    - type: recall_at_5\n      value: 70.122\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.721\n    - type: map_at_10\n      value: 35.626999999999995\n    - type: map_at_100\n      value: 36.719\n    - type: map_at_1000\n      value: 36.8\n    - type: map_at_3\n      value: 32.781\n    - type: map_at_5\n      value: 34.333999999999996\n    - type: mrr_at_1\n      value: 29.604999999999997\n    - type: mrr_at_10\n      value: 37.564\n    - type: mrr_at_100\n      value: 38.505\n    - type: mrr_at_1000\n      value: 38.565\n    - type: mrr_at_3\n      value: 34.727000000000004\n    - type: mrr_at_5\n      value: 36.207\n    - type: ndcg_at_1\n      value: 29.604999999999997\n    - type: ndcg_at_10\n      value: 40.575\n    - type: ndcg_at_100\n      value: 45.613\n    - type: ndcg_at_1000\n      value: 47.676\n    - type: ndcg_at_3\n      value: 34.811\n    - type: ndcg_at_5\n      value: 37.491\n    - type: precision_at_1\n      value: 29.604999999999997\n    - type: precision_at_10\n      value: 6.1690000000000005\n    - type: precision_at_100\n      value: 0.906\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 14.237\n    - type: precision_at_5\n      value: 10.056\n    - type: recall_at_1\n      value: 27.721\n    - type: recall_at_10\n      value: 54.041\n    - type: recall_at_100\n      value: 76.62299999999999\n    - type: recall_at_1000\n      value: 92.134\n    - type: recall_at_3\n      value: 38.582\n    - type: recall_at_5\n      value: 44.989000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.553\n    - type: map_at_10\n      value: 25.384\n    - type: map_at_100\n      value: 26.655\n    - type: map_at_1000\n      value: 26.778000000000002\n    - type: map_at_3\n      value: 22.733\n    - type: map_at_5\n      value: 24.119\n    - type: mrr_at_1\n      value: 20.149\n    - type: mrr_at_10\n      value: 29.705\n    - type: mrr_at_100\n      value: 30.672\n    - type: mrr_at_1000\n      value: 30.737\n    - type: mrr_at_3\n      value: 27.032\n    - type: mrr_at_5\n      value: 28.369\n    - type: ndcg_at_1\n      value: 20.149\n    - type: ndcg_at_10\n      value: 30.843999999999998\n    - type: ndcg_at_100\n      value: 36.716\n    - type: ndcg_at_1000\n      value: 39.495000000000005\n    - type: ndcg_at_3\n      value: 25.918999999999997\n    - type: ndcg_at_5\n      value: 27.992\n    - type: precision_at_1\n      value: 20.149\n    - type: precision_at_10\n      value: 5.858\n    - type: precision_at_100\n      value: 1.009\n    - type: precision_at_1000\n      value: 0.13799999999999998\n    - type: precision_at_3\n      value: 12.645000000000001\n    - type: precision_at_5\n      value: 9.179\n    - type: recall_at_1\n      value: 16.553\n    - type: recall_at_10\n      value: 43.136\n    - type: recall_at_100\n      value: 68.562\n    - type: recall_at_1000\n      value: 88.208\n    - type: recall_at_3\n      value: 29.493000000000002\n    - type: recall_at_5\n      value: 34.751\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.000999999999998\n    - type: map_at_10\n      value: 39.004\n    - type: map_at_100\n      value: 40.461999999999996\n    - type: map_at_1000\n      value: 40.566\n    - type: map_at_3\n      value: 35.805\n    - type: map_at_5\n      value: 37.672\n    - type: mrr_at_1\n      value: 33.782000000000004\n    - type: mrr_at_10\n      value: 44.702\n    - type: mrr_at_100\n      value: 45.528\n    - type: mrr_at_1000\n      value: 45.576\n    - type: mrr_at_3\n      value: 42.14\n    - type: mrr_at_5\n      value: 43.651\n    - type: ndcg_at_1\n      value: 33.782000000000004\n    - type: ndcg_at_10\n      value: 45.275999999999996\n    - type: ndcg_at_100\n      value: 50.888\n    - type: ndcg_at_1000\n      value: 52.879\n    - type: ndcg_at_3\n      value: 40.191\n    - type: ndcg_at_5\n      value: 42.731\n    - type: precision_at_1\n      value: 33.782000000000004\n    - type: precision_at_10\n      value: 8.200000000000001\n    - type: precision_at_100\n      value: 1.287\n    - type: precision_at_1000\n      value: 0.16199999999999998\n    - type: precision_at_3\n      value: 19.185\n    - type: precision_at_5\n      value: 13.667000000000002\n    - type: recall_at_1\n      value: 28.000999999999998\n    - type: recall_at_10\n      value: 58.131\n    - type: recall_at_100\n      value: 80.869\n    - type: recall_at_1000\n      value: 93.931\n    - type: recall_at_3\n      value: 44.161\n    - type: recall_at_5\n      value: 50.592000000000006\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.047\n    - type: map_at_10\n      value: 38.596000000000004\n    - type: map_at_100\n      value: 40.116\n    - type: map_at_1000\n      value: 40.232\n    - type: map_at_3\n      value: 35.205\n    - type: map_at_5\n      value: 37.076\n    - type: mrr_at_1\n      value: 34.932\n    - type: mrr_at_10\n      value: 44.496\n    - type: mrr_at_100\n      value: 45.47\n    - type: mrr_at_1000\n      value: 45.519999999999996\n    - type: mrr_at_3\n      value: 41.743\n    - type: mrr_at_5\n      value: 43.352000000000004\n    - type: ndcg_at_1\n      value: 34.932\n    - type: ndcg_at_10\n      value: 44.901\n    - type: ndcg_at_100\n      value: 50.788999999999994\n    - type: ndcg_at_1000\n      value: 52.867\n    - type: ndcg_at_3\n      value: 39.449\n    - type: ndcg_at_5\n      value: 41.929\n    - type: precision_at_1\n      value: 34.932\n    - type: precision_at_10\n      value: 8.311\n    - type: precision_at_100\n      value: 1.3050000000000002\n    - type: precision_at_1000\n      value: 0.166\n    - type: precision_at_3\n      value: 18.836\n    - type: precision_at_5\n      value: 13.447000000000001\n    - type: recall_at_1\n      value: 28.047\n    - type: recall_at_10\n      value: 57.717\n    - type: recall_at_100\n      value: 82.182\n    - type: recall_at_1000\n      value: 95.82000000000001\n    - type: recall_at_3\n      value: 42.448\n    - type: recall_at_5\n      value: 49.071\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.861250000000005\n    - type: map_at_10\n      value: 37.529583333333335\n    - type: map_at_100\n      value: 38.7915\n    - type: map_at_1000\n      value: 38.90558333333335\n    - type: map_at_3\n      value: 34.57333333333333\n    - type: map_at_5\n      value: 36.187166666666656\n    - type: mrr_at_1\n      value: 32.88291666666666\n    - type: mrr_at_10\n      value: 41.79750000000001\n    - type: mrr_at_100\n      value: 42.63183333333333\n    - type: mrr_at_1000\n      value: 42.68483333333333\n    - type: mrr_at_3\n      value: 39.313750000000006\n    - type: mrr_at_5\n      value: 40.70483333333333\n    - type: ndcg_at_1\n      value: 32.88291666666666\n    - type: ndcg_at_10\n      value: 43.09408333333333\n    - type: ndcg_at_100\n      value: 48.22158333333333\n    - type: ndcg_at_1000\n      value: 50.358000000000004\n    - type: ndcg_at_3\n      value: 38.129583333333336\n    - type: ndcg_at_5\n      value: 40.39266666666666\n    - type: precision_at_1\n      value: 32.88291666666666\n    - type: precision_at_10\n      value: 7.5584999999999996\n    - type: precision_at_100\n      value: 1.1903333333333332\n    - type: precision_at_1000\n      value: 0.15658333333333332\n    - type: precision_at_3\n      value: 17.495916666666666\n    - type: precision_at_5\n      value: 12.373833333333332\n    - type: recall_at_1\n      value: 27.861250000000005\n    - type: recall_at_10\n      value: 55.215916666666665\n    - type: recall_at_100\n      value: 77.392\n    - type: recall_at_1000\n      value: 92.04908333333334\n    - type: recall_at_3\n      value: 41.37475\n    - type: recall_at_5\n      value: 47.22908333333333\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.064999999999998\n    - type: map_at_10\n      value: 31.635999999999996\n    - type: map_at_100\n      value: 32.596000000000004\n    - type: map_at_1000\n      value: 32.695\n    - type: map_at_3\n      value: 29.612\n    - type: map_at_5\n      value: 30.768\n    - type: mrr_at_1\n      value: 28.528\n    - type: mrr_at_10\n      value: 34.717\n    - type: mrr_at_100\n      value: 35.558\n    - type: mrr_at_1000\n      value: 35.626000000000005\n    - type: mrr_at_3\n      value: 32.745000000000005\n    - type: mrr_at_5\n      value: 33.819\n    - type: ndcg_at_1\n      value: 28.528\n    - type: ndcg_at_10\n      value: 35.647\n    - type: ndcg_at_100\n      value: 40.207\n    - type: ndcg_at_1000\n      value: 42.695\n    - type: ndcg_at_3\n      value: 31.878\n    - type: ndcg_at_5\n      value: 33.634\n    - type: precision_at_1\n      value: 28.528\n    - type: precision_at_10\n      value: 5.46\n    - type: precision_at_100\n      value: 0.84\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 13.547999999999998\n    - type: precision_at_5\n      value: 9.325\n    - type: recall_at_1\n      value: 25.064999999999998\n    - type: recall_at_10\n      value: 45.096000000000004\n    - type: recall_at_100\n      value: 65.658\n    - type: recall_at_1000\n      value: 84.128\n    - type: recall_at_3\n      value: 34.337\n    - type: recall_at_5\n      value: 38.849000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.276\n    - type: map_at_10\n      value: 24.535\n    - type: map_at_100\n      value: 25.655\n    - type: map_at_1000\n      value: 25.782\n    - type: map_at_3\n      value: 22.228\n    - type: map_at_5\n      value: 23.612\n    - type: mrr_at_1\n      value: 21.266\n    - type: mrr_at_10\n      value: 28.474\n    - type: mrr_at_100\n      value: 29.398000000000003\n    - type: mrr_at_1000\n      value: 29.482000000000003\n    - type: mrr_at_3\n      value: 26.245\n    - type: mrr_at_5\n      value: 27.624\n    - type: ndcg_at_1\n      value: 21.266\n    - type: ndcg_at_10\n      value: 29.087000000000003\n    - type: ndcg_at_100\n      value: 34.374\n    - type: ndcg_at_1000\n      value: 37.433\n    - type: ndcg_at_3\n      value: 25.040000000000003\n    - type: ndcg_at_5\n      value: 27.116\n    - type: precision_at_1\n      value: 21.266\n    - type: precision_at_10\n      value: 5.258\n    - type: precision_at_100\n      value: 0.9299999999999999\n    - type: precision_at_1000\n      value: 0.13699999999999998\n    - type: precision_at_3\n      value: 11.849\n    - type: precision_at_5\n      value: 8.699\n    - type: recall_at_1\n      value: 17.276\n    - type: recall_at_10\n      value: 38.928000000000004\n    - type: recall_at_100\n      value: 62.529\n    - type: recall_at_1000\n      value: 84.44800000000001\n    - type: recall_at_3\n      value: 27.554000000000002\n    - type: recall_at_5\n      value: 32.915\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.297\n    - type: map_at_10\n      value: 36.957\n    - type: map_at_100\n      value: 38.252\n    - type: map_at_1000\n      value: 38.356\n    - type: map_at_3\n      value: 34.121\n    - type: map_at_5\n      value: 35.782000000000004\n    - type: mrr_at_1\n      value: 32.275999999999996\n    - type: mrr_at_10\n      value: 41.198\n    - type: mrr_at_100\n      value: 42.131\n    - type: mrr_at_1000\n      value: 42.186\n    - type: mrr_at_3\n      value: 38.557\n    - type: mrr_at_5\n      value: 40.12\n    - type: ndcg_at_1\n      value: 32.275999999999996\n    - type: ndcg_at_10\n      value: 42.516\n    - type: ndcg_at_100\n      value: 48.15\n    - type: ndcg_at_1000\n      value: 50.344\n    - type: ndcg_at_3\n      value: 37.423\n    - type: ndcg_at_5\n      value: 39.919\n    - type: precision_at_1\n      value: 32.275999999999996\n    - type: precision_at_10\n      value: 7.155\n    - type: precision_at_100\n      value: 1.123\n    - type: precision_at_1000\n      value: 0.14200000000000002\n    - type: precision_at_3\n      value: 17.163999999999998\n    - type: precision_at_5\n      value: 12.127\n    - type: recall_at_1\n      value: 27.297\n    - type: recall_at_10\n      value: 55.238\n    - type: recall_at_100\n      value: 79.2\n    - type: recall_at_1000\n      value: 94.258\n    - type: recall_at_3\n      value: 41.327000000000005\n    - type: recall_at_5\n      value: 47.588\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.142000000000003\n    - type: map_at_10\n      value: 38.769\n    - type: map_at_100\n      value: 40.292\n    - type: map_at_1000\n      value: 40.510000000000005\n    - type: map_at_3\n      value: 35.39\n    - type: map_at_5\n      value: 37.009\n    - type: mrr_at_1\n      value: 34.19\n    - type: mrr_at_10\n      value: 43.418\n    - type: mrr_at_100\n      value: 44.132\n    - type: mrr_at_1000\n      value: 44.175\n    - type: mrr_at_3\n      value: 40.547\n    - type: mrr_at_5\n      value: 42.088\n    - type: ndcg_at_1\n      value: 34.19\n    - type: ndcg_at_10\n      value: 45.14\n    - type: ndcg_at_100\n      value: 50.364\n    - type: ndcg_at_1000\n      value: 52.481\n    - type: ndcg_at_3\n      value: 39.466\n    - type: ndcg_at_5\n      value: 41.772\n    - type: precision_at_1\n      value: 34.19\n    - type: precision_at_10\n      value: 8.715\n    - type: precision_at_100\n      value: 1.6150000000000002\n    - type: precision_at_1000\n      value: 0.247\n    - type: precision_at_3\n      value: 18.248\n    - type: precision_at_5\n      value: 13.161999999999999\n    - type: recall_at_1\n      value: 29.142000000000003\n    - type: recall_at_10\n      value: 57.577999999999996\n    - type: recall_at_100\n      value: 81.428\n    - type: recall_at_1000\n      value: 94.017\n    - type: recall_at_3\n      value: 41.402\n    - type: recall_at_5\n      value: 47.695\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.039\n    - type: map_at_10\n      value: 30.669999999999998\n    - type: map_at_100\n      value: 31.682\n    - type: map_at_1000\n      value: 31.794\n    - type: map_at_3\n      value: 28.139999999999997\n    - type: map_at_5\n      value: 29.457\n    - type: mrr_at_1\n      value: 24.399\n    - type: mrr_at_10\n      value: 32.687\n    - type: mrr_at_100\n      value: 33.622\n    - type: mrr_at_1000\n      value: 33.698\n    - type: mrr_at_3\n      value: 30.407\n    - type: mrr_at_5\n      value: 31.552999999999997\n    - type: ndcg_at_1\n      value: 24.399\n    - type: ndcg_at_10\n      value: 35.472\n    - type: ndcg_at_100\n      value: 40.455000000000005\n    - type: ndcg_at_1000\n      value: 43.15\n    - type: ndcg_at_3\n      value: 30.575000000000003\n    - type: ndcg_at_5\n      value: 32.668\n    - type: precision_at_1\n      value: 24.399\n    - type: precision_at_10\n      value: 5.656\n    - type: precision_at_100\n      value: 0.874\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 13.062000000000001\n    - type: precision_at_5\n      value: 9.242\n    - type: recall_at_1\n      value: 22.039\n    - type: recall_at_10\n      value: 48.379\n    - type: recall_at_100\n      value: 71.11800000000001\n    - type: recall_at_1000\n      value: 91.095\n    - type: recall_at_3\n      value: 35.108\n    - type: recall_at_5\n      value: 40.015\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.144\n    - type: map_at_10\n      value: 18.238\n    - type: map_at_100\n      value: 20.143\n    - type: map_at_1000\n      value: 20.346\n    - type: map_at_3\n      value: 14.809\n    - type: map_at_5\n      value: 16.567999999999998\n    - type: mrr_at_1\n      value: 22.671\n    - type: mrr_at_10\n      value: 34.906\n    - type: mrr_at_100\n      value: 35.858000000000004\n    - type: mrr_at_1000\n      value: 35.898\n    - type: mrr_at_3\n      value: 31.238\n    - type: mrr_at_5\n      value: 33.342\n    - type: ndcg_at_1\n      value: 22.671\n    - type: ndcg_at_10\n      value: 26.540000000000003\n    - type: ndcg_at_100\n      value: 34.138000000000005\n    - type: ndcg_at_1000\n      value: 37.72\n    - type: ndcg_at_3\n      value: 20.766000000000002\n    - type: ndcg_at_5\n      value: 22.927\n    - type: precision_at_1\n      value: 22.671\n    - type: precision_at_10\n      value: 8.619\n    - type: precision_at_100\n      value: 1.678\n    - type: precision_at_1000\n      value: 0.23500000000000001\n    - type: precision_at_3\n      value: 15.592\n    - type: precision_at_5\n      value: 12.43\n    - type: recall_at_1\n      value: 10.144\n    - type: recall_at_10\n      value: 33.46\n    - type: recall_at_100\n      value: 59.758\n    - type: recall_at_1000\n      value: 79.704\n    - type: recall_at_3\n      value: 19.604\n    - type: recall_at_5\n      value: 25.367\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.654\n    - type: map_at_10\n      value: 18.506\n    - type: map_at_100\n      value: 26.412999999999997\n    - type: map_at_1000\n      value: 28.13\n    - type: map_at_3\n      value: 13.379\n    - type: map_at_5\n      value: 15.529000000000002\n    - type: mrr_at_1\n      value: 66.0\n    - type: mrr_at_10\n      value: 74.13\n    - type: mrr_at_100\n      value: 74.48700000000001\n    - type: mrr_at_1000\n      value: 74.49799999999999\n    - type: mrr_at_3\n      value: 72.75\n    - type: mrr_at_5\n      value: 73.762\n    - type: ndcg_at_1\n      value: 54.50000000000001\n    - type: ndcg_at_10\n      value: 40.236\n    - type: ndcg_at_100\n      value: 44.690999999999995\n    - type: ndcg_at_1000\n      value: 52.195\n    - type: ndcg_at_3\n      value: 45.632\n    - type: ndcg_at_5\n      value: 42.952\n    - type: precision_at_1\n      value: 66.0\n    - type: precision_at_10\n      value: 31.724999999999998\n    - type: precision_at_100\n      value: 10.299999999999999\n    - type: precision_at_1000\n      value: 2.194\n    - type: precision_at_3\n      value: 48.75\n    - type: precision_at_5\n      value: 41.6\n    - type: recall_at_1\n      value: 8.654\n    - type: recall_at_10\n      value: 23.74\n    - type: recall_at_100\n      value: 50.346999999999994\n    - type: recall_at_1000\n      value: 74.376\n    - type: recall_at_3\n      value: 14.636\n    - type: recall_at_5\n      value: 18.009\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 53.245\n    - type: f1\n      value: 48.74520523753552\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 51.729\n    - type: map_at_10\n      value: 63.904\n    - type: map_at_100\n      value: 64.363\n    - type: map_at_1000\n      value: 64.38199999999999\n    - type: map_at_3\n      value: 61.393\n    - type: map_at_5\n      value: 63.02100000000001\n    - type: mrr_at_1\n      value: 55.686\n    - type: mrr_at_10\n      value: 67.804\n    - type: mrr_at_100\n      value: 68.15299999999999\n    - type: mrr_at_1000\n      value: 68.161\n    - type: mrr_at_3\n      value: 65.494\n    - type: mrr_at_5\n      value: 67.01599999999999\n    - type: ndcg_at_1\n      value: 55.686\n    - type: ndcg_at_10\n      value: 70.025\n    - type: ndcg_at_100\n      value: 72.011\n    - type: ndcg_at_1000\n      value: 72.443\n    - type: ndcg_at_3\n      value: 65.32900000000001\n    - type: ndcg_at_5\n      value: 68.05600000000001\n    - type: precision_at_1\n      value: 55.686\n    - type: precision_at_10\n      value: 9.358\n    - type: precision_at_100\n      value: 1.05\n    - type: precision_at_1000\n      value: 0.11\n    - type: precision_at_3\n      value: 26.318\n    - type: precision_at_5\n      value: 17.321\n    - type: recall_at_1\n      value: 51.729\n    - type: recall_at_10\n      value: 85.04\n    - type: recall_at_100\n      value: 93.777\n    - type: recall_at_1000\n      value: 96.824\n    - type: recall_at_3\n      value: 72.521\n    - type: recall_at_5\n      value: 79.148\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.765\n    - type: map_at_10\n      value: 39.114\n    - type: map_at_100\n      value: 40.987\n    - type: map_at_1000\n      value: 41.155\n    - type: map_at_3\n      value: 34.028000000000006\n    - type: map_at_5\n      value: 36.925000000000004\n    - type: mrr_at_1\n      value: 46.451\n    - type: mrr_at_10\n      value: 54.711\n    - type: mrr_at_100\n      value: 55.509\n    - type: mrr_at_1000\n      value: 55.535000000000004\n    - type: mrr_at_3\n      value: 52.649\n    - type: mrr_at_5\n      value: 53.729000000000006\n    - type: ndcg_at_1\n      value: 46.451\n    - type: ndcg_at_10\n      value: 46.955999999999996\n    - type: ndcg_at_100\n      value: 53.686\n    - type: ndcg_at_1000\n      value: 56.230000000000004\n    - type: ndcg_at_3\n      value: 43.374\n    - type: ndcg_at_5\n      value: 44.372\n    - type: precision_at_1\n      value: 46.451\n    - type: precision_at_10\n      value: 13.256\n    - type: precision_at_100\n      value: 2.019\n    - type: precision_at_1000\n      value: 0.247\n    - type: precision_at_3\n      value: 29.115000000000002\n    - type: precision_at_5\n      value: 21.389\n    - type: recall_at_1\n      value: 23.765\n    - type: recall_at_10\n      value: 53.452999999999996\n    - type: recall_at_100\n      value: 78.828\n    - type: recall_at_1000\n      value: 93.938\n    - type: recall_at_3\n      value: 39.023\n    - type: recall_at_5\n      value: 45.18\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.918000000000003\n    - type: map_at_10\n      value: 46.741\n    - type: map_at_100\n      value: 47.762\n    - type: map_at_1000\n      value: 47.849000000000004\n    - type: map_at_3\n      value: 43.578\n    - type: map_at_5\n      value: 45.395\n    - type: mrr_at_1\n      value: 63.834999999999994\n    - type: mrr_at_10\n      value: 71.312\n    - type: mrr_at_100\n      value: 71.695\n    - type: mrr_at_1000\n      value: 71.714\n    - type: mrr_at_3\n      value: 69.82000000000001\n    - type: mrr_at_5\n      value: 70.726\n    - type: ndcg_at_1\n      value: 63.834999999999994\n    - type: ndcg_at_10\n      value: 55.879999999999995\n    - type: ndcg_at_100\n      value: 59.723000000000006\n    - type: ndcg_at_1000\n      value: 61.49400000000001\n    - type: ndcg_at_3\n      value: 50.964\n    - type: ndcg_at_5\n      value: 53.47\n    - type: precision_at_1\n      value: 63.834999999999994\n    - type: precision_at_10\n      value: 11.845\n    - type: precision_at_100\n      value: 1.4869999999999999\n    - type: precision_at_1000\n      value: 0.172\n    - type: precision_at_3\n      value: 32.158\n    - type: precision_at_5\n      value: 21.278\n    - type: recall_at_1\n      value: 31.918000000000003\n    - type: recall_at_10\n      value: 59.223000000000006\n    - type: recall_at_100\n      value: 74.328\n    - type: recall_at_1000\n      value: 86.05000000000001\n    - type: recall_at_3\n      value: 48.238\n    - type: recall_at_5\n      value: 53.193999999999996\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 79.7896\n    - type: ap\n      value: 73.65166029460288\n    - type: f1\n      value: 79.71794693711813\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.239\n    - type: map_at_10\n      value: 34.542\n    - type: map_at_100\n      value: 35.717999999999996\n    - type: map_at_1000\n      value: 35.764\n    - type: map_at_3\n      value: 30.432\n    - type: map_at_5\n      value: 32.81\n    - type: mrr_at_1\n      value: 22.908\n    - type: mrr_at_10\n      value: 35.127\n    - type: mrr_at_100\n      value: 36.238\n    - type: mrr_at_1000\n      value: 36.278\n    - type: mrr_at_3\n      value: 31.076999999999998\n    - type: mrr_at_5\n      value: 33.419\n    - type: ndcg_at_1\n      value: 22.908\n    - type: ndcg_at_10\n      value: 41.607\n    - type: ndcg_at_100\n      value: 47.28\n    - type: ndcg_at_1000\n      value: 48.414\n    - type: ndcg_at_3\n      value: 33.253\n    - type: ndcg_at_5\n      value: 37.486000000000004\n    - type: precision_at_1\n      value: 22.908\n    - type: precision_at_10\n      value: 6.645\n    - type: precision_at_100\n      value: 0.9490000000000001\n    - type: precision_at_1000\n      value: 0.105\n    - type: precision_at_3\n      value: 14.130999999999998\n    - type: precision_at_5\n      value: 10.616\n    - type: recall_at_1\n      value: 22.239\n    - type: recall_at_10\n      value: 63.42\n    - type: recall_at_100\n      value: 89.696\n    - type: recall_at_1000\n      value: 98.351\n    - type: recall_at_3\n      value: 40.77\n    - type: recall_at_5\n      value: 50.93\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 95.06839945280439\n    - type: f1\n      value: 94.74276398224072\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 72.25718194254446\n    - type: f1\n      value: 53.91164489161391\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.47948890383323\n    - type: f1\n      value: 69.98520247230257\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.46603900470748\n    - type: f1\n      value: 76.44111526065399\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 33.19106070798198\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 30.78772205248094\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.811231631488507\n    - type: mrr\n      value: 32.98200485378021\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 6.9\n    - type: map_at_10\n      value: 13.703000000000001\n    - type: map_at_100\n      value: 17.251\n    - type: map_at_1000\n      value: 18.795\n    - type: map_at_3\n      value: 10.366999999999999\n    - type: map_at_5\n      value: 11.675\n    - type: mrr_at_1\n      value: 47.059\n    - type: mrr_at_10\n      value: 55.816\n    - type: mrr_at_100\n      value: 56.434\n    - type: mrr_at_1000\n      value: 56.467\n    - type: mrr_at_3\n      value: 53.973000000000006\n    - type: mrr_at_5\n      value: 55.257999999999996\n    - type: ndcg_at_1\n      value: 44.737\n    - type: ndcg_at_10\n      value: 35.997\n    - type: ndcg_at_100\n      value: 33.487\n    - type: ndcg_at_1000\n      value: 41.897\n    - type: ndcg_at_3\n      value: 41.18\n    - type: ndcg_at_5\n      value: 38.721\n    - type: precision_at_1\n      value: 46.129999999999995\n    - type: precision_at_10\n      value: 26.533\n    - type: precision_at_100\n      value: 8.706\n    - type: precision_at_1000\n      value: 2.16\n    - type: precision_at_3\n      value: 38.493\n    - type: precision_at_5\n      value: 33.189\n    - type: recall_at_1\n      value: 6.9\n    - type: recall_at_10\n      value: 17.488999999999997\n    - type: recall_at_100\n      value: 34.583000000000006\n    - type: recall_at_1000\n      value: 64.942\n    - type: recall_at_3\n      value: 11.494\n    - type: recall_at_5\n      value: 13.496\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 33.028999999999996\n    - type: map_at_10\n      value: 49.307\n    - type: map_at_100\n      value: 50.205\n    - type: map_at_1000\n      value: 50.23\n    - type: map_at_3\n      value: 44.782\n    - type: map_at_5\n      value: 47.599999999999994\n    - type: mrr_at_1\n      value: 37.108999999999995\n    - type: mrr_at_10\n      value: 51.742999999999995\n    - type: mrr_at_100\n      value: 52.405\n    - type: mrr_at_1000\n      value: 52.422000000000004\n    - type: mrr_at_3\n      value: 48.087999999999994\n    - type: mrr_at_5\n      value: 50.414\n    - type: ndcg_at_1\n      value: 37.08\n    - type: ndcg_at_10\n      value: 57.236\n    - type: ndcg_at_100\n      value: 60.931999999999995\n    - type: ndcg_at_1000\n      value: 61.522\n    - type: ndcg_at_3\n      value: 48.93\n    - type: ndcg_at_5\n      value: 53.561\n    - type: precision_at_1\n      value: 37.08\n    - type: precision_at_10\n      value: 9.386\n    - type: precision_at_100\n      value: 1.1480000000000001\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 22.258\n    - type: precision_at_5\n      value: 16.025\n    - type: recall_at_1\n      value: 33.028999999999996\n    - type: recall_at_10\n      value: 78.805\n    - type: recall_at_100\n      value: 94.643\n    - type: recall_at_1000\n      value: 99.039\n    - type: recall_at_3\n      value: 57.602\n    - type: recall_at_5\n      value: 68.253\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.122\n    - type: map_at_10\n      value: 85.237\n    - type: map_at_100\n      value: 85.872\n    - type: map_at_1000\n      value: 85.885\n    - type: map_at_3\n      value: 82.27499999999999\n    - type: map_at_5\n      value: 84.13199999999999\n    - type: mrr_at_1\n      value: 81.73\n    - type: mrr_at_10\n      value: 87.834\n    - type: mrr_at_100\n      value: 87.92\n    - type: mrr_at_1000\n      value: 87.921\n    - type: mrr_at_3\n      value: 86.878\n    - type: mrr_at_5\n      value: 87.512\n    - type: ndcg_at_1\n      value: 81.73\n    - type: ndcg_at_10\n      value: 88.85499999999999\n    - type: ndcg_at_100\n      value: 89.992\n    - type: ndcg_at_1000\n      value: 90.07\n    - type: ndcg_at_3\n      value: 85.997\n    - type: ndcg_at_5\n      value: 87.55199999999999\n    - type: precision_at_1\n      value: 81.73\n    - type: precision_at_10\n      value: 13.491\n    - type: precision_at_100\n      value: 1.536\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.623\n    - type: precision_at_5\n      value: 24.742\n    - type: recall_at_1\n      value: 71.122\n    - type: recall_at_10\n      value: 95.935\n    - type: recall_at_100\n      value: 99.657\n    - type: recall_at_1000\n      value: 99.996\n    - type: recall_at_3\n      value: 87.80799999999999\n    - type: recall_at_5\n      value: 92.161\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 63.490029238193756\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 65.13153408508836\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.202999999999999\n    - type: map_at_10\n      value: 10.174\n    - type: map_at_100\n      value: 12.138\n    - type: map_at_1000\n      value: 12.418\n    - type: map_at_3\n      value: 7.379\n    - type: map_at_5\n      value: 8.727\n    - type: mrr_at_1\n      value: 20.7\n    - type: mrr_at_10\n      value: 30.389\n    - type: mrr_at_100\n      value: 31.566\n    - type: mrr_at_1000\n      value: 31.637999999999998\n    - type: mrr_at_3\n      value: 27.133000000000003\n    - type: mrr_at_5\n      value: 29.078\n    - type: ndcg_at_1\n      value: 20.7\n    - type: ndcg_at_10\n      value: 17.355999999999998\n    - type: ndcg_at_100\n      value: 25.151\n    - type: ndcg_at_1000\n      value: 30.37\n    - type: ndcg_at_3\n      value: 16.528000000000002\n    - type: ndcg_at_5\n      value: 14.396999999999998\n    - type: precision_at_1\n      value: 20.7\n    - type: precision_at_10\n      value: 8.98\n    - type: precision_at_100\n      value: 2.015\n    - type: precision_at_1000\n      value: 0.327\n    - type: precision_at_3\n      value: 15.367\n    - type: precision_at_5\n      value: 12.559999999999999\n    - type: recall_at_1\n      value: 4.202999999999999\n    - type: recall_at_10\n      value: 18.197\n    - type: recall_at_100\n      value: 40.903\n    - type: recall_at_1000\n      value: 66.427\n    - type: recall_at_3\n      value: 9.362\n    - type: recall_at_5\n      value: 12.747\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.69890989765257\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_spearman\n      value: 75.31953790551489\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_spearman\n      value: 87.44050861280759\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.86922869270393\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_spearman\n      value: 88.9399170304284\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_spearman\n      value: 85.38015314088582\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_spearman\n      value: 90.53653527788835\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_spearman\n      value: 68.64526474250209\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_spearman\n      value: 86.56156983963042\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 79.48610254648003\n    - type: mrr\n      value: 94.02481505422682\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 48.983\n    - type: map_at_10\n      value: 59.077999999999996\n    - type: map_at_100\n      value: 59.536\n    - type: map_at_1000\n      value: 59.575\n    - type: map_at_3\n      value: 55.691\n    - type: map_at_5\n      value: 57.410000000000004\n    - type: mrr_at_1\n      value: 51.666999999999994\n    - type: mrr_at_10\n      value: 60.427\n    - type: mrr_at_100\n      value: 60.763\n    - type: mrr_at_1000\n      value: 60.79900000000001\n    - type: mrr_at_3\n      value: 57.556\n    - type: mrr_at_5\n      value: 59.089000000000006\n    - type: ndcg_at_1\n      value: 51.666999999999994\n    - type: ndcg_at_10\n      value: 64.559\n    - type: ndcg_at_100\n      value: 66.58\n    - type: ndcg_at_1000\n      value: 67.64\n    - type: ndcg_at_3\n      value: 58.287\n    - type: ndcg_at_5\n      value: 61.001000000000005\n    - type: precision_at_1\n      value: 51.666999999999994\n    - type: precision_at_10\n      value: 9.067\n    - type: precision_at_100\n      value: 1.0170000000000001\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_3\n      value: 23.0\n    - type: precision_at_5\n      value: 15.6\n    - type: recall_at_1\n      value: 48.983\n    - type: recall_at_10\n      value: 80.289\n    - type: recall_at_100\n      value: 89.43299999999999\n    - type: recall_at_1000\n      value: 97.667\n    - type: recall_at_3\n      value: 62.978\n    - type: recall_at_5\n      value: 69.872\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.79009900990098\n    - type: cos_sim_ap\n      value: 94.94115052608419\n    - type: cos_sim_f1\n      value: 89.1260162601626\n    - type: cos_sim_precision\n      value: 90.599173553719\n    - type: cos_sim_recall\n      value: 87.7\n    - type: dot_accuracy\n      value: 99.79009900990098\n    - type: dot_ap\n      value: 94.94115052608419\n    - type: dot_f1\n      value: 89.1260162601626\n    - type: dot_precision\n      value: 90.599173553719\n    - type: dot_recall\n      value: 87.7\n    - type: euclidean_accuracy\n      value: 99.79009900990098\n    - type: euclidean_ap\n      value: 94.94115052608419\n    - type: euclidean_f1\n      value: 89.1260162601626\n    - type: euclidean_precision\n      value: 90.599173553719\n    - type: euclidean_recall\n      value: 87.7\n    - type: manhattan_accuracy\n      value: 99.7940594059406\n    - type: manhattan_ap\n      value: 94.95271414642431\n    - type: manhattan_f1\n      value: 89.24508790072387\n    - type: manhattan_precision\n      value: 92.3982869379015\n    - type: manhattan_recall\n      value: 86.3\n    - type: max_accuracy\n      value: 99.7940594059406\n    - type: max_ap\n      value: 94.95271414642431\n    - type: max_f1\n      value: 89.24508790072387\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 68.43866571935851\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 35.16579026551532\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 52.518952473513934\n    - type: mrr\n      value: 53.292457134368895\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 31.12529588316604\n    - type: cos_sim_spearman\n      value: 32.31662126895294\n    - type: dot_pearson\n      value: 31.125303796647056\n    - type: dot_spearman\n      value: 32.31662126895294\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.219\n    - type: map_at_10\n      value: 1.7469999999999999\n    - type: map_at_100\n      value: 10.177999999999999\n    - type: map_at_1000\n      value: 26.108999999999998\n    - type: map_at_3\n      value: 0.64\n    - type: map_at_5\n      value: 0.968\n    - type: mrr_at_1\n      value: 82.0\n    - type: mrr_at_10\n      value: 89.067\n    - type: mrr_at_100\n      value: 89.067\n    - type: mrr_at_1000\n      value: 89.067\n    - type: mrr_at_3\n      value: 88.333\n    - type: mrr_at_5\n      value: 88.73299999999999\n    - type: ndcg_at_1\n      value: 78.0\n    - type: ndcg_at_10\n      value: 71.398\n    - type: ndcg_at_100\n      value: 55.574999999999996\n    - type: ndcg_at_1000\n      value: 51.771\n    - type: ndcg_at_3\n      value: 77.765\n    - type: ndcg_at_5\n      value: 73.614\n    - type: precision_at_1\n      value: 82.0\n    - type: precision_at_10\n      value: 75.4\n    - type: precision_at_100\n      value: 58.040000000000006\n    - type: precision_at_1000\n      value: 23.516000000000002\n    - type: precision_at_3\n      value: 84.0\n    - type: precision_at_5\n      value: 78.4\n    - type: recall_at_1\n      value: 0.219\n    - type: recall_at_10\n      value: 1.958\n    - type: recall_at_100\n      value: 13.797999999999998\n    - type: recall_at_1000\n      value: 49.881\n    - type: recall_at_3\n      value: 0.672\n    - type: recall_at_5\n      value: 1.0370000000000001\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 1.8610000000000002\n    - type: map_at_10\n      value: 8.705\n    - type: map_at_100\n      value: 15.164\n    - type: map_at_1000\n      value: 16.78\n    - type: map_at_3\n      value: 4.346\n    - type: map_at_5\n      value: 6.151\n    - type: mrr_at_1\n      value: 22.448999999999998\n    - type: mrr_at_10\n      value: 41.556\n    - type: mrr_at_100\n      value: 42.484\n    - type: mrr_at_1000\n      value: 42.494\n    - type: mrr_at_3\n      value: 37.755\n    - type: mrr_at_5\n      value: 40.102\n    - type: ndcg_at_1\n      value: 21.429000000000002\n    - type: ndcg_at_10\n      value: 23.439\n    - type: ndcg_at_100\n      value: 36.948\n    - type: ndcg_at_1000\n      value: 48.408\n    - type: ndcg_at_3\n      value: 22.261\n    - type: ndcg_at_5\n      value: 23.085\n    - type: precision_at_1\n      value: 22.448999999999998\n    - type: precision_at_10\n      value: 21.633\n    - type: precision_at_100\n      value: 8.02\n    - type: precision_at_1000\n      value: 1.5939999999999999\n    - type: precision_at_3\n      value: 23.810000000000002\n    - type: precision_at_5\n      value: 24.490000000000002\n    - type: recall_at_1\n      value: 1.8610000000000002\n    - type: recall_at_10\n      value: 15.876000000000001\n    - type: recall_at_100\n      value: 50.300999999999995\n    - type: recall_at_1000\n      value: 86.098\n    - type: recall_at_3\n      value: 5.892\n    - type: recall_at_5\n      value: 9.443\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 70.3264\n    - type: ap\n      value: 13.249577616243794\n    - type: f1\n      value: 53.621518367695685\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 61.57611771363894\n    - type: f1\n      value: 61.79797478568639\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 53.38315344479284\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 87.55438993860642\n    - type: cos_sim_ap\n      value: 77.98702600017738\n    - type: cos_sim_f1\n      value: 71.94971653931476\n    - type: cos_sim_precision\n      value: 67.50693802035153\n    - type: cos_sim_recall\n      value: 77.01846965699208\n    - type: dot_accuracy\n      value: 87.55438993860642\n    - type: dot_ap\n      value: 77.98702925907986\n    - type: dot_f1\n      value: 71.94971653931476\n    - type: dot_precision\n      value: 67.50693802035153\n    - type: dot_recall\n      value: 77.01846965699208\n    - type: euclidean_accuracy\n      value: 87.55438993860642\n    - type: euclidean_ap\n      value: 77.98702951957925\n    - type: euclidean_f1\n      value: 71.94971653931476\n    - type: euclidean_precision\n      value: 67.50693802035153\n    - type: euclidean_recall\n      value: 77.01846965699208\n    - type: manhattan_accuracy\n      value: 87.54246885617214\n    - type: manhattan_ap\n      value: 77.95531413902947\n    - type: manhattan_f1\n      value: 71.93605683836589\n    - type: manhattan_precision\n      value: 69.28152492668622\n    - type: manhattan_recall\n      value: 74.80211081794195\n    - type: max_accuracy\n      value: 87.55438993860642\n    - type: max_ap\n      value: 77.98702951957925\n    - type: max_f1\n      value: 71.94971653931476\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.47296930182016\n    - type: cos_sim_ap\n      value: 86.92853616302108\n    - type: cos_sim_f1\n      value: 79.35138351681047\n    - type: cos_sim_precision\n      value: 76.74820143884892\n    - type: cos_sim_recall\n      value: 82.13735756082538\n    - type: dot_accuracy\n      value: 89.47296930182016\n    - type: dot_ap\n      value: 86.92854339601595\n    - type: dot_f1\n      value: 79.35138351681047\n    - type: dot_precision\n      value: 76.74820143884892\n    - type: dot_recall\n      value: 82.13735756082538\n    - type: euclidean_accuracy\n      value: 89.47296930182016\n    - type: euclidean_ap\n      value: 86.92854191061649\n    - type: euclidean_f1\n      value: 79.35138351681047\n    - type: euclidean_precision\n      value: 76.74820143884892\n    - type: euclidean_recall\n      value: 82.13735756082538\n    - type: manhattan_accuracy\n      value: 89.47685023479644\n    - type: manhattan_ap\n      value: 86.90063722679578\n    - type: manhattan_f1\n      value: 79.30753865502702\n    - type: manhattan_precision\n      value: 76.32066068631639\n    - type: manhattan_recall\n      value: 82.53772713273791\n    - type: max_accuracy\n      value: 89.47685023479644\n    - type: max_ap\n      value: 86.92854339601595\n    - type: max_f1\n      value: 79.35138351681047\n---\n\n# hkunlp/instructor-xl\nWe introduce **Instructor**👨‍🏫, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) ***by simply providing the task instruction, without any finetuning***. Instructor👨‍ achieves sota on 70 diverse embedding tasks!\nThe model is easy to use with **our customized** `sentence-transformer` library. For more details, check out [our paper](https://arxiv.org/abs/2212.09741) and [project page](https://instructor-embedding.github.io/)! \n\n**************************** **Updates** ****************************\n\n* 01/21: We released a new [checkpoint](https://huggingface.co/hkunlp/instructor-xl) trained with hard negatives, which gives better performance.\n* 12/21: We released our [paper](https://arxiv.org/abs/2212.09741), [code](https://github.com/HKUNLP/instructor-embedding), [checkpoint](https://huggingface.co/hkunlp/instructor-xl) and [project page](https://instructor-embedding.github.io/)! Check them out!\n\n## Quick start\n<hr />\n\n## Installation\n```bash\npip install InstructorEmbedding\n```\n\n## Compute your customized embeddings\nThen you can use the model like this to calculate domain-specific and task-aware embeddings:\n```python\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-xl')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n```\n\n## Use cases\n<hr />\n\n## Calculate embeddings for your customized texts\nIf you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Represent the `domain` `text_type` for `task_objective`:\n* `domain` is optional, and it specifies the domain of the text, e.g., science, finance, medicine, etc.\n* `text_type` is required, and it specifies the encoding unit, e.g., sentence, document, paragraph, etc.\n* `task_objective` is optional, and it specifies the objective of embedding, e.g., retrieve a document, classify the sentence, etc.\n\n## Calculate Sentence similarities\nYou can further use the model to compute similarities between two groups of sentences, with **customized embeddings**.\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'], \n               ['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\nsentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n               ['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n```\n\n## Information Retrieval\nYou can also use **customized embeddings** for information retrieval.\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [['Represent the Wikipedia question for retrieving supporting documents: ','where is the food stored in a yam plant']]\ncorpus = [['Represent the Wikipedia document for retrieval: ','Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term \"mixed economies\" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.'],\n          ['Represent the Wikipedia document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loansâ€”and some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n          ['Represent the Wikipedia document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n```\n\n## Clustering\nUse **customized embeddings** for clustering texts in groups.\n```python\nimport sklearn.cluster\nsentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity'],\n             ['Represent the Medicine sentence for clustering: ','Comparison of Atmospheric Neutrino Flux Calculations at Low Energies'],\n             ['Represent the Medicine sentence for clustering: ','Fermion Bags in the Massive Gross-Neveu Model'],\n             ['Represent the Medicine sentence for clustering: ',\"QCD corrections to Associated t-tbar-H production at the Tevatron\"],\n             ['Represent the Medicine sentence for clustering: ','A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n```",
    "meta_json": "{\"pipeline_tag\":\"sentence-similarity\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":null,\"storage_bytes\":19868112990,\"files_count\":14,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"T5EncoderModel\"],\"model_type\":\"t5\",\"tokenizer_config\":{\"eos_token\":\"</s>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:HKUNLP:instructor-embedding\",\"source_url\":\"https://github.com/HKUNLP/instructor-embedding\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2212.09741\",\"source_url\":\"https://arxiv.org/abs/2212.09741\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 77.6,
    "content_hash": "f335b9f3ecd3ab122b4247dcad40c0e1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/hkunlp/instructor-xl\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:xlabs-ai:flux-lora-collection",
    "name": "flux-lora-collection",
    "author": "XLabs-AI",
    "description": "--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE. language: - en pipeline_tag: text-to-image tags: - LoRA - Stable Diffusion - image-generation - Flux --- !FLUX LoRA Collections This repository provides a checkpoint with trained LoRAs for FLUX.1-dev model by Black Forest Labs <img src=\"https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true\"> !Examp...",
    "tags": [
      "lora",
      "stable diffusion",
      "image-generation",
      "flux",
      "text-to-image",
      "en",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 579,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/XLabs-AI/flux-lora-collection",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.\nlanguage:\n- en\npipeline_tag: text-to-image\ntags:\n- LoRA\n- Stable Diffusion\n- image-generation\n- Flux\n---\n![FLUX LoRA Collections](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/flux-lora-collection-rev1.png?raw=true)\nThis repository provides a checkpoint with trained LoRAs for\n[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) by Black Forest Labs\n[<img src=\"https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true\">](https://discord.gg/FHY2guThfy)\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/furry4.png?raw=true)\n# ComfyUI\n\n[See our github](https://github.com/XLabs-AI/x-flux-comfyui) for comfy ui workflows.\n![Example Picture 1](https://github.com/XLabs-AI/x-flux-comfyui/blob/main/assets/image1.png?raw=true)\n# Training details\n[XLabs AI](https://github.com/XLabs-AI) team is happy to publish fune-tuning Flux scripts, including:\n\n- **LoRA** 🔥\n- **ControlNet** 🔥\n\n[See our github](https://github.com/XLabs-AI/x-flux) for train script and train configs.\n\n# Training Dataset\nDataset has the following format for the training process:\n\n```\n├── images/\n│    ├── 1.png\n│    ├── 1.json\n│    ├── 2.png\n│    ├── 2.json\n│    ├── ...\n```\nA .json file contains \"caption\" field with a text prompt.\n\nThank https://civitai.com/user/dobrosketchkun and https://civitai.com/user/sadxzero for datasets for loras\n\n# Inference\n## furry_lora\n```bash\npython3 main.py \\\n --prompt \"Female furry Pixie with text 'hello world'\" \\\n --lora_repo_id XLabs-AI/flux-furry-lora --lora_name furry_lora.safetensors --device cuda --offload --use_lora \\\n --model_type flux-dev-fp8 --width 1024 --height 1024 \\\n --timestep_to_start_cfg 1 --num_steps 25 --true_gs 3.5 --guidance 4\n\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/furry4.png?raw=true)\n```bash\npython3 main.py \\\n --prompt \"Male furry Lycanthrope with fur-covered body in ancient ruins, howling at the full moon, surrounded by eerie mist, werewolf transformation, elder scrolls, eslweyr, glitch aesthetic, anime-inspired, digital illustration, artstation, furry\" \\\n --lora_repo_id XLabs-AI/flux-furry-lora --lora_name furry_lora.safetensors --device cuda --offload --use_lora \\\n --model_type flux-dev-fp8 --width 1024 --height 1024 \\\n --timestep_to_start_cfg 1 --num_steps 25 --true_gs 3.5\n\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/furry2.png?raw=true)\n## mjv6_lora\n```bash\npython3 main.py \\\n--prompt \"A handsome man in a suit, 25 years old, cool, futuristic\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name mjv6_lora.safetensors \\\n--device cuda:4 --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_13.png?raw=true)\n\n```bash\npython3 main.py \\\n--prompt \"A girl in a suit covered with bold tattoos and holding a vest pistol, beautiful woman, 25 years old, cool, future fantasy, turquoise & light orange ping curl hair\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name mjv6_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_12.png?raw=true)\n## anime_lora\n```bash\npython3 main.py \\\n--prompt \"A cute corgi lives in a house made out of sushi, anime\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name anime_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_14.png?raw=true)\n```bash\npython3 main.py \\\n--prompt \"a girl with orange hair, standing in a room with a window, looking out at a cityscape, anime\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name anime_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_15.png?raw=true)\n\n## disney_lora\n```bash\npython3 main.py \\\n--prompt \"An aerial view of beach with people on it, disney style\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name disney_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_19.png?raw=true)\n\n```bash\npython3 main.py \\\n--prompt \"A blue jay standing on a large basket of rainbow macarons, disney style\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name disney_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_18.png?raw=true)\n\n## scenery_lora\n```bash\npython3 main.py \\\n--prompt \"A fantasy cityscape with multiple buildings and skyscrapers all of which are covered in snow and ice, scenery style\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name scenery_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_21.png?raw=true)\n\n```bash\npython3 main.py \\\n--prompt \"A large ornate building with multiple levels and arches surrounded by trees and greenery. In front of it there are several statues and sculptures on pedestals with fire burning brightly in front of them, scenery style\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name scenery_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_22.png?raw=true)\n## art_lora\n```bash\npython3 main.py \\\n--prompt \"white rabbit in blue dress and hat holding bow and arrow, art\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name art_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_23.png?raw=true)\n\n```bash\npython3 main.py \\\n--prompt \"castle in the middle of forest at night, art\" \\\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name art_lora.safetensors \\\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_24.png?raw=true)\n# License\n\nlora.safetensors falls under the [FLUX.1 [dev]](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License<br/>",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":1479610132,\"files_count\":15,\"spaces_count\":49,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux-comfyui\",\"source_url\":\"https://github.com/XLabs-AI/x-flux-comfyui\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux-comfyui\",\"source_url\":\"https://github.com/XLabs-AI/x-flux-comfyui\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "6421016c9eefc9d3503342452db44f8b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/XLabs-AI/flux-lora-collection\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:black-forest-labs:flux.1-redux-dev",
    "name": "FLUX.1-Redux-dev",
    "author": "black-forest-labs",
    "description": "",
    "tags": [
      "diffusers",
      "safetensors",
      "image-generation",
      "flux",
      "diffusion-single-file",
      "en",
      "license:other",
      "diffusers:fluxpriorreduxpipeline",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 577,
    "downloads": 27114,
    "source": "huggingface",
    "source_url": "https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":1243640584,\"files_count\":11,\"spaces_count\":38,\"gated\":\"auto\",\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"FluxPriorReduxPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 37.6,
    "content_hash": "c3a3298d41feda8abc1460a344ed14e4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:agentica-org:deepscaler-1.5b-preview",
    "name": "DeepScaleR-1.5B-Preview",
    "author": "agentica-org",
    "description": "--- license: mit library_name: transformers datasets: - AI-MO/NuminaMath-CoT - KbsdJames/Omni-MATH - RUC-AIBOX/STILL-3-Preview-RL-Data - hendrycks/competition_math language: - en base_model: - deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B pipeline_tag: text-generation --- <div align=\"center\"> <span style=\"font-family: default; font-size: 1.5em;\">DeepScaleR-1.5B-Preview</span> <div> 🚀 Democratizing Reinforcement Learning for LLMs 🌟 </div> </div> <br> <div align=\"center\" style=\"line-height: 1;\"> ...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "en",
      "dataset:ai-mo/numinamath-cot",
      "dataset:kbsdjames/omni-math",
      "dataset:ruc-aibox/still-3-preview-rl-data",
      "dataset:hendrycks/competition_math",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 577,
    "downloads": 79594,
    "source": "huggingface",
    "source_url": "https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\ndatasets:\n- AI-MO/NuminaMath-CoT\n- KbsdJames/Omni-MATH\n- RUC-AIBOX/STILL-3-Preview-RL-Data\n- hendrycks/competition_math\nlanguage:\n- en\nbase_model:\n- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\npipeline_tag: text-generation\n---\n\n<div align=\"center\">\n<span style=\"font-family: default; font-size: 1.5em;\">DeepScaleR-1.5B-Preview</span>\n<div>\n🚀 Democratizing Reinforcement Learning for LLMs 🌟\n</div>\n</div>\n<br>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/agentica-project/rllm\" style=\"margin: 2px;\">\n    <img alt=\"Code\" src=\"https://img.shields.io/badge/DeepScaleR-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Blog\" src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://x.com/Agentica_/status/1889006266661617779\" style=\"margin: 2px;\">\n    <img alt=\"X.ai\" src=\"https://img.shields.io/badge/Agentica-white?style=for-the-badge&logo=X&logoColor=000&color=000&labelColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/agentica-org\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/Agentica-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n</div>\n</div>\n\n## DeepScaleR Overview\nDeepScaleR-1.5B-Preview is a language model fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 43.1% Pass@1 accuracy on AIME 2024, representing a 15% improvement over the base model (28.8%) and surpassing OpenAI's O1-Preview performance with just 1.5B parameters.\n\n## Data\nOur training dataset consists of approximately 40,000 unique problem-answer pairs compiled from:\n- AIME problems (1984-2023)\n- AMC problems (prior to 2023)\n- Omni-MATH dataset\n- Still dataset\n\n## Training Recipe\nWe employ Deepseek's Group Relative Policy Optimization (GRPO), a simplified RL algorithm that extends PPO by:\n- Normalizing advantage function over all samples generated from the same prompt.\n- Applying KL divergence regularization on top of PPO's surrogate loss to prevent significant policy drift.\n\n**Reward Function**: Our reward function is simple but effective:\n- 1 for correct answers passing LaTeX/Sympy checks\n- 0 for incorrect or improperly formatted answers\n- Note: No partial rewards (such as PRMs) or intermediate feedback.\n\n**Iterative Context Lengthening**: A key challenge in scaling RL for reasoning is compute cost. Our approach trains models with progressively longer contexts as the model improves, thus saving monetary costs and end2end training time: \n- Initial 8K Context (0-1040 steps):\n    - 22.9% -> 33% Pass@1 on AIME 2024\n    - Trained on 8 A100-80GB GPUs, BS= (Prompts) * (Samples/Prompt) = 128 * 8 = 1024\n- Extended to 16K (steps 1040-1520):\n    - 33% -> 43% Pass@1 on AIME 2024\n    - Trained on 32 A100-80GB GPUs, BS= (Prompts) * (Samples/Prompt) = 128 * 16 = 2048\n- Further extended to 24K (step 1520+):\n    - 38% -> 43% Pass@1 on AIME 2024\n    - Trained on 32 A100-80GB GPUs, BS= (Prompts) * (Samples/Prompt) = 128 * 16 = 2048\n    - Significant improvements within <200 steps\n\nA more detailed description of the training recipe can be found in our [blog post](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2).\n\n## Evaluation\nWe report Pass@1 accuracy averaged over 16 samples for each problem.\n| Model | AIME 2024 | MATH 500 | AMC 2023 | Minerva Math | OlympiadBench | Avg. |\n|-------|-----------|-----------|-----------|--------------|---------------|------|\n| Qwen-2.5-7B-Instruct | 13.3 | 79.8 | 50.6 | 34.6 | 40.7 | 43.8 |\n| rStar-Math-7B | 26.7 | 78.4 | 47.5 | - | 47.1 | - |\n| Eurus-2-7B-PRIME | 26.7 | 79.2 | 57.8 | 38.6 | 42.1 | 48.9 |\n| Qwen2.5-7B-SimpleRL | 26.7 | 82.4 | 62.5 | <strong>39.7</strong> | 43.3 | 50.9 |\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.8 | 82.8 | 62.9 | 26.5 | 43.3 | 48.9 |\n| Still-1.5B | 32.5 | 84.4 | 66.7 | 29.0 | 45.4 | 51.6 |\n| <strong>DeepScaleR-1.5B-Preview</strong> | <strong>43.1</strong> | <strong>87.8</strong> | <strong>73.6</strong> | 30.2 | <strong>50.0</strong> | <strong>57.0</strong> |\n| O1-Preview | 40.0 | 81.4 | - | - | - | - |\n\n## Serving DeepScaleR\nOur model can be served using popular high-performance inference systems:\n- vLLM\n- Hugging Face Text Generation Inference (TGI)\n- SGLang\n- TensorRT-LLM\n\nAll these systems support the OpenAI Chat Completions API format.\n\n## License\nThis project is released under the MIT License, reflecting our commitment to open and accessible AI development.\nWe believe in democratizing AI technology by making our work freely available for anyone to use, modify, and build upon.\nThis permissive license ensures that researchers, developers, and enthusiasts worldwide can leverage and extend our work without restrictions, fostering innovation and collaboration in the AI community.\n\n## Acknowledgement\n- Our training experiments are powered by our heavily modified fork of [Verl](https://github.com/agentica-project/verl), an open-source RLHF library.\n- Our model is trained on top of [`DeepSeek-R1-Distill-Qwen-1.5B`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B).\n- Our work is done as part of  [Berkeley Sky Computing Lab](https://skycomputing.berkeley.edu/) and [Berkeley AI Research](https://bair.berkeley.edu/).\n\n## Citation \n```bibtex\n@misc{deepscaler2025,\n  title={DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL},\n  author={Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Y. Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Li Erran Li and Raluca Ada Popa and Ion Stoica},\n  year={2025},\n  howpublished={\\url{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}},\n  note={Notion Blog}\n  year={2025}\n}",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1777088000,\"storage_bytes\":7119849176,\"files_count\":11,\"spaces_count\":11,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:agentica-project:rllm\\\"\",\"source_url\":\"https://github.com/agentica-project/rllm\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:agentica-project:verl\",\"source_url\":\"https://github.com/agentica-project/verl\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "90323b784eefe9ae2d6185745ec451cf",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:eleutherai:gpt-neox-20b",
    "name": "gpt-neox-20b",
    "author": "EleutherAI",
    "description": "--- language: - en tags: - pytorch - causal-lm license: apache-2.0 datasets: - EleutherAI/pile --- GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on the Pile using the GPT-NeoX library. Its architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J- 6B. Its training dataset contains a multitude of English-language texts, reflecting the general-purpose nature of this model. See the accompanying paper for details about model archit...",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "gpt_neox",
      "text-generation",
      "causal-lm",
      "en",
      "dataset:eleutherai/pile",
      "arxiv:2204.06745",
      "arxiv:2101.00027",
      "arxiv:2201.07311",
      "arxiv:2104.09864",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 574,
    "downloads": 20075,
    "source": "huggingface",
    "source_url": "https://huggingface.co/EleutherAI/gpt-neox-20b",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage:\n- en\ntags:\n- pytorch\n- causal-lm\nlicense: apache-2.0\ndatasets:\n- EleutherAI/pile\n---\n\nGPT-NeoX-20B is a 20 billion parameter autoregressive language model trained \non [the Pile](https://pile.eleuther.ai/) using the [GPT-NeoX \nlibrary](https://github.com/EleutherAI/gpt-neox). Its architecture intentionally \nresembles that of GPT-3, and is almost identical to that of [GPT-J-\n6B](https://huggingface.co/EleutherAI/gpt-j-6B). Its training dataset contains \na multitude of English-language texts, reflecting the general-purpose nature \nof this model. See the [accompanying paper](https://arxiv.org/abs/2204.06745) \nfor details about model architecture (including how it differs from GPT-3), \ntraining procedure, and additional evaluations.\n\n### Model details\n\n- Developed by: [EleutherAI](http://eleuther.ai)\n- Model type: Transformer-based Language Model\n- Language: English\n- Learn more: [GPT-NeoX-20B: An Open-Source Autoregressive Language \nModel](https://arxiv.org/abs/2204.06745). For details about the training dataset, \nsee [the Pile paper](https://arxiv.org/abs/2101.00027), and [its data\nsheet](https://arxiv.org/abs/2201.07311).\n- License: Apache 2.0\n- Contact: to ask questions about this model, join the [EleutherAI \nDiscord](https://discord.gg/zBGx3azzUn), and post them in `#release-discussion`. \nPlease read the existing GPT-NeoX-20B documentation before asking about the model \non Discord. For general correspondence: [contact@eleuther.\nai](mailto:contact@eleuther.ai).\n\n<figure style=\"width:30em\">\n\n| Hyperparameter         | Value       |\n| ---------------------- | ----------- |\n| n<sub>parameters</sub> | 20554567680 |\n| n<sub>layers</sub>     | 44          |\n| d<sub>model</sub>      | 6144        |\n| n<sub>heads</sub>      | 64          |\n| d<sub>head</sub>       | 96          |\n| n<sub>vocab</sub>      | 50257       |\n| Sequence Length        | 2048        |\n| Learning Rate          | 0.97 x 10<sup>-5</sup> |\n| Positional Encoding    | [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864) |\n</figure>\n\n### Uses and limitations\n\n#### Intended use\n\nGPT-NeoX-20B was developed primarily for research purposes. It learns an inner \nrepresentation of the English language that can be used to extract features \nuseful for downstream tasks.\n\nIn addition to scientific uses, you may also further fine-tune and adapt \nGPT-NeoX-20B for deployment, as long as your use is in accordance with the \nApache 2.0 license. This model works with the [Transformers \nLibrary](https://huggingface.co/docs/transformers/index). If you decide to use \npre-trained GPT-NeoX-20B as a basis for your fine-tuned model, please note that \nyou need to conduct your own risk and bias assessment. \n\n#### Out-of-scope use\n\nGPT-NeoX-20B is **not** intended for deployment as-is. It is not a product \nand cannot be used for human-facing interactions without supervision.\n\nGPT-NeoX-20B has not been fine-tuned for downstream tasks for which language \nmodels are commonly deployed, such as writing genre prose, or commercial \nchatbots. This means GPT-NeoX-20B will likely **not** respond to a given prompt \nthe way products such as ChatGPT do. This is because, unlike GPT-NeoX-20B, \nChatGPT was fine-tuned using methods such as Reinforcement Learning from Human \nFeedback (RLHF) to better “understand” human instructions and dialogue.\n\nThis model is English-language only, and thus cannot be used for translation\nor generating text in other languages.\n\n#### Limitations and biases\n\nThe core functionality of GPT-NeoX-20B is to take a string of text and predict \nthe next token. Remember that the statistically most likely next token need \nnot result in the most “accurate” text. Never rely on GPT-NeoX-20B to produce \nfactually accurate output.\n\nThis model was trained on [the Pile](https://pile.eleuther.ai/), a dataset \nknown to contain profanity and texts that are lewd or otherwise offensive. \nSee [Section 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a \ndiscussion of documented biases with regards to gender, religion, and race. \nGPT-NeoX-20B may produce socially unacceptable or undesirable text, *even if*\n the prompt itself does not include anything explicitly offensive. \n\nWe recommend curating the outputs of this model before presenting it to a human \nreader. Please inform your audience that you are using artificially generated \ntext. \n\n#### How to use\n If you simply want to try out some prompts, check out [this \n playground](https://20b.eleuther.ai/).\n \n GPT-NeoX-20B can be loaded using the `AutoModelForCausalLM` functionality:\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n```\n\n### Training\n\n#### Training dataset\n\nThe Pile is a 825GiB general-purpose dataset in English. It was created by \nEleutherAI specifically for training large language models. It contains texts \nfrom 22 diverse sources, roughly broken down into five categories: academic \nwriting (e.g. arXiv), internet (e.g. CommonCrawl), prose (e.g. Project \nGutenberg), dialogue (e.g. YouTube subtitles), and miscellaneous (e.g. GitHub, \nEnron Emails). See [the Pile paper](https://arxiv.org/abs/2101.00027) for \na breakdown of all data sources, methodology, and a discussion of ethical \nimplications. Consult [the datasheet](https://arxiv.org/abs/2201.07311) for \nmore detailed documentation about the Pile and its component datasets. The \nPile can be downloaded from the [official website](https://pile.eleuther.ai/), \nor from a [community mirror](https://the-eye.eu/public/AI/pile/).\n\nThe Pile was **not** deduplicated before being used to train GPT-NeoX-20B.\n\n#### Training procedure\n\nGPT-NeoX-20B was trained with a batch size of approximately 3.15M tokens \n(1538 sequences of 2048 tokens each), for a total of 150,000 steps. Tensor \nparallelism and pipeline parallelism were used to distribute the model across \nGPUs. Additional details about the training procedure are in [Section 3 of \nthe accompanying paper](https://arxiv.org/abs/2204.06745).\n\n\n### Evaluations\n\n<figure style=\"width:55em\">\n\n| Model         | OpenAI’s LAMBADA | SciQ          | PIQA          | TriviaQA      | ARC (Challenge) |\n| ------------- | :--------------: | :-----------: | :-----------: | :-----------: | :-------------: |\n| GPT-J-6B      | 0.683 ± 0.006    | 0.910 ± 0.009 | 0.752 ± 0.010 | 0.170 ± 0.004 | 0.340 ± 0.014   |\n| FairSeq 6.7B  | 0.673 ± 0.007    | 0.895 ± 0.010 | 0.762 ± 0.010 | 0.221 ± 0.004 | 0.329 ± 0.014   |\n| GPT-3 Curie   | 0.693 ± 0.006    | 0.918 ± 0.009 | 0.767 ± 0.010 | 0.196 ± 0.004 | 0.334 ± 0.014   |\n| FairSeq 13B   | 0.709 ± 0.006    | 0.910 ± 0.009 | 0.769 ± 0.010 | 0.270 ± 0.004 | 0.345 ± 0.014   |\n| GPT-NeoX-20B  | 0.720 ± 0.006    | 0.928 ± 0.008 | 0.779 ± 0.010 | 0.259 ± 0.004 | 0.380 ± 0.014   |\n| GPT-3 DaVinci | 0.752 ± 0.006    | 0.949 ± 0.007 | 0.791 ± 0.009 | 0.409 ± 0.005 | 0.435 ± 0.014   |\n<figcaption>Zero-shot performance on selected natural language tasks.</figcaption>\n</figure>\n\nThis is a heavily abridged version of the evaluation results. Appendix D of the\n [GPT-NeoX-20B paper](https://arxiv.org/abs/2204.06745) compares more model \nsizes, and contains additional evaluations, including on: zero and five-shot \nnatural language tasks, zero and five-shot Basic Arithmetic and MATH, \nand zero-shot Hendrycks tasks.\n\n### BibTeX\n\nTo cite the GPT-NeoX-20B paper:\n\n```\n@misc{https://doi.org/10.48550/arxiv.2204.06745,\n  doi = {10.48550/ARXIV.2204.06745},\n  \n  url = {https://arxiv.org/abs/2204.06745},\n  \n  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},\n  \n  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-neox-20b)\n\n| Metric                | Value                     |\n|-----------------------|---------------------------|\n| Avg.                  | 36.02   |\n| ARC (25-shot)         | 45.73          |\n| HellaSwag (10-shot)   | 73.45    |\n| MMLU (5-shot)         | 25.0         |\n| TruthfulQA (0-shot)   | 31.61   |\n| Winogrande (5-shot)   | 68.9   |\n| GSM8K (5-shot)        | 2.43        |\n| DROP (3-shot)         | 5.04         |\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":20739117584,\"storage_bytes\":123696860531,\"files_count\":102,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"GPTNeoXForCausalLM\"],\"model_type\":\"gpt_neox\",\"tokenizer_config\":{\"unk_token\":\"<|endoftext|>\",\"bos_token\":\"<|endoftext|>\",\"eos_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:EleutherAI:gpt-neox\",\"source_url\":\"https://github.com/EleutherAI/gpt-neox\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.06745\",\"source_url\":\"https://arxiv.org/abs/2204.06745\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2101.00027\",\"source_url\":\"https://arxiv.org/abs/2101.00027\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2201.07311\",\"source_url\":\"https://arxiv.org/abs/2201.07311\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.09864\",\"source_url\":\"https://arxiv.org/abs/2104.09864\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "55f472100ec68d7e52ca1c24d0a91256",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/EleutherAI/gpt-neox-20b\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:crucibleai:controlnetmediapipeface",
    "name": "ControlNetMediaPipeFace",
    "author": "CrucibleAI",
    "description": "--- language: - en thumbnail: '' tags: - controlnet - laion - face - mediapipe - image-to-image license: openrail base_model: stabilityai/stable-diffusion-2-1-base datasets: - LAION-Face - LAION pipeline_tag: image-to-image --- - Overview: Samples, Contents, and Construction - Usage: Downloading, Training, and Inference - License - Credits and Thanks This dataset is designed to train a ControlNet with human facial expressions. It includes keypoints for pupils to allow gaze direction. Training...",
    "tags": [
      "diffusers",
      "safetensors",
      "controlnet",
      "laion",
      "face",
      "mediapipe",
      "image-to-image",
      "en",
      "dataset:laion-face",
      "dataset:laion",
      "arxiv:2302.05543",
      "arxiv:2112.10752",
      "arxiv:2210.08402",
      "base_model:stabilityai/stable-diffusion-2-1-base",
      "license:openrail",
      "region:us"
    ],
    "pipeline_tag": "image-to-image",
    "likes": 574,
    "downloads": 1201,
    "source": "huggingface",
    "source_url": "https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage:\n- en\nthumbnail: ''\ntags:\n- controlnet\n- laion\n- face\n- mediapipe\n- image-to-image\nlicense: openrail\nbase_model: stabilityai/stable-diffusion-2-1-base\ndatasets:\n- LAION-Face\n- LAION\npipeline_tag: image-to-image\n---\n\n# ControlNet LAION Face Dataset\n\n## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n\n## Samples:\n\nCherry-picked from ControlNet + Stable Diffusion v2.1 Base\n\n|Input|Face Detection|Output|\n|:---:|:---:|:---:|\n|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_source.jpg\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_annotation.png\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_result.png\">|\n|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_source.jpg\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_annotation.png\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_result.png\">|\n|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_source.jpg\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_annotation.png\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_result.png\">|\n|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_source.jpg\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_annotation.png\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_result.png\">|\n|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_source.jpg\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_annotation.png\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_result.png\">|\n|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_source.jpg\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_annotation.png\">|<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_result.png\">|\n\nImages with multiple faces are also supported:\n\n<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_source.jpg\">\n\n<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_annotation.png\">\n\n<img src=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_result.png\">\n\n\n## Dataset Contents:\n\n- train_laion_face.py - Entrypoint for ControlNet training.\n- laion_face_dataset.py - Code for performing dataset iteration.  Cropping and resizing happens here.\n- tool_download_face_targets.py - A tool to read metadata.json and populate the target folder.\n- tool_generate_face_poses.py - The original file used to generate the source images.  Included for reproducibility, but not required for training.\n- training/laion-face-processed/prompt.jsonl - Read by laion_face_dataset.  Includes prompts for the images.\n- training/laion-face-processed/metadata.json - Excerpts from LAION for the relevant data.  Also used for downloading the target dataset.\n- training/laion-face-processed/source/xxxxxxxxx.jpg - Images with detections performed.  Generated from the target images.\n- training/laion-face-processed/target/xxxxxxxxx.jpg - Selected images from LAION Face.\n\n## Dataset Construction:\n\nSource images were generated by pulling slice 00000 from LAION Face and passing them through MediaPipe's face detector with special configuration parameters.  \n\nThe colors and line thicknesses used for MediaPipe are as follows:\n\n```\nf_thick = 2\nf_rad = 1\nright_iris_draw = DrawingSpec(color=(10, 200, 250), thickness=f_thick, circle_radius=f_rad)\nright_eye_draw = DrawingSpec(color=(10, 200, 180), thickness=f_thick, circle_radius=f_rad)\nright_eyebrow_draw = DrawingSpec(color=(10, 220, 180), thickness=f_thick, circle_radius=f_rad)\nleft_iris_draw = DrawingSpec(color=(250, 200, 10), thickness=f_thick, circle_radius=f_rad)\nleft_eye_draw = DrawingSpec(color=(180, 200, 10), thickness=f_thick, circle_radius=f_rad)\nleft_eyebrow_draw = DrawingSpec(color=(180, 220, 10), thickness=f_thick, circle_radius=f_rad)\nmouth_draw = DrawingSpec(color=(10, 180, 10), thickness=f_thick, circle_radius=f_rad)\nhead_draw = DrawingSpec(color=(10, 200, 10), thickness=f_thick, circle_radius=f_rad)\n\niris_landmark_spec = {468: right_iris_draw, 473: left_iris_draw}\n```\n\nWe have implemented a method named `draw_pupils` which modifies some functionality from MediaPipe.  It exists as a stopgap until some pending changes are merged.\n\n\n# Usage:\n\nThe containing ZIP file should be decompressed into the root of the ControlNet directory.  The `train_laion_face.py`, `laion_face_dataset.py`, and other `.py` files should sit adjacent to `tutorial_train.py` and `tutorial_train_sd21.py`.  We are assuming a checkout of the ControlNet repo at 0acb7e5, but there is no direct dependency on the repository.\n\n## Downloading:\n\nFor copyright reasons, we cannot include the original target files.  We have provided a script (tool_download_face_targets.py) which will read from training/laion-face-processed/metadata.json and populate the target folder.  This file has no requirements, but will use tqdm if it is installed.\n\n## Training:\n\nWhen the targets folder is fully populated, training can be run on a machine with at least 24 gigabytes of VRAM.  Our model was trained for 200 hours (four epochs) on an A6000.\n\n```bash\npython tool_add_control.py ./models/v1-5-pruned-emaonly.ckpt ./models/controlnet_sd15_laion_face.ckpt\npython ./train_laion_face_sd15.py\n```\n\n## Inference:\n\nWe have provided `gradio_face2image.py`.  Update the following two lines to point them to your trained model.\n\n```\nmodel = create_model('./models/cldm_v21.yaml').cpu()  # If you fine-tune on SD2.1 base, this does not need to change.\nmodel.load_state_dict(load_state_dict('./models/control_sd21_openpose.pth', location='cuda'))\n```\n\nThe model has some limitations: while it is empirically better at tracking gaze and mouth poses than previous attempts, it may still ignore controls.  Adding details to the prompt like, \"looking right\" can abate bad behavior. \n\n## 🧨 Diffusers\n\nIt is recommended to use the checkpoint with [Stable Diffusion 2.1 - Base](stabilityai/stable-diffusion-2-1-base) as the checkpoint has been trained on it.\nExperimentally, the checkpoint can be used with other diffusion models such as dreamboothed stable diffusion.\n\nTo use with Stable Diffusion 1.5, insert `subfolder=\"diffusion_sd15\"` into the from_pretrained arguments.  A v1.5 half-precision variant is provided but untested.\n\n1. Install `diffusers` and related packages:\n```\n$ pip install diffusers transformers accelerate\n```\n\n2. Run code:\n```py\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    \"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_annotation.png\"\n)\n\n# Stable Diffusion 2.1-base:\ncontrolnet = ControlNetModel.from_pretrained(\"CrucibleAI/ControlNetMediaPipeFace\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\t\"stabilityai/stable-diffusion-2-1-base\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\n# OR\n# Stable Diffusion 1.5:\ncontrolnet = ControlNetModel.from_pretrained(\"CrucibleAI/ControlNetMediaPipeFace\", subfolder=\"diffusion_sd15\")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n# Remove if you do not have xformers installed\n# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n# for installation instructions\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\n\nimage = pipe(\"a happy family at a dentist advertisement\", image=image, num_inference_steps=30).images[0]\nimage.save('./images.png')\n```\n\n\n# License:\n\n### Source Images: (/training/laion-face-processed/source/)\nThis work is marked with CC0 1.0. To view a copy of this license, visit http://creativecommons.org/publicdomain/zero/1.0\n\n### Trained Models:\nOur trained ControlNet checkpoints are released under CreativeML Open RAIL-M.\n\n### Source Code:\nlllyasviel/ControlNet is licensed under the Apache License 2.0\n\nOur modifications are released under the same license.\n\n\n# Credits and Thanks:\n\nGreatest thanks to Zhang et al. for ControlNet, Rombach et al. (StabilityAI) for Stable Diffusion, and Schuhmann et al. for LAION.\n\nSample images for this document were obtained from Unsplash and are CC0.\n\n```\n@misc{zhang2023adding,\n  title={Adding Conditional Control to Text-to-Image Diffusion Models}, \n  author={Lvmin Zhang and Maneesh Agrawala},\n  year={2023},\n  eprint={2302.05543},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n\n@misc{rombach2021highresolution,\n      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},\n      year={2021},\n      eprint={2112.10752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{schuhmann2022laion5b,\n      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, \n      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},\n      year={2022},\n      eprint={2210.08402},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\nThis project was made possible by Crucible AI.",
    "meta_json": "{\"pipeline_tag\":\"image-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":34147689491,\"files_count\":47,\"spaces_count\":20,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2302.05543\",\"source_url\":\"https://arxiv.org/abs/2302.05543\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2112.10752\",\"source_url\":\"https://arxiv.org/abs/2112.10752\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.08402\",\"source_url\":\"https://arxiv.org/abs/2210.08402\"}]",
    "canonical_id": null,
    "license_spdx": "OpenRAIL",
    "compliance_status": "approved",
    "quality_score": 77.6,
    "content_hash": "414bff0a823c910296b264b53da5cd50",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stabilityai:triposr",
    "name": "TripoSR",
    "author": "stabilityai",
    "description": "--- datasets: - allenai/objaverse tags: - 3d extra_gated_fields: Name: text Email: text Country: text Organization or Affiliation: text I ALLOW Stability AI to email me about new model releases: checkbox license: mit pipeline_tag: image-to-3d --- > Try our new model: **SF3D** with several improvements such as faster generation and more game-ready assets. > > The model is available here and we also have a demo. TripoSR is a fast and feed-forward 3D generative model developed in collaboration b...",
    "tags": [
      "3d",
      "image-to-3d",
      "dataset:allenai/objaverse",
      "arxiv:2311.04400",
      "arxiv:2403.02151",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "image-to-3d",
    "likes": 572,
    "downloads": 28641,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stabilityai/TripoSR",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ndatasets:\n- allenai/objaverse\ntags:\n- 3d\nextra_gated_fields:\n  Name: text\n  Email: text\n  Country: text\n  Organization or Affiliation: text\n  I ALLOW Stability AI to email me about new model releases: checkbox\nlicense: mit\npipeline_tag: image-to-3d\n---\n\n> Try our new model: **SF3D** with several improvements such as faster generation and more game-ready assets.\n> \n> The model is available [here](https://huggingface.co/stabilityai/stable-fast-3d) and we also have a [demo](https://huggingface.co/spaces/stabilityai/stable-fast-3d).\n \n# TripoSR\n![](figures/input800.mp4)\nTripoSR is a fast and feed-forward 3D generative model developed in collaboration between Stability AI and Tripo AI.\n\n\n## Model Details\n\n### Model Description\n\nWe closely follow [LRM](https://arxiv.org/abs/2311.04400) network architecture for the model design, where TripoSR incorporates a series of technical advancements over the LRM model in terms of both data curation as well as model and training improvements. For more technical details and evaluations, please refer to [our tech report](https://arxiv.org/abs/2403.02151).\n\n* **Developed by**: [Stability AI](https://stability.ai/), [Tripo AI](https://tripo3d.ai/)\n* **Model type**: Feed-forward 3D reconstruction from a single image\n* **License**: MIT\n* **Hardware**: We train `TripoSR` for 5 days on 22 GPU nodes each with 8 A100 40GB GPUs\n\n### Model Sources\n\n* **Repository**: https://github.com/VAST-AI-Research/TripoSR\n* **Tech report**: https://arxiv.org/abs/2403.02151\n* **Demo**: https://huggingface.co/spaces/stabilityai/TripoSR\n\n### Training Dataset\n\nWe use renders from the [Objaverse](https://objaverse.allenai.org/objaverse-1.0) dataset, utilizing our enhanced rendering method that more closely replicate the distribution of images found in the real world, significantly improving our model’s ability to generalize. We selected a carefully curated subset of the Objaverse dataset for the training data, which is available under the CC-BY license. \n\n\n## Usage\n\n* For usage instructions, please refer to our [TripoSR GitHub repository](https://github.com/VAST-AI-Research/TripoSR)\n\n* You can also try it in [our gradio demo](https://huggingface.co/spaces/stabilityai/TripoSR)\n\n\n### Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate 3D models that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.",
    "meta_json": "{\"pipeline_tag\":\"image-to-3d\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":5262702691,\"files_count\":6,\"spaces_count\":60,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:VAST-AI-Research:TripoSR\",\"source_url\":\"https://github.com/VAST-AI-Research/TripoSR\"},{\"type\":\"has_code\",\"target_id\":\"github:VAST-AI-Research:TripoSR\",\"source_url\":\"https://github.com/VAST-AI-Research/TripoSR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.04400\",\"source_url\":\"https://arxiv.org/abs/2311.04400\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.02151\",\"source_url\":\"https://arxiv.org/abs/2403.02151\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "c47993efce76945342e2749d12c3e5c7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stabilityai/TripoSR\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:fofr:sdxl-emoji",
    "name": "sdxl-emoji",
    "author": "fofr",
    "description": "--- license: creativeml-openrail-m tags: - text-to-image - stable-diffusion - lora - diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 pivotal_tuning: true textual_embeddings: embeddings.pti instance_prompt: an <s0><s1> emoji inference: false --- !lora_image > Grab your replicate token here You may also do inference via the API with Node.js or curl, and locally with COG and Docker, check out the Replicate API page for this model Replicate SDXL LoRAs are trained with Pivotal Tunin...",
    "tags": [
      "diffusers",
      "text-to-image",
      "stable-diffusion",
      "lora",
      "license:creativeml-openrail-m",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 572,
    "downloads": 125,
    "source": "huggingface",
    "source_url": "https://huggingface.co/fofr/sdxl-emoji",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\ntags:\n  - text-to-image\n  - stable-diffusion\n  - lora\n  - diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\npivotal_tuning: true\ntextual_embeddings: embeddings.pti\ninstance_prompt: an <s0><s1> emoji\ninference: false\n---\n# sdxl-emoji LoRA by [fofr](https://replicate.com/fofr)\n### An SDXL fine-tune based on Apple Emojis\n\n![lora_image](https://replicate.delivery/pbxt/a3z81v5vwlKfLq1H5uBqpVmkHalOVup0jSLma9E2UaF3tawIA/out-0.png)\n>\n\n## Inference with Replicate API\nGrab your replicate token [here](https://replicate.com/account)\n```bash\npip install replicate\nexport REPLICATE_API_TOKEN=r8_*************************************\n```\n\n```py\nimport replicate\n\noutput = replicate.run(\n    \"sdxl-emoji@sha256:dee76b5afde21b0f01ed7925f0665b7e879c50ee718c5f78a9d38e04d523cc5e\",\n    input={\"prompt\": \"A TOK emoji of a man\"}\n)\nprint(output)\n```\nYou may also do inference via the API with Node.js or curl, and locally with COG and Docker, [check out the Replicate API page for this model](https://replicate.com/fofr/sdxl-emoji/api)\n\n## Inference with 🧨 diffusers\nReplicate SDXL LoRAs are trained with Pivotal Tuning, which combines training a concept via Dreambooth LoRA with training a new token with Textual Inversion.\nAs `diffusers` doesn't yet support textual inversion for SDXL, we will use cog-sdxl `TokenEmbeddingsHandler` class.\n\nThe trigger tokens for your prompt will be `<s0><s1>`\n\n```shell\npip install diffusers transformers accelerate safetensors huggingface_hub\ngit clone https://github.com/replicate/cog-sdxl cog_sdxl\n```\n\n```py\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom diffusers import DiffusionPipeline\nfrom cog_sdxl.dataset_and_utils import TokenEmbeddingsHandler\nfrom diffusers.models import AutoencoderKL\n\npipe = DiffusionPipeline.from_pretrained(\n        \"stabilityai/stable-diffusion-xl-base-1.0\",\n        torch_dtype=torch.float16,\n        variant=\"fp16\",\n).to(\"cuda\")\n\npipe.load_lora_weights(\"fofr/sdxl-emoji\", weight_name=\"lora.safetensors\")\n\ntext_encoders = [pipe.text_encoder, pipe.text_encoder_2]\ntokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n\nembedding_path = hf_hub_download(repo_id=\"fofr/sdxl-emoji\", filename=\"embeddings.pti\", repo_type=\"model\")\nembhandler = TokenEmbeddingsHandler(text_encoders, tokenizers)\nembhandler.load_embeddings(embedding_path)\nprompt=\"A <s0><s1> emoji of a man\"\nimages = pipe(\n    prompt,\n    cross_attention_kwargs={\"scale\": 0.8},\n).images\n#your output image\nimages[0]\n```\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":185968776,\"files_count\":5,\"spaces_count\":20,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:replicate:cog-sdxl\",\"source_url\":\"https://github.com/replicate/cog-sdxl\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "72a1c3c596db84c658f91b72c117ee39",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/fofr/sdxl-emoji\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen2.5-vl-3b-instruct",
    "name": "Qwen2.5-VL-3B-Instruct",
    "author": "Qwen",
    "description": "--- license_name: qwen-research license_link: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text tags: - multimodal library_name: transformers --- <a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> In the past five months since Qwen2-VL’s release, num...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2_5_vl",
      "image-to-text",
      "multimodal",
      "image-text-to-text",
      "conversational",
      "en",
      "arxiv:2309.00071",
      "arxiv:2409.12191",
      "arxiv:2308.12966",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 572,
    "downloads": 7729205,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "\n---\nlicense_name: qwen-research\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Qwen2.5-VL-3B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VL’s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg\" width=\"80%\"/>\n<p>\n\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 3B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n| Benchmark | InternVL2.5-4B |Qwen2-VL-7B |Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MMMU<sub>val</sub>  | 52.3 | 54.1 | 53.1| \n| MMMU-Pro<sub>val</sub>  | **32.7** | 30.5 | 31.6|\n| AI2D<sub>test</sub> | 81.4 | **83.0** | 81.5 |\n| DocVQA<sub>test</sub>  | 91.6 | 94.5 | **93.9** | \n| InfoVQA<sub>test</sub>  | 72.1 | 76.5 | **77.1** |\n| TextVQA<sub>val</sub>  | 76.8 | **84.3** | 79.3|\n| MMBench-V1.1<sub>test</sub>  | 79.3 | **80.7** | 77.6 | \n| MMStar | 58.3 | **60.7** | 55.9 | \n| MathVista<sub>testmini</sub>  | 60.5 | 58.2 | **62.3** |\n| MathVision<sub>full</sub>  | 20.9 | 16.3  | **21.2** |\n\n\n### Video benchmark\n| Benchmark | InternVL2.5-4B | Qwen2-VL-7B | Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MVBench | 71.6 | 67.0 | 67.0 |\n| VideoMME | 63.6/62.3 | 69.0/63.3 | 67.6/61.5 |\n| MLVU | 48.3 | - | 68.2 |\n| LVBench | - | - | 43.3 |\n| MMBench-Video | 1.73 | 1.44 | 1.63 |\n| EgoSchema | - | - | 64.8 |\n| PerceptionTest | - | - | 66.9 |\n| TempCompass | - | - | 64.4 |\n| LongVideoBench | 55.2 | 55.6 | 54.2 |\n| CharadesSTA/mIoU | - | - | 38.8 |\n\n\n### Agent benchmark\n| Benchmarks              | Qwen2.5-VL-3B |\n|-------------------------|---------------|\n| ScreenSpot              |     55.5    |\n| ScreenSpot Pro          |     23.9    |\n| AITZ_EM                 |  \t76.9    |\n| Android Control High_EM |    \t63.7    |\n| Android Control Low_EM  |  \t22.2    |\n| AndroidWorld_SR         | \t90.8  \t|\n| MobileMiniWob++_SR      | \t67.9    |\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with 🤖 ModelScope and 🤗 Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using 🤗  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-3B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors=\"pt\",\n    **video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | ✅  | ✅   |\n| torchvision < 0.19.0  | ❌  | ❌   |\n| decord      | ✅  | ❌   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### 🤖 ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```\n{\n\t...,\n    \"type\": \"yarn\",\n    \"mrope_section\": [\n        16,\n        24,\n        24\n    ],\n    \"factor\": 4,\n    \"original_max_position_embeddings\": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":3754622976,\"storage_bytes\":7509337976,\"files_count\":14,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2_5_VLForConditionalGeneration\"],\"model_type\":\"qwen2_5_vl\",\"processor_config\":{\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"},\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5-VL\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5-VL\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:dmlc:decord\",\"source_url\":\"https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2409.12191\",\"source_url\":\"https://arxiv.org/abs/2409.12191\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2308.12966\",\"source_url\":\"https://arxiv.org/abs/2308.12966\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 67.6,
    "content_hash": "6e2dd9e5e50a2e2c613a779533661cd7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-v3.2-speciale",
    "name": "DeepSeek-V3.2-Speciale",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" targ...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v32",
      "text-generation",
      "base_model:deepseek-ai/deepseek-v3.2-exp-base",
      "license:mit",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 573,
    "downloads": 9310,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale/resolve/main/assets/benchmark.png",
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf\"><b>Technical Report</b>👁️</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* 🥇 **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align=\"center\">\n <img src=\"assets/benchmark.png\" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a \"thinking with tools\" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model's text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\nencode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: \"<｜begin▁of▁sentence｜><｜User｜>hello<｜Assistant｜></think>Hello! I am DeepSeek.<｜end▁of▁sentence｜><｜User｜>1+1=?<｜Assistant｜><think>\"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output that the model might occasionally generate. It is not suitable for production use without robust error handling.\n3. A new role named `developer` has been introduced in the chat template. This role is dedicated exclusively to search agent scenarios and is designated for no other tasks. The official API does not accept messages assigned to `developer`.\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.2 and DeepSeek-V3.2-Speciale are the same as DeepSeek-V3.2-Exp. Please visit [DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) repo for more information about running this model locally.\n\nUsage Recommendations:\n\n1. For local deployment, we recommend setting the sampling parameters to `temperature = 1.0, top_p = 0.95`.\n2. Please note that the DeepSeek-V3.2-Speciale variant is designed exclusively for deep reasoning tasks and does not support the tool-calling functionality.\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2025deepseekv32,\n      title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":685396921376,\"storage_bytes\":689484423011,\"files_count\":192,\"spaces_count\":0,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV32ForCausalLM\"],\"model_type\":\"deepseek_v32\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<｜begin▁of▁sentence｜>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<｜end▁of▁sentence｜>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<｜end▁of▁sentence｜>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3.2-Exp\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 82.6,
    "content_hash": "3bcfd457e471fb9bcbb42b9df7e79361",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale/resolve/main/assets/benchmark.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nousresearch:yarn-mistral-7b-128k",
    "name": "Yarn-Mistral-7b-128k",
    "author": "NousResearch",
    "description": "--- datasets: - emozilla/yarn-train-tokenized-16k-mistral metrics: - perplexity library_name: transformers license: apache-2.0 language: - en --- Preprint (arXiv) GitHub !yarn Nous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method. It is an extension of Mistral-7B-v0.1 and supports a 128k token context window. To use, pass when loading the model, for example In addition you will ne...",
    "tags": [
      "transformers",
      "pytorch",
      "mistral",
      "text-generation",
      "custom_code",
      "en",
      "dataset:emozilla/yarn-train-tokenized-16k-mistral",
      "arxiv:2309.00071",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 571,
    "downloads": 2248,
    "source": "huggingface",
    "source_url": "https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ndatasets:\n- emozilla/yarn-train-tokenized-16k-mistral\nmetrics:\n- perplexity\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\n---\n\n# Model Card: Nous-Yarn-Mistral-7b-128k\n\n[Preprint (arXiv)](https://arxiv.org/abs/2309.00071)  \n[GitHub](https://github.com/jquesnelle/yarn)\n![yarn](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n\n## Model Description\n\nNous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method.\nIt is an extension of [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) and supports a 128k token context window.\n\nTo use, pass `trust_remote_code=True` when loading the model, for example\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"NousResearch/Yarn-Mistral-7b-128k\",\n  use_flash_attention_2=True,\n  torch_dtype=torch.bfloat16,\n  device_map=\"auto\",\n  trust_remote_code=True)\n```\n\nIn addition you will need to use the latest version of `transformers` (until 4.35 comes out)\n```sh\npip install git+https://github.com/huggingface/transformers\n```\n\n## Benchmarks\n\nLong context benchmarks:\n| Model | Context Window | 8k PPL | 16k PPL | 32k PPL | 64k PPL | 128k PPL |\n|-------|---------------:|------:|----------:|-----:|-----:|------------:|\n| [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 8k | 2.96 | - | - | - | - |\n| [Yarn-Mistral-7b-64k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) | 64k | 3.04 | 2.65 | 2.44 | 2.20 | - |\n| [Yarn-Mistral-7b-128k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) | 128k | 3.08 | 2.68 | 2.47 | 2.24 | 2.19 |\n\nShort context benchmarks showing that quality degradation is minimal:\n| Model | Context Window | ARC-c | Hellaswag | MMLU | Truthful QA |\n|-------|---------------:|------:|----------:|-----:|------------:|\n| [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 8k | 59.98 | 83.31 | 64.16 | 42.15 |\n| [Yarn-Mistral-7b-64k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) | 64k | 59.38 | 81.21 | 61.32 | 42.50 |\n| [Yarn-Mistral-7b-128k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) | 128k | 58.87 | 80.58 | 60.64 | 42.46 |\n\n## Collaborators\n\n - [bloc97](https://github.com/bloc97): Methods, paper and evals\n - [@theemozilla](https://twitter.com/theemozilla): Methods, paper, model training, and evals\n - [@EnricoShippole](https://twitter.com/EnricoShippole): Model training\n - [honglu2875](https://github.com/honglu2875): Paper and evals\n\nThe authors would like to thank LAION AI for their support of compute for this model.\nIt was trained on the [JUWELS](https://www.fz-juelich.de/en/ias/jsc/systems/supercomputers/juwels) supercomputer.",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":28967555174,\"files_count\":14,\"spaces_count\":34,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_mistral.MistralConfig\",\"AutoModelForCausalLM\":\"modeling_mistral_yarn.MistralForCausalLM\"},\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"</s>\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":true}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:jquesnelle:yarn\",\"source_url\":\"https://github.com/jquesnelle/yarn\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "74a499e02b21468d0871814eebe88ff5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-3n-e2b-it-litert-preview",
    "name": "gemma-3n-E2B-it-litert-preview",
    "author": "google",
    "description": "",
    "tags": [
      "image-text-to-text",
      "arxiv:1905.07830",
      "arxiv:1905.10044",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1705.03551",
      "arxiv:1911.01547",
      "arxiv:1907.10641",
      "arxiv:1903.00161",
      "arxiv:2210.03057",
      "arxiv:2502.12404",
      "arxiv:2411.19799",
      "arxiv:2009.03300",
      "arxiv:2502.21228",
      "arxiv:2311.12022",
      "arxiv:2403.07974",
      "arxiv:2108.07732",
      "arxiv:2107.03374",
      "license:gemma",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 571,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-3n-E2B-it-litert-preview",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":3136926777,\"files_count\":3,\"spaces_count\":0,\"gated\":\"manual\",\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1903.00161\",\"source_url\":\"https://arxiv.org/abs/1903.00161\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.03057\",\"source_url\":\"https://arxiv.org/abs/2210.03057\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.12404\",\"source_url\":\"https://arxiv.org/abs/2502.12404\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2411.19799\",\"source_url\":\"https://arxiv.org/abs/2411.19799\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.21228\",\"source_url\":\"https://arxiv.org/abs/2502.21228\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.12022\",\"source_url\":\"https://arxiv.org/abs/2311.12022\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.07974\",\"source_url\":\"https://arxiv.org/abs/2403.07974\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 37.6,
    "content_hash": "581a1d8c55441f56830427924f113f38",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-3n-E2B-it-litert-preview\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen2.5-coder-7b-instruct",
    "name": "Qwen2.5-Coder-7B-Instruct",
    "author": "Qwen",
    "description": "--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE language: - en base_model: - Qwen/Qwen2.5-Coder-7B pipeline_tag: text-generation library_name: transformers tags: - code - codeqwen - chat - qwen - qwen-coder --- <a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "code",
      "codeqwen",
      "chat",
      "qwen",
      "qwen-coder",
      "conversational",
      "en",
      "arxiv:2409.12186",
      "arxiv:2309.00071",
      "arxiv:2407.10671",
      "base_model:qwen/qwen2.5-coder-7b",
      "base_model:finetune:qwen/qwen2.5-coder-7b",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 570,
    "downloads": 671059,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-Coder-7B\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- code\n- codeqwen\n- chat\n- qwen\n- qwen-coder\n---\n\n\n# Qwen2.5-Coder-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n- **Long-context Support** up to 128K tokens.\n\n**This repo contains the instruction-tuned 7B Qwen2.5-Coder model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n  \nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/), [GitHub](https://github.com/QwenLM/Qwen2.5-Coder), [Documentation](https://qwen.readthedocs.io/en/latest/), [Arxiv](https://arxiv.org/abs/2409.12186).\n\n## Requirements\n\nThe code of Qwen2.5-Coder has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7615616512,\"storage_bytes\":30462543728,\"files_count\":14,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5-Coder\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5-Coder\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2409.12186\",\"source_url\":\"https://arxiv.org/abs/2409.12186\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2407.10671\",\"source_url\":\"https://arxiv.org/abs/2407.10671\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "135938a1f3d2efb2a2da64876a40f0b2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-3.2-11b-vision",
    "name": "Llama-3.2-11B-Vision",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "mllama",
      "image-to-text",
      "facebook",
      "meta",
      "pytorch",
      "llama",
      "llama-3",
      "image-text-to-text",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "license:llama3.2",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 570,
    "downloads": 12037,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":10642941475,\"storage_bytes\":52480505184,\"files_count\":20,\"spaces_count\":70,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"MllamaForConditionalGeneration\"],\"model_type\":\"mllama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"eos_token\":\"<|end_of_text|>\",\"pad_token\":\"<|finetune_right_pad_id|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.05149\",\"source_url\":\"https://arxiv.org/abs/2204.05149\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.2",
    "compliance_status": "approved",
    "quality_score": 37.6,
    "content_hash": "d97adc0f3aeb21aa05d38d7b4820e00c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-3.2-11B-Vision\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen2.5-vl-72b-instruct",
    "name": "Qwen2.5-VL-72B-Instruct",
    "author": "Qwen",
    "description": "--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text tags: - multimodal library_name: transformers --- <a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> In the past five months since Qwen2-VL’s relea...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2_5_vl",
      "image-to-text",
      "multimodal",
      "image-text-to-text",
      "conversational",
      "en",
      "arxiv:2309.00071",
      "arxiv:2409.12191",
      "arxiv:2308.12966",
      "license:other",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 569,
    "downloads": 114512,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "\n---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Qwen2.5-VL-72B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VL’s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg\" width=\"80%\"/>\n<p>\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 72B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n|      Benchmarks        | GPT4o     | Claude3.5 Sonnet  | Gemini-2-flash  | InternVL2.5-78B | Qwen2-VL-72B | Qwen2.5-VL-72B |\n|-----------------------|-----------|-------------------|-----------------|-----------------|--------------|----------------|\n| MMMU<sub>val</sub>    | 70.3      | 70.4              | 70.7            | 70.1                 | 64.5         | 70.2          |\n| MMMU_Pro              | 54.5      | 54.7              | 57.0            | 48.6              | 46.2         | 51.1           |\n| MathVista_MINI        | 63.8      | 65.4              | 73.1            | 76.6                | 70.5         | 74.8           |\n| MathVision_FULL       | 30.4      | 38.3              | 41.3            | 32.2               | 25.9         | 38.1           |\n| Hallusion Bench       | 55.0      | 55.16             |                | 57.4             | 58.1         | 55.16           |\n| MMBench_DEV_EN_V11    | 82.1      | 83.4              | 83.0            | 88.5             | 86.6         | 88           |\n| AI2D_TEST             | 84.6      | 81.2              |                 | 89.1           | 88.1         | 88.4           |\n| ChartQA_TEST          | 86.7      | 90.8              | 85.2            | 88.3               | 88.3         | 89.5           |\n| DocVQA_VAL            | 91.1      | 95.2              | 92.1            | 96.5             | 96.1         |      96.4      |\n| MMStar                | 64.7      | 65.1              | 69.4            | 69.5             | 68.3         |       70.8         |\n| MMVet_turbo           | 69.1      |  70.1              |                 | 72.3           | 74.0         |       76.19         |\n| OCRBench              | 736       | 788               |                 | 854               | 877          |         885       |\n| OCRBench-V2(en/zh)    |  46.5/32.3 |  45.2/39.6         | 51.9/43.1       | 45/46.2     | 47.8/46.1    | 61.5/63.7    |\n| CC-OCR                | 66.6     | 62.7              | 73.0            | 64.7          | 68.7       |79.8           |\n\n\n### Video benchmark\n| Benchmarks          | GPT4o | Gemini-1.5-Pro | InternVL2.5-78B | Qwen2VL-72B | Qwen2.5VL-72B |\n|---------------------|-------|----------------|-----------------|-------------|---------------|\n| VideoMME w/o sub.   | 71.9  | 75.0           | 72.1            | 71.2        | 73.3          |\n| VideoMME w sub.     | 77.2  | 81.3           | 74.0            | 77.8        | 79.1          |\n| MVBench             | 64.6  | 60.5           | 76.4            | 73.6        | 70.4          |\n| MMBench-Video       | 1.63  | 1.30           | 1.97            | 1.70        | 2.02          |\n| LVBench             | 30.8  | 33.1           | -               | 41.3        | 47.3          |\n| EgoSchema           | 72.2  | 71.2           | -               | 77.9        | 76.2          |\n| PerceptionTest_test | -     | -              | -               | 68.0        | 73.2          |\n| MLVU_M-Avg_dev      | 64.6  | -              | 75.7            |             | 74.6          |\n| TempCompass_overall | 73.8  | -              | -               |             | 74.8          |\n\n\n### Agent benchmark\n\n| Benchmarks              | GPT4o       | Gemini 2.0 | Claude | Aguvis-72B | Qwen2VL-72B | Qwen2.5VL-72B |\n|-------------------------|-------------|------------|--------|------------|-------------|---------------|\n| ScreenSpot              | 18.1        | 84.0       | 83.0   |            |             | 87.1          |\n| ScreenSpot Pro          |             |            | 17.1   |            | 1.6         | 43.6          |\n| AITZ_EM                 | 35.3        |            |        |            | 72.8        | 83.2          |\n| Android Control High_EM |             |            |        | 66.4       | 59.1        | 67.36         |\n| Android Control Low_EM  |             |            |        | 84.4       | 59.2        | 93.7          |\n| AndroidWorld_SR         | 34.5% (SoM) |            | 27.9%  | 26.1%      |             | 35%           |\n| MobileMiniWob++_SR      |             |            |        | 66%        |             | 68%           |\n| OSWorld                 |             |            | 14.90  | 10.26      |             | 8.83          |\n\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with 🤖 ModelScope and 🤗 Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using 🤗  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-72B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-72B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-72B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-72B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors=\"pt\",\n    **video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | ✅  | ✅   |\n| torchvision < 0.19.0  | ❌  | ❌   |\n| decord      | ✅  | ❌   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### 🤖 ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-72B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```json\n{\n\t...,\n    \"type\": \"yarn\",\n    \"mrope_section\": [\n        16,\n        24,\n        24\n    ],\n    \"factor\": 4,\n    \"original_max_position_embeddings\": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":73410777344,\"storage_bytes\":146821823583,\"files_count\":50,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2_5_VLForConditionalGeneration\"],\"model_type\":\"qwen2_5_vl\",\"processor_config\":{\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"},\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5-VL\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5-VL\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:dmlc:decord\",\"source_url\":\"https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2409.12191\",\"source_url\":\"https://arxiv.org/abs/2409.12191\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2308.12966\",\"source_url\":\"https://arxiv.org/abs/2308.12966\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 77.6,
    "content_hash": "99c98e784da12fdad3d0720fe721606c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nerijs:pixel-art-xl",
    "name": "pixel-art-xl",
    "author": "nerijs",
    "description": "--- license: creativeml-openrail-m tags: - text-to-image - stable-diffusion - lora - diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 instance_prompt: pixel art widget: - text: pixel art, a cute corgi, simple, flat colors --- !F1hS8XHXwAQrMEW.jpeg !F1hS489X0AE-PK5.jpeg Downscale 8 times to get pixel perfect images (use Nearest Neighbors) Use a fixed VAE to avoid artifacts (0.9 or fp16 fix) Use it with a LCM Lora! Use 8 steps and guidance scale of 1.5 1.2 Lora strength for the Pi...",
    "tags": [
      "diffusers",
      "text-to-image",
      "stable-diffusion",
      "lora",
      "license:creativeml-openrail-m",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 568,
    "downloads": 4756,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nerijs/pixel-art-xl",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\ntags:\n  - text-to-image\n  - stable-diffusion\n  - lora\n  - diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\ninstance_prompt: pixel art\nwidget:\n  - text: pixel art, a cute corgi, simple, flat colors\n---\n# Pixel Art XL\n## Consider supporting further research on [Patreon](https://www.patreon.com/user?u=29466374) or [Twitter](https://twitter.com/nerijs)\n\n![F1hS8XHXwAQrMEW.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6303f37c3926de1f7ec42d3e/SSOQ9lfB1PVhXVWJiL7Mx.jpeg)\n![F1hS489X0AE-PK5.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6303f37c3926de1f7ec42d3e/tY19J3xWDlSY2hhTTHySc.jpeg)\n\n\nDownscale 8 times to get pixel perfect images (use Nearest Neighbors)\nUse a fixed VAE to avoid artifacts (0.9 or fp16 fix)\n\n### Need more performance?\nUse it with a LCM Lora!\n\nUse 8 steps and guidance scale of 1.5\n1.2 Lora strength for the Pixel Art XL works better\n\n```python\nfrom diffusers import DiffusionPipeline, LCMScheduler\nimport torch\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nlcm_lora_id = \"latent-consistency/lcm-lora-sdxl\"\npipe = DiffusionPipeline.from_pretrained(model_id, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(lcm_lora_id, adapter_name=\"lora\")\npipe.load_lora_weights(\"./pixel-art-xl.safetensors\", adapter_name=\"pixel\")\n\npipe.set_adapters([\"lora\", \"pixel\"], adapter_weights=[1.0, 1.2])\npipe.to(device=\"cuda\", dtype=torch.float16)\n\nprompt = \"pixel, a cute corgi\"\nnegative_prompt = \"3d render, realistic\"\n\nnum_images = 9\n\nfor i in range(num_images):\n    img = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=8,\n        guidance_scale=1.5,\n    ).images[0]\n    \n    img.save(f\"lcm_lora_{i}.png\")\n```\n\n### Tips:\nDon't use refiner\n\nWorks great with only 1 text encoder\n\nNo style prompt required\n\nNo trigger keyword require\n\nWorks great with isometric and non-isometric\n\nWorks with 0.9 and 1.0\n\n#### Changelog\nv1: Initial release",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":170543052,\"files_count\":3,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 62.6,
    "content_hash": "cab9fcaaf900e659744f77ad0ea36286",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nerijs/pixel-art-xl\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nvidia:llama-3.1-nemotron-70b-instruct",
    "name": "Llama-3.1-Nemotron-70B-Instruct",
    "author": "nvidia",
    "description": "--- license: llama3.1 language: - en inference: false fine-tuning: false tags: - nvidia - llama3.1 datasets: - nvidia/HelpSteer2 base_model: meta-llama/Llama-3.1-70B-Instruct library_name: nemo --- Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model reaches Arena Hard of 85.0, AlpacaEval 2 LC of 57.6 and GPT-4-Turbo MT-Bench of 8.98, which are known to be predictive of LMSys Chatbot Ar...",
    "tags": [
      "nemo",
      "nvidia",
      "llama3.1",
      "en",
      "dataset:nvidia/helpsteer2",
      "arxiv:2410.01257",
      "arxiv:2310.05344",
      "arxiv:2311.09528",
      "arxiv:2406.08673",
      "base_model:meta-llama/llama-3.1-70b-instruct",
      "license:llama3.1",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 567,
    "downloads": 69,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: llama3.1\nlanguage:\n- en\ninference: false\nfine-tuning: false\ntags:\n- nvidia\n- llama3.1\ndatasets:\n- nvidia/HelpSteer2\nbase_model: meta-llama/Llama-3.1-70B-Instruct\nlibrary_name: nemo\n---\n# Model Overview\n\n## Description:\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\n\n\nThis model reaches [Arena Hard](https://github.com/lmarena/arena-hard-auto) of 85.0, [AlpacaEval 2 LC](https://tatsu-lab.github.io/alpaca_eval/) of 57.6 and [GPT-4-Turbo MT-Bench](https://github.com/lm-sys/FastChat/pull/3158) of 8.98, which are known to be predictive of [LMSys Chatbot Arena Elo](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n\nAs of 1 Oct 2024, this model is #1 on all three automatic alignment benchmarks (verified tab for AlpacaEval 2 LC), edging out strong frontier models such as GPT-4o and Claude 3.5 Sonnet.\n\nAs of Oct 24th, 2024 the model has Elo Score of 1267(+-7), rank 9 and style controlled rank of 26 on [ChatBot Arena leaderboard](https://lmarena.ai/?leaderboard).\n\nThis model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a Llama-3.1-70B-Instruct model as the initial policy.\n\nIf you prefer to use the model in the HuggingFace Transformers codebase, we have done a model conversion format into [Llama-3.1-Nemotron-70B-Instruct-HF](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF) .\n\nTry hosted inference for free at [build.nvidia.com](https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct) - it comes with an OpenAI-compatible API interface.\n\n\nSee details on our paper at [https://arxiv.org/abs/2410.01257](https://arxiv.org/abs/2410.01257) - as a preview, this model can correctly the question ```How many r in strawberry?``` without specialized prompting or additional reasoning tokens:\n\n```\nA sweet question!\nLet’s count the “R”s in “strawberry”:\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\nThere are **3 “R”s** in the word “strawberry”.\n```\n\nNote: This model is a demonstration of our techniques for improving helpfulness in general-domain instruction following. It has not been tuned for performance in specialized domains such as math.\n\n## License\nYour use of this model is governed by the [NVIDIA Open Model License](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\nAdditional Information: [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_1/license/). Built with Llama.\n\n\n## Evaluation Metrics\n\nAs of 1 Oct 2024, Llama-3.1-Nemotron-70B-Instruct performs best on Arena Hard, AlpacaEval 2 LC (verified tab) and MT Bench (GPT-4-Turbo)\n\n | Model  | Arena Hard | AlpacaEval | MT-Bench | Mean Response Length |\n|:-----------------------------|:----------------|:-----|:----------|:-------|\n|Details | (95% CI) | 2 LC (SE) | (GPT-4-Turbo) | (# of Characters for MT-Bench)| \n| _**Llama-3.1-Nemotron-70B-Instruct**_ | **85.0** (-1.5, 1.5) | **57.6** (1.65) | **8.98** | 2199.8 | \n| Llama-3.1-70B-Instruct | 55.7 (-2.9, 2.7) | 38.1 (0.90)  | 8.22 | 1728.6 |\n| Llama-3.1-405B-Instruct | 69.3 (-2.4, 2.2) | 39.3 (1.43) | 8.49 | 1664.7 |\n| Claude-3-5-Sonnet-20240620 | 79.2 (-1.9, 1.7) | 52.4 (1.47) | 8.81 | 1619.9 |\n| GPT-4o-2024-05-13 | 79.3 (-2.1, 2.0) | 57.5 (1.47) | 8.74 | 1752.2 |\n         \n## Usage:\n\nWe demonstrate inference using NVIDIA NeMo Framework, which allows hassle-free model deployment based on [NVIDIA TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), a highly optimized inference solution focussing on high throughput and low latency.\n\nPre-requisite: You would need at least a machine with 4 40GB or 2 80GB NVIDIA GPUs, and 150GB of free disk space. \n\n1. Please sign up to get **free and immediate** access to [NVIDIA NeMo Framework container](https://developer.nvidia.com/nemo-framework). If you don’t have an NVIDIA NGC account, you will be prompted to sign up for an account before proceeding.\n2. If you don’t have an NVIDIA NGC API key, sign into [NVIDIA NGC](https://ngc.nvidia.com/setup), selecting organization/team: ea-bignlp/ga-participants and click Generate API key. Save this key for the next step. Else, skip this step. \n3. On your machine, docker login to nvcr.io using\n   ```\n   docker login nvcr.io\n   Username: $oauthtoken\n   Password: <Your Saved NGC API Key>\n   ```\n4. Download the required container\n   ```\n   docker pull nvcr.io/nvidia/nemo:24.05.llama3.1\n   ```\n   \n5. Download the checkpoint\n   ```\n   git lfs install\n   git clone https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\n   ```\n\n6. Run Docker container\n   (In addition, to use Llama3.1 tokenizer, you need to ```export HF_HOME=<YOUR_HF_HOME_CONTAINING_TOKEN_WITH_LLAMA3.1_70B_ACCESS>```)\n   ```\n   docker run --gpus all -it --rm --shm-size=150g -p 8000:8000 -v ${PWD}/Llama-3.1-Nemotron-70B-Instruct:/opt/checkpoints/Llama-3.1-Nemotron-70B-Instruct,${HF_HOME}:/hf_home -w /opt/NeMo nvcr.io/nvidia/nemo:24.05.llama3.1\n   ```\n   \n7. Within the container, start the server in the background. This step does both conversion of the nemo checkpoint to TRT-LLM and then deployment using TRT-LLM. For an explanation of each argument and advanced usage, please refer to [NeMo FW Deployment Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/deployment/llm/in_framework.html)\n   \n   ```\n   HF_HOME=/hf_home python scripts/deploy/nlp/deploy_inframework_triton.py --nemo_checkpoint /opt/checkpoints/Llama-3.1-Nemotron-70B-Instruct --model_type=\"llama\" --triton_model_name nemotron --triton_http_address 0.0.0.0 --triton_port 8000 --num_gpus 2 --max_input_len 3072 --max_output_len 1024 --max_batch_size 1 &\n   ```\n   \n8. Once the server is ready (i.e. when you see this messages below), you are ready to launch your client code\n\n    ```\n    Started HTTPService at 0.0.0.0:8000\n    Started GRPCInferenceService at 0.0.0.0:8001\n    Started Metrics Service at 0.0.0.0:8002\n    ```\n\n    ```\n    python scripts/deploy/nlp/query_inframework.py -mn nemotron -p \"How many r in strawberry?\" -mol 1024\n    ```\n    \n \n\n## References(s):\n\n* [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257)\n* [SteerLM method](https://arxiv.org/abs/2310.05344)\n* [HelpSteer](https://arxiv.org/abs/2311.09528)\n* [HelpSteer2](https://arxiv.org/abs/2406.08673)\n* [Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/) \n* [Meta's Llama 3.1 Webpage](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) \n* [Meta's Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)\n\n\n## Model Architecture: \n**Architecture Type:** Transformer <br>\n**Network Architecture:** Llama 3.1 <br>\n\n## Input:\n**Input Type(s):** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Input:** Max of 128k tokens<br>\n\n## Output:\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Output:**  Max of 4k tokens <br>\n\n\n## Software Integration:\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Ampere <br>\n* NVIDIA Hopper <br>\n* NVIDIA Turing <br>\n**Supported Operating System(s):** Linux <br>\n\n## Model Version: \nv1.0\n\n# Training & Evaluation: \n\n* REINFORCE implemented in NeMo Aligner\n\n## Datasets:\n\n**Data Collection Method by dataset** <br>\n* [Hybrid: Human, Synthetic] <br>\n\n**Labeling Method by dataset** <br>\n* [Human] <br>\n\n**Link:** \n* [HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)\n\n**Properties (Quantity, Dataset Descriptions, Sensor(s)):** <br>\n* 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity.\n* 20, 324 prompt-responses used for training and 1, 038 used for validation.\n\n\n# Inference:\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server) <br>\n**Test Hardware:** H100, A100 80GB, A100 40GB <br>\n\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@misc{wang2024helpsteer2preferencecomplementingratingspreferences,\n      title={HelpSteer2-Preference: Complementing Ratings with Preferences}, \n      author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},\n      year={2024},\n      eprint={2410.01257},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2410.01257}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"nemo\",\"framework\":\"nemo\",\"params\":null,\"storage_bytes\":141130115749,\"files_count\":3711,\"spaces_count\":54,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:lmarena:arena-hard-auto\",\"source_url\":\"https://github.com/lmarena/arena-hard-auto\"},{\"type\":\"has_code\",\"target_id\":\"github:lm-sys:FastChat\",\"source_url\":\"https://github.com/lm-sys/FastChat\"},{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:TensorRT-LLM\",\"source_url\":\"https://github.com/NVIDIA/TensorRT-LLM\"},{\"type\":\"has_code\",\"target_id\":\"github:meta-llama:llama-models\",\"source_url\":\"https://github.com/meta-llama/llama-models\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2410.01257\",\"source_url\":\"https://arxiv.org/abs/2410.01257\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.05344\",\"source_url\":\"https://arxiv.org/abs/2310.05344\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.09528\",\"source_url\":\"https://arxiv.org/abs/2311.09528\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2406.08673\",\"source_url\":\"https://arxiv.org/abs/2406.08673\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.1",
    "compliance_status": "approved",
    "quality_score": 62.5,
    "content_hash": "ad788aab833f2f9a96c70c06fd39f866",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:reducto:rolmocr",
    "name": "RolmOCR",
    "author": "reducto",
    "description": "--- library_name: transformers license: apache-2.0 datasets: - allenai/olmOCR-mix-0225 base_model: Qwen/Qwen2.5-VL-7B-Instruct --- Earlier this year, the Allen Institute for AI released olmOCR, an open-source tool that performs document OCR using the Qwen2-VL-7B vision language model (VLM). We were excited to see a high-quality, openly available approach to parsing PDFs and other complex documents — and curious to explore what else might be possible using newer foundation models and some ligh...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2_5_vl",
      "image-to-text",
      "dataset:allenai/olmocr-mix-0225",
      "base_model:qwen/qwen2.5-vl-7b-instruct",
      "base_model:finetune:qwen/qwen2.5-vl-7b-instruct",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-to-text",
    "likes": 567,
    "downloads": 3880,
    "source": "huggingface",
    "source_url": "https://huggingface.co/reducto/RolmOCR",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- allenai/olmOCR-mix-0225\nbase_model: Qwen/Qwen2.5-VL-7B-Instruct\n---\n\n# RolmOCR by [Reducto AI](https://reducto.ai/)\n\nEarlier this year, the [Allen Institute for AI](https://allenai.org/) released olmOCR, an open-source tool that performs document OCR using the Qwen2-VL-7B vision language model (VLM). We were excited to see a high-quality, openly available approach to parsing PDFs and other complex documents — and curious to explore what else might be possible using newer foundation models and some lightweight optimizations.\n\nThe result is **RolmOCR**, a drop-in alternative to olmOCR that’s faster, uses less memory, and still performs well on a variety of document types. We're releasing it under **Apache 2.0** for anyone to try out, explore, or build on.\n\nThis model is a fine-tuned version of [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) on the full [allenai/olmOCR-mix-0225](https://huggingface.co/datasets/allenai/olmOCR-mix-0225) dataset.\n\n## Key changes\nWe made three notable changes: \n\n1. **New Base Model**: We swapped in a more recent version of the existing model (Qwen2.5-VL-7B) as the foundation.\n\n2. **No Metadata inputs**: Unlike the original, we don’t use metadata extracted from PDFs. This significantly reduces prompt length, which in turn lowers both processing time and VRAM usage — without hurting accuracy in most cases. \n\n3. **Rotation of training data:** About 15% of the training data was rotated to enhance robustness to off-angle documents. We otherwise use the same training set. \n\n## Usage\n\nHost your model with vLLM:\n```bash\nexport VLLM_USE_V1=1\nvllm serve reducto/RolmOCR \n```\n\nCall the model via openai compatible server:\n```python\n# HOST YOUR OPENAI COMPATIBLE API WITH THE FOLLOWING COMMAND in VLLM:\n# export VLLM_USE_V1=1\n# vllm serve reducto/RolmOCR \n\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(api_key=\"123\", base_url=\"http://localhost:8000/v1\")\n\nmodel = \"reducto/RolmOCR-7b\"\n\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\ndef ocr_page_with_rolm(img_base64):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Return the plain text representation of this document as if you were reading it naturally.\\n\",\n                    },\n                ],\n            }\n        ],\n        temperature=0.2,\n        max_tokens=4096\n    )\n    return response.choices[0].message.content\n\ntest_img_path = \"path/to/image.png\"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_rolm(img_base64))\n```\n\n## Limitations\n\n- RolmOCR, like other VLM-based OCR solutions, still suffer from hallucination or dropping contents.\n- Unlike the [Reducto Parsing API](https://app.reducto.ai/), RolmOCR cannot output layout bounding boxes.\n- We have not evaluated the performance of any quantized versions.\n\n## BibTex and citation info\n```\n@misc{RolmOCR,\n  author = {Reducto AI},\n  title = {RolmOCR: A Faster, Lighter Open Source OCR Model},\n  year = {2025},\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"image-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":8292166656,\"storage_bytes\":16595836440,\"files_count\":17,\"spaces_count\":16,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2_5_VLForConditionalGeneration\"],\"model_type\":\"qwen2_5_vl\",\"processor_config\":{\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"},\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.5,
    "content_hash": "f25dcb024a7f19e7e497494eb33f655c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/reducto/RolmOCR\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:microsoft:phi-3.5-moe-instruct",
    "name": "Phi-3.5-MoE-instruct",
    "author": "microsoft",
    "description": "--- license: mit license_link: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? library_name: transformers --- Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very...",
    "tags": [
      "transformers",
      "safetensors",
      "phimoe",
      "text-generation",
      "nlp",
      "code",
      "conversational",
      "custom_code",
      "multilingual",
      "arxiv:2404.14219",
      "arxiv:2407.13833",
      "arxiv:2403.06412",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 566,
    "downloads": 110688,
    "source": "huggingface",
    "source_url": "https://huggingface.co/microsoft/Phi-3.5-MoE-instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\nlibrary_name: transformers\n---\n\n## Model Summary\n\nPhi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. \n\n🏡 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n📰 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n📖 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n👩‍🍳 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n🖥️ [Try It](https://aka.ms/try-phi3.5moe) <br>\n\nMoE references:\n📜[Phi-3.5-MoE Blog](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-the-availability-of-phi-3-5-moe-in-azure-ai-studio/ba-p/4256278) | 😁[GRIN MoE](https://huggingface.co/microsoft/GRIN-MoE)\n\n**Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Usage\n\n### Requirements\nPhi-3.5-MoE-instruct is integrated in the official version of `transformers` starting from **4.46.0**. \nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.46.0\n```\n\nPhi-3.5-MoE-instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3.5moe)\n\n### Tokenizer\n\nPhi-3.5-MoE-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3.5-moe-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\n\n### Input Formats\nGiven the nature of the training data, the Phi-3.5-MoE-instruct model is best suited for prompts using the chat format as follows:\n\n```\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\n### Loading the model locally\nAfter obtaining the Phi-3.5-MoE-instruct model checkpoints, users can use this sample code for inference.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \n\nmodel = AutoModelForCausalLM.from_pretrained( \n    \"microsoft/Phi-3.5-MoE-instruct\",  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=False,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\") \n\nmessages = [ \n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n] \n\npipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"return_full_text\": False, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0]['generated_text'])\n```\n\n## Benchmarks\n\nTo understand the capabilities, we compare Phi-3.5-MoE with a set of models over a variety of benchmarks using our internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\n\n| Category | Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | Arena Hard | 37.9 | 39.4 | 25.7 | 42.0 | 55.2 | 75.0 |\n| | BigBench Hard CoT (0-shot) | 79.1 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n| | MMLU (5-shot) | 78.9 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n| | MMLU-Pro (0-shot, CoT) | 54.3 | 40.7 | 44.0 | 50.1 | 57.2 | 62.8 |\n| Reasoning | ARC Challenge (10-shot) | 91.0 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n| | BoolQ (2-shot) | 84.6 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n| | GPQA (0-shot, CoT) | 36.8 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n| | HellaSwag (5-shot) | 83.8 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n| | OpenBookQA (10-shot) | 89.6 | 84.4 | 84.8 | 89.6 | 89.0 | 90.0 |\n| | PIQA (5-shot) | 88.6 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n| | Social IQA (5-shot) | 78.0 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n| | TruthfulQA (MC2) (10-shot) | 77.5 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n| | WinoGrande (5-shot) | 81.3 | 70.4 | 64.7 | 74.0 | 74.7 | 76.9 |\n| Multilingual | Multilingual MMLU (5-shot) | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| | MGSM (0-shot CoT) | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| Math | GSM8K (8-shot, CoT) | 88.7 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n| | MATH (0-shot, CoT) | 59.5 | 31.2 | 47.6 | 50.9 | 38.0 | 70.2 |\n| Long context | Qasper | 40.0 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n| | SQuALITY | 24.1 | 25.8 | 26.2 | 0.0 | 23.5 | 23.8 |\n| Code Generation | HumanEval (0-shot) | 70.7 | 63.4 | 66.5 | 61.0 | 74.4 | 86.6 |\n| | MBPP (3-shot) | 80.8 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **69.2** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | 62.6 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning | 78.7 | 72.2 | 70.5 | 75.4 | 77.7 | 80.0 |\n| Language understanding | 71.8 | 67.0 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness | 75.6 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context | 25.5 | 24.5 | 25.5 | 0.0 | 27.0 | 25.4 |\n| Math | 74.1 | 57.7 | 65.0 | 67.9 | 60.2 | 80.8 |\n| Code generation | 68.3 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual | 65.8 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, Phi-3.5-MoE with only **6.6B active parameters** achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4o-mini. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.\n\n### Multilingual\n\nThe table below highlights multilingual capability of Phi-3.5-MoE on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 6.6B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\n\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Multilingual MMLU | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| Multilingual MMLU-Pro | 45.3 | 34.0 | 21.4 | 43.0 | 57.9 | 53.2 |\n| MGSM | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| MEGA MLQA | 65.3 | 61.2 | 45.2 | 54.4 | 61.6 | 70.0 |\n| MEGA TyDi QA | 67.1 | 63.7 | 54.5 | 65.6 | 63.6 | 81.8 |\n| MEGA UDPOS | 60.4 | 58.2 | 54.1 | 56.6 | 62.4 | 66.0 |\n| MEGA XCOPA | 76.6 | 10.8 | 21.1 | 31.2 | 95.0 | 90.3 |\n| MEGA XStoryCloze | 82.8 | 92.3 | 71.0 | 87.0 | 20.7 | 96.6 |\n| **Average** | **65.8** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\n### Long Context\n\nPhi-3.5-MoE supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, multilingual context retrieval. We see that Phi-3.5 is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-MoE-instruct is very competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|\n| GovReport | 26.4 | 25.6 | 25.1 | 27.8 | 24.8 |\n| QMSum | 19.9 | 22.1 | 21.6 | 24.0 | 21.7 |\n| Qasper | 40.0 | 30.7 | 37.2 | 43.5 | 39.8 |\n| SQuALITY | 24.1 | 25.8 | 26.2 | 23.5 | 23.8 |\n| SummScreenFD | 16.9 | 18.2 | 17.6 | 16.3 | 17.0 |\n| **Average** | **25.5** | **24.5** | **25.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 94.8 | 93 | 93.2 | 91.6 | 85.7 | 64.2 | **87.1** |\n| Llama-3.1-8B-instruct | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| Mistral-Nemo-12B-instruct-2407 | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 89 | 74 | 81 | 88 | 95 | **85** |\n| Llama-3.1-8B-instruct | 80 | 65 | 73 | 76 | 63 | **71** |\n| Mistral-7B-instruct-v0.3 | 61 | 57 | 51 | 61 | 80 | **62** |\n\n## Training\n\n### Model\n\n**Architecture:** Phi-3.5-MoE has 16x3.8B parameters with **6.6B active parameters** when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.<br>\n**Inputs:** Text. It is best suited for prompts using chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 512 H100-80G<br>\n**Training time:** 23 days<br>\n**Training data:** 4.9T tokens<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between April and August 2024<br>\n**Status:** This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.<br>\n**Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n**Release date:** August 2024<br>\n\n### Training Datasets\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n* Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\n* Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n* Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n* Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\n* Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\n* Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n* Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include: \n* Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n* High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n* Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n* Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n* Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Safety Evaluation and Red-Teaming\n\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to \nevaluate Phi-3.5 models' propensity to produce undesirable outputs across multiple languages and risk categories. \nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety \npost-training that was done as detailed in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833) had a positive impact across multiple languages and risk categories as observed by \nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted \nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as \nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833). \nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output \nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings \nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, \nand risk areas that account for cultural nuances where those languages are spoken.\n\n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3.5-MoE-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n\n\n## Appendix A: Korean benchmarks\n\nThe prompt is the same as the [CLIcK paper](https://arxiv.org/abs/2403.06412) prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\n\n- GPT-4o: 2024-05-13 version\n- GPT-4o-mini: 2024-07-18 version\n- GPT-4-turbo: 2024-04-09 version\n- GPT-3.5-turbo: 2023-06-13 version\n\nOverall, the Phi-3.5 MoE model with just 6.6B active params outperforms GPT-3.5-Turbo.\n\n| Benchmarks               |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:-------------------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| CLIcK                    |                  56.44 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n| HAERAE 1.0               |                  61.83 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n| KMMLU (0-shot, CoT)      |                  47.43 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n| KMMLU (5-shot)           |                  47.92 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n| KMMLU-HARD (0-shot, CoT) |                  25.34 |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n| KMMLU-HARD (5-shot)      |                  25.66 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |\n| **Average**              |          **45.82** |                **29.99** |            **29.29** |    **62.54** |         **50.08** |         **56.74** |           **39.61** |\n\n#### CLIcK (Cultural and Linguistic Intelligence in Korean)\n\n##### Accuracy by supercategory\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         |                  58.44 |                           29.74 |                   51.15 |    81.89 |         70.95 |         73.61 |           53.38 |\n| Language        |                  52.31 |                           27.85 |                   40.92 |    77.54 |         63.54 |         71.23 |           46    |\n| **Overall**     |                  56.44 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n\n##### Accuracy by category\n| supercategory   | category    |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|:------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         | Economy     |                  77.97 |                           28.81 |                   66.1  |    94.92 |         83.05 |         89.83 |           64.41 |\n| Culture         | Geography   |                  60.31 |                           29.01 |                   54.2  |    80.15 |         77.86 |         82.44 |           53.44 |\n| Culture         | History     |                  33.93 |                           30    |                   29.64 |    66.92 |         48.4  |         46.4  |           31.79 |\n| Culture         | Law         |                  52.51 |                           22.83 |                   44.29 |    70.78 |         57.53 |         61.19 |           41.55 |\n| Culture         | Politics    |                  70.24 |                           33.33 |                   59.52 |    88.1  |         83.33 |         89.29 |           65.48 |\n| Culture         | Pop Culture |                  80.49 |                           34.15 |                   60.98 |    97.56 |         85.37 |         92.68 |           75.61 |\n| Culture         | Society     |                  74.43 |                           31.72 |                   65.05 |    92.88 |         85.44 |         86.73 |           71.2  |\n| Culture         | Tradition   |                  58.11 |                           31.98 |                   54.95 |    87.39 |         74.77 |         79.28 |           55.86 |\n| Language        | Functional  |                  48    |                           24    |                   32.8  |    84.8  |         64.8  |         80    |           40    |\n| Language        | Grammar     |                  29.58 |                           23.33 |                   22.92 |    57.08 |         42.5  |         47.5  |           30    |\n| Language        | Textual     |                  73.33 |                           33.33 |                   59.65 |    91.58 |         80.7  |         87.37 |           62.11 |\n\n#### HAERAE 1.0\n\n| category              |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| General Knowledge     |                  39.77 |                           28.41 |                   34.66 |    77.27 |         53.41 |         66.48 |           40.91 |\n| History               |                  60.64 |                           22.34 |                   44.15 |    92.02 |         84.57 |         78.72 |           30.32 |\n| Loan Words            |                  70.41 |                           35.5  |                   63.31 |    79.88 |         76.33 |         78.11 |           59.17 |\n| Rare Words            |                  63.95 |                           42.96 |                   63.21 |    87.9  |         81.98 |         79.01 |           61.23 |\n| Reading Comprehension |                  64.43 |                           41.16 |                   51.9  |    85.46 |         77.18 |         80.09 |           56.15 |\n| Standard Nomenclature |                  66.01 |                           32.68 |                   58.82 |    88.89 |         75.82 |         79.08 |           53.59 |\n| **Overall**           |                  61.83 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n\n#### KMMLU (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  45.15 |                           31.68 |                   37.03 |    61.52 |         49.29 |         55.98 |           38.47 |\n| HUMSS           |                  49.75 |                           26.47 |                   37.29 |    69.45 |         56.59 |         63    |           40.9  |\n| Other           |                  47.24 |                           31.01 |                   39.15 |    63.79 |         52.35 |         57.53 |           40.19 |\n| STEM            |                  49.08 |                           31.9  |                   40.42 |    65.16 |         54.74 |         60.84 |           42.24 |\n| **Overall**     |                  47.43 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n\n#### KMMLU (5-shot)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  45.9  |                           29.98 |                   19.24 |    61.47 |         48.66 |         56.85 |           40.22 |\n| HUMSS           |                  49.18 |                           27.27 |                   22.5  |    68.79 |         55.95 |         63.68 |           43.35 |\n| Other           |                  48.43 |                           30.76 |                   20.95 |    64.21 |         51.1  |         57.85 |           41.92 |\n| STEM            |                  49.21 |                           30.73 |                   19.55 |    65.28 |         53.29 |         61.08 |           44.43 |\n| **Overall**     |                  47.92 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n\n#### KMMLU-HARD (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024)|   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  25.83 |                           26.17 |                   26.25 |    37.12 |         22.25 |         29.17 |           21.07 |\n| HUMSS           |                  21.52 |                           24.38 |                   20.21 |    41.97 |         23.31 |         31.51 |           19.44 |\n| Other           |                  24.82 |                           24.82 |                   23.88 |    40.39 |         26.48 |         29.59 |           22.22 |\n| STEM            |                  28.18 |                           26.91 |                   24.64 |    39.82 |         26.36 |         32.18 |           20.91 |\n| **Overall**     |                  25.34 |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n\n#### KMMLU-HARD (5-shot) \n\n| supercategory   |   Phi-3.5-MoE-Instruct |  Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  21    |                           29    |                   12    |    31    |         21    |         25    |           20    |\n| HUMSS           |                  22.88 |                           19.92 |                   14    |    43.98 |         23.47 |         33.53 |           19.53 |\n| Other           |                  25.13 |                           27.27 |                   12.83 |    39.84 |         28.34 |         29.68 |           23.22 |\n| STEM            |                  21.75 |                           25.25 |                   12.75 |    40.25 |         23.25 |         27.25 |           19.75 |\n| **Overall**     |                  25.66 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":41873153344,\"storage_bytes\":83747055891,\"files_count\":34,\"spaces_count\":13,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"PhiMoEForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_phimoe.PhiMoEConfig\",\"AutoModelForCausalLM\":\"modeling_phimoe.PhiMoEForCausalLM\"},\"model_type\":\"phimoe\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:microsoft:Phi-3CookBook\",\"source_url\":\"https://github.com/microsoft/Phi-3CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:pytorch:pytorch\",\"source_url\":\"https://github.com/pytorch/pytorch\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:HazyResearch:flash-attention\",\"source_url\":\"https://github.com/HazyResearch/flash-attention\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.14219\",\"source_url\":\"https://arxiv.org/abs/2404.14219\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2407.13833\",\"source_url\":\"https://arxiv.org/abs/2407.13833\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.06412\",\"source_url\":\"https://arxiv.org/abs/2403.06412\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 77.5,
    "content_hash": "1733594dcbb8a5175564c1a097c6813f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen2.5-1.5b-instruct",
    "name": "Qwen2.5-1.5B-Instruct",
    "author": "Qwen",
    "description": "--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-1.5B tags: - chat library_name: transformers --- Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: - Significantly **mo...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "chat",
      "conversational",
      "en",
      "arxiv:2407.10671",
      "base_model:qwen/qwen2.5-1.5b",
      "base_model:finetune:qwen/qwen2.5-1.5b",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 565,
    "downloads": 5650929,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-1.5B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-1.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1543714304,\"storage_bytes\":3087467144,\"files_count\":10,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen2ForCausalLM\"],\"model_type\":\"qwen2\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2407.10671\",\"source_url\":\"https://arxiv.org/abs/2407.10671\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 62.5,
    "content_hash": "adfee1e46e8057f1c6bc5f622f7cd383",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.555Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mistralai:ministral-8b-instruct-2410",
    "name": "Ministral-8B-Instruct-2410",
    "author": "mistralai",
    "description": "--- library_name: vllm language: - en - fr - de - es - it - pt - zh - ja - ru - ko license: other license_name: mrl inference: false license_link: https://mistral.ai/licenses/MRL-0.1.md extra_gated_prompt: >- # Mistral AI Research License If You want to use a Mistral Model, a Derivative or an Output for any purpose that is not expressly authorized under this Agreement, You must request a license from Mistral AI, which Mistral AI may grant to You in Mistral AI's sole discretion. To discuss suc...",
    "tags": [
      "vllm",
      "safetensors",
      "mistral",
      "mistral-common",
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ja",
      "ru",
      "ko",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 564,
    "downloads": 274174,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- zh\n- ja\n- ru\n- ko\nlicense: other\nlicense_name: mrl\ninference: false\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\nextra_gated_prompt: >-\n  # Mistral AI Research License\n\n  If You want to use a Mistral Model, a Derivative or an Output for any purpose\n  that is not expressly authorized under this Agreement, You must request a\n  license from Mistral AI, which Mistral AI may grant to You in Mistral AI's\n  sole discretion. To discuss such a license, please contact Mistral AI via the\n  website contact form: https://mistral.ai/contact/\n\n  ## 1. Scope and acceptance\n\n  **1.1. Scope of the Agreement.** This Agreement applies to any use,\n  modification, or Distribution of any Mistral Model by You, regardless of the\n  source You obtained a copy of such Mistral Model.\n\n  **1.2. Acceptance.** By accessing, using, modifying, Distributing a Mistral\n  Model, or by creating, using or distributing a Derivative of the Mistral\n  Model, You agree to be bound by this Agreement.\n\n  **1.3. Acceptance on behalf of a third-party.** If You accept this Agreement\n  on behalf of Your employer or another person or entity, You warrant and\n  represent that You have the authority to act and accept this Agreement on\n  their behalf. In such a case, the word \"You\" in this Agreement will refer to\n  Your employer or such other person or entity.\n\n  ## 2. License\n\n  **2.1. Grant of rights**.  Subject to Section 3 below, Mistral AI hereby\n  grants You a non-exclusive, royalty-free, worldwide, non-sublicensable,\n  non-transferable, limited license to use, copy, modify, and Distribute under\n  the conditions provided in Section 2.2 below, the Mistral Model and any\n  Derivatives made by or for Mistral AI and to create Derivatives of the Mistral\n  Model.\n\n  **2.2. Distribution of Mistral Model and Derivatives made by or for Mistral\n  AI.** Subject to Section 3 below, You may Distribute copies of the Mistral\n  Model and/or Derivatives made by or for Mistral AI, under the following\n  conditions: You must make available a copy of this Agreement to third-party\n  recipients of the Mistral Models and/or Derivatives made by or for Mistral AI\n  you Distribute, it being specified that any rights to use the Mistral Models\n  and/or Derivatives made by or for Mistral AI shall be directly granted by\n  Mistral AI to said third-party recipients pursuant to the Mistral AI Research\n  License agreement executed between these parties; You must retain in all\n  copies of the Mistral Models the following attribution notice within a\n  \"Notice\" text file distributed as part of such copies: \"Licensed by Mistral AI\n  under the Mistral AI Research License\".\n\n  **2.3. Distribution of Derivatives made by or for You.** Subject to Section 3\n  below, You may Distribute any Derivatives made by or for You under additional\n  or different terms and conditions, provided that: In any event, the use and\n  modification of Mistral Model and/or Derivatives made by or for Mistral AI\n  shall remain governed by the terms and conditions of this Agreement; You\n  include in any such Derivatives made by or for You prominent notices stating\n  that You modified the concerned Mistral Model; and Any terms and conditions\n  You impose on any third-party recipients relating to Derivatives made by or\n  for You shall neither limit such third-party recipients' use of the Mistral\n  Model or any Derivatives made by or for Mistral AI in accordance with the\n  Mistral AI Research License nor conflict with any of its terms and conditions.\n\n  ## 3. Limitations\n\n  **3.1. Misrepresentation.** You must not misrepresent or imply, through any\n  means, that the Derivatives made by or for You and/or any modified version of\n  the Mistral Model You Distribute under your name and responsibility is an\n  official product of Mistral AI or has been endorsed, approved or validated by\n  Mistral AI, unless You are authorized by Us to do so in writing.\n\n  **3.2. Usage Limitation.** You shall only use the Mistral Models, Derivatives\n  (whether or not created by Mistral AI) and Outputs for Research Purposes.\n\n  ## 4. Intellectual Property\n\n  **4.1. Trademarks.** No trademark licenses are granted under this Agreement,\n  and in connection with the Mistral Models, You may not use any name or mark\n  owned by or associated with Mistral AI or any of its affiliates, except (i) as\n  required for reasonable and customary use in describing and Distributing the\n  Mistral Models and Derivatives made by or for Mistral AI and (ii) for\n  attribution purposes as required by this Agreement.\n\n  **4.2. Outputs.** We claim no ownership rights in and to the Outputs. You are\n  solely responsible for the Outputs You generate and their subsequent uses in\n  accordance with this Agreement. Any Outputs shall be subject to the\n  restrictions set out in Section 3 of this Agreement.\n\n  **4.3. Derivatives.** By entering into this Agreement, You accept that any\n  Derivatives that You may create or that may be created for You shall be\n  subject to the restrictions set out in Section 3 of this Agreement.\n\n  ## 5. Liability\n\n  **5.1. Limitation of liability.** In no event, unless required by applicable\n  law (such as deliberate and grossly negligent acts) or agreed to in writing,\n  shall Mistral AI be liable to You for damages, including any direct, indirect,\n  special, incidental, or consequential damages of any character arising as a\n  result of this Agreement or out of the use or inability to use the Mistral\n  Models and Derivatives (including but not limited to damages for loss of data,\n  loss of goodwill, loss of expected profit or savings, work stoppage, computer\n  failure or malfunction, or any damage caused by malware or security breaches),\n  even if  Mistral AI has been advised of the possibility of such damages.\n\n  **5.2. Indemnification.** You agree to indemnify and hold harmless Mistral AI\n  from and against any claims, damages, or losses arising out of or related to\n  Your use or Distribution of the Mistral Models and Derivatives.\n\n  ## 6. Warranty\n\n  **6.1. Disclaimer.** Unless required by applicable law or prior agreed to by\n  Mistral AI in writing, Mistral AI provides the Mistral Models and Derivatives\n  on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n  express or implied, including, without limitation, any warranties or\n  conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n  PARTICULAR PURPOSE. Mistral AI does not represent nor warrant that the Mistral\n  Models and Derivatives will be error-free, meet Your or any third party's\n  requirements, be secure or will allow You or any third party to achieve any\n  kind of result or generate any kind of content. You are solely responsible for\n  determining the appropriateness of using or Distributing the Mistral Models\n  and Derivatives and assume any risks associated with Your exercise of rights\n  under this Agreement.\n\n  ## 7. Termination\n\n  **7.1. Term.** This Agreement is effective as of the date of your acceptance\n  of this Agreement or access to the concerned Mistral Models or Derivatives and\n  will continue until terminated in accordance with the following terms.\n\n  **7.2. Termination.** Mistral AI may terminate this Agreement at any time if\n  You are in breach of this Agreement. Upon termination of this Agreement, You\n  must cease to use all Mistral Models and Derivatives and shall permanently\n  delete any copy thereof. The following provisions, in their relevant parts,\n  will survive any termination or expiration of this Agreement, each for the\n  duration necessary to achieve its own intended purpose (e.g. the liability\n  provision will survive until the end of the applicable limitation\n  period):Sections 5 (Liability), 6(Warranty), 7 (Termination) and 8 (General\n  Provisions).\n\n  **7.3. Litigation.** If You initiate any legal action or proceedings against\n  Us or any other entity (including a cross-claim or counterclaim in a lawsuit),\n  alleging that the Model or a Derivative, or any part thereof, infringe upon\n  intellectual property or other rights owned or licensable by You, then any\n  licenses granted to You under this Agreement will immediately terminate as of\n  the date such legal action or claim is filed or initiated.\n\n  ## 8. General provisions\n\n  **8.1. Governing laws.** This Agreement will be governed by the laws of\n  France, without regard to choice of law principles, and the UN Convention on\n  Contracts for the International Sale of Goods does not apply to this\n  Agreement.\n\n  **8.2. Competent jurisdiction.** The courts of Paris shall have exclusive\n  jurisdiction of any dispute arising out of this Agreement.\n\n  **8.3. Severability.** If any provision of this Agreement is held to be\n  invalid, illegal or unenforceable, the remaining provisions shall be\n  unaffected thereby and remain valid as if such provision had not been set\n  forth herein.\n\n  ## 9. Definitions\n\n  \"Agreement\": means this Mistral AI Research License agreement governing the\n  access, use, and Distribution of the Mistral Models, Derivatives and Outputs.\n\n  \"Derivative\": means any (i) modified version of the Mistral Model (including\n  but not limited to any customized or fine-tuned version thereof), (ii) work\n  based on the Mistral Model, or (iii) any other derivative work thereof.\n\n  \"Distribution\", \"Distributing\", \"Distribute\" or \"Distributed\": means\n  supplying, providing or making available, by any means, a copy of the Mistral\n  Models and/or the Derivatives as the case may be, subject to Section 3 of this\n  Agreement.\n\n  \"Mistral AI\", \"We\" or \"Us\": means Mistral AI, a French société par actions\n  simplifiée registered in the Paris commercial registry under the number 952\n  418 325, and having its registered seat at 15, rue des Halles, 75001 Paris.\n\n  \"Mistral Model\": means the foundational large language model(s), and its\n  elements which include algorithms, software, instructed checkpoints,\n  parameters, source code (inference code, evaluation code and, if applicable,\n  fine-tuning code) and any other elements associated thereto made available by\n  Mistral AI under this Agreement, including, if any, the technical\n  documentation, manuals and instructions for the use and operation thereof.\n\n  \"Research Purposes\": means any use of a Mistral Model,  Derivative, or Output\n  that is solely for (a) personal, scientific or academic research, and (b) for\n  non-profit and non-commercial purposes, and not directly or indirectly\n  connected to any commercial activities or business operations. For\n  illustration purposes, Research Purposes does not include (1) any usage of the\n  Mistral Model, Derivative or Output by individuals or contractors employed in\n  or engaged by companies in the context of (a) their daily tasks, or (b) any\n  activity (including but not limited to any testing or proof-of-concept) that\n  is intended to generate revenue, nor (2) any Distribution by a commercial\n  entity of the Mistral Model, Derivative or Output whether in return for\n  payment or free of charge, in any medium or form, including but not limited to\n  through a hosted or managed service (e.g. SaaS, cloud instances, etc.), or\n  behind a software layer.\n\n  \"Outputs\": means any content generated by the operation of the Mistral Models\n  or the Derivatives from  a prompt (i.e., text instructions) provided by users.\n  For the avoidance of doubt, Outputs do not include any components of a Mistral\n  Models, such as any fine-tuned versions of the Mistral Models, the weights, or\n  parameters.\n\n  \"You\": means the individual or entity entering into this Agreement with\n  Mistral AI.\n\n\n  *Mistral AI processes your personal data below to provide the model and\n  enforce its license. If you are affiliated with a commercial entity, we may\n  also send you communications about our models. For more information on your\n  rights and data handling, please see our <a\n  href=\"https://mistral.ai/terms/\">privacy policy</a>.*\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Country: country\n  Affiliation: text\n  Job title: text\n  I understand that I can only use the model, any derivative versions and their outputs for non-commercial research purposes: checkbox\n  I understand that if I am a commercial entity, I am not permitted to use or distribute the model internally or externally, or expose it in my own offerings without a commercial license: checkbox\n  I understand that if I upload the model, or any derivative version, on any platform, I must include the Mistral Research License: checkbox\n  I understand that for commercial use of the model, I can contact Mistral or use the Mistral AI API on la Plateforme or any of our cloud provider partners: checkbox\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Mistral Privacy Policy: checkbox\n  geo: ip_location\nextra_gated_description: >-\n  Mistral AI processes your personal data below to provide the model and enforce\n  its license. If you are affiliated with a commercial entity, we may also send\n  you communications about our models. For more information on your rights and\n  data handling, please see our <a href=\"https://mistral.ai/terms/\">privacy\n  policy</a>.\nextra_gated_button_content: Submit\ntags:\n- mistral-common\n---\n\n# Model Card for Ministral-8B-Instruct-2410\n\nWe introduce two new state-of-the-art models for local intelligence, on-device computing, and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B. \n\nThe Ministral-8B-Instruct-2410 Language Model is an instruct fine-tuned model significantly outperforming existing models of similar size, released under the Mistral Research License.\n\nIf you are interested in using Ministral-3B or Ministral-8B commercially, outperforming Mistral-7B, [reach out to us](https://mistral.ai/contact/).\n\nFor more details about les Ministraux please refer to our release [blog post](https://mistral.ai/news/ministraux).\n\n## Ministral 8B Key features\n- Released under the **Mistral Research License**, reach out to us for a commercial license\n- Trained with a **128k context window** with **interleaved sliding-window attention**\n- Trained on a large proportion of **multilingual and code data**\n- Supports **function calling**\n- Vocabulary size of **131k**, using the **V3-Tekken** tokenizer\n\n### Basic Instruct Template (V3-Tekken)\n\n```\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n*For more information about the tokenizer please refer to [mistral-common](https://github.com/mistralai/mistral-common)*\n\n## Ministral 8B Architecture\n\n| Feature               | Value                |\n|:---------------------:|:--------------------:|\n| **Architecture**      | Dense Transformer    |\n| **Parameters**        | 8,019,808,256        |\n| **Layers**            | 36                   |\n| **Heads**             | 32                   |\n| **Dim**               | 4096                 |\n| **KV Heads (GQA)**    | 8                    |\n| **Hidden Dim**        | 12288                |\n| **Head Dim**          | 128                  |\n| **Vocab Size**        | 131,072              |\n| **Context Length**    | 128k                 |\n| **Attention Pattern** | Ragged (128k,32k,32k,32k) |\n\n## Benchmarks\n\n#### Base Models\n\n<u>Knowledge & Commonsense</u>\n\n| Model       | MMLU | AGIEval | Winogrande | Arc-c | TriviaQA |\n|:-------------:|:------:|:---------:|:------------:|:-------:|:----------:|\n| Mistral 7B Base  | 62.5 | 42.5    | 74.2   | 67.9  | 62.5 |\n| Llama 3.1 8B Base | 64.7 | 44.4    | 74.6       | 46.0  | 60.2     |\n| ***Ministral 8B Base*** | ***<u>65.0</u>*** | ***<u>48.3</u>*** | ***<u>75.3</u>***   | ***<u>71.9</u>*** | ***<u>65.5</u>*** |\n|  |  |     |        |   |      |\n| Gemma 2 2B Base | 52.4 | 33.8    | 68.7   | 42.6  | 47.8     |\n| Llama 3.2 3B Base | 56.2 | 37.4    | 59.6       | 43.1  | 50.7     |\n| ***Ministral 3B Base*** | ***<u>60.9</u>*** | ***<u>42.1</u>***    | ***<u>72.7</u>***       | ***<u>64.2</u>*** | ***<u>56.7</u>***     |\n\n<u>Code & Math</u>\n\n| Model       | HumanEval pass@1 |GSM8K maj@8 |\n|:-------------:|:-------------------:|:---------------:|\n| Mistral 7B Base  | 26.8              | 32.0           |\n| Llama 3.1 8B Base | ***<u>37.8</u>***          | 42.2           |\n| ***Ministral 8B Base***  | 34.8              | ***<u>64.5</u>***       |\n|   |               |            |\n| Gemma 2 2B  | 20.1              | 35.5           |\n| Llama 3.2 3B | 14.6              | 33.5           |\n| ***Ministral 3B*** | ***<u>34.2</u>***          | ***<u>50.9</u>***       |\n\n<u>Multilingual</u>\n\n| Model       | French MMLU | German MMLU | Spanish MMLU |\n|:-------------:|:-------------:|:-------------:|:-------------:|\n| Mistral 7B Base  | 50.6         | 49.6         | 51.4         |\n| Llama 3.1 8B Base | 50.8         | 52.8         | 54.6         |\n| ***Ministral 8B Base*** | ***<u>57.5</u>***     | ***<u>57.4</u>***     | ***<u>59.6</u>***     |\n|   |          |          |          |\n| Gemma 2 2B Base  | 41.0         | 40.1         | 41.7         |\n| Llama 3.2 3B Base | 42.3         | 42.2         | 43.1         |\n| ***Ministral 3B Base*** | ***<u>49.1</u>***     | ***<u>48.3</u>***     | ***<u>49.5</u>***     |\n\n### Instruct Models\n\n<u>Chat/Arena (gpt-4o judge)</u>\n\n| Model       | MTBench | Arena Hard | Wild bench |\n|:-------------:|:---------:|:------------:|:------------:|\n| Mistral 7B Instruct v0.3  | 6.7     | 44.3       | 33.1       |\n| Llama 3.1 8B Instruct | 7.5     | 62.4       | 37.0       |\n| Gemma 2 9B Instruct | 7.6     | 68.7       | ***<u>43.8</u>***       |\n| ***Ministral 8B Instruct*** | ***<u>8.3</u>*** | ***<u>70.9</u>***   | 41.3   |\n|   |      |        |        |\n| Gemma 2 2B Instruct  | 7.5     | 51.7       | 32.5       |\n| Llama 3.2 3B Instruct | 7.2     | 46.0       | 27.2       |\n| ***Ministral 3B Instruct*** | ***<u>8.1</u>*** | ***<u>64.3</u>***   | ***<u>36.3</u>***   |\n\n<u>Code & Math</u>\n\n| Model       | MBPP pass@1 | HumanEval pass@1 | Math maj@1 |\n|:-------------:|:-------------:|:------------------:|:-------------:|\n| Mistral 7B Instruct v0.3  | 50.2        | 38.4             | 13.2        |\n| Gemma 2 9B Instruct | 68.5   | 67.7             | 47.4        |\n Llama 3.1 8B Instruct | 69.7   | 67.1             | 49.3        |\n| ***Ministral 8B Instruct*** | ***<u>70.0</u>***        | ***<u>76.8</u>***         | ***<u>54.5</u>***   |\n|   |         |              |         |\n| Gemma 2 2B Instruct  | 54.5        | 42.7             | 22.8        |\n| Llama 3.2 3B Instruct | 64.6        | 61.0             | 38.4        |\n| ***Ministral 3B* Instruct** | ***<u>67.7</u>***   | ***<u>77.4</u>***         | ***<u>51.7</u>***   |\n\n<u>Function calling</u>\n\n| Model       | Internal bench |\n|:-------------:|:-----------------:|\n| Mistral 7B Instruct v0.3  | 6.9             |\n| Llama 3.1 8B Instruct | N/A             |\n| Gemma 2 9B Instruct | N/A             |\n| ***Ministral 8B Instruct*** | ***<u>31.6</u>***       |\n|   |              |\n| Gemma 2 2B Instruct  | N/A             |\n| Llama 3.2 3B Instruct | N/A             |\n| ***Ministral 3B Instruct*** | ***<u>28.4</u>***       |\n\n## Usage Examples\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n> [!IMPORTANT]\n> Currently vLLM is capped at 32k context size because interleaved attention kernels for paged attention are not yet implemented in vLLM.\n> Attention kernels for paged attention are being worked on and as soon as it is fully supported in vLLM, this model card will be updated.\n> To take advantage of the full 128k context size we recommend [Mistral Inference](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410#mistral-inference)\n\n**_Installation_**\n\n\nMake sure you install `vLLM >= v0.6.4`:\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have `mistral_common >= 1.4.4` installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile).\n\n**_Offline_**\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\n\nsampling_params = SamplingParams(max_tokens=8192)\n\n# note that running Ministral 8B on a single GPU requires 24 GB of GPU RAM\n# If you want to divide the GPU requirement over multiple devices, please add *e.g.* `tensor_parallel=2`\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"Do we need to think for 10 seconds to find the answer of 1 + 1?\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# You don't need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n# and you can easily add these two numbers in your mind very quickly without any delay.\n```\n\n**_Server_**\n\nYou can also use Ministral-8B in a server/client setting. \n\n1. Spin up a server:\n\n\n```\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n\n**Note:** Running Ministral-8B on a single GPU requires 24 GB of GPU RAM. \n\nIf you want to divide the GPU requirement over multiple devices, please add *e.g.* `--tensor_parallel=2`\n\n2. And ping the client:\n\n```\ncurl --location 'http://<your-node-url>:8000/v1/chat/completions' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer token' \\\n--data '{\n    \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Do we need to think for 10 seconds to find the answer of 1 + 1?\"\n      }\n    ]\n}'\n\n```\n\n### Mistral-inference\n\nWe recommend using [mistral-inference](https://github.com/mistralai/mistral-inference) to quickly try out / \"vibe-check\" the model.\n\n\n**_Install_**\n\nMake sure to have `mistral_inference >= 1.5.0` installed.\n\n```\npip install mistral_inference --upgrade\n```\n\n**_Download_**\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', '8B-Instruct')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Ministral-8B-Instruct-2410\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/8B-Instruct --instruct --max_tokens 256\n```\n\n### Passkey detection\n\n> [!IMPORTANT]\n> In this example the passkey message has over >100k tokens and mistral-inference\n> does not have a chunked pre-fill mechanism. Therefore you will need a lot of\n> GPU memory in order to run the below example (80 GB). For a more memory-efficient\n> solution we recommend using vLLM.\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom pathlib import Path\nimport json\nfrom mistral_inference.generate import generate\nfrom huggingface_hub import hf_hub_download\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\ndef load_passkey_request() -> ChatCompletionRequest:\n    passkey_file = hf_hub_download(repo_id=\"mistralai/Ministral-8B-Instruct-2410\", filename=\"passkey_example.json\")\n\n    with open(passkey_file, \"r\") as f:\n        data = json.load(f)\n\n    message_content = data[\"messages\"][0][\"content\"]\n    return ChatCompletionRequest(messages=[UserMessage(content=message_content)])\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path, softmax_fp32=False)\n\ncompletion_request = load_passkey_request()\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)  # The pass key is 13005.\n```\n\n\n### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"How often does the letter r occur in Mistral?\")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.tekken import SpecialTokenPolicy\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\ntekken = tokenizer.instruct_tokenizer.tokenizer\ntekken.special_token_policy = SpecialTokenPolicy.IGNORE\n\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, Amélie Héliou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste Rozière, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, Eléonore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, Mickaël Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia Rozé, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, Timothée Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"vllm\",\"framework\":\"vllm\",\"params\":8019808256,\"storage_bytes\":32111185775,\"files_count\":16,\"spaces_count\":19,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{%- if messages[0][\\\"role\\\"] == \\\"system\\\" %}\\n    {%- set system_message = messages[0][\\\"content\\\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n{%- set user_messages = loop_messages | selectattr(\\\"role\\\", \\\"equalto\\\", \\\"user\\\") | list %}\\n\\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\\n{%- set ns = namespace() %}\\n{%- set ns.index = 0 %}\\n{%- for message in loop_messages %}\\n    {%- if not (message.role == \\\"tool\\\" or message.role == \\\"tool_results\\\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\\n        {%- if (message[\\\"role\\\"] == \\\"user\\\") != (ns.index % 2 == 0) %}\\n            {{- raise_exception(\\\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n        {%- endif %}\\n        {%- set ns.index = ns.index + 1 %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if message[\\\"role\\\"] == \\\"user\\\" %}\\n        {%- if tools is not none and (message == user_messages[-1]) %}\\n            {{- \\\"[AVAILABLE_TOOLS][\\\" }}\\n            {%- for tool in tools %}\\n                {%- set tool = tool.function %}\\n                {{- '{\\\"type\\\": \\\"function\\\", \\\"function\\\": {' }}\\n                {%- for key, val in tool.items() if key != \\\"return\\\" %}\\n                    {%- if val is string %}\\n                        {{- '\\\"' + key + '\\\": \\\"' + val + '\\\"' }}\\n                    {%- else %}\\n                        {{- '\\\"' + key + '\\\": ' + val|tojson }}\\n                    {%- endif %}\\n                    {%- if not loop.last %}\\n                        {{- \\\", \\\" }}\\n                    {%- endif %}\\n                {%- endfor %}\\n                {{- \\\"}}\\\" }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- else %}\\n                    {{- \\\"]\\\" }}\\n                {%- endif %}\\n            {%- endfor %}\\n            {{- \\\"[/AVAILABLE_TOOLS]\\\" }}\\n            {%- endif %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- \\\"[INST]\\\" + system_message + \\\"\\\\n\\\\n\\\" + message[\\\"content\\\"] + \\\"[/INST]\\\" }}\\n        {%- else %}\\n            {{- \\\"[INST]\\\" + message[\\\"content\\\"] + \\\"[/INST]\\\" }}\\n        {%- endif %}\\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\\n        {{- \\\"[TOOL_CALLS][\\\" }}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- set out = tool_call.function|tojson %}\\n            {{- out[:-1] }}\\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\\n                {{- raise_exception(\\\"Tool call IDs should be alphanumeric strings with length 9!\\\") }}\\n            {%- endif %}\\n            {{- ', \\\"id\\\": \\\"' + tool_call.id + '\\\"}' }}\\n            {%- if not loop.last %}\\n                {{- \\\", \\\" }}\\n            {%- else %}\\n                {{- \\\"]\\\" + eos_token }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- elif message[\\\"role\\\"] == \\\"assistant\\\" %}\\n        {{- message[\\\"content\\\"] + eos_token}}\\n    {%- elif message[\\\"role\\\"] == \\\"tool_results\\\" or message[\\\"role\\\"] == \\\"tool\\\" %}\\n        {%- if message.content is defined and message.content.content is defined %}\\n            {%- set content = message.content.content %}\\n        {%- else %}\\n            {%- set content = message.content %}\\n        {%- endif %}\\n        {{- '[TOOL_RESULTS]{\\\"content\\\": ' + content|string + \\\", \\\" }}\\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\\n            {{- raise_exception(\\\"Tool call IDs should be alphanumeric strings with length 9!\\\") }}\\n        {%- endif %}\\n        {{- '\\\"call_id\\\": \\\"' + message.tool_call_id + '\\\"}[/TOOL_RESULTS]' }}\\n    {%- else %}\\n        {{- raise_exception(\\\"Only user and assistant roles are supported, with the exception of an initial optional system message!\\\") }}\\n    {%- endif %}\\n{%- endfor %}\\n\",\"eos_token\":\"</s>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-common\",\"source_url\":\"https://github.com/mistralai/mistral-common\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:mistralai:mistral-inference\",\"source_url\":\"https://github.com/mistralai/mistral-inference\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 77.5,
    "content_hash": "0517519afb09e825b8ade511f0012a6f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mistralai/Ministral-8B-Instruct-2410\",\"fetched_at\":\"2025-12-10T01:31:39.556Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:playgroundai:playground-v2-1024px-aesthetic",
    "name": "playground-v2-1024px-aesthetic",
    "author": "playgroundai",
    "description": "--- license: other license_name: playground-v2-community license_link: https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md tags: - text-to-image - playground inference: parameters: guidance_scale: 3.0 --- This repository contains a model that generates highly aesthetic images of resolution 1024x1024. You can use the model with Hugging Face 🧨 Diffusers. !image/png **Playground v2** is a diffusion-based text-to-image generative model. The model was trained f...",
    "tags": [
      "diffusers",
      "safetensors",
      "text-to-image",
      "playground",
      "license:other",
      "endpoints_compatible",
      "diffusers:stablediffusionxlpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 563,
    "downloads": 767,
    "source": "huggingface",
    "source_url": "https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: playground-v2-community\nlicense_link: https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md\ntags:\n- text-to-image\n- playground\ninference:\n  parameters:\n    guidance_scale: 3.0\n---\n# Playground v2 – 1024px Aesthetic Model\n\nThis repository contains a model that generates highly aesthetic images of resolution 1024x1024. You can use the model with Hugging Face 🧨 Diffusers.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/p0up5GNQgO0vVIiJ672K7.png)\n\n\n**Playground v2** is a diffusion-based text-to-image generative model. The model was trained from scratch by the research team at [Playground](https://playground.com). \n\nImages generated by Playground v2 are favored **2.5** times more than those produced by Stable Diffusion XL, according to Playground’s [user study](#user-study).\n\nWe are thrilled to release [intermediate checkpoints](#intermediate-base-models) at different training stages, including evaluation metrics, to the community. We hope this will encourage further research into foundational models for image generation.\n\nLastly, we introduce a new benchmark, [MJHQ-30K](#mjhq-30k-benchmark), for automatic evaluation of a model’s aesthetic quality.\n\nPlease see our [blog](https://blog.playgroundai.com/playground-v2/) for more details.\n\n### Model Description\n\n- **Developed by:** [Playground](https://playground.com)\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [Playground v2 Community License](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md)\n- **Summary:** This model generates images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pre-trained text encoders ([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip) and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main)). It follows the same architecture as [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).\n\n### Using the model with 🧨 Diffusers\n\nInstall diffusers >= 0.24.0 and some dependencies:\n```\npip install transformers accelerate safetensors\n```\n\nTo use the model, run the following snippet.\n\n**Note**: It is recommend to use **`guidance_scale=3.0`**.\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"playgroundai/playground-v2-1024px-aesthetic\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    add_watermarker=False,\n    variant=\"fp16\"\n)\npipe.to(\"cuda\")\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nimage  = pipe(prompt=prompt, guidance_scale=3.0).images[0]\n```\n\n### Using the model with Automatic1111/ComfyUI\n\nIn order to use the model with software such as Automatic1111 or ComfyUI you can use [`playground-v2.fp16.safetensors`](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/playground-v2.fp16.safetensors) file.\n\n### User Study\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/8VzBkSYaUU3dt509Co9sk.png)\n\nAccording to user studies conducted by Playground, involving over 2,600 prompts and thousands of users, the images generated by Playground v2 are favored **2.5** times more than those produced by [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).\n\nWe report user preference metrics on [PartiPrompts](https://github.com/google-research/parti), following standard practice, and on an internal prompt dataset curated by the Playground team. The “Internal 1K” prompt dataset is diverse and covers various categories and tasks.\n\nDuring the user study, we give users instructions to evaluate image pairs based on both (1) their aesthetic preference and (2) the image-text alignment.\n\n### MJHQ-30K Benchmark\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/o3Bt62qFsTO9DkeX2yLua.png)\n\n| Model                                 | Overall FID   |\n| ------------------------------------- | ----- |\n| SDXL-1-0-refiner                      | 9.55  |\n| [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)        | **7.07**  |\n\nWe introduce a new benchmark, [MJHQ-30K](https://huggingface.co/datasets/playgroundai/MJHQ-30K), for automatic evaluation of a model’s aesthetic quality. The benchmark computes FID on a high-quality dataset to gauge aesthetic quality.\n\nWe have curated a high-quality dataset from Midjourney, featuring 10 common categories, with each category containing 3,000 samples. Following common practice, we use aesthetic score and CLIP score to ensure high image quality and high image-text alignment. Furthermore, we take extra care to make the data diverse within each category.\n\nFor Playground v2, we report both the overall FID and per-category FID. All FID metrics are computed at resolution 1024x1024. Our benchmark results show that our model outperforms SDXL-1-0-refiner in overall FID and all category FIDs, especially in people and fashion categories. This is in line with the results of the user study, which indicates a correlation between human preference and FID score on the MJHQ-30K benchmark.\n\nWe release this benchmark to the public and encourage the community to adopt it for benchmarking their models’ aesthetic quality.\n\n### Intermediate Base Models\n\n| Model                        | FID    | Clip Score |\n| ---------------------------- | ------ | ---------- |\n| SDXL-1-0-refiner             | 13.04  | 32.62      |\n| [playground-v2-256px-base](https://huggingface.co/playgroundai/playground-v2-256px-base)     | 9.83   | 31.90      |\n| [playground-v2-512px-base](https://huggingface.co/playgroundai/playground-v2-512px-base)     | 9.55   | 32.08      |\n\n\nApart from [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic), we release intermediate checkpoints at different training stages to the community in order to foster foundation model research in pixels. Here, we report the FID score and CLIP score on the MSCOCO14 evaluation set for the reference purposes. (Note that our reported numbers may differ from the numbers reported in SDXL’s published results, as our prompt list may be different.)\n\n### How to cite us\n\n```\n@misc{playground-v2,\n      url={[https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)},\n      title={Playground v2},\n      author={Li, Daiqing and Kamko, Aleks and Sabet, Ali and Akhgari, Ehsan and Xu, Linmiao and Doshi, Suhail}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":76318365586,\"files_count\":35,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionXLPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:mlfoundations:open_clip\",\"source_url\":\"https://github.com/mlfoundations/open_clip\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:CLIP\",\"source_url\":\"https://github.com/openai/CLIP\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:parti\",\"source_url\":\"https://github.com/google-research/parti\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 62.5,
    "content_hash": "776d436834d410b2dc60e7b99621a237",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic\",\"fetched_at\":\"2025-12-10T01:31:39.556Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]