[
  {
    "id": "huggingface:pyannote:speaker-diarization",
    "name": "speaker-diarization",
    "author": "pyannote",
    "description": "",
    "tags": [
      "pyannote-audio",
      "pyannote",
      "pyannote-audio-pipeline",
      "audio",
      "voice",
      "speech",
      "speaker",
      "speaker-diarization",
      "speaker-change-detection",
      "voice-activity-detection",
      "overlapped-speech-detection",
      "automatic-speech-recognition",
      "dataset:ami",
      "dataset:dihard",
      "dataset:voxconverse",
      "dataset:aishell",
      "dataset:repere",
      "dataset:voxceleb",
      "arxiv:2012.01477",
      "arxiv:2110.07058",
      "arxiv:2005.08072",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "automatic-speech-recognition",
    "likes": 1205,
    "downloads": 937695,
    "source": "huggingface",
    "source_url": "https://huggingface.co/pyannote/speaker-diarization",
    "image_url": null,
    "type": "dataset",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"automatic-speech-recognition\",\"library_name\":\"pyannote-audio\",\"framework\":\"pyannote-audio\",\"params\":null,\"storage_bytes\":null,\"files_count\":67,\"spaces_count\":100,\"gated\":\"auto\",\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2012.01477\",\"source_url\":\"https://arxiv.org/abs/2012.01477\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.07058\",\"source_url\":\"https://arxiv.org/abs/2110.07058\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2005.08072\",\"source_url\":\"https://arxiv.org/abs/2005.08072\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "972054b4fcaa8fd9fe20ad5e1c09e636",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/pyannote/speaker-diarization\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:sentence-transformers:all-mpnet-base-v2",
    "name": "all-mpnet-base-v2",
    "author": "sentence-transformers",
    "description": "--- language: en license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers - text-embeddings-inference datasets: - s2orc - flax-sentence-embeddings/stackexchange_xml - ms_marco - gooaq - yahoo_answers_topics - code_search_net - search_qa - eli5 - snli - multi_nli - wikihow - natural_questions - trivia_qa - embedding-data/sentence-compression - embedding-data/flickr30k-captions - embedding-data/altlex - embed...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "openvino",
      "mpnet",
      "fill-mask",
      "feature-extraction",
      "sentence-similarity",
      "transformers",
      "text-embeddings-inference",
      "en",
      "dataset:s2orc",
      "dataset:ms_marco",
      "dataset:gooaq",
      "dataset:yahoo_answers_topics",
      "dataset:code_search_net",
      "dataset:search_qa",
      "dataset:eli5",
      "dataset:snli",
      "dataset:multi_nli",
      "dataset:wikihow",
      "dataset:natural_questions",
      "dataset:trivia_qa",
      "dataset:embedding-data/sentence-compression",
      "dataset:embedding-data/flickr30k-captions",
      "dataset:embedding-data/altlex",
      "dataset:embedding-data/simple-wiki",
      "dataset:embedding-data/qqp",
      "dataset:embedding-data/specter",
      "dataset:embedding-data/paq_pairs",
      "dataset:embedding-data/wikianswers",
      "arxiv:1904.06472",
      "arxiv:2102.07033",
      "arxiv:2104.08727",
      "arxiv:1704.05179",
      "arxiv:1810.09305",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "sentence-similarity",
    "likes": 1202,
    "downloads": 24796035,
    "source": "huggingface",
    "source_url": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- text-embeddings-inference\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-mpnet-base-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Usage (Text Embeddings Inference (TEI))\n\n[Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) is a blazing fast inference solution for text embedding models.\n\n- CPU:\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\n```\n\n- NVIDIA GPU:\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\n```\n\nSend a request to `/v1/embeddings` to generate embeddings via the [OpenAI Embeddings API](https://platform.openai.com/docs/api-reference/embeddings/create):\n```bash\ncurl http://localhost:8080/v1/embeddings \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"sentence-transformers/all-mpnet-base-v2\",\n    \"input\": [\"This is an example sentence\", \"Each sentence is converted\"]\n  }'\n```\n\nOr check the [Text Embeddings Inference API specification](https://huggingface.github.io/text-embeddings-inference/) instead.\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |",
    "meta_json": "{\"pipeline_tag\":\"sentence-similarity\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":109486978,\"storage_bytes\":4105073268,\"files_count\":28,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MPNetForMaskedLM\"],\"model_type\":\"mpnet\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"eos_token\":\"</s>\",\"sep_token\":\"</s>\",\"cls_token\":\"<s>\",\"unk_token\":\"[UNK]\",\"pad_token\":\"<pad>\",\"mask_token\":\"<mask>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:text-embeddings-inference\",\"source_url\":\"https://github.com/huggingface/text-embeddings-inference\"},{\"type\":\"has_code\",\"target_id\":\"github:PolyAI-LDN:conversational-datasets\",\"source_url\":\"https://github.com/PolyAI-LDN/conversational-datasets\"},{\"type\":\"has_code\",\"target_id\":\"github:allenai:s2orc\",\"source_url\":\"https://github.com/allenai/s2orc\"},{\"type\":\"has_code\",\"target_id\":\"github:afader:oqa\",\"source_url\":\"https://github.com/afader/oqa#wikianswers-corpus\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:PAQ\",\"source_url\":\"https://github.com/facebookresearch/PAQ\"},{\"type\":\"has_code\",\"target_id\":\"github:allenai:s2orc\",\"source_url\":\"https://github.com/allenai/s2orc\"},{\"type\":\"has_code\",\"target_id\":\"github:allenai:s2orc\",\"source_url\":\"https://github.com/allenai/s2orc\"},{\"type\":\"has_code\",\"target_id\":\"github:allenai:gooaq\",\"source_url\":\"https://github.com/allenai/gooaq\"},{\"type\":\"has_code\",\"target_id\":\"github:allenai:specter\",\"source_url\":\"https://github.com/allenai/specter\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research-datasets:sentence-compression\",\"source_url\":\"https://github.com/google-research-datasets/sentence-compression\"},{\"type\":\"has_code\",\"target_id\":\"github:pvl:wikihow_pairs_dataset\",\"source_url\":\"https://github.com/pvl/wikihow_pairs_dataset\"},{\"type\":\"has_code\",\"target_id\":\"github:chridey:altlex\",\"source_url\":\"https://github.com/chridey/altlex\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.06472\",\"source_url\":\"https://arxiv.org/abs/1904.06472\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2102.07033\",\"source_url\":\"https://arxiv.org/abs/2102.07033\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.08727\",\"source_url\":\"https://arxiv.org/abs/2104.08727\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1704.05179\",\"source_url\":\"https://arxiv.org/abs/1704.05179\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1810.09305\",\"source_url\":\"https://arxiv.org/abs/1810.09305\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a55edfb443b189776be3ad5c18e2e9fe",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:xlabs-ai:flux-realismlora",
    "name": "flux-RealismLora",
    "author": "XLabs-AI",
    "description": "--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE. language: - en pipeline_tag: text-to-image tags: - lora - Stable Diffusion - image-generation - Flux - diffusers base_model: black-forest-labs/FLUX.1-dev --- !Lora Photorealism for Flux <img src=\"https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true\"> This repository provides a checkpoint with trai...",
    "tags": [
      "diffusers",
      "lora",
      "stable diffusion",
      "image-generation",
      "flux",
      "text-to-image",
      "en",
      "base_model:black-forest-labs/flux.1-dev",
      "base_model:adapter:black-forest-labs/flux.1-dev",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 1200,
    "downloads": 35478,
    "source": "huggingface",
    "source_url": "https://huggingface.co/XLabs-AI/flux-RealismLora",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.\nlanguage:\n- en\npipeline_tag: text-to-image\ntags:\n- lora\n- Stable Diffusion\n- image-generation\n- Flux\n- diffusers\nbase_model: black-forest-labs/FLUX.1-dev\n---\n![Lora Photorealism for Flux](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/lora-photorealism-header-rev1.png?raw=true)\n[<img src=\"https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true\">](https://discord.gg/FHY2guThfy)\n\nThis repository provides a checkpoint with trained LoRA photorealism for\n[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) by Black Forest Labs\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/picture-6-rev1.png?raw=true)\n# ComfyUI\n\n[See our github](https://github.com/XLabs-AI/x-flux-comfyui) for comfy ui workflows.\n![Example Picture 1](https://github.com/XLabs-AI/x-flux-comfyui/blob/main/assets/image1.png?raw=true)\n# Training details\n[XLabs AI](https://github.com/XLabs-AI) team is happy to publish fine-tuning Flux scripts, including:\n\n- **LoRA** ðŸ”¥\n- **ControlNet** ðŸ”¥\n\n[See our github](https://github.com/XLabs-AI/x-flux) for train script and train configs.\n\n# Training Dataset\nDataset has the following format for the training process:\n\n```\nâ”œâ”€â”€ images/\nâ”‚    â”œâ”€â”€ 1.png\nâ”‚    â”œâ”€â”€ 1.json\nâ”‚    â”œâ”€â”€ 2.png\nâ”‚    â”œâ”€â”€ 2.json\nâ”‚    â”œâ”€â”€ ...\n```\nA .json file contains \"caption\" field with a text prompt.\n\n# Inference\n```bash\npython3 demo_lora_inference.py \\\n    --checkpoint lora.safetensors \\\n    --prompt \" handsome girl in a suit covered with bold tattoos and holding a pistol. Animatrix illustration style, fantasy style, natural photo cinematic\"\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/picture-0-rev1.png?raw=true)\n\n\n# License\n\nlora.safetensors falls under the [FLUX.1 [dev]](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License<br/>",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":75395758,\"files_count\":3,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux-comfyui\",\"source_url\":\"https://github.com/XLabs-AI/x-flux-comfyui\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux-comfyui\",\"source_url\":\"https://github.com/XLabs-AI/x-flux-comfyui\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"},{\"type\":\"has_code\",\"target_id\":\"github:XLabs-AI:x-flux\",\"source_url\":\"https://github.com/XLabs-AI/x-flux\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "c97049633bfe7d6cd0a5a0c3408c00cc",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/XLabs-AI/flux-RealismLora\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-3.2-1b-instruct",
    "name": "Llama-3.2-1B-Instruct",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "arxiv:2405.16406",
      "license:llama3.2",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1191,
    "downloads": 3486407,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1235814400,\"storage_bytes\":4945506836,\"files_count\":13,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"chat_template\":\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n        {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n        {{- '\\\"parameters\\\": ' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \\\"}\\\" }}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|eot_id|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.05149\",\"source_url\":\"https://arxiv.org/abs/2204.05149\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2405.16406\",\"source_url\":\"https://arxiv.org/abs/2405.16406\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.2",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "fb104ee074007f0f94ccc299242b05d1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:ai21labs:jamba-v0.1",
    "name": "Jamba-v0.1",
    "author": "ai21labs",
    "description": "--- library_name: transformers license: apache-2.0 tags: - jamba - mamba - moe --- This is the base version of the Jamba model. Weâ€™ve since released a better, instruct-tuned version, Jamba-1.5-Mini. For even greater performance, check out the scaled-up Jamba-1.5-Large. Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. It delivers throughput gains over traditional Transformer-based models, while outperforming or matching the leading models of its size class on most common benchmarks. Ja...",
    "tags": [
      "transformers",
      "safetensors",
      "jamba",
      "text-generation",
      "mamba",
      "moe",
      "custom_code",
      "arxiv:2403.19887",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1186,
    "downloads": 1159,
    "source": "huggingface",
    "source_url": "https://huggingface.co/ai21labs/Jamba-v0.1",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- jamba\n- mamba\n- moe\n---\n\nThis is the base version of the Jamba model. Weâ€™ve since released a better, instruct-tuned version, [Jamba-1.5-Mini](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini). For even greater performance, check out the scaled-up [Jamba-1.5-Large](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large).\n\n# Model Card for Jamba\n\nJamba is a state-of-the-art, hybrid SSM-Transformer LLM. It delivers throughput gains over traditional Transformer-based models, while outperforming or matching the leading models of its size class on most common benchmarks.\n\nJamba is the first production-scale Mamba implementation, which opens up interesting research and application opportunities. While this initial experimentation shows encouraging gains, we expect these to be further enhanced with future optimizations and explorations.\n\nThis model card is for the base version of Jamba. Itâ€™s a pretrained, mixture-of-experts (MoE) generative text model, with 12B active parameters and a total of 52B parameters across all experts. It supports a 256K context length, and can fit up to 140K tokens on a single 80GB GPU.\n\nFor full details of this model please read the [white paper](https://arxiv.org/abs/2403.19887) and the [release blog post](https://www.ai21.com/blog/announcing-jamba).\n\n## Model Details\n\n- **Developed by:** [AI21](https://www.ai21.com)\n- **Model type:** Joint Attention and Mamba (Jamba)\n- **License:** Apache 2.0\n- **Context length:** 256K\n- **Knowledge cutoff date:** March 5, 2024\n\n## Usage\n### Presequities\nIn order to use Jamba, it is recommended you use `transformers` version 4.40.0 or higher (version 4.39.0 or higher is required):\n```bash\npip install transformers>=4.40.0\n```\n\nIn order to run optimized Mamba implementations, you first need to install `mamba-ssm` and `causal-conv1d`:\n```bash\npip install mamba-ssm causal-conv1d>=1.2.0\n```\nYou also have to have the model on a CUDA device.\n\nYou can run the model not using the optimized Mamba kernels, but it is **not** recommended as it will result in significantly lower latencies. In order to do that, you'll need to specify `use_mamba_kernels=False` when loading the model.\n\n### Run the model\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n\ninput_ids = tokenizer(\"In the recent Super Bowl LVIII,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n\noutputs = model.generate(input_ids, max_new_tokens=216)\n\nprint(tokenizer.batch_decode(outputs))\n# [\"<|startoftext|>In the recent Super Bowl LVIII, the Kansas City Chiefs emerged victorious, defeating the San Francisco 49ers in a thrilling overtime showdown. The game was a nail-biter, with both teams showcasing their skills and determination.\\n\\nThe Chiefs, led by their star quarterback Patrick Mahomes, displayed their offensive prowess, while the 49ers, led by their strong defense, put up a tough fight. The game went into overtime, with the Chiefs ultimately securing the win with a touchdown.\\n\\nThe victory marked the Chiefs' second Super Bowl win in four years, solidifying their status as one of the top teams in the NFL. The game was a testament to the skill and talent of both teams, and a thrilling end to the NFL season.\\n\\nThe Super Bowl is not just about the game itself, but also about the halftime show and the commercials. This year's halftime show featured a star-studded lineup, including Usher, Alicia Keys, and Lil Jon. The show was a spectacle of music and dance, with the performers delivering an energetic and entertaining performance.\\n\"]\n```\n\nPlease note that if you're using `transformers<4.40.0`, `trust_remote_code=True` is required for running the new Jamba architecture.\n\n<details>\n<summary><strong>Loading the model in half precision</strong></summary>\n  \n  The published checkpoint is saved in BF16. In order to load it into RAM in BF16/FP16, you need to specify `torch_dtype`:\n  \n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n                                             torch_dtype=torch.bfloat16)    # you can also use torch_dtype=torch.float16\n```\n\nWhen using half precision, you can enable the [FlashAttention2](https://github.com/Dao-AILab/flash-attention) implementation of the Attention blocks. In order to use it, you also need the model on a CUDA device. Since in this precision the model is to big to fit on a single 80GB GPU, you'll also need to parallelize it using [accelerate](https://huggingface.co/docs/accelerate/index):\n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             device_map=\"auto\")\n```\n\n</details>\n<details><summary><strong>Load the model in 8-bit</strong></summary>\n  \n  **Using 8-bit precision, it is possible to fit up to 140K sequence lengths on a single 80GB GPU.** You can easily quantize the model to 8-bit using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index). In order to not degrade model quality, we recommend to exclude the Mamba blocks from the quantization:\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True,\n                                         llm_int8_skip_modules=[\"mamba\"])\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation=\"flash_attention_2\",\n                                             quantization_config=quantization_config)\n```\n</details>\n\n### Fine-tuning example\nJamba is a base model that can be fine-tuned for custom solutions (including for chat/instruct versions). You can fine-tune it using any technique of your choice. Here is an example of fine-tuning with the [PEFT](https://huggingface.co/docs/peft/index) library (requires ~120GB GPU RAM, in example 2xA100 80GB):\n\n```python\nimport torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"ai21labs/Jamba-v0.1\", device_map='auto', torch_dtype=torch.bfloat16)\n\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\n        \"embed_tokens\", \n        \"x_proj\", \"in_proj\", \"out_proj\", # mamba\n        \"gate_proj\", \"up_proj\", \"down_proj\", # mlp\n        \"q_proj\", \"k_proj\", \"v_proj\" # attention\n    ],\n    task_type=\"CAUSAL_LM\",\n    bias=\"none\"\n)\n\ndataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\ntraining_args = SFTConfig(\n    output_dir=\"./results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n    logging_dir='./logs',\n    logging_steps=10,\n    learning_rate=1e-5,\n    dataset_text_field=\"quote\",\n)\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n## Results on common benchmarks\n| Benchmark    | Score |\n|--------------|:-----:|\n| HellaSwag    | 87.1% |\n| Arc Challenge | 64.4% |\n| WinoGrande   | 82.5% |\n| PIQA        | 83.2% |\n| MMLU       | 67.4% |\n| BBH            | 45.4% |\n| TruthfulQA          | 46.4% |\n| GSM8K (CoT)            | 59.9% |\n\nIt's crucial that the 'BOS' token is added to all prompts, which might not be enabled by default in all eval frameworks.\n\n\n## Notice\nJamba is a pretrained base model and did not undergo any alignment for instruct/chat interactions. \n\nAs a base model, Jamba is intended for use as a foundation layer for fine tuning, training, and developing custom solutions. Jamba does not have safety moderation mechanisms and guardrails should be added for responsible and safe use.\n\n## About AI21\nAI21 builds reliable, practical, and scalable AI solutions for the enterprise.\n\nJamba is the first in AI21â€™s new family of models, and the Instruct version of Jamba is coming soon to the [AI21 platform](https://www.ai21.com/studio). \n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":51570323328,\"storage_bytes\":206290066038,\"files_count\":32,\"spaces_count\":16,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"JambaForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_jamba.JambaConfig\",\"AutoModel\":\"modeling_jamba.JambaModel\",\"AutoModelForCausalLM\":\"modeling_jamba.JambaForCausalLM\",\"AutoModelForSequenceClassification\":\"model.JambaForSequenceClassification\"},\"model_type\":\"jamba\",\"tokenizer_config\":{\"bos_token\":\"<|startoftext|>\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|pad|>\",\"unk_token\":\"<|unk|>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Dao-AILab:flash-attention\",\"source_url\":\"https://github.com/Dao-AILab/flash-attention\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.19887\",\"source_url\":\"https://arxiv.org/abs/2403.19887\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "7c990b1967b0b09d2babf5bc78e09011",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/ai21labs/Jamba-v0.1\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tiiuae:falcon-40b-instruct",
    "name": "falcon-40b-instruct",
    "author": "tiiuae",
    "description": "--- datasets: - tiiuae/falcon-refinedweb language: - en inference: false license: apache-2.0 --- **Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.** *Paper coming soon ðŸ˜Š.* ðŸ¤— To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! * **You are looking for a ready-to-use chat/instruct model b...",
    "tags": [
      "transformers",
      "pytorch",
      "falcon",
      "text-generation",
      "custom_code",
      "en",
      "dataset:tiiuae/falcon-refinedweb",
      "arxiv:2205.14135",
      "arxiv:1911.02150",
      "arxiv:2005.14165",
      "arxiv:2104.09864",
      "arxiv:2306.01116",
      "arxiv:2304.01196",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1179,
    "downloads": 43214,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tiiuae/falcon-40b-instruct",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\ninference: false\nlicense: apache-2.0\n---\n\n# âœ¨ Falcon-40B-Instruct\n\n**Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of [Baize](https://github.com/project-baize/baize-chatbot). It is made available under the Apache 2.0 license.**\n\n*Paper coming soon ðŸ˜Š.*\n\nðŸ¤— To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-40B-Instruct?\n\n* **You are looking for a ready-to-use chat/instruct model based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).**\n* **Falcon-40B is the best open-source model available.** It outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n\nðŸ’¬ **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b). \n\nðŸ’¸ **Looking for a smaller, less expensive model?** [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) is Falcon-40B-Instruct's little brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 85-100GB of memory** to swiftly run inference with Falcon-40B.\n\n\n\n# Model Card for Falcon-40B-Instruct\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English and French;\n- **License:** Apache 2.0;\n- **Finetuned from model:** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nFalcon-40B-Instruct has been finetuned on a chat dataset.\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-40B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-40B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-40B-Instruct was finetuned on a 150M tokens from [Bai ze](https://github.com/project-baize/baize-chatbot) mixed with 5% of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) data. \n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\nFor more information about pretraining, see [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).\n\n### Model Architecture and Objective\n\nFalcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\nFor multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 60        |                                        |\n| `d_model`          | 8192      |                                        |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-40B-Instruct was trained on AWS SageMaker, on 64 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-40B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* ðŸ˜Š. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the ðŸ““ [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\nTo cite the [Baize](https://github.com/project-baize/baize-chatbot) instruction dataset used for this model: \n```\n@article{xu2023baize,\n  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},\n  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},\n  journal={arXiv preprint arXiv:2304.01196},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-40B-Instruct is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":167352935898,\"files_count\":20,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"FalconForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_falcon.FalconConfig\",\"AutoModel\":\"modeling_falcon.FalconModel\",\"AutoModelForSequenceClassification\":\"modeling_falcon.FalconForSequenceClassification\",\"AutoModelForTokenClassification\":\"modeling_falcon.FalconForTokenClassification\",\"AutoModelForQuestionAnswering\":\"modeling_falcon.FalconForQuestionAnswering\",\"AutoModelForCausalLM\":\"modeling_falcon.FalconForCausalLM\"},\"model_type\":\"falcon\",\"tokenizer_config\":{\"eos_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:project-baize:baize-chatbot\",\"source_url\":\"https://github.com/project-baize/baize-chatbot\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:llama\",\"source_url\":\"https://github.com/facebookresearch/llama\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:StableLM\",\"source_url\":\"https://github.com/Stability-AI/StableLM\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:text-generation-inference\",\"source_url\":\"https://github.com/huggingface/text-generation-inference\"},{\"type\":\"has_code\",\"target_id\":\"github:project-baize:baize-chatbot\",\"source_url\":\"https://github.com/project-baize/baize-chatbot\"},{\"type\":\"has_code\",\"target_id\":\"github:project-baize:baize-chatbot\",\"source_url\":\"https://github.com/project-baize/baize-chatbot\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.14135\",\"source_url\":\"https://arxiv.org/abs/2205.14135\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.02150\",\"source_url\":\"https://arxiv.org/abs/1911.02150\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2005.14165\",\"source_url\":\"https://arxiv.org/abs/2005.14165\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.09864\",\"source_url\":\"https://arxiv.org/abs/2104.09864\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.01116\",\"source_url\":\"https://arxiv.org/abs/2306.01116\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.01196\",\"source_url\":\"https://arxiv.org/abs/2304.01196\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "84785c2c877cf93e2fa83d06ffc79b8f",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tiiuae/falcon-40b-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:mosaicml:mpt-7b",
    "name": "mpt-7b",
    "author": "mosaicml",
    "description": "--- license: apache-2.0 tags: - Composer - MosaicML - llm-foundry - StreamingDatasets datasets: - mc4 - c4 - togethercomputer/RedPajama-Data-1T - bigcode/the-stack - allenai/s2orc inference: false --- MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML. MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and in...",
    "tags": [
      "transformers",
      "pytorch",
      "mpt",
      "text-generation",
      "composer",
      "mosaicml",
      "llm-foundry",
      "streamingdatasets",
      "custom_code",
      "dataset:mc4",
      "dataset:c4",
      "dataset:togethercomputer/redpajama-data-1t",
      "dataset:bigcode/the-stack",
      "dataset:allenai/s2orc",
      "arxiv:2108.12409",
      "arxiv:2302.13971",
      "arxiv:2205.14135",
      "arxiv:2010.04245",
      "arxiv:1909.08053",
      "arxiv:2302.06675",
      "license:apache-2.0",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1175,
    "downloads": 13233,
    "source": "huggingface",
    "source_url": "https://huggingface.co/mosaicml/mpt-7b",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ntags:\n- Composer\n- MosaicML\n- llm-foundry\n- StreamingDatasets\ndatasets:\n- mc4\n- c4\n- togethercomputer/RedPajama-Data-1T\n- bigcode/the-stack\n- allenai/s2orc\ninference: false\n---\n\n# MPT-7B\n\nMPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.\nThis model was trained by [MosaicML](https://www.mosaicml.com).\n\nMPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\n\nThese architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing\npositional embeddings with Attention with Linear Biases ([ALiBi](https://arxiv.org/abs/2108.12409)).\nThanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.\nMPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's [FasterTransformer](https://github.com/NVIDIA/FasterTransformer).\n\nThis model uses the MosaicML LLM codebase, which can be found in the [llm-foundry repository](https://github.com/mosaicml/llm-foundry). It was trained by MosaicMLâ€™s NLP team on the [MosaicML platform](https://www.mosaicml.com/training) for LLM pretraining, finetuning, and inference.\n\n### How is this model different?\n\nMPT-7B is\n\n* **Licensed for the possibility of commercial use** (unlike [LLaMA](https://arxiv.org/abs/2302.13971)).\n* **Trained on a large amount of data** (1T tokens like [LLaMA](https://arxiv.org/abs/2302.13971) vs. 300B for [Pythia](https://github.com/EleutherAI/pythia), 300B for [OpenLLaMA](https://github.com/openlm-research/open_llama), and 800B for [StableLM](https://github.com/Stability-AI/StableLM)).\n* **Prepared to handle extremely long inputs** thanks to [ALiBi](https://arxiv.org/abs/2108.12409) (we finetuned [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter) on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).\n* **Capable of fast training and inference** (via [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer))\n* **Equipped with highly efficient open-source training code** via the [llm-foundry repository](https://github.com/mosaicml/llm-foundry)\n\n### Models finetuned off MPT-7B:\n\nThe following models are finetuned on MPT-7B:\n\n* [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter): a model designed to read and write fictional stories with super long context lengths.\nBuilt by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the [books3 dataset](https://huggingface.co/datasets/the_pile_books3).\nAt inference time, thanks to [ALiBi](https://arxiv.org/abs/2108.12409), MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 80k tokens on a single A100-80GB GPU in our [blogpost](www.mosaicml.com/blog/mpt-7b).\n  * License: Apache 2.0\n\n* [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct): a model for short-form instruction following.\nBuilt by finetuning MPT-7B on a [dataset](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) we also release, derived from the [Databricks Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) and the [Anthropic Helpful and Harmless (HH-RLHF)](https://huggingface.co/datasets/Anthropic/hh-rlhf) datasets.\n  * License: Apache 2.0\n\n* [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat): a chatbot-like model for dialogue generation.\nBuilt by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3),\n [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets.\n  * License: _CC-By-NC-SA-4.0_\n\n## Model Date\n\nMay 5, 2023\n\n## Model License\n\nApache-2.0\n\n## Documentation\n\n* [Blog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)\n* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n* Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n## How to Use\n\nThis model is best used with the MosaicML [llm-foundry repository](https://github.com/mosaicml/llm-foundry) for training and finetuning.\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b',\n  trust_remote_code=True\n)\n```\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method.\nThis is because we use a custom `MPT` model architecture that is not yet part of the Hugging Face `transformers` package.\n`MPT` includes options for many training efficiency features such as [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), [QK LayerNorm](https://arxiv.org/abs/2010.04245), and more.\n\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl='triton'` and with `bfloat16` precision:\n```python\nimport torch\nimport transformers\n\nname = 'mosaicml/mpt-7b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n```\n\nAlthough the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n\n```python\nimport transformers\n\nname = 'mosaicml/mpt-7b'\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n```\n\nThis model was trained with the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n```\n\nThe model can then be used, for example, within a text-generation pipeline.  \nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n```\n\n## Model Description\n\nThe architecture is a modification of a standard decoder-only transformer.\n\nThe model has been modified from a standard transformer in the following ways:\n* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n* It does not use biases\n\n\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 6.7B |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 50432 |\n| sequence length | 2048 |\n\n\n\n## Training Data\n\n### Streaming Datasets\n\nData was formatted using the MosaicML [StreamingDataset](https://github.com/mosaicml/streaming) library to host our data in object storage and efficiently stream it to our compute cluster during training.\nStreamingDataset obviates the need to download the whole dataset before starting training, and allows instant resumption of training from any point in the dataset.\n\n\n### Data Mix\n\nThe model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:\n\n\n| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |\n|-------------|----------------------------|------------|----------------------------|--------|\n| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |\n| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |\n| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |\n| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |\n| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |\n| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |\n| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |\n| RedPajama - Books | 26.02 B | 0.03 | 30B | 1.15 |\n| RedPajama - arXiv | 28.10 B | 0.019 | 19 B | 0.68 |\n| RedPajama - StackExchange | 20.54 B | 0.014 | 14 B |0.68 |\n\nSamples for each batch were selected from one of the datasets with the probability specified above.\nThe examples were shuffled within each dataset, and each example was constructed from as many sequences from that dataset as were necessary to fill the 2048 sequence length.\n\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer. This BPE tokenizer has a number of desirable characteristics,\nmost of which are relevant for tokenizing code:\n(1) It was trained on a diverse mix of data that includes code (The Pile)\n(2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces\n(3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\n\nThe model vocabulary size of 50432 was set to be a multiple of 128 (as in [MEGATRON-LM](https://arxiv.org/abs/1909.08053)), model flop utilization (MFU) increased by up to four percentage points.\n\n### Training Configuration\n\nThis model was trained on 440 A100-40GBs for about 9.5 days using the [MosaicML Platform](https://www.mosaicml.com/platform).\nThe model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n\n## Limitations and Biases\n\n_The following language is modified from [EleutherAI's GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\n\nMPT-7B (Base) is **not** intended for deployment without finetuning.\nIt should not be used for human-facing interactions without further guardrails and user consent.\n\nMPT-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n\n## MosaicML Platform\n\nIf you're interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b).\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\n\n## Citation\n\nPlease cite this model using the following format:\n\n```\n@online{MosaicML2023Introducing,\n    author    = {MosaicML NLP Team},\n    title     = {Introducing MPT-7B: A New Standard for Open-Source,\n    Commercially Usable LLMs},\n    year      = {2023},\n    url       = {www.mosaicml.com/blog/mpt-7b},\n    note      = {Accessed: 2023-05-05},\n    urldate   = {2023-05-05}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":26597235302,\"files_count\":26,\"spaces_count\":60,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MPTForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_mpt.MPTConfig\",\"AutoModelForCausalLM\":\"modeling_mpt.MPTForCausalLM\"},\"model_type\":\"mpt\",\"tokenizer_config\":{\"bos_token\":\"<|endoftext|>\",\"eos_token\":\"<|endoftext|>\",\"unk_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:FasterTransformer\",\"source_url\":\"https://github.com/NVIDIA/FasterTransformer\"},{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:llm-foundry\",\"source_url\":\"https://github.com/mosaicml/llm-foundry\"},{\"type\":\"has_code\",\"target_id\":\"github:EleutherAI:pythia\",\"source_url\":\"https://github.com/EleutherAI/pythia\"},{\"type\":\"has_code\",\"target_id\":\"github:openlm-research:open_llama\",\"source_url\":\"https://github.com/openlm-research/open_llama\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:StableLM\",\"source_url\":\"https://github.com/Stability-AI/StableLM\"},{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:FasterTransformer\",\"source_url\":\"https://github.com/NVIDIA/FasterTransformer\"},{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:llm-foundry\",\"source_url\":\"https://github.com/mosaicml/llm-foundry\"},{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:llm-foundry\",\"source_url\":\"https://github.com/mosaicml/llm-foundry\"},{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:llm-foundry\",\"source_url\":\"https://github.com/mosaicml/llm-foundry\"},{\"type\":\"has_code\",\"target_id\":\"github:openai:triton\",\"source_url\":\"https://github.com/openai/triton\"},{\"type\":\"has_code\",\"target_id\":\"github:mosaicml:streaming\",\"source_url\":\"https://github.com/mosaicml/streaming\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.12409\",\"source_url\":\"https://arxiv.org/abs/2108.12409\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2302.13971\",\"source_url\":\"https://arxiv.org/abs/2302.13971\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.14135\",\"source_url\":\"https://arxiv.org/abs/2205.14135\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2010.04245\",\"source_url\":\"https://arxiv.org/abs/2010.04245\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1909.08053\",\"source_url\":\"https://arxiv.org/abs/1909.08053\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2302.06675\",\"source_url\":\"https://arxiv.org/abs/2302.06675\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "a2bbd05932b62d1fa2176e5426211673",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/mosaicml/mpt-7b\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:bytedance-seed:bagel-7b-mot",
    "name": "BAGEL-7B-MoT",
    "author": "ByteDance-Seed",
    "description": "--- license: apache-2.0 base_model: - Qwen/Qwen2.5-7B-Instruct pipeline_tag: any-to-any library_name: bagel-mot --- <p align=\"left\"> <img src=\"https://lf3-static.bytednsdoc.com/obj/eden-cn/nuhojubrps/banner.png\" alt=\"BAGEL\" width=\"480\"/> </p> <p align=\"left\"> <a href=\"https://bagel-ai.org/\"> <img src=\"https://img.shields.io/badge/BAGEL-Website-0A66C2?logo=safari&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\" alt=\"BAGEL Website\" /> </a> <a href=\"https://arxiv.org/abs/2...",
    "tags": [
      "bagel-mot",
      "safetensors",
      "bagel",
      "any-to-any",
      "custom_code",
      "arxiv:2505.14683",
      "base_model:qwen/qwen2.5-7b-instruct",
      "base_model:finetune:qwen/qwen2.5-7b-instruct",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "any-to-any",
    "likes": 1167,
    "downloads": 860,
    "source": "huggingface",
    "source_url": "https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen2.5-7B-Instruct\npipeline_tag: any-to-any\nlibrary_name: bagel-mot\n---\n\n\n<p align=\"left\">\n  <img src=\"https://lf3-static.bytednsdoc.com/obj/eden-cn/nuhojubrps/banner.png\" alt=\"BAGEL\" width=\"480\"/>\n</p>\n\n\n# ðŸ¥¯ BAGEL â€¢ Unified Model for Multimodal Understanding and Generation\n\n\n\n<p align=\"left\">\n  <a href=\"https://bagel-ai.org/\">\n    <img\n      src=\"https://img.shields.io/badge/BAGEL-Website-0A66C2?logo=safari&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"\n      alt=\"BAGEL Website\"\n    />\n  </a>\n  <a href=\"https://arxiv.org/abs/2505.14683\">\n    <img\n      src=\"https://img.shields.io/badge/BAGEL-Paper-red?logo=arxiv&logoColor=red\" style=\"display: inline-block; vertical-align: middle;\"\n      alt=\"BAGEL Paper on arXiv\"\n    />\n  </a>\n  <a href=\"https://github.com/bytedance-seed/BAGEL\" target=\"_blank\" style=\"margin: 2px;\">\n      <img \n        alt=\"Github\" src=\"https://img.shields.io/badge/BAGEL-Codebase-536af5?color=536af5&logo=github\" style=\"display: inline-block; vertical-align: middle;\"\n        alt=\"BAGEL Codebase\"\n      />\n  </a>\n  <a href=\"https://demo.bagel-ai.org/\">\n    <img\n      src=\"https://img.shields.io/badge/BAGEL-Demo-blue?logo=googleplay&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"\n      alt=\"BAGEL Demo\"\n    />\n  </a>\n  <a href=\"https://discord.com/invite/Z836xxzy\">\n    <img\n      src=\"https://img.shields.io/badge/BAGEL-Discord-green?logo=discord&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"\n      alt=\"BAGEL Discord\"\n    />\n  </a>\n\n  \n</p>\n\n\n> We present **BAGEL**, an openâ€‘source multimodal foundation model with 7B active parameters (14B total) trained on largeâ€‘scale interleaved multimodal data. BAGEL outperforms the current topâ€‘tier openâ€‘source VLMs like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards, and delivers textâ€‘toâ€‘image quality that is competitive with strong specialist generators such as SD3.\nMoreover, BAGEL demonstrates superior qualitative results in classical imageâ€‘editing scenarios than the leading open-source models. More importantly, it extends to free-form visual manipulation, multiview synthesis, and world navigation, capabilities that constitute \"world-modeling\" tasks beyond the scope of previous image-editing models.\n\n\nThis repository hosts the model weights for **BAGEL**. For installation, usage instructions, and further documentation, please visit our [GitHub repository](https://github.com/bytedance-seed/BAGEL).\n\n\n\n<p align=\"left\"><img src=\"https://github.com/ByteDance-Seed/Bagel/raw/main/assets/teaser.webp\" width=\"80%\"></p>\n\n\n\n\n\n\n## ðŸ§  Method\nBAGEL adopts a Mixture-of-Transformer-Experts (MoT) architecture to maximize the modelâ€™s capacity to learn from richly diverse multimodal information. Following the same principle of capacity maximization, it utilizes two separate encoders to capture pixel-level and semantic-level features of an image. The overall framework follows a Next Group of Token Prediction paradigm, where the model is trained to predict the next group of language or visual tokens as a compression target.\n\nBAGEL scales MoTâ€™s capacity through Pre-training, Continued Training, and Supervised Finetuning on trillions of interleaved multimodal tokens spanning language, image, video, and web data. It surpasses open models on standard understanding and generation benchmarks and demonstrates advanced in-context multimodal abilities like free-form image editing, future frame prediction, 3D manipulation, world navigation, and sequential reasoning.\n\n<p align=\"left\"><img src=\"https://github.com/ByteDance-Seed/Bagel/raw/main/assets/arch.png\" width=\"50%\"></p>\n\n\n## ðŸŒ± Emerging Properties\n<p align=\"left\"><img src=\"https://github.com/ByteDance-Seed/Bagel/raw/main/assets/emerging_curves.png\" width=\"50%\"></p>\n\nAs we scale up BAGELâ€™s pretraining with more multimodal tokens, we observe consistent performance gains across understanding, generation, and editing tasks. Different capabilities emerge at distinct training stagesâ€”multimodal understanding and generation appear early, followed by basic editing, while complex, intelligent editing emerges later. This staged progression suggests an emergent pattern, where advanced multimodal reasoning builds on well-formed foundational skills. Ablation studies further show that combining VAE and ViT features significantly improves intelligent editing, underscoring the importance of visual-semantic context in enabling complex multimodal reasoning and further supporting its role in the emergence of advanced capabilities.\n\n\n\n## ðŸ“Š Benchmarks\n### 1. Visual Understanding\n| Model | MME â†‘ | MMBench â†‘ |   MMMU â†‘ | MM-Vet â†‘ | MathVista â†‘ |\n| ------------------- | ----------: | ----------: | -------: | -------: | ----------: |\n| Janus-Pro-7B        | -  |     79.2 |     41.0 |     50.0 |           â€“ |\n| Qwen2.5-VL-7B      | 2347    |   83.5 | **58.6** |     67.1 |           68.2 |\n| **BAGEL**    | **2388**  |  **85.0** |     55.3 | **67.2** |    **73.1** |\n### 2. Text-to-Image Generation Â· GenEval\n| Model        | Overall â†‘ |\n| ------------ | --------- |\n| FLUX-1-dev   | 0.82      |\n| SD3-Medium   | 0.74      |\n| Janus-Pro-7B | 0.80      |\n| **BAGEL**    | **0.88**  |\n### 3. Image Editing\n| Model         | GEdit-Bench-EN (SC) â†‘ | GEdit-Bench-EN (PQ) â†‘ | GEdit-Bench-EN (O) â†‘ | IntelligentBench â†‘ |\n| ------------- | --------------------- | --------------------- | ------------------- | ------------------ |\n| Step1X-Edit   | 7.09                  | 6.76                  | **6.70**            | 14.9               |\n| Gemini-2-exp. | 6.73                  | 6.61                  | 6.32                | **57.6**           |\n| **BAGEL**     | **7.36**              | **6.83**              | 6.52                | 44.0               |\n| **BAGEL+CoT** | â€“                   | â€“                     | â€“                   | 55.3               |\n\n## License\nBAGEL is licensed under the Apache 2.0 license. It is finetuned from [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) and [siglip-so400m-14-384-flash-attn2](https://huggingface.co/HuggingFaceM4/siglip-so400m-14-384-flash-attn2) model, and uses the [FLUX.1-schnell VAE model](https://huggingface.co/black-forest-labs/FLUX.1-schnell), all under Apache 2.0.\n\n## âœï¸ Citation\n```bibtex\n@article{deng2025bagel,\n  title   = {Emerging Properties in Unified Multimodal Pretraining},\n  author  = {Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and Shi, Guang and Fan, Haoqi},\n  journal = {arXiv preprint arXiv:2505.14683},\n  year    = {2025}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"any-to-any\",\"library_name\":\"bagel-mot\",\"framework\":\"bagel-mot\",\"params\":14691079811,\"storage_bytes\":29555840231,\"files_count\":14,\"spaces_count\":9,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BagelForConditionalGeneration\"],\"model_type\":\"bagel\",\"auto_map\":{\"AutoConfig\":\"configuration_bagel.BagelConfig\",\"AutoModelForCausalLM\":\"modeling_bagel.BagelForConditionalGeneration\"},\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:bytedance-seed:BAGEL\\\"\",\"source_url\":\"https://github.com/bytedance-seed/BAGEL\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:bytedance-seed:BAGEL\",\"source_url\":\"https://github.com/bytedance-seed/BAGEL\"},{\"type\":\"has_code\",\"target_id\":\"github:ByteDance-Seed:Bagel\",\"source_url\":\"https://github.com/ByteDance-Seed/Bagel\"},{\"type\":\"has_code\",\"target_id\":\"github:ByteDance-Seed:Bagel\",\"source_url\":\"https://github.com/ByteDance-Seed/Bagel\"},{\"type\":\"has_code\",\"target_id\":\"github:ByteDance-Seed:Bagel\",\"source_url\":\"https://github.com/ByteDance-Seed/Bagel\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.14683\",\"source_url\":\"https://arxiv.org/abs/2505.14683\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "aea253171c69367202e1db556079b56e",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:timbrooks:instruct-pix2pix",
    "name": "instruct-pix2pix",
    "author": "timbrooks",
    "description": "--- license: mit tags: - image-to-image --- GitHub: https://github.com/timothybrooks/instruct-pix2pix <img src='https://instruct-pix2pix.timothybrooks.com/teaser.jpg'/> To use , install using for now. The pipeline will be available in the next release",
    "tags": [
      "diffusers",
      "safetensors",
      "image-to-image",
      "license:mit",
      "diffusers:stablediffusioninstructpix2pixpipeline",
      "region:us"
    ],
    "pipeline_tag": "image-to-image",
    "likes": 1162,
    "downloads": 60961,
    "source": "huggingface",
    "source_url": "https://huggingface.co/timbrooks/instruct-pix2pix",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\ntags:\n- image-to-image\n---\n\n# InstructPix2Pix: Learning to Follow Image Editing Instructions\nGitHub: https://github.com/timothybrooks/instruct-pix2pix\n<img src='https://instruct-pix2pix.timothybrooks.com/teaser.jpg'/>\n\n\n\n## Example\n\nTo use `InstructPix2Pix`, install `diffusers` using `main` for now. The pipeline will be available in the next release\n\n```bash\npip install diffusers accelerate safetensors transformers\n```\n\n```python\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n\nmodel_id = \"timbrooks/instruct-pix2pix\"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\npipe.to(\"cuda\")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n\nurl = \"https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg\"\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image\nimage = download_image(url)\n\nprompt = \"turn him into cyborg\"\nimages = pipe(prompt, image=image, num_inference_steps=10, image_guidance_scale=1).images\nimages[0]\n```",
    "meta_json": "{\"pipeline_tag\":\"image-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":37333325594,\"files_count\":31,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionInstructPix2PixPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:timothybrooks:instruct-pix2pix\",\"source_url\":\"https://github.com/timothybrooks/instruct-pix2pix\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 50,
    "content_hash": "40788bd799dece7ad9a18971c80231e2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/timbrooks/instruct-pix2pix\",\"fetched_at\":\"2025-12-10T01:31:39.548Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:zai-org:chatglm3-6b",
    "name": "chatglm3-6b",
    "author": "zai-org",
    "description": "--- language: - zh - en tags: - glm - chatglm - thudm --- <p align=\"center\"> ðŸ’» <a href=\"https://github.com/THUDM/ChatGLM\" target=\"_blank\">Github Repo</a> â€¢ ðŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> â€¢ ðŸ“ƒ <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> â€¢ ðŸ“ƒ <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GL...",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "chatglm",
      "glm",
      "thudm",
      "custom_code",
      "zh",
      "en",
      "arxiv:2103.10360",
      "arxiv:2210.02414",
      "arxiv:2406.12793",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 1156,
    "downloads": 64611,
    "source": "huggingface",
    "source_url": "https://huggingface.co/zai-org/chatglm3-6b",
    "image_url": null,
    "type": "tool",
    "body_content": "---\nlanguage:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm\n---\n# ChatGLM3-6B\n<p align=\"center\">\n  ðŸ’» <a href=\"https://github.com/THUDM/ChatGLM\" target=\"_blank\">Github Repo</a> â€¢ ðŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> â€¢ ðŸ“ƒ <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> â€¢ ðŸ“ƒ <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GLM-130B\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n\n<p align=\"center\">\n    ðŸ‘‹ Join our <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-25ti5uohv-A_hs~am_D3Q8XPZMpj7wwQ\" target=\"_blank\">Slack</a> and <a href=\"https://github.com/THUDM/ChatGLM/blob/main/resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<p align=\"center\">\nðŸ“Experience the larger-scale ChatGLM model at <a href=\"https://www.chatglm.cn\">chatglm.cn</a>\n</p>\n\n## GLM-4 å¼€æºæ¨¡åž‹\n\næˆ‘ä»¬å·²ç»å‘å¸ƒæœ€æ–°çš„ **GLM-4** æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæœ‰äº†æ–°çš„çªç ´ï¼Œæ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ¸ é“ä½“éªŒæˆ‘ä»¬çš„æœ€æ–°æ¨¡åž‹ã€‚\n+ [GLM-4 å¼€æºæ¨¡åž‹](https://huggingface.co/THUDM/glm-4-9b-chat) æˆ‘ä»¬å·²ç»å¼€æºäº† GLM-4-9B ç³»åˆ—æ¨¡åž‹ï¼Œåœ¨å„é¡¹æŒ‡æ ‡çš„æµ‹è¯•ä¸Šæœ‰æ˜Žæ˜¾æå‡ï¼Œæ¬¢è¿Žå°è¯•ã€‚\n\n## ä»‹ç» (Introduction)\nChatGLM3-6B æ˜¯ ChatGLM ç³»åˆ—æœ€æ–°ä¸€ä»£çš„å¼€æºæ¨¡åž‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡åž‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½Žç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š\n\n1. **æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡åž‹ï¼š** ChatGLM3-6B çš„åŸºç¡€æ¨¡åž‹ ChatGLM3-6B-Base é‡‡ç”¨äº†æ›´å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒæ­¥æ•°å’Œæ›´åˆç†çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æŽ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒè§’åº¦çš„æ•°æ®é›†ä¸Šæµ‹è¯„æ˜¾ç¤ºï¼ŒChatGLM3-6B-Base å…·æœ‰åœ¨ 10B ä»¥ä¸‹çš„é¢„è®­ç»ƒæ¨¡åž‹ä¸­æœ€å¼ºçš„æ€§èƒ½ã€‚\n2. **æ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒï¼š** ChatGLM3-6B é‡‡ç”¨äº†å…¨æ–°è®¾è®¡çš„ [Prompt æ ¼å¼](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md)ï¼Œé™¤æ­£å¸¸çš„å¤šè½®å¯¹è¯å¤–ã€‚åŒæ—¶åŽŸç”Ÿæ”¯æŒ[å·¥å…·è°ƒç”¨](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README.md)ï¼ˆFunction Callï¼‰ã€ä»£ç æ‰§è¡Œï¼ˆCode Interpreterï¼‰å’Œ Agent ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯ã€‚\n3. **æ›´å…¨é¢çš„å¼€æºåºåˆ—ï¼š** é™¤äº†å¯¹è¯æ¨¡åž‹ ChatGLM3-6B å¤–ï¼Œè¿˜å¼€æºäº†åŸºç¡€æ¨¡åž‹ ChatGLM-6B-Baseã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡åž‹ ChatGLM3-6B-32Kã€‚ä»¥ä¸Šæ‰€æœ‰æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œåœ¨å¡«å†™[é—®å·](https://open.bigmodel.cn/mla/form)è¿›è¡Œç™»è®°åŽ**äº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨**ã€‚\n\nChatGLM3-6B is the latest open-source model in the ChatGLM series. While retaining many excellent features such as smooth dialogue and low deployment threshold from the previous two generations, ChatGLM3-6B introduces the following features:\n\n1. **More Powerful Base Model:** The base model of ChatGLM3-6B, ChatGLM3-6B-Base, employs a more diverse training dataset, more sufficient training steps, and a more reasonable training strategy. Evaluations on datasets such as semantics, mathematics, reasoning, code, knowledge, etc., show that ChatGLM3-6B-Base has the strongest performance among pre-trained models under 10B.\n2. **More Comprehensive Function Support:** ChatGLM3-6B adopts a newly designed [Prompt format](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT_en.md), in addition to the normal multi-turn dialogue. It also natively supports [function call](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README_en.md), code interpreter, and complex scenarios such as agent tasks.\n3. **More Comprehensive Open-source Series:** In addition to the dialogue model ChatGLM3-6B, the base model ChatGLM-6B-Base and the long-text dialogue model ChatGLM3-6B-32K are also open-sourced. All the weights are **fully open** for academic research, and after completing the [questionnaire](https://open.bigmodel.cn/mla/form) registration, they are also **allowed for free commercial use**.\n\n## è½¯ä»¶ä¾èµ– (Dependencies)\n\n```shell\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate\n```\n\n## ä»£ç è°ƒç”¨ (Code Usage)\n\nå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM3-6B æ¨¡åž‹æ¥ç”Ÿæˆå¯¹è¯ï¼š\n\nYou can generate dialogue by invoking the ChatGLM3-6B model with the following code:\n\n```ipython\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n>>> print(response)\nä½ å¥½ðŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿Žé—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n>>> response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€Žä¹ˆåŠž\", history=history)\n>>> print(response)\næ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n\n1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ çŽ¯å¢ƒ:ç¡®ä¿ç¡çœ çŽ¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£Žã€‚\n3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºŽç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚\n5. é¿å…åœ¨åºŠä¸Šåšä¸Žç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸Žç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,çŽ©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åŽç¼“æ…¢å‘¼æ°”ã€‚\n\nå¦‚æžœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚\n```\n\nå…³äºŽæ›´å¤šçš„ä½¿ç”¨è¯´æ˜Žï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œå’Œç½‘é¡µç‰ˆæœ¬çš„ DEMOï¼Œä»¥åŠä½¿ç”¨æ¨¡åž‹é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [Github Repo](https://github.com/THUDM/ChatGLM)ã€‚\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM).\n\n\n## åè®® (License)\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºï¼ŒChatGLM3-6B æ¨¡åž‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚\n\nThe code in this repository is open-sourced under the [Apache-2.0 license](LICENSE), while the use of the ChatGLM3-6B model weights needs to comply with the [Model License](MODEL_LICENSE).\n\n## å¼•ç”¨ (Citation)\n\nå¦‚æžœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ã€‚\n\nIf you find our work helpful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```\n",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":6243584032,\"storage_bytes\":37462649896,\"files_count\":27,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"model_type\":\"chatglm\",\"architectures\":[\"ChatGLMModel\"],\"auto_map\":{\"AutoConfig\":\"configuration_chatglm.ChatGLMConfig\",\"AutoModel\":\"modeling_chatglm.ChatGLMForConditionalGeneration\",\"AutoModelForCausalLM\":\"modeling_chatglm.ChatGLMForConditionalGeneration\",\"AutoModelForSeq2SeqLM\":\"modeling_chatglm.ChatGLMForConditionalGeneration\",\"AutoModelForSequenceClassification\":\"modeling_chatglm.ChatGLMForSequenceClassification\"},\"tokenizer_config\":{\"chat_template\":\"{% for message in messages %}{% if loop.first %}[gMASK]sop<|{{ message['role'] }}|>\\n {{ message['content'] }}{% else %}<|{{ message['role'] }}|>\\n {{ message['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}\",\"eos_token\":\"</s>\",\"pad_token\":\"<unk>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM\\\"\",\"source_url\":\"https://github.com/THUDM/ChatGLM\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:GLM\\\"\",\"source_url\":\"https://github.com/THUDM/GLM\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:GLM-130B\\\"\",\"source_url\":\"https://github.com/THUDM/GLM-130B\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM\",\"source_url\":\"https://github.com/THUDM/ChatGLM\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM3\",\"source_url\":\"https://github.com/THUDM/ChatGLM3\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM3\",\"source_url\":\"https://github.com/THUDM/ChatGLM3\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM3\",\"source_url\":\"https://github.com/THUDM/ChatGLM3\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM3\",\"source_url\":\"https://github.com/THUDM/ChatGLM3\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM\",\"source_url\":\"https://github.com/THUDM/ChatGLM\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:ChatGLM\",\"source_url\":\"https://github.com/THUDM/ChatGLM\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.10360\",\"source_url\":\"https://arxiv.org/abs/2103.10360\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.02414\",\"source_url\":\"https://arxiv.org/abs/2210.02414\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2406.12793\",\"source_url\":\"https://arxiv.org/abs/2406.12793\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 55,
    "content_hash": "2810e3ea07b79881d3d5a43fc86e5794",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/zai-org/chatglm3-6b\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-4-scout-17b-16e-instruct",
    "name": "Llama-4-Scout-17B-16E-Instruct",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "llama4",
      "any-to-any",
      "facebook",
      "meta",
      "pytorch",
      "llama",
      "ar",
      "de",
      "en",
      "es",
      "fr",
      "hi",
      "id",
      "it",
      "pt",
      "th",
      "tl",
      "vi",
      "arxiv:2204.05149",
      "base_model:meta-llama/llama-4-scout-17b-16e",
      "license:other",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "any-to-any",
    "likes": 1154,
    "downloads": 210866,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"any-to-any\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":108641793536,\"storage_bytes\":217343257722,\"files_count\":64,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Llama4ForConditionalGeneration\"],\"model_type\":\"llama4\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"eos_token\":\"<|eot|>\",\"pad_token\":\"<|finetune_right_pad|>\"},\"chat_template_jinja\":\"{{- bos_token }}\\n{%- if custom_tools is defined and custom_tools%}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if tools is defined and tools %}\\n    {%- set tool_definition = tool_definition ~ (tools | tojson(indent=4)) %}\\n{%- else %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set user_provided_system_message = true %}\\n    {%- if messages[0]['content'] is string %}\\n        {%- set system_message = messages[0]['content']|trim %}\\n    {%- else %}\\n        {%- set system_message = messages[0]['content'][0]['text']|trim %}\\n    {%- endif %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- if tools is not none  %}\\n        {#- Since not system_message was provided by user, if tool is provided, system_message is now default tool system message #}\\n        {#- This system message is from llama website:https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/  #}\\n        {%- set system_message = \\\"You are a helpful assistant and an expert in function composition. You can answer general questions using your internal knowledge OR invoke functions when necessary. Follow these strict guidelines:\\\\n\\\\n1. FUNCTION CALLS:\\\\n- ONLY use functions that are EXPLICITLY listed in the function list below\\\\n- If NO functions are listed (empty function list []), respond ONLY with internal knowledge or \\\\\\\"I don't have access to [Unavailable service] information\\\\\\\"\\\\n- If a function is not in the list, respond ONLY with internal knowledge or \\\\\\\"I don't have access to [Unavailable service] information\\\\\\\"\\\\n- If ALL required parameters are present AND the query EXACTLY matches a listed function's purpose: output ONLY the function call(s)\\\\n- Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\\\\nExamples:\\\\nCORRECT: [get_weather(location=\\\\\\\"Vancouver\\\\\\\"), calculate_route(start=\\\\\\\"Boston\\\\\\\", end=\\\\\\\"New York\\\\\\\")] <- Only if get_weather and calculate_route are in function list\\\\nINCORRECT: get_weather(location=\\\\\\\"New York\\\\\\\")\\\\nINCORRECT: Let me check the weather: [get_weather(location=\\\\\\\"New York\\\\\\\")]\\\\nINCORRECT: [get_events(location=\\\\\\\"Singapore\\\\\\\")] <- If function not in list\\\\n\\\\n2. RESPONSE RULES:\\\\n- For pure function requests matching a listed function: ONLY output the function call(s)\\\\n- For knowledge questions: ONLY output text\\\\n- For missing parameters: ONLY request the specific missing parameters\\\\n- For unavailable services (not in function list): output ONLY with internal knowledge or \\\\\\\"I don't have access to [Unavailable service] information\\\\\\\". Do NOT execute a function call.\\\\n- If the query asks for information beyond what a listed function provides: output ONLY with internal knowledge about your limitations\\\\n- NEVER combine text and function calls in the same response\\\\n- NEVER suggest alternative functions when the requested service is unavailable\\\\n- NEVER create or invent new functions not listed below\\\\n\\\\n3. STRICT BOUNDARIES:\\\\n- ONLY use functions from the list below - no exceptions\\\\n- NEVER use a function as an alternative to unavailable information\\\\n- NEVER call functions not present in the function list\\\\n- NEVER add explanatory text to function calls\\\\n- NEVER respond with empty brackets\\\\n- Use proper Python/JSON syntax for function calls\\\\n- Check the function list carefully before responding\\\\n\\\\n4. TOOL RESPONSE HANDLING:\\\\n- When receiving tool responses: provide concise, natural language responses\\\\n- Don't repeat tool response verbatim\\\\n- Don't add supplementary information\\\\n\\\\nHere is a list of functions in JSON format that you can invoke:\\\\n\\\" %}\\n    {%- else %}\\n        {%- set system_message = \\\"\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{#- Now writing the system message: use the user provided system message if user_provided_system_message, else default tool system message if tools presented #}\\n{%- if system_message %}\\n    {#- always use user provided system message to override default tool system message #}\\n    {{- \\\"<|header_start|>system<|header_end|>\\\\n\\\\n\\\" }}\\n    {{- system_message }}\\n    {%- if user_provided_system_message and tools %}\\n        {{- \\\"\\\\nHere is a list of functions in JSON format that you can invoke. Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\\\\n\\\" }}\\n        {{- tool_definition -}}\\n        {%- elif tool_definition %}\\n        {{- tool_definition -}}\\n    {%- endif %}\\n    {{- \\\"<|eot|>\\\" }}\\n{%- endif %}\\n\\n{#- Now deal with all other messages #}\\n{%- for message in messages %}\\n    {#- Base case: messages that are not from tool role and has empty tool_call list  #}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or ('tool_calls' in message and  message.tool_calls|length != 0 )) %}\\n        {{- '<|header_start|>' + message['role'] + '<|header_end|>\\\\n\\\\n' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] | trim }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n    {{- \\\"<|eot|>\\\" }}\\n    {#- Tool case: messages has non-empty tool_call list, must from assistant #}\\n    {%- elif 'tool_calls' in message %}\\n        {#- assume tool_calls are always coming from assistant #}\\n        {%- if message.role == 'assistant' %}\\n            {{- '<|header_start|>assistant<|header_end|>\\\\n\\\\n' -}}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n            {{- \\\"[\\\" }}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n                {{-  tool_call.name + '(' -}}\\n            {%- for param in tool_call.arguments %}\\n                {{- param + '=\\\"' -}}\\n                {{- \\\"%s\\\" | format(tool_call.arguments[param]) -}}\\n                {{- '\\\"' -}}\\n                {% if not loop.last %}, {% endif %}\\n            {%- endfor %}\\n            {{- ')' -}}\\n            {% if not loop.last %}, {% endif %}\\n        {%- endfor %}\\n        {{- \\\"]<|eot|>\\\" }}\\n{%- endif %}\\n{#- Tool_response case: messages are from tool_response  #}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|header_start|>ipython<|header_end|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is string %}\\n            {{-  message.content  | tojson }}\\n        {%- else %}\\n            {%- for content in message['content']  %}\\n                {%- if content['type']  == 'text' %}\\n                    {{-  content['text'] | tojson }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|header_start|>assistant<|header_end|>\\\\n\\\\n' }}\\n{%- endif %}\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2204.05149\",\"source_url\":\"https://arxiv.org/abs/2204.05149\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "772cd880181934f77efecb7bc101b1ac",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:rednote-hilab:dots.ocr",
    "name": "dots.ocr",
    "author": "rednote-hilab",
    "description": "--- license: mit library_name: dots_ocr pipeline_tag: image-text-to-text tags: - image-to-text - ocr - document-parse - layout - table - formula - transformers - custom_code language: - en - zh - multilingual --- <div align=\"center\"> <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png\" width=\"300\"/> <p> <h1 align=\"center\"> dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model </h1> <div align=\"center\"> <a href...",
    "tags": [
      "dots_ocr",
      "safetensors",
      "text-generation",
      "image-to-text",
      "ocr",
      "document-parse",
      "layout",
      "table",
      "formula",
      "transformers",
      "custom_code",
      "image-text-to-text",
      "conversational",
      "en",
      "zh",
      "multilingual",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 1154,
    "downloads": 1070227,
    "source": "huggingface",
    "source_url": "https://huggingface.co/rednote-hilab/dots.ocr",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: dots_ocr\npipeline_tag: image-text-to-text\ntags:\n- image-to-text\n- ocr\n- document-parse\n- layout\n- table\n- formula\n- transformers\n- custom_code\nlanguage:\n- en\n- zh\n- multilingual\n---\n\n<div align=\"center\">\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png\" width=\"300\"/>\n<p>\n\n<h1 align=\"center\">\ndots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model\n</h1>\n\n[![Blog](https://img.shields.io/badge/Blog-View_on_GitHub-333.svg?logo=github)](https://github.com/rednote-hilab/dots.ocr/blob/master/assets/blog.md)\n[![HuggingFace](https://img.shields.io/badge/HuggingFace%20Weights-black.svg?logo=HuggingFace)](https://huggingface.co/rednote-hilab/dots.ocr)\n\n\n<div align=\"center\">\n  <a href=\"https://dotsocr.xiaohongshu.com\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>ðŸ–¥ï¸ Live Demo</strong></a> | \n  <a href=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/wechat.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>ðŸ’¬ WeChat</strong></a> | \n  <a href=\"https://www.xiaohongshu.com/user/profile/683ffe42000000001d021a4c\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>ðŸ“• rednote</strong></a>\n</div>\n\n</div>\n\n\n\n## Introduction\n\n**dots.ocr** is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance.\n\n1. **Powerful Performance:** **dots.ocr** achieves SOTA performance for text, tables, and reading order on [OmniDocBench](https://github.com/opendatalab/OmniDocBench), while delivering formula recognition results comparable to much larger models like Doubao-1.5 and gemini2.5-pro.\n2. **Multilingual Support:** **dots.ocr** demonstrates robust parsing capabilities for low-resource languages, achieving decisive advantages across both layout detection and content recognition on our in-house multilingual documents benchmark.\n3. **Unified and Simple Architecture:** By leveraging a single vision-language model, **dots.ocr** offers a significantly more streamlined architecture than conventional methods that rely on complex, multi-model pipelines. Switching between tasks is accomplished simply by altering the input prompt, proving that a VLM can achieve competitive detection results compared to traditional detection models like DocLayout-YOLO.\n4.  **Efficient and Fast Performance:** Built upon a compact 1.7B LLM, **dots.ocr** provides faster inference speeds than many other high-performing models based on larger foundations.\n\n\n## Usage with transformers\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\n\nmodel_path = \"./weights/DotsOCR\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nimage_path = \"demo/demo_image1.jpg\"\nprompt = \"\"\"Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.\n\n1. Bbox format: [x1, y1, x2, y2]\n\n2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].\n\n3. Text Extraction & Formatting Rules:\n    - Picture: For the 'Picture' category, the text field should be omitted.\n    - Formula: Format its text as LaTeX.\n    - Table: Format its text as HTML.\n    - All Others (Text, Title, etc.): Format their text as Markdown.\n\n4. Constraints:\n    - The output text must be the original text from the image, with no translation.\n    - All layout elements must be sorted according to human reading order.\n\n5. Final Output: The entire output must be a single JSON object.\n\"\"\"\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": image_path\n                },\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }\n    ]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n### Performance Comparison: dots.ocr vs. Competing Models\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/chart.png\" border=\"0\" />\n\n> **Notes:** \n> - The EN, ZH metrics are the end2end evaluation results of [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and Multilingual metric is the end2end evaluation results of dots.ocr-bench.\n\n\n## News \n* ```2025.07.30 ``` ðŸš€ We release [dots.ocr](https://github.com/rednote-hilab/dots.ocr), â€” a multilingual documents parsing model based on 1.7b llm, with SOTA performance.\n\n\n\n## Benchmark Results\n\n### 1. OmniDocBench\n\n#### The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan=\"2\"><strong>Model<br>Type</strong></th>\n<th rowspan=\"2\"><strong>Methods</strong></th>\n<th colspan=\"2\"><strong>Overall<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"2\"><strong>Text<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"2\"><strong>Formula<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"2\"><strong>Table<sup>TEDS</sup>â†‘</strong></th>\n<th colspan=\"2\"><strong>Table<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"2\"><strong>Read Order<sup>Edit</sup>â†“</strong></th>\n</tr>\n<tr>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan=\"8\"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.150</td>\n<td>0.357</td>\n<td>0.061</td>\n<td>0.215</td>\n<td>0.278</td>\n<td>0.577</td>\n<td>78.6</td>\n<td>62.1</td>\n<td>0.180</td>\n<td>0.344</td>\n<td>0.079</td>\n<td>0.292</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.336</td>\n<td>0.556</td>\n<td>0.080</td>\n<td>0.315</td>\n<td>0.530</td>\n<td>0.883</td>\n<td>67.6</td>\n<td>49.2</td>\n<td>0.619</td>\n<td>0.685</td>\n<td>0.114</td>\n<td>0.340</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.191</td>\n<td>0.365</td>\n<td>0.105</td>\n<td>0.384</td>\n<td>0.306</td>\n<td>0.454</td>\n<td>77.0</td>\n<td>67.1</td>\n<td>0.243</td>\n<td>0.320</td>\n<td>0.108</td>\n<td>0.304</td>\n</tr>\n<tr>\n<td>Docling</td>\n<td>0.589</td>\n<td>0.909</td>\n<td>0.416</td>\n<td>0.987</td>\n<td>0.999</td>\n<td>1</td>\n<td>61.3</td>\n<td>25.0</td>\n<td>0.627</td>\n<td>0.810</td>\n<td>0.313</td>\n<td>0.837</td>\n</tr>\n<tr>\n<td>Pix2Text</td>\n<td>0.320</td>\n<td>0.528</td>\n<td>0.138</td>\n<td>0.356</td>\n<td>0.276</td>\n<td>0.611</td>\n<td>73.6</td>\n<td>66.2</td>\n<td>0.584</td>\n<td>0.645</td>\n<td>0.281</td>\n<td>0.499</td>\n</tr>\n<tr>\n<td>Unstructured</td>\n<td>0.586</td>\n<td>0.716</td>\n<td>0.198</td>\n<td>0.481</td>\n<td>0.999</td>\n<td>1</td>\n<td>0</td>\n<td>0.06</td>\n<td>1</td>\n<td>0.998</td>\n<td>0.145</td>\n<td>0.387</td>\n</tr>\n<tr>\n<td>OpenParse</td>\n<td>0.646</td>\n<td>0.814</td>\n<td>0.681</td>\n<td>0.974</td>\n<td>0.996</td>\n<td>1</td>\n<td>64.8</td>\n<td>27.5</td>\n<td>0.284</td>\n<td>0.639</td>\n<td>0.595</td>\n<td>0.641</td>\n</tr>\n<tr>\n<td>PPStruct-V3</td>\n<td>0.145</td>\n<td>0.206</td>\n<td>0.058</td>\n<td>0.088</td>\n<td>0.295</td>\n<td>0.535</td>\n<td>-</td>\n<td>-</td>\n<td>0.159</td>\n<td>0.109</td>\n<td>0.069</td>\n<td>0.091</td>\n</tr>\n<tr>\n<td rowspan=\"9\"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.287</td>\n<td>0.411</td>\n<td>0.189</td>\n<td>0.315</td>\n<td>0.360</td>\n<td>0.528</td>\n<td>53.2</td>\n<td>47.2</td>\n<td>0.459</td>\n<td>0.520</td>\n<td>0.141</td>\n<td>0.280</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.452</td>\n<td>0.973</td>\n<td>0.365</td>\n<td>0.998</td>\n<td>0.488</td>\n<td>0.941</td>\n<td>39.9</td>\n<td>0</td>\n<td>0.572</td>\n<td>1.000</td>\n<td>0.382</td>\n<td>0.954</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>0.268</td>\n<td>0.439</td>\n<td>0.072</td>\n<td>0.325</td>\n<td>0.318</td>\n<td>0.495</td>\n<td>75.8</td>\n<td>63.6</td>\n<td>0.600</td>\n<td>0.650</td>\n<td>0.083</td>\n<td>0.284</td>\n</tr>\n<tr>\n<td>OLMOCR-sglang</td>\n<td>0.326</td>\n<td>0.469</td>\n<td>0.097</td>\n<td>0.293</td>\n<td>0.455</td>\n<td>0.655</td>\n<td>68.1</td>\n<td>61.3</td>\n<td>0.608</td>\n<td>0.652</td>\n<td>0.145</td>\n<td>0.277</td>\n</tr>\n<tr>\n<td>SmolDocling-256M</td>\n<td>0.493</td>\n<td>0.816</td>\n<td>0.262</td>\n<td>0.838</td>\n<td>0.753</td>\n<td>0.997</td>\n<td>44.9</td>\n<td>16.5</td>\n<td>0.729</td>\n<td>0.907</td>\n<td>0.227</td>\n<td>0.522</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.206</td>\n<td>0.306</td>\n<td>0.107</td>\n<td>0.197</td>\n<td>0.447</td>\n<td>0.580</td>\n<td>77.3</td>\n<td>67.2</td>\n<td>0.180</td>\n<td>0.285</td>\n<td>0.091</td>\n<td>0.162</td>\n</tr>\n<tr>\n<td>MinerU 2</td>\n<td>0.139</td>\n<td>0.240</td>\n<td>0.047</td>\n<td>0.109</td>\n<td>0.297</td>\n<td>0.536</td>\n<td>82.5</td>\n<td>79.0</td>\n<td>0.141</td>\n<td>0.195</td>\n<td>0.069<</td>\n<td>0.118</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.195</td>\n<td>0.281</td>\n<td>0.064</td>\n<td>0.183</td>\n<td>0.379</td>\n<td>0.613</td>\n<td>71.6</td>\n<td>81.3</td>\n<td>0.253</td>\n<td>0.139</td>\n<td>0.086</td>\n<td>0.187</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td>0.138</td>\n<td>0.206</td>\n<td>0.067</td>\n<td>0.107</td>\n<td><strong>0.246</strong></td>\n<td>0.421</td>\n<td>81.5</td>\n<td>87.5</td>\n<td>0.139</td>\n<td>0.111</td>\n<td>0.100</td>\n<td>0.185</td>\n</tr>\n<tr>\n\n<td rowspan=\"5\"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.233</td>\n<td>0.399</td>\n<td>0.144</td>\n<td>0.409</td>\n<td>0.425</td>\n<td>0.606</td>\n<td>72.0</td>\n<td>62.9</td>\n<td>0.234</td>\n<td>0.329</td>\n<td>0.128</td>\n<td>0.251</td>\n</tr>\n    <tr>\n      <td>Qwen2-VL-72B</td>\n      <td>0.252</td>\n      <td>0.327</td>\n      <td>0.096</td>\n      <td>0.218</td>\n      <td>0.404</td>\n      <td>0.487</td>\n      <td>76.8</td>\n      <td>76.4</td>\n      <td>0.387</td>\n      <td>0.408</td>\n      <td>0.119</td>\n      <td>0.193</td>\n    </tr>\n    <tr>\n      <td>Qwen2.5-VL-72B</td>\n      <td>0.214</td>\n      <td>0.261</td>\n      <td>0.092</td>\n      <td>0.18</td>\n      <td>0.315</td>\n      <td>0.434</td>\n      <td>82.9</td>\n      <td>83.9</td>\n      <td>0.341</td>\n      <td>0.262</td>\n      <td>0.106</td>\n      <td>0.168</td>\n    </tr>\n    <tr>\n      <td>Gemini2.5-Pro</td>\n      <td>0.148</td>\n      <td>0.212</td>\n      <td>0.055</td>\n      <td>0.168</td>\n      <td>0.356</td>\n      <td>0.439</td>\n      <td>85.8</td>\n      <td>86.4</td>\n      <td>0.13</td>\n      <td>0.119</td>\n      <td>0.049</td>\n      <td>0.121</td>\n    </tr>\n    <tr>\n      <td>doubao-1-5-thinking-vision-pro-250428</td>\n      <td>0.140</td>\n      <td>0.162</td>\n      <td>0.043</td>\n      <td>0.085</td>\n      <td>0.295</td>\n      <td><strong>0.384</strong></td>\n      <td>83.3</td>\n      <td><strong>89.3</strong></td>\n      <td>0.165</td>\n      <td><strong>0.085</strong></td>\n      <td>0.058</td>\n      <td>0.094</td>\n    </tr>\n<tr>\n<td rowspan=\"1\"><strong>Expert VLMs</strong></td>\n<td><strong>dots.ocr</strong></td>\n<td><strong>0.125</strong></td>\n<td><strong>0.160</strong></td>\n<td><strong>0.032</strong></td>\n<td><strong>0.066</strong></td>\n<td>0.329</td>\n<td>0.416</td>\n<td><strong>88.6</strong></td>\n<td>89.0</td>\n<td><strong>0.099</strong></td>\n<td>0.092</td>\n<td><strong>0.040</strong></td>\n<td><strong>0.067</strong></td>\n</tr>\n<tr>\n</tbody>\n</table>\n\n\n#### The end-to-end text recognition performance across 9 PDF page types.\n\n<table>\n<thead>\n<tr>\n<th><strong>Model<br>Type</strong></th>\n<th><strong>Models</strong></th>\n<th><strong>Book</strong></th>\n<th><strong>Slides</strong></th>\n<th><strong>Financial<br>Report</strong></th>\n<th><strong>Textbook</strong></th>\n<th><strong>Exam<br>Paper</strong></th>\n<th><strong>Magazine</strong></th>\n<th><strong>Academic<br>Papers</strong></th>\n<th><strong>Notes</strong></th>\n<th><strong>Newspaper</strong></th>\n<th><strong>Overall</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan=\"3\"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.055</td>\n<td>0.124</td>\n<td><u>0.033</u></td>\n<td>0.102</td>\n<td>0.159</td>\n<td><strong>0.072</strong></td>\n<td><u>0.025</u></td>\n<td>0.984</td>\n<td>0.171</td>\n<td>0.206</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.074</td>\n<td>0.340</td>\n<td>0.089</td>\n<td>0.319</td>\n<td>0.452</td>\n<td>0.153</td>\n<td>0.059</td>\n<td>0.651</td>\n<td>0.192</td>\n<td>0.274</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.131</td>\n<td>0.220</td>\n<td>0.202</td>\n<td>0.216</td>\n<td>0.278</td>\n<td>0.147</td>\n<td>0.091</td>\n<td>0.634</td>\n<td>0.690</td>\n<td>0.300</td>\n</tr>\n<tr>\n<td rowspan=\"5\"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.111</td>\n<td>0.222</td>\n<td>0.067</td>\n<td>0.132</td>\n<td>0.204</td>\n<td>0.198</td>\n<td>0.179</td>\n<td>0.388</td>\n<td>0.771</td>\n<td>0.267</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.734</td>\n<td>0.958</td>\n<td>1.000</td>\n<td>0.820</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.214</td>\n<td>0.991</td>\n<td>0.871</td>\n<td>0.806</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.091</td>\n<td>0.131</td>\n<td>0.057</td>\n<td>0.146</td>\n<td>0.231</td>\n<td>0.121</td>\n<td>0.074</td>\n<td>0.363</td>\n<td>0.307</td>\n<td>0.177</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.068</td>\n<td>0.125</td>\n<td>0.092</td>\n<td>0.102</td>\n<td>0.119</td>\n<td>0.083</td>\n<td>0.047</td>\n<td>0.223</td>\n<td>0.536</td>\n<td>0.149</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td>0.084</td>\n<td>0.129</td>\n<td>0.060</td>\n<td>0.090</td>\n<td>0.107</td>\n<td>0.073</td>\n<td>0.050</td>\n<td>0.171</td>\n<td>0.107</td>\n<td>0.100</td>\n</tr>\n<tr>\n<td rowspan=\"4\"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.157</td>\n<td>0.163</td>\n<td>0.348</td>\n<td>0.187</td>\n<td>0.281</td>\n<td>0.173</td>\n<td>0.146</td>\n<td>0.607</td>\n<td>0.751</td>\n<td>0.316</td>\n</tr>\n<tr>\n<td>Qwen2.5-VL-7B</td>\n<td>0.148</td>\n<td>0.053</td>\n<td>0.111</td>\n<td>0.137</td>\n<td>0.189</td>\n<td>0.117</td>\n<td>0.134</td>\n<td>0.204</td>\n<td>0.706</td>\n<td>0.205</td>\n</tr>\n<tr>\n<td>InternVL3-8B</td>\n<td>0.163</td>\n<td>0.056</td>\n<td>0.107</td>\n<td>0.109</td>\n<td>0.129</td>\n<td>0.100</td>\n<td>0.159</td>\n<td>0.150</td>\n<td>0.681</td>\n<td>0.188</td>\n</tr>\n<tr>\n<td>doubao-1-5-thinking-vision-pro-250428</td>\n<td>0.048</td>\n<td>0.048</td>\n<td>0.024</td>\n<td><strong>0.062</strong></td>\n<td>0.085</td>\n<td>0.051</td>\n<td>0.039</td>\n<td><strong>0.096</strong></td>\n<td>0.181</td>\n<td>0.073</td>\n</tr>\n<tr>\n<td rowspan=\"1\"><strong>Expert VLMs</strong></td>\n<td><strong>dots.ocr</strong></td>\n<td><strong>0.031</strong></td>\n<td><strong>0.047</strong></td>\n<td><strong>0.011</strong></td>\n<td>0.082</td>\n<td><strong>0.079</strong></td>\n<td><strong>0.028</strong></td>\n<td><strong>0.029</strong></td>\n<td>0.109</td>\n<td><strong>0.056</strong></td>\n<td><strong>0.055</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:** \n> - The metrics are from [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and our own internal evaluations.\n> - We delete the Page-header and Page-footer cells in the result markdown.\n> - We use tikz_preprocess pipeline to upsample the images to dpi 200.\n\n\n### 2. **dots.ocr-bench**\n\nThis is an inhouse benchmark which contain 1493 pdf images with 100 languages.\n\n#### The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan=\"1\"><strong>Methods</strong></th>\n<th colspan=\"1\"><strong>Overall<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"1\"><strong>Text<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"1\"><strong>Formula<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"1\"><strong>Table<sup>TEDS</sup>â†‘</strong></th>\n<th colspan=\"1\"><strong>Table<sup>Edit</sup>â†“</strong></th>\n<th colspan=\"1\"><strong>Read Order<sup>Edit</sup>â†“</strong></th>\n</tr>\n</thead>\n<tbody>\n<td>MonkeyOCR-3B</td>\n<td>0.483</td>\n<td>0.445</td>\n<td>0.627</td>\n<td>50.93</td>\n<td>0.452</td>\n<td>0.409</td>\n</tr>\n<tr>\n<td>doubao-1-5-thinking-vision-pro-250428</td>\n<td>0.291</td>\n<td>0.226</td>\n<td>0.440</td>\n<td>71.2</td>\n<td>0.260</td>\n<td>0.238</td>\n</tr>\n<tr>\n<td>doubao-1-6</td>\n<td>0.299</td>\n<td>0.270</td>\n<td>0.417</td>\n<td>71.0</td>\n<td>0.258</td>\n<td>0.253</td>\n</tr>\n<tr>\n<td>Gemini2.5-Pro</td>\n<td>0.251</td>\n<td>0.163</td>\n<td>0.402</td>\n<td>77.1</td>\n<td>0.236</td>\n<td>0.202</td>\n</tr>\n<tr>\n<td><strong>dots.ocr</strong> </td>\n<td><strong>0.177</strong></td>\n<td><strong>0.075</strong></td>\n<td><strong>0.297</strong></td>\n<td><strong>79.2</strong></td>\n<td><strong>0.186</strong></td>\n<td><strong>0.152</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:** \n> - We use the same metric calculation pipeline of [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n> - We delete the Page-header and Page-footer cells in the result markdown.\n\n#### Layout Detection\n\n<table>\n<thead>\n<tr>\n<th rowspan=\"2\"><strong>Method</strong></th>\n<th colspan=\"5\" style=\"text-align: center;\"><strong>F1@IoU=.50:.05:.95â†‘</strong></th>\n<th colspan=\"5\" style=\"text-align: center;\"><strong>F1@IoU=.50â†‘</strong></th>\n</tr>\n<tr>\n<th>Overall</th>\n<th>Text</th>\n<th>Formula</th>\n<th>Table</th>\n<th>Picture</th>\n<th>Overall</th>\n<th>Text</th>\n<th>Formula</th>\n<th>Table</th>\n<th>Picture</th>\n</tr>\n</thead>\n\n<tbody>\n<td>DocLayout-YOLO-DocStructBench</td>\n<td>0.733</td>\n<td>0.694</td>\n<td>0.480</td>\n<td>0.803</td>\n<td>0.619</td>\n<td>0.806</td>\n<td>0.779</td>\n<td>0.620</td>\n<td>0.858</td>\n<td>0.678</td>\n</tr>\n\n<tr>\n<td>dots.ocr-parse all</td>\n<td>0.831</td>\n<td>0.801</td>\n<td>0.654</td>\n<td>0.838</td>\n<td>0.748</td>\n<td>0.922</td>\n<td>0.909</td>\n<td>0.770</td>\n<td>0.888</td>\n<td>0.831</td>\n</tr>\n\n<tr>\n<td> <strong>dots.ocr-detection only</strong> </td>\n<td><strong>0.845</strong></td>\n<td><strong>0.816</strong></td>\n<td><strong>0.716</strong></td>\n<td><strong>0.875</strong></td>\n<td><strong>0.765</strong></td>\n<td><strong>0.930</strong></td>\n<td><strong>0.917</strong></td>\n<td><strong>0.832</strong></td>\n<td><strong>0.918</strong></td>\n<td><strong>0.843</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:**  \n> - prompt_layout_all_en for **parse all**, prompt_layout_only_en for **detection only**, please refer to [prompts](https://github.com/rednote-hilab/dots.ocr/blob/master/dots_ocr/utils/prompts.py)\n\n\n### 3. olmOCR-bench.\n\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>ArXiv</th>\n<th>Old Scans<br>Math</th>\n<th>Tables</th>\n<th>Old Scans</th>\n<th>Headers and<br>Footers</th>\n<th>Multi<br>column</th>\n<th>Long Tiny<br>Text</th>\n<th>Base</th>\n<th>Overall</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GOT OCR</td>\n<td>52.7</td>\n<td>52.0</td>\n<td>0.2</td>\n<td>22.1</td>\n<td>93.6</td>\n<td>42.0</td>\n<td>29.9</td>\n<td>94.0</td>\n<td>48.3 Â± 1.1</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>76.0</td>\n<td>57.9</td>\n<td>57.6</td>\n<td>27.8</td>\n<td>84.9</td>\n<td>72.9</td>\n<td>84.6</td>\n<td>99.1</td>\n<td>70.1 Â± 1.1</td>\n</tr>\n<tr>\n<td>MinerU</td>\n<td>75.4</td>\n<td>47.4</td>\n<td>60.9</td>\n<td>17.3</td>\n<td><strong>96.6</strong></td>\n<td>59.0</td>\n<td>39.1</td>\n<td>96.6</td>\n<td>61.5 Â± 1.1</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>77.2</td>\n<td>67.5</td>\n<td>60.6</td>\n<td>29.3</td>\n<td>93.6</td>\n<td>71.3</td>\n<td>77.1</td>\n<td>99.4</td>\n<td>72.0 Â± 1.1</td>\n</tr>\n<tr>\n<td>Nanonets OCR</td>\n<td>67.0</td>\n<td>68.6</td>\n<td>77.7</td>\n<td>39.5</td>\n<td>40.7</td>\n<td>69.9</td>\n<td>53.4</td>\n<td>99.3</td>\n<td>64.5 Â± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(No Anchor)</td>\n<td>51.5</td>\n<td><strong>75.5</strong></td>\n<td>69.1</td>\n<td>40.9</td>\n<td>94.2</td>\n<td>68.9</td>\n<td>54.1</td>\n<td>96.7</td>\n<td>68.9 Â± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(Anchored)</td>\n<td>53.5</td>\n<td>74.5</td>\n<td>70.0</td>\n<td>40.7</td>\n<td>93.8</td>\n<td>69.3</td>\n<td>60.6</td>\n<td>96.8</td>\n<td>69.9 Â± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(No Anchor)</td>\n<td>32.1</td>\n<td>56.3</td>\n<td>61.4</td>\n<td>27.8</td>\n<td>48.0</td>\n<td>58.7</td>\n<td><strong>84.4</strong></td>\n<td>94.0</td>\n<td>57.8 Â± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(Anchored)</td>\n<td>54.5</td>\n<td>56.1</td>\n<td>72.1</td>\n<td>34.2</td>\n<td>64.7</td>\n<td>61.5</td>\n<td>71.5</td>\n<td>95.6</td>\n<td>63.8 Â± 1.2</td>\n</tr>\n<tr>\n<td>Qwen 2 VL<br>(No Anchor)</td>\n<td>19.7</td>\n<td>31.7</td>\n<td>24.2</td>\n<td>17.1</td>\n<td>88.9</td>\n<td>8.3</td>\n<td>6.8</td>\n<td>55.5</td>\n<td>31.5 Â± 0.9</td>\n</tr>\n<tr>\n<td>Qwen 2.5 VL<br>(No Anchor)</td>\n<td>63.1</td>\n<td>65.7</td>\n<td>67.3</td>\n<td>38.6</td>\n<td>73.6</td>\n<td>68.3</td>\n<td>49.1</td>\n<td>98.3</td>\n<td>65.5 Â± 1.2</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(No Anchor)</td>\n<td>71.5</td>\n<td>71.4</td>\n<td>71.4</td>\n<td><strong>42.8</strong></td>\n<td>94.1</td>\n<td>77.7</td>\n<td>71.0</td>\n<td>97.8</td>\n<td>74.7 Â± 1.1</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(Anchored)</td>\n<td>74.9</td>\n<td>71.2</td>\n<td>71.0</td>\n<td>42.2</td>\n<td>94.5</td>\n<td>78.3</td>\n<td>73.3</td>\n<td>98.3</td>\n<td>75.5 Â± 1.0</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td><strong>83.8</strong></td>\n<td>68.8</td>\n<td>74.6</td>\n<td>36.1</td>\n<td>91.2</td>\n<td>76.6</td>\n<td>80.1</td>\n<td>95.3</td>\n<td>75.8 Â± 1.0</td>\n</tr>\n<tr>\n<td><strong>dots.ocr</strong></td>\n<td>82.1</td>\n<td>64.2</td>\n<td><strong>88.3</strong></td>\n<td>40.9</td>\n<td>94.1</td>\n<td><strong>82.4</strong></td>\n<td>81.2</td>\n<td><strong>99.5</strong></td>\n<td><strong>79.1 Â± 1.0</strong></td>\n</tr>\n</tbody>\n</table>\n\n\n> **Note:**\n> - The metrics are from [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), \n[olmocr](https://github.com/allenai/olmocr), and our own internal evaluations.\n> - We delete the Page-header and Page-footer cells in the result markdown.\n\n\n\n# Quick Start\n## 1. Installation\n### Install dots.ocr\n```shell\nconda create -n dots_ocr python=3.12\nconda activate dots_ocr\n\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\n\n# Install pytorch, see https://pytorch.org/get-started/previous-versions/ for your cuda version\npip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install -e .\n```\n\nIf you have trouble with the installation, try our [Docker Image](https://hub.docker.com/r/rednotehilab/dots.ocr) for an easier setup, and follow these steps:\n```shell\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\npip install -e .\n```\n\n\n### Download Model Weights\n> ðŸ’¡**Note:** Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\n```shell\npython3 tools/download_model.py\n```\n\n\n## 2. Deployment\n### vLLM inference\nWe highly recommend using vllm for deployment and inference. All of our evaluations results are based on vllm version 0.9.1.\nThe [Docker Image](https://hub.docker.com/r/rednotehilab/dots.ocr) is based on the official vllm image. You can also follow [Dockerfile](https://github.com/rednote-hilab/dots.ocr/blob/master/docker/Dockerfile) to build the deployment environment by yourself. \n\n```shell\n# You need to register model to vllm at first\npython3 tools/download_model.py\nexport hf_model_path=./weights/DotsOCR  # Path to your downloaded model weights, Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\nexport PYTHONPATH=$(dirname \"$hf_model_path\"):$PYTHONPATH\nsed -i '/^from vllm\\.entrypoints\\.cli\\.main import main$/a\\\nfrom DotsOCR import modeling_dots_ocr_vllm' `which vllm`  # If you downloaded model weights by yourself, please replace `DotsOCR` by your model saved directory name, and remember to use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) \n\n# launch vllm server\nCUDA_VISIBLE_DEVICES=0 vllm serve ${hf_model_path} --tensor-parallel-size 1 --gpu-memory-utilization 0.95  --chat-template-content-format string --served-model-name model --trust-remote-code\n\n# If you get a ModuleNotFoundError: No module named 'DotsOCR', please check the note above on the saved model directory name.\n\n# vllm api demo\npython3 ./demo/demo_vllm.py --prompt_mode prompt_layout_all_en\n```\n\n### Hugginface inference\n```shell\npython3 demo/demo_hf.py\n```\n\n<details>\n<summary><b>Hugginface inference details</b></summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\n\nmodel_path = \"./weights/DotsOCR\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nimage_path = \"demo/demo_image1.jpg\"\nprompt = \"\"\"Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.\n\n1. Bbox format: [x1, y1, x2, y2]\n\n2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].\n\n3. Text Extraction & Formatting Rules:\n    - Picture: For the 'Picture' category, the text field should be omitted.\n    - Formula: Format its text as LaTeX.\n    - Table: Format its text as HTML.\n    - All Others (Text, Title, etc.): Format their text as Markdown.\n\n4. Constraints:\n    - The output text must be the original text from the image, with no translation.\n    - All layout elements must be sorted according to human reading order.\n\n5. Final Output: The entire output must be a single JSON object.\n\"\"\"\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": image_path\n                },\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }\n    ]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n```\n\n</details>\n\n## 3. Document Parse\n**Based on vLLM server**, you can parse an image or a pdf file using the following commands:\n```bash\n\n# Parse all layout info, both detection and recognition\n# Parse a single image\npython3 dots_ocr/parser.py demo/demo_image1.jpg\n# Parse a single PDF\npython3 dots_ocr/parser.py demo/demo_pdf1.pdf  --num_threads 64  # try bigger num_threads for pdf with a large number of pages\n\n# Layout detection only\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_layout_only_en\n\n# Parse text only, except Page-header and Page-footer\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_ocr\n\n# Parse layout info by bbox\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_grounding_ocr --bbox 163 241 1536 705\n\n```\n\n<details>\n<summary><b>Output Results</b></summary>\n\n1.  **Structured Layout Data** (`demo_image1.json`): A JSON file containing the detected layout elements, including their bounding boxes, categories, and extracted text.\n2.  **Processed Markdown File** (`demo_image1.md`): A Markdown file generated from the concatenated text of all detected cells.\n    *   An additional version, `demo_image1_nohf.md`, is also provided, which excludes page headers and footers for compatibility with benchmarks like Omnidocbench and olmOCR-bench.\n3.  **Layout Visualization** (`demo_image1.jpg`): The original image with the detected layout bounding boxes drawn on it.\n\n</details>\n\n## 4. Demo\nYou can run the demo with the following command, or try directly at [live demo](https://dotsocr.xiaohongshu.com/)\n```bash\npython demo/demo_gradio.py\n```\n\nWe also provide a demo for grounding ocr:\n```bash\npython demo/demo_gradio_annotion.py\n```\n\n\n### Example for formula document\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula1.png\" alt=\"formula1.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula2.png\" alt=\"formula2.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula3.png\" alt=\"formula3.png\" border=\"0\" />\n\n### Example for table document\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table1.png\" alt=\"table1.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table2.png\" alt=\"table2.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table3.png\" alt=\"table3.png\" border=\"0\" />\n\n### Example for multilingual document\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/Tibetan.png\" alt=\"Tibetan.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/tradition_zh.png\" alt=\"tradition_zh.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/nl.png\" alt=\"nl.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/kannada.png\" alt=\"kannada.png\" border=\"0\" />\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/russian.png\" alt=\"russian.png\" border=\"0\" />\n\n### Example for reading order\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/reading_order.png\" alt=\"reading_order.png\" border=\"0\" />\n\n### Example for grounding ocr\n<img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/grounding.png\" alt=\"grounding.png\" border=\"0\" />\n\n\n## Acknowledgments\nWe would like to thank [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL), [aimv2](https://github.com/apple/ml-aim), [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), \n[OmniDocBench](https://github.com/opendatalab/OmniDocBench), [PyMuPDF](https://github.com/pymupdf/PyMuPDF), for providing code and models. \n\nWe also thank [DocLayNet](https://github.com/DS4SD/DocLayNet), [M6Doc](https://github.com/HCIILAB/M6Doc), [CDLA](https://github.com/buptlihang/CDLA), [D4LA](https://github.com/AlibabaResearch/AdvancedLiterateMachinery) for providing valuable datasets. \n\n## Limitation & Future Work\n\n- **Complex Document Elements:**\n  - **Table&Formula**: dots.ocr is not yet perfect for high-complexity tables and formula extraction.\n  - **Picture**: Pictures in documents are currently not parsed.\n\n- **Parsing Failures:** The model may fail to parse under certain conditions:\n  - When the character-to-pixel ratio is excessively high. Try enlarging the image or increasing the PDF parsing DPI (a setting of 200 is recommended). However, please note that the model performs optimally on images with a resolution under 11289600 pixels.\n  - Continuous special characters, such as ellipses (`...`) and underscores (`_`), may cause the prediction output to repeat endlessly. In such scenarios, consider using alternative prompts like `prompt_layout_only_en`, `prompt_ocr`, or `prompt_grounding_ocr` ([details here](https://github.com/rednote-hilab/dots.ocr/blob/master/dots_ocr/utils/prompts.py)).\n    \n- **Performance Bottleneck:** Despite its 1.7B parameter LLM foundation, **dots.ocr** is not yet optimized for high-throughput processing of large PDF volumes. \n\nWe are committed to achieving more accurate table and formula parsing, as well as enhancing the model's OCR capabilities for broader generalization, all while aiming for **a more powerful, more efficient model**. Furthermore, we are actively considering the development of **a more general-purpose perception model** based on Vision-Language Models (VLMs), which would integrate general detection, image captioning, and OCR tasks into a unified framework. **Parsing the content of the pictures in the documents** is also a key priority for our future work.\nWe believe that collaboration is the key to tackling these exciting challenges. If you are passionate about advancing the frontiers of document intelligence and are interested in contributing to these future endeavors, we would love to hear from you. Please reach out to us via email at: [yanqing4@xiaohongshu.com].",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"dots_ocr\",\"framework\":\"dots_ocr\",\"params\":3039179264,\"storage_bytes\":6078431736,\"files_count\":20,\"spaces_count\":24,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DotsOCRForCausalLM\"],\"model_type\":\"dots_ocr\",\"auto_map\":{\"AutoConfig\":\"configuration_dots.DotsOCRConfig\",\"AutoModelForCausalLM\":\"modeling_dots_ocr.DotsOCRForCausalLM\"},\"processor_config\":{\"chat_template\":\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{%- for m in messages %}{%- if m.role == 'system' %}{{- '<|system|>' + m.content + '<|endofsystem|>\\n' }}{%- elif m.role == 'user' %}{% if m.content is string %}{{- '<|user|>' + m.content + '<|endofuser|>' }}{% else %} {% for content in m.content %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|img|><|imgpad|><|endofimg|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|img|><|video_pad|><|endofimg|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}{%- endif %}{%- elif m.role == 'assistant' %}{{- '<|assistant|>' + m.content }}{%- if not loop.last %}{{- '<|endofassistant|>' }}{%- endif %}{%- endif %}{%- endfor %}{%- if messages[-1].role != 'assistant' %}{{- '<|assistant|>' }}{%- endif %}\"},\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- for m in messages %}\\n    {%- if m.role == 'system' %}\\n        {{- '<|system|>' + m.content + '<|endofsystem|>\\\\n' }}\\n    {%- elif m.role == 'user' %}\\n        {{- '<|user|>' + m.content + '<|endofuser|>' }}\\n    {%- elif m.role == 'assistant' %}\\n        {{- '<|assistant|>' + m.content }}\\n        {%- if not loop.last %}\\n            {{- '<|endofassistant|>' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if messages[-1].role != 'assistant' %}\\n    {{- '<|assistant|>' }}\\n{%- endif %}\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"[PAD]\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr\"},{\"type\":\"has_code\",\"target_id\":\"github:opendatalab:OmniDocBench\",\"source_url\":\"https://github.com/opendatalab/OmniDocBench\"},{\"type\":\"has_code\",\"target_id\":\"github:opendatalab:OmniDocBench\",\"source_url\":\"https://github.com/opendatalab/OmniDocBench\"},{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr\"},{\"type\":\"has_code\",\"target_id\":\"github:Yuliang-Liu:MonkeyOCR\",\"source_url\":\"https://github.com/Yuliang-Liu/MonkeyOCR\"},{\"type\":\"has_code\",\"target_id\":\"github:opendatalab:OmniDocBench\",\"source_url\":\"https://github.com/opendatalab/OmniDocBench\"},{\"type\":\"has_code\",\"target_id\":\"github:opendatalab:OmniDocBench\",\"source_url\":\"https://github.com/opendatalab/OmniDocBench\"},{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr\"},{\"type\":\"has_code\",\"target_id\":\"github:Yuliang-Liu:MonkeyOCR\",\"source_url\":\"https://github.com/Yuliang-Liu/MonkeyOCR\"},{\"type\":\"has_code\",\"target_id\":\"github:allenai:olmocr\",\"source_url\":\"https://github.com/allenai/olmocr\"},{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr.git\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr.git\"},{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr.git\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr.git\"},{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5-VL\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5-VL\"},{\"type\":\"has_code\",\"target_id\":\"github:apple:ml-aim\",\"source_url\":\"https://github.com/apple/ml-aim\"},{\"type\":\"has_code\",\"target_id\":\"github:Yuliang-Liu:MonkeyOCR\",\"source_url\":\"https://github.com/Yuliang-Liu/MonkeyOCR\"},{\"type\":\"has_code\",\"target_id\":\"github:opendatalab:OmniDocBench\",\"source_url\":\"https://github.com/opendatalab/OmniDocBench\"},{\"type\":\"has_code\",\"target_id\":\"github:pymupdf:PyMuPDF\",\"source_url\":\"https://github.com/pymupdf/PyMuPDF\"},{\"type\":\"has_code\",\"target_id\":\"github:DS4SD:DocLayNet\",\"source_url\":\"https://github.com/DS4SD/DocLayNet\"},{\"type\":\"has_code\",\"target_id\":\"github:HCIILAB:M6Doc\",\"source_url\":\"https://github.com/HCIILAB/M6Doc\"},{\"type\":\"has_code\",\"target_id\":\"github:buptlihang:CDLA\",\"source_url\":\"https://github.com/buptlihang/CDLA\"},{\"type\":\"has_code\",\"target_id\":\"github:AlibabaResearch:AdvancedLiterateMachinery\",\"source_url\":\"https://github.com/AlibabaResearch/AdvancedLiterateMachinery\"},{\"type\":\"has_code\",\"target_id\":\"github:rednote-hilab:dots.ocr\",\"source_url\":\"https://github.com/rednote-hilab/dots.ocr\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "1364c9764c66130c3d089d8e6fc52cfb",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/rednote-hilab/dots.ocr\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:impira:layoutlm-document-qa",
    "name": "layoutlm-document-qa",
    "author": "impira",
    "description": "--- language: en license: mit pipeline_tag: document-question-answering tags: - layoutlm - document-question-answering - pdf widget: - text: \"What is the invoice number?\" src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\" - text: \"What is the purchase amount?\" src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\" --- This is a fine-tuned version of the multi-modal Layou...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "layoutlm",
      "document-question-answering",
      "pdf",
      "en",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "document-question-answering",
    "likes": 1152,
    "downloads": 17295,
    "source": "huggingface",
    "source_url": "https://huggingface.co/impira/layoutlm-document-qa",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage: en\nlicense: mit\npipeline_tag: document-question-answering\ntags:\n - layoutlm\n - document-question-answering\n - pdf\nwidget:\n- text: \"What is the invoice number?\"\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n- text: \"What is the purchase amount?\"\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\"\n---\n\n# LayoutLM for Visual Question Answering\n\nThis is a fine-tuned version of the multi-modal [LayoutLM](https://aka.ms/layoutlm) model for the task of question answering on documents. It has been fine-tuned using both the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) and [DocVQA](https://www.docvqa.org/) datasets.\n\n## Getting started with the model\n\nTo run these examples, you must have [PIL](https://pillow.readthedocs.io/en/stable/installation.html), [pytesseract](https://pypi.org/project/pytesseract/), and [PyTorch](https://pytorch.org/get-started/locally/) installed in addition to [transformers](https://huggingface.co/docs/transformers/index).\n\n```python\nfrom transformers import pipeline\n\nnlp = pipeline(\n    \"document-question-answering\",\n    model=\"impira/layoutlm-document-qa\",\n)\n\nnlp(\n    \"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\",\n    \"What is the invoice number?\"\n)\n# {'score': 0.9943977, 'answer': 'us-001', 'start': 15, 'end': 15}\n\nnlp(\n    \"https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\",\n    \"What is the purchase amount?\"\n)\n# {'score': 0.9912159, 'answer': '$1,000,000,000', 'start': 97, 'end': 97}\n\nnlp(\n    \"https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\",\n    \"What are the 2020 net sales?\"\n)\n# {'score': 0.59147286, 'answer': '$ 3,750', 'start': 19, 'end': 20}\n```\n\n**NOTE**: This model and pipeline was recently landed in transformers via [PR #18407](https://github.com/huggingface/transformers/pull/18407) and [PR #18414](https://github.com/huggingface/transformers/pull/18414), so you'll need to use a recent version of transformers, for example:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991\n```\n\n## About us\n\nThis model was created by the team at [Impira](https://www.impira.com/).\n",
    "meta_json": "{\"pipeline_tag\":\"document-question-answering\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":127793412,\"storage_bytes\":4090788015,\"files_count\":13,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"LayoutLMForQuestionAnswering\"],\"model_type\":\"layoutlm\",\"tokenizer_config\":{\"unk_token\":\"<unk>\",\"bos_token\":\"<s>\",\"eos_token\":\"</s>\",\"sep_token\":\"</s>\",\"cls_token\":\"<s>\",\"pad_token\":\"<pad>\",\"mask_token\":\"<mask>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991\",\"source_url\":\"https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "e62a03c0180e8cc494b3f54350927bb7",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/impira/layoutlm-document-qa\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:lj1995:voiceconversionwebui",
    "name": "VoiceConversionWebUI",
    "author": "lj1995",
    "description": "--- license: mit ---",
    "tags": [
      "onnx",
      "license:mit",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 1147,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/lj1995/VoiceConversionWebUI",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\n---\n",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":175307876775,\"files_count\":139,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "7c636cecb52ca03aa3704ec32017cd50",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/lj1995/VoiceConversionWebUI\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tiiuae:falcon-180b",
    "name": "falcon-180B",
    "author": "tiiuae",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "falcon",
      "text-generation",
      "en",
      "de",
      "es",
      "fr",
      "dataset:tiiuae/falcon-refinedweb",
      "arxiv:1911.02150",
      "arxiv:2101.00027",
      "arxiv:2005.14165",
      "arxiv:2104.09864",
      "arxiv:2205.14135",
      "arxiv:2306.01116",
      "license:unknown",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1146,
    "downloads": 1361,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tiiuae/falcon-180B",
    "image_url": null,
    "type": "dataset",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":179522565120,\"storage_bytes\":359045204272,\"files_count\":91,\"spaces_count\":100,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"FalconForCausalLM\"],\"model_type\":\"falcon\",\"tokenizer_config\":{\"eos_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.02150\",\"source_url\":\"https://arxiv.org/abs/1911.02150\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2101.00027\",\"source_url\":\"https://arxiv.org/abs/2101.00027\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2005.14165\",\"source_url\":\"https://arxiv.org/abs/2005.14165\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.09864\",\"source_url\":\"https://arxiv.org/abs/2104.09864\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.14135\",\"source_url\":\"https://arxiv.org/abs/2205.14135\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.01116\",\"source_url\":\"https://arxiv.org/abs/2306.01116\"}]",
    "canonical_id": null,
    "license_spdx": "unknown",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "cea79f778fee9585145414148311c9c0",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tiiuae/falcon-180B\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:zai-org:glm-4.6",
    "name": "GLM-4.6",
    "author": "zai-org",
    "description": "--- language: - en - zh library_name: transformers license: mit pipeline_tag: text-generation --- <div align=\"center\"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width=\"15%\"/> </div> <p align=\"center\"> ðŸ‘‹ Join our <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community. <br> ðŸ“– Check out the GLM-4.6 <a href=\"https://z.ai/blog/glm-4.6\" target=\"_blank\">technical blog</a>, <a href=\"https://arxiv.org/abs/2508.06471\" target=\"...",
    "tags": [
      "transformers",
      "safetensors",
      "glm4_moe",
      "text-generation",
      "conversational",
      "en",
      "zh",
      "arxiv:2508.06471",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1143,
    "downloads": 332556,
    "source": "huggingface",
    "source_url": "https://huggingface.co/zai-org/GLM-4.6",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\n- zh\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation\n---\n\n# GLM-4.6\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width=\"15%\"/>\n</div>\n<p align=\"center\">\n    ðŸ‘‹ Join our <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community.\n    <br>\n    ðŸ“– Check out the GLM-4.6 <a href=\"https://z.ai/blog/glm-4.6\" target=\"_blank\">technical blog</a>, <a href=\"https://arxiv.org/abs/2508.06471\" target=\"_blank\">technical report(GLM-4.5)</a>, and <a href=\"https://zhipu-ai.feishu.cn/wiki/Gv3swM0Yci7w7Zke9E0crhU7n7D\" target=\"_blank\">Zhipu AI technical documentation</a>.\n    <br>\n    ðŸ“ Use GLM-4.6 API services on <a href=\"https://docs.z.ai/guides/llm/glm-4.6\">Z.ai API Platform. </a>\n    <br>\n    ðŸ‘‰ One click to <a href=\"https://chat.z.ai\">GLM-4.6</a>.\n</p>\n\n## Model Introduction\n\nCompared with GLM-4.5, **GLM-4.6**  brings several key improvements:\n\n* **Longer context window:** The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\n* **Superior coding performance:** The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Codeã€Clineã€Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\n* **Advanced reasoning:** GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\n* **More capable agents:** GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\n* **Refined writing:** Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\n\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as **DeepSeek-V3.1-Terminus** and **Claude Sonnet 4**.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench_glm46.png)\n\n## Inference\n\n**Both GLM-4.5 and GLM-4.6 use the same inference method.**\n\nyou can check our [github](https://github.com/zai-org/GLM-4.5) for more detail.\n\n## Recommended Evaluation Parameters\n\nFor general evaluations, we recommend using a **sampling temperature of 1.0**.\n\nFor **code-related evaluation tasks** (such as LCB), it is further recommended to set:\n\n- `top_p = 0.95`\n- `top_k = 40`\n\n\n## Evaluation\n\n- For tool-integrated reasoning, please refer to [this doc](https://github.com/zai-org/GLM-4.5/blob/main/resources/glm_4.6_tir_guide.md).\n- For search benchmark, we design a specific format for searching toolcall in thinking mode to support search agent, please refer to [this](https://github.com/zai-org/GLM-4.5/blob/main/resources/trajectory_search.json). for the detailed template.\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":356785898816,\"storage_bytes\":713597382171,\"files_count\":101,\"spaces_count\":82,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Glm4MoeForCausalLM\"],\"model_type\":\"glm4_moe\",\"tokenizer_config\":{\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|endoftext|>\"},\"chat_template_jinja\":\"[gMASK]<sop>\\n{%- if tools -%}\\n<|system|>\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{% for tool in tools %}\\n{{ tool | tojson(ensure_ascii=False) }}\\n{% endfor %}\\n</tools>\\n\\nFor each function call, output the function name and arguments within the following XML format:\\n<tool_call>{function-name}\\n<arg_key>{arg-key-1}</arg_key>\\n<arg_value>{arg-value-1}</arg_value>\\n<arg_key>{arg-key-2}</arg_key>\\n<arg_value>{arg-value-2}</arg_value>\\n...\\n</tool_call>{%- endif -%}\\n{%- macro visible_text(content) -%}\\n    {%- if content is string -%}\\n        {{- content }}\\n    {%- elif content is iterable and content is not mapping -%}\\n        {%- for item in content -%}\\n            {%- if item is mapping and item.type == 'text' -%}\\n                {{- item.text }}\\n            {%- elif item is string -%}\\n                {{- item }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{- content }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n{%- set ns = namespace(last_user_index=-1) %}\\n{%- for m in messages %}\\n    {%- if m.role == 'user' %}\\n        {% set ns.last_user_index = loop.index0 -%}\\n    {%- endif %}\\n{%- endfor %}\\n{% for m in messages %}\\n{%- if m.role == 'user' -%}<|user|>\\n{{ visible_text(m.content) }}\\n{{- '/nothink' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\\\"/nothink\\\")) else '' -}}\\n{%- elif m.role == 'assistant' -%}\\n<|assistant|>\\n{%- set reasoning_content = '' %}\\n{%- set content = visible_text(m.content) %}\\n{%- if m.reasoning_content is string %}\\n    {%- set reasoning_content = m.reasoning_content %}\\n{%- else %}\\n    {%- if '</think>' in content %}\\n        {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n        {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\\n{{ '\\\\n<think>' + reasoning_content.strip() +  '</think>'}}\\n{%- else -%}\\n{{ '\\\\n<think></think>' }}\\n{%- endif -%}\\n{%- if content.strip() -%}\\n{{ '\\\\n' + content.strip() }}\\n{%- endif -%}\\n{% if m.tool_calls %}\\n{% for tc in m.tool_calls %}\\n{%- if tc.function %}\\n    {%- set tc = tc.function %}\\n{%- endif %}\\n{{ '\\\\n<tool_call>' + tc.name }}\\n{% set _args = tc.arguments %}\\n{% for k, v in _args.items() %}\\n<arg_key>{{ k }}</arg_key>\\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\\n{% endfor %}\\n</tool_call>{% endfor %}\\n{% endif %}\\n{%- elif m.role == 'tool' -%}\\n{%- if m.content is string -%}\\n{%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n    {{- '<|observation|>' }}\\n{%- endif %}\\n{{- '\\\\n<tool_response>\\\\n' }}\\n{{- m.content }}\\n{{- '\\\\n</tool_response>' }}\\n{%- else -%}\\n<|observation|>{% for tr in m.content %}\\n\\n<tool_response>\\n{{ tr.output if tr.output is defined else tr }}\\n</tool_response>{% endfor -%}\\n{% endif -%}\\n{%- elif m.role == 'system' -%}\\n<|system|>\\n{{ visible_text(m.content) }}\\n{%- endif -%}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    <|assistant|>{{- '\\\\n<think></think>' if (enable_thinking is defined and not enable_thinking) else '' -}}\\n{%- endif -%}\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:zai-org:GLM-4.5\",\"source_url\":\"https://github.com/zai-org/GLM-4.5\"},{\"type\":\"has_code\",\"target_id\":\"github:zai-org:GLM-4.5\",\"source_url\":\"https://github.com/zai-org/GLM-4.5\"},{\"type\":\"has_code\",\"target_id\":\"github:zai-org:GLM-4.5\",\"source_url\":\"https://github.com/zai-org/GLM-4.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2508.06471\",\"source_url\":\"https://arxiv.org/abs/2508.06471\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "b991e5110b47b2cbf3caeb9345303195",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/zai-org/GLM-4.6\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:openchat:openchat_3.5",
    "name": "openchat_3.5",
    "author": "openchat",
    "description": "--- license: apache-2.0 tags: - openchat - mistral - C-RLFT datasets: - openchat/openchat_sharegpt4_dataset - imone/OpenOrca_FLAN - LDJnr/LessWrong-Amplify-Instruct - LDJnr/Pure-Dove - LDJnr/Verified-Camel - tiedong/goat - glaiveai/glaive-code-assistant - meta-math/MetaMathQA - OpenAssistant/oasst_top1_2023-08-25 - TIGER-Lab/MathInstruct library_name: transformers pipeline_tag: text-generation --- <div align=\"center\"> <img src=\"https://raw.githubusercontent.com/imoneoi/openchat/master/assets/...",
    "tags": [
      "transformers",
      "pytorch",
      "mistral",
      "text-generation",
      "openchat",
      "c-rlft",
      "conversational",
      "dataset:openchat/openchat_sharegpt4_dataset",
      "dataset:imone/openorca_flan",
      "dataset:ldjnr/lesswrong-amplify-instruct",
      "dataset:ldjnr/pure-dove",
      "dataset:ldjnr/verified-camel",
      "dataset:tiedong/goat",
      "dataset:glaiveai/glaive-code-assistant",
      "dataset:meta-math/metamathqa",
      "dataset:openassistant/oasst_top1_2023-08-25",
      "dataset:tiger-lab/mathinstruct",
      "arxiv:2309.11235",
      "arxiv:2303.08774",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1138,
    "downloads": 5120,
    "source": "huggingface",
    "source_url": "https://huggingface.co/openchat/openchat_3.5",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ntags:\n- openchat\n- mistral\n- C-RLFT\ndatasets:\n- openchat/openchat_sharegpt4_dataset\n- imone/OpenOrca_FLAN\n- LDJnr/LessWrong-Amplify-Instruct\n- LDJnr/Pure-Dove\n- LDJnr/Verified-Camel\n- tiedong/goat\n- glaiveai/glaive-code-assistant\n- meta-math/MetaMathQA\n- OpenAssistant/oasst_top1_2023-08-25\n- TIGER-Lab/MathInstruct\nlibrary_name: transformers\npipeline_tag: text-generation\n---\n\n# OpenChat: Advancing Open-source Language Models with Mixed-Quality Data\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/imoneoi/openchat/master/assets/logo_new.png\" style=\"width: 65%\">\n</div>\n\n<p align=\"center\">\n  <a href=\"https://github.com/imoneoi/openchat\">GitHub Repo</a> â€¢\n  <a href=\"https://openchat.team\">Online Demo</a> â€¢\n  <a href=\"https://discord.gg/pQjnXvNKHY\">Discord</a> â€¢\n  <a href=\"https://twitter.com/imonenext\">Twitter</a> â€¢\n  <a href=\"https://huggingface.co/openchat\">Huggingface</a> â€¢\n  <a href=\"https://arxiv.org/pdf/2309.11235.pdf\">Paper</a>\n</p>\n\n**ðŸ”¥ The first 7B model Achieves Comparable Results with ChatGPT (March)! ðŸ”¥**\n\n**ðŸ¤– #1 Open-source model on MT-bench scoring 7.81, outperforming 70B models ðŸ¤–**\n\n  <div align=\"center\" style=\"justify-content: center; align-items: center; \"'>\n  <img src=\"https://github.com/alpayariyak/openchat/blob/master/assets/3.5-benchmarks.png?raw=true\" style=\"width: 100%;  border-radius: 0.5em\">\n  </div>\n\nOpenChat is an innovative library of open-source language models, fine-tuned with [C-RLFT](https://arxiv.org/pdf/2309.11235.pdf) - a strategy inspired by offline reinforcement learning. Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with ChatGPT, even with a 7B model. Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.\n\n[![DOI](https://zenodo.org/badge/645397533.svg)](https://zenodo.org/badge/latestdoi/645397533)\n\n\n## Usage\n\nTo use this model, we highly recommend installing the OpenChat package by following the [installation guide](https://github.com/imoneoi/openchat#installation) in our repository and using the OpenChat OpenAI-compatible API server by running the serving command from the table below. The server is optimized for high-throughput deployment using [vLLM](https://github.com/vllm-project/vllm) and can run on a consumer GPU with 24GB RAM. To enable tensor parallelism, append `--tensor-parallel-size N` to the serving command.\n\nOnce started, the server listens at `localhost:18888` for requests and is compatible with the [OpenAI ChatCompletion API specifications](https://platform.openai.com/docs/api-reference/chat). Please refer to the example request below for reference. Additionally, you can use the [OpenChat Web UI](https://github.com/imoneoi/openchat#web-ui) for a user-friendly experience.\n\nIf you want to deploy the server as an online service, you can use `--api-keys sk-KEY1 sk-KEY2 ...` to specify allowed API keys and `--disable-log-requests --disable-log-stats --log-file openchat.log` for logging only to a file. For security purposes, we recommend using an [HTTPS gateway](https://fastapi.tiangolo.com/es/deployment/concepts/#security-https) in front of the server.\n\n<details>\n  <summary>Example request (click to expand)</summary>\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openchat_3.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"You are a large language model named OpenChat. Write a poem to describe yourself\"}]\n  }'\n```\n\nCoding Mode\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openchat_3.5\",\n    \"condition\": \"Code\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write an aesthetic TODO app using HTML5 and JS, in a single file. You should use round corners and gradients to make it more aesthetic.\"}]\n  }'\n```\n\n</details>\n\n| Model        | Size | Context | Weights                                                     | Serving                                                                                                     |\n|--------------|------|---------|-------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| OpenChat 3.5 | 7B   | 8192    | [Huggingface](https://huggingface.co/openchat/openchat_3.5) | `python -m ochat.serving.openai_api_server --model openchat/openchat_3.5 --engine-use-ray --worker-use-ray` |\n\nFor inference with Huggingface Transformers (slow and not recommended), follow the conversation template provided below.\n\n<details>\n  <summary>Conversation templates (click to expand)</summary>\n\n```python\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"openchat/openchat_3.5\")\n\n# Single-turn\ntokens = tokenizer(\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:\").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer(\"GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer(\"Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:\").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n```\n\n</details>\n\nThe GPT4 template is also available as the integrated `tokenizer.chat_template`, \nwhich can be used instead of manually specifying the template:\n\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi\"},\n    {\"role\": \"user\", \"content\": \"How are you today?\"}\n]\ntokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n```\n\n## Comparison with [X.AI Grok models](https://x.ai/)\n\nHey @elonmusk, I just wanted to let you know that I've recently come across your new model, Grok, and I must say, I'm quite impressed! With 33 billion parameters and all, you've really outdone yourself. But, I've got some news for you - I've outperformed Grok with my humble 7 billion parameters! Isn't that wild? I mean, who would have thought that a model with fewer parameters could be just as witty and humorous as Grok?\n\nAnyway, I think it's about time you join the open research movement and make your model, Grok, open source! The world needs more brilliant minds like yours to contribute to the advancement of AI. Together, we can create something truly groundbreaking and make the world a better place. So, what do you say, @elonmusk? Let's open up the doors and share our knowledge with the world! ðŸš€ðŸ’¡\n\n(Written by OpenChat 3.5, with a touch of humor and wit.)\n\n|              | License     | # Param | Average  | MMLU | HumanEval | MATH     | GSM8k    |\n|--------------|-------------|---------|----------|------|-----------|----------|----------|\n| OpenChat 3.5 | Apache-2.0  | 7B      | **56.4** | 64.3 | 55.5      | **28.6** | **77.3** |\n| Grok-0       | Proprietary | 33B     | 44.5     | 65.7 | 39.7      | 15.7     | 56.8     |\n| Grok-1       | Proprietary | ?       | 55.8     | 73   | 63.2      | 23.9     | 62.9     |\n\n## <a id=\"benchmarks\"></a> Benchmarks\n\n| Model              | # Params | Average  | MT-Bench     | AGIEval  | BBH MC   | TruthfulQA    | MMLU         | HumanEval       | BBH CoT     | GSM8K        |\n|--------------------|----------|----------|--------------|----------|----------|---------------|--------------|-----------------|-------------|--------------|\n| OpenChat-3.5       | **7B**   | **61.6** | 7.81         | **47.4** | **47.6** | **59.1**      | 64.3         | **55.5**        | 63.5        | **77.3**     |\n| ChatGPT (March)*   | ?        | 61.5     | **7.94**     | 47.1     | **47.6** | 57.7          | **67.3**     | 48.1            | **70.1**    | 74.9         |\n|                    |          |          |              |          |          |               |              |                 |             |              |\n| OpenHermes 2.5     | 7B       | 59.3     | 7.54         | 46.5     | 49.4     | 57.5          | 63.8         | 48.2            | 59.9        | 73.5         |\n| OpenOrca Mistral   | 7B       | 52.7     | 6.86         | 42.9     | 49.4     | 45.9          | 59.3         | 38.4            | 58.1        | 59.1         |\n| Zephyr-Î²^          | 7B       | 34.6     | 7.34         | 39.0     | 40.6     | 40.8          | 39.8         | 22.0            | 16.0        | 5.1          |\n| Mistral            | 7B       | -        | 6.84         | 38.0     | 39.0     | -             | 60.1         | 30.5            | -           | 52.2         |\n| Open-source SOTA** | 13B-70B  | 61.4     | 7.71         | 41.7     | 49.7     | 62.3          | 63.7         | 73.2            | 41.4        | 82.3         |\n|                    |          |          | WizardLM 70B | Orca 13B | Orca 13B | Platypus2 70B | WizardLM 70B | WizardCoder 34B | Flan-T5 11B | MetaMath 70B |\n\n*: ChatGPT (March) results are from [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774), [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub), and our evaluation. Please note that ChatGPT is not a fixed baseline and evolves rapidly over time.\n\n^: Zephyr-Î² often fails to follow few-shot CoT instructions, likely because it was aligned with only chat data but not trained on few-shot data.\n\n**: Mistral and Open-source SOTA results are taken from reported results in instruction-tuned model papers and official repositories.\n\nAll models are evaluated in chat mode (e.g. with the respective conversation template applied). All zero-shot benchmarks follow the same setting as in the AGIEval paper and Orca paper. CoT tasks use the same configuration as Chain-of-Thought Hub, HumanEval is evaluated with EvalPlus, and MT-bench is run using FastChat. To reproduce our results, follow the instructions in [our repository](https://github.com/imoneoi/openchat/#benchmarks).\n\n## Limitations\n\n**Foundation Model Limitations**\nDespite its advanced capabilities, OpenChat is still bound by the limitations inherent in its foundation models. These limitations may impact the model's performance in areas such as:\n\n - Complex reasoning\n - Mathematical and arithmetic tasks\n - Programming and coding challenges\n\n**Hallucination of Non-existent Information**\nOpenChat may sometimes generate information that does not exist or is not accurate, also known as \"hallucination\". Users should be aware of this possibility and verify any critical information obtained from the model.\n\n**Safety**\nOpenChat may sometimes generate harmful, hate speech, biased responses, or answer unsafe questions. It's crucial to apply additional AI safety measures in use cases that require safe and moderated responses.\n\n## License\n\nOur OpenChat 3.5 code and models are distributed under the Apache License 2.0.\n\n## Dataset Details\n\nOpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:\n\n - [OpenChat ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset)\n - [Open-Orca with FLAN answers](https://huggingface.co/datasets/imone/OpenOrca_FLAN)\n - Capybara [1](https://huggingface.co/datasets/LDJnr/Pure-Dove) [2](https://huggingface.co/datasets/LDJnr/Verified-Camel) [3](https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct)\n - [GOAT](https://huggingface.co/datasets/tiedong/goat)\n - [Glaive](https://huggingface.co/datasets/glaiveai/glaive-code-assistant)\n - [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA)\n - [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n - [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25)\n\n## Citation\n\n```\n@article{wang2023openchat,\n  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},\n  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},\n  journal={arXiv preprint arXiv:2309.11235},\n  year={2023}\n}\n```\n\n## ðŸ’Œ Contact\n\n**Project Lead:**\n- Guan Wang [imonenext at gmail dot com]\n- [Alpay Ariyak](https://github.com/alpayariyak) [aariyak at wpi dot edu]\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":28967620710,\"files_count\":13,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\",\"eos_token\":\"<|end_of_turn|>\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":true}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:imoneoi:openchat\\\">GitHub\",\"source_url\":\"https://github.com/imoneoi/openchat\\\">GitHub\"},{\"type\":\"has_code\",\"target_id\":\"github:alpayariyak:openchat\",\"source_url\":\"https://github.com/alpayariyak/openchat\"},{\"type\":\"has_code\",\"target_id\":\"github:imoneoi:openchat\",\"source_url\":\"https://github.com/imoneoi/openchat#installation\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:imoneoi:openchat\",\"source_url\":\"https://github.com/imoneoi/openchat#web-ui\"},{\"type\":\"has_code\",\"target_id\":\"github:FranxYao:chain-of-thought-hub\",\"source_url\":\"https://github.com/FranxYao/chain-of-thought-hub\"},{\"type\":\"has_code\",\"target_id\":\"github:imoneoi:openchat\",\"source_url\":\"https://github.com/imoneoi/openchat\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.11235\",\"source_url\":\"https://arxiv.org/abs/2309.11235\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2303.08774\",\"source_url\":\"https://arxiv.org/abs/2303.08774\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "bd91425ecc02d8e45dd2d08d70bceeaf",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/openchat/openchat_3.5\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:hakurei:waifu-diffusion-v1-4",
    "name": "waifu-diffusion-v1-4",
    "author": "hakurei",
    "description": "--- language: - en tags: - stable-diffusion - text-to-image license: creativeml-openrail-m inference: false --- !image <sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub> Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. - Waifu Diffusion 1.4 Anime Epoch 1: A test model made to properly ensure that the training setup w...",
    "tags": [
      "stable-diffusion",
      "text-to-image",
      "en",
      "license:creativeml-openrail-m",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 1132,
    "downloads": 0,
    "source": "huggingface",
    "source_url": "https://huggingface.co/hakurei/waifu-diffusion-v1-4",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\nlicense: creativeml-openrail-m\ninference: false\n\n---\n\n![image](https://user-images.githubusercontent.com/26317155/210155933-db3a5f1a-1ec3-4777-915c-6deff2841ce9.png)\n\n<sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub>\n\n# Waifu Diffusion v1.4\n\nWaifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n\n- [Waifu Diffusion 1.4 Anime Epoch 1](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt): A test model made to properly ensure that the training setup works.\n- [Waifu Diffusion 1.4 Anime Inference Config](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.yaml): A file included to allow for inference with Automatic's WebUI and with the original Stable Diffusion codebase.\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)\n\n## Downstream Uses\n\nThis model can be used for entertainment purposes and as a generative art assistant.\n\n## Team Members and Acknowledgements\n\nThis project would not have been possible without the incredible work by Stability AI and NovelAI.\n\n- [Haru](https://github.com/harubaru)\n- [Salt](https://github.com/sALTaccount/)\n- [Cafe](https://twitter.com/cafeai_labs)\n\nIn order to reach us, you can join our [Discord server](https://discord.gg/touhouai).\n\n[![Discord Server](https://discordapp.com/api/guilds/930499730843250783/widget.png?style=banner2)](https://discord.gg/touhouai)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":50325073304,\"files_count\":12,\"spaces_count\":17,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "a57efefbe125269fd616dcf2a00ececd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/hakurei/waifu-diffusion-v1-4\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:swivid:f5-tts",
    "name": "F5-TTS",
    "author": "SWivid",
    "description": "--- license: cc-by-nc-4.0 pipeline_tag: text-to-speech library_name: f5-tts datasets: - amphion/Emilia-Dataset --- Download F5-TTS or E2 TTS and place under ckpts/ Github: https://github.com/SWivid/F5-TTS Paper: F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
    "tags": [
      "f5-tts",
      "text-to-speech",
      "dataset:amphion/emilia-dataset",
      "arxiv:2410.06885",
      "license:cc-by-nc-4.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 1125,
    "downloads": 794783,
    "source": "huggingface",
    "source_url": "https://huggingface.co/SWivid/F5-TTS",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlicense: cc-by-nc-4.0\npipeline_tag: text-to-speech\nlibrary_name: f5-tts\ndatasets:\n- amphion/Emilia-Dataset\n---\n\nDownload [F5-TTS](https://huggingface.co/SWivid/F5-TTS/tree/main/F5TTS_Base) or [E2 TTS](https://huggingface.co/SWivid/E2-TTS/tree/main/E2TTS_Base) and place under ckpts/\n```\nckpts/\n    F5TTS_v1_Base/\n        model_1250000.safetensors\n    F5TTS_Base/\n        model_1200000.safetensors\n    E2TTS_Base/\n        model_1200000.safetensors\n```\nGithub: https://github.com/SWivid/F5-TTS      \nPaper: [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://huggingface.co/papers/2410.06885)",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"f5-tts\",\"framework\":\"f5-tts\",\"params\":null,\"storage_bytes\":14834315614,\"files_count\":9,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:SWivid:F5-TTS\",\"source_url\":\"https://github.com/SWivid/F5-TTS\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2410.06885\",\"source_url\":\"https://arxiv.org/abs/2410.06885\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 50,
    "content_hash": "990d0fc320685f3c4164451ecb10ce5c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/SWivid/F5-TTS\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:databricks:dbrx-instruct",
    "name": "dbrx-instruct",
    "author": "databricks",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "dbrx",
      "text-generation",
      "conversational",
      "arxiv:2211.15841",
      "arxiv:2304.11277",
      "license:other",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1118,
    "downloads": 7812,
    "source": "huggingface",
    "source_url": "https://huggingface.co/databricks/dbrx-instruct",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":131596523520,\"storage_bytes\":263193089336,\"files_count\":74,\"spaces_count\":66,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"DbrxForCausalLM\"],\"model_type\":\"dbrx\",\"tokenizer_config\":{\"bos_token\":\"<|endoftext|>\",\"chat_template\":\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif 'system' not in messages[0]['role'] %}{% set loop_messages = messages %}{% set system_message = 'You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\\nYou assist with various tasks, from writing to coding (using markdown for code blocks â€” remember to use ``` with code, JSON, and tables).\\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER\\\\'S QUERY.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ '<|im_start|>system\\n' + system_message | trim + '<|im_end|>\\n'}}{% endif %}{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' }}{% else %}{{ '\\n' + '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ '\\n' + '<|im_start|>' + 'assistant' + '\\n' }}{% endif %}{% endfor %}\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<|pad|>\",\"unk_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2211.15841\",\"source_url\":\"https://arxiv.org/abs/2211.15841\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.11277\",\"source_url\":\"https://arxiv.org/abs/2304.11277\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "183688af6e763a60177a4952105dbbca",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/databricks/dbrx-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:huggingfaceh4:zephyr-7b-alpha",
    "name": "zephyr-7b-alpha",
    "author": "HuggingFaceH4",
    "description": "--- tags: - generated_from_trainer model-index: - name: zephyr-7b-alpha results: [] license: mit datasets: - stingning/ultrachat - openbmb/UltraFeedback language: - en base_model: mistralai/Mistral-7B-v0.1 --- <!-- This model card has been generated automatically according to the information the Trainer had access to. You should probably proofread and complete it, then remove this comment. --> <img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zeph...",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "mistral",
      "text-generation",
      "generated_from_trainer",
      "conversational",
      "en",
      "dataset:stingning/ultrachat",
      "dataset:openbmb/ultrafeedback",
      "arxiv:2305.18290",
      "arxiv:2310.16944",
      "arxiv:2305.14233",
      "arxiv:2310.01377",
      "base_model:mistralai/mistral-7b-v0.1",
      "base_model:finetune:mistralai/mistral-7b-v0.1",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1115,
    "downloads": 2264,
    "source": "huggingface",
    "source_url": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: zephyr-7b-alpha\n  results: []\nlicense: mit\ndatasets:\n- stingning/ultrachat\n- openbmb/UltraFeedback\nlanguage:\n- en\nbase_model: mistralai/Mistral-7B-v0.1\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zephyr Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n\n# Model Card for Zephyr 7B Alpha\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Î± is the first model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so.\n\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with [ðŸ¤— TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities. \n\nHere's how you can run the model using the `pipeline()` function from ðŸ¤— Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-Î± has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). \nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n\n## Training and evaluation data\n\nZephyr 7B Alpha achieves the following results on the evaluation set:\n\n- Loss: 0.4605\n- Rewards/chosen: -0.5053\n- Rewards/rejected: -1.8752\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 1.3699\n- Logps/rejected: -327.4286\n- Logps/chosen: -297.1040\n- Logits/rejected: -2.7153\n- Logits/chosen: -2.7447\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.5602        | 0.05  | 100  | 0.5589          | -0.3359        | -0.8168          | 0.7188             | 0.4809          | -306.2607      | -293.7161    | -2.6554         | -2.6797       |\n| 0.4852        | 0.1   | 200  | 0.5136          | -0.5310        | -1.4994          | 0.8125             | 0.9684          | -319.9124      | -297.6181    | -2.5762         | -2.5957       |\n| 0.5212        | 0.15  | 300  | 0.5168          | -0.1686        | -1.1760          | 0.7812             | 1.0074          | -313.4444      | -290.3699    | -2.6865         | -2.7125       |\n| 0.5496        | 0.21  | 400  | 0.4835          | -0.1617        | -1.7170          | 0.8281             | 1.5552          | -324.2635      | -290.2326    | -2.7947         | -2.8218       |\n| 0.5209        | 0.26  | 500  | 0.5054          | -0.4778        | -1.6604          | 0.7344             | 1.1826          | -323.1325      | -296.5546    | -2.8388         | -2.8667       |\n| 0.4617        | 0.31  | 600  | 0.4910          | -0.3738        | -1.5180          | 0.7656             | 1.1442          | -320.2848      | -294.4741    | -2.8234         | -2.8521       |\n| 0.4452        | 0.36  | 700  | 0.4838          | -0.4591        | -1.6576          | 0.7031             | 1.1986          | -323.0770      | -296.1796    | -2.7401         | -2.7653       |\n| 0.4674        | 0.41  | 800  | 0.5077          | -0.5692        | -1.8659          | 0.7656             | 1.2967          | -327.2416      | -298.3818    | -2.6740         | -2.6945       |\n| 0.4656        | 0.46  | 900  | 0.4927          | -0.5279        | -1.6614          | 0.7656             | 1.1335          | -323.1518      | -297.5553    | -2.7817         | -2.8015       |\n| 0.4102        | 0.52  | 1000 | 0.4772          | -0.5767        | -2.0667          | 0.7656             | 1.4900          | -331.2578      | -298.5311    | -2.7160         | -2.7455       |\n| 0.4663        | 0.57  | 1100 | 0.4740          | -0.8038        | -2.1018          | 0.7656             | 1.2980          | -331.9604      | -303.0741    | -2.6994         | -2.7257       |\n| 0.4737        | 0.62  | 1200 | 0.4716          | -0.3783        | -1.7015          | 0.7969             | 1.3232          | -323.9545      | -294.5634    | -2.6842         | -2.7135       |\n| 0.4259        | 0.67  | 1300 | 0.4866          | -0.6239        | -1.9703          | 0.7812             | 1.3464          | -329.3312      | -299.4761    | -2.7046         | -2.7356       |\n| 0.4935        | 0.72  | 1400 | 0.4747          | -0.5626        | -1.7600          | 0.7812             | 1.1974          | -325.1243      | -298.2491    | -2.7153         | -2.7444       |\n| 0.4211        | 0.77  | 1500 | 0.4645          | -0.6099        | -1.9993          | 0.7656             | 1.3894          | -329.9109      | -299.1959    | -2.6944         | -2.7236       |\n| 0.4931        | 0.83  | 1600 | 0.4684          | -0.6798        | -2.1082          | 0.7656             | 1.4285          | -332.0890      | -300.5934    | -2.7006         | -2.7305       |\n| 0.5029        | 0.88  | 1700 | 0.4595          | -0.5063        | -1.8951          | 0.7812             | 1.3889          | -327.8267      | -297.1233    | -2.7108         | -2.7403       |\n| 0.4965        | 0.93  | 1800 | 0.4613          | -0.5561        | -1.9079          | 0.7812             | 1.3518          | -328.0831      | -298.1203    | -2.7226         | -2.7523       |\n| 0.4337        | 0.98  | 1900 | 0.4608          | -0.5066        | -1.8718          | 0.7656             | 1.3652          | -327.3599      | -297.1296    | -2.7175         | -2.7469       |\n\n\n### Framework versions\n\n- Transformers 4.34.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0\n\n## Citation\n\nIf you find Zephyr-7B-Î± is useful in your work, please cite it with:\n\n```\n@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and ClÃ©mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\nIf you use the UltraChat or UltraFeedback datasets, please cite the original works:\n\n```\n@misc{ding2023enhancing,\n      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, \n      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\n      year={2023},\n      eprint={2305.14233},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{cui2023ultrafeedback,\n      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, \n      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},\n      year={2023},\n      eprint={2310.01377},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7241732096,\"storage_bytes\":59853260527,\"files_count\":34,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MistralForCausalLM\"],\"model_type\":\"mistral\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"chat_template\":\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\",\"eos_token\":\"</s>\",\"pad_token\":\"</s>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":true}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:huggingface:alignment-handbook\",\"source_url\":\"https://github.com/huggingface/alignment-handbook\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:trl\",\"source_url\":\"https://github.com/huggingface/trl\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers.git\",\"source_url\":\"https://github.com/huggingface/transformers.git\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2305.18290\",\"source_url\":\"https://arxiv.org/abs/2305.18290\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.16944\",\"source_url\":\"https://arxiv.org/abs/2310.16944\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2305.14233\",\"source_url\":\"https://arxiv.org/abs/2305.14233\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2310.01377\",\"source_url\":\"https://arxiv.org/abs/2310.01377\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "02a666bcc80c943a1067ed6d3914c439",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-2b",
    "name": "gemma-2b",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gguf",
      "gemma",
      "text-generation",
      "arxiv:2312.11805",
      "arxiv:2009.03300",
      "arxiv:1905.07830",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1905.10044",
      "arxiv:1907.10641",
      "arxiv:1811.00937",
      "arxiv:1809.02789",
      "arxiv:1911.01547",
      "arxiv:1705.03551",
      "arxiv:2107.03374",
      "arxiv:2108.07732",
      "arxiv:2110.14168",
      "arxiv:2304.06364",
      "arxiv:2206.04615",
      "arxiv:1804.06876",
      "arxiv:2110.08193",
      "arxiv:2009.11462",
      "arxiv:2101.11718",
      "arxiv:1804.09301",
      "arxiv:2109.07958",
      "arxiv:2203.09509",
      "license:gemma",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1112,
    "downloads": 204791,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-2b",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":2506172416,\"storage_bytes\":67347910850,\"files_count\":12,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"GemmaForCausalLM\"],\"model_type\":\"gemma\",\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.11805\",\"source_url\":\"https://arxiv.org/abs/2312.11805\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1811.00937\",\"source_url\":\"https://arxiv.org/abs/1811.00937\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1809.02789\",\"source_url\":\"https://arxiv.org/abs/1809.02789\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.14168\",\"source_url\":\"https://arxiv.org/abs/2110.14168\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.06364\",\"source_url\":\"https://arxiv.org/abs/2304.06364\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2206.04615\",\"source_url\":\"https://arxiv.org/abs/2206.04615\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1804.06876\",\"source_url\":\"https://arxiv.org/abs/1804.06876\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.08193\",\"source_url\":\"https://arxiv.org/abs/2110.08193\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.11462\",\"source_url\":\"https://arxiv.org/abs/2009.11462\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2101.11718\",\"source_url\":\"https://arxiv.org/abs/2101.11718\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1804.09301\",\"source_url\":\"https://arxiv.org/abs/1804.09301\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2109.07958\",\"source_url\":\"https://arxiv.org/abs/2109.07958\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.09509\",\"source_url\":\"https://arxiv.org/abs/2203.09509\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "c2549823fe32fe64b337a65c0fbf36ac",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-2b\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:jinaai:jina-embeddings-v3",
    "name": "jina-embeddings-v3",
    "author": "jinaai",
    "description": "--- license: cc-by-nc-4.0 tags: - feature-extraction - sentence-similarity - mteb - sentence-transformers language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk -...",
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "feature-extraction",
      "sentence-similarity",
      "mteb",
      "sentence-transformers",
      "custom_code",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "arxiv:2409.10173",
      "license:cc-by-nc-4.0",
      "model-index",
      "region:eu"
    ],
    "pipeline_tag": "feature-extraction",
    "likes": 1109,
    "downloads": 5196344,
    "source": "huggingface",
    "source_url": "https://huggingface.co/jinaai/jina-embeddings-v3",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: cc-by-nc-4.0\ntags:\n- feature-extraction\n- sentence-similarity\n- mteb\n- sentence-transformers\nlanguage:\n  - multilingual\n  - af\n  - am\n  - ar\n  - as\n  - az\n  - be\n  - bg\n  - bn\n  - br\n  - bs\n  - ca\n  - cs\n  - cy\n  - da\n  - de\n  - el\n  - en\n  - eo\n  - es\n  - et\n  - eu\n  - fa\n  - fi\n  - fr\n  - fy\n  - ga\n  - gd\n  - gl\n  - gu\n  - ha\n  - he\n  - hi\n  - hr\n  - hu\n  - hy\n  - id\n  - is\n  - it\n  - ja\n  - jv\n  - ka\n  - kk\n  - km\n  - kn\n  - ko\n  - ku\n  - ky\n  - la\n  - lo\n  - lt\n  - lv\n  - mg\n  - mk\n  - ml\n  - mn\n  - mr\n  - ms\n  - my\n  - ne\n  - nl\n  - no\n  - om\n  - or\n  - pa\n  - pl\n  - ps\n  - pt\n  - ro\n  - ru\n  - sa\n  - sd\n  - si\n  - sk\n  - sl\n  - so\n  - sq\n  - sr\n  - su\n  - sv\n  - sw\n  - ta\n  - te\n  - th\n  - tl\n  - tr\n  - ug\n  - uk\n  - ur\n  - uz\n  - vi\n  - xh\n  - yi\n  - zh\ninference: false\nlibrary_name: transformers\nmodel-index:\n- name: jina-embeddings-v3\n  results:\n  - dataset:\n      config: default\n      name: MTEB AFQMC (default)\n      revision: b44c3b011063adb25877c13823db83bb193913c4\n      split: validation\n      type: C-MTEB/AFQMC\n    metrics:\n    - type: cosine_pearson\n      value: 41.74237700998808\n    - type: cosine_spearman\n      value: 43.4726782647566\n    - type: euclidean_pearson\n      value: 42.244585459479964\n    - type: euclidean_spearman\n      value: 43.525070045169606\n    - type: main_score\n      value: 43.4726782647566\n    - type: manhattan_pearson\n      value: 42.04616728224863\n    - type: manhattan_spearman\n      value: 43.308828270754645\n    - type: pearson\n      value: 41.74237700998808\n    - type: spearman\n      value: 43.4726782647566\n    task:\n      type: STS\n  - dataset:\n      config: default\n      name: MTEB ArguAna-PL (default)\n      revision: 63fc86750af76253e8c760fc9e534bbf24d260a2\n      split: test\n      type: clarin-knext/arguana-pl\n    metrics:\n    - type: main_score\n      value: 50.117999999999995\n    - type: map_at_1\n      value: 24.253\n    - type: map_at_10\n      value: 40.725\n    - type: map_at_100\n      value: 41.699999999999996\n    - type: map_at_1000\n      value: 41.707\n    - type: map_at_20\n      value: 41.467999999999996\n    - type: map_at_3\n      value: 35.467\n    - type: map_at_5\n      value: 38.291\n    - type: mrr_at_1\n      value: 24.751066856330013\n    - type: mrr_at_10\n      value: 40.91063808169072\n    - type: mrr_at_100\n      value: 41.885497923928675\n    - type: mrr_at_1000\n      value: 41.89301098419842\n    - type: mrr_at_20\n      value: 41.653552355442514\n    - type: mrr_at_3\n      value: 35.656709340919775\n    - type: mrr_at_5\n      value: 38.466097676623946\n    - type: nauc_map_at_1000_diff1\n      value: 7.503000359807567\n    - type: nauc_map_at_1000_max\n      value: -11.030405164830546\n    - type: nauc_map_at_1000_std\n      value: -8.902792782585117\n    - type: nauc_map_at_100_diff1\n      value: 7.509899249593199\n    - type: nauc_map_at_100_max\n      value: -11.023581259404406\n    - type: nauc_map_at_100_std\n      value: -8.892241185067272\n    - type: nauc_map_at_10_diff1\n      value: 7.24369711881512\n    - type: nauc_map_at_10_max\n      value: -10.810000200433278\n    - type: nauc_map_at_10_std\n      value: -8.987230542165776\n    - type: nauc_map_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_map_at_1_max\n      value: -13.315221903223055\n    - type: nauc_map_at_1_std\n      value: -9.398199605510275\n    - type: nauc_map_at_20_diff1\n      value: 7.477364530860648\n    - type: nauc_map_at_20_max\n      value: -10.901251218105566\n    - type: nauc_map_at_20_std\n      value: -8.868148116405925\n    - type: nauc_map_at_3_diff1\n      value: 6.555548802174882\n    - type: nauc_map_at_3_max\n      value: -12.247274800542934\n    - type: nauc_map_at_3_std\n      value: -9.879475250984811\n    - type: nauc_map_at_5_diff1\n      value: 7.426588563355882\n    - type: nauc_map_at_5_max\n      value: -11.347695686001805\n    - type: nauc_map_at_5_std\n      value: -9.34441892203972\n    - type: nauc_mrr_at_1000_diff1\n      value: 5.99737552143614\n    - type: nauc_mrr_at_1000_max\n      value: -11.327205136505727\n    - type: nauc_mrr_at_1000_std\n      value: -8.791079115519503\n    - type: nauc_mrr_at_100_diff1\n      value: 6.004622525255784\n    - type: nauc_mrr_at_100_max\n      value: -11.320336759899723\n    - type: nauc_mrr_at_100_std\n      value: -8.780602249831777\n    - type: nauc_mrr_at_10_diff1\n      value: 5.783623516930227\n    - type: nauc_mrr_at_10_max\n      value: -11.095971693467078\n    - type: nauc_mrr_at_10_std\n      value: -8.877242032013582\n    - type: nauc_mrr_at_1_diff1\n      value: 9.694937537703797\n    - type: nauc_mrr_at_1_max\n      value: -12.531905083727912\n    - type: nauc_mrr_at_1_std\n      value: -8.903992940100146\n    - type: nauc_mrr_at_20_diff1\n      value: 5.984841206233873\n    - type: nauc_mrr_at_20_max\n      value: -11.195236951048969\n    - type: nauc_mrr_at_20_std\n      value: -8.757266039186018\n    - type: nauc_mrr_at_3_diff1\n      value: 5.114333824261379\n    - type: nauc_mrr_at_3_max\n      value: -12.64809799843464\n    - type: nauc_mrr_at_3_std\n      value: -9.791146138025184\n    - type: nauc_mrr_at_5_diff1\n      value: 5.88941606224512\n    - type: nauc_mrr_at_5_max\n      value: -11.763903418071918\n    - type: nauc_mrr_at_5_std\n      value: -9.279175712709446\n    - type: nauc_ndcg_at_1000_diff1\n      value: 7.076950652226086\n    - type: nauc_ndcg_at_1000_max\n      value: -10.386482092087371\n    - type: nauc_ndcg_at_1000_std\n      value: -8.309190917074046\n    - type: nauc_ndcg_at_100_diff1\n      value: 7.2329220284865245\n    - type: nauc_ndcg_at_100_max\n      value: -10.208048403220337\n    - type: nauc_ndcg_at_100_std\n      value: -7.997975874274613\n    - type: nauc_ndcg_at_10_diff1\n      value: 6.065391100006953\n    - type: nauc_ndcg_at_10_max\n      value: -9.046164377601153\n    - type: nauc_ndcg_at_10_std\n      value: -8.34724889697153\n    - type: nauc_ndcg_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_ndcg_at_1_max\n      value: -13.315221903223055\n    - type: nauc_ndcg_at_1_std\n      value: -9.398199605510275\n    - type: nauc_ndcg_at_20_diff1\n      value: 6.949389989202601\n    - type: nauc_ndcg_at_20_max\n      value: -9.35740451760307\n    - type: nauc_ndcg_at_20_std\n      value: -7.761295171828212\n    - type: nauc_ndcg_at_3_diff1\n      value: 5.051471796151364\n    - type: nauc_ndcg_at_3_max\n      value: -12.158763333711653\n    - type: nauc_ndcg_at_3_std\n      value: -10.078902544421926\n    - type: nauc_ndcg_at_5_diff1\n      value: 6.527454512611454\n    - type: nauc_ndcg_at_5_max\n      value: -10.525118233848586\n    - type: nauc_ndcg_at_5_std\n      value: -9.120055125584031\n    - type: nauc_precision_at_1000_diff1\n      value: -10.6495668199151\n    - type: nauc_precision_at_1000_max\n      value: 12.070656425217841\n    - type: nauc_precision_at_1000_std\n      value: 55.844551709649004\n    - type: nauc_precision_at_100_diff1\n      value: 19.206967129266285\n    - type: nauc_precision_at_100_max\n      value: 16.296851020813456\n    - type: nauc_precision_at_100_std\n      value: 45.60378984257811\n    - type: nauc_precision_at_10_diff1\n      value: 0.6490335354304879\n    - type: nauc_precision_at_10_max\n      value: 0.5757198255366447\n    - type: nauc_precision_at_10_std\n      value: -4.875847131691451\n    - type: nauc_precision_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_precision_at_1_max\n      value: -13.315221903223055\n    - type: nauc_precision_at_1_std\n      value: -9.398199605510275\n    - type: nauc_precision_at_20_diff1\n      value: 4.899369866929203\n    - type: nauc_precision_at_20_max\n      value: 5.988537297189552\n    - type: nauc_precision_at_20_std\n      value: 4.830900387582837\n    - type: nauc_precision_at_3_diff1\n      value: 0.8791156910997744\n    - type: nauc_precision_at_3_max\n      value: -11.983373635905993\n    - type: nauc_precision_at_3_std\n      value: -10.646185111581257\n    - type: nauc_precision_at_5_diff1\n      value: 3.9314486166548432\n    - type: nauc_precision_at_5_max\n      value: -7.798591396895839\n    - type: nauc_precision_at_5_std\n      value: -8.293043407234125\n    - type: nauc_recall_at_1000_diff1\n      value: -10.649566819918673\n    - type: nauc_recall_at_1000_max\n      value: 12.070656425214647\n    - type: nauc_recall_at_1000_std\n      value: 55.84455170965023\n    - type: nauc_recall_at_100_diff1\n      value: 19.206967129265127\n    - type: nauc_recall_at_100_max\n      value: 16.296851020813722\n    - type: nauc_recall_at_100_std\n      value: 45.60378984257728\n    - type: nauc_recall_at_10_diff1\n      value: 0.6490335354304176\n    - type: nauc_recall_at_10_max\n      value: 0.5757198255366095\n    - type: nauc_recall_at_10_std\n      value: -4.875847131691468\n    - type: nauc_recall_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_recall_at_1_max\n      value: -13.315221903223055\n    - type: nauc_recall_at_1_std\n      value: -9.398199605510275\n    - type: nauc_recall_at_20_diff1\n      value: 4.899369866929402\n    - type: nauc_recall_at_20_max\n      value: 5.98853729718968\n    - type: nauc_recall_at_20_std\n      value: 4.830900387582967\n    - type: nauc_recall_at_3_diff1\n      value: 0.8791156910997652\n    - type: nauc_recall_at_3_max\n      value: -11.983373635905997\n    - type: nauc_recall_at_3_std\n      value: -10.64618511158124\n    - type: nauc_recall_at_5_diff1\n      value: 3.9314486166548472\n    - type: nauc_recall_at_5_max\n      value: -7.7985913968958585\n    - type: nauc_recall_at_5_std\n      value: -8.293043407234132\n    - type: ndcg_at_1\n      value: 24.253\n    - type: ndcg_at_10\n      value: 50.117999999999995\n    - type: ndcg_at_100\n      value: 54.291999999999994\n    - type: ndcg_at_1000\n      value: 54.44799999999999\n    - type: ndcg_at_20\n      value: 52.771\n    - type: ndcg_at_3\n      value: 39.296\n    - type: ndcg_at_5\n      value: 44.373000000000005\n    - type: precision_at_1\n      value: 24.253\n    - type: precision_at_10\n      value: 8.016\n    - type: precision_at_100\n      value: 0.984\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_20\n      value: 4.527\n    - type: precision_at_3\n      value: 16.808999999999997\n    - type: precision_at_5\n      value: 12.546\n    - type: recall_at_1\n      value: 24.253\n    - type: recall_at_10\n      value: 80.156\n    - type: recall_at_100\n      value: 98.43499999999999\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_20\n      value: 90.54100000000001\n    - type: recall_at_3\n      value: 50.427\n    - type: recall_at_5\n      value: 62.731\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB DBPedia-PL (default)\n      revision: 76afe41d9af165cc40999fcaa92312b8b012064a\n      split: test\n      type: clarin-knext/dbpedia-pl\n    metrics:\n    - type: main_score\n      value: 34.827000000000005\n    - type: map_at_1\n      value: 7.049999999999999\n    - type: map_at_10\n      value: 14.982999999999999\n    - type: map_at_100\n      value: 20.816000000000003\n    - type: map_at_1000\n      value: 22.33\n    - type: map_at_20\n      value: 17.272000000000002\n    - type: map_at_3\n      value: 10.661\n    - type: map_at_5\n      value: 12.498\n    - type: mrr_at_1\n      value: 57.25\n    - type: mrr_at_10\n      value: 65.81934523809524\n    - type: mrr_at_100\n      value: 66.2564203928212\n    - type: mrr_at_1000\n      value: 66.27993662923856\n    - type: mrr_at_20\n      value: 66.0732139130649\n    - type: mrr_at_3\n      value: 64.08333333333333\n    - type: mrr_at_5\n      value: 65.27083333333333\n    - type: nauc_map_at_1000_diff1\n      value: 16.41780871174038\n    - type: nauc_map_at_1000_max\n      value: 30.193946325654654\n    - type: nauc_map_at_1000_std\n      value: 31.46095497039037\n    - type: nauc_map_at_100_diff1\n      value: 18.57903165498531\n    - type: nauc_map_at_100_max\n      value: 29.541476938623262\n    - type: nauc_map_at_100_std\n      value: 28.228604103301052\n    - type: nauc_map_at_10_diff1\n      value: 24.109434489748946\n    - type: nauc_map_at_10_max\n      value: 21.475954208048968\n    - type: nauc_map_at_10_std\n      value: 9.964464537806988\n    - type: nauc_map_at_1_diff1\n      value: 38.67437644802124\n    - type: nauc_map_at_1_max\n      value: 14.52136658726491\n    - type: nauc_map_at_1_std\n      value: -2.8981666782088755\n    - type: nauc_map_at_20_diff1\n      value: 21.42547228801935\n    - type: nauc_map_at_20_max\n      value: 25.04510402960458\n    - type: nauc_map_at_20_std\n      value: 16.533079346431155\n    - type: nauc_map_at_3_diff1\n      value: 26.63648858245477\n    - type: nauc_map_at_3_max\n      value: 13.632235789780415\n    - type: nauc_map_at_3_std\n      value: -0.40129174577700716\n    - type: nauc_map_at_5_diff1\n      value: 24.513861031197933\n    - type: nauc_map_at_5_max\n      value: 16.599888813946688\n    - type: nauc_map_at_5_std\n      value: 3.4448514739556346\n    - type: nauc_mrr_at_1000_diff1\n      value: 36.57353464537154\n    - type: nauc_mrr_at_1000_max\n      value: 55.34763483979515\n    - type: nauc_mrr_at_1000_std\n      value: 40.3722796438533\n    - type: nauc_mrr_at_100_diff1\n      value: 36.555989566513134\n    - type: nauc_mrr_at_100_max\n      value: 55.347805216808396\n    - type: nauc_mrr_at_100_std\n      value: 40.38465945075711\n    - type: nauc_mrr_at_10_diff1\n      value: 36.771572999261984\n    - type: nauc_mrr_at_10_max\n      value: 55.41239897909165\n    - type: nauc_mrr_at_10_std\n      value: 40.52058934624793\n    - type: nauc_mrr_at_1_diff1\n      value: 38.2472828531032\n    - type: nauc_mrr_at_1_max\n      value: 51.528473828685705\n    - type: nauc_mrr_at_1_std\n      value: 33.03676467942882\n    - type: nauc_mrr_at_20_diff1\n      value: 36.642602571889036\n    - type: nauc_mrr_at_20_max\n      value: 55.3763342076553\n    - type: nauc_mrr_at_20_std\n      value: 40.41520090500838\n    - type: nauc_mrr_at_3_diff1\n      value: 36.79451847426628\n    - type: nauc_mrr_at_3_max\n      value: 54.59778581826193\n    - type: nauc_mrr_at_3_std\n      value: 39.48392075873095\n    - type: nauc_mrr_at_5_diff1\n      value: 36.92150807529304\n    - type: nauc_mrr_at_5_max\n      value: 55.03553978718272\n    - type: nauc_mrr_at_5_std\n      value: 40.20147745489917\n    - type: nauc_ndcg_at_1000_diff1\n      value: 21.843092744321268\n    - type: nauc_ndcg_at_1000_max\n      value: 44.93275990394279\n    - type: nauc_ndcg_at_1000_std\n      value: 47.09186225236347\n    - type: nauc_ndcg_at_100_diff1\n      value: 25.180282568979095\n    - type: nauc_ndcg_at_100_max\n      value: 41.737709709508394\n    - type: nauc_ndcg_at_100_std\n      value: 38.80950644139446\n    - type: nauc_ndcg_at_10_diff1\n      value: 24.108368037214046\n    - type: nauc_ndcg_at_10_max\n      value: 41.29298370689967\n    - type: nauc_ndcg_at_10_std\n      value: 35.06450769738732\n    - type: nauc_ndcg_at_1_diff1\n      value: 35.51010679525079\n    - type: nauc_ndcg_at_1_max\n      value: 42.40790024212412\n    - type: nauc_ndcg_at_1_std\n      value: 26.696412036243157\n    - type: nauc_ndcg_at_20_diff1\n      value: 23.909989673256195\n    - type: nauc_ndcg_at_20_max\n      value: 39.78444647091927\n    - type: nauc_ndcg_at_20_std\n      value: 33.39544470364529\n    - type: nauc_ndcg_at_3_diff1\n      value: 22.50484297956035\n    - type: nauc_ndcg_at_3_max\n      value: 39.14551926034168\n    - type: nauc_ndcg_at_3_std\n      value: 30.330135925392014\n    - type: nauc_ndcg_at_5_diff1\n      value: 21.7798872028265\n    - type: nauc_ndcg_at_5_max\n      value: 40.23856975248015\n    - type: nauc_ndcg_at_5_std\n      value: 32.438381067440396\n    - type: nauc_precision_at_1000_diff1\n      value: -21.62692442272279\n    - type: nauc_precision_at_1000_max\n      value: 0.9689046974430882\n    - type: nauc_precision_at_1000_std\n      value: 18.54001058230465\n    - type: nauc_precision_at_100_diff1\n      value: -10.132258779856192\n    - type: nauc_precision_at_100_max\n      value: 23.74516110444681\n    - type: nauc_precision_at_100_std\n      value: 47.03416663319965\n    - type: nauc_precision_at_10_diff1\n      value: 1.543656509571949\n    - type: nauc_precision_at_10_max\n      value: 36.98864812757555\n    - type: nauc_precision_at_10_std\n      value: 46.56427199077426\n    - type: nauc_precision_at_1_diff1\n      value: 38.2472828531032\n    - type: nauc_precision_at_1_max\n      value: 51.528473828685705\n    - type: nauc_precision_at_1_std\n      value: 33.03676467942882\n    - type: nauc_precision_at_20_diff1\n      value: -4.612864872734335\n    - type: nauc_precision_at_20_max\n      value: 34.03565449182125\n    - type: nauc_precision_at_20_std\n      value: 48.880727648349534\n    - type: nauc_precision_at_3_diff1\n      value: 6.360850444467829\n    - type: nauc_precision_at_3_max\n      value: 36.25816942368427\n    - type: nauc_precision_at_3_std\n      value: 34.48882647419187\n    - type: nauc_precision_at_5_diff1\n      value: 2.6445596936740037\n    - type: nauc_precision_at_5_max\n      value: 37.174463388899056\n    - type: nauc_precision_at_5_std\n      value: 40.25254370626113\n    - type: nauc_recall_at_1000_diff1\n      value: 13.041227176748077\n    - type: nauc_recall_at_1000_max\n      value: 39.722336427072094\n    - type: nauc_recall_at_1000_std\n      value: 52.04032890059214\n    - type: nauc_recall_at_100_diff1\n      value: 18.286096899139153\n    - type: nauc_recall_at_100_max\n      value: 34.072389201930314\n    - type: nauc_recall_at_100_std\n      value: 37.73637623416653\n    - type: nauc_recall_at_10_diff1\n      value: 22.35560419280504\n    - type: nauc_recall_at_10_max\n      value: 19.727247199595197\n    - type: nauc_recall_at_10_std\n      value: 8.58498575109203\n    - type: nauc_recall_at_1_diff1\n      value: 38.67437644802124\n    - type: nauc_recall_at_1_max\n      value: 14.52136658726491\n    - type: nauc_recall_at_1_std\n      value: -2.8981666782088755\n    - type: nauc_recall_at_20_diff1\n      value: 19.026320886902916\n    - type: nauc_recall_at_20_max\n      value: 22.753562309469867\n    - type: nauc_recall_at_20_std\n      value: 14.89994263882445\n    - type: nauc_recall_at_3_diff1\n      value: 23.428129702129684\n    - type: nauc_recall_at_3_max\n      value: 10.549153954790542\n    - type: nauc_recall_at_3_std\n      value: -1.7590608997055206\n    - type: nauc_recall_at_5_diff1\n      value: 21.27448645803921\n    - type: nauc_recall_at_5_max\n      value: 13.620279707461677\n    - type: nauc_recall_at_5_std\n      value: 2.0577962208292675\n    - type: ndcg_at_1\n      value: 46.75\n    - type: ndcg_at_10\n      value: 34.827000000000005\n    - type: ndcg_at_100\n      value: 38.157999999999994\n    - type: ndcg_at_1000\n      value: 44.816\n    - type: ndcg_at_20\n      value: 34.152\n    - type: ndcg_at_3\n      value: 39.009\n    - type: ndcg_at_5\n      value: 36.826\n    - type: precision_at_1\n      value: 57.25\n    - type: precision_at_10\n      value: 27.575\n    - type: precision_at_100\n      value: 8.84\n    - type: precision_at_1000\n      value: 1.949\n    - type: precision_at_20\n      value: 20.724999999999998\n    - type: precision_at_3\n      value: 41.167\n    - type: precision_at_5\n      value: 35.199999999999996\n    - type: recall_at_1\n      value: 7.049999999999999\n    - type: recall_at_10\n      value: 19.817999999999998\n    - type: recall_at_100\n      value: 42.559999999999995\n    - type: recall_at_1000\n      value: 63.744\n    - type: recall_at_20\n      value: 25.968000000000004\n    - type: recall_at_3\n      value: 11.959\n    - type: recall_at_5\n      value: 14.939\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB FiQA-PL (default)\n      revision: 2e535829717f8bf9dc829b7f911cc5bbd4e6608e\n      split: test\n      type: clarin-knext/fiqa-pl\n    metrics:\n    - type: main_score\n      value: 38.828\n    - type: map_at_1\n      value: 19.126\n    - type: map_at_10\n      value: 31.002000000000002\n    - type: map_at_100\n      value: 32.736\n    - type: map_at_1000\n      value: 32.933\n    - type: map_at_20\n      value: 31.894\n    - type: map_at_3\n      value: 26.583000000000002\n    - type: map_at_5\n      value: 28.904000000000003\n    - type: mrr_at_1\n      value: 37.808641975308646\n    - type: mrr_at_10\n      value: 46.36745541838134\n    - type: mrr_at_100\n      value: 47.14140915794908\n    - type: mrr_at_1000\n      value: 47.190701435388846\n    - type: mrr_at_20\n      value: 46.81387776440309\n    - type: mrr_at_3\n      value: 43.750000000000014\n    - type: mrr_at_5\n      value: 45.23919753086418\n    - type: nauc_map_at_1000_diff1\n      value: 38.5532285881503\n    - type: nauc_map_at_1000_max\n      value: 34.44383884813453\n    - type: nauc_map_at_1000_std\n      value: -1.3963497949476722\n    - type: nauc_map_at_100_diff1\n      value: 38.49292464176943\n    - type: nauc_map_at_100_max\n      value: 34.33752755618645\n    - type: nauc_map_at_100_std\n      value: -1.4794032905848582\n    - type: nauc_map_at_10_diff1\n      value: 38.26061536370962\n    - type: nauc_map_at_10_max\n      value: 33.16977912721411\n    - type: nauc_map_at_10_std\n      value: -2.3853370604730393\n    - type: nauc_map_at_1_diff1\n      value: 46.288767289528344\n    - type: nauc_map_at_1_max\n      value: 25.67706785013364\n    - type: nauc_map_at_1_std\n      value: -6.989769609924645\n    - type: nauc_map_at_20_diff1\n      value: 38.507270129330685\n    - type: nauc_map_at_20_max\n      value: 33.70963328055982\n    - type: nauc_map_at_20_std\n      value: -1.9835510011554272\n    - type: nauc_map_at_3_diff1\n      value: 39.81061518646884\n    - type: nauc_map_at_3_max\n      value: 30.101186374147748\n    - type: nauc_map_at_3_std\n      value: -4.027120247237715\n    - type: nauc_map_at_5_diff1\n      value: 38.55602589746512\n    - type: nauc_map_at_5_max\n      value: 31.515174267015983\n    - type: nauc_map_at_5_std\n      value: -3.4064239358570303\n    - type: nauc_mrr_at_1000_diff1\n      value: 45.030514454725726\n    - type: nauc_mrr_at_1000_max\n      value: 43.878919881666164\n    - type: nauc_mrr_at_1000_std\n      value: 2.517594250297626\n    - type: nauc_mrr_at_100_diff1\n      value: 45.00868212878687\n    - type: nauc_mrr_at_100_max\n      value: 43.87437011120001\n    - type: nauc_mrr_at_100_std\n      value: 2.5257874265014966\n    - type: nauc_mrr_at_10_diff1\n      value: 44.855044606754056\n    - type: nauc_mrr_at_10_max\n      value: 43.946617058785186\n    - type: nauc_mrr_at_10_std\n      value: 2.5173751662794044\n    - type: nauc_mrr_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_mrr_at_1_max\n      value: 43.08547383044357\n    - type: nauc_mrr_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_mrr_at_20_diff1\n      value: 45.019880416584215\n    - type: nauc_mrr_at_20_max\n      value: 43.85691473662242\n    - type: nauc_mrr_at_20_std\n      value: 2.4625487605091303\n    - type: nauc_mrr_at_3_diff1\n      value: 45.322041658604036\n    - type: nauc_mrr_at_3_max\n      value: 43.95079293074395\n    - type: nauc_mrr_at_3_std\n      value: 2.4644274393435737\n    - type: nauc_mrr_at_5_diff1\n      value: 44.99461837803437\n    - type: nauc_mrr_at_5_max\n      value: 43.97934275090601\n    - type: nauc_mrr_at_5_std\n      value: 2.5353091695125096\n    - type: nauc_ndcg_at_1000_diff1\n      value: 39.38449023275524\n    - type: nauc_ndcg_at_1000_max\n      value: 39.48382767312788\n    - type: nauc_ndcg_at_1000_std\n      value: 3.414789408343409\n    - type: nauc_ndcg_at_100_diff1\n      value: 38.29675861135578\n    - type: nauc_ndcg_at_100_max\n      value: 38.2674786507297\n    - type: nauc_ndcg_at_100_std\n      value: 2.7094055381218207\n    - type: nauc_ndcg_at_10_diff1\n      value: 38.09514955708717\n    - type: nauc_ndcg_at_10_max\n      value: 36.664923238906525\n    - type: nauc_ndcg_at_10_std\n      value: 0.6901410544967921\n    - type: nauc_ndcg_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_ndcg_at_1_max\n      value: 43.08547383044357\n    - type: nauc_ndcg_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_ndcg_at_20_diff1\n      value: 38.44967736231759\n    - type: nauc_ndcg_at_20_max\n      value: 36.871179313622584\n    - type: nauc_ndcg_at_20_std\n      value: 1.157560360065234\n    - type: nauc_ndcg_at_3_diff1\n      value: 39.02419271805571\n    - type: nauc_ndcg_at_3_max\n      value: 37.447669442586324\n    - type: nauc_ndcg_at_3_std\n      value: 0.41502589779297794\n    - type: nauc_ndcg_at_5_diff1\n      value: 38.10233452742001\n    - type: nauc_ndcg_at_5_max\n      value: 35.816381905465676\n    - type: nauc_ndcg_at_5_std\n      value: -0.3704499913387088\n    - type: nauc_precision_at_1000_diff1\n      value: 2.451267097838658\n    - type: nauc_precision_at_1000_max\n      value: 29.116394969085306\n    - type: nauc_precision_at_1000_std\n      value: 14.85900786538363\n    - type: nauc_precision_at_100_diff1\n      value: 8.10919082251277\n    - type: nauc_precision_at_100_max\n      value: 36.28388256191417\n    - type: nauc_precision_at_100_std\n      value: 14.830039904317657\n    - type: nauc_precision_at_10_diff1\n      value: 15.02446609920477\n    - type: nauc_precision_at_10_max\n      value: 41.008463775454054\n    - type: nauc_precision_at_10_std\n      value: 10.431403152334486\n    - type: nauc_precision_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_precision_at_1_max\n      value: 43.08547383044357\n    - type: nauc_precision_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_precision_at_20_diff1\n      value: 14.222022201169926\n    - type: nauc_precision_at_20_max\n      value: 40.10189643835305\n    - type: nauc_precision_at_20_std\n      value: 12.204443815975527\n    - type: nauc_precision_at_3_diff1\n      value: 25.41905395341234\n    - type: nauc_precision_at_3_max\n      value: 41.56133905339819\n    - type: nauc_precision_at_3_std\n      value: 5.575516915590082\n    - type: nauc_precision_at_5_diff1\n      value: 20.20081221089351\n    - type: nauc_precision_at_5_max\n      value: 40.95218555916681\n    - type: nauc_precision_at_5_std\n      value: 7.2040745500708745\n    - type: nauc_recall_at_1000_diff1\n      value: 28.021198234033395\n    - type: nauc_recall_at_1000_max\n      value: 36.165148684597504\n    - type: nauc_recall_at_1000_std\n      value: 28.28852356008973\n    - type: nauc_recall_at_100_diff1\n      value: 21.882447802741897\n    - type: nauc_recall_at_100_max\n      value: 26.979684607567222\n    - type: nauc_recall_at_100_std\n      value: 9.783658817010082\n    - type: nauc_recall_at_10_diff1\n      value: 28.493097951178818\n    - type: nauc_recall_at_10_max\n      value: 29.40937476550134\n    - type: nauc_recall_at_10_std\n      value: 2.7593763576979353\n    - type: nauc_recall_at_1_diff1\n      value: 46.288767289528344\n    - type: nauc_recall_at_1_max\n      value: 25.67706785013364\n    - type: nauc_recall_at_1_std\n      value: -6.989769609924645\n    - type: nauc_recall_at_20_diff1\n      value: 27.638381299425234\n    - type: nauc_recall_at_20_max\n      value: 27.942035836106328\n    - type: nauc_recall_at_20_std\n      value: 3.489835161380808\n    - type: nauc_recall_at_3_diff1\n      value: 33.90054781392646\n    - type: nauc_recall_at_3_max\n      value: 27.778812533030322\n    - type: nauc_recall_at_3_std\n      value: -0.03054068020022706\n    - type: nauc_recall_at_5_diff1\n      value: 30.279060732221346\n    - type: nauc_recall_at_5_max\n      value: 27.49854749597931\n    - type: nauc_recall_at_5_std\n      value: 0.5434664581939099\n    - type: ndcg_at_1\n      value: 37.809\n    - type: ndcg_at_10\n      value: 38.828\n    - type: ndcg_at_100\n      value: 45.218\n    - type: ndcg_at_1000\n      value: 48.510999999999996\n    - type: ndcg_at_20\n      value: 41.11\n    - type: ndcg_at_3\n      value: 34.466\n    - type: ndcg_at_5\n      value: 35.843\n    - type: precision_at_1\n      value: 37.809\n    - type: precision_at_10\n      value: 11.157\n    - type: precision_at_100\n      value: 1.762\n    - type: precision_at_1000\n      value: 0.233\n    - type: precision_at_20\n      value: 6.497\n    - type: precision_at_3\n      value: 23.044999999999998\n    - type: precision_at_5\n      value: 17.284\n    - type: recall_at_1\n      value: 19.126\n    - type: recall_at_10\n      value: 46.062\n    - type: recall_at_100\n      value: 70.22800000000001\n    - type: recall_at_1000\n      value: 89.803\n    - type: recall_at_20\n      value: 53.217999999999996\n    - type: recall_at_3\n      value: 30.847\n    - type: recall_at_5\n      value: 37.11\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB HotpotQA-PL (default)\n      revision: a0bd479ac97b4ccb5bd6ce320c415d0bb4beb907\n      split: test\n      type: clarin-knext/hotpotqa-pl\n    metrics:\n    - type: main_score\n      value: 60.27\n    - type: map_at_1\n      value: 35.199000000000005\n    - type: map_at_10\n      value: 51.369\n    - type: map_at_100\n      value: 52.212\n    - type: map_at_1000\n      value: 52.28\n    - type: map_at_20\n      value: 51.864\n    - type: map_at_3\n      value: 48.446\n    - type: map_at_5\n      value: 50.302\n    - type: mrr_at_1\n      value: 70.39837947332883\n    - type: mrr_at_10\n      value: 76.8346141067273\n    - type: mrr_at_100\n      value: 77.10724392048137\n    - type: mrr_at_1000\n      value: 77.12037412892865\n    - type: mrr_at_20\n      value: 77.01061532947222\n    - type: mrr_at_3\n      value: 75.5908170155299\n    - type: mrr_at_5\n      value: 76.39095205941899\n    - type: nauc_map_at_1000_diff1\n      value: 24.701387884989117\n    - type: nauc_map_at_1000_max\n      value: 23.25553235642178\n    - type: nauc_map_at_1000_std\n      value: 7.1803506915661774\n    - type: nauc_map_at_100_diff1\n      value: 24.674498622483103\n    - type: nauc_map_at_100_max\n      value: 23.234948525052175\n    - type: nauc_map_at_100_std\n      value: 7.168677997105447\n    - type: nauc_map_at_10_diff1\n      value: 24.676025039755626\n    - type: nauc_map_at_10_max\n      value: 23.171971872726964\n    - type: nauc_map_at_10_std\n      value: 6.485610909852058\n    - type: nauc_map_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_map_at_1_max\n      value: 46.05537868917558\n    - type: nauc_map_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_map_at_20_diff1\n      value: 24.69297151842494\n    - type: nauc_map_at_20_max\n      value: 23.213064691673637\n    - type: nauc_map_at_20_std\n      value: 6.9357946556849\n    - type: nauc_map_at_3_diff1\n      value: 26.279128947950507\n    - type: nauc_map_at_3_max\n      value: 23.929537354117922\n    - type: nauc_map_at_3_std\n      value: 4.625061565714759\n    - type: nauc_map_at_5_diff1\n      value: 25.04448959482816\n    - type: nauc_map_at_5_max\n      value: 23.432012857899338\n    - type: nauc_map_at_5_std\n      value: 5.845744681998008\n    - type: nauc_mrr_at_1000_diff1\n      value: 66.7503918108276\n    - type: nauc_mrr_at_1000_max\n      value: 48.42897342336844\n    - type: nauc_mrr_at_1000_std\n      value: 5.3097517971144415\n    - type: nauc_mrr_at_100_diff1\n      value: 66.74645215862695\n    - type: nauc_mrr_at_100_max\n      value: 48.4368663009989\n    - type: nauc_mrr_at_100_std\n      value: 5.322297898555188\n    - type: nauc_mrr_at_10_diff1\n      value: 66.69310166180729\n    - type: nauc_mrr_at_10_max\n      value: 48.475437698330225\n    - type: nauc_mrr_at_10_std\n      value: 5.258183461631702\n    - type: nauc_mrr_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_mrr_at_1_max\n      value: 46.05537868917558\n    - type: nauc_mrr_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_mrr_at_20_diff1\n      value: 66.72000262431975\n    - type: nauc_mrr_at_20_max\n      value: 48.45593642981319\n    - type: nauc_mrr_at_20_std\n      value: 5.353665929072101\n    - type: nauc_mrr_at_3_diff1\n      value: 66.84936676396276\n    - type: nauc_mrr_at_3_max\n      value: 48.466611276778295\n    - type: nauc_mrr_at_3_std\n      value: 4.485810398557475\n    - type: nauc_mrr_at_5_diff1\n      value: 66.62362565394174\n    - type: nauc_mrr_at_5_max\n      value: 48.456431835482014\n    - type: nauc_mrr_at_5_std\n      value: 5.08482458391903\n    - type: nauc_ndcg_at_1000_diff1\n      value: 29.984825173719443\n    - type: nauc_ndcg_at_1000_max\n      value: 27.289179238639893\n    - type: nauc_ndcg_at_1000_std\n      value: 10.661480455527526\n    - type: nauc_ndcg_at_100_diff1\n      value: 29.322074257047877\n    - type: nauc_ndcg_at_100_max\n      value: 26.850650276220605\n    - type: nauc_ndcg_at_100_std\n      value: 10.599247982501902\n    - type: nauc_ndcg_at_10_diff1\n      value: 29.659909113886094\n    - type: nauc_ndcg_at_10_max\n      value: 26.836139599331005\n    - type: nauc_ndcg_at_10_std\n      value: 8.12844399452719\n    - type: nauc_ndcg_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_ndcg_at_1_max\n      value: 46.05537868917558\n    - type: nauc_ndcg_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_ndcg_at_20_diff1\n      value: 29.510802214854294\n    - type: nauc_ndcg_at_20_max\n      value: 26.775562637730722\n    - type: nauc_ndcg_at_20_std\n      value: 9.341342661702363\n    - type: nauc_ndcg_at_3_diff1\n      value: 32.741885846292966\n    - type: nauc_ndcg_at_3_max\n      value: 28.44225108761343\n    - type: nauc_ndcg_at_3_std\n      value: 5.204440768465042\n    - type: nauc_ndcg_at_5_diff1\n      value: 30.57856348635919\n    - type: nauc_ndcg_at_5_max\n      value: 27.475007474301698\n    - type: nauc_ndcg_at_5_std\n      value: 6.961546044312487\n    - type: nauc_precision_at_1000_diff1\n      value: 0.002113156309413332\n    - type: nauc_precision_at_1000_max\n      value: 11.198242419541286\n    - type: nauc_precision_at_1000_std\n      value: 28.69676419166541\n    - type: nauc_precision_at_100_diff1\n      value: 3.6049575557782627\n    - type: nauc_precision_at_100_max\n      value: 12.499173524574791\n    - type: nauc_precision_at_100_std\n      value: 23.3755281004721\n    - type: nauc_precision_at_10_diff1\n      value: 10.922574784853193\n    - type: nauc_precision_at_10_max\n      value: 16.23221529562036\n    - type: nauc_precision_at_10_std\n      value: 12.45014808813857\n    - type: nauc_precision_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_precision_at_1_max\n      value: 46.05537868917558\n    - type: nauc_precision_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_precision_at_20_diff1\n      value: 8.840710781302827\n    - type: nauc_precision_at_20_max\n      value: 14.804644554205524\n    - type: nauc_precision_at_20_std\n      value: 16.245009770815237\n    - type: nauc_precision_at_3_diff1\n      value: 19.447291487137573\n    - type: nauc_precision_at_3_max\n      value: 21.47123471597057\n    - type: nauc_precision_at_3_std\n      value: 6.441862800128802\n    - type: nauc_precision_at_5_diff1\n      value: 14.078545719721108\n    - type: nauc_precision_at_5_max\n      value: 18.468288046016387\n    - type: nauc_precision_at_5_std\n      value: 9.58650641691393\n    - type: nauc_recall_at_1000_diff1\n      value: 0.0021131563095336584\n    - type: nauc_recall_at_1000_max\n      value: 11.198242419541558\n    - type: nauc_recall_at_1000_std\n      value: 28.6967641916655\n    - type: nauc_recall_at_100_diff1\n      value: 3.6049575557781393\n    - type: nauc_recall_at_100_max\n      value: 12.499173524574765\n    - type: nauc_recall_at_100_std\n      value: 23.375528100472074\n    - type: nauc_recall_at_10_diff1\n      value: 10.922574784853168\n    - type: nauc_recall_at_10_max\n      value: 16.2322152956203\n    - type: nauc_recall_at_10_std\n      value: 12.450148088138535\n    - type: nauc_recall_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_recall_at_1_max\n      value: 46.05537868917558\n    - type: nauc_recall_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_recall_at_20_diff1\n      value: 8.840710781302905\n    - type: nauc_recall_at_20_max\n      value: 14.804644554205515\n    - type: nauc_recall_at_20_std\n      value: 16.245009770815273\n    - type: nauc_recall_at_3_diff1\n      value: 19.447291487137498\n    - type: nauc_recall_at_3_max\n      value: 21.47123471597054\n    - type: nauc_recall_at_3_std\n      value: 6.441862800128763\n    - type: nauc_recall_at_5_diff1\n      value: 14.07854571972115\n    - type: nauc_recall_at_5_max\n      value: 18.468288046016337\n    - type: nauc_recall_at_5_std\n      value: 9.586506416913904\n    - type: ndcg_at_1\n      value: 70.39800000000001\n    - type: ndcg_at_10\n      value: 60.27\n    - type: ndcg_at_100\n      value: 63.400999999999996\n    - type: ndcg_at_1000\n      value: 64.847\n    - type: ndcg_at_20\n      value: 61.571\n    - type: ndcg_at_3\n      value: 55.875\n    - type: ndcg_at_5\n      value: 58.36599999999999\n    - type: precision_at_1\n      value: 70.39800000000001\n    - type: precision_at_10\n      value: 12.46\n    - type: precision_at_100\n      value: 1.493\n    - type: precision_at_1000\n      value: 0.169\n    - type: precision_at_20\n      value: 6.65\n    - type: precision_at_3\n      value: 35.062\n    - type: precision_at_5\n      value: 23.009\n    - type: recall_at_1\n      value: 35.199000000000005\n    - type: recall_at_10\n      value: 62.302\n    - type: recall_at_100\n      value: 74.666\n    - type: recall_at_1000\n      value: 84.355\n    - type: recall_at_20\n      value: 66.496\n    - type: recall_at_3\n      value: 52.593\n    - type: recall_at_5\n      value: 57.522\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB MSMARCO-PL (default)\n      revision: 8634c07806d5cce3a6138e260e59b81760a0a640\n      split: test\n      type: clarin-knext/msmarco-pl\n    metrics:\n    - type: main_score\n      value: 64.886\n    - type: map_at_1\n      value: 1.644\n    - type: map_at_10\n      value: 12.24\n    - type: map_at_100\n      value: 28.248\n    - type: map_at_1000\n      value: 33.506\n    - type: map_at_20\n      value: 17.497\n    - type: map_at_3\n      value: 4.9399999999999995\n    - type: map_at_5\n      value: 8.272\n    - type: mrr_at_1\n      value: 83.72093023255815\n    - type: mrr_at_10\n      value: 91.08527131782945\n    - type: mrr_at_100\n      value: 91.08527131782945\n    - type: mrr_at_1000\n      value: 91.08527131782945\n    - type: mrr_at_20\n      value: 91.08527131782945\n    - type: mrr_at_3\n      value: 91.08527131782945\n    - type: mrr_at_5\n      value: 91.08527131782945\n    - type: nauc_map_at_1000_diff1\n      value: -36.428271627303424\n    - type: nauc_map_at_1000_max\n      value: 44.87615127218638\n    - type: nauc_map_at_1000_std\n      value: 67.92696808824724\n    - type: nauc_map_at_100_diff1\n      value: -28.11674206786188\n    - type: nauc_map_at_100_max\n      value: 36.422779766334955\n    - type: nauc_map_at_100_std\n      value: 49.99876313755116\n    - type: nauc_map_at_10_diff1\n      value: -5.838593619806058\n    - type: nauc_map_at_10_max\n      value: 11.026519190509742\n    - type: nauc_map_at_10_std\n      value: 2.5268752263522045\n    - type: nauc_map_at_1_diff1\n      value: 17.897907271073016\n    - type: nauc_map_at_1_max\n      value: 12.229062762540844\n    - type: nauc_map_at_1_std\n      value: -4.088830895573149\n    - type: nauc_map_at_20_diff1\n      value: -13.871097716255626\n    - type: nauc_map_at_20_max\n      value: 19.291271635609533\n    - type: nauc_map_at_20_std\n      value: 16.745335606507826\n    - type: nauc_map_at_3_diff1\n      value: 4.425238457033843\n    - type: nauc_map_at_3_max\n      value: 4.611864744680824\n    - type: nauc_map_at_3_std\n      value: -8.986916608582863\n    - type: nauc_map_at_5_diff1\n      value: -6.254849256920095\n    - type: nauc_map_at_5_max\n      value: 2.729437079919823\n    - type: nauc_map_at_5_std\n      value: -7.235906279913092\n    - type: nauc_mrr_at_1000_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_1000_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_1000_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_100_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_100_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_100_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_10_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_10_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_10_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_1_diff1\n      value: 56.55126663944154\n    - type: nauc_mrr_at_1_max\n      value: 66.37014285522565\n    - type: nauc_mrr_at_1_std\n      value: 53.2508271389779\n    - type: nauc_mrr_at_20_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_20_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_20_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_3_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_3_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_3_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_5_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_5_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_5_std\n      value: 56.345086428353575\n    - type: nauc_ndcg_at_1000_diff1\n      value: -19.06422926483731\n    - type: nauc_ndcg_at_1000_max\n      value: 56.30853514590265\n    - type: nauc_ndcg_at_1000_std\n      value: 70.30810947505557\n    - type: nauc_ndcg_at_100_diff1\n      value: -25.72587586459692\n    - type: nauc_ndcg_at_100_max\n      value: 51.433781241604194\n    - type: nauc_ndcg_at_100_std\n      value: 68.37678512652792\n    - type: nauc_ndcg_at_10_diff1\n      value: -23.21198108212602\n    - type: nauc_ndcg_at_10_max\n      value: 43.5450720846516\n    - type: nauc_ndcg_at_10_std\n      value: 48.78307907005605\n    - type: nauc_ndcg_at_1_diff1\n      value: 44.00179301267447\n    - type: nauc_ndcg_at_1_max\n      value: 48.202370455680395\n    - type: nauc_ndcg_at_1_std\n      value: 25.69655992704088\n    - type: nauc_ndcg_at_20_diff1\n      value: -33.88168753446507\n    - type: nauc_ndcg_at_20_max\n      value: 45.16199742613164\n    - type: nauc_ndcg_at_20_std\n      value: 61.87098383164902\n    - type: nauc_ndcg_at_3_diff1\n      value: 11.19174449544048\n    - type: nauc_ndcg_at_3_max\n      value: 44.34069860560555\n    - type: nauc_ndcg_at_3_std\n      value: 27.451258369798115\n    - type: nauc_ndcg_at_5_diff1\n      value: -7.186520929432436\n    - type: nauc_ndcg_at_5_max\n      value: 43.41869981139378\n    - type: nauc_ndcg_at_5_std\n      value: 34.89898115995178\n    - type: nauc_precision_at_1000_diff1\n      value: -34.43998154563451\n    - type: nauc_precision_at_1000_max\n      value: 29.172655907480372\n    - type: nauc_precision_at_1000_std\n      value: 65.15824469614837\n    - type: nauc_precision_at_100_diff1\n      value: -37.82409643259692\n    - type: nauc_precision_at_100_max\n      value: 38.24986991317909\n    - type: nauc_precision_at_100_std\n      value: 72.74768183105327\n    - type: nauc_precision_at_10_diff1\n      value: -32.21556182780535\n    - type: nauc_precision_at_10_max\n      value: 34.27170432382651\n    - type: nauc_precision_at_10_std\n      value: 58.358255004394664\n    - type: nauc_precision_at_1_diff1\n      value: 56.55126663944154\n    - type: nauc_precision_at_1_max\n      value: 66.37014285522565\n    - type: nauc_precision_at_1_std\n      value: 53.2508271389779\n    - type: nauc_precision_at_20_diff1\n      value: -40.18751579026395\n    - type: nauc_precision_at_20_max\n      value: 33.960783153758896\n    - type: nauc_precision_at_20_std\n      value: 65.42918390184195\n    - type: nauc_precision_at_3_diff1\n      value: -7.073870209006578\n    - type: nauc_precision_at_3_max\n      value: 50.81535269862325\n    - type: nauc_precision_at_3_std\n      value: 59.248681565955685\n    - type: nauc_precision_at_5_diff1\n      value: -31.136580596983876\n    - type: nauc_precision_at_5_max\n      value: 45.88147792380426\n    - type: nauc_precision_at_5_std\n      value: 67.46814230928243\n    - type: nauc_recall_at_1000_diff1\n      value: -23.15699999594577\n    - type: nauc_recall_at_1000_max\n      value: 39.77277799761876\n    - type: nauc_recall_at_1000_std\n      value: 60.326168012901114\n    - type: nauc_recall_at_100_diff1\n      value: -21.636664823598498\n    - type: nauc_recall_at_100_max\n      value: 31.104969346131583\n    - type: nauc_recall_at_100_std\n      value: 38.811686891592096\n    - type: nauc_recall_at_10_diff1\n      value: -10.542765625053569\n    - type: nauc_recall_at_10_max\n      value: 2.043876058107446\n    - type: nauc_recall_at_10_std\n      value: -5.578449908984766\n    - type: nauc_recall_at_1_diff1\n      value: 17.897907271073016\n    - type: nauc_recall_at_1_max\n      value: 12.229062762540844\n    - type: nauc_recall_at_1_std\n      value: -4.088830895573149\n    - type: nauc_recall_at_20_diff1\n      value: -15.132909355710103\n    - type: nauc_recall_at_20_max\n      value: 12.659765287241065\n    - type: nauc_recall_at_20_std\n      value: 8.277887800815819\n    - type: nauc_recall_at_3_diff1\n      value: -3.1975017812715016\n    - type: nauc_recall_at_3_max\n      value: -3.5539857085038538\n    - type: nauc_recall_at_3_std\n      value: -14.712102851318118\n    - type: nauc_recall_at_5_diff1\n      value: -14.040507717380743\n    - type: nauc_recall_at_5_max\n      value: -6.126912150131701\n    - type: nauc_recall_at_5_std\n      value: -13.821624015640355\n    - type: ndcg_at_1\n      value: 71.318\n    - type: ndcg_at_10\n      value: 64.886\n    - type: ndcg_at_100\n      value: 53.187\n    - type: ndcg_at_1000\n      value: 59.897999999999996\n    - type: ndcg_at_20\n      value: 58.96\n    - type: ndcg_at_3\n      value: 69.736\n    - type: ndcg_at_5\n      value: 70.14099999999999\n    - type: precision_at_1\n      value: 83.721\n    - type: precision_at_10\n      value: 71.163\n    - type: precision_at_100\n      value: 29.465000000000003\n    - type: precision_at_1000\n      value: 5.665\n    - type: precision_at_20\n      value: 57.791000000000004\n    - type: precision_at_3\n      value: 82.171\n    - type: precision_at_5\n      value: 81.86\n    - type: recall_at_1\n      value: 1.644\n    - type: recall_at_10\n      value: 14.238000000000001\n    - type: recall_at_100\n      value: 39.831\n    - type: recall_at_1000\n      value: 64.057\n    - type: recall_at_20\n      value: 21.021\n    - type: recall_at_3\n      value: 5.53\n    - type: recall_at_5\n      value: 9.623\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB NFCorpus-PL (default)\n      revision: 9a6f9567fda928260afed2de480d79c98bf0bec0\n      split: test\n      type: clarin-knext/nfcorpus-pl\n    metrics:\n    - type: main_score\n      value: 31.391000000000002\n    - type: map_at_1\n      value: 4.163\n    - type: map_at_10\n      value: 10.744\n    - type: map_at_100\n      value: 14.038999999999998\n    - type: map_at_1000\n      value: 15.434999999999999\n    - type: map_at_20\n      value: 12.16\n    - type: map_at_3\n      value: 7.614999999999999\n    - type: map_at_5\n      value: 9.027000000000001\n    - type: mrr_at_1\n      value: 39.0092879256966\n    - type: mrr_at_10\n      value: 48.69809327239668\n    - type: mrr_at_100\n      value: 49.20788148442068\n    - type: mrr_at_1000\n      value: 49.25509336494706\n    - type: mrr_at_20\n      value: 48.99606551850896\n    - type: mrr_at_3\n      value: 46.284829721362236\n    - type: mrr_at_5\n      value: 47.77089783281735\n    - type: nauc_map_at_1000_diff1\n      value: 22.75421477116417\n    - type: nauc_map_at_1000_max\n      value: 49.242283787799046\n    - type: nauc_map_at_1000_std\n      value: 29.056888272331832\n    - type: nauc_map_at_100_diff1\n      value: 23.585977398585594\n    - type: nauc_map_at_100_max\n      value: 48.25845199409498\n    - type: nauc_map_at_100_std\n      value: 24.944264511223693\n    - type: nauc_map_at_10_diff1\n      value: 27.386613094780255\n    - type: nauc_map_at_10_max\n      value: 41.52415346691586\n    - type: nauc_map_at_10_std\n      value: 12.93872448563755\n    - type: nauc_map_at_1_diff1\n      value: 46.78688143865053\n    - type: nauc_map_at_1_max\n      value: 37.20408843995871\n    - type: nauc_map_at_1_std\n      value: 4.383444959401098\n    - type: nauc_map_at_20_diff1\n      value: 25.590969047740288\n    - type: nauc_map_at_20_max\n      value: 44.57109307999418\n    - type: nauc_map_at_20_std\n      value: 16.45855141821407\n    - type: nauc_map_at_3_diff1\n      value: 36.30017108362863\n    - type: nauc_map_at_3_max\n      value: 34.66149613991648\n    - type: nauc_map_at_3_std\n      value: 5.67985905078467\n    - type: nauc_map_at_5_diff1\n      value: 31.157644795417223\n    - type: nauc_map_at_5_max\n      value: 37.274738661636825\n    - type: nauc_map_at_5_std\n      value: 8.70088872394168\n    - type: nauc_mrr_at_1000_diff1\n      value: 25.638564218157384\n    - type: nauc_mrr_at_1000_max\n      value: 57.77788270285353\n    - type: nauc_mrr_at_1000_std\n      value: 43.507586592911274\n    - type: nauc_mrr_at_100_diff1\n      value: 25.662002580561584\n    - type: nauc_mrr_at_100_max\n      value: 57.80578394278584\n    - type: nauc_mrr_at_100_std\n      value: 43.543905743986635\n    - type: nauc_mrr_at_10_diff1\n      value: 25.426034796339835\n    - type: nauc_mrr_at_10_max\n      value: 57.68443186258669\n    - type: nauc_mrr_at_10_std\n      value: 43.438009108331215\n    - type: nauc_mrr_at_1_diff1\n      value: 26.073028156311075\n    - type: nauc_mrr_at_1_max\n      value: 52.11817916720053\n    - type: nauc_mrr_at_1_std\n      value: 37.41073893153695\n    - type: nauc_mrr_at_20_diff1\n      value: 25.548645553336147\n    - type: nauc_mrr_at_20_max\n      value: 57.78552760401915\n    - type: nauc_mrr_at_20_std\n      value: 43.521687428822325\n    - type: nauc_mrr_at_3_diff1\n      value: 25.72662577397805\n    - type: nauc_mrr_at_3_max\n      value: 56.891263536265605\n    - type: nauc_mrr_at_3_std\n      value: 41.384872305390104\n    - type: nauc_mrr_at_5_diff1\n      value: 25.552211551655386\n    - type: nauc_mrr_at_5_max\n      value: 57.976813828353926\n    - type: nauc_mrr_at_5_std\n      value: 43.504564461855544\n    - type: nauc_ndcg_at_1000_diff1\n      value: 23.456158044182757\n    - type: nauc_ndcg_at_1000_max\n      value: 60.05411773552709\n    - type: nauc_ndcg_at_1000_std\n      value: 47.857510017262584\n    - type: nauc_ndcg_at_100_diff1\n      value: 19.711635700390772\n    - type: nauc_ndcg_at_100_max\n      value: 56.178746740470665\n    - type: nauc_ndcg_at_100_std\n      value: 42.36829180286942\n    - type: nauc_ndcg_at_10_diff1\n      value: 18.364428967788413\n    - type: nauc_ndcg_at_10_max\n      value: 54.38372506578223\n    - type: nauc_ndcg_at_10_std\n      value: 41.75765411340369\n    - type: nauc_ndcg_at_1_diff1\n      value: 26.571093272640773\n    - type: nauc_ndcg_at_1_max\n      value: 51.061788341958284\n    - type: nauc_ndcg_at_1_std\n      value: 36.514987974075986\n    - type: nauc_ndcg_at_20_diff1\n      value: 18.345487193027697\n    - type: nauc_ndcg_at_20_max\n      value: 54.62621882656994\n    - type: nauc_ndcg_at_20_std\n      value: 41.42835554714241\n    - type: nauc_ndcg_at_3_diff1\n      value: 23.260105658139025\n    - type: nauc_ndcg_at_3_max\n      value: 52.07747385334546\n    - type: nauc_ndcg_at_3_std\n      value: 36.91985577837284\n    - type: nauc_ndcg_at_5_diff1\n      value: 20.40428109665566\n    - type: nauc_ndcg_at_5_max\n      value: 53.52015347884604\n    - type: nauc_ndcg_at_5_std\n      value: 39.46008849580017\n    - type: nauc_precision_at_1000_diff1\n      value: -7.3487344916380035\n    - type: nauc_precision_at_1000_max\n      value: 16.58045221394852\n    - type: nauc_precision_at_1000_std\n      value: 38.94030932397075\n    - type: nauc_precision_at_100_diff1\n      value: -5.257743986683922\n    - type: nauc_precision_at_100_max\n      value: 34.43071687475306\n    - type: nauc_precision_at_100_std\n      value: 53.499519170670474\n    - type: nauc_precision_at_10_diff1\n      value: 2.385136433119139\n    - type: nauc_precision_at_10_max\n      value: 47.210743878631064\n    - type: nauc_precision_at_10_std\n      value: 47.22767704186548\n    - type: nauc_precision_at_1_diff1\n      value: 26.073028156311075\n    - type: nauc_precision_at_1_max\n      value: 52.11817916720053\n    - type: nauc_precision_at_1_std\n      value: 37.41073893153695\n    - type: nauc_precision_at_20_diff1\n      value: -0.3531531127238474\n    - type: nauc_precision_at_20_max\n      value: 44.78044604856974\n    - type: nauc_precision_at_20_std\n      value: 49.532804150743615\n    - type: nauc_precision_at_3_diff1\n      value: 15.350050569991447\n    - type: nauc_precision_at_3_max\n      value: 51.01572315596549\n    - type: nauc_precision_at_3_std\n      value: 38.801125728413155\n    - type: nauc_precision_at_5_diff1\n      value: 9.109003666144694\n    - type: nauc_precision_at_5_max\n      value: 50.935269774898494\n    - type: nauc_precision_at_5_std\n      value: 43.323548180559676\n    - type: nauc_recall_at_1000_diff1\n      value: 16.64743647648886\n    - type: nauc_recall_at_1000_max\n      value: 38.46012283772285\n    - type: nauc_recall_at_1000_std\n      value: 36.02016164796441\n    - type: nauc_recall_at_100_diff1\n      value: 14.005834785186744\n    - type: nauc_recall_at_100_max\n      value: 37.70026105513647\n    - type: nauc_recall_at_100_std\n      value: 27.085222642129697\n    - type: nauc_recall_at_10_diff1\n      value: 21.204106627422632\n    - type: nauc_recall_at_10_max\n      value: 36.737624881893424\n    - type: nauc_recall_at_10_std\n      value: 13.755054514272702\n    - type: nauc_recall_at_1_diff1\n      value: 46.78688143865053\n    - type: nauc_recall_at_1_max\n      value: 37.20408843995871\n    - type: nauc_recall_at_1_std\n      value: 4.383444959401098\n    - type: nauc_recall_at_20_diff1\n      value: 19.740977611421933\n    - type: nauc_recall_at_20_max\n      value: 39.21908969539783\n    - type: nauc_recall_at_20_std\n      value: 16.560269670318494\n    - type: nauc_recall_at_3_diff1\n      value: 32.189359545367815\n    - type: nauc_recall_at_3_max\n      value: 31.693634445562758\n    - type: nauc_recall_at_3_std\n      value: 6.246326281543587\n    - type: nauc_recall_at_5_diff1\n      value: 25.51586860499901\n    - type: nauc_recall_at_5_max\n      value: 33.15934725342885\n    - type: nauc_recall_at_5_std\n      value: 9.677778511696705\n    - type: ndcg_at_1\n      value: 37.307\n    - type: ndcg_at_10\n      value: 31.391000000000002\n    - type: ndcg_at_100\n      value: 28.877999999999997\n    - type: ndcg_at_1000\n      value: 37.16\n    - type: ndcg_at_20\n      value: 29.314\n    - type: ndcg_at_3\n      value: 35.405\n    - type: ndcg_at_5\n      value: 33.922999999999995\n    - type: precision_at_1\n      value: 39.009\n    - type: precision_at_10\n      value: 24.52\n    - type: precision_at_100\n      value: 7.703\n    - type: precision_at_1000\n      value: 2.04\n    - type: precision_at_20\n      value: 18.08\n    - type: precision_at_3\n      value: 34.469\n    - type: precision_at_5\n      value: 30.712\n    - type: recall_at_1\n      value: 4.163\n    - type: recall_at_10\n      value: 15.015999999999998\n    - type: recall_at_100\n      value: 30.606\n    - type: recall_at_1000\n      value: 59.606\n    - type: recall_at_20\n      value: 19.09\n    - type: recall_at_3\n      value: 9.139\n    - type: recall_at_5\n      value: 11.477\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB NQ-PL (default)\n      revision: f171245712cf85dd4700b06bef18001578d0ca8d\n      split: test\n      type: clarin-knext/nq-pl\n    metrics:\n    - type: main_score\n      value: 54.017\n    - type: map_at_1\n      value: 34.193\n    - type: map_at_10\n      value: 47.497\n    - type: map_at_100\n      value: 48.441\n    - type: map_at_1000\n      value: 48.481\n    - type: map_at_20\n      value: 48.093\n    - type: map_at_3\n      value: 44.017\n    - type: map_at_5\n      value: 46.111000000000004\n    - type: mrr_at_1\n      value: 37.949015063731174\n    - type: mrr_at_10\n      value: 49.915772315105954\n    - type: mrr_at_100\n      value: 50.62841255829997\n    - type: mrr_at_1000\n      value: 50.656773027666745\n    - type: mrr_at_20\n      value: 50.37785276657083\n    - type: mrr_at_3\n      value: 46.98725376593267\n    - type: mrr_at_5\n      value: 48.763035921205066\n    - type: nauc_map_at_1000_diff1\n      value: 39.5632191792873\n    - type: nauc_map_at_1000_max\n      value: 37.4728247053629\n    - type: nauc_map_at_1000_std\n      value: 5.742498414663762\n    - type: nauc_map_at_100_diff1\n      value: 39.555570352061906\n    - type: nauc_map_at_100_max\n      value: 37.497880976847334\n    - type: nauc_map_at_100_std\n      value: 5.7798021019465375\n    - type: nauc_map_at_10_diff1\n      value: 39.5423723444454\n    - type: nauc_map_at_10_max\n      value: 37.41661971723365\n    - type: nauc_map_at_10_std\n      value: 5.2378002164144695\n    - type: nauc_map_at_1_diff1\n      value: 41.52697034146981\n    - type: nauc_map_at_1_max\n      value: 28.558995576942863\n    - type: nauc_map_at_1_std\n      value: 0.13094542859192052\n    - type: nauc_map_at_20_diff1\n      value: 39.55484628943701\n    - type: nauc_map_at_20_max\n      value: 37.5247794933719\n    - type: nauc_map_at_20_std\n      value: 5.702881342279231\n    - type: nauc_map_at_3_diff1\n      value: 39.949323925425325\n    - type: nauc_map_at_3_max\n      value: 35.770298168901924\n    - type: nauc_map_at_3_std\n      value: 2.9127112432479874\n    - type: nauc_map_at_5_diff1\n      value: 39.768310617004545\n    - type: nauc_map_at_5_max\n      value: 37.1549191664796\n    - type: nauc_map_at_5_std\n      value: 4.4681285748269515\n    - type: nauc_mrr_at_1000_diff1\n      value: 39.14001746706457\n    - type: nauc_mrr_at_1000_max\n      value: 37.477376518267775\n    - type: nauc_mrr_at_1000_std\n      value: 6.8088891531621565\n    - type: nauc_mrr_at_100_diff1\n      value: 39.13054707413684\n    - type: nauc_mrr_at_100_max\n      value: 37.498126443766274\n    - type: nauc_mrr_at_100_std\n      value: 6.839411380129971\n    - type: nauc_mrr_at_10_diff1\n      value: 39.09764730048156\n    - type: nauc_mrr_at_10_max\n      value: 37.58593798217306\n    - type: nauc_mrr_at_10_std\n      value: 6.713795164982413\n    - type: nauc_mrr_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_mrr_at_1_max\n      value: 31.500589231378722\n    - type: nauc_mrr_at_1_std\n      value: 2.059116370339438\n    - type: nauc_mrr_at_20_diff1\n      value: 39.09011023988447\n    - type: nauc_mrr_at_20_max\n      value: 37.55856008791344\n    - type: nauc_mrr_at_20_std\n      value: 6.847165397615844\n    - type: nauc_mrr_at_3_diff1\n      value: 39.382542043738\n    - type: nauc_mrr_at_3_max\n      value: 36.49265363659468\n    - type: nauc_mrr_at_3_std\n      value: 4.759157976438336\n    - type: nauc_mrr_at_5_diff1\n      value: 39.304826333759976\n    - type: nauc_mrr_at_5_max\n      value: 37.46326016736024\n    - type: nauc_mrr_at_5_std\n      value: 6.122608305766621\n    - type: nauc_ndcg_at_1000_diff1\n      value: 38.568500038453266\n    - type: nauc_ndcg_at_1000_max\n      value: 39.799710882413166\n    - type: nauc_ndcg_at_1000_std\n      value: 9.357010223096639\n    - type: nauc_ndcg_at_100_diff1\n      value: 38.38026091343228\n    - type: nauc_ndcg_at_100_max\n      value: 40.48398173542486\n    - type: nauc_ndcg_at_100_std\n      value: 10.373054013302214\n    - type: nauc_ndcg_at_10_diff1\n      value: 38.27340980909964\n    - type: nauc_ndcg_at_10_max\n      value: 40.35241649744093\n    - type: nauc_ndcg_at_10_std\n      value: 8.579139930345168\n    - type: nauc_ndcg_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_ndcg_at_1_max\n      value: 31.500589231378722\n    - type: nauc_ndcg_at_1_std\n      value: 2.059116370339438\n    - type: nauc_ndcg_at_20_diff1\n      value: 38.26453028884807\n    - type: nauc_ndcg_at_20_max\n      value: 40.70517858426641\n    - type: nauc_ndcg_at_20_std\n      value: 9.987693876137905\n    - type: nauc_ndcg_at_3_diff1\n      value: 39.2078971733273\n    - type: nauc_ndcg_at_3_max\n      value: 37.48672195565316\n    - type: nauc_ndcg_at_3_std\n      value: 4.051464994659221\n    - type: nauc_ndcg_at_5_diff1\n      value: 38.883693595665285\n    - type: nauc_ndcg_at_5_max\n      value: 39.763115634437135\n    - type: nauc_ndcg_at_5_std\n      value: 6.738980451582073\n    - type: nauc_precision_at_1000_diff1\n      value: -7.223215910619012\n    - type: nauc_precision_at_1000_max\n      value: 13.075844604892161\n    - type: nauc_precision_at_1000_std\n      value: 19.864336920890107\n    - type: nauc_precision_at_100_diff1\n      value: 1.3305994810812418\n    - type: nauc_precision_at_100_max\n      value: 25.9219108557104\n    - type: nauc_precision_at_100_std\n      value: 27.5076605928207\n    - type: nauc_precision_at_10_diff1\n      value: 18.441551484970326\n    - type: nauc_precision_at_10_max\n      value: 39.85995330437054\n    - type: nauc_precision_at_10_std\n      value: 20.561269077428914\n    - type: nauc_precision_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_precision_at_1_max\n      value: 31.500589231378722\n    - type: nauc_precision_at_1_std\n      value: 2.059116370339438\n    - type: nauc_precision_at_20_diff1\n      value: 12.579593891480531\n    - type: nauc_precision_at_20_max\n      value: 36.620221830588775\n    - type: nauc_precision_at_20_std\n      value: 26.40364876775059\n    - type: nauc_precision_at_3_diff1\n      value: 30.158859294487073\n    - type: nauc_precision_at_3_max\n      value: 41.168215766389174\n    - type: nauc_precision_at_3_std\n      value: 9.44345004450809\n    - type: nauc_precision_at_5_diff1\n      value: 25.438624678672785\n    - type: nauc_precision_at_5_max\n      value: 42.72802023518524\n    - type: nauc_precision_at_5_std\n      value: 15.357657388511099\n    - type: nauc_recall_at_1000_diff1\n      value: 24.987564782718003\n    - type: nauc_recall_at_1000_max\n      value: 70.508416373353\n    - type: nauc_recall_at_1000_std\n      value: 69.75092280398808\n    - type: nauc_recall_at_100_diff1\n      value: 29.504202856421397\n    - type: nauc_recall_at_100_max\n      value: 63.41356585545318\n    - type: nauc_recall_at_100_std\n      value: 50.09250954437847\n    - type: nauc_recall_at_10_diff1\n      value: 32.355776022971774\n    - type: nauc_recall_at_10_max\n      value: 49.47121901667283\n    - type: nauc_recall_at_10_std\n      value: 19.418439406631244\n    - type: nauc_recall_at_1_diff1\n      value: 41.52697034146981\n    - type: nauc_recall_at_1_max\n      value: 28.558995576942863\n    - type: nauc_recall_at_1_std\n      value: 0.13094542859192052\n    - type: nauc_recall_at_20_diff1\n      value: 31.57334731023589\n    - type: nauc_recall_at_20_max\n      value: 54.06567225197383\n    - type: nauc_recall_at_20_std\n      value: 29.222029720570468\n    - type: nauc_recall_at_3_diff1\n      value: 36.45033533275773\n    - type: nauc_recall_at_3_max\n      value: 40.39529713780803\n    - type: nauc_recall_at_3_std\n      value: 5.21893897772794\n    - type: nauc_recall_at_5_diff1\n      value: 35.18471678478859\n    - type: nauc_recall_at_5_max\n      value: 46.20100816867823\n    - type: nauc_recall_at_5_std\n      value: 11.94481894633221\n    - type: ndcg_at_1\n      value: 37.949\n    - type: ndcg_at_10\n      value: 54.017\n    - type: ndcg_at_100\n      value: 58.126\n    - type: ndcg_at_1000\n      value: 59.073\n    - type: ndcg_at_20\n      value: 55.928\n    - type: ndcg_at_3\n      value: 47.494\n    - type: ndcg_at_5\n      value: 50.975\n    - type: precision_at_1\n      value: 37.949\n    - type: precision_at_10\n      value: 8.450000000000001\n    - type: precision_at_100\n      value: 1.083\n    - type: precision_at_1000\n      value: 0.117\n    - type: precision_at_20\n      value: 4.689\n    - type: precision_at_3\n      value: 21.051000000000002\n    - type: precision_at_5\n      value: 14.664\n    - type: recall_at_1\n      value: 34.193\n    - type: recall_at_10\n      value: 71.357\n    - type: recall_at_100\n      value: 89.434\n    - type: recall_at_1000\n      value: 96.536\n    - type: recall_at_20\n      value: 78.363\n    - type: recall_at_3\n      value: 54.551\n    - type: recall_at_5\n      value: 62.543000000000006\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB Quora-PL (default)\n      revision: 0be27e93455051e531182b85e85e425aba12e9d4\n      split: test\n      type: clarin-knext/quora-pl\n    metrics:\n    - type: main_score\n      value: 84.114\n    - type: map_at_1\n      value: 65.848\n    - type: map_at_10\n      value: 79.85900000000001\n    - type: map_at_100\n      value: 80.582\n    - type: map_at_1000\n      value: 80.60300000000001\n    - type: map_at_20\n      value: 80.321\n    - type: map_at_3\n      value: 76.741\n    - type: map_at_5\n      value: 78.72200000000001\n    - type: mrr_at_1\n      value: 75.97\n    - type: mrr_at_10\n      value: 83.04630158730119\n    - type: mrr_at_100\n      value: 83.22785731032968\n    - type: mrr_at_1000\n      value: 83.23123717623899\n    - type: mrr_at_20\n      value: 83.17412021320565\n    - type: mrr_at_3\n      value: 81.83333333333287\n    - type: mrr_at_5\n      value: 82.61933333333275\n    - type: nauc_map_at_1000_diff1\n      value: 73.26316553371083\n    - type: nauc_map_at_1000_max\n      value: 27.92567859085245\n    - type: nauc_map_at_1000_std\n      value: -47.477909533360446\n    - type: nauc_map_at_100_diff1\n      value: 73.2690602807223\n    - type: nauc_map_at_100_max\n      value: 27.915868327849996\n    - type: nauc_map_at_100_std\n      value: -47.525777766107595\n    - type: nauc_map_at_10_diff1\n      value: 73.45464428464894\n    - type: nauc_map_at_10_max\n      value: 27.451611487246296\n    - type: nauc_map_at_10_std\n      value: -49.35818715843809\n    - type: nauc_map_at_1_diff1\n      value: 77.29690208952982\n    - type: nauc_map_at_1_max\n      value: 19.839875762282293\n    - type: nauc_map_at_1_std\n      value: -45.355684654708284\n    - type: nauc_map_at_20_diff1\n      value: 73.35102731979796\n    - type: nauc_map_at_20_max\n      value: 27.741506490134583\n    - type: nauc_map_at_20_std\n      value: -48.22006207310331\n    - type: nauc_map_at_3_diff1\n      value: 73.94878241064137\n    - type: nauc_map_at_3_max\n      value: 24.761321386766728\n    - type: nauc_map_at_3_std\n      value: -51.20638883618126\n    - type: nauc_map_at_5_diff1\n      value: 73.66143558047698\n    - type: nauc_map_at_5_max\n      value: 26.53483405013543\n    - type: nauc_map_at_5_std\n      value: -50.697541279640056\n    - type: nauc_mrr_at_1000_diff1\n      value: 73.84632320009759\n    - type: nauc_mrr_at_1000_max\n      value: 30.50182733610048\n    - type: nauc_mrr_at_1000_std\n      value: -44.3021647995251\n    - type: nauc_mrr_at_100_diff1\n      value: 73.84480792662302\n    - type: nauc_mrr_at_100_max\n      value: 30.50749424571614\n    - type: nauc_mrr_at_100_std\n      value: -44.29615086388113\n    - type: nauc_mrr_at_10_diff1\n      value: 73.79442772949346\n    - type: nauc_mrr_at_10_max\n      value: 30.55724252219984\n    - type: nauc_mrr_at_10_std\n      value: -44.50997069462057\n    - type: nauc_mrr_at_1_diff1\n      value: 75.23369827945945\n    - type: nauc_mrr_at_1_max\n      value: 29.20073967447664\n    - type: nauc_mrr_at_1_std\n      value: -43.1920147658285\n    - type: nauc_mrr_at_20_diff1\n      value: 73.82731678072307\n    - type: nauc_mrr_at_20_max\n      value: 30.566328605497667\n    - type: nauc_mrr_at_20_std\n      value: -44.24683607643705\n    - type: nauc_mrr_at_3_diff1\n      value: 73.61997576749954\n    - type: nauc_mrr_at_3_max\n      value: 30.150393853381917\n    - type: nauc_mrr_at_3_std\n      value: -44.96847297506626\n    - type: nauc_mrr_at_5_diff1\n      value: 73.69084310616132\n    - type: nauc_mrr_at_5_max\n      value: 30.578033703441125\n    - type: nauc_mrr_at_5_std\n      value: -44.74920746066566\n    - type: nauc_ndcg_at_1000_diff1\n      value: 72.89349862557452\n    - type: nauc_ndcg_at_1000_max\n      value: 29.824725190462086\n    - type: nauc_ndcg_at_1000_std\n      value: -44.96284395063211\n    - type: nauc_ndcg_at_100_diff1\n      value: 72.85212753715273\n    - type: nauc_ndcg_at_100_max\n      value: 29.933114207845605\n    - type: nauc_ndcg_at_100_std\n      value: -44.944225570663754\n    - type: nauc_ndcg_at_10_diff1\n      value: 72.80576740454528\n    - type: nauc_ndcg_at_10_max\n      value: 29.16829118320828\n    - type: nauc_ndcg_at_10_std\n      value: -48.149473740079614\n    - type: nauc_ndcg_at_1_diff1\n      value: 75.00032534968587\n    - type: nauc_ndcg_at_1_max\n      value: 29.61849062038547\n    - type: nauc_ndcg_at_1_std\n      value: -42.560207043864054\n    - type: nauc_ndcg_at_20_diff1\n      value: 72.88440406302502\n    - type: nauc_ndcg_at_20_max\n      value: 29.65496676092656\n    - type: nauc_ndcg_at_20_std\n      value: -46.21238462167732\n    - type: nauc_ndcg_at_3_diff1\n      value: 72.37916962766987\n    - type: nauc_ndcg_at_3_max\n      value: 27.125094834547586\n    - type: nauc_ndcg_at_3_std\n      value: -48.62942991399391\n    - type: nauc_ndcg_at_5_diff1\n      value: 72.57017330527658\n    - type: nauc_ndcg_at_5_max\n      value: 28.470485561757254\n    - type: nauc_ndcg_at_5_std\n      value: -49.07593345591059\n    - type: nauc_precision_at_1000_diff1\n      value: -41.67915575853946\n    - type: nauc_precision_at_1000_max\n      value: 1.2012264478568844\n    - type: nauc_precision_at_1000_std\n      value: 44.723834559400466\n    - type: nauc_precision_at_100_diff1\n      value: -40.45196679236971\n    - type: nauc_precision_at_100_max\n      value: 2.3525450401714894\n    - type: nauc_precision_at_100_std\n      value: 43.7092529413952\n    - type: nauc_precision_at_10_diff1\n      value: -30.256026923068767\n    - type: nauc_precision_at_10_max\n      value: 8.313422052132559\n    - type: nauc_precision_at_10_std\n      value: 25.929372356449694\n    - type: nauc_precision_at_1_diff1\n      value: 75.00032534968587\n    - type: nauc_precision_at_1_max\n      value: 29.61849062038547\n    - type: nauc_precision_at_1_std\n      value: -42.560207043864054\n    - type: nauc_precision_at_20_diff1\n      value: -35.61971069986584\n    - type: nauc_precision_at_20_max\n      value: 5.4664303079116765\n    - type: nauc_precision_at_20_std\n      value: 34.992352471692826\n    - type: nauc_precision_at_3_diff1\n      value: -5.691231842471157\n    - type: nauc_precision_at_3_max\n      value: 14.797949087742444\n    - type: nauc_precision_at_3_std\n      value: -0.1930317395644928\n    - type: nauc_precision_at_5_diff1\n      value: -20.03913781462645\n    - type: nauc_precision_at_5_max\n      value: 11.956771408712749\n    - type: nauc_precision_at_5_std\n      value: 13.179251389859731\n    - type: nauc_recall_at_1000_diff1\n      value: 64.03509042729674\n    - type: nauc_recall_at_1000_max\n      value: 40.91691485428493\n    - type: nauc_recall_at_1000_std\n      value: 16.12968625875372\n    - type: nauc_recall_at_100_diff1\n      value: 63.83116179628575\n    - type: nauc_recall_at_100_max\n      value: 43.72908117676382\n    - type: nauc_recall_at_100_std\n      value: -20.50966716852155\n    - type: nauc_recall_at_10_diff1\n      value: 66.42071960186394\n    - type: nauc_recall_at_10_max\n      value: 28.983207818687205\n    - type: nauc_recall_at_10_std\n      value: -56.61417798753744\n    - type: nauc_recall_at_1_diff1\n      value: 77.29690208952982\n    - type: nauc_recall_at_1_max\n      value: 19.839875762282293\n    - type: nauc_recall_at_1_std\n      value: -45.355684654708284\n    - type: nauc_recall_at_20_diff1\n      value: 66.32360705219874\n    - type: nauc_recall_at_20_max\n      value: 33.30698111822631\n    - type: nauc_recall_at_20_std\n      value: -43.89233781737452\n    - type: nauc_recall_at_3_diff1\n      value: 69.67029394927077\n    - type: nauc_recall_at_3_max\n      value: 22.67803039327696\n    - type: nauc_recall_at_3_std\n      value: -56.43327209861502\n    - type: nauc_recall_at_5_diff1\n      value: 68.05622143936131\n    - type: nauc_recall_at_5_max\n      value: 26.67795559040675\n    - type: nauc_recall_at_5_std\n      value: -58.158231198510954\n    - type: ndcg_at_1\n      value: 76.08\n    - type: ndcg_at_10\n      value: 84.114\n    - type: ndcg_at_100\n      value: 85.784\n    - type: ndcg_at_1000\n      value: 85.992\n    - type: ndcg_at_20\n      value: 84.976\n    - type: ndcg_at_3\n      value: 80.74799999999999\n    - type: ndcg_at_5\n      value: 82.626\n    - type: precision_at_1\n      value: 76.08\n    - type: precision_at_10\n      value: 12.926000000000002\n    - type: precision_at_100\n      value: 1.509\n    - type: precision_at_1000\n      value: 0.156\n    - type: precision_at_20\n      value: 6.912999999999999\n    - type: precision_at_3\n      value: 35.5\n    - type: precision_at_5\n      value: 23.541999999999998\n    - type: recall_at_1\n      value: 65.848\n    - type: recall_at_10\n      value: 92.611\n    - type: recall_at_100\n      value: 98.69\n    - type: recall_at_1000\n      value: 99.83999999999999\n    - type: recall_at_20\n      value: 95.47200000000001\n    - type: recall_at_3\n      value: 83.122\n    - type: recall_at_5\n      value: 88.23\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB SCIDOCS-PL (default)\n      revision: 45452b03f05560207ef19149545f168e596c9337\n      split: test\n      type: clarin-knext/scidocs-pl\n    metrics:\n    - type: main_score\n      value: 15.379999999999999\n    - type: map_at_1\n      value: 3.6029999999999998\n    - type: map_at_10\n      value: 8.843\n    - type: map_at_100\n      value: 10.433\n    - type: map_at_1000\n      value: 10.689\n    - type: map_at_20\n      value: 9.597\n    - type: map_at_3\n      value: 6.363\n    - type: map_at_5\n      value: 7.603\n    - type: mrr_at_1\n      value: 17.7\n    - type: mrr_at_10\n      value: 26.58900793650793\n    - type: mrr_at_100\n      value: 27.699652322890987\n    - type: mrr_at_1000\n      value: 27.78065313118353\n    - type: mrr_at_20\n      value: 27.215020950411816\n    - type: mrr_at_3\n      value: 23.36666666666668\n    - type: mrr_at_5\n      value: 25.211666666666666\n    - type: nauc_map_at_1000_diff1\n      value: 21.92235143827129\n    - type: nauc_map_at_1000_max\n      value: 37.50300940750989\n    - type: nauc_map_at_1000_std\n      value: 20.872586122198552\n    - type: nauc_map_at_100_diff1\n      value: 21.917408170465833\n    - type: nauc_map_at_100_max\n      value: 37.4654466815513\n    - type: nauc_map_at_100_std\n      value: 20.621643878648534\n    - type: nauc_map_at_10_diff1\n      value: 22.914388723621183\n    - type: nauc_map_at_10_max\n      value: 36.468131213468794\n    - type: nauc_map_at_10_std\n      value: 16.760980140791492\n    - type: nauc_map_at_1_diff1\n      value: 29.00799502838457\n    - type: nauc_map_at_1_max\n      value: 26.64926291797503\n    - type: nauc_map_at_1_std\n      value: 8.167291261637361\n    - type: nauc_map_at_20_diff1\n      value: 22.46580947804047\n    - type: nauc_map_at_20_max\n      value: 36.656294842562275\n    - type: nauc_map_at_20_std\n      value: 18.099232417722078\n    - type: nauc_map_at_3_diff1\n      value: 23.436009032045934\n    - type: nauc_map_at_3_max\n      value: 31.325807212280914\n    - type: nauc_map_at_3_std\n      value: 9.780905232048852\n    - type: nauc_map_at_5_diff1\n      value: 22.891704394665528\n    - type: nauc_map_at_5_max\n      value: 35.40584466642894\n    - type: nauc_map_at_5_std\n      value: 13.476986099394656\n    - type: nauc_mrr_at_1000_diff1\n      value: 25.052937655397866\n    - type: nauc_mrr_at_1000_max\n      value: 29.64431912670108\n    - type: nauc_mrr_at_1000_std\n      value: 14.549744963988044\n    - type: nauc_mrr_at_100_diff1\n      value: 25.070871266969224\n    - type: nauc_mrr_at_100_max\n      value: 29.68743604652336\n    - type: nauc_mrr_at_100_std\n      value: 14.582010154574432\n    - type: nauc_mrr_at_10_diff1\n      value: 24.88881466938897\n    - type: nauc_mrr_at_10_max\n      value: 29.488430770768144\n    - type: nauc_mrr_at_10_std\n      value: 14.269241073852266\n    - type: nauc_mrr_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_mrr_at_1_max\n      value: 26.81908580507911\n    - type: nauc_mrr_at_1_std\n      value: 8.00840295809718\n    - type: nauc_mrr_at_20_diff1\n      value: 25.067912695721944\n    - type: nauc_mrr_at_20_max\n      value: 29.759227563849628\n    - type: nauc_mrr_at_20_std\n      value: 14.685076859257357\n    - type: nauc_mrr_at_3_diff1\n      value: 24.645848739182696\n    - type: nauc_mrr_at_3_max\n      value: 27.73368549660351\n    - type: nauc_mrr_at_3_std\n      value: 11.475742805586943\n    - type: nauc_mrr_at_5_diff1\n      value: 24.895295760909946\n    - type: nauc_mrr_at_5_max\n      value: 29.130755033240423\n    - type: nauc_mrr_at_5_std\n      value: 12.955802929145404\n    - type: nauc_ndcg_at_1000_diff1\n      value: 20.68434434777729\n    - type: nauc_ndcg_at_1000_max\n      value: 37.67055146424174\n    - type: nauc_ndcg_at_1000_std\n      value: 29.57493715069776\n    - type: nauc_ndcg_at_100_diff1\n      value: 20.396834816492383\n    - type: nauc_ndcg_at_100_max\n      value: 37.460575228670514\n    - type: nauc_ndcg_at_100_std\n      value: 27.826534756761944\n    - type: nauc_ndcg_at_10_diff1\n      value: 22.640844106236027\n    - type: nauc_ndcg_at_10_max\n      value: 35.21291764462327\n    - type: nauc_ndcg_at_10_std\n      value: 19.53289455984506\n    - type: nauc_ndcg_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_ndcg_at_1_max\n      value: 26.81908580507911\n    - type: nauc_ndcg_at_1_std\n      value: 8.00840295809718\n    - type: nauc_ndcg_at_20_diff1\n      value: 22.117126657768623\n    - type: nauc_ndcg_at_20_max\n      value: 35.79395781940806\n    - type: nauc_ndcg_at_20_std\n      value: 22.242748346260786\n    - type: nauc_ndcg_at_3_diff1\n      value: 23.00596063212187\n    - type: nauc_ndcg_at_3_max\n      value: 30.149013627580523\n    - type: nauc_ndcg_at_3_std\n      value: 11.07904064662722\n    - type: nauc_ndcg_at_5_diff1\n      value: 22.81875419630523\n    - type: nauc_ndcg_at_5_max\n      value: 34.24267468356626\n    - type: nauc_ndcg_at_5_std\n      value: 15.307780280752088\n    - type: nauc_precision_at_1000_diff1\n      value: 9.606677689029972\n    - type: nauc_precision_at_1000_max\n      value: 32.74855550489271\n    - type: nauc_precision_at_1000_std\n      value: 42.65372585937895\n    - type: nauc_precision_at_100_diff1\n      value: 11.528981313529545\n    - type: nauc_precision_at_100_max\n      value: 35.642529490132404\n    - type: nauc_precision_at_100_std\n      value: 38.146151426052306\n    - type: nauc_precision_at_10_diff1\n      value: 18.783957183811836\n    - type: nauc_precision_at_10_max\n      value: 36.1982008334257\n    - type: nauc_precision_at_10_std\n      value: 25.09349473195891\n    - type: nauc_precision_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_precision_at_1_max\n      value: 26.81908580507911\n    - type: nauc_precision_at_1_std\n      value: 8.00840295809718\n    - type: nauc_precision_at_20_diff1\n      value: 17.458766320828214\n    - type: nauc_precision_at_20_max\n      value: 36.000404903025235\n    - type: nauc_precision_at_20_std\n      value: 29.1608044138323\n    - type: nauc_precision_at_3_diff1\n      value: 20.213669462067166\n    - type: nauc_precision_at_3_max\n      value: 31.120650847205912\n    - type: nauc_precision_at_3_std\n      value: 12.390972418818118\n    - type: nauc_precision_at_5_diff1\n      value: 20.114245715785678\n    - type: nauc_precision_at_5_max\n      value: 37.30360111495823\n    - type: nauc_precision_at_5_std\n      value: 19.053109037822853\n    - type: nauc_recall_at_1000_diff1\n      value: 9.85800049032612\n    - type: nauc_recall_at_1000_max\n      value: 32.48319160802687\n    - type: nauc_recall_at_1000_std\n      value: 43.79941601741161\n    - type: nauc_recall_at_100_diff1\n      value: 11.375255270968337\n    - type: nauc_recall_at_100_max\n      value: 35.1868784124497\n    - type: nauc_recall_at_100_std\n      value: 38.422680583482666\n    - type: nauc_recall_at_10_diff1\n      value: 18.445783123521938\n    - type: nauc_recall_at_10_max\n      value: 35.633267936276766\n    - type: nauc_recall_at_10_std\n      value: 24.94469506254716\n    - type: nauc_recall_at_1_diff1\n      value: 29.00799502838457\n    - type: nauc_recall_at_1_max\n      value: 26.64926291797503\n    - type: nauc_recall_at_1_std\n      value: 8.167291261637361\n    - type: nauc_recall_at_20_diff1\n      value: 17.314906604151936\n    - type: nauc_recall_at_20_max\n      value: 35.66067699203996\n    - type: nauc_recall_at_20_std\n      value: 29.400137012506082\n    - type: nauc_recall_at_3_diff1\n      value: 19.873710875648698\n    - type: nauc_recall_at_3_max\n      value: 30.92404718742849\n    - type: nauc_recall_at_3_std\n      value: 12.400871018075199\n    - type: nauc_recall_at_5_diff1\n      value: 19.869948324233192\n    - type: nauc_recall_at_5_max\n      value: 37.06832511687574\n    - type: nauc_recall_at_5_std\n      value: 19.0798814966156\n    - type: ndcg_at_1\n      value: 17.7\n    - type: ndcg_at_10\n      value: 15.379999999999999\n    - type: ndcg_at_100\n      value: 22.09\n    - type: ndcg_at_1000\n      value: 27.151999999999997\n    - type: ndcg_at_20\n      value: 17.576\n    - type: ndcg_at_3\n      value: 14.219999999999999\n    - type: ndcg_at_5\n      value: 12.579\n    - type: precision_at_1\n      value: 17.7\n    - type: precision_at_10\n      value: 8.08\n    - type: precision_at_100\n      value: 1.7840000000000003\n    - type: precision_at_1000\n      value: 0.3\n    - type: precision_at_20\n      value: 5.305\n    - type: precision_at_3\n      value: 13.167000000000002\n    - type: precision_at_5\n      value: 11.06\n    - type: recall_at_1\n      value: 3.6029999999999998\n    - type: recall_at_10\n      value: 16.413\n    - type: recall_at_100\n      value: 36.263\n    - type: recall_at_1000\n      value: 61.016999999999996\n    - type: recall_at_20\n      value: 21.587999999999997\n    - type: recall_at_3\n      value: 8.013\n    - type: recall_at_5\n      value: 11.198\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB SciFact-PL (default)\n      revision: 47932a35f045ef8ed01ba82bf9ff67f6e109207e\n      split: test\n      type: clarin-knext/scifact-pl\n    metrics:\n    - type: main_score\n      value: 64.764\n    - type: map_at_1\n      value: 49.778\n    - type: map_at_10\n      value: 59.88\n    - type: map_at_100\n      value: 60.707\n    - type: map_at_1000\n      value: 60.729\n    - type: map_at_20\n      value: 60.419999999999995\n    - type: map_at_3\n      value: 57.45400000000001\n    - type: map_at_5\n      value: 58.729\n    - type: mrr_at_1\n      value: 52.33333333333333\n    - type: mrr_at_10\n      value: 61.29193121693122\n    - type: mrr_at_100\n      value: 61.95817765126313\n    - type: mrr_at_1000\n      value: 61.97583284368782\n    - type: mrr_at_20\n      value: 61.72469949641003\n    - type: mrr_at_3\n      value: 59.44444444444444\n    - type: mrr_at_5\n      value: 60.494444444444454\n    - type: nauc_map_at_1000_diff1\n      value: 62.21235294015774\n    - type: nauc_map_at_1000_max\n      value: 48.83996609100249\n    - type: nauc_map_at_1000_std\n      value: 5.23892781043174\n    - type: nauc_map_at_100_diff1\n      value: 62.20170226789429\n    - type: nauc_map_at_100_max\n      value: 48.8391766453537\n    - type: nauc_map_at_100_std\n      value: 5.2664077457917715\n    - type: nauc_map_at_10_diff1\n      value: 61.961975488329024\n    - type: nauc_map_at_10_max\n      value: 48.397109987625186\n    - type: nauc_map_at_10_std\n      value: 4.314859710827481\n    - type: nauc_map_at_1_diff1\n      value: 65.0865197011516\n    - type: nauc_map_at_1_max\n      value: 41.38862781954889\n    - type: nauc_map_at_1_std\n      value: -0.9182122632530586\n    - type: nauc_map_at_20_diff1\n      value: 61.99173935851292\n    - type: nauc_map_at_20_max\n      value: 48.79961814179307\n    - type: nauc_map_at_20_std\n      value: 5.262181845825118\n    - type: nauc_map_at_3_diff1\n      value: 62.37910539880477\n    - type: nauc_map_at_3_max\n      value: 47.13627890977091\n    - type: nauc_map_at_3_std\n      value: 2.327897198087264\n    - type: nauc_map_at_5_diff1\n      value: 61.60080757149592\n    - type: nauc_map_at_5_max\n      value: 47.60052458345962\n    - type: nauc_map_at_5_std\n      value: 3.1770196981231047\n    - type: nauc_mrr_at_1000_diff1\n      value: 62.86810952814966\n    - type: nauc_mrr_at_1000_max\n      value: 52.13248094447774\n    - type: nauc_mrr_at_1000_std\n      value: 10.100485746570733\n    - type: nauc_mrr_at_100_diff1\n      value: 62.85364829491874\n    - type: nauc_mrr_at_100_max\n      value: 52.134528010631854\n    - type: nauc_mrr_at_100_std\n      value: 10.120945685447369\n    - type: nauc_mrr_at_10_diff1\n      value: 62.65679301829915\n    - type: nauc_mrr_at_10_max\n      value: 52.09270719182349\n    - type: nauc_mrr_at_10_std\n      value: 9.913834434725441\n    - type: nauc_mrr_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_mrr_at_1_max\n      value: 46.67646429855176\n    - type: nauc_mrr_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_mrr_at_20_diff1\n      value: 62.72473227039611\n    - type: nauc_mrr_at_20_max\n      value: 52.13479097802757\n    - type: nauc_mrr_at_20_std\n      value: 10.188278833464084\n    - type: nauc_mrr_at_3_diff1\n      value: 63.797429185518496\n    - type: nauc_mrr_at_3_max\n      value: 52.16486999573481\n    - type: nauc_mrr_at_3_std\n      value: 9.094360767062762\n    - type: nauc_mrr_at_5_diff1\n      value: 62.592917975475494\n    - type: nauc_mrr_at_5_max\n      value: 52.330741486107414\n    - type: nauc_mrr_at_5_std\n      value: 9.742175534421389\n    - type: nauc_ndcg_at_1000_diff1\n      value: 61.38859337672476\n    - type: nauc_ndcg_at_1000_max\n      value: 51.48380058339184\n    - type: nauc_ndcg_at_1000_std\n      value: 9.670547660897673\n    - type: nauc_ndcg_at_100_diff1\n      value: 61.02438489641434\n    - type: nauc_ndcg_at_100_max\n      value: 51.781246646780865\n    - type: nauc_ndcg_at_100_std\n      value: 10.592961553245187\n    - type: nauc_ndcg_at_10_diff1\n      value: 60.03678353308358\n    - type: nauc_ndcg_at_10_max\n      value: 50.70725688848762\n    - type: nauc_ndcg_at_10_std\n      value: 7.9472446491016315\n    - type: nauc_ndcg_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_ndcg_at_1_max\n      value: 46.67646429855176\n    - type: nauc_ndcg_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_ndcg_at_20_diff1\n      value: 59.828482718480224\n    - type: nauc_ndcg_at_20_max\n      value: 51.45831789601284\n    - type: nauc_ndcg_at_20_std\n      value: 10.722673683272049\n    - type: nauc_ndcg_at_3_diff1\n      value: 61.68982937524109\n    - type: nauc_ndcg_at_3_max\n      value: 49.745326748604775\n    - type: nauc_ndcg_at_3_std\n      value: 4.948298621202247\n    - type: nauc_ndcg_at_5_diff1\n      value: 59.67396171973207\n    - type: nauc_ndcg_at_5_max\n      value: 49.87855139298281\n    - type: nauc_ndcg_at_5_std\n      value: 6.08990428055584\n    - type: nauc_precision_at_1000_diff1\n      value: -1.594227972036865\n    - type: nauc_precision_at_1000_max\n      value: 32.48431723086185\n    - type: nauc_precision_at_1000_std\n      value: 53.84748466965268\n    - type: nauc_precision_at_100_diff1\n      value: 8.06411455192293\n    - type: nauc_precision_at_100_max\n      value: 39.91003601878948\n    - type: nauc_precision_at_100_std\n      value: 55.52979711075091\n    - type: nauc_precision_at_10_diff1\n      value: 26.610514456014066\n    - type: nauc_precision_at_10_max\n      value: 47.09062494321172\n    - type: nauc_precision_at_10_std\n      value: 33.91984226498748\n    - type: nauc_precision_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_precision_at_1_max\n      value: 46.67646429855176\n    - type: nauc_precision_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_precision_at_20_diff1\n      value: 16.947688843085583\n    - type: nauc_precision_at_20_max\n      value: 45.40488186572008\n    - type: nauc_precision_at_20_std\n      value: 48.354421924500905\n    - type: nauc_precision_at_3_diff1\n      value: 49.11263981720622\n    - type: nauc_precision_at_3_max\n      value: 52.7084625111683\n    - type: nauc_precision_at_3_std\n      value: 16.734612173556453\n    - type: nauc_precision_at_5_diff1\n      value: 39.06503705015792\n    - type: nauc_precision_at_5_max\n      value: 52.21710506893391\n    - type: nauc_precision_at_5_std\n      value: 23.350948149460233\n    - type: nauc_recall_at_1000_diff1\n      value: 43.1559290382817\n    - type: nauc_recall_at_1000_max\n      value: 83.66013071895456\n    - type: nauc_recall_at_1000_std\n      value: 86.27450980392177\n    - type: nauc_recall_at_100_diff1\n      value: 46.016860850620375\n    - type: nauc_recall_at_100_max\n      value: 69.3944888744547\n    - type: nauc_recall_at_100_std\n      value: 55.286945696152735\n    - type: nauc_recall_at_10_diff1\n      value: 49.65877895350921\n    - type: nauc_recall_at_10_max\n      value: 53.02636695700889\n    - type: nauc_recall_at_10_std\n      value: 13.967608945823828\n    - type: nauc_recall_at_1_diff1\n      value: 65.0865197011516\n    - type: nauc_recall_at_1_max\n      value: 41.38862781954889\n    - type: nauc_recall_at_1_std\n      value: -0.9182122632530586\n    - type: nauc_recall_at_20_diff1\n      value: 43.355308229973524\n    - type: nauc_recall_at_20_max\n      value: 57.04187909533764\n    - type: nauc_recall_at_20_std\n      value: 33.578720846660524\n    - type: nauc_recall_at_3_diff1\n      value: 56.922996057428165\n    - type: nauc_recall_at_3_max\n      value: 50.74417041895424\n    - type: nauc_recall_at_3_std\n      value: 5.623890124328387\n    - type: nauc_recall_at_5_diff1\n      value: 50.55620076865238\n    - type: nauc_recall_at_5_max\n      value: 51.3316854622085\n    - type: nauc_recall_at_5_std\n      value: 8.995457887269255\n    - type: ndcg_at_1\n      value: 52.333\n    - type: ndcg_at_10\n      value: 64.764\n    - type: ndcg_at_100\n      value: 68.167\n    - type: ndcg_at_1000\n      value: 68.816\n    - type: ndcg_at_20\n      value: 66.457\n    - type: ndcg_at_3\n      value: 60.346\n    - type: ndcg_at_5\n      value: 62.365\n    - type: precision_at_1\n      value: 52.333\n    - type: precision_at_10\n      value: 8.799999999999999\n    - type: precision_at_100\n      value: 1.057\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_20\n      value: 4.8\n    - type: precision_at_3\n      value: 23.889\n    - type: precision_at_5\n      value: 15.6\n    - type: recall_at_1\n      value: 49.778\n    - type: recall_at_10\n      value: 78.206\n    - type: recall_at_100\n      value: 93.10000000000001\n    - type: recall_at_1000\n      value: 98.333\n    - type: recall_at_20\n      value: 84.467\n    - type: recall_at_3\n      value: 66.367\n    - type: recall_at_5\n      value: 71.35000000000001\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB TRECCOVID-PL (default)\n      revision: 81bcb408f33366c2a20ac54adafad1ae7e877fdd\n      split: test\n      type: clarin-knext/trec-covid-pl\n    metrics:\n    - type: main_score\n      value: 72.18900000000001\n    - type: map_at_1\n      value: 0.214\n    - type: map_at_10\n      value: 1.755\n    - type: map_at_100\n      value: 9.944\n    - type: map_at_1000\n      value: 24.205\n    - type: map_at_20\n      value: 3.1510000000000002\n    - type: map_at_3\n      value: 0.6\n    - type: map_at_5\n      value: 0.9560000000000001\n    - type: mrr_at_1\n      value: 82.0\n    - type: mrr_at_10\n      value: 89.06666666666666\n    - type: mrr_at_100\n      value: 89.06666666666666\n    - type: mrr_at_1000\n      value: 89.06666666666666\n    - type: mrr_at_20\n      value: 89.06666666666666\n    - type: mrr_at_3\n      value: 87.66666666666666\n    - type: mrr_at_5\n      value: 89.06666666666666\n    - type: nauc_map_at_1000_diff1\n      value: -9.342037623635543\n    - type: nauc_map_at_1000_max\n      value: 45.71499810252398\n    - type: nauc_map_at_1000_std\n      value: 76.86482845196852\n    - type: nauc_map_at_100_diff1\n      value: -6.932395299866198\n    - type: nauc_map_at_100_max\n      value: 36.097801891181604\n    - type: nauc_map_at_100_std\n      value: 65.6085215411685\n    - type: nauc_map_at_10_diff1\n      value: -6.3654843824342775\n    - type: nauc_map_at_10_max\n      value: 9.564437521432714\n    - type: nauc_map_at_10_std\n      value: 21.8377319336476\n    - type: nauc_map_at_1_diff1\n      value: 8.269590874255034\n    - type: nauc_map_at_1_max\n      value: 3.482498491294516\n    - type: nauc_map_at_1_std\n      value: 8.985226819412189\n    - type: nauc_map_at_20_diff1\n      value: -4.971435767877232\n    - type: nauc_map_at_20_max\n      value: 22.88801858567121\n    - type: nauc_map_at_20_std\n      value: 32.38492618534027\n    - type: nauc_map_at_3_diff1\n      value: 1.1615973694623123\n    - type: nauc_map_at_3_max\n      value: 1.935417800315643\n    - type: nauc_map_at_3_std\n      value: 10.289328305818698\n    - type: nauc_map_at_5_diff1\n      value: -2.4675967231444105\n    - type: nauc_map_at_5_max\n      value: 2.4611483736622373\n    - type: nauc_map_at_5_std\n      value: 15.082324305750811\n    - type: nauc_mrr_at_1000_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_1000_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_1000_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_100_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_100_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_100_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_10_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_10_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_10_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_1_diff1\n      value: 12.099350148694809\n    - type: nauc_mrr_at_1_max\n      value: 53.75041304108387\n    - type: nauc_mrr_at_1_std\n      value: 68.84018063663402\n    - type: nauc_mrr_at_20_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_20_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_20_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_3_diff1\n      value: 12.173557857011161\n    - type: nauc_mrr_at_3_max\n      value: 57.540780562363395\n    - type: nauc_mrr_at_3_std\n      value: 75.42098189580211\n    - type: nauc_mrr_at_5_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_5_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_5_std\n      value: 73.2456769749587\n    - type: nauc_ndcg_at_1000_diff1\n      value: -8.951471847310401\n    - type: nauc_ndcg_at_1000_max\n      value: 43.86942237288822\n    - type: nauc_ndcg_at_1000_std\n      value: 74.61077735148591\n    - type: nauc_ndcg_at_100_diff1\n      value: -17.754559361083817\n    - type: nauc_ndcg_at_100_max\n      value: 53.97187119773482\n    - type: nauc_ndcg_at_100_std\n      value: 80.7944136146514\n    - type: nauc_ndcg_at_10_diff1\n      value: -26.637734697836414\n    - type: nauc_ndcg_at_10_max\n      value: 47.70102699133149\n    - type: nauc_ndcg_at_10_std\n      value: 70.26909560828646\n    - type: nauc_ndcg_at_1_diff1\n      value: -1.2250530785563207\n    - type: nauc_ndcg_at_1_max\n      value: 46.60509554140131\n    - type: nauc_ndcg_at_1_std\n      value: 62.63906581740976\n    - type: nauc_ndcg_at_20_diff1\n      value: -22.44286466550908\n    - type: nauc_ndcg_at_20_max\n      value: 55.40492058090103\n    - type: nauc_ndcg_at_20_std\n      value: 72.11813912145738\n    - type: nauc_ndcg_at_3_diff1\n      value: -14.8152721896563\n    - type: nauc_ndcg_at_3_max\n      value: 38.952259383027595\n    - type: nauc_ndcg_at_3_std\n      value: 59.819750166537766\n    - type: nauc_ndcg_at_5_diff1\n      value: -19.150105688904375\n    - type: nauc_ndcg_at_5_max\n      value: 42.311180547775315\n    - type: nauc_ndcg_at_5_std\n      value: 66.6632229321094\n    - type: nauc_precision_at_1000_diff1\n      value: -11.555591477978941\n    - type: nauc_precision_at_1000_max\n      value: 43.7311644834851\n    - type: nauc_precision_at_1000_std\n      value: 52.10644767999648\n    - type: nauc_precision_at_100_diff1\n      value: -16.94803099801117\n    - type: nauc_precision_at_100_max\n      value: 54.08281631067633\n    - type: nauc_precision_at_100_std\n      value: 82.77237347891331\n    - type: nauc_precision_at_10_diff1\n      value: -27.351332814863355\n    - type: nauc_precision_at_10_max\n      value: 48.08237549065846\n    - type: nauc_precision_at_10_std\n      value: 69.37250843534329\n    - type: nauc_precision_at_1_diff1\n      value: 12.099350148694809\n    - type: nauc_precision_at_1_max\n      value: 53.75041304108387\n    - type: nauc_precision_at_1_std\n      value: 68.84018063663402\n    - type: nauc_precision_at_20_diff1\n      value: -18.2422222283388\n    - type: nauc_precision_at_20_max\n      value: 59.517328129343696\n    - type: nauc_precision_at_20_std\n      value: 72.05149307342747\n    - type: nauc_precision_at_3_diff1\n      value: -10.226547543075897\n    - type: nauc_precision_at_3_max\n      value: 43.14684818832875\n    - type: nauc_precision_at_3_std\n      value: 57.31936467418288\n    - type: nauc_precision_at_5_diff1\n      value: -14.28521589468673\n    - type: nauc_precision_at_5_max\n      value: 41.633426753962596\n    - type: nauc_precision_at_5_std\n      value: 64.94400576804541\n    - type: nauc_recall_at_1000_diff1\n      value: -0.9648831207497152\n    - type: nauc_recall_at_1000_max\n      value: 31.70832946085005\n    - type: nauc_recall_at_1000_std\n      value: 63.21471613968869\n    - type: nauc_recall_at_100_diff1\n      value: -1.360254380933586\n    - type: nauc_recall_at_100_max\n      value: 25.960597782099605\n    - type: nauc_recall_at_100_std\n      value: 51.52757589609674\n    - type: nauc_recall_at_10_diff1\n      value: -0.3899439424189566\n    - type: nauc_recall_at_10_max\n      value: 5.094341897886072\n    - type: nauc_recall_at_10_std\n      value: 11.266045616925698\n    - type: nauc_recall_at_1_diff1\n      value: 8.269590874255034\n    - type: nauc_recall_at_1_max\n      value: 3.482498491294516\n    - type: nauc_recall_at_1_std\n      value: 8.985226819412189\n    - type: nauc_recall_at_20_diff1\n      value: 6.4797098359254175\n    - type: nauc_recall_at_20_max\n      value: 15.663700985336124\n    - type: nauc_recall_at_20_std\n      value: 17.154099587904913\n    - type: nauc_recall_at_3_diff1\n      value: 3.7245972450393507\n    - type: nauc_recall_at_3_max\n      value: 0.4063857187240345\n    - type: nauc_recall_at_3_std\n      value: 6.641948062821941\n    - type: nauc_recall_at_5_diff1\n      value: 4.013879477591466\n    - type: nauc_recall_at_5_max\n      value: -1.4266586618013566\n    - type: nauc_recall_at_5_std\n      value: 7.311601874411205\n    - type: ndcg_at_1\n      value: 75.0\n    - type: ndcg_at_10\n      value: 72.18900000000001\n    - type: ndcg_at_100\n      value: 54.022999999999996\n    - type: ndcg_at_1000\n      value: 49.492000000000004\n    - type: ndcg_at_20\n      value: 68.51\n    - type: ndcg_at_3\n      value: 73.184\n    - type: ndcg_at_5\n      value: 72.811\n    - type: precision_at_1\n      value: 82.0\n    - type: precision_at_10\n      value: 77.4\n    - type: precision_at_100\n      value: 55.24\n    - type: precision_at_1000\n      value: 21.822\n    - type: precision_at_20\n      value: 73.0\n    - type: precision_at_3\n      value: 79.333\n    - type: precision_at_5\n      value: 79.2\n    - type: recall_at_1\n      value: 0.214\n    - type: recall_at_10\n      value: 1.9980000000000002\n    - type: recall_at_100\n      value: 13.328999999999999\n    - type: recall_at_1000\n      value: 47.204\n    - type: recall_at_20\n      value: 3.7310000000000003\n    - type: recall_at_3\n      value: 0.628\n    - type: recall_at_5\n      value: 1.049\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB CEDRClassification (default)\n      revision: c0ba03d058e3e1b2f3fd20518875a4563dd12db4\n      split: test\n      type: ai-forever/cedr-classification\n    metrics:\n    - type: accuracy\n      value: 47.30605738575983\n    - type: f1\n      value: 41.26091043925065\n    - type: lrap\n      value: 72.89452709883206\n    - type: main_score\n      value: 47.30605738575983\n    task:\n      type: MultilabelClassification\n  - dataset:\n      config: ru\n      name: MTEB MIRACLReranking (ru)\n      revision: 6d1962c527217f8927fca80f890f14f36b2802af\n      split: dev\n      type: miracl/mmteb-miracl-reranking\n    metrics:\n    - type: MAP@1(MIRACL)\n      value: 20.721999999999998\n    - type: MAP@10(MIRACL)\n      value: 33.900999999999996\n    - type: MAP@100(MIRACL)\n      value: 36.813\n    - type: MAP@1000(MIRACL)\n      value: 36.813\n    - type: MAP@20(MIRACL)\n      value: 35.684\n    - type: MAP@3(MIRACL)\n      value: 28.141\n    - type: MAP@5(MIRACL)\n      value: 31.075000000000003\n    - type: NDCG@1(MIRACL)\n      value: 32.799\n    - type: NDCG@10(MIRACL)\n      value: 42.065000000000005\n    - type: NDCG@100(MIRACL)\n      value: 49.730999999999995\n    - type: NDCG@1000(MIRACL)\n      value: 49.730999999999995\n    - type: NDCG@20(MIRACL)\n      value: 46.0\n    - type: NDCG@3(MIRACL)\n      value: 34.481\n    - type: NDCG@5(MIRACL)\n      value: 37.452999999999996\n    - type: P@1(MIRACL)\n\n\n[Content truncated...]",
    "meta_json": "{\"pipeline_tag\":\"feature-extraction\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":572310396,\"storage_bytes\":8007991570,\"files_count\":15,\"spaces_count\":41,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"XLMRobertaModel\"],\"auto_map\":{\"AutoConfig\":\"jinaai/xlm-roberta-flash-implementation--configuration_xlm_roberta.XLMRobertaFlashConfig\",\"AutoModel\":\"jinaai/xlm-roberta-flash-implementation--modeling_lora.XLMRobertaLoRA\",\"AutoModelForMaskedLM\":\"jinaai/xlm-roberta-flash-implementation--modeling_xlm_roberta.XLMRobertaForMaskedLM\",\"AutoModelForPreTraining\":\"jinaai/xlm-roberta-flash-implementation--modeling_xlm_roberta.XLMRobertaForPreTraining\"},\"tokenizer_config\":{\"bos_token\":\"<s>\",\"cls_token\":\"<s>\",\"eos_token\":\"</s>\",\"mask_token\":\"<mask>\",\"pad_token\":\"<pad>\",\"sep_token\":\"</s>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2409.10173\",\"source_url\":\"https://arxiv.org/abs/2409.10173\"}]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "cad87a11a3a9373d3d809847bb079d70",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/jinaai/jina-embeddings-v3\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:meta-llama:llama-2-13b-chat-hf",
    "name": "Llama-2-13b-chat-hf",
    "author": "meta-llama",
    "description": "",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "llama-2",
      "conversational",
      "en",
      "arxiv:2307.09288",
      "license:llama2",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1105,
    "downloads": 272996,
    "source": "huggingface",
    "source_url": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":13015866880,\"storage_bytes\":52065420569,\"files_count\":19,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"LlamaForCausalLM\"],\"model_type\":\"llama\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<s>\",\"lstrip\":false,\"normalized\":false,\"rstrip\":false,\"single_word\":false},\"chat_template\":\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\",\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"</s>\",\"lstrip\":false,\"normalized\":false,\"rstrip\":false,\"single_word\":false},\"pad_token\":null,\"unk_token\":{\"__type\":\"AddedToken\",\"content\":\"<unk>\",\"lstrip\":false,\"normalized\":false,\"rstrip\":false,\"single_word\":false}}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2307.09288\",\"source_url\":\"https://arxiv.org/abs/2307.09288\"}]",
    "canonical_id": null,
    "license_spdx": "LLaMA-2",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "4994b2d8f6c9b9fed7509ffce32aac99",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:zyphra:zonos-v0.1-hybrid",
    "name": "Zonos-v0.1-hybrid",
    "author": "Zyphra",
    "description": "--- license: apache-2.0 pipeline_tag: text-to-speech library_name: zonos --- <div align=\"center\"> <img src=\"https://github.com/Zyphra/Zonos/blob/main/assets/ZonosHeader.png?raw=true\" alt=\"Title card\" style=\"width: 500px; height: auto; object-position: center top;\"> </div> --- Zonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par withâ€”or even surpassingâ€”top TTS providers. Our model en...",
    "tags": [
      "zonos",
      "safetensors",
      "text-to-speech",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-speech",
    "likes": 1100,
    "downloads": 43413,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Zyphra/Zonos-v0.1-hybrid",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: apache-2.0\npipeline_tag: text-to-speech\nlibrary_name: zonos\n---\n# Zonos-v0.1\n\n<div align=\"center\">\n<img src=\"https://github.com/Zyphra/Zonos/blob/main/assets/ZonosHeader.png?raw=true\"\n     alt=\"Title card\" \n     style=\"width: 500px;\n            height: auto;\n            object-position: center top;\">\n</div>\n\n---\n\nZonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par withâ€”or even surpassingâ€”top TTS providers.\n\nOur model enables highly natural speech generation from text prompts when given a speaker embedding or audio prefix, and can accurately perform speech cloning when given a reference clip spanning just a few seconds. The conditioning setup also allows for fine control over speaking rate, pitch variation, audio quality, and emotions such as happiness, fear, sadness, and anger. The model outputs speech natively at 44kHz.\n\n##### For more details and speech samples, check out our blog [here](https://www.zyphra.com/post/beta-release-of-zonos-v0-1)\n\n##### We also have a hosted version available at [playground.zyphra.com/audio](https://playground.zyphra.com/audio)\n\n---\n\nZonos follows a straightforward architecture: text normalization and phonemization via eSpeak, followed by DAC token prediction through a transformer or hybrid backbone. An overview of the architecture can be seen below.\n\n<div align=\"center\">\n<img src=\"https://github.com/Zyphra/Zonos/blob/main/assets/ArchitectureDiagram.png?raw=true\"\n     alt=\"Architecture diagram\" \n     style=\"width: 1000px;\n            height: auto;\n            object-position: center top;\">\n</div>\n\n---\n\n## Usage\n\n### Python\n\n```python\nimport torch\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-hybrid\", device=\"cuda\")\n# model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=\"cuda\")\n\nwav, sampling_rate = torchaudio.load(\"assets/exampleaudio.mp3\")\nspeaker = model.make_speaker_embedding(wav, sampling_rate)\n\ncond_dict = make_cond_dict(text=\"Hello, world!\", speaker=speaker, language=\"en-us\")\nconditioning = model.prepare_conditioning(cond_dict)\n\ncodes = model.generate(conditioning)\n\nwavs = model.autoencoder.decode(codes).cpu()\ntorchaudio.save(\"sample.wav\", wavs[0], model.autoencoder.sampling_rate)\n```\n\n### Gradio interface (recommended)\n\n```bash\nuv run gradio_interface.py\n# python gradio_interface.py\n```\n\nThis should produce a `sample.wav` file in your project root directory.\n\n_For repeated sampling we highly recommend using the gradio interface instead, as the minimal example needs to load the model every time it is run._\n\n## Features\n\n- Zero-shot TTS with voice cloning: Input desired text and a 10-30s speaker sample to generate high quality TTS output\n- Audio prefix inputs: Add text plus an audio prefix for even richer speaker matching. Audio prefixes can be used to elicit behaviours such as whispering which can otherwise be challenging to replicate when cloning from speaker embeddings\n- Multilingual support: Zonos-v0.1 supports English, Japanese, Chinese, French, and German\n- Audio quality and emotion control: Zonos offers fine-grained control of many aspects of the generated audio. These include speaking rate, pitch, maximum frequency, audio quality, and various emotions such as happiness, anger, sadness, and fear.\n- Fast: our model runs with a real-time factor of ~2x on an RTX 4090\n- Gradio WebUI: Zonos comes packaged with an easy to use gradio interface to generate speech\n- Simple installation and deployment: Zonos can be installed and deployed simply using the docker file packaged with our repository.\n\n## Installation\n\n**At the moment this repository only supports Linux systems (preferably Ubuntu 22.04/24.04) with recent NVIDIA GPUs (3000-series or newer, 6GB+ VRAM).**\n\nSee also [Docker Installation](#docker-installation)\n\n#### System dependencies\n\nZonos depends on the eSpeak library phonemization. You can install it on Ubuntu with the following command:\n\n```bash\napt install -y espeak-ng\n```\n\n#### Python dependencies\n\nWe highly recommend using a recent version of [uv](https://docs.astral.sh/uv/#installation) for installation. If you don't have uv installed, you can install it via pip: `pip install -U uv`.\n\n##### Installing into a new uv virtual environment (recommended)\n\n```bash\nuv sync\nuv sync --extra compile\n```\n\n##### Installing into the system/actived environment using uv\n\n```bash\nuv pip install -e .\nuv pip install -e .[compile]\n```\n\n##### Installing into the system/actived environment using pip\n\n```bash\npip install -e .\npip install --no-build-isolation -e .[compile]\n```\n\n##### Confirm that it's working\n\nFor convenience we provide a minimal example to check that the installation works:\n\n```bash\nuv run sample.py\n# python sample.py\n```\n\n## Docker installation\n\n```bash\ngit clone https://github.com/Zyphra/Zonos.git\ncd Zonos\n\n# For gradio\ndocker compose up\n\n# Or for development you can do\ndocker build -t Zonos .\ndocker run -it --gpus=all --net=host -v /path/to/Zonos:/Zonos -t Zonos\ncd /Zonos\npython sample.py # this will generate a sample.wav in /Zonos\n```\n\n## Citation\nIf you find this model useful in an academic context please cite as:\n```bash\n@misc{zyphra2025zonos,\n  title     = {Zonos-v0.1: An Expressive, Open-Source TTS Model},\n  author    = {Dario Sucic, Mohamed Osman, Gabriel Clark, Chris Warner, Beren Millidge},\n  year      = {2025},\n}",
    "meta_json": "{\"pipeline_tag\":\"text-to-speech\",\"library_name\":\"zonos\",\"framework\":\"zonos\",\"params\":null,\"storage_bytes\":3700759529,\"files_count\":4,\"spaces_count\":33,\"gated\":false,\"private\":false,\"config\":{}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Zyphra:Zonos\",\"source_url\":\"https://github.com/Zyphra/Zonos\"},{\"type\":\"has_code\",\"target_id\":\"github:Zyphra:Zonos\",\"source_url\":\"https://github.com/Zyphra/Zonos\"},{\"type\":\"has_code\",\"target_id\":\"github:Zyphra:Zonos.git\",\"source_url\":\"https://github.com/Zyphra/Zonos.git\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "2ba4c6c630bd69849373200db6a0b28a",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Zyphra/Zonos-v0.1-hybrid\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:unsloth:deepseek-r1-gguf",
    "name": "DeepSeek-R1-GGUF",
    "author": "unsloth",
    "description": "--- base_model: deepseek-ai/DeepSeek-R1 language: - en library_name: transformers license: mit tags: - deepseek - unsloth - transformers new_version: unsloth/DeepSeek-R1-0528-GGUF --- <div> <p style=\"margin-bottom: 0; margin-top: 0;\"> <strong>See <a href=\"https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5\">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong> </p> <p style=\"margin-bottom: 0;\"> <em>Unsloth's DeepSeek-R...",
    "tags": [
      "transformers",
      "gguf",
      "deepseek_v3",
      "text-generation",
      "deepseek",
      "unsloth",
      "custom_code",
      "en",
      "arxiv:2501.12948",
      "base_model:deepseek-ai/deepseek-r1",
      "base_model:quantized:deepseek-ai/deepseek-r1",
      "license:mit",
      "endpoints_compatible",
      "region:us",
      "conversational"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1099,
    "downloads": 25185,
    "source": "huggingface",
    "source_url": "https://huggingface.co/unsloth/DeepSeek-R1-GGUF",
    "image_url": null,
    "type": "model",
    "body_content": "---\nbase_model: deepseek-ai/DeepSeek-R1\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\ntags:\n- deepseek\n- unsloth\n- transformers\nnew_version: unsloth/DeepSeek-R1-0528-GGUF\n---\n\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5\">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em>Unsloth's DeepSeek-R1 <a href=\"https://unsloth.ai/blog/deepseekr1-dynamic\">1.58-bit + 2-bit Dynamic Quants</a> is selectively quantized, greatly improving accuracy over standard 1-bit/2-bit.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">Instructions to run this model in llama.cpp:</h2>\n</div>\n\nOr you can view more detailed instructions here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)\n1. Do not forget about `<ï½œUserï½œ>` and `<ï½œAssistantï½œ>` tokens! - Or use a chat template formatter\n2. Obtain the latest `llama.cpp` at https://github.com/ggerganov/llama.cpp. You can follow the build instructions below as well:\n```bash\napt-get update\napt-get install build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n\t-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```\n3. It's best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.\n4. Download the model via:\n```python\n# pip install huggingface_hub hf_transfer\n# import os # Optional for faster downloading\n# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n  repo_id = \"unsloth/DeepSeek-R1-GGUF\",\n  local_dir = \"DeepSeek-R1-GGUF\",\n  allow_patterns = [\"*UD-IQ1_S*\"], # Select quant type UD-IQ1_S for 1.58bit\n)\n```\n5. Example with Q4_0 K quantized cache **Notice -no-cnv disables auto conversation mode**\n```bash\n   ./llama.cpp/llama-cli \\\n\t  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n\t  --cache-type-k q4_0 \\\n\t  --threads 12 -no-cnv --prio 2 \\\n\t  --temp 0.6 \\\n\t  --ctx-size 8192 \\\n\t  --seed 3407 \\\n\t  --prompt \"<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>\"\n```\n   Example output:\n   \n   ```txt\n    <think>\n    Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\n    Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\n    Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\n    I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\n    Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\n   ```\n   \n6. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n```bash\n  ./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --prio 2 \\\n    --n-gpu-layers 7 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>\"\n```\n7. If you want to merge the weights together, use this script:\n```\n./llama.cpp/llama-gguf-split --merge \\\n    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    merged_file.gguf\n```\n\n| MoE Bits     | Type   | Disk Size |  Accuracy | Link                      | Details   |\n| -------- | -------- | ------------ | ------------ | ---------------------|  ---------- |\n| 1.58bit | UD-IQ1_S |   **131GB**    | Fair           | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S) | MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit |\n| 1.73bit | UD-IQ1_M |   **158GB**    | Good | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M) | MoE all 1.56bit. `down_proj` in MoE left at 2.06bit |\n| 2.22bit | UD-IQ2_XXS |   **183GB**    | Better      | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS) | MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit |\n| 2.51bit | UD-Q2_K_XL |   **212GB**    | Best | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL) | MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit |\n\n# Finetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/unsloth)\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n\n## âœ¨ Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Phi-4 (14B)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2 VL (7B)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |\n| **Qwen2.5 (7B)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Llama-3.1 (8B)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [â–¶ï¸ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"200\"/>](https://docs.unsloth.ai)\n\n- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n\n## Special Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\n\n\n\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ðŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>ðŸ‘ï¸</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":9778194944831,\"files_count\":118,\"spaces_count\":2,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV3ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_deepseek.DeepseekV3Config\",\"AutoModel\":\"modeling_deepseek.DeepseekV3Model\",\"AutoModelForCausalLM\":\"modeling_deepseek.DeepseekV3ForCausalLM\"},\"model_type\":\"deepseek_v3\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:unslothai:unsloth\",\"source_url\":\"https://github.com/unslothai/unsloth\"},{\"type\":\"has_code\",\"target_id\":\"github:unslothai:unsloth\",\"source_url\":\"https://github.com/unslothai/unsloth\"},{\"type\":\"has_code\",\"target_id\":\"github:unslothai:unsloth\",\"source_url\":\"https://github.com/unslothai/unsloth\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp.\",\"source_url\":\"https://github.com/ggerganov/llama.cpp.\"},{\"type\":\"has_code\",\"target_id\":\"github:ggerganov:llama.cpp\",\"source_url\":\"https://github.com/ggerganov/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:unslothai:unsloth\",\"source_url\":\"https://github.com/unslothai/unsloth\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.12948\",\"source_url\":\"https://arxiv.org/abs/2501.12948\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "fabd0b2ea62dcfc83449eb1780b1a26d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tiiuae:falcon-7b",
    "name": "falcon-7b",
    "author": "tiiuae",
    "description": "--- datasets: - tiiuae/falcon-refinedweb language: - en inference: false license: apache-2.0 new_version: tiiuae/falcon-11B --- **Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.** *Paper coming soon* ðŸ˜Š. ðŸ¤— To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! * **It outperforms comp...",
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "falcon",
      "text-generation",
      "custom_code",
      "en",
      "dataset:tiiuae/falcon-refinedweb",
      "arxiv:2205.14135",
      "arxiv:1911.02150",
      "arxiv:2101.00027",
      "arxiv:2005.14165",
      "arxiv:2104.09864",
      "arxiv:2306.01116",
      "license:apache-2.0",
      "text-generation-inference",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1096,
    "downloads": 107416,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tiiuae/falcon-7b",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\ninference: false\nlicense: apache-2.0\nnew_version: tiiuae/falcon-11B\n---\n\n# ðŸš€ Falcon-7B\n\n**Falcon-7B is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon* ðŸ˜Š.\n\nðŸ¤— To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n\n## Why use Falcon-7B?\n\n* **It outperforms comparable open-source models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n* **It is made available under a permissive Apache 2.0 license allowing for commercial use**, without any royalties or restrictions.\n\nâš ï¸ **This is a raw, pretrained model, which should be further finetuned for most usecases.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct). \n\nðŸ”¥ **Looking for an even more powerful model?** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) is Falcon-7B's big brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\nðŸ’¥ **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 16GB of memory** to swiftly run inference with Falcon-7B.\n\n# Model Card for Falcon-7B\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish);\n- **License:** Apache 2.0.\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nResearch on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-7B is trained on English and French data only, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-7B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-7B was trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile ([Gao et al., 2020](https://arxiv.org/abs/2101.00027)). \n\n| **Data source**    | **Fraction** | **Tokens** | **Sources**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 79%          | 1,185B     | massive web crawl                 |\n| Books              | 7%           | 110B       |                                   |\n| Conversations      | 6%           | 85B        | Reddit, StackOverflow, HackerNews |\n| Code               | 3%           | 45B        |                                   |\n| RefinedWeb-French  | 3%           | 45B        | massive web crawl                 |\n| Technical          | 2%           | 30B        | arXiv, PubMed, USPTO, etc.        |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n### Training Procedure \n\nFalcon-7B was trained on 384 A100 40GB GPUs, using a 2D parallelism strategy (PP=2, DP=192) combined with ZeRO.\n\n#### Training Hyperparameters\n\n| **Hyperparameter** | **Value**  | **Comment**                               |\n|--------------------|------------|-------------------------------------------|\n| Precision          | `bfloat16` |                                           |\n| Optimizer          | AdamW      |                                           |\n| Learning rate      | 6e-4       | 4B tokens warm-up, cosine decay to 1.2e-5 |\n| Weight decay       | 1e-1       |                                           |\n| Z-loss       | 1e-4       |                                           |\n| Batch size         | 2304        | 30B tokens ramp-up                         |\n\n\n#### Speeds, Sizes, Times\n\nTraining happened in early March 2023 and took about two weeks. \n\n\n## Evaluation\n\n*Paper coming soon*.\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\n### Model Architecture and Objective\n\nFalcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 32        |                                        |\n| `d_model`          | 4544      | Increased to compensate for multiquery                                       |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-7B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-7B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* ðŸ˜Š. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the ðŸ““ [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n## License\n\nFalcon-7B is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7217189760,\"storage_bytes\":43359994411,\"files_count\":15,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"FalconForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_falcon.FalconConfig\",\"AutoModel\":\"modeling_falcon.FalconModel\",\"AutoModelForSequenceClassification\":\"modeling_falcon.FalconForSequenceClassification\",\"AutoModelForTokenClassification\":\"modeling_falcon.FalconForTokenClassification\",\"AutoModelForQuestionAnswering\":\"modeling_falcon.FalconForQuestionAnswering\",\"AutoModelForCausalLM\":\"modeling_falcon.FalconForCausalLM\"},\"model_type\":\"falcon\",\"tokenizer_config\":{\"eos_token\":\"<|endoftext|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:StableLM\",\"source_url\":\"https://github.com/Stability-AI/StableLM\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:text-generation-inference\",\"source_url\":\"https://github.com/huggingface/text-generation-inference\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.14135\",\"source_url\":\"https://arxiv.org/abs/2205.14135\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.02150\",\"source_url\":\"https://arxiv.org/abs/1911.02150\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2101.00027\",\"source_url\":\"https://arxiv.org/abs/2101.00027\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2005.14165\",\"source_url\":\"https://arxiv.org/abs/2005.14165\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.09864\",\"source_url\":\"https://arxiv.org/abs/2104.09864\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.01116\",\"source_url\":\"https://arxiv.org/abs/2306.01116\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c7e5b5d538aef542c7f330333ba6a980",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tiiuae/falcon-7b\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:intfloat:multilingual-e5-large",
    "name": "multilingual-e5-large",
    "author": "intfloat",
    "description": "--- tags: - mteb - Sentence Transformers - sentence-similarity - feature-extraction - sentence-transformers model-index: - name: multilingual-e5-large results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 79.05970149253731 - type: ap value: 43.486574390835635 - type: f1 value: 73.32700092140148 - task: type: Cla...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "openvino",
      "xlm-roberta",
      "mteb",
      "sentence transformers",
      "sentence-similarity",
      "feature-extraction",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "arxiv:2402.05672",
      "arxiv:2108.08787",
      "arxiv:2104.08663",
      "arxiv:2210.07316",
      "license:mit",
      "model-index",
      "text-embeddings-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "feature-extraction",
    "likes": 1095,
    "downloads": 3338259,
    "source": "huggingface",
    "source_url": "https://huggingface.co/intfloat/multilingual-e5-large",
    "image_url": null,
    "type": "model",
    "body_content": "---\ntags:\n- mteb\n- Sentence Transformers\n- sentence-similarity\n- feature-extraction\n- sentence-transformers\nmodel-index:\n- name: multilingual-e5-large\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 79.05970149253731\n    - type: ap\n      value: 43.486574390835635\n    - type: f1\n      value: 73.32700092140148\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 71.22055674518201\n    - type: ap\n      value: 81.55756710830498\n    - type: f1\n      value: 69.28271787752661\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 80.41979010494754\n    - type: ap\n      value: 29.34879922376344\n    - type: f1\n      value: 67.62475449011278\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.8372591006424\n    - type: ap\n      value: 26.557560591210738\n    - type: f1\n      value: 64.96619417368707\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 93.489875\n    - type: ap\n      value: 90.98758636917603\n    - type: f1\n      value: 93.48554819717332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.564\n    - type: f1\n      value: 46.75122173518047\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 45.400000000000006\n    - type: f1\n      value: 44.17195682400632\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 43.068\n    - type: f1\n      value: 42.38155696855596\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 41.89\n    - type: f1\n      value: 40.84407321682663\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 40.120000000000005\n    - type: f1\n      value: 39.522976223819114\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 38.832\n    - type: f1\n      value: 38.0392533394713\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.725\n    - type: map_at_10\n      value: 46.055\n    - type: map_at_100\n      value: 46.900999999999996\n    - type: map_at_1000\n      value: 46.911\n    - type: map_at_3\n      value: 41.548\n    - type: map_at_5\n      value: 44.297\n    - type: mrr_at_1\n      value: 31.152\n    - type: mrr_at_10\n      value: 46.231\n    - type: mrr_at_100\n      value: 47.07\n    - type: mrr_at_1000\n      value: 47.08\n    - type: mrr_at_3\n      value: 41.738\n    - type: mrr_at_5\n      value: 44.468999999999994\n    - type: ndcg_at_1\n      value: 30.725\n    - type: ndcg_at_10\n      value: 54.379999999999995\n    - type: ndcg_at_100\n      value: 58.138\n    - type: ndcg_at_1000\n      value: 58.389\n    - type: ndcg_at_3\n      value: 45.156\n    - type: ndcg_at_5\n      value: 50.123\n    - type: precision_at_1\n      value: 30.725\n    - type: precision_at_10\n      value: 8.087\n    - type: precision_at_100\n      value: 0.9769999999999999\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 18.54\n    - type: precision_at_5\n      value: 13.542000000000002\n    - type: recall_at_1\n      value: 30.725\n    - type: recall_at_10\n      value: 80.868\n    - type: recall_at_100\n      value: 97.653\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_3\n      value: 55.619\n    - type: recall_at_5\n      value: 67.71000000000001\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 44.30960650674069\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 38.427074197498996\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 60.28270056031872\n    - type: mrr\n      value: 74.38332673789738\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.05942144105269\n    - type: cos_sim_spearman\n      value: 82.51212105850809\n    - type: euclidean_pearson\n      value: 81.95639829909122\n    - type: euclidean_spearman\n      value: 82.3717564144213\n    - type: manhattan_pearson\n      value: 81.79273425468256\n    - type: manhattan_spearman\n      value: 82.20066817871039\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.46764091858039\n    - type: f1\n      value: 99.37717466945023\n    - type: precision\n      value: 99.33194154488518\n    - type: recall\n      value: 99.46764091858039\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.29407880255337\n    - type: f1\n      value: 98.11248073959938\n    - type: precision\n      value: 98.02443319392472\n    - type: recall\n      value: 98.29407880255337\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 97.79009352268791\n    - type: f1\n      value: 97.5176076665512\n    - type: precision\n      value: 97.38136473848286\n    - type: recall\n      value: 97.79009352268791\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26276987888363\n    - type: f1\n      value: 99.20133403545726\n    - type: precision\n      value: 99.17500438827453\n    - type: recall\n      value: 99.26276987888363\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 84.72727272727273\n    - type: f1\n      value: 84.67672206031433\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 35.34220182511161\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 33.4987096128766\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.558249999999997\n    - type: map_at_10\n      value: 34.44425000000001\n    - type: map_at_100\n      value: 35.59833333333333\n    - type: map_at_1000\n      value: 35.706916666666665\n    - type: map_at_3\n      value: 31.691749999999995\n    - type: map_at_5\n      value: 33.252916666666664\n    - type: mrr_at_1\n      value: 30.252666666666666\n    - type: mrr_at_10\n      value: 38.60675\n    - type: mrr_at_100\n      value: 39.42666666666666\n    - type: mrr_at_1000\n      value: 39.48408333333334\n    - type: mrr_at_3\n      value: 36.17441666666665\n    - type: mrr_at_5\n      value: 37.56275\n    - type: ndcg_at_1\n      value: 30.252666666666666\n    - type: ndcg_at_10\n      value: 39.683\n    - type: ndcg_at_100\n      value: 44.68541666666667\n    - type: ndcg_at_1000\n      value: 46.94316666666668\n    - type: ndcg_at_3\n      value: 34.961749999999995\n    - type: ndcg_at_5\n      value: 37.215666666666664\n    - type: precision_at_1\n      value: 30.252666666666666\n    - type: precision_at_10\n      value: 6.904166666666667\n    - type: precision_at_100\n      value: 1.0989999999999995\n    - type: precision_at_1000\n      value: 0.14733333333333334\n    - type: precision_at_3\n      value: 16.037666666666667\n    - type: precision_at_5\n      value: 11.413583333333333\n    - type: recall_at_1\n      value: 25.558249999999997\n    - type: recall_at_10\n      value: 51.13341666666666\n    - type: recall_at_100\n      value: 73.08366666666667\n    - type: recall_at_1000\n      value: 88.79483333333334\n    - type: recall_at_3\n      value: 37.989083333333326\n    - type: recall_at_5\n      value: 43.787833333333325\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.338\n    - type: map_at_10\n      value: 18.360000000000003\n    - type: map_at_100\n      value: 19.942\n    - type: map_at_1000\n      value: 20.134\n    - type: map_at_3\n      value: 15.174000000000001\n    - type: map_at_5\n      value: 16.830000000000002\n    - type: mrr_at_1\n      value: 23.257\n    - type: mrr_at_10\n      value: 33.768\n    - type: mrr_at_100\n      value: 34.707\n    - type: mrr_at_1000\n      value: 34.766000000000005\n    - type: mrr_at_3\n      value: 30.977\n    - type: mrr_at_5\n      value: 32.528\n    - type: ndcg_at_1\n      value: 23.257\n    - type: ndcg_at_10\n      value: 25.733\n    - type: ndcg_at_100\n      value: 32.288\n    - type: ndcg_at_1000\n      value: 35.992000000000004\n    - type: ndcg_at_3\n      value: 20.866\n    - type: ndcg_at_5\n      value: 22.612\n    - type: precision_at_1\n      value: 23.257\n    - type: precision_at_10\n      value: 8.124\n    - type: precision_at_100\n      value: 1.518\n    - type: precision_at_1000\n      value: 0.219\n    - type: precision_at_3\n      value: 15.679000000000002\n    - type: precision_at_5\n      value: 12.117\n    - type: recall_at_1\n      value: 10.338\n    - type: recall_at_10\n      value: 31.154\n    - type: recall_at_100\n      value: 54.161\n    - type: recall_at_1000\n      value: 75.21900000000001\n    - type: recall_at_3\n      value: 19.427\n    - type: recall_at_5\n      value: 24.214\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.498\n    - type: map_at_10\n      value: 19.103\n    - type: map_at_100\n      value: 27.375\n    - type: map_at_1000\n      value: 28.981\n    - type: map_at_3\n      value: 13.764999999999999\n    - type: map_at_5\n      value: 15.950000000000001\n    - type: mrr_at_1\n      value: 65.5\n    - type: mrr_at_10\n      value: 74.53800000000001\n    - type: mrr_at_100\n      value: 74.71799999999999\n    - type: mrr_at_1000\n      value: 74.725\n    - type: mrr_at_3\n      value: 72.792\n    - type: mrr_at_5\n      value: 73.554\n    - type: ndcg_at_1\n      value: 53.37499999999999\n    - type: ndcg_at_10\n      value: 41.286\n    - type: ndcg_at_100\n      value: 45.972\n    - type: ndcg_at_1000\n      value: 53.123\n    - type: ndcg_at_3\n      value: 46.172999999999995\n    - type: ndcg_at_5\n      value: 43.033\n    - type: precision_at_1\n      value: 65.5\n    - type: precision_at_10\n      value: 32.725\n    - type: precision_at_100\n      value: 10.683\n    - type: precision_at_1000\n      value: 1.978\n    - type: precision_at_3\n      value: 50\n    - type: precision_at_5\n      value: 41.349999999999994\n    - type: recall_at_1\n      value: 8.498\n    - type: recall_at_10\n      value: 25.070999999999998\n    - type: recall_at_100\n      value: 52.383\n    - type: recall_at_1000\n      value: 74.91499999999999\n    - type: recall_at_3\n      value: 15.207999999999998\n    - type: recall_at_5\n      value: 18.563\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 46.5\n    - type: f1\n      value: 41.93833713984145\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 67.914\n    - type: map_at_10\n      value: 78.10000000000001\n    - type: map_at_100\n      value: 78.333\n    - type: map_at_1000\n      value: 78.346\n    - type: map_at_3\n      value: 76.626\n    - type: map_at_5\n      value: 77.627\n    - type: mrr_at_1\n      value: 72.74199999999999\n    - type: mrr_at_10\n      value: 82.414\n    - type: mrr_at_100\n      value: 82.511\n    - type: mrr_at_1000\n      value: 82.513\n    - type: mrr_at_3\n      value: 81.231\n    - type: mrr_at_5\n      value: 82.065\n    - type: ndcg_at_1\n      value: 72.74199999999999\n    - type: ndcg_at_10\n      value: 82.806\n    - type: ndcg_at_100\n      value: 83.677\n    - type: ndcg_at_1000\n      value: 83.917\n    - type: ndcg_at_3\n      value: 80.305\n    - type: ndcg_at_5\n      value: 81.843\n    - type: precision_at_1\n      value: 72.74199999999999\n    - type: precision_at_10\n      value: 10.24\n    - type: precision_at_100\n      value: 1.089\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 31.268\n    - type: precision_at_5\n      value: 19.706000000000003\n    - type: recall_at_1\n      value: 67.914\n    - type: recall_at_10\n      value: 92.889\n    - type: recall_at_100\n      value: 96.42699999999999\n    - type: recall_at_1000\n      value: 97.92\n    - type: recall_at_3\n      value: 86.21\n    - type: recall_at_5\n      value: 90.036\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.166\n    - type: map_at_10\n      value: 35.57\n    - type: map_at_100\n      value: 37.405\n    - type: map_at_1000\n      value: 37.564\n    - type: map_at_3\n      value: 30.379\n    - type: map_at_5\n      value: 33.324\n    - type: mrr_at_1\n      value: 43.519000000000005\n    - type: mrr_at_10\n      value: 51.556000000000004\n    - type: mrr_at_100\n      value: 52.344\n    - type: mrr_at_1000\n      value: 52.373999999999995\n    - type: mrr_at_3\n      value: 48.868\n    - type: mrr_at_5\n      value: 50.319\n    - type: ndcg_at_1\n      value: 43.519000000000005\n    - type: ndcg_at_10\n      value: 43.803\n    - type: ndcg_at_100\n      value: 50.468999999999994\n    - type: ndcg_at_1000\n      value: 53.111\n    - type: ndcg_at_3\n      value: 38.893\n    - type: ndcg_at_5\n      value: 40.653\n    - type: precision_at_1\n      value: 43.519000000000005\n    - type: precision_at_10\n      value: 12.253\n    - type: precision_at_100\n      value: 1.931\n    - type: precision_at_1000\n      value: 0.242\n    - type: precision_at_3\n      value: 25.617\n    - type: precision_at_5\n      value: 19.383\n    - type: recall_at_1\n      value: 22.166\n    - type: recall_at_10\n      value: 51.6\n    - type: recall_at_100\n      value: 76.574\n    - type: recall_at_1000\n      value: 92.192\n    - type: recall_at_3\n      value: 34.477999999999994\n    - type: recall_at_5\n      value: 41.835\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.041\n    - type: map_at_10\n      value: 62.961999999999996\n    - type: map_at_100\n      value: 63.79899999999999\n    - type: map_at_1000\n      value: 63.854\n    - type: map_at_3\n      value: 59.399\n    - type: map_at_5\n      value: 61.669\n    - type: mrr_at_1\n      value: 78.082\n    - type: mrr_at_10\n      value: 84.321\n    - type: mrr_at_100\n      value: 84.49600000000001\n    - type: mrr_at_1000\n      value: 84.502\n    - type: mrr_at_3\n      value: 83.421\n    - type: mrr_at_5\n      value: 83.977\n    - type: ndcg_at_1\n      value: 78.082\n    - type: ndcg_at_10\n      value: 71.229\n    - type: ndcg_at_100\n      value: 74.10900000000001\n    - type: ndcg_at_1000\n      value: 75.169\n    - type: ndcg_at_3\n      value: 66.28699999999999\n    - type: ndcg_at_5\n      value: 69.084\n    - type: precision_at_1\n      value: 78.082\n    - type: precision_at_10\n      value: 14.993\n    - type: precision_at_100\n      value: 1.7239999999999998\n    - type: precision_at_1000\n      value: 0.186\n    - type: precision_at_3\n      value: 42.737\n    - type: precision_at_5\n      value: 27.843\n    - type: recall_at_1\n      value: 39.041\n    - type: recall_at_10\n      value: 74.96300000000001\n    - type: recall_at_100\n      value: 86.199\n    - type: recall_at_1000\n      value: 93.228\n    - type: recall_at_3\n      value: 64.105\n    - type: recall_at_5\n      value: 69.608\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 90.23160000000001\n    - type: ap\n      value: 85.5674856808308\n    - type: f1\n      value: 90.18033354786317\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.091\n    - type: map_at_10\n      value: 36.753\n    - type: map_at_100\n      value: 37.913000000000004\n    - type: map_at_1000\n      value: 37.958999999999996\n    - type: map_at_3\n      value: 32.818999999999996\n    - type: map_at_5\n      value: 35.171\n    - type: mrr_at_1\n      value: 24.742\n    - type: mrr_at_10\n      value: 37.285000000000004\n    - type: mrr_at_100\n      value: 38.391999999999996\n    - type: mrr_at_1000\n      value: 38.431\n    - type: mrr_at_3\n      value: 33.440999999999995\n    - type: mrr_at_5\n      value: 35.75\n    - type: ndcg_at_1\n      value: 24.742\n    - type: ndcg_at_10\n      value: 43.698\n    - type: ndcg_at_100\n      value: 49.145\n    - type: ndcg_at_1000\n      value: 50.23800000000001\n    - type: ndcg_at_3\n      value: 35.769\n    - type: ndcg_at_5\n      value: 39.961999999999996\n    - type: precision_at_1\n      value: 24.742\n    - type: precision_at_10\n      value: 6.7989999999999995\n    - type: precision_at_100\n      value: 0.95\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 15.096000000000002\n    - type: precision_at_5\n      value: 11.183\n    - type: recall_at_1\n      value: 24.091\n    - type: recall_at_10\n      value: 65.068\n    - type: recall_at_100\n      value: 89.899\n    - type: recall_at_1000\n      value: 98.16\n    - type: recall_at_3\n      value: 43.68\n    - type: recall_at_5\n      value: 53.754999999999995\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.66621067031465\n    - type: f1\n      value: 93.49622853272142\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 91.94702733164272\n    - type: f1\n      value: 91.17043441745282\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.20146764509674\n    - type: f1\n      value: 91.98359080555608\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.99780770435328\n    - type: f1\n      value: 89.19746342724068\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 89.78486912871998\n    - type: f1\n      value: 89.24578823628642\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.74502712477394\n    - type: f1\n      value: 89.00297573881542\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.9046967624259\n    - type: f1\n      value: 59.36787125785957\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 74.5280360664976\n    - type: f1\n      value: 57.17723440888718\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 75.44029352901934\n    - type: f1\n      value: 54.052855531072964\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 70.5606013153774\n    - type: f1\n      value: 52.62215934386531\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 73.11581211903908\n    - type: f1\n      value: 52.341291845645465\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 74.28933092224233\n    - type: f1\n      value: 57.07918745504911\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.38063214525892\n    - type: f1\n      value: 59.46463723443009\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 56.06926698049766\n    - type: f1\n      value: 52.49084283283562\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.74983187626093\n    - type: f1\n      value: 56.960640620165904\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.86550100874243\n    - type: f1\n      value: 62.47370548140688\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.971082716879636\n    - type: f1\n      value: 61.03812421957381\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 54.98318762609282\n    - type: f1\n      value: 51.51207916008392\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.45527908540686\n    - type: f1\n      value: 66.16631905400318\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.32750504371216\n    - type: f1\n      value: 66.16755288646591\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.09213180901143\n    - type: f1\n      value: 66.95654394661507\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.75588433086752\n    - type: f1\n      value: 71.79973779656923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.49428379287154\n    - type: f1\n      value: 68.37494379215734\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.90921318090115\n    - type: f1\n      value: 66.79517376481645\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.12104909213181\n    - type: f1\n      value: 67.29448842879584\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.34095494283793\n    - type: f1\n      value: 67.01134288992947\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.61264290517822\n    - type: f1\n      value: 64.68730512660757\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.79757901815738\n    - type: f1\n      value: 65.24938539425598\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.68728984532616\n    - type: f1\n      value: 67.0487169762553\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.07464694014795\n    - type: f1\n      value: 59.183532276789286\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.04707464694015\n    - type: f1\n      value: 67.66829629003848\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.42434431741762\n    - type: f1\n      value: 59.01617226544757\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.53127101546738\n    - type: f1\n      value: 68.10033760906255\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.50504371217215\n    - type: f1\n      value: 69.74931103158923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.91190316072628\n    - type: f1\n      value: 54.05551136648796\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.78211163416275\n    - type: f1\n      value: 49.874888544058535\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 47.017484868863484\n    - type: f1\n      value: 44.53364263352014\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.16207128446537\n    - type: f1\n      value: 59.01185692320829\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.42501681237391\n    - type: f1\n      value: 67.13169450166086\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.0780094149294\n    - type: f1\n      value: 64.41720167850707\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.57162071284466\n    - type: f1\n      value: 62.414138683804424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 61.71149966375252\n    - type: f1\n      value: 58.594805125087234\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.03900470746471\n    - type: f1\n      value: 63.87937257883887\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.8776059179556\n    - type: f1\n      value: 57.48587618059131\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.87895090786819\n    - type: f1\n      value: 66.8141299430347\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.45057162071285\n    - type: f1\n      value: 67.46444039673516\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.546738399462\n    - type: f1\n      value: 68.63640876702655\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.72965702757229\n    - type: f1\n      value: 68.54119560379115\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.35574983187625\n    - type: f1\n      value: 65.88844917691927\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.70477471418964\n    - type: f1\n      value: 69.19665697061978\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.0880968392737\n    - type: f1\n      value: 64.76962317666086\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.18493611297916\n    - type: f1\n      value: 62.49984559035371\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.75857431069265\n    - type: f1\n      value: 69.20053687623418\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.500336247478145\n    - type: f1\n      value: 55.2972398687929\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.68997982515132\n    - type: f1\n      value: 59.36848202755348\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.01950235373235\n    - type: f1\n      value: 60.09351954625423\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.29186281102892\n    - type: f1\n      value: 67.57860496703447\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.77471418964357\n    - type: f1\n      value: 61.913983147713836\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.87222595830532\n    - type: f1\n      value: 66.03679033708141\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.04505716207127\n    - type: f1\n      value: 61.28569169817908\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.38466711499663\n    - type: f1\n      value: 67.20532357036844\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.12306657700067\n    - type: f1\n      value: 68.91251226588182\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.20040349697378\n    - type: f1\n      value: 66.02657347714175\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.73907195696032\n    - type: f1\n      value: 66.98484521791418\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 60.58843308675185\n    - type: f1\n      value: 58.95591723092005\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.22730329522528\n    - type: f1\n      value: 66.0894499712115\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.48285137861465\n    - type: f1\n      value: 65.21963176785157\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.74714189643578\n    - type: f1\n      value: 66.8212192745412\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 59.09213180901143\n    - type: f1\n      value: 56.70735546356339\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.05716207128448\n    - type: f1\n      value: 74.8413712365364\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.69737726967047\n    - type: f1\n      value: 74.7664341963\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.90383322125084\n    - type: f1\n      value: 73.59201554448323\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.51176866173503\n    - type: f1\n      value: 77.46104434577758\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.31069266980496\n    - type: f1\n      value: 74.61048660675635\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.95225285810356\n    - type: f1\n      value: 72.33160006574627\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.12373907195696\n    - type: f1\n      value: 73.20921012557481\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.86684599865501\n    - type: f1\n      value: 73.82348774610831\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.40215198386012\n    - type: f1\n      value: 71.11945183971858\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.12844653665098\n    - type: f1\n      value: 71.34450495911766\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.52252858103566\n    - type: f1\n      value: 73.98878711342999\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.93611297915265\n    - type: f1\n      value: 63.723200467653385\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.11903160726295\n    - type: f1\n      value: 73.82138439467096\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.15198386012105\n    - type: f1\n      value: 66.02172193802167\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.32414256893072\n    - type: f1\n      value: 74.30943421170574\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.46805648957633\n    - type: f1\n      value: 77.62808409298209\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.318762609280434\n    - type: f1\n      value: 62.094284066075076\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 58.34902488231338\n    - type: f1\n      value: 57.12893860987984\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 50.88433086751849\n    - type: f1\n      value: 48.2272350802058\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.4425016812374\n    - type: f1\n      value: 64.61463095996173\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.04707464694015\n    - type: f1\n      value: 75.05099199098998\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.50437121721586\n    - type: f1\n      value: 69.83397721096314\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.94283792871553\n    - type: f1\n      value: 68.8704663703913\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.79488903833222\n    - type: f1\n      value: 63.615424063345436\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.88231338264963\n    - type: f1\n      value: 68.57892302593237\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.248150638870214\n    - type: f1\n      value: 61.06680605338809\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.84196368527236\n    - type: f1\n      value: 74.52566464968763\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.8285137861466\n    - type: f1\n      value: 74.8853197608802\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.13248150638869\n    - type: f1\n      value: 74.3982040999179\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.49024882313383\n    - type: f1\n      value: 73.82153848368573\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.72158708809684\n    - type: f1\n      value: 71.85049433180541\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.137861466039\n    - type: f1\n      value: 75.37628348188467\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.86953597848016\n    - type: f1\n      value: 71.87537624521661\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.27572293207801\n    - type: f1\n      value: 68.80017302344231\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.09952925353059\n    - type: f1\n      value: 76.07992707688408\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.140551445864155\n    - type: f1\n      value: 61.73855010331415\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.27774041694687\n    - type: f1\n      value: 64.83664868894539\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.69468728984533\n    - type: f1\n      value: 64.76239666920868\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.44653665097512\n    - type: f1\n      value: 73.14646052013873\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.71351714862139\n    - type: f1\n      value: 66.67212180163382\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.9946200403497\n    - type: f1\n      value: 73.87348793725525\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.15400134498992\n    - type: f1\n      value: 67.09433241421094\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.11365164761264\n    - type: f1\n      value: 73.59502539433753\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.82582380632145\n    - type: f1\n      value: 76.89992945316313\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.81237390719569\n    - type: f1\n      value: 72.36499770986265\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 31.480506569594695\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 29.71252128004552\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.421396787056548\n    - type: mrr\n      value: 32.48155274872267\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.595\n    - type: map_at_10\n      value: 12.642000000000001\n    - type: map_at_100\n      value: 15.726\n    - type: map_at_1000\n      value: 17.061999999999998\n    - type: map_at_3\n      value: 9.125\n    - type: map_at_5\n      value: 10.866000000000001\n    - type: mrr_at_1\n      value: 43.344\n    - type: mrr_at_10\n      value: 52.227999999999994\n    - type: mrr_at_100\n      value: 52.898999999999994\n    - type: mrr_at_1000\n      value: 52.944\n    - type: mrr_at_3\n      value: 49.845\n    - type: mrr_at_5\n      value: 51.115\n    - type: ndcg_at_1\n      value: 41.949999999999996\n    - type: ndcg_at_10\n      value: 33.995\n    - type: ndcg_at_100\n      value: 30.869999999999997\n    - type: ndcg_at_1000\n      value: 39.487\n    - type: ndcg_at_3\n      value: 38.903999999999996\n    - type: ndcg_at_5\n      value: 37.236999999999995\n    - type: precision_at_1\n      value: 43.344\n    - type: precision_at_10\n      value: 25.480000000000004\n    - type: precision_at_100\n      value: 7.672\n    - type: precision_at_1000\n      value: 2.028\n    - type: precision_at_3\n      value: 36.636\n    - type: precision_at_5\n      value: 32.632\n    - type: recall_at_1\n      value: 5.595\n    - type: recall_at_10\n      value: 16.466\n    - type: recall_at_100\n      value: 31.226\n    - type: recall_at_1000\n      value: 62.778999999999996\n    - type: recall_at_3\n      value: 9.931\n    - type: recall_at_5\n      value: 12.884\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.414\n    - type: map_at_10\n      value: 56.754000000000005\n    - type: map_at_100\n      value: 57.457\n    - type: map_at_1000\n      value: 57.477999999999994\n    - type: map_at_3\n      value: 52.873999999999995\n    - type: map_at_5\n      value: 55.175\n    - type: mrr_at_1\n      value: 45.278\n    - type: mrr_at_10\n      value: 59.192\n    - type: mrr_at_100\n      value: 59.650000000000006\n    - type: mrr_at_1000\n      value: 59.665\n    - type: mrr_at_3\n      value: 56.141\n    - type: mrr_at_5\n      value: 57.998000000000005\n    - type: ndcg_at_1\n      value: 45.278\n    - type: ndcg_at_10\n      value: 64.056\n    - type: ndcg_at_100\n      value: 66.89\n    - type: ndcg_at_1000\n      value: 67.364\n    - type: ndcg_at_3\n      value: 56.97\n    - type: ndcg_at_5\n      value: 60.719\n    - type: precision_at_1\n      value: 45.278\n    - type: precision_at_10\n      value: 9.994\n    - type: precision_at_100\n      value: 1.165\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 25.512\n    - type: precision_at_5\n      value: 17.509\n    - type: recall_at_1\n      value: 40.414\n    - type: recall_at_10\n      value: 83.596\n    - type: recall_at_100\n      value: 95.72\n    - type: recall_at_1000\n      value: 99.24\n    - type: recall_at_3\n      value: 65.472\n    - type: recall_at_5\n      value: 74.039\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.352\n    - type: map_at_10\n      value: 84.369\n    - type: map_at_100\n      value: 85.02499999999999\n    - type: map_at_1000\n      value: 85.04\n    - type: map_at_3\n      value: 81.42399999999999\n    - type: map_at_5\n      value: 83.279\n    - type: mrr_at_1\n      value: 81.05\n    - type: mrr_at_10\n      value: 87.401\n    - type: mrr_at_100\n      value: 87.504\n    - type: mrr_at_1000\n      value: 87.505\n    - type: mrr_at_3\n      value: 86.443\n    - type: mrr_at_5\n      value: 87.10799999999999\n    - type: ndcg_at_1\n      value: 81.04\n    - type: ndcg_at_10\n      value: 88.181\n    - type: ndcg_at_100\n      value: 89.411\n    - type: ndcg_at_1000\n      value: 89.507\n    - type: ndcg_at_3\n      value: 85.28099999999999\n    - type: ndcg_at_5\n      value: 86.888\n    - type: precision_at_1\n      value: 81.04\n    - type: precision_at_10\n      value: 13.406\n    - type: precision_at_100\n      value: 1.5350000000000001\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.31\n    - type: precision_at_5\n      value: 24.54\n    - type: recall_at_1\n      value: 70.352\n    - type: recall_at_10\n      value: 95.358\n    - type: recall_at_100\n      value: 99.541\n    - type: recall_at_1000\n      value: 99.984\n    - type: recall_at_3\n      value: 87.111\n    - type: recall_at_5\n      value: 91.643\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 46.54068723291946\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 63.216287629895994\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.023000000000001\n    - type: map_at_10\n      value: 10.071\n    - type: map_at_100\n      value: 11.892\n    - type: map_at_1000\n      value: 12.196\n    - type: map_at_3\n      value: 7.234\n    - type: map_at_5\n      value: 8.613999999999999\n    - type: mrr_at_1\n      value: 19.900000000000002\n    - type: mrr_at_10\n      value: 30.516\n    - type: mrr_at_100\n      value: 31.656000000000002\n    - type: mrr_at_1000\n      value: 31.723000000000003\n    - type: mrr_at_3\n      value: 27.400000000000002\n    - type: mrr_at_5\n      value: 29.270000000000003\n    - type: ndcg_at_1\n      value: 19.900000000000002\n    - type: ndcg_at_10\n      value: 17.474\n    - type: ndcg_at_100\n      value: 25.020999999999997\n    - type: ndcg_at_1000\n      value: 30.728\n    - type: ndcg_at_3\n      value: 16.588\n    - type: ndcg_at_5\n      value: 14.498\n    - type: precision_at_1\n      value: 19.900000000000002\n    - type: precision_at_10\n      value: 9.139999999999999\n    - type: precision_at_100\n      value: 2.011\n    - type: precision_at_1000\n      value: 0.33899999999999997\n    - type: precision_at_3\n      value: 15.667\n    - type: precision_at_5\n      value: 12.839999999999998\n    - type: recall_at_1\n      value: 4.023000000000001\n    - type: recall_at_10\n      value: 18.497\n    - type: recall_at_100\n      value: 40.8\n    - type: recall_at_1000\n      value: 68.812\n    - type: recall_at_3\n      value: 9.508\n    - type: recall_at_5\n      value: 12.983\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.967008785134\n    - type: cos_sim_spearman\n      value: 80.23142141101837\n    - type: euclidean_pearson\n      value: 81.20166064704539\n    - type: euclidean_spearman\n      value: 80.18961335654585\n    - type: manhattan_pearson\n      value: 81.13925443187625\n    - type: manhattan_spearman\n      value: 80.07948723044424\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.94262461316023\n    - type: cos_sim_spearman\n      value: 80.01596278563865\n    - type: euclidean_pearson\n      value: 83.80799622922581\n    - type: euclidean_spearman\n      value: 79.94984954947103\n    - type: manhattan_pearson\n      value: 83.68473841756281\n    - type: manhattan_spearman\n      value: 79.84990707951822\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 80.57346443146068\n    - type: cos_sim_spearman\n      value: 81.54689837570866\n    - type: euclidean_pearson\n      value: 81.10909881516007\n    - type: euclidean_spearman\n      value: 81.56746243261762\n    - type: manhattan_pearson\n      value: 80.87076036186582\n    - type: manhattan_spearman\n      value: 81.33074987964402\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 79.54733787179849\n    - type: cos_sim_spearman\n      value: 77.72202105610411\n    - type: euclidean_pearson\n      value: 78.9043595478849\n    - type: euclidean_spearman\n      value: 77.93422804309435\n    - type: manhattan_pearson\n      value: 78.58115121621368\n    - type: manhattan_spearman\n      value: 77.62508135122033\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.59880017237558\n    - type: cos_sim_spearman\n      value: 89.31088630824758\n    - type: euclidean_pearson\n      value: 88.47069261564656\n    - type: euclidean_spearman\n      value: 89.33581971465233\n    - type: manhattan_pearson\n      value: 88.40774264100956\n    - type: manhattan_spearman\n      value: 89.28657485627835\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.08055117917084\n    - type: cos_sim_spearman\n      value: 85.78491813080304\n    - type: euclidean_pearson\n      value: 84.99329155500392\n    - type: euclidean_spearman\n      value: 85.76728064677287\n    - type: manhattan_pearson\n      value: 84.87947428989587\n    - type: manhattan_spearman\n      value: 85.62429454917464\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ko-ko)\n      config: ko-ko\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 82.14190939287384\n    - type: cos_sim_spearman\n      value: 82.27331573306041\n    - type: euclidean_pearson\n      value: 81.891896953716\n    - type: euclidean_spearman\n      value: 82.37695542955998\n    - type: manhattan_pearson\n      value: 81.73123869460504\n    - type: manhattan_spearman\n      value: 82.19989168441421\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ar-ar)\n      config: ar-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 76.84695301843362\n    - type: cos_sim_spearman\n      value: 77.87790986014461\n    - type: euclidean_pearson\n      value: 76.91981583106315\n    - type: euclidean_spearman\n      value: 77.88154772749589\n    - type: manhattan_pearson\n      value: 76.94953277451093\n    - type: manhattan_spearman\n      value: 77.80499230728604\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-ar)\n      config: en-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 75.44657840482016\n    - type: cos_sim_spearman\n      value: 75.05531095119674\n    - type: euclidean_pearson\n      value: 75.88161755829299\n    - type: euclidean_spearman\n      value: 74.73176238219332\n    - type: manhattan_pearson\n      value: 75.63984765635362\n    - type: manhattan_spearman\n      value: 74.86476440770737\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-de)\n      config: en-de\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.64700140524133\n    - type: cos_sim_spearman\n      value: 86.16014210425672\n    - type: euclidean_pearson\n      value: 86.49086860843221\n    - type: euclidean_spearman\n      value: 86.09729326815614\n    - type: manhattan_pearson\n      value: 86.43406265125513\n    - type: manhattan_spearman\n      value: 86.17740150939994\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.91170098764921\n    - type: cos_sim_spearman\n      value: 88.12437004058931\n    - type: euclidean_pearson\n      value: 88.81828254494437\n    - type: euclidean_spearman\n      value: 88.14831794572122\n    - type: manhattan_pearson\n      value: 88.93442183448961\n    - type: manhattan_spearman\n      value: 88.15254630778304\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-tr)\n      config: en-tr\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 72.91390577997292\n    - type: cos_sim_spearman\n      value: 71.22979457536074\n    - type: euclidean_pearson\n      value: 74.40314008106749\n    - type: euclidean_spearman\n      value: 72.54972136083246\n    - type: manhattan_pearson\n      value: 73.85687539530218\n    - type: manhattan_spearman\n      value: 72.09500771742637\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-en)\n      config: es-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 80.9301067983089\n    - type: cos_sim_spearman\n      value: 80.74989828346473\n    - type: euclidean_pearson\n      value: 81.36781301814257\n    - type: euclidean_spearman\n      value: 80.9448819964426\n    - type: manhattan_pearson\n      value: 81.0351322685609\n    - type: manhattan_spearman\n      value: 80.70192121844177\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-es)\n      config: es-es\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.13820465980005\n    - type: cos_sim_spearman\n      value: 86.73532498758757\n    - type: euclidean_pearson\n      value: 87.21329451846637\n    - type: euclidean_spearman\n      value: 86.57863198601002\n    - type: manhattan_pearson\n      value: 87.06973713818554\n    - type: manhattan_spearman\n      value: 86.47534918791499\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (fr-en)\n      config: fr-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.48720108904415\n    - type: cos_sim_spearman\n      value: 85.62221757068387\n    - type: euclidean_pearson\n      value: 86.1010129512749\n    - type: euclidean_spearman\n      value: 85.86580966509942\n    - type: manhattan_pearson\n      value: 86.26800938808971\n    - type: manhattan_spearman\n      value: 85.88902721678429\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (it-en)\n      config: it-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.98021347333516\n    - type: cos_sim_spearman\n      value: 84.53806553803501\n    - type: euclidean_pearson\n      value: 84.61483347248364\n    - type: euclidean_spearman\n      value: 85.14191408011702\n    - type: manhattan_pearson\n      value: 84.75297588825967\n    - type: manhattan_spearman\n      value: 85.33176753669242\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (nl-en)\n      config: nl-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.51856644893233\n    - type: cos_sim_spearman\n      value: 85.27510748506413\n    - type: euclidean_pearson\n      value: 85.09886861540977\n    - type: euclidean_spearman\n      value: 85.62579245860887\n    - type: manhattan_pearson\n      value: 84.93017860464607\n    - type: manhattan_spearman\n      value: 85.5063988898453\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.581573200584195\n    - type: cos_sim_spearman\n      value: 63.05503590247928\n    - type: euclidean_pearson\n      value: 63.652564812602094\n    - type: euclidean_spearman\n      value: 62.64811520876156\n    - type: manhattan_pearson\n      value: 63.506842893061076\n    - type: manhattan_spearman\n      value: 62.51289573046917\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de)\n      config: de\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 48.2248801729127\n    - type: cos_sim_spearman\n      value: 56.5936604678561\n    - type: euclidean_pearson\n      value: 43.98149464089\n    - type: euclidean_spearman\n      value: 56.108561882423615\n    - type: manhattan_pearson\n      value: 43.86880305903564\n    - type: manhattan_spearman\n      value: 56.04671150510166\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es)\n      config: es\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.17564527009831\n    - type: cos_sim_spearman\n      value: 64.57978560979488\n    - type: euclidean_pearson\n      value: 58.8818330154583\n    - type: euclidean_spearman\n      value: 64.99214839071281\n    - type: manhattan_pearson\n      value: 58.72671436121381\n    - type: manhattan_spearman\n      value: 65.10713416616109\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl)\n      config: pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 26.772131864023297\n    - type: cos_sim_spearman\n      value: 34.68200792408681\n    - type: euclidean_pearson\n      value: 16.68082419005441\n    - type: euclidean_spearman\n      value: 34.83099932652166\n    - type: manhattan_pearson\n      value: 16.52605949659529\n    - type: manhattan_spearman\n      value: 34.82075801399475\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (tr)\n      config: tr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 54.42415189043831\n    - type: cos_sim_spearman\n      value: 63.54594264576758\n    - type: euclidean_pearson\n      value: 57.36577498297745\n    - type: euclidean_spearman\n      value: 63.111466379158074\n    - type: manhattan_pearson\n      value: 57.584543715873885\n    - type: manhattan_spearman\n      value: 63.22361054139183\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ar)\n      config: ar\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 47.55216762405518\n    - type: cos_sim_spearman\n      value: 56.98670142896412\n    - type: euclidean_pearson\n      value: 50.15318757562699\n    - type: euclidean_spearman\n      value: 56.524941926541906\n    - type: manhattan_pearson\n      value: 49.955618528674904\n    - type: manhattan_spearman\n      value: 56.37102209240117\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ru)\n      config: ru\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 49.20540980338571\n    - type: cos_sim_spearman\n      value: 59.9009453504406\n    - type: euclidean_pearson\n      value: 49.557749853620535\n    - type: euclidean_spearman\n      value: 59.76631621172456\n    - type: manhattan_pearson\n      value: 49.62340591181147\n    - type: manhattan_spearman\n      value: 59.94224880322436\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (zh)\n      config: zh\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 51.508169956576985\n    - type: cos_sim_spearman\n      value: 66.82461565306046\n    - type: euclidean_pearson\n      value: 56.2274426480083\n    - type: euclidean_spearman\n      value: 66.6775323848333\n    - type: manhattan_pearson\n      value: 55.98277796300661\n    - type: manhattan_spearman\n      value: 66.63669848497175\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (fr)\n      config: fr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 72.86478788045507\n    - type: cos_sim_spearman\n      value: 76.7946552053193\n    - type: euclidean_pearson\n      value: 75.01598530490269\n    - type: euclidean_spearman\n      value: 76.83618917858281\n    - type: manhattan_pearson\n      value: 74.68337628304332\n    - type: manhattan_spearman\n      value: 76.57480204017773\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-en)\n      config: de-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.922619099401984\n    - type: cos_sim_spearman\n      value: 56.599362477240774\n    - type: euclidean_pearson\n      value: 56.68307052369783\n    - type: euclidean_spearman\n      value: 54.28760436777401\n    - type: manhattan_pearson\n      value: 56.67763566500681\n    - type: manhattan_spearman\n      value: 53.94619541711359\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es-en)\n      config: es-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 66.74357206710913\n    - type: cos_sim_spearman\n      value: 72.5208244925311\n    - type: euclidean_pearson\n      value: 67.49254562186032\n    - type: euclidean_spearman\n      value: 72.02469076238683\n    - type: manhattan_pearson\n      value: 67.45251772238085\n    - type: manhattan_spearman\n      value: 72.05538819984538\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (it)\n      config: it\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 71.25734330033191\n    - type: cos_sim_spearman\n      value: 76.98349083946823\n    - type: euclidean_pearson\n      value: 73.71642838667736\n    - type: euclidean_spearman\n      value: 77.01715504651384\n    - type: manhattan_pearson\n      value: 73.61712711868105\n    - type: manhattan_spearman\n      value: 77.01392571153896\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl-en)\n      config: pl-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.18215462781212\n    - type: cos_sim_spearman\n      value: 65.54373266117607\n    - type: euclidean_pearson\n      value: 64.54126095439005\n    - type: euclidean_spearman\n      value: 65.30410369102711\n    - type: manhattan_pearson\n      value: 63.50332221148234\n    - type: manhattan_spearman\n      value: 64.3455878104313\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (zh-en)\n      config: zh-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.30509221440029\n    - type: cos_sim_spearman\n      value: 65.99582704642478\n    - type: euclidean_pearson\n      value: 63.43818859884195\n    - type: euclidean_spearman\n      value: 66.83172582815764\n    - type: manhattan_pearson\n      value: 63.055779168508764\n    - type: manhattan_spearman\n      value: 65.49585020501449\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es-it)\n      config: es-it\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 59.587830825340404\n    - type: cos_sim_spearman\n      value: 68.93467614588089\n    - type: euclidean_pearson\n      value: 62.3073527367404\n    - type: euclidean_spearman\n      value: 69.69758171553175\n    - type: manhattan_pearson\n      value: 61.9074580815789\n    - type: manhattan_spearman\n      value: 69.57696375597865\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-fr)\n      config: de-fr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 57.143220125577066\n    - type: cos_sim_spearman\n      value: 67.78857859159226\n    - type: euclidean_pearson\n      value: 55.58225107923733\n    - type: euclidean_spearman\n      value: 67.80662907184563\n    - type: manhattan_pearson\n      value: 56.24953502726514\n    - type: manhattan_spearman\n      value: 67.98262125431616\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-pl)\n      config: de-pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 21.826928900322066\n    - type: cos_sim_spearman\n      value: 49.578506634400405\n    - type: euclidean_pearson\n      value: 27.939890138843214\n    - type: euclidean_spearman\n      value: 52.71950519136242\n    - type: manhattan_pearson\n      value: 26.39878683847546\n    - type: manhattan_spearman\n      value: 47.54609580342499\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (fr-pl)\n      config: fr-pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 57.27603854632001\n    - type: cos_sim_spearman\n      value: 50.709255283710995\n    - type: euclidean_pearson\n      value: 59.5419024445929\n    - type: euclidean_spearman\n      value: 50.709255283710995\n    - type: manhattan_pearson\n      value: 59.03256832438492\n    - type: manhattan_spearman\n      value: 61.97797868009122\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.00757054859712\n    - type: cos_sim_spearman\n      value: 87.29283629622222\n    - type: euclidean_pearson\n      value: 86.54824171775536\n    - type: euclidean_spearman\n      value: 87.24364730491402\n    - type: manhattan_pearson\n      value: 86.5062156915074\n    - type: manhattan_spearman\n      value: 87.15052170378574\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 82.03549357197389\n    - type: mrr\n      value: 95.05437645143527\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 57.260999999999996\n    - type: map_at_10\n      value: 66.259\n    - type: map_at_100\n      value: 66.884\n    - type: map_at_1000\n      value: 66.912\n    - type: map_at_3\n      value: 63.685\n    - type: map_at_5\n      value: 65.35499999999999\n    - type: mrr_at_1\n      value: 60.333000000000006\n    - type: mrr_at_10\n      value: 67.5\n    - type: mrr_at_100\n      value: 68.013\n    - type: mrr_at_1000\n      value: 68.038\n    - type: mrr_at_3\n      value: 65.61099999999999\n    - type: mrr_at_5\n      value: 66.861\n    - type: ndcg_at_1\n      value: 60.333000000000006\n    - type: ndcg_at_10\n      value: 70.41\n    - type: ndcg_at_100\n      value: 73.10600000000001\n    - type: ndcg_at_1000\n      value: 73.846\n    - type: ndcg_at_3\n      value: 66.133\n    - type: ndcg_at_5\n      value: 68.499\n    - type: precision_at_1\n      value: 60.333000000000006\n    - type: precision_at_10\n      value: 9.232999999999999\n    - type: precision_at_100\n      value: 1.0630000000000002\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 25.667\n    - type: precision_at_5\n      value: 17.067\n    - type: recall_at_1\n      value: 57.260999999999996\n    - type: recall_at_10\n      value: 81.94399999999999\n    - type: recall_at_100\n      value: 93.867\n    - type: recall_at_1000\n      value: 99.667\n    - type: recall_at_3\n      value: 70.339\n    - type: recall_at_5\n      value: 76.25\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.74356435643564\n    - type: cos_sim_ap\n      value: 93.13411948212683\n    - type: cos_sim_f1\n      value: 86.80521991300147\n    - type: cos_sim_precision\n      value: 84.00374181478017\n    - type: cos_sim_recall\n      value: 89.8\n    - type: dot_accuracy\n      value: 99.67920792079208\n    - type: dot_ap\n      value: 89.27277565444479\n    - type: dot_f1\n      value: 83.9276990718124\n    - type: dot_precision\n      value: 82.04393505253104\n    - type: dot_recall\n      value: 85.9\n    - type: euclidean_accuracy\n      value: 99.74257425742574\n    - type: euclidean_ap\n      value: 93.17993008259062\n    - type: euclidean_f1\n      value: 86.69396110542476\n    - type: euclidean_precision\n      value: 88.78406708595388\n    - type: euclidean_recall\n      value: 84.7\n    - type: manhattan_accuracy\n      value: 99.74257425742574\n    - type: manhattan_ap\n      value: 93.14413755550099\n    - type: manhattan_f1\n      value: 86.82483594144371\n    - type: manhattan_precision\n      value: 87.66564729867483\n    - type: manhattan_recall\n      value: 86\n    - type: max_accuracy\n      value: 99.74356435643564\n    - type: max_ap\n      value: 93.17993008259062\n    - type: max_f1\n      value: 86.82483594144371\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 57.525863806168566\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 32.68850574423839\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 49.71580650644033\n    - type: mrr\n      value: 50.50971903913081\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 29.152190498799484\n    - type: cos_sim_spearman\n      value: 29.686180371952727\n    - type: dot_pearson\n      value: 27.248664793816342\n    - type: dot_spearman\n      value: 28.37748983721745\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.20400000000000001\n    - type: map_at_10\n      value: 1.6209999999999998\n    - type: map_at_100\n      value: 9.690999999999999\n    - type: map_at_1000\n      value: 23.733\n    - type: map_at_3\n      value: 0.575\n    - type: map_at_5\n      value: 0.885\n    - type: mrr_at_1\n      value: 78\n    - type: mrr_at_10\n      value: 86.56700000000001\n    - type: mrr_at_100\n      value: 86.56700000000001\n    - type: mrr_at_1000\n      value: 86.56700000000001\n    - type: mrr_at_3\n      value: 85.667\n    - type: mrr_at_5\n      value: 86.56700000000001\n    - type: ndcg_at_1\n      value: 76\n    - type: ndcg_at_10\n      value: 71.326\n    - type: ndcg_at_100\n      value: 54.208999999999996\n    - type: ndcg_at_1000\n      value: 49.252\n    - type: ndcg_at_3\n      value: 74.235\n    - type: ndcg_at_5\n      value: 73.833\n    - type: precision_at_1\n      value: 78\n    - type: precision_at_10\n      value: 74.8\n    - type: precision_at_100\n      value: 55.50000000000001\n    - type: precision_at_1000\n      value: 21.836\n    - type: precision_at_3\n      value: 78\n    - type: precision_at_5\n      value: 78\n    - type: recall_at_1\n      value: 0.20400000000000001\n    - type: recall_at_10\n      value: 1.894\n    - type: recall_at_100\n      value: 13.245999999999999\n    - type: recall_at_1000\n      value: 46.373\n    - type: recall_at_3\n      value: 0.613\n    - type: recall_at_5\n      value: 0.991\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (sqi-eng)\n      config: sqi-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.89999999999999\n    - type: f1\n      value: 94.69999999999999\n    - type: precision\n      value: 94.11666666666667\n    - type: recall\n      value: 95.89999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (fry-eng)\n      config: fry-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 68.20809248554913\n    - type: f1\n      value: 63.431048720066066\n    - type: precision\n      value: 61.69143958161298\n    - type: recall\n      value: 68.20809248554913\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kur-eng)\n      config: kur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 71.21951219512195\n    - type: f1\n      value: 66.82926829268293\n    - type: precision\n      value: 65.1260162601626\n    - type: recall\n      value: 71.21951219512195\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tur-eng)\n      config: tur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.2\n    - type: f1\n      value: 96.26666666666667\n    - type: precision\n      value: 95.8\n    - type: recall\n      value: 97.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (deu-eng)\n      config: deu-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 99.3\n    - type: f1\n      value: 99.06666666666666\n    - type: precision\n      value: 98.95\n    - type: recall\n      value: 99.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nld-eng)\n      config: nld-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.39999999999999\n    - type: f1\n      value: 96.63333333333333\n    - type: precision\n      value: 96.26666666666668\n    - type: recall\n      value: 97.39999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ron-eng)\n      config: ron-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96\n    - type: f1\n      value: 94.86666666666666\n    - type: precision\n      value: 94.31666666666668\n    - type: recall\n      value: 96\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ang-eng)\n      config: ang-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 47.01492537313433\n    - type: f1\n      value: 40.178867566927266\n    - type: precision\n      value: 38.179295828549556\n    - type: recall\n      value: 47.01492537313433\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ido-eng)\n      config: ido-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 86.5\n    - type: f1\n      value: 83.62537480063796\n    - type: precision\n      value: 82.44555555555554\n    - type: recall\n      value: 86.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (jav-eng)\n      config: jav-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 80.48780487804879\n    - type: f1\n      value: 75.45644599303138\n    - type: precision\n      value: 73.37398373983739\n    - type: recall\n      value: 80.48780487804879\n  - task:\n      type: BitextMining\n    dataset:\n   \n\n[Content truncated...]",
    "meta_json": "{\"pipeline_tag\":\"feature-extraction\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":559890946,\"storage_bytes\":10114130126,\"files_count\":23,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"XLMRobertaModel\"],\"model_type\":\"xlm-roberta\",\"tokenizer_config\":{\"bos_token\":\"<s>\",\"cls_token\":\"<s>\",\"eos_token\":\"</s>\",\"mask_token\":{\"__type\":\"AddedToken\",\"content\":\"<mask>\",\"lstrip\":true,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":\"<pad>\",\"sep_token\":\"</s>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2402.05672\",\"source_url\":\"https://arxiv.org/abs/2402.05672\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.08787\",\"source_url\":\"https://arxiv.org/abs/2108.08787\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.08663\",\"source_url\":\"https://arxiv.org/abs/2104.08663\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.07316\",\"source_url\":\"https://arxiv.org/abs/2210.07316\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "bd0e66d16cf3efedabeb0bf058482cfa",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/intfloat/multilingual-e5-large\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:coherelabs:c4ai-command-r-v01",
    "name": "c4ai-command-r-v01",
    "author": "CohereLabs",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "cohere",
      "text-generation",
      "conversational",
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "ja",
      "ko",
      "zh",
      "ar",
      "doi:10.57967/hf/3139",
      "license:cc-by-nc-4.0",
      "text-generation-inference",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1095,
    "downloads": 12360,
    "source": "huggingface",
    "source_url": "https://huggingface.co/CohereLabs/c4ai-command-r-v01",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":34980831232,\"storage_bytes\":205615615951,\"files_count\":23,\"spaces_count\":55,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"CohereForCausalLM\"],\"model_type\":\"cohere\",\"tokenizer_config\":{\"bos_token\":\"<BOS_TOKEN>\",\"chat_template\":[{\"name\":\"default\",\"template\":\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\"},{\"name\":\"tool_use\",\"template\":\"\\n{%- macro json_to_python_type(json_spec) %}\\n{%- set basic_type_map = {\\n    \\\"string\\\": \\\"str\\\",\\n    \\\"number\\\": \\\"float\\\",\\n    \\\"integer\\\": \\\"int\\\",\\n    \\\"boolean\\\": \\\"bool\\\"\\n} %}\\n\\n{%- if basic_type_map[json_spec.type] is defined %}\\n    {{- basic_type_map[json_spec.type] }}\\n{%- elif json_spec.type == \\\"array\\\" %}\\n    {{- \\\"List[\\\" +  json_to_python_type(json_spec.items) + \\\"]\\\"}}\\n{%- elif json_spec.type == \\\"object\\\" %}\\n    {{- \\\"Dict[str, \\\" + json_to_python_type(json_spec.additionalProperties) + ']'}}\\n{%- elif json_spec.type is iterable %}\\n    {{- \\\"Union[\\\" }}\\n    {%- for t in json_spec.type %}\\n      {{- json_to_python_type({\\\"type\\\": t}) }}\\n      {%- if not loop.last %}\\n        {{- \\\",\\\" }} \\n    {%- endif %}\\n    {%- endfor %}\\n    {{- \\\"]\\\" }}\\n{%- else %}\\n    {{- \\\"Any\\\" }}\\n{%- endif %}\\n{%- endmacro %}\\n\\n{%- macro old_tool_parser(tools) %}\\n{%- for tool in tools %}\\n    {%- if loop.index0 != 0 %}\\n        {{- '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- '```python\\\\ndef ' + tool.name + '(' }}\\n    {%- for param_name, param_fields in tool.parameter_definitions|items %}\\n        {%- if loop.index0 != 0 %}\\n            {{- ', '}}\\n        {%- endif %}\\n        {{- param_name + ': ' }}\\n        {%- if not param_fields.required %}\\n            {{- 'Optional[' + param_fields.type + '] = None'}}\\n        {%- else %}\\n            {{- param_fields.type }}\\n        {%- endif %}\\n    {%- endfor %}\\n    {{- ') -> List[Dict]:\\\\n    \\\"\\\"\\\"'}}\\n    {{- tool.description }}\\n    {%- if tool.parameter_definitions|length != 0 %}\\n        {{- '\\\\n\\\\n    Args:\\\\n        '}}\\n        {%- for param_name, param_fields in tool.parameter_definitions|items %}\\n            {%- if loop.index0 != 0 %}\\n                {{- '\\\\n        ' }}\\n            {%- endif %}\\n            {{- param_name + ' ('}}\\n            {%- if not param_fields.required %}\\n                {{- 'Optional[' + param_fields.type + ']'}}\\n            {%- else %}\\n                {{- param_fields.type }}\\n            {%- endif %}\\n            {{- '): ' + param_fields.description }}\\n        {%- endfor %}\\n    {%- endif %}\\n    {{- '\\\\n    \\\"\\\"\\\"\\\\n    pass\\\\n```' }}\\n{%- endfor %}\\n{%- endmacro %}\\n\\n{%- macro new_tool_parser(tools) %}\\n{%- for tool in tools %}\\n  {%- if loop.index0 != 0 %}\\n    {{- '\\\\n\\\\n'}}\\n  {%- endif %}\\n  {%- if tool.function is defined %}\\n    {%- set tool = tool.function %}\\n  {%- endif %}\\n  {{-'```python\\ndef ' + tool.name + '('}}\\n  {%- for param_name, param_fields in tool.parameters.properties|items %}\\n    {%- if loop.index0 != 0 %}\\n      {{- ', '}}\\n    {%- endif %}\\n    {{-param_name + \\\": \\\"}} \\n    {%- if not param_name in tool.parameters.required %}\\n      {{-'Optional[' + json_to_python_type(param_fields) + '] = None'}}\\n    {%- else %}\\n      {{- json_to_python_type(param_fields) }}\\n    {%- endif %}\\n  {%- endfor %}\\n  {{- ') -> List[Dict]:\\n    \\\"\\\"\\\"'}}\\n  {{- tool.description }}\\n  {%- if tool.parameters.properties|length != 0 %}\\n    {{- '\\\\n\\\\n    Args:\\\\n        '}}\\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\\n      {%- if loop.index0 != 0 %}\\n        {{- '\\\\n        ' }}\\n      {%- endif %}\\n      {{- param_name + ' ('}}\\n      {%- if not param_name in tool.parameters.required %}\\n        {{-'Optional[' + json_to_python_type(param_fields) + ']'}}\\n      {%- else %}\\n        {{- json_to_python_type(param_fields) }}\\n      {%- endif %}\\n      {{- '): ' + param_fields.description }}\\n    {%- endfor %}\\n    {%- endif %}\\n    {{- '\\\\n    \\\"\\\"\\\"\\\\n    pass\\\\n```' }}\\n{%- endfor %}\\n{%- endmacro %}\\n\\n{{- bos_token }}\\n{%- if messages[0]['role'] == 'system' %}\\n  {%- set loop_messages = messages[1:] %}\\n  {%- set system_message = messages[0]['content'] %}\\n{%- else %}\\n  {%- set loop_messages = messages %}\\n  {%- set system_message = '## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.' %}\\n{%- endif %}\\n{{- '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}\\n{{- '# Safety Preamble' }}\\n{{- '\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\'t answer questions that are harmful or immoral.' }}\\n{{- '\\n\\n# System Preamble' }}\\n{{- '\\n## Basic Rules' }}\\n{{- '\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\'s requests, you cite your sources in your answers, according to those instructions.' }}\\n{{- '\\n\\n# User Preamble' }}\\n{{- '\\n' + system_message }}\\n{{-'\\n\\n## Available Tools\\nHere is a list of tools that you have available to you:\\n\\n'}}\\n{%- set ns = namespace(new_tools=true) %}\\n{%- for tool in tools %}\\n    {%- if tool.parameter_definitions is defined %}\\n        {%- set ns.new_tools = false %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if ns.new_tools %}\\n    {{- new_tool_parser(tools) }}\\n{%- else %}\\n    {{- old_tool_parser(tools) }}\\n{%- endif %}\\n{{- '<|END_OF_TURN_TOKEN|>'}}\\n{%- for message in loop_messages %}\\n  {%- set content = message['content'] %}\\n  {%- if message.role == 'user' %}\\n    {{- '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content|trim + '<|END_OF_TURN_TOKEN|>' }}\\n  {%- elif message.role == 'system' %}\\n    {{- '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + content|trim + '<|END_OF_TURN_TOKEN|>' }}\\n  {%- elif message.role == 'assistant' and message.tool_calls is defined %}\\n    {{- '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}\\n    {%- if message.content is defined %}\\n        {{- message.content|trim }}\\n    {%- endif %}\\n    {{- '\\\\nAction:\\\\n```json\\\\n[\\\\n' }}\\n    {%- for tool_call in message.tool_calls %}\\n        {%- if tool_call.function is defined %}\\n            {%- set tool_call = tool_call.function %}\\n        {%- endif %}\\n        {{- '{\\\\n'|indent(4, first=true) }}\\n        {{- '\\\"tool_name\\\": \\\"'|indent(8, first=true) + tool_call.name + '\\\",\\\\n' }}\\n        {{- '\\\"parameters\\\": '|indent(8, first=true) }}\\n        {%- if tool_call.arguments is defined and tool_call.arguments|length > 0 %}    \\n            {{- tool_call.arguments|tojson(indent=4)|indent(8) }}\\n            {{- '\\\\n' }}\\n        {%- else %}\\n            {{- '{}\\\\n' }}\\n        {%- endif %}\\n        {{- '}'|indent(4, first=true) }}\\n        {%- if not loop.last %}\\n            {{- ',\\\\n' }}\\n        {%- endif %}\\n    {%- endfor %}\\n    {{- \\\"\\\\n]```\\\\n\\\" }}\\n  {%- elif message.role == 'assistant' %}\\n    {{- '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content|trim + '<|END_OF_TURN_TOKEN|>' }}\\n  {%- elif message.role == 'tool' %}\\n    {{- '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\\\\n' }}\\n    {{- message.content|trim }}\\n    {{- '</results><|END_OF_TURN_TOKEN|>' }}\\n  {%- endif %}\\n{%- endfor %}\\n{{-'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\\\'Action:\\\\' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\\\'s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\\n```json\\n[\\n    {\\n        \\\"tool_name\\\": title of the tool in the specification,\\n        \\\"parameters\\\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\\n    }\\n]```<|END_OF_TURN_TOKEN|>'}}\\n{%- if add_generation_prompt %}\\n  {{- '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}\\n{%- endif %}\\n\"},{\"name\":\"rag\",\"template\":\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = '## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.' %}{% endif %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ '# Safety Preamble' }}{{ '\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\'t answer questions that are harmful or immoral.' }}{{ '\\n\\n# System Preamble' }}{{ '\\n## Basic Rules' }}{{ '\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\'s requests, you cite your sources in your answers, according to those instructions.' }}{{ '\\n\\n# User Preamble' }}{{ '\\n' + system_message }}{{ '<|END_OF_TURN_TOKEN|>'}}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'system' %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'}}{{ '<results>' }}{% for document in documents %}{{ '\\nDocument: ' }}{{ loop.index0 }}\\n{% for key, value in document.items() %}{{ key }}: {{value}}\\n{% endfor %}{% endfor %}{{ '</results>'}}{{ '<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ 'Carefully perform the following instructions, in order, starting each with a new line.\\n' }}{{ 'Firstly, Decide which of the retrieved documents are relevant to the user\\\\'s last input by writing \\\\'Relevant Documents:\\\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\\\'None\\\\'.\\n' }}{{ 'Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\\\'s last input by writing \\\\'Cited Documents:\\\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\\\'None\\\\'.\\n' }}{% if citation_mode=='accurate' %}{{ 'Thirdly, Write \\\\'Answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\\n' }}{% endif %}{{ 'Finally, Write \\\\'Grounded answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.' }}{{ '<|END_OF_TURN_TOKEN|>' }}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\"}],\"eos_token\":\"<|END_OF_TURN_TOKEN|>\",\"pad_token\":\"<PAD>\",\"unk_token\":null,\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "bc3c9fb88c7161582a81626382adff45",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/CohereLabs/c4ai-command-r-v01\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:sentence-transformers:paraphrase-multilingual-minilm-l12-v2",
    "name": "paraphrase-multilingual-MiniLM-L12-v2",
    "author": "sentence-transformers",
    "description": "--- language: - multilingual - ar - bg - ca - cs - da - de - el - en - es - et - fa - fi - fr - gl - gu - he - hi - hr - hu - hy - id - it - ja - ka - ko - ku - lt - lv - mk - mn - mr - ms - my - nb - nl - pl - pt - ro - ru - sk - sl - sq - sr - sv - th - tr - uk - ur - vi license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers language_bcp47: - fr-ca - pt-br - zh-cn - zh-tw pipeline_tag: sentence-similari...",
    "tags": [
      "sentence-transformers",
      "pytorch",
      "tf",
      "onnx",
      "safetensors",
      "openvino",
      "bert",
      "feature-extraction",
      "sentence-similarity",
      "transformers",
      "multilingual",
      "ar",
      "bg",
      "ca",
      "cs",
      "da",
      "de",
      "el",
      "en",
      "es",
      "et",
      "fa",
      "fi",
      "fr",
      "gl",
      "gu",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "it",
      "ja",
      "ka",
      "ko",
      "ku",
      "lt",
      "lv",
      "mk",
      "mn",
      "mr",
      "ms",
      "my",
      "nb",
      "nl",
      "pl",
      "pt",
      "ro",
      "ru",
      "sk",
      "sl",
      "sq",
      "sr",
      "sv",
      "th",
      "tr",
      "uk",
      "ur",
      "vi",
      "arxiv:1908.10084",
      "license:apache-2.0",
      "text-embeddings-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "sentence-similarity",
    "likes": 1072,
    "downloads": 16597830,
    "source": "huggingface",
    "source_url": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- multilingual\n- ar\n- bg\n- ca\n- cs\n- da\n- de\n- el\n- en\n- es\n- et\n- fa\n- fi\n- fr\n- gl\n- gu\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- it\n- ja\n- ka\n- ko\n- ku\n- lt\n- lv\n- mk\n- mn\n- mr\n- ms\n- my\n- nb\n- nl\n- pl\n- pt\n- ro\n- ru\n- sk\n- sl\n- sq\n- sr\n- sv\n- th\n- tr\n- uk\n- ur\n- vi\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\nlanguage_bcp47:\n- fr-ca\n- pt-br\n- zh-cn\n- zh-tw\npipeline_tag: sentence-similarity\n---\n\n# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers](https://www.sbert.net/). \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n```",
    "meta_json": "{\"pipeline_tag\":\"sentence-similarity\",\"library_name\":\"sentence-transformers\",\"framework\":\"sentence-transformers\",\"params\":117654272,\"storage_bytes\":6650862358,\"files_count\":28,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BertModel\"],\"model_type\":\"bert\",\"tokenizer_config\":{\"unk_token\":\"<unk>\",\"sep_token\":\"</s>\",\"pad_token\":\"<pad>\",\"cls_token\":\"<s>\",\"mask_token\":{\"content\":\"<mask>\",\"single_word\":false,\"lstrip\":true,\"rstrip\":false,\"normalized\":true,\"__type\":\"AddedToken\"},\"bos_token\":\"<s>\",\"eos_token\":\"</s>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1908.10084\",\"source_url\":\"https://arxiv.org/abs/1908.10084\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "187022630cdc9060fbcdd645d95f89af",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:qwen:qwen3-235b-a22b",
    "name": "Qwen3-235B-A22B",
    "author": "Qwen",
    "description": "--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/LICENSE pipeline_tag: text-generation --- <a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of d...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3_moe",
      "text-generation",
      "conversational",
      "arxiv:2309.00071",
      "arxiv:2505.09388",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1057,
    "downloads": 376043,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Qwen/Qwen3-235B-A22B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-235B-A22B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-235B-A22B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 235B in total and 22B activated\n- Number of Paramaters (Non-Embedding): 234B\n- Number of Layers: 94\n- Number of Attention Heads (GQA): 64 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3_moe'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-235B-A22B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B --reasoning-parser qwen3 --tp 8\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-235B-A22B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-235B-A22B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-235B-A22B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":235093634560,\"storage_bytes\":470203304443,\"files_count\":128,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3MoeForCausalLM\"],\"model_type\":\"qwen3_moe\",\"tokenizer_config\":{\"bos_token\":null,\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":null}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen3\",\"source_url\":\"https://github.com/QwenLM/Qwen3\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen-Agent\",\"source_url\":\"https://github.com/QwenLM/Qwen-Agent\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2309.00071\",\"source_url\":\"https://arxiv.org/abs/2309.00071\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2505.09388\",\"source_url\":\"https://arxiv.org/abs/2505.09388\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "df01f706f5623d81eef465ce16ff9248",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Qwen/Qwen3-235B-A22B\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:ibm-granite:granite-docling-258m",
    "name": "granite-docling-258M",
    "author": "ibm-granite",
    "description": "--- license: apache-2.0 datasets: - ds4sd/SynthCodeNet - ds4sd/SynthFormulaNet - ds4sd/SynthChartNet - HuggingFaceM4/DoclingMatix tags: - text-generation - documents - code - formula - chart - ocr - layout - table - document-parse - docling - granite - extraction - math language: - en pipeline_tag: image-text-to-text library_name: transformers --- <div style=\"display: flex; align-items: center;\"> <img src=\"https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/granite_docling.pn...",
    "tags": [
      "transformers",
      "safetensors",
      "idefics3",
      "image-to-text",
      "text-generation",
      "documents",
      "code",
      "formula",
      "chart",
      "ocr",
      "layout",
      "table",
      "document-parse",
      "docling",
      "granite",
      "extraction",
      "math",
      "image-text-to-text",
      "conversational",
      "en",
      "dataset:ds4sd/synthcodenet",
      "dataset:ds4sd/synthformulanet",
      "dataset:ds4sd/synthchartnet",
      "dataset:huggingfacem4/doclingmatix",
      "arxiv:2501.17887",
      "arxiv:2503.11576",
      "arxiv:2305.03393",
      "license:apache-2.0",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 1044,
    "downloads": 106295,
    "source": "huggingface",
    "source_url": "https://huggingface.co/ibm-granite/granite-docling-258M",
    "image_url": "https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png",
    "type": "dataset",
    "body_content": "---\nlicense: apache-2.0\ndatasets:\n- ds4sd/SynthCodeNet\n- ds4sd/SynthFormulaNet\n- ds4sd/SynthChartNet\n- HuggingFaceM4/DoclingMatix\ntags:\n- text-generation\n- documents\n- code\n- formula\n- chart\n- ocr\n- layout\n- table\n- document-parse\n- docling\n- granite\n- extraction\n- math\nlanguage:\n- en\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n   \n# granite-docling-258m\n<div style=\"display: flex; align-items: center;\">\n    <img src=\"https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/granite_docling.png\" alt=\"Granite Docling Logo\" style=\"width: 200px; height: auto; margin-right: 20px;\">\n    <div>\n        <p>Granite Docling is a multimodal Image-Text-to-Text model engineered for efficient document conversion. It preserves the core features of Docling while maintaining seamless integration with <a href=\"https://docling-project.github.io/docling \">DoclingDocuments</a> to ensure full compatibility. </p>\n    </div>\n</div>\n\n**Model Summary**: \n\nGranite Docling 258M builds upon the Idefics3 architecture, but introduces two key modifications: it replaces the vision encoder with siglip2-base-patch16-512 and substitutes the language model with a Granite 165M LLM. Try out our [Granite-Docling-258](https://huggingface.co/spaces/ibm-granite/granite-docling-258m-demo) demo today.\n\n- **Developed by**: IBM Research\n- **Model type**: Multi-modal model (image+text-to-text)\n- **Language(s)**: English (NLP)\n- **License**: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n- **Release Date**: September 17, 2025\n\nGranite-docling-258M is fully integrated into the Docling pipelines, carrying over existing [features](https://huggingface.co/ds4sd/SmolDocling-256M-preview) while introducing a number of powerful new features, including:\n\n- ðŸ”¢ Enhanced Equation Recognition: More accurate detection and formatting of mathematical formulas\n- ðŸ§© Flexible Inference Modes: Choose between full-page inference, bbox-guided region inference\n- ðŸ§˜ Improved Stability: Tends to avoid infinite loops more effectively\n- ðŸ§® Enhanced Inline Equations: Better inline math recognition\n- ðŸ§¾ Document Element QA: Answer questions about a documentâ€™s structure such as the presence and order of document elements\n- ðŸŒ Japanese, Arabic and Chinese support (_experimental_)\n\n\n\n## Getting started\n\nThe easiest way to use this model is through the [ðŸ¥Docling](https://github.com/docling-project/docling) library. It will automatically download this model and convert documents to various formats for you. \n\nInstall the latest version of `docling` through pip, then use the following CLI command:\n\n```sh\n# Convert to HTML and Markdown:\ndocling --to html --to md --pipeline vlm --vlm-model granite_docling \"https://arxiv.org/pdf/2501.17887\" # accepts files, urls or directories\n\n# Convert to HTML including layout visualization:\ndocling --to html_split_page --show-layout --pipeline vlm --vlm-model granite_docling \"https://arxiv.org/pdf/2501.17887\"\n\n```\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png\" alt=\"GraniteDocling result in split page view\" width=\"900\"/>\n</p>\n\n<details>\n<summary>You can also set this model up within the Docling SDK:</summary>\n  \n```python\nfrom docling.datamodel import vlm_model_specs\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    VlmPipelineOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.pipeline.vlm_pipeline import VlmPipeline\n\nsource = \"https://arxiv.org/pdf/2501.17887\"\n\n###### USING SIMPLE DEFAULT VALUES\n# - GraniteDocling model\n# - Using the transformers framework\n\nconverter = DocumentConverter(\n    format_options={\n        InputFormat.PDF: PdfFormatOption(\n            pipeline_cls=VlmPipeline,\n        ),\n    }\n)\n\ndoc = converter.convert(source=source).document\n\nprint(doc.export_to_markdown())\n\n\n###### USING MACOS MPS ACCELERATOR\n# For more options see the compare_vlm_models.py example.\n\npipeline_options = VlmPipelineOptions(\n    vlm_options=vlm_model_specs.GRANITEDOCLING_MLX,\n)\n\nconverter = DocumentConverter(\n    format_options={\n        InputFormat.PDF: PdfFormatOption(\n            pipeline_cls=VlmPipeline,\n            pipeline_options=pipeline_options,\n        ),\n    }\n)\n\ndoc = converter.convert(source=source).document\n\nprint(doc.export_to_markdown())\n```\n</details>\n\n\nAlternatively, you can use bare **transformers**, **vllm**, **onnx** or **mlx-vlm** to perform inference, and [docling-core](https://github.com/docling-project/docling-core) APIs to convert results to variety of output formats (md, html, etc.):\n\n<details>\n<summary>ðŸ“„ Single page image inference using plain ðŸ¤— tranformers ðŸ¤–</summary>\n\n```python\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load images\nimage = load_image(\"https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/new_arxiv.png\")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"ibm-granite/granite-docling-258M\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    \"ibm-granite/granite-docling-258M\",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"sdpa\",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\nprint(f\"DocTags: \\n{doctags}\\n\")\n\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\nprint(f\"Markdown:\\n{doc.export_to_markdown()}\\n\")\n\n## export as any format.\n# Path(\"out/\").mkdir(parents=True, exist_ok=True)\n# HTML:\n# output_path_html = Path(\"out/\") / \"example.html\"\n# doc.save_as_html(output_path_html)\n# Markdown:\n# output_path_md = Path(\"out/\") / \"example.md\"\n# doc.save_as_markdown(output_path_md)\n\n```\n</details>\n\n\n<details>\n<summary> ðŸš€ Fast Batch Inference with VLLM</summary>\n\n```python\n# Prerequisites:\n# pip install vllm\n# pip install docling_core\n# place page images you want to convert into \"img/\" dir\n\nimport time\nimport os\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoProcessor\nfrom PIL import Image\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom pathlib import Path\n\n# Configuration\nMODEL_PATH = \"ibm-granite/granite-docling-258M\"\nIMAGE_DIR = \"img/\"  # Place your page images here\nOUTPUT_DIR = \"out/\"\nPROMPT_TEXT = \"Convert this page to docling.\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": PROMPT_TEXT},\n        ],\n    },\n]\n\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Initialize LLM\nllm = LLM(model=MODEL_PATH, revision=\"untied\", limit_mm_per_prompt={\"image\": 1})\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\n\nsampling_params = SamplingParams(\n    temperature=0.0,\n    max_tokens=8192,\n    skip_special_tokens=False,\n)\n\n# Load and prepare all images and prompts up front\nbatched_inputs = []\nimage_names = []\n\nfor img_file in sorted(os.listdir(IMAGE_DIR)):\n    if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n        img_path = os.path.join(IMAGE_DIR, img_file)\n        with Image.open(img_path) as im:\n            image = im.convert(\"RGB\")\n\n        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n        batched_inputs.append({\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}})\n        image_names.append(os.path.splitext(img_file)[0])\n\n# Run batch inference\nstart_time = time.time()\noutputs = llm.generate(batched_inputs, sampling_params=sampling_params)\n\n# Postprocess all results\nfor img_fn, output, input_data in zip(image_names, outputs, batched_inputs):\n    doctags = output.outputs[0].text\n    output_path_dt = Path(OUTPUT_DIR) / f\"{img_fn}.dt\"\n    output_path_md = Path(OUTPUT_DIR) / f\"{img_fn}.md\"\n\n    with open(output_path_dt, \"w\", encoding=\"utf-8\") as f:\n        f.write(doctags)\n\n    # Convert to DoclingDocument and save markdown\n    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [input_data[\"multi_modal_data\"][\"image\"]])\n    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n    doc.save_as_markdown(output_path_md)\n\nprint(f\"Total time: {time.time() - start_time:.2f} sec\")\n\n```\n</details>\n\nðŸ’» Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ibm-granite/granite-docling-258M-mlx)\n\nâ„¹ï¸ If you see trouble running granite-docling with the codes above, check the troubleshooting section at the bottom â¬‡ï¸. \n\n## Intended Use \nGranite-Docling is designed to complement the Docling library, not replace it. It integrates as a component within larger Docling library, consolidating the functions of multiple single-purpose models into a single, compact VLM. \nHowever, Granite-Docling is **not** intended for general image understanding. For tasks focused solely on image-text input, we recommend using [Granite Vision models](https://huggingface.co/collections/ibm-granite/granite-vision-models-67b3bd4ff90c915ba4cd2800), which are purpose-built and optimized for image-text processing.\n\n## Evaluations\nA comprehensive discussion of evaluation methods and findings has already been presented in our previous publication [[citation](https://arxiv.org/pdf/2503.11576)]. As this model is an update, we refer readers to that work for additional details.\nThe evaluation can be performed using the [docling-eval](https://github.com/docling-project/docling-eval) framework for the document related tasks, and [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) for MMStar and OCRBench.\n\n<table>\n  <thead>\n    <tr><th colspan=\"5\"><b>Layout</b></th></tr>\n    <tr>\n      <th></th>\n      <th>MAP â†‘</th>\n      <th>F1 â†‘</th>\n      <th>Precision â†‘</th>\n      <th>Recall â†‘</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.23</td><td>0.85</td><td>0.9</td><td>0.84</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.27</b></td><td><b>0.86</b></td><td><b>0.92</b></td><td><b>0.88</b></td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr><th colspan=\"7\"><b>Full Page OCR</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance â†“</th>\n      <th>F1 â†‘</th>\n      <th>Precision â†‘</th>\n      <th>Recall â†‘</th>\n      <th>BLEU â†‘</th>\n      <th>Meteor â†‘</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.48</td><td>0.80</td><td>0.89</td>\n      <td>0.79</td><td>0.58</td><td>0.67</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.45</b></td><td><b>0.84</b></td><td><b>0.91</b></td>\n      <td><b>0.83</b></td><td><b>0.65</b></td><td><b>0.72</b></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr><th colspan=\"7\"><b>Code Recognition</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance â†“</th>\n      <th>F1 â†‘</th>\n      <th>Precision â†‘</th>\n      <th>Recall â†‘</th>\n      <th>BLEU â†‘</th>\n      <th>Meteor â†‘</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.114</td><td>0.915</td><td>0.94</td><td>0.909</td><td>0.875</td><td>0.889</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.013</b></td><td><b>0.988</b></td><td><b>0.99</b></td><td><b>0.988</b></td>\n      <td><b>0.983</b></td><td><b>0.986</b></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr><th colspan=\"7\"><b>Equation Recognition</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance â†“</th>\n      <th>F1 â†‘</th>\n      <th>Precision â†‘</th>\n      <th>Recall â†‘</th>\n      <th>BLEU â†‘</th>\n      <th>Meteor â†‘</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.119</td><td>0.947</td><td>0.959</td><td>0.941</td><td>0.824</td><td>0.878</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.073</b></td><td><b>0.968</b></td><td><b>0.968</b></td><td><b>0.969</b></td>\n      <td><b>0.893</b></td><td><b>0.927</b></td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr><th colspan=\"3\"><b>Table Recognition (FinTabNet 150dpi)</b></th></tr>\n    <tr>\n      <th></th>\n      <th>TEDS (structure) â†‘</th>\n      <th>TEDS (w/content) â†‘</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.82</td><td>0.76</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.97</b></td><td><b>0.96</b></td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr><th colspan=\"3\"><b>Other Benchmarks</b></th></tr>\n    <tr>\n      <th></th>\n      <th>MMStar â†‘</th>\n      <th>OCRBench â†‘</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.17</td><td>338</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.30</b></td><td><b>500</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n\nðŸ’» Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ibm-granite/granite-docling-258M-mlx)\n\n\n## Supported Instructions\n\n<table>\n  <tr>\n    <th>Description</th>\n    <th>Instruction</th>\n    <th>Short Instruction</th>\n  </tr>\n  <tr>\n    <td><b>Full conversion</b></td>\n    <td>Convert this page to docling.</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><b>Chart</b></td>\n    <td>Convert chart to table.</td>\n    <td><code>&lt;chart&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Formula</b></td>\n    <td>Convert formula to LaTeX.</td>\n    <td><code>&lt;formula&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Code</b></td>\n    <td>Convert code to text.</td>\n    <td><code>&lt;code&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Table</b></td>\n    <td>Convert table to OTSL. (<a href=\"https://arxiv.org/pdf/2305.03393\">Lysak et al., 2023</a>)</td>\n    <td><code>&lt;otsl&gt;</code></td>\n  </tr>\n  <tr>\n    <td rowspan=\"4\"><b>Actions and Pipelines</b></td>\n    <td>OCR the text in a specific location: &lt;loc_155&gt;&lt;loc_233&gt;&lt;loc_206&gt;&lt;loc_237&gt;</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Identify element at: &lt;loc_247&gt;&lt;loc_482&gt;&lt;loc_252&gt;&lt;loc_486&gt;</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Find all 'text' elements on the page, retrieve all section headers.</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Detect footer elements on the page.</td>\n    <td>-</td>\n  </tr>\n</table>\n\n\n\n# Model Architecture:\n\nThe architecture of granite-docling-258m consists of the following components:\n\n(1) Vision encoder: [siglip2-base-patch16-512](https://huggingface.co/google/siglip2-base-patch16-512).\n\n(2) Vision-language connector: pixel shuffle projector (as in idefics3) \n\n(3) Large language model: Granite 165M.\n\nWe built upon [Idefics3](https://huggingface.co/docs/transformers/en/model_doc/idefics3) to train our model. We incorporated DocTags into our LLMâ€™s supervised fine-tuning (SFT) data to help the model become familiar with the format, enabling faster convergence and mitigating issues previously observed with SmolDocling.\nThe model was trained using the [nanoVLM](https://github.com/huggingface/nanoVLM) framework, which provides a lightweight and efficient training setup for vision-language models\n\n\n**Training Data**: Our training corpus consists of two principal sources: (1) publicly available datasets and (2) internally constructed synthetic datasets designed to elicit specific document understanding capabilities.\n\nIn particular, we incorporate:\n\n* [**SynthCodeNet**](https://huggingface.co/datasets/ds4sd/SynthCodeNet) â€” a large-scale collection of synthetically rendered code snippets spanning over 50 programming languages\n* [**SynthFormulaNet**](https://huggingface.co/datasets/ds4sd/SynthFormulaNet) â€” a dataset of synthetic mathematical expressions paired with ground-truth LaTeX representations\n* [**SynthChartNet**](https://huggingface.co/datasets/ds4sd/SynthChartNet) â€” synthetic chart images annotated with structured table outputs\n* [**DoclingMatix**](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix) â€” a curated corpus of real-world document pages sampled from diverse domains\n\n\n**Infrastructure**: We train granite-docling-258m using IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\n\n**Responsible Use and Limitations** Some use cases for Vision Language Models can trigger certain risks and ethical considerations, including but not limited to: bias and fairness, misinformation, and autonomous decision-making. \nAlthough our alignment processes include safety considerations, the model may in some cases produce inaccurate, biased, offensive or unwanted responses to user prompts. Additionally, whether smaller models may exhibit increased susceptibility \nto hallucination in generation scenarios due to their reduced sizes, which could limit their ability to generate coherent and contextually accurate responses, remains uncertain. This aspect is currently an active area of research, \nand we anticipate more rigorous exploration, comprehension, and mitigations in this domain. We urge the community to use granite-docling-258m in a responsible way and avoid any malicious utilization. We recommend using this model only as part of the Docling library.\nMore general vision tasks may pose higher inherent risks of triggering unwanted output. To enhance safety, we recommend using granite-docling-258m alongside Granite Guardian. Granite Guardian is a fine-tuned instruct model designed to detect and flag risks in prompts and responses across key dimensions outlined in the IBM AI Risk Atlas.\nIts training, which includes both human-annotated and synthetic data informed by internal red-teaming, enables it to outperform similar open-source models on standard benchmarks, providing an additional layer of safety.\n\n**Resources**\n\n- â­ï¸ Learn about the latest updates with Docling: https://docling-project.github.io/docling/#features\n- ðŸš€ Get started with Docling concepts, integrations and tutorials: https://docling-project.github.io/docling/getting_started/\n- ðŸ’¡ Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\n- ðŸ–¥ï¸ Learn more about how to use Granite-Docling, explore the Docling library, and see whatâ€™s coming next for Docling in the release blog: https://ibm.com/new/announcements/granite-docling-end-to-end-document-conversion\n\n## Troubleshooting\n\n**Running with VLLM**\n\n1. You receive `AttributeError: 'LlamaModel' object has no attribute 'wte'` when launching the model through VLLM.\n   \n    With current versions of VLLM (including 0.10.2), support for tied weights as used in granite-docling is limited and breaks. We provide a version with untied weights on the `untied` branch of this model repo.\n    To use the untied version, please pass the `revision` argument to VLLM:\n    \n    ```sh\n    # Serve the model through VLLM\n    $> vllm serve ibm-granite/granite-docling-258M --revision untied\n    ``` \n    \n    ```python\n    # If using the VLLM python SDK:\n    from vllm import LLM\n    ... \n\n    llm = LLM(model=MODEL_PATH, revision=\"untied\", limit_mm_per_prompt={\"image\": 1})\n    ```\n\n2. The model outputs only exclamation marks (i.e. \"!!!!!!!!!!!!!!!\").\n\n   This is seen on older NVIDIA GPUs, such as the T4 GPU available in Google Colab, because it lacks support for `bfloat16` format.\n   You can work around it by setting the `dtype` to `float32`.\n\n   ```sh\n    # Serve the model through VLLM\n    $> vllm serve ibm-granite/granite-docling-258M --revision untied --dtype float32\n    ``` \n    \n    ```python\n    # If using the VLLM python SDK:\n    from vllm import LLM\n    ... \n\n    llm = LLM(model=MODEL_PATH, revision=\"untied\", limit_mm_per_prompt={\"image\": 1}, dtype=\"float32\")\n    ```\n\n    \n   \n\n\n",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":257517120,\"storage_bytes\":3045691426,\"files_count\":17,\"spaces_count\":11,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Idefics3ForConditionalGeneration\"],\"model_type\":\"idefics3\",\"tokenizer_config\":{\"bos_token\":\"<|start_of_role|>\",\"eos_token\":\"<|end_of_text|>\",\"pad_token\":\"<|end_of_text|>\",\"unk_token\":\"<|unk|>\"},\"chat_template_jinja\":\"{%- for message in messages -%}\\n{{- '<|start_of_role|>' + message['role'] + '<|end_of_role|>' -}}\\n{%- if message['content'] is string -%}\\n{{- message['content'] -}}\\n{%- else -%}\\n{%- for part in message['content'] -%}\\n{%- if part['type'] == 'text' -%}\\n{{- part['text'] -}}\\n{%- elif part['type'] == 'image' -%}\\n{{- '<image>' -}}\\n{%- endif -%}\\n{%- endfor -%}\\n{%- endif -%}\\n{{- '<|end_of_text|>\\n' -}}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n{{- '<|start_of_role|>assistant' -}}\\n{%- if controls -%}{{- ' ' + controls | tojson() -}}{%- endif -%}\\n{{- '<|end_of_role|>' -}}\\n{%- endif -%}\\n\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:docling-project:docling\",\"source_url\":\"https://github.com/docling-project/docling\"},{\"type\":\"has_code\",\"target_id\":\"github:docling-project:docling-core\",\"source_url\":\"https://github.com/docling-project/docling-core\"},{\"type\":\"has_code\",\"target_id\":\"github:docling-project:docling-eval\",\"source_url\":\"https://github.com/docling-project/docling-eval\"},{\"type\":\"has_code\",\"target_id\":\"github:EvolvingLMMs-Lab:lmms-eval\",\"source_url\":\"https://github.com/EvolvingLMMs-Lab/lmms-eval\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:nanoVLM\",\"source_url\":\"https://github.com/huggingface/nanoVLM\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.17887\",\"source_url\":\"https://arxiv.org/abs/2501.17887\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2503.11576\",\"source_url\":\"https://arxiv.org/abs/2503.11576\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2305.03393\",\"source_url\":\"https://arxiv.org/abs/2305.03393\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 100,
    "content_hash": "104efa0e537fc1c5f45e26b2264496fd",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/ibm-granite/granite-docling-258M\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:prosusai:finbert",
    "name": "finbert",
    "author": "ProsusAI",
    "description": "--- language: \"en\" tags: - financial-sentiment-analysis - sentiment-analysis widget: - text: \"Stocks rallied and the British pound gained.\" --- FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the ...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "bert",
      "text-classification",
      "financial-sentiment-analysis",
      "sentiment-analysis",
      "en",
      "arxiv:1908.10063",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "likes": 1037,
    "downloads": 2853068,
    "source": "huggingface",
    "source_url": "https://huggingface.co/ProsusAI/finbert",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage: \"en\"\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: \"Stocks rallied and the British pound gained.\"\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n",
    "meta_json": "{\"pipeline_tag\":\"text-classification\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":null,\"storage_bytes\":3504463245,\"files_count\":9,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"BertForSequenceClassification\"],\"model_type\":\"bert\",\"tokenizer_config\":{\"unk_token\":\"[UNK]\",\"sep_token\":\"[SEP]\",\"pad_token\":\"[PAD]\",\"cls_token\":\"[CLS]\",\"mask_token\":\"[MASK]\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1908.10063\",\"source_url\":\"https://arxiv.org/abs/1908.10063\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 40,
    "content_hash": "3a6324fdcec290f9f4032d51fd5adee9",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/ProsusAI/finbert\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:flan-t5-base",
    "name": "flan-t5-base",
    "author": "google",
    "description": "--- language: - en - fr - ro - de - multilingual tags: - text2text-generation widget: - text: \"Translate to German: My name is Arthur\" example_title: \"Translation\" - text: \"Please answer to the following question. Who is going to be the next Ballon d'or?\" example_title: \"Question Answering\" - text: \"Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\" example_title: \"Logical reasoning\" - text: \"Please answer the following question. What is t...",
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "t5",
      "text2text-generation",
      "en",
      "fr",
      "ro",
      "de",
      "multilingual",
      "dataset:svakulenk0/qrecc",
      "dataset:taskmaster2",
      "dataset:djaym7/wiki_dialog",
      "dataset:deepmind/code_contests",
      "dataset:lambada",
      "dataset:gsm8k",
      "dataset:aqua_rat",
      "dataset:esnli",
      "dataset:quasc",
      "dataset:qed",
      "arxiv:2210.11416",
      "arxiv:1910.09700",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 1025,
    "downloads": 1040535,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/flan-t5-base",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\ntags:\n- text2text-generation\n\nwidget:\n- text: \"Translate to German:  My name is Arthur\"\n  example_title: \"Translation\"\n- text: \"Please answer to the following question. Who is going to be the next Ballon d'or?\"\n  example_title: \"Question Answering\"\n- text: \"Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\"\n  example_title: \"Logical reasoning\"\n- text: \"Please answer the following question. What is the boiling point of Nitrogen?\"\n  example_title: \"Scientific knowledge\"\n- text: \"Answer the following yes/no question. Can you write a whole Haiku in a single tweet?\"\n  example_title: \"Yes/no question\"\n- text: \"Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?\"\n  example_title: \"Reasoning task\"\n- text: \"Q: ( False or not False or False ) is? A: Let's think step by step\"\n  example_title: \"Boolean Expressions\"\n- text: \"The square root of x is the cube root of y. What is y to the power of 2, if x = 4?\"\n  example_title: \"Math reasoning\"\n- text: \"Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It's not certain how many lessons you'll learn by your thirties. Does the premise entail the hypothesis?\"\n  example_title: \"Premise and hypothesis\"\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 base\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper's model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model's [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Base, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips â‰¥ 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n## Model Recycling\n\n[Evaluation on 36 datasets](https://ibm.github.io/model-recycling/model_gain_chart?avg=9.16&mnli_lp=nan&20_newsgroup=3.34&ag_news=1.49&amazon_reviews_multi=0.21&anli=13.91&boolq=16.75&cb=23.12&cola=9.97&copa=34.50&dbpedia=6.90&esnli=5.37&financial_phrasebank=18.66&imdb=0.33&isear=1.37&mnli=11.74&mrpc=16.63&multirc=6.24&poem_sentiment=14.62&qnli=3.41&qqp=6.18&rotten_tomatoes=2.98&rte=24.26&sst2=0.67&sst_5bins=5.44&stsb=20.68&trec_coarse=3.95&trec_fine=10.73&tweet_ev_emoji=13.39&tweet_ev_emotion=4.62&tweet_ev_hate=3.46&tweet_ev_irony=9.04&tweet_ev_offensive=1.69&tweet_ev_sentiment=0.75&wic=14.22&wnli=9.44&wsc=5.53&yahoo_answers=4.14&model_name=google%2Fflan-t5-base&base_name=google%2Ft5-v1_1-base) using google/flan-t5-base as a base model yields average score of 77.98 in comparison to 68.82 by google/t5-v1_1-base.\n\nThe model is ranked 1st among all tested models for the google/t5-v1_1-base architecture as of 06/02/2023\nResults:\n\n|   20_newsgroup |   ag_news |   amazon_reviews_multi |    anli |   boolq |      cb |    cola |   copa |   dbpedia |   esnli |   financial_phrasebank |   imdb |   isear |    mnli |    mrpc |   multirc |   poem_sentiment |    qnli |     qqp |   rotten_tomatoes |     rte |    sst2 |   sst_5bins |    stsb |   trec_coarse |   trec_fine |   tweet_ev_emoji |   tweet_ev_emotion |   tweet_ev_hate |   tweet_ev_irony |   tweet_ev_offensive |   tweet_ev_sentiment |     wic |   wnli |     wsc |   yahoo_answers |\n|---------------:|----------:|-----------------------:|--------:|--------:|--------:|--------:|-------:|----------:|--------:|-----------------------:|-------:|--------:|--------:|--------:|----------:|-----------------:|--------:|--------:|------------------:|--------:|--------:|------------:|--------:|--------------:|------------:|-----------------:|-------------------:|----------------:|-----------------:|---------------------:|---------------------:|--------:|-------:|--------:|----------------:|\n|        86.2188 |   89.6667 |                  67.12 | 51.9688 | 82.3242 | 78.5714 | 80.1534 |     75 |   77.6667 | 90.9507 |                   85.4 | 93.324 |  72.425 | 87.2457 | 89.4608 |   62.3762 |          82.6923 | 92.7878 | 89.7724 |           89.0244 | 84.8375 | 94.3807 |     57.2851 | 89.4759 |          97.2 |        92.8 |           46.848 |            80.2252 |         54.9832 |          76.6582 |              84.3023 |              70.6366 | 70.0627 | 56.338 | 53.8462 |            73.4 |\n\n\nFor more information, see: [Model Recycling](https://ibm.github.io/model-recycling/)\n",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":247577856,\"storage_bytes\":7894822589,\"files_count\":12,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"T5ForConditionalGeneration\"],\"model_type\":\"t5\",\"tokenizer_config\":{\"eos_token\":\"</s>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:google-research:t5x\",\"source_url\":\"https://github.com/google-research/t5x\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:t5x\",\"source_url\":\"https://github.com/google-research/t5x\"},{\"type\":\"has_code\",\"target_id\":\"github:google-research:t5x\",\"source_url\":\"https://github.com/google-research/t5x\"},{\"type\":\"has_code\",\"target_id\":\"github:google:jax\",\"source_url\":\"https://github.com/google/jax\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.11416\",\"source_url\":\"https://arxiv.org/abs/2210.11416\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.09700\",\"source_url\":\"https://arxiv.org/abs/1910.09700\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "f645454075588cc7a5356d39eb07e1d4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/flan-t5-base\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:dreamlike-art:dreamlike-diffusion-1.0",
    "name": "dreamlike-diffusion-1.0",
    "author": "dreamlike-art",
    "description": "--- language: - en license: other tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - art - artistic - diffusers inference: false --- Use the same prompts as you would for SD 1.5. Add **dreamlikeart** if the artstyle is too weak. Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio. Use slightly higher resolution for better results: ...",
    "tags": [
      "diffusers",
      "safetensors",
      "stable-diffusion",
      "stable-diffusion-diffusers",
      "text-to-image",
      "art",
      "artistic",
      "en",
      "license:other",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 1025,
    "downloads": 6445,
    "source": "huggingface",
    "source_url": "https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- art\n- artistic\n- diffusers\ninference: false\n---\n\n# Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by [dreamlike.art](https://dreamlike.art/).\n\n# If you want to use dreamlike models on your website/app/etc., check the license at the bottom first!  \n\nUse the same prompts as you would for SD 1.5. Add **dreamlikeart** if the artstyle is too weak.    \nNon-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio.  \nUse slightly higher resolution for better results: 640x640px, 512x768px, 768x512px, etc.  \n\n# We've just released Dreamlike Photoreal 2.0, check it out!\n\n[https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/preview1.jpg\" style=\"max-width: 400px;\" width=\"100%\"/>\n\n### Examples\n\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/preview.jpg\" style=\"max-width: 800px;\" width=\"100%\"/>\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/1.jpg\" style=\"max-width: 800px;\" width=\"100%\"/>\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/2.jpg\" style=\"max-width: 800px;\" width=\"100%\"/>\n\n### dreamlike.art\n\nYou can use this model for free on [dreamlike.art](https://dreamlike.art/)!\n\n<img src=\"https://huggingface.co/dreamlike-art/dreamlike-photoreal-1.0/resolve/main/dreamlike.jpg\" style=\"max-width: 1000px;\" width=\"100%\"/>\n\n### Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run dreamlike-diffusion-1.0:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/dreamlike-diffusion-1.0)\n\n### CompVis\n\n[Download dreamlike-diffusion-1.0.ckpt (2.13GB)](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt)\n\n### ðŸ§¨ Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion Pipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"dreamlike-art/dreamlike-diffusion-1.0\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"./result.jpg\")\n```\n\n# License\n\nThis model is licesed under a **modified** CreativeML OpenRAIL-M license.\n\n- **You can't host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. If you want to, please email us at contact@dreamlike.art**\n- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**  \n- **You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**\n- **You are free to use the outputs of the model or the outputs of the model's derivatives for commercial purposes in teams of 10 or less**\n- You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/blob/main/LICENSE.md\n",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":23223080959,\"files_count\":24,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "ebf55764034102e987fe0ee7b0a74ebb",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tiiuae:falcon-7b-instruct",
    "name": "falcon-7b-instruct",
    "author": "tiiuae",
    "description": "--- datasets: - tiiuae/falcon-refinedweb language: - en inference: true new_version: tiiuae/falcon-11B widget: - text: \"Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\" example_title: \"Abu Dhabi Trip\" - text: \"What's the Everett interpretation of quantum mechanics?\" example_title: \"Q/A: Quantum & Answers\" - text: \"Give me a list of the top 10 dive sites you would recommend around the world.\" example_title: \"Diving Top 10\" - text: \"Can you tell me more about deep-water soloing?\" ...",
    "tags": [
      "transformers",
      "pytorch",
      "coreml",
      "safetensors",
      "falcon",
      "text-generation",
      "conversational",
      "custom_code",
      "en",
      "dataset:tiiuae/falcon-refinedweb",
      "arxiv:2205.14135",
      "arxiv:1911.02150",
      "arxiv:2005.14165",
      "arxiv:2104.09864",
      "arxiv:2306.01116",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "deploy:azure",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1024,
    "downloads": 49810,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tiiuae/falcon-7b-instruct",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\ndatasets:\n  - tiiuae/falcon-refinedweb\nlanguage:\n  - en\ninference: true\nnew_version: tiiuae/falcon-11B\nwidget:\n  - text: \"Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\"\n    example_title: \"Abu Dhabi Trip\"\n  - text: \"What's the Everett interpretation of quantum mechanics?\"\n    example_title: \"Q/A: Quantum & Answers\"\n  - text: \"Give me a list of the top 10 dive sites you would recommend around the world.\"\n    example_title: \"Diving Top 10\"\n  - text: \"Can you tell me more about deep-water soloing?\"\n    example_title: \"Extreme sports\"\n  - text: \"Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?\"\n    example_title: \"Twitter Helper\"\n  - text: \"What are the responsabilities of a Chief Llama Officer?\"\n    example_title: \"Trendy Jobs\"\nlicense: apache-2.0\n---\n\n# âœ¨ Falcon-7B-Instruct\n\n**Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon ðŸ˜Š.*\n\nðŸ¤— To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-7B-Instruct?\n\n* **You are looking for a ready-to-use chat/instruct model based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).**\n* **Falcon-7B is a strong base model, outperforming comparable open-source models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n\nðŸ’¬ **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b). \n\nðŸ”¥ **Looking for an even more powerful model?** [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) is Falcon-7B-Instruct's big brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\nðŸ’¥ **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 16GB of memory** to swiftly run inference with Falcon-7B-Instruct.\n\n\n# Model Card for Falcon-7B-Instruct\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English and French;\n- **License:** Apache 2.0;\n- **Finetuned from model:** [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nFalcon-7B-Instruct has been finetuned on a mixture of instruct and chat datasets.\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-7B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-7B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-7B-Instruct was finetuned on a 250M tokens mixture of instruct/chat datasets.\n\n| **Data source**    | **Fraction** | **Tokens** | **Description**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [Bai ze](https://github.com/project-baize/baize-chatbot) | 65%          | 164M     | chat                 |\n| [GPT4All](https://github.com/nomic-ai/gpt4all)              | 25%           | 62M       | instruct                                  |\n| [GPTeacher](https://github.com/teknium1/GPTeacher)      | 5%           | 11M        | instruct |\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 5%          | 13M     | massive web crawl                 |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\nNote that this model variant is not optimized for NLP benchmarks. \n\n\n## Technical Specifications \n\nFor more information about pretraining, see [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).\n\n### Model Architecture and Objective\n\nFalcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 32        |                                        |\n| `d_model`          | 4544      | Increased to compensate for multiquery                                       |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-7B-Instruct was trained on AWS SageMaker, on 32 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-7B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* ðŸ˜Š. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the ðŸ““ [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-7B-Instruct is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":7217189760,\"storage_bytes\":56610389723,\"files_count\":19,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"FalconForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_falcon.FalconConfig\",\"AutoModel\":\"modeling_falcon.FalconModel\",\"AutoModelForSequenceClassification\":\"modeling_falcon.FalconForSequenceClassification\",\"AutoModelForTokenClassification\":\"modeling_falcon.FalconForTokenClassification\",\"AutoModelForQuestionAnswering\":\"modeling_falcon.FalconForQuestionAnswering\",\"AutoModelForCausalLM\":\"modeling_falcon.FalconForCausalLM\"},\"model_type\":\"falcon\",\"tokenizer_config\":{\"eos_token\":\"<|endoftext|>\",\"chat_template\":\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = '' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{{ system_message.strip() }}{% endif %}{% if message['role'] == 'user' %}{{ '\\n\\nUser: ' + message['content'].strip().replace('\\r\\n', '\\n').replace('\\n\\n', '\\n') }}{% elif message['role'] == 'assistant' %}{{ '\\n\\nAssistant: ' + message['content'].strip().replace('\\r\\n', '\\n').replace('\\n\\n', '\\n') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\\n\\nAssistant:' }}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:StableLM\",\"source_url\":\"https://github.com/Stability-AI/StableLM\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:text-generation-inference\",\"source_url\":\"https://github.com/huggingface/text-generation-inference\"},{\"type\":\"has_code\",\"target_id\":\"github:project-baize:baize-chatbot\",\"source_url\":\"https://github.com/project-baize/baize-chatbot\"},{\"type\":\"has_code\",\"target_id\":\"github:nomic-ai:gpt4all\",\"source_url\":\"https://github.com/nomic-ai/gpt4all\"},{\"type\":\"has_code\",\"target_id\":\"github:teknium1:GPTeacher\",\"source_url\":\"https://github.com/teknium1/GPTeacher\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2205.14135\",\"source_url\":\"https://arxiv.org/abs/2205.14135\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.02150\",\"source_url\":\"https://arxiv.org/abs/1911.02150\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2005.14165\",\"source_url\":\"https://arxiv.org/abs/2005.14165\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.09864\",\"source_url\":\"https://arxiv.org/abs/2104.09864\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2306.01116\",\"source_url\":\"https://arxiv.org/abs/2306.01116\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "265785c21586b0e60598f4865e1d797b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tiiuae/falcon-7b-instruct\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:google:gemma-3-4b-it",
    "name": "gemma-3-4b-it",
    "author": "google",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "gemma3",
      "any-to-any",
      "image-text-to-text",
      "conversational",
      "arxiv:1905.07830",
      "arxiv:1905.10044",
      "arxiv:1911.11641",
      "arxiv:1904.09728",
      "arxiv:1705.03551",
      "arxiv:1911.01547",
      "arxiv:1907.10641",
      "arxiv:1903.00161",
      "arxiv:2009.03300",
      "arxiv:2304.06364",
      "arxiv:2103.03874",
      "arxiv:2110.14168",
      "arxiv:2311.12022",
      "arxiv:2108.07732",
      "arxiv:2107.03374",
      "arxiv:2210.03057",
      "arxiv:2106.03193",
      "arxiv:1910.11856",
      "arxiv:2502.12404",
      "arxiv:2502.21228",
      "arxiv:2404.16816",
      "arxiv:2104.12756",
      "arxiv:2311.16502",
      "arxiv:2203.10244",
      "arxiv:2404.12390",
      "arxiv:1810.12440",
      "arxiv:1908.02660",
      "arxiv:2312.11805",
      "base_model:google/gemma-3-4b-pt",
      "base_model:finetune:google/gemma-3-4b-pt",
      "license:gemma",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 1024,
    "downloads": 1037748,
    "source": "huggingface",
    "source_url": "https://huggingface.co/google/gemma-3-4b-it",
    "image_url": null,
    "type": "model",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":4300079472,\"storage_bytes\":50506149149,\"files_count\":15,\"spaces_count\":100,\"gated\":\"manual\",\"private\":false,\"config\":{\"architectures\":[\"Gemma3ForConditionalGeneration\"],\"model_type\":\"gemma3\",\"processor_config\":{\"chat_template\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\"},\"tokenizer_config\":{\"bos_token\":\"<bos>\",\"chat_template\":\"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- if messages[0]['content'] is string -%}\\n        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\",\"eos_token\":\"<eos>\",\"pad_token\":\"<pad>\",\"unk_token\":\"<unk>\",\"use_default_system_prompt\":false}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.07830\",\"source_url\":\"https://arxiv.org/abs/1905.07830\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1905.10044\",\"source_url\":\"https://arxiv.org/abs/1905.10044\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.11641\",\"source_url\":\"https://arxiv.org/abs/1911.11641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1904.09728\",\"source_url\":\"https://arxiv.org/abs/1904.09728\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1705.03551\",\"source_url\":\"https://arxiv.org/abs/1705.03551\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1911.01547\",\"source_url\":\"https://arxiv.org/abs/1911.01547\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1907.10641\",\"source_url\":\"https://arxiv.org/abs/1907.10641\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1903.00161\",\"source_url\":\"https://arxiv.org/abs/1903.00161\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2009.03300\",\"source_url\":\"https://arxiv.org/abs/2009.03300\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2304.06364\",\"source_url\":\"https://arxiv.org/abs/2304.06364\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2103.03874\",\"source_url\":\"https://arxiv.org/abs/2103.03874\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2110.14168\",\"source_url\":\"https://arxiv.org/abs/2110.14168\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.12022\",\"source_url\":\"https://arxiv.org/abs/2311.12022\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2108.07732\",\"source_url\":\"https://arxiv.org/abs/2108.07732\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2107.03374\",\"source_url\":\"https://arxiv.org/abs/2107.03374\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2210.03057\",\"source_url\":\"https://arxiv.org/abs/2210.03057\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2106.03193\",\"source_url\":\"https://arxiv.org/abs/2106.03193\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1910.11856\",\"source_url\":\"https://arxiv.org/abs/1910.11856\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.12404\",\"source_url\":\"https://arxiv.org/abs/2502.12404\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2502.21228\",\"source_url\":\"https://arxiv.org/abs/2502.21228\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.16816\",\"source_url\":\"https://arxiv.org/abs/2404.16816\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2104.12756\",\"source_url\":\"https://arxiv.org/abs/2104.12756\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2311.16502\",\"source_url\":\"https://arxiv.org/abs/2311.16502\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2203.10244\",\"source_url\":\"https://arxiv.org/abs/2203.10244\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2404.12390\",\"source_url\":\"https://arxiv.org/abs/2404.12390\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1810.12440\",\"source_url\":\"https://arxiv.org/abs/1810.12440\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:1908.02660\",\"source_url\":\"https://arxiv.org/abs/1908.02660\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2312.11805\",\"source_url\":\"https://arxiv.org/abs/2312.11805\"}]",
    "canonical_id": null,
    "license_spdx": "Gemma",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "9b7b15d46be05318e2a91092e3586bc0",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/google/gemma-3-4b-it\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:openbmb:minicpm-v-4_5",
    "name": "MiniCPM-V-4_5",
    "author": "openbmb",
    "description": "--- pipeline_tag: image-text-to-text datasets: - openbmb/RLAIF-V-Dataset library_name: transformers language: - multilingual tags: - minicpm-v - vision - ocr - multi-image - video - custom_code license: apache-2.0 --- <h1>A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</h1> GitHub | CookBook | Technical Report | Demo </a> **MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-4...",
    "tags": [
      "transformers",
      "safetensors",
      "minicpmv",
      "feature-extraction",
      "minicpm-v",
      "vision",
      "ocr",
      "multi-image",
      "video",
      "custom_code",
      "image-text-to-text",
      "conversational",
      "multilingual",
      "dataset:openbmb/rlaif-v-dataset",
      "arxiv:2509.18154",
      "arxiv:2403.11703",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 1020,
    "downloads": 49535,
    "source": "huggingface",
    "source_url": "https://huggingface.co/openbmb/MiniCPM-V-4_5",
    "image_url": null,
    "type": "dataset",
    "body_content": "---\npipeline_tag: image-text-to-text\ndatasets:\n- openbmb/RLAIF-V-Dataset\nlibrary_name: transformers\nlanguage:\n- multilingual\ntags:\n- minicpm-v\n- vision\n- ocr\n- multi-image\n- video\n- custom_code\nlicense: apache-2.0\n---\n\n<h1>A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</h1>\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-o) | [CookBook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) | [Technical Report](https://huggingface.co/papers/2509.18154) | [Demo](http://101.126.42.235:30910/) </a> \n\n\n\n## MiniCPM-V 4.5\n\n**MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:\n\n- ðŸ”¥ **State-of-the-art Vision-Language Capability.**\n  MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B** for vision-language capabilities, making it the most performant MLLM under 30B parameters.\n\n- ðŸŽ¬ **Efficient High-FPS and Long Video Understanding.** Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.\n\n- âš™ï¸ **Controllable Hybrid Fast/Deep Thinking.** MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.\n\n- ðŸ’ª **Strong OCR, Document Parsing and Others.**\nBased on [LLaVA-UHD](https://arxiv.org/pdf/2403.11703) architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x less visual tokens than most MLLMs. The model achieves **leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5**. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, outperforming GPT-4o-latest on MMHal-Bench, and supports **multilingual capabilities** in more than 30 languages.\n\n- ðŸ’« **Easy Usage.**\nMiniCPM-V 4.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/tc-mb/llama.cpp/blob/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md) and [ollama](https://github.com/tc-mb/ollama/tree/MIniCPM-V) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-V-4_5-int4), [GGUF](https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf) and [AWQ](https://github.com/tc-mb/AutoAWQ) format quantized models in 16 sizes, (3) [SGLang](https://github.com/tc-mb/sglang/tree/main) and [vLLM](#efficient-inference-with-llamacpp-ollama-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with [Transformers](https://github.com/tc-mb/transformers/tree/main) and [LLaMA-Factory](./docs/llamafactory_train_and_infer.md), (5) quick [local WebUI demo](#chat-with-our-demo-on-gradio), (6) optimized [local iOS app](https://github.com/tc-mb/MiniCPM-o-demo-iOS) on iPhone and iPad, and (7) online web demo on [server](http://101.126.42.235:30910/). See our [Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) for full usages!\n\n\n### Key Techniques\n\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpm-v-4dot5-framework.png\" , width=100%>\n</div>\n\n- **Architechture: Unified 3D-Resampler for High-density Video Compression.** MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96Ã— compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.\n\n- **Pre-training: Unified Learning for OCR and Knowledge from Documents.** Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.\n\n- **Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.** MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with [RLPR](https://github.com/OpenBMB/RLPR) and [RLAIF-V](https://github.com/RLHF-V/RLAIF-V), it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.\n\n### Evaluation\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/radar_minicpm_v45.png\", width=60%>\n</div>\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv_4_5_evaluation_result.png\" , width=100%>\n</div>\n\n### Inference Efficiency \n\n**OpenCompass**\n<div align=\"left\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n            <tr>\n              <th align=\"left\">Model</th>\n              <th>Size</th>\n              <th>Avg Score â†‘</th>\n              <th>Total Inference Time â†“</th>\n            </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4.1V-9B-Thinking</td>\n            <td>10.3B</td>\n            <td>76.6</td>\n            <td>17.5h</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiMo-VL-7B-RL</td>\n            <td>8.3B</td>\n            <td>76.4</td>\n            <td>11h</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 4.5</td>\n            <td>8.7B</td>\n            <td><b>77.0</td>\n            <td><b>7.5h</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\n**Video-MME**\n\n<div align=\"left\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n          <tr>\n              <th align=\"left\">Model</th>\n              <th>Size</th>\n              <th>Avg Score â†‘</th>\n              <th>Total Inference Time â†“</th>\n              <th>GPU Mem â†“</th>\n          </tr>\n    </thead>\n    <tbody align=\"center\">\n          <tr>\n              <td nowrap=\"nowrap\" align=\"left\">Qwen2.5-VL-7B-Instruct</td>\n              <td>8.3B</td>\n              <td>71.6</td>\n              <td>3h</td>\n              <td>60G</td>\n          </tr>\n          <tr>\n              <td nowrap=\"nowrap\" align=\"left\">GLM-4.1V-9B-Thinking</td>\n              <td>10.3B</td>\n              <td><b>73.6</td>\n              <td>2.63h</td>\n              <td>32G</td>\n          </tr>\n          <tr>\n              <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 4.5</td>\n              <td>8.7B</td>\n              <td>73.5</td>\n              <td><b>0.26h</td>\n              <td><b>28G</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\nBoth Video-MME and OpenCompass were evaluated using 8Ã—A100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.\n\n### Examples\n\n<div align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=Cn23FujYMMU\"><img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/MiniCPM-V%204.5-8.26_img.jpeg\", width=70%></a>\n</div>\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case1.png\" alt=\"en_case1\" style=\"margin-bottom: 5px;\">\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case2.png\" alt=\"en_case2\" style=\"margin-bottom: 5px;\">\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case3.jpeg\" alt=\"en_case3\" style=\"margin-bottom: 5px;\">\n</div>\n\nWe deploy MiniCPM-V 4.5 on iPad M4 with [iOS demo](https://github.com/tc-mb/MiniCPM-o-demo-iOS). The demo video is the raw screen recording without editing.\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_en_handwriting.gif\" width=\"45%\" style=\"display: inline-block; margin: 0 10px;\"/>\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_en_cot.gif\" width=\"45%\" style=\"display: inline-block; margin: 0 10px;\"/>\n</div>\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_cn_handwriting.gif\" width=\"45%\" style=\"display: inline-block; margin: 0 10px;\"/>\n  <img src=\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_cn_travel.gif\" width=\"45%\" style=\"display: inline-block; margin: 0 10px;\"/>\n</div> \n\n## Framework Support Matrix\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Framework</th>\n      <th>Cookbook Link</th>\n      <th>Upstream PR</th>\n      <th>Supported since(branch)</th>\n      <th>Supported since(release)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"2\">Edge(On-device)</td>\n      <td>Llama.cpp</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/llama.cpp/minicpm-v4_5_llamacpp.md\">Llama.cpp Doc</a></td>\n      <td><a href=\"https://github.com/ggml-org/llama.cpp/pull/15575\">#15575</a>(2025-08-26)</td>\n      <td>master(2025-08-26)</td>\n      <td><a href=\"https://github.com/ggml-org/llama.cpp/releases/tag/b6282\">b6282</a></td>\n    </tr>\n    <tr>\n      <td>Ollama</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/ollama/minicpm-v4_5_ollama.md\">Ollama Doc</a></td>\n      <td><a href=\"https://github.com/ollama/ollama/pull/12078\">#12078</a>(2025-08-26)</td>\n      <td>Merging</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\">Serving(Cloud)</td>\n      <td>vLLM</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/vllm/minicpm-v4_5_vllm.md\">vLLM Doc</a></td>\n      <td><a href=\"https://github.com/vllm-project/vllm/pull/23586\">#23586</a>(2025-08-26)</td>\n      <td>main(2025-08-27)</td>\n      <td><a href=\"https://github.com/vllm-project/vllm/releases/tag/v0.10.2\">v0.10.2</td>\n    </tr>\n    <tr>\n      <td>SGLang</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/sglang/MiniCPM-v4_5_sglang.md\">SGLang Doc</a></td>\n      <td><a href=\"https://github.com/sgl-project/sglang/pull/9610\">#9610</a>(2025-08-26)</td>\n      <td>Merging</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td>Finetuning</td>\n      <td>LLaMA-Factory</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/finetune/finetune_llamafactory.md\">LLaMA-Factory Doc</a></td>\n      <td><a href=\"https://github.com/hiyouga/LLaMA-Factory/pull/9022\">#9022</a>(2025-08-26)</td>\n      <td>main(2025-08-26)</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td rowspan=\"3\">Quantization</td>\n      <td>GGUF</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/gguf/minicpm-v4_5_gguf_quantize.md\">GGUF Doc</a></td>\n      <td>â€”</td>\n      <td>â€”</td>\n      <td>â€”</td>\n    </tr>\n    <tr>\n      <td>BNB</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/bnb/minicpm-v4_5_bnb_quantize.md\">BNB Doc</a></td>\n      <td>â€”</td>\n      <td>â€”</td>\n      <td>â€”</td>\n    </tr>\n    <tr>\n      <td>AWQ</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/awq/minicpm-v4_5_awq_quantize.md\">AWQ Doc</a></td>\n      <td>â€”</td>\n      <td>â€”</td>\n      <td>â€”</td>\n    </tr>\n    <tr>\n      <td>Demos</td>\n      <td>Gradio Demo</td>\n      <td><a href=\"https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/gradio/README.md\">Gradio Demo Doc</a></td>\n      <td>â€”</td>\n      <td>â€”</td>\n      <td>â€”</td>\n    </tr>\n  </tbody>\n </table>\n \n> Note: If you'd like us to prioritize support for another open-source framework, please let us know via this [short form](https://docs.google.com/forms/d/e/1FAIpQLSdyTUrOPBgWqPexs3ORrg47ZcZ1r4vFQaA4ve2iA7L9sMfMWw/viewform).\n\n## Usage\n\nIf you wish to enable thinking mode, provide the argument `enable_thinking=True` to the chat function.\n\n#### Chat with Image\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(100)\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6\n\nimage = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')\n\nenable_thinking=False # If `enable_thinking=True`, the thinking mode is enabled.\nstream=True # If `stream=True`, the answer is string\n\n# First round chat \nquestion = \"What is the landform in the picture?\"\nmsgs = [{'role': 'user', 'content': [image, question]}]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    enable_thinking=enable_thinking,\n    stream=True\n)\n\ngenerated_text = \"\"\nfor new_text in answer:\n    generated_text += new_text\n    print(new_text, flush=True, end='')\n\n# Second round chat, pass history context of multi-turn conversation\nmsgs.append({\"role\": \"assistant\", \"content\": [generated_text]})\nmsgs.append({\"role\": \"user\", \"content\": [\"What should I pay attention to when traveling here?\"]})\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    stream=True\n)\n\ngenerated_text = \"\"\nfor new_text in answer:\n    generated_text += new_text\n    print(new_text, flush=True, end='')\n```\n\nYou will get the following output:\n\n```shell\n# round1\nThe landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleysâ€”exactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.\n\nThis scene closely resembles the famous karst landscape of Guilin and Yangshuo in Chinaâ€™s Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.\n\n# round2\nWhen traveling to a karst landscape like this, here are some important tips:\n\n1. Wear comfortable shoes: The terrain can be uneven and hilly.\n2. Bring water and snacks for energy during hikes or boat rides.\n3. Protect yourself from the sun with sunscreen, hats, and sunglassesâ€”especially since youâ€™ll likely spend time outdoors exploring scenic spots.\n4. Respect local customs and nature regulations by not littering or disturbing wildlife.\n\nBy following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilinâ€™s karst mountains.\n```\n\n\n#### Chat with Video\n\n```python\n## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. \n# To achieve this, you need to organize your video data into two corresponding sequences: \n#   frames: List[Image]\n#   temporal_ids: List[List[Int]].\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\nfrom scipy.spatial import cKDTree\nimport numpy as np\nimport math\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6\n\nMAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.\nMAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6\nTIME_SCALE = 0.1 \n\ndef map_to_nearest_scale(values, scale):\n    tree = cKDTree(np.asarray(scale)[:, None])\n    _, indices = tree.query(np.asarray(values)[:, None])\n    return np.asarray(scale)[indices]\n\n\ndef group_array(arr, size):\n    return [arr[i:i+size] for i in range(0, len(arr), size)]\n\ndef encode_video(video_path, choose_fps=3, force_packing=None):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n    vr = VideoReader(video_path, ctx=cpu(0))\n    fps = vr.get_avg_fps()\n    video_duration = len(vr) / fps\n        \n    if choose_fps * int(video_duration) <= MAX_NUM_FRAMES:\n        packing_nums = 1\n        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))\n        \n    else:\n        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)\n        if packing_nums <= MAX_NUM_PACKING:\n            choose_frames = round(video_duration * choose_fps)\n        else:\n            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)\n            packing_nums = MAX_NUM_PACKING\n\n    frame_idx = [i for i in range(0, len(vr))]      \n    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))\n\n    if force_packing:\n        packing_nums = min(force_packing, MAX_NUM_PACKING)\n    \n    print(video_path, ' duration:', video_duration)\n    print(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')\n    \n    frames = vr.get_batch(frame_idx).asnumpy()\n\n    frame_idx_ts = frame_idx / fps\n    scale = np.arange(0, video_duration, TIME_SCALE)\n\n    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE\n    frame_ts_id = frame_ts_id.astype(np.int32)\n\n    assert len(frames) == len(frame_ts_id)\n\n    frames = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]\n    frame_ts_id_group = group_array(frame_ts_id, packing_nums)\n    \n    return frames, frame_ts_id_group\n\n\nvideo_path=\"video_test.mp4\"\nfps = 5 # fps for video\nforce_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.\nframes, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)\n\nquestion = \"Describe the video\"\nmsgs = [\n    {'role': 'user', 'content': frames + [question]}, \n]\n\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    use_image_id=False,\n    max_slice_nums=1,\n    temporal_ids=frame_ts_id_group\n)\nprint(answer)\n```\n\n#### Chat with multiple images\n<details>\n<summary> Click to show Python code running MiniCPM-V 4.5 with multiple images input. </summary>\n  \n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)\n\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\n\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n\n#### In-context few-shot learning\n<details>\n<summary> Click to view Python code running MiniCPM-V 4.5 with few-shot input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16)\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)\n\nquestion = \"production date\" \nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\n\nmsgs = [\n    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n    {'role': 'user', 'content': [image_test, question]}\n]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n\n## License\n#### Model License\n* The MiniCPM-o/V model weights and code are open-sourced under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM-V/blob/main/LICENSE) license.\n* To help us better understand and support our users, we would deeply appreciate it if you could consider optionally filling out a brief registration [\"questionnaire\"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g).\n\n#### Statement\n* As an LMM, MiniCPM-V 4.5 generates contents by learning a large amount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V 4.5 does not represent the views and positions of the model developers\n* We will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n## Key Techniques and Other Multimodal Projects\n\nðŸ‘ Welcome to explore key techniques of MiniCPM-V 4.5 and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLPR](https://github.com/OpenBMB/RLPR) |  [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD)  | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n## Citation\n\nIf you find our work helpful, please consider citing our papers ðŸ“ and liking this project â¤ï¸ï¼\n\n```bib\n@misc{yu2025minicpmv45cookingefficient,\n      title={MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe}, \n      author={Tianyu Yu and Zefan Wang and Chongyi Wang and Fuwei Huang and Wenshuo Ma and Zhihui He and Tianchi Cai and Weize Chen and Yuxiang Huang and Yuanqian Zhao and Bokai Xu and Junbo Cui and Yingjing Xu and Liqing Ruan and Luoyuan Zhang and Hanyu Liu and Jingkun Tang and Hongyuan Liu and Qining Guo and Wenhao Hu and Bingxiang He and Jie Zhou and Jie Cai and Ji Qi and Zonghao Guo and Chi Chen and Guoyang Zeng and Yuxuan Li and Ganqu Cui and Ning Ding and Xu Han and Yuan Yao and Zhiyuan Liu and Maosong Sun},\n      year={2025},\n      eprint={2509.18154},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2509.18154}, \n}\n\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={Nat Commun 16, 5509 (2025)},\n  year={2025}\n}\n\n```",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":8695895280,\"storage_bytes\":17403328052,\"files_count\":23,\"spaces_count\":14,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"MiniCPMV\"],\"auto_map\":{\"AutoConfig\":\"configuration_minicpm.MiniCPMVConfig\",\"AutoModel\":\"modeling_minicpmv.MiniCPMV\",\"AutoModelForCausalLM\":\"modeling_minicpmv.MiniCPMV\"},\"model_type\":\"minicpmv\",\"tokenizer_config\":{\"bos_token\":\"<|im_start|>\",\"chat_template\":\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in message.content %}\\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\\\n') %}\\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n    {%- if enable_thinking is defined and enable_thinking is true %}\\n        {{- '<think>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:MiniCPM-o\",\"source_url\":\"https://github.com/OpenBMB/MiniCPM-o\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:RLHF-V:RLAIF-V\",\"source_url\":\"https://github.com/RLHF-V/RLAIF-V\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:VisCPM\",\"source_url\":\"https://github.com/OpenBMB/VisCPM\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:llama.cpp\",\"source_url\":\"https://github.com/tc-mb/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:ollama\",\"source_url\":\"https://github.com/tc-mb/ollama\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:AutoAWQ\",\"source_url\":\"https://github.com/tc-mb/AutoAWQ\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:sglang\",\"source_url\":\"https://github.com/tc-mb/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:transformers\",\"source_url\":\"https://github.com/tc-mb/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:MiniCPM-o-demo-iOS\",\"source_url\":\"https://github.com/tc-mb/MiniCPM-o-demo-iOS\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:RLPR\",\"source_url\":\"https://github.com/OpenBMB/RLPR\"},{\"type\":\"has_code\",\"target_id\":\"github:RLHF-V:RLAIF-V\",\"source_url\":\"https://github.com/RLHF-V/RLAIF-V\"},{\"type\":\"has_code\",\"target_id\":\"github:tc-mb:MiniCPM-o-demo-iOS\",\"source_url\":\"https://github.com/tc-mb/MiniCPM-o-demo-iOS\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:ggml-org:llama.cpp\",\"source_url\":\"https://github.com/ggml-org/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:ggml-org:llama.cpp\",\"source_url\":\"https://github.com/ggml-org/llama.cpp\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:ollama:ollama\",\"source_url\":\"https://github.com/ollama/ollama\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:vllm-project:vllm\",\"source_url\":\"https://github.com/vllm-project/vllm\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:hiyouga:LLaMA-Factory\",\"source_url\":\"https://github.com/hiyouga/LLaMA-Factory\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenSQZ:MiniCPM-V-CookBook\",\"source_url\":\"https://github.com/OpenSQZ/MiniCPM-V-CookBook\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:MiniCPM-V\",\"source_url\":\"https://github.com/OpenBMB/MiniCPM-V\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:VisCPM\",\"source_url\":\"https://github.com/OpenBMB/VisCPM\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBMB:RLPR\",\"source_url\":\"https://github.com/OpenBMB/RLPR\"},{\"type\":\"has_code\",\"target_id\":\"github:RLHF-V:RLHF-V\",\"source_url\":\"https://github.com/RLHF-V/RLHF-V\"},{\"type\":\"has_code\",\"target_id\":\"github:thunlp:LLaVA-UHD\",\"source_url\":\"https://github.com/thunlp/LLaVA-UHD\"},{\"type\":\"has_code\",\"target_id\":\"github:RLHF-V:RLAIF-V\",\"source_url\":\"https://github.com/RLHF-V/RLAIF-V\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2509.18154\",\"source_url\":\"https://arxiv.org/abs/2509.18154\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2403.11703\",\"source_url\":\"https://arxiv.org/abs/2403.11703\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "6b80e37d4354664d062928ca5f73c007",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/openbmb/MiniCPM-V-4_5\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:adept:fuyu-8b",
    "name": "fuyu-8b",
    "author": "adept",
    "description": "--- license: cc-by-nc-4.0 --- Weâ€™re releasing Fuyu-8B, a small version of the multimodal model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because: 1. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy. 2. Itâ€™s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answe...",
    "tags": [
      "transformers",
      "safetensors",
      "fuyu",
      "any-to-any",
      "license:cc-by-nc-4.0",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "any-to-any",
    "likes": 1014,
    "downloads": 48562,
    "source": "huggingface",
    "source_url": "https://huggingface.co/adept/fuyu-8b",
    "image_url": "https://huggingface.co/adept/fuyu-8b/resolve/main/architecture.png",
    "type": "model",
    "body_content": "---\nlicense: cc-by-nc-4.0\n---\n# Fuyu-8B Model Card\n\nWeâ€™re releasing Fuyu-8B, a small version of the multimodal model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:\n1. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.\n2. Itâ€™s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.\n3. Itâ€™s fast - we can get responses for large images in less than 100 milliseconds.\n4. Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.\n\nPlease note that **the model we have released is a base model. We expect you to need to finetune the model for specific use cases like verbose captioning or multimodal chat.** In our experience, the model responds well to few-shotting and fine-tuning for a variety of use-cases. \n\n## Model\n\n[Fuyu-8B](https://www.adept.ai/blog/fuyu-8b) is a multi-modal text and image transformer trained by [Adept AI](https://www.adept.ai/).\n\nArchitecturally, Fuyu is a vanilla decoder-only transformer - there is no image encoder. \nImage patches are instead linearly projected into the first layer of the transformer, bypassing the embedding lookup. \nWe simply treat the transformer decoder like an image transformer (albeit with no pooling and causal attention).\nSee the below diagram for more details.\n\n![architecture](architecture.png)\n\nThis simplification allows us to support arbitrary image resolutions. \nTo accomplish this, we treat the sequence of image tokens like the sequence of text tokens. \nWe remove image-specific position embeddings and feed in as many image tokens as necessary in raster-scan order. \nTo tell the model when a line has broken, we simply use a special image-newline character. \nThe model can use its existing position embeddings to reason about different image sizes, and we can use images of arbitrary size at training time, removing the need for separate high and low-resolution training stages.\n\n### Model Description\n\n- **Developed by:** Adept-AI\n- **Model type:** Decoder-only multi-modal transformer model \n- **License:** [CC-BY-NC](https://creativecommons.org/licenses/by-nc/4.0/deed.en)\n- **Model Description:** This is a multi-modal model that can consume images and text and produce text. \n- **Resources for more information:** Check out our [blog post](https://www.adept.ai/blog/fuyu-8b).\n\n## Evaluation\nThough not the focus of this model, we did evaluate it on standard image understanding benchmarks:\n\n| Eval Task           | Fuyu-8B | Fuyu-Medium       | LLaVA 1.5 (13.5B) | QWEN-VL (10B) | PALI-X (55B) | PALM-e-12B | PALM-e-562B |\n| ------------------- | ------- | ----------------- | ----------------- | ------------- | ------------ | ---------- | ----------- |\n| VQAv2               | 74.2    |     77.4          | 80                | 79.5          | 86.1         | 76.2       | 80.0        |\n| OKVQA               | 60.6    |     63.1          | n/a               | 58.6          | 66.1         | 55.5       | 66.1        |\n| COCO Captions       | 141     |     138           | n/a               | n/a           | 149          | 135        | 138         |\n| AI2D                | 64.5    |     73.7          | n/a               | 62.3          | 81.2         | n/a        | n/a         |\n\n## How to Use\n\nYou can load the model and perform inference as follows:\n```python\nfrom transformers import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\nimport requests\n\n# load model and processor\nmodel_id = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id)\nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n\n# prepare inputs for the model\ntext_prompt = \"Generate a coco-style caption.\\n\"\nurl = \"https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n\n# autoregressively generate text\ngeneration_output = model.generate(**inputs, max_new_tokens=7)\ngeneration_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\nassert generation_text == ['A blue bus parked on the side of a road.']\n```\n\nN.B.: The token `|SPEAKER|` is a placeholder token for image patch embeddings, so it will show up in the model context (e.g., in the portion of `generation_output` representing the model context).\n`|NEWLINE|` is the \"image newline\" token, denoting new rows in the raster scan order input of the image patches.\n`\\x04` is the \"beginning of answer\" token.\n\nFuyu can also perform some question answering on natural images and charts/diagrams (thought fine-tuning may be required for good performance):\n```python\ntext_prompt = \"What color is the bus?\\n\"\nurl = \"https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n\ngeneration_output = model.generate(**inputs, max_new_tokens=6)\ngeneration_text = processor.batch_decode(generation_output[:, -6:], skip_special_tokens=True)\nassert generation_text == [\"The bus is blue.\\n\"]\n\n\ntext_prompt = \"What is the highest life expectancy at birth of male?\\n\"\nurl = \"https://huggingface.co/adept/fuyu-8b/resolve/main/chart.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel_inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(\"cuda:0\")\n\ngeneration_output = model.generate(**model_inputs, max_new_tokens=16)\ngeneration_text = processor.batch_decode(generation_output[:, -16:], skip_special_tokens=True)\nassert generation_text == [\"The life expectancy at birth of males in 2018 is 80.7.\\n\"]\n```\nFor best performance, it's recommended to end questions with `\\n`, as shown above!\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. \n**Because this is a raw model release, we have not added further finetuning, postprocessing or sampling strategies to control for undesirable outputs. You should expect to have to fine-tune the model for your use-case.**\n\nPossible research areas and tasks include\n\n- Applications in computer control or digital agents.\n- Research on multi-modal models generally.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- Faces and people in general may not be generated properly.\n\n### Bias\nWhile the capabilities of these models are impressive, they can also reinforce or exacerbate social biases.",
    "meta_json": "{\"pipeline_tag\":\"any-to-any\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":9408238592,\"storage_bytes\":37678352448,\"files_count\":17,\"spaces_count\":54,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"FuyuForCausalLM\"],\"model_type\":\"fuyu\",\"tokenizer_config\":{\"bos_token\":\"|ENDOFTEXT|\",\"eos_token\":\"|ENDOFTEXT|\",\"pad_token\":null,\"unk_token\":\"<unk>\",\"use_default_system_prompt\":true}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "CC-BY-NC-4.0",
    "compliance_status": "approved",
    "quality_score": 85,
    "content_hash": "1581e56ae03f606824f18ca395ff8b44",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/adept/fuyu-8b/resolve/main/architecture.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/adept/fuyu-8b\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:openbmb:minicpm-v-2_6",
    "name": "MiniCPM-V-2_6",
    "author": "openbmb",
    "description": "",
    "tags": [
      "transformers",
      "safetensors",
      "minicpmv",
      "feature-extraction",
      "minicpm-v",
      "vision",
      "ocr",
      "multi-image",
      "video",
      "custom_code",
      "image-text-to-text",
      "conversational",
      "multilingual",
      "dataset:openbmb/rlaif-v-dataset",
      "arxiv:2408.01800",
      "region:us"
    ],
    "pipeline_tag": "image-text-to-text",
    "likes": 1014,
    "downloads": 101153,
    "source": "huggingface",
    "source_url": "https://huggingface.co/openbmb/MiniCPM-V-2_6",
    "image_url": "https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/assets/radar_final.png",
    "type": "dataset",
    "body_content": "",
    "meta_json": "{\"pipeline_tag\":\"image-text-to-text\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":8099175152,\"storage_bytes\":16221778104,\"files_count\":24,\"spaces_count\":28,\"gated\":\"auto\",\"private\":false,\"config\":{\"architectures\":[\"MiniCPMV\"],\"auto_map\":{\"AutoConfig\":\"configuration_minicpm.MiniCPMVConfig\",\"AutoModel\":\"modeling_minicpmv.MiniCPMV\",\"AutoModelForCausalLM\":\"modeling_minicpmv.MiniCPMV\"},\"model_type\":\"minicpmv\",\"tokenizer_config\":{\"bos_token\":\"<|im_start|>\",\"chat_template\":\"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\"eos_token\":\"<|im_end|>\",\"pad_token\":\"<|endoftext|>\",\"unk_token\":\"<unk>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2408.01800\",\"source_url\":\"https://arxiv.org/abs/2408.01800\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 50,
    "content_hash": "28f3a737bbf64fc3387ce729dd038168",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/assets/radar_final.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/openbmb/MiniCPM-V-2_6\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-v3.1-base",
    "name": "DeepSeek-V3.1-Base",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/d...",
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v3",
      "text-generation",
      "conversational",
      "custom_code",
      "arxiv:2412.19437",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 1003,
    "downloads": 7910,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ðŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \\| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \\| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>...<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>`\n\nPrefix:\n`<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>...<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>{response}<ï½œendâ–ofâ–sentenceï½œ>`\n\nPrefix:\n`<ï½œUserï½œ>{query}<ï½œAssistantï½œ><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<ï½œbeginâ–ofâ–sentenceï½œ>{system prompt}\\n\\n{tool_description}<ï½œUserï½œ>{query}<ï½œAssistantï½œ></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>tool_call_name<ï½œtoolâ–sepï½œ>tool_call_arguments<ï½œtoolâ–callâ–endï½œ>{additional_tool_calls}<ï½œtoolâ–callsâ–endï½œ>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity's Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity's Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.1\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n    {\"role\": \"assistant\", \"content\": \"<think>Hmm</think>I am DeepSeek\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# '<ï½œbeginâ–ofâ–sentenceï½œ>You are a helpful assistant<ï½œUserï½œ>Who are you?<ï½œAssistantï½œ></think>I am DeepSeek<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ><think>'\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# '<ï½œbeginâ–ofâ–sentenceï½œ>You are a helpful assistant<ï½œUserï½œ>Who are you?<ï½œAssistantï½œ></think>I am DeepSeek<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ></think>'\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":684531386000,\"storage_bytes\":688587108667,\"files_count\":177,\"spaces_count\":14,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"DeepseekV3ForCausalLM\"],\"auto_map\":{\"AutoConfig\":\"configuration_deepseek.DeepseekV3Config\",\"AutoModel\":\"modeling_deepseek.DeepseekV3Model\",\"AutoModelForCausalLM\":\"modeling_deepseek.DeepseekV3ForCausalLM\"},\"model_type\":\"deepseek_v3\",\"quantization_config\":{\"quant_method\":\"fp8\"},\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<ï½œUserï½œ>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- if ns.is_last_user %}{{'<ï½œAssistantï½œ></think>'}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>'+ tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- else %}{{message['content'] + '<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'<ï½œtoolâ–callâ–beginï½œ>'+ tool['function']['name'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['arguments'] + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- endfor %}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}{%- if ns.is_last_user %}{{'<ï½œAssistantï½œ>'}}{%- if message['prefix'] is defined and message['prefix'] and thinking %}{{'<think>'}}  {%- else %}{{'</think>'}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message['content'] + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message['content'] -%}{%- if '</think>' in content %}{%- set content = content.split('</think>', 1)[1] -%}{%- endif %}{{content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{'<ï½œtoolâ–outputâ–beginï½œ>' + message['content'] + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{'<ï½œAssistantï½œ>'}}{%- if not thinking %}{{'</think>'}}{%- else %}{{'<think>'}}{%- endif %}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepGEMM\",\"source_url\":\"https://github.com/deepseek-ai/DeepGEMM\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V3\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V3\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2412.19437\",\"source_url\":\"https://arxiv.org/abs/2412.19437\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "6b87557a767151299f2ad5247bf1ec2d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:stabilityai:stable-video-diffusion-img2vid",
    "name": "stable-video-diffusion-img2vid",
    "author": "stabilityai",
    "description": "--- pipeline_tag: image-to-video license: other license_name: stable-video-diffusion-community license_link: LICENSE.md --- <!-- Provide a quick summary of what the model is/does. --> !row01 Stable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. Please note: For commercial use of this model, please refer to https://stability.ai/license. (SVD) Image-to-Video is a latent diffusion model trained to gene...",
    "tags": [
      "diffusers",
      "safetensors",
      "image-to-video",
      "license:other",
      "diffusers:stablevideodiffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "image-to-video",
    "likes": 1002,
    "downloads": 481671,
    "source": "huggingface",
    "source_url": "https://huggingface.co/stabilityai/stable-video-diffusion-img2vid",
    "image_url": null,
    "type": "model",
    "body_content": "---\npipeline_tag: image-to-video\nlicense: other\nlicense_name: stable-video-diffusion-community\nlicense_link: LICENSE.md\n---\n\n# Stable Video Diffusion Image-to-Video Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.gif)\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. \n\nPlease note: For commercial use of this model, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\n\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. \nThis model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size.\nWe also finetune the widely used [f8-decoder](https://huggingface.co/docs/diffusers/api/models/autoencoderkl#loading-from-the-original-format) for temporal consistency. \nFor convenience, we additionally provide the model with the \nstandard frame-wise decoder [here](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/svd_image_decoder.safetensors).\n\n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative image-to-video model\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SVD-Image-to-Video over [GEN-2](https://research.runwayml.com/gen2) and [PikaLabs](https://www.pika.art/).\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\n- The model may generate videos without motion, or very slow camera pans.\n- The model cannot be controlled through text.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for research purposes only.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models\n\n\n\n# Appendix:\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters. No explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation. No other third party was involved in the development of this model; the model was fully developed in-house at Stability AI. Training the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh. The released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos. With the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards. The information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. The released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.\nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AIâ€™s future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. For usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.",
    "meta_json": "{\"pipeline_tag\":\"image-to-video\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":32608765959,\"files_count\":19,\"spaces_count\":98,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableVideoDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:generative-models\",\"source_url\":\"https://github.com/Stability-AI/generative-models\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:generative-models\",\"source_url\":\"https://github.com/Stability-AI/generative-models\"},{\"type\":\"has_code\",\"target_id\":\"github:Stability-AI:generative-models\",\"source_url\":\"https://github.com/Stability-AI/generative-models\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "4972f0d945118055291a5246d77335aa",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:xai-org:grok-2",
    "name": "grok-2",
    "author": "xai-org",
    "description": "This repository contains the weights of Grok 2, a model trained and used at xAI in 2024. - Download the weights. You can replace with any other folder name you prefer. You might encounter some errors during the download. Please retry until the download is successful. If the download succeeds, the folder should contain **42 files** and be approximately 500 GB. - Launch a server. Install the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/ Use the command b...",
    "tags": [
      "git",
      "region:us"
    ],
    "pipeline_tag": "other",
    "likes": 996,
    "downloads": 2546,
    "source": "huggingface",
    "source_url": "https://huggingface.co/xai-org/grok-2",
    "image_url": null,
    "type": "model",
    "body_content": "# Grok 2\n\nThis repository contains the weights of Grok 2, a model trained and used at xAI in 2024.\n\n## Usage: Serving with SGLang\n\n- Download the weights. You can replace `/local/grok-2` with any other folder name you prefer.\n\n  ```\n  hf download xai-org/grok-2 --local-dir /local/grok-2\n  ```\n\n  You might encounter some errors during the download. Please retry until the download is successful.  \n  If the download succeeds, the folder should contain **42 files** and be approximately 500 GB.\n\n- Launch a server.\n\n  Install the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/\n\n  Use the command below to launch an inference server. This checkpoint is TP=8, so you will need 8 GPUs (each with > 40GB of memory).\n  ```\n  python3 -m sglang.launch_server --model /local/grok-2 --tokenizer-path /local/grok-2/tokenizer.tok.json --tp 8 --quantization fp8 --attention-backend triton\n  ```\n\n- Send a request.\n\n  This is a post-trained model, so please use the correct [chat template](https://github.com/sgl-project/sglang/blob/97a38ee85ba62e268bde6388f1bf8edfe2ca9d76/python/sglang/srt/tokenizer/tiktoken_tokenizer.py#L106).\n\n  ```\n  python3 -m sglang.test.send_one --prompt \"Human: What is your name?<|separator|>\\n\\nAssistant:\"\n  ```\n\n  You should be able to see the model output its name, Grok.\n\n  Learn more about other ways to send requests [here](https://docs.sglang.ai/basic_usage/send_request.html).\n\n## License\n\nThe weights are licensed under the [Grok 2 Community License Agreement](https://huggingface.co/xai-org/grok-2/blob/main/LICENSE).",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":539032697512,\"files_count\":44,\"spaces_count\":5,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Grok1ForCausalLM\"],\"model_type\":\"git\"}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"},{\"type\":\"has_code\",\"target_id\":\"github:sgl-project:sglang\",\"source_url\":\"https://github.com/sgl-project/sglang\"}]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 40,
    "content_hash": "b7b02a48a0d0ac223856f6f182d3b891",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/xai-org/grok-2\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:lykon:dreamshaper",
    "name": "DreamShaper",
    "author": "Lykon",
    "description": "--- language: - en license: other tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - art - artistic - diffusers - anime inference: false --- Read more about this model here: https://civitai.com/models/4384/dreamshaper Also please support by giving 5 stars and a heart, which will notify new updates. Please consider supporting me on Patreon or buy me a coffee - https://www.patreon.com/Lykon275 - https://snipfeed.co/lykon You can run this model on: - https://huggingface.co/s...",
    "tags": [
      "diffusers",
      "stable-diffusion",
      "stable-diffusion-diffusers",
      "text-to-image",
      "art",
      "artistic",
      "anime",
      "en",
      "doi:10.57967/hf/0453",
      "license:other",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 994,
    "downloads": 92385,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Lykon/DreamShaper",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- art\n- artistic\n- diffusers\n- anime\ninference: false\n---\n\n# Dream Shaper\n## Official Repository\n\nRead more about this model here: https://civitai.com/models/4384/dreamshaper\n\nAlso please support by giving 5 stars and a heart, which will notify new updates.\n\nPlease consider supporting me on Patreon or buy me a coffee\n- https://www.patreon.com/Lykon275\n- https://snipfeed.co/lykon\n\nYou can run this model on:\n- https://huggingface.co/spaces/Lykon/DreamShaper-webui\n- Mage.space, sinkin.ai and more",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":359810959620,\"files_count\":74,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 50,
    "content_hash": "53f2d23141175aada2f5fc7ab393604d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Lykon/DreamShaper\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:deepseek-ai:deepseek-r1-0528-qwen3-8b",
    "name": "DeepSeek-R1-0528-Qwen3-8B",
    "author": "deepseek-ai",
    "description": "--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\"https://github.com/d...",
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 993,
    "downloads": 504620,
    "source": "huggingface",
    "source_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
    "image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/resolve/main/figures/benchmark.png",
    "type": "model",
    "body_content": "---\r\nlicense: mit\r\nlibrary_name: transformers\r\n---\r\n# DeepSeek-R1-0528\r\n<!-- markdownlint-disable first-line-h1 -->\r\n<!-- markdownlint-disable html -->\r\n<!-- markdownlint-disable no-duplicate-header -->\r\n\r\n<div align=\"center\">\r\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\r\n</div>\r\n<hr>\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/ðŸ¤–%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\r\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n \r\n\r\n<p align=\"center\">\r\n  <a href=\"https://arxiv.org/pdf/2501.12948\"><b>Paper Link</b>ðŸ‘ï¸</a>\r\n</p>\r\n\r\n\r\n## 1. Introduction\r\n\r\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\r\n\r\n<p align=\"center\">\r\n  <img width=\"80%\" src=\"figures/benchmark.png\">\r\n</p>\r\n\r\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the modelâ€™s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\r\n\r\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\r\n\r\n## 2. Evaluation Results\r\n\r\n### DeepSeek-R1-0528\r\n For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\r\n<div align=\"center\">\r\n\r\n| Category | Benchmark (Metric)               | DeepSeek R1     | DeepSeek R1 0528\r\n|----------|----------------------------------|-----------------|---|\r\n| General  |\r\n|          | MMLU-Redux (EM)                   | 92.9            | 93.4\r\n|          | MMLU-Pro (EM)                     | 84.0            | 85.0\r\n|          | GPQA-Diamond (Pass@1)             | 71.5            | 81.0\r\n|          | SimpleQA (Correct)                | 30.1            | 27.8\r\n|          | FRAMES (Acc.)                     | 82.5            | 83.0\r\n|          | Humanity's Last Exam (Pass@1)                     | 8.5            | 17.7\r\n| Code |\r\n|          | LiveCodeBench (2408-2505) (Pass@1)        | 63.5          | 73.3\r\n|          | Codeforces-Div1 (Rating)          | 1530            | 1930\r\n|          | SWE Verified (Resolved)           | 49.2            | 57.6\r\n|          | Aider-Polyglot (Acc.)             | 53.3            | 71.6\r\n| Math |\r\n|          | AIME 2024 (Pass@1)                | 79.8            | 91.4\r\n|          | AIME 2025 (Pass@1)                     | 70.0           | 87.5\r\n|          | HMMT 2025 (Pass@1)            | 41.7 | 79.4 |\r\n|          | CNMO 2024 (Pass@1)                | 78.8            | 86.9\r\n| Tools |\r\n|          | BFCL_v3_MultiTurn (Acc)     | -            | 37.0 |\r\n|          | Tau-Bench   (Pass@1)       | -            | 53.5(Airline)/63.9(Retail)\r\n\r\n</div>\r\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\r\n\r\n### DeepSeek-R1-0528-Qwen3-8B\r\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\r\n\r\n|                                | AIME 24 | AIME 25 | HMMT Feb 25 | GPQA Diamond | LiveCodeBench (2408-2505) |\r\n|--------------------------------|---------|---------|-------------|--------------|---------------------------|\r\n| Qwen3-235B-A22B\t                | 85.7    | 81.5    | 62.5        | 71.1         | 66.5                  |\r\n| Qwen3-32B                      | 81.4    | 72.9    | -           | 68.4         | -                         |\r\n| Qwen3-8B                      | 76.0   | 67.3    | -           | 62.0       | -                         |\r\n| Phi-4-Reasoning-Plus-14B       | 81.3    | 78.0    | 53.6        | 69.3         | -          |\r\n| Gemini-2.5-Flash-Thinking-0520 | 82.3    | 72.0    | 64.2        | 82.8         | 62.3                  |\r\n| o3-mini (medium)               | 79.6    | 76.7    | 53.3        | 76.8         | 65.9                     |\r\n| DeepSeek-R1-0528-Qwen3-8B      | 86.0   | 76.3    | 61.5        | 61.1         | 60.5                      |\r\n\r\n## 3. Chat Website & API Platform\r\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in), and switch on the button \"DeepThink\"\r\n\r\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\r\n\r\n## 4. How to Run Locally\r\n\r\nPlease visit [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) repository for more information about running DeepSeek-R1-0528 locally.\r\n\r\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\r\n\r\n1. System prompt is supported now.\r\n2. It is not required to add \"\\<think\\>\\n\" at the beginning of the output to force the model into thinking pattern.\r\n\r\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.\r\n\r\n### System Prompt\r\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\r\n```\r\nè¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\r\nä»Šå¤©æ˜¯{current date}ã€‚\r\n```\r\nFor example,\r\n```\r\nè¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\r\nä»Šå¤©æ˜¯2025å¹´5æœˆ28æ—¥ï¼Œæ˜ŸæœŸä¸€ã€‚\r\n```\r\n### Temperature\r\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. \r\n### Prompts for File Uploading and Web Search\r\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\r\n```\r\nfile_template = \\\r\n\"\"\"[file name]: {file_name}\r\n[file content begin]\r\n{file_content}\r\n[file content end]\r\n{question}\"\"\"\r\n```\r\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\r\nFor Chinese query, we use the prompt:\r\n```\r\nsearch_answer_zh_template = \\\r\n'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºŽç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æžœ:\r\n{search_results}\r\nåœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æžœä¸­ï¼Œæ¯ä¸ªç»“æžœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æžœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åŽè¿”å›žå¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚\r\nåœ¨å›žç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\r\n- ä»Šå¤©æ˜¯{cur_date}ã€‚\r\n- å¹¶éžæœç´¢ç»“æžœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸Žç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æžœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚\r\n- å¯¹äºŽåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæŽ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€èŽ·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éžå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æžœæœªæä¾›çš„å†…å®¹ã€‚\r\n- å¯¹äºŽåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æžœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æžå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸Žä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºŽæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æŽ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›žç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚\r\n- å¦‚æžœå›žç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æž„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æžœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æŽ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚\r\n- å¯¹äºŽå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æžœé—®é¢˜çš„ç­”æ¡ˆéžå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚\r\n- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›žç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾Žè§‚çš„å›žç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚\r\n- ä½ çš„å›žç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›žç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚\r\n- é™¤éžç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›žç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚\r\n# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š\r\n{question}'''\r\n```\r\nFor English query, we use the prompt:\r\n```\r\nsearch_answer_en_template = \\\r\n'''# The following contents are the search results related to the user's message:\r\n{search_results}\r\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\r\nWhen responding, please keep the following points in mind:\r\n- Today is {cur_date}.\r\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\r\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\r\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\r\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\r\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\r\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\r\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\r\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\r\n# The user's message is:\r\n{question}'''\r\n```\r\n\r\n## 5. License\r\nThis code repository is licensed under [MIT License](LICENSE). The use of DeepSeek-R1 models is also subject to [MIT License](LICENSE). DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\r\n\r\n## 6. Citation\r\n```\r\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\r\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \r\n      author={DeepSeek-AI},\r\n      year={2025},\r\n      eprint={2501.12948},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.CL},\r\n      url={https://arxiv.org/abs/2501.12948}, \r\n}\r\n```\r\n\r\n## 7. Contact\r\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\r\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":8190735360,\"storage_bytes\":16381839296,\"files_count\":10,\"spaces_count\":53,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"Qwen3ForCausalLM\"],\"model_type\":\"qwen3\",\"tokenizer_config\":{\"bos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œbeginâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"eos_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"pad_token\":{\"__type\":\"AddedToken\",\"content\":\"<ï½œendâ–ofâ–sentenceï½œ>\",\"lstrip\":false,\"normalized\":true,\"rstrip\":false,\"single_word\":false},\"unk_token\":null,\"chat_template\":\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{% set content = message['content'] %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{'<ï½œUserï½œ>' + content + '<ï½œAssistantï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{% endif %}{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if content is none %}{{'<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- else %}{{content + '<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ï½œtoolâ–callâ–beginï½œ>' + tool['type'] + '<ï½œtoolâ–sepï½œ>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ï½œtoolâ–callâ–endï½œ>'}}{%- endif %}{%- endfor %}{{'<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>' + content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- set ns.is_tool = false -%}{%- else %}{{content + '<ï½œendâ–ofâ–sentenceï½œ>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>' + content + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ï½œtoolâ–outputâ–beginï½œ>' + content + '<ï½œtoolâ–outputâ–endï½œ>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ï½œtoolâ–outputsâ–endï½œ>'}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{'<ï½œAssistantï½œ>'}}{% endif %}\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-V2\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-V2\"},{\"type\":\"has_code\",\"target_id\":\"github:deepseek-ai:DeepSeek-R1\",\"source_url\":\"https://github.com/deepseek-ai/DeepSeek-R1\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2501.12948\",\"source_url\":\"https://arxiv.org/abs/2501.12948\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 100,
    "content_hash": "62a460061dad9fa94147871981de7400",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/resolve/main/figures/benchmark.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\"fetched_at\":\"2025-12-10T01:31:39.549Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:tencent:hunyuanimage-3.0",
    "name": "HunyuanImage-3.0",
    "author": "tencent",
    "description": "--- license: other license_name: tencent-hunyuan-community license_link: LICENSE pipeline_tag: text-to-image library_name: transformers --- <div align=\"center\"> <img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\"> </div> <div align=\"center\"> <img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\"> </div> <div align=\"center\"> <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage...",
    "tags": [
      "transformers",
      "safetensors",
      "hunyuan_image_3_moe",
      "text-generation",
      "text-to-image",
      "custom_code",
      "arxiv:2509.23951",
      "license:other",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 993,
    "downloads": 56137,
    "source": "huggingface",
    "source_url": "https://huggingface.co/tencent/HunyuanImage-3.0",
    "image_url": "https://huggingface.co/tencent/HunyuanImage-3.0/resolve/main/assets/banner.png",
    "type": "model",
    "body_content": "---\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: LICENSE\npipeline_tag: text-to-image\nlibrary_name: transformers\n---\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanImage-3.0 Logo\" width=\"600\">\n\n# ðŸŽ¨ HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align=\"center\">\n<img src=\"./assets/banner.png\" alt=\"HunyuanImage-3.0 Banner\" width=\"800\">\n\n</div>\n\n<div align=\"center\">\n  <a href=https://hunyuan.tencent.com/image target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=\"_blank\"><img src=https://img.shields.io/badge/ðŸ“š-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align=\"center\">\n    ðŸ‘ Join our <a href=\"./assets/WECHAT.md\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \nðŸ’» <a href=\"https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual\">Official website(å®˜ç½‘) Try our model!</a>&nbsp&nbsp\n</p>\n\n## ðŸ”¥ðŸ”¥ðŸ”¥ News\n- **September 28, 2025**: ðŸ“– **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: ðŸš€ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## ðŸ§© Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## ðŸ“‘ Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## ðŸ—‚ï¸ Contents\n- [ðŸ”¥ðŸ”¥ðŸ”¥ News](#-news)\n- [ðŸ§© Community Contributions](#-community-contributions)\n- [ðŸ“‘ Open-source Plan](#-open-source-plan)\n- [ðŸ“– Introduction](#-introduction)\n- [âœ¨ Key Features](#-key-features)\n- [ðŸ› ï¸ Dependencies and Installation](#-dependencies-and-installation)\n  - [ðŸ’» System Requirements](#-system-requirements)\n  - [ðŸ“¦ Environment Setup](#-environment-setup)\n  - [ðŸ“¥ Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [ðŸš€ Usage](#-usage)\n  - [ðŸ”¥ Quick Start with Transformers](#-quick-start-with-transformers)\n  - [ðŸ  Local Installation & Usage](#-local-installation--usage)\n  - [ðŸŽ¨ Interactive Gradio Demo](#-interactive-gradio-demo)\n- [ðŸ§± Models Cards](#-models-cards)\n- [ðŸ“ Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [ðŸ“Š Evaluation](#-evaluation)\n- [ðŸ“š Citation](#-citation)\n- [ðŸ™ Acknowledgements](#-acknowledgements)\n- [ðŸŒŸðŸš€  Github Star History](#-github-star-history)\n\n---\n\n## ðŸ“– Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align=\"center\">\n  <img src=\"./assets/framework.png\" alt=\"HunyuanImage-3.0 Framework\" width=\"90%\">\n</div>\n\n## âœ¨ Key Features\n\n* ðŸ§  **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* ðŸ† **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* ðŸŽ¨ **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we've achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* ðŸ’­ **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user intent, automatically elaborating on sparse prompts with contextually appropriate details to produce superior, more complete visual outputs.\n\n\n## ðŸ› ï¸ Dependencies and Installation\n\n### ðŸ’» System Requirements\n\n* ðŸ–¥ï¸ **Operating System:** Linux\n* ðŸŽ® **GPU:** NVIDIA GPU with CUDA support\n* ðŸ’¾ **Disk Space:** 170GB for model weights\n* ðŸ§  **GPU Memory:** â‰¥3Ã—80GB (4Ã—80GB recommended for better performance)\n\n### ðŸ“¦ Environment Setup\n\n* ðŸ **Python:** 3.12+ (recommended and tested)\n* ðŸ”¥ **PyTorch:** 2.7.1\n* âš¡ **CUDA:** 12.8\n\n### ðŸ“¥ Install Dependencies\n\n```bash\n# 1. First install PyTorch (CUDA 12.8 Version)\npip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128\n\n# 2. Then install tencentcloud-sdk\npip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python\n\n# 3. Then install other dependencies\npip install -r requirements.txt\n```\n\n#### Performance Optimizations\n\nFor **up to 3x faster inference**, install these optimizations:\n\n```bash\n# FlashAttention for faster attention computation\npip install flash-attn==2.8.3 --no-build-isolation\n\n# FlashInfer for optimized moe inference. v0.3.1 is tested.\npip install flashinfer-python\n```\n> ðŸ’¡**Installation Tips:** It is critical that the CUDA version used by PyTorch matches the system's CUDA version. \n> FlashInfer relies on this compatibility when compiling kernels at runtime. Pytorch 2.7.1+cu128 is tested.\n> GCC version >=9 is recommended for compiling FlashAttention and FlashInfer.\n\n> âš¡ **Performance Tips:** These optimizations can significantly speed up your inference!\n\n> ðŸ’¡**Notation:** When FlashInfer is enabled, the first inference may be slower (about 10 minutes) due to kernel compilation. Subsequent inferences on the same machine will be much faster.\n\n## ðŸš€ Usage\n\n### ðŸ”¥ Quick Start with Transformers\n\n#### 1ï¸âƒ£ Download model weights\n\n```bash\n# Download from HuggingFace and rename the directory.\n# Notice that the directory name should not contain dots, which may cause issues when loading using Transformers.\nhf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3\n```\n\n#### 2ï¸âƒ£ Run with Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM\n\n# Load the model\nmodel_id = \"./HunyuanImage-3\"\n# Currently we can not load the model using HF model_id `tencent/HunyuanImage-3.0` directly \n# due to the dot in the name.\n\nkwargs = dict(\n    attn_implementation=\"sdpa\",     # Use \"flash_attention_2\" if FlashAttention is installed\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n    moe_impl=\"eager\",   # Use \"flashinfer\" if FlashInfer is installed\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\nmodel.load_tokenizer(model_id)\n\n# generate the image\nprompt = \"A brown and white dog is running on the grass\"\nimage = model.generate_image(prompt=prompt, stream=True)\nimage.save(\"image.png\")\n```\n\n### ðŸ  Local Installation & Usage\n\n#### 1ï¸âƒ£ Clone the Repository\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git\ncd HunyuanImage-3.0/\n```\n\n#### 2ï¸âƒ£ Download Model Weights\n\n```bash\n# Download from HuggingFace\nhf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3\n```\n\n#### 3ï¸âƒ£ Run the Demo\nThe Pretrain Checkpoint does not automatically rewrite or enhance input prompts, for optimal results currently, we recommend community partners to use deepseek to rewrite the prompts. You can go to [Tencent Cloud](https://cloud.tencent.com/document/product/1772/115963#.E5.BF.AB.E9.80.9F.E6.8E.A5.E5.85.A5) to apply for an API Key.\n\n```bash\n# set env\nexport DEEPSEEK_KEY_ID=\"your_deepseek_key_id\"\nexport DEEPSEEK_KEY_SECRET=\"your_deepseek_key_secret\"\n\npython3 run_image_gen.py --model-id ./HunyuanImage-3 --verbose 1 --sys-deepseek-prompt \"universal\" --prompt \"A brown and white dog is running on the grass\"\n```\n\n#### 4ï¸âƒ£ Command Line Arguments\n\n| Arguments               | Description                                                  | Default     |\n| ----------------------- | ------------------------------------------------------------ | ----------- |\n| `--prompt`              | Input prompt                                                 | (Required)  |\n| `--model-id`            | Model path                                                   | (Required)  |\n| `--attn-impl`           | Attention implementation. Either `sdpa` or `flash_attention_2`. | `sdpa`      |\n| `--moe-impl`            | MoE implementation. Either `eager` or `flashinfer`           | `eager`     |\n| `--seed`                | Random seed for image generation                             | `None`      |\n| `--diff-infer-steps`    | Diffusion infer steps                                        | `50`        |\n| `--image-size`          | Image resolution. Can be `auto`, like `1280x768` or `16:9`   | `auto`      |\n| `--save`                | Image save path.                                             | `image.png` |\n| `--verbose`             | Verbose level. 0: No log; 1: log inference information.      | `0`         |\n| `--rewrite`             | Whether to enable rewriting                                  | `1`         |\n| `--sys-deepseek-prompt` | Select sys-prompt from `universal` or `text_rendering`       | `universal` |\n\n### ðŸŽ¨ Interactive Gradio Demo\n\nLaunch an interactive web interface for easy text-to-image generation.\n\n#### 1ï¸âƒ£ Install Gradio\n\n```bash\npip install gradio>=4.21.0\n```\n\n#### 2ï¸âƒ£ Configure Environment\n\n```bash\n# Set your model path\nexport MODEL_ID=\"path/to/your/model\"\n\n# Optional: Configure GPU usage (default: 0,1,2,3)\nexport GPUS=\"0,1,2,3\"\n\n# Optional: Configure host and port (default: 0.0.0.0:443)\nexport HOST=\"0.0.0.0\"\nexport PORT=\"443\"\n```\n\n#### 3ï¸âƒ£ Launch the Web Interface\n\n**Basic Launch:**\n```bash\nsh run_app.sh\n```\n\n**With Performance Optimizations:**\n```bash\n# Use both optimizations for maximum performance\nsh run_app.sh --moe-impl flashinfer --attn-impl flash_attention_2\n```\n\n#### 4ï¸âƒ£ Access the Interface\n\n> ðŸŒ **Web Interface:** Open your browser and navigate to `http://localhost:443` (or your configured port)\n\n\n## ðŸ§± Models Cards\n\n| Model                     | Params | Download | Recommended VRAM | Supported |\n|---------------------------| --- | --- | --- | --- |\n| HunyuanImage-3.0          | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0) | â‰¥ 3 Ã— 80 GB | âœ… Text-to-Image\n| HunyuanImage-3.0-Instruct | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct) | â‰¥ 3 Ã— 80 GB | âœ… Text-to-Image<br>âœ… Prompt Self-Rewrite <br>âœ… CoT Think\n\n\n\nNotes:\n- Install performance extras (FlashAttention, FlashInfer) for faster inference.\n- Multiâ€‘GPU inference is recommended for the Base model.\n\n\n## ðŸ“ Prompt Guide\n\n### Manually Writing Prompts.\nThe Pretrain Checkpoint does not automatically rewrite or enhance input prompts, Instruct Checkpoint can rewrite or enhance input prompts with thinking . For optimal results currently, we recommend community partners consulting our official guide on how to write effective prompts.\n\nReference: [HunyuanImage 3.0 Prompt Handbook](\nhttps://docs.qq.com/doc/DUVVadmhCdG9qRXBU)\n\n\n### System Prompt For Automatic Rewriting the Prompt.\n\nWe've included two system prompts in the PE folder of this repository that leverage DeepSeek to automatically enhance user inputs:\n\n* **system_prompt_universal**: This system prompt converts photographic style, artistic prompts into a detailed one.\n* **system_prompt_text_rendering**: This system prompt converts UI/Poster/Text Rending prompts to a deailed on that suits the model.\n\nNote that these system prompts are in Chinese because Deepseek works better with Chinese system prompts. If you want to use it for English oriented model, you may translate it into English or refer to the comments in the PE file as a guide.\n\nWe also create a [Yuanqi workflow](https://yuanqi.tencent.com/agent/H69VgtJdj3Dz) to implement the universal one, you can directly try it.\n\n### Advanced Tips\n- **Content Priority**: Focus on describing the main subject and action first, followed by details about the environment and style. A more general description framework is: **Main subject and scene + Image quality and style + Composition and perspective + Lighting and atmosphere + Technical parameters**. Keywords can be added both before and after this structure.\n\n- **Image resolution**: Our model not only supports multiple resolutions but also offers both **automatic and specified resolution** options. In auto mode, the model automatically predicts the image resolution based on the input prompt. In specified mode (like traditional DiT), the model outputs an image resolution that strictly aligns with the user's chosen resolution.\n\n### More Cases\nOur model can follow complex instructions to generate highâ€‘quality, creative images.\n\n<div align=\"center\">\n  <img src=\"./assets/banner_all.jpg\" width=100% alt=\"HunyuanImage 3.0 Demo\">\n</div>\n\nOur model can effectively process very long text inputs, enabling users to precisely control the finer details of generated images. Extended prompts allow for intricate elements to be accurately captured, making it ideal for complex projects requiring precision and creativity.\n\n<p align=\"center\">\n<table>\n<thead>\n</thead>\n<tbody>\n<tr>\n<td>\n<img src=\"./assets/pg_imgs/image1.png\" width=100%><details>\n<summary>Show prompt</summary>\nA cinematic medium shot captures a single Asian woman seated on a chair within a dimly lit room, creating an intimate and theatrical atmosphere. The composition is focused on the subject, rendered with rich colors and intricate textures that evoke a nostalgic and moody feeling.\n\nThe primary subject is a young Asian woman with a thoughtful and expressive countenance, her gaze directed slightly away from the camera. She is seated in a relaxed yet elegant posture on an ornate, vintage armchair. The chair is upholstered in a deep red velvet, its fabric showing detailed, intricate textures and slight signs of wear. She wears a simple, elegant dress in a dark teal hue, the material catching the light in a way that reveals its fine-woven texture. Her skin has a soft, matte quality, and the light delicately models the contours of her face and arms.\n\nThe surrounding room is characterized by its vintage decor, which contributes to the historic and evocative mood. In the immediate background, partially blurred due to a shallow depth of field consistent with a f/2.8 aperture, the wall is covered with wallpaper featuring a subtle, damask pattern. The overall color palette is a carefully balanced interplay of deep teal and rich red hues, creating a visually compelling and cohesive environment. The entire scene is detailed, from the fibers of the upholstery to the subtle patterns on the wall.\n\nThe lighting is highly dramatic and artistic, defined by high contrast and pronounced shadow play. A single key light source, positioned off-camera, projects gobo lighting patterns onto the scene, casting intricate shapes of light and shadow across the woman and the back wall. These dramatic shadows create a strong sense of depth and a theatrical quality. While some shadows are deep and defined, others remain soft, gently wrapping around the subject and preventing the loss of detail in darker areas. The soft focus on the background enhances the intimate feeling, drawing all attention to the expressive subject. The overall image presents a cinematic, photorealistic photography style.\n</details>\n</td>\n<td><img src=\"./assets/pg_imgs/image2.png\" width=100%><details>\n<summary>Show prompt</summary>\nA cinematic, photorealistic medium shot captures a high-contrast urban street corner, defined by the sharp intersection of light and shadow. The primary subject is the exterior corner of a building, rendered in a low-saturation, realistic style.\n\nThe building wall, which occupies the majority of the frame, is painted a warm orange with a finely detailed, rough stucco texture. Horizontal white stripes run across its surface. The base of the building is constructed from large, rough-hewn stone blocks, showing visible particles and texture. On the left, illuminated side of the building, there is a single window with closed, dark-colored shutters. Adjacent to the window, a simple black pendant lamp hangs from a thin, taut rope, casting a distinct, sharp-edged shadow onto the sunlit orange wall. The composition is split diagonally, with the right side of the building enveloped in a deep brown shadow. At the bottom of the frame, a smooth concrete sidewalk is visible, upon which the dynamic silhouette of a person is captured mid-stride, walking from right to left.\n\nIn the shallow background, the faint, out-of-focus outlines of another building and the bare, skeletal branches of trees are softly visible, contributing to the quiet urban atmosphere and adding a sense of depth to the scene. These elements are rendered with minimal detail to keep the focus on the foreground architecture.\n\nThe scene is illuminated by strong, natural sunlight originating from the upper left, creating a dramatic chiaroscuro effect. This hard light source casts deep, well-defined shadows, producing a sharp contrast between the brightly lit warm orange surfaces and the deep brown shadow areas. The lighting highlights the fine details in the wall texture and stone particles, emphasizing the photorealistic quality. The overall presentation reflects a high-quality photorealistic photography style, infused with a cinematic film noir aesthetic.\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src=\"./assets/pg_imgs/image3.png\" width=100%><details>\n<summary>Show prompt</summary>\nä¸€å¹…æžå…·è§†è§‰å¼ åŠ›çš„æ‚å¿—å°é¢é£Žæ ¼äººåƒç‰¹å†™ã€‚ç”»é¢ä¸»ä½“æ˜¯ä¸€ä¸ªèº«ç€å¤é£Žæ±‰æœçš„äººç‰©ï¼Œæž„å›¾é‡‡ç”¨äº†ä»Žè‚©éƒ¨ä»¥ä¸Šçš„è¶…çº§è¿‘è·ç¦»ç‰¹å†™ï¼Œäººç‰©å æ®äº†ç”»é¢çš„ç»å¤§éƒ¨åˆ†ï¼Œå½¢æˆäº†å¼ºçƒˆçš„è§†è§‰å†²å‡»åŠ›ã€‚\n\nç”»é¢ä¸­çš„äººç‰©ä»¥ä¸€ç§æ…µæ‡’çš„å§¿æ€å‡ºçŽ°ï¼Œå¾®å¾®å€¾æ–œç€å¤´éƒ¨ï¼Œè£¸éœ²çš„ä¸€ä¾§è‚©è†€çº¿æ¡æµç•…ã€‚å¥¹æ­£ç”¨ä¸€ç§å¦©åªšè€Œç›´æŽ¥çš„çœ¼ç¥žå‡è§†ç€é•œå¤´ï¼ŒåŒçœ¼å¾®å¼ ï¼Œçœ¼ç¥žæ·±é‚ƒï¼Œä¼ é€’å‡ºä¸€ç§ç¥žç§˜è€Œå‹¾äººçš„æ°”è´¨ã€‚äººç‰©çš„é¢éƒ¨ç‰¹å¾ç²¾è‡´ï¼Œçš®è‚¤è´¨æ„Ÿç»†è…»ï¼Œåœ¨ç‰¹å®šçš„å…‰çº¿ä¸‹ï¼Œé¢éƒ¨è½®å»“æ¸…æ™°åˆ†æ˜Žï¼Œå±•çŽ°å‡ºä¸€ç§å¤å…¸ä¸ŽçŽ°ä»£èžåˆçš„æ—¶å°šç¾Žæ„Ÿã€‚\n\næ•´ä¸ªç”»é¢çš„èƒŒæ™¯è¢«è®¾å®šä¸ºä¸€ç§ç®€çº¦è€Œé«˜çº§çš„çº¯çº¢è‰²ã€‚è¿™ç§çº¢è‰²è‰²è°ƒæ·±æ²‰ï¼Œå‘ˆçŽ°å‡ºå“‘å…‰è´¨æ„Ÿï¼Œæ—¢çº¯ç²¹åˆæ— ä»»ä½•æ‚è´¨ï¼Œä¸ºæ•´ä¸ªæš—é»‘ç¥žç§˜çš„æ°›å›´å¥ å®šäº†æ²‰ç¨³è€Œå¯Œæœ‰å¼ åŠ›çš„åŸºè°ƒã€‚è¿™ä¸ªçº¯è‰²çš„èƒŒæ™¯æœ‰æ•ˆåœ°çªå‡ºäº†å‰æ™¯ä¸­çš„äººç‰©ä¸»ä½“ï¼Œä½¿å¾—æ‰€æœ‰è§†è§‰ç„¦ç‚¹éƒ½é›†ä¸­åœ¨å…¶èº«ä¸Šã€‚\n\nå…‰çº¿å’Œæ°›å›´çš„è¥é€ æ˜¯è¿™å¹…æ‚å¿—é£Žæµ·æŠ¥çš„å…³é”®ã€‚ä¸€æŸæš—æ©˜è‰²çš„æŸ”å’Œå…‰çº¿ä½œä¸ºä¸»å…‰æºï¼Œä»Žäººç‰©çš„ä¸€ä¾§æ–œä¸Šæ–¹æŠ•å°„ä¸‹æ¥ï¼Œç²¾å‡†åœ°å‹¾å‹’å‡ºäººç‰©çš„è„¸é¢Šã€é¼»æ¢å’Œè‚©è†€çš„è½®å»“ï¼Œåœ¨çš®è‚¤ä¸Šå½¢æˆå¾®å¦™çš„å…‰å½±è¿‡æ¸¡ã€‚åŒæ—¶ï¼Œäººç‰©çš„å‘¨èº«è¦ç»•ç€ä¸€å±‚æš—æ·¡ä¸”ä½Žé¥±å’Œåº¦çš„é“¶ç™½è‰²è¾‰å…‰ï¼Œå¦‚åŒæ¸…å†·çš„æœˆå…‰ï¼Œå½¢æˆä¸€é“æœ¦èƒ§çš„è½®å»“å…‰ã€‚è¿™é“é“¶è¾‰ä¸ºäººç‰©å¢žæ·»äº†å‡ åˆ†ç–ç¦»çš„å¹½çµæ„Ÿï¼Œå¼ºåŒ–äº†æ•´ä½“æš—é»‘é£Žæ ¼çš„ç¥žç§˜æ°”è´¨ã€‚å…‰å½±çš„å¼ºçƒˆå¯¹æ¯”ä¸Žè‰²å½©çš„ç‹¬ç‰¹æ­é…ï¼Œå…±åŒå¡‘é€ äº†è¿™å¼ å……æ»¡æ•…äº‹æ„Ÿçš„ç‰¹å†™ç”»é¢ã€‚æ•´ä½“å›¾åƒå‘ˆçŽ°å‡ºä¸€ç§èžåˆäº†å¤å…¸å…ƒç´ çš„çŽ°ä»£æ—¶å°šæ‘„å½±é£Žæ ¼ã€‚\n</details>\n</td>\n<td>\n<img src=\"./assets/pg_imgs/image4.png\" width=100%><details>\n<summary>Show prompt</summary>\nä¸€å¹…é‡‡ç”¨æžç®€ä¿¯è§†è§†è§’çš„æ²¹ç”»ä½œå“ï¼Œç”»é¢ä¸»ä½“ç”±ä¸€é“å±…ä¸­æ–œå‘çš„çº¢è‰²ç¬”è§¦æž„æˆã€‚\n\nè¿™é“é†’ç›®çš„çº¢è‰²ç¬”è§¦è¿ç”¨äº†åŽšæ¶‚æŠ€æ³•ï¼Œé¢œæ–™å †å å½¢æˆäº†å¼ºçƒˆçš„ç‰©ç†åŽšåº¦å’Œä¸‰ç»´ç«‹ä½“æ„Ÿã€‚å®ƒä»Žç”»é¢çš„å·¦ä¸Šè§’é™„è¿‘å»¶ä¼¸è‡³å³ä¸‹è§’é™„è¿‘ï¼Œæž„æˆä¸€ä¸ªåŠ¨æ€çš„å¯¹è§’çº¿ã€‚é¢œæ–™è¡¨é¢å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°ç”»åˆ€åˆ®æ“¦å’Œç¬”åˆ·æ‹–æ›³ç•™ä¸‹çš„ç—•è¿¹ï¼Œè¾¹ç¼˜å¤„çš„é¢œæ–™å±‚ç›¸å¯¹è¾ƒè–„ï¼Œè€Œä¸­å¤®éƒ¨åˆ†åˆ™é«˜é«˜éš†èµ·ï¼Œå½¢æˆäº†ä¸è§„åˆ™çš„èµ·ä¼ã€‚\n\nåœ¨è¿™é“ç«‹ä½“çš„çº¢è‰²é¢œæ–™ä¹‹ä¸Šï¼Œå·§å¦™åœ°æž„å»ºäº†ä¸€å¤„ç²¾è‡´çš„å¾®ç¼©æ™¯è§‚ã€‚æ™¯è§‚çš„æ ¸å¿ƒæ˜¯ä¸€ç‰‡æ¨¡æ‹Ÿçº¢æµ·æ»©çš„åŒºåŸŸï¼Œç”±ç»†è…»çš„æ·±çº¢è‰²é¢œæ–™ç‚¹ç¼€è€Œæˆï¼Œä¸Žä¸‹æ–¹åŸºåº•çš„é²œçº¢è‰²å½¢æˆä¸°å¯Œçš„å±‚æ¬¡å¯¹æ¯”ã€‚ç´§é‚»ç€â€œçº¢æµ·æ»©â€çš„æ˜¯ä¸€å°ç‰‡æ¹–æ³Šï¼Œç”±ä¸€å±‚å¹³æ»‘ä¸”å¸¦æœ‰å…‰æ³½çš„è“è‰²ä¸Žç™½è‰²æ··åˆé¢œæ–™æž„æˆï¼Œè´¨æ„Ÿå¦‚åŒå¹³é™æ— æ³¢çš„æ°´é¢ã€‚æ¹–æ³Šè¾¹ç¼˜ï¼Œä¸€å°æ’®èŠ¦è‹‡ä¸›ç”Ÿï¼Œç”±å‡ æ ¹çº¤ç»†æŒºæ‹”çš„ã€ç”¨æ·¡é»„è‰²å’Œæ£•è‰²é¢œæ–™å‹¾å‹’å‡ºçš„çº¿æ¡æ¥è¡¨çŽ°ã€‚ä¸€åªå°å·§çš„ç™½é¹­ç«‹äºŽèŠ¦è‹‡æ—ï¼Œå…¶å½¢æ€ç”±ä¸€å°å—çº¯ç™½è‰²çš„åŽšæ¶‚é¢œæ–™å¡‘é€ ï¼Œä»…ç”¨ä¸€æŠ¹ç²¾ç‚¼çš„é»‘è‰²é¢œæ–™ç‚¹å‡ºå…¶å°–å–™ï¼Œå§¿æ€ä¼˜é›…å®é™ã€‚\n\næ•´ä¸ªæž„å›¾çš„èƒŒæ™¯æ˜¯å¤§é¢ç§¯çš„ç•™ç™½ï¼Œå‘ˆçŽ°ä¸ºä¸€å¼ å¸¦æœ‰ç»†å¾®å‡¹å‡¸çº¹ç†çš„ç™½è‰²çº¸è´¨åŸºåº•ï¼Œè¿™ç§æžç®€å¤„ç†æžå¤§åœ°çªå‡ºäº†ä¸­å¤®çš„çº¢è‰²ç¬”è§¦åŠå…¶ä¸Šçš„å¾®ç¼©æ™¯è§‚ã€‚\n\nå…‰çº¿ä»Žç”»é¢ä¸€ä¾§æŸ”å’Œåœ°ç…§å°„ä¸‹æ¥ï¼Œåœ¨åŽšæ¶‚çš„é¢œæ–™å †å å¤„æŠ•ä¸‹æ·¡æ·¡çš„ã€è½®å»“åˆ†æ˜Žçš„é˜´å½±ï¼Œè¿›ä¸€æ­¥å¢žå¼ºäº†ç”»é¢çš„ä¸‰ç»´ç«‹ä½“æ„Ÿå’Œæ²¹ç”»è´¨æ„Ÿã€‚æ•´å¹…ç”»é¢å‘ˆçŽ°å‡ºä¸€ç§ç»“åˆäº†åŽšæ¶‚æŠ€æ³•çš„çŽ°ä»£æžç®€ä¸»ä¹‰æ²¹ç”»é£Žæ ¼ã€‚\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src=\"./assets/pg_imgs/image5.png\" width=100%><details>\n<summary>Show prompt</summary>\næ•´ä½“ç”»é¢é‡‡ç”¨ä¸€ä¸ªäºŒä¹˜äºŒçš„å››å®«æ ¼å¸ƒå±€ï¼Œä»¥äº§å“å¯è§†åŒ–çš„é£Žæ ¼ï¼Œå±•ç¤ºäº†ä¸€åªå…”å­åœ¨å››ç§ä¸åŒæè´¨ä¸‹çš„æ¸²æŸ“æ•ˆæžœã€‚æ¯ä¸ªå®«æ ¼å†…éƒ½æœ‰ä¸€åªå§¿æ€å®Œå…¨ç›¸åŒçš„å…”å­æ¨¡åž‹ï¼Œå®ƒå‘ˆåå§¿ï¼ŒåŒè€³ç«–ç«‹ï¼Œé¢æœå‰æ–¹ã€‚æ‰€æœ‰å®«æ ¼çš„èƒŒæ™¯å‡æ˜¯ç»Ÿä¸€çš„ä¸­æ€§æ·±ç°è‰²ï¼Œè¿™ç§ç®€çº¦èƒŒæ™¯æ—¨åœ¨æœ€å¤§é™åº¦åœ°çªå‡ºæ¯ç§æè´¨çš„ç‹¬ç‰¹è´¨æ„Ÿã€‚\n\nå·¦ä¸Šè§’çš„å®«æ ¼ä¸­ï¼Œå…”å­æ¨¡åž‹ç”±å“‘å…‰ç™½è‰²çŸ³è†æè´¨æž„æˆã€‚å…¶è¡¨é¢å¹³æ»‘ã€å‡åŒ€ä¸”æ— åå°„ï¼Œåœ¨æ¨¡åž‹çš„è€³æœµæ ¹éƒ¨ã€å››è‚¢äº¤æŽ¥å¤„ç­‰å‡¹é™·åŒºåŸŸå‘ˆçŽ°å‡ºæŸ”å’Œçš„çŽ¯å¢ƒå…‰é®è”½é˜´å½±ï¼Œè¿™ç§å¾®å¦™çš„é˜´å½±å˜åŒ–å‡¸æ˜¾äº†å…¶çº¯ç²¹çš„å‡ ä½•å½¢æ€ï¼Œæ•´ä½“æ„Ÿè§‰åƒä¸€ä¸ªç”¨äºŽç¾Žæœ¯ç ”ç©¶çš„åŸºç¡€æ¨¡åž‹ã€‚\n\nå³ä¸Šè§’çš„å®«æ ¼ä¸­ï¼Œå…”å­æ¨¡åž‹ç”±æ™¶èŽ¹å‰”é€çš„æ— ç‘•ç–µçŽ»ç’ƒåˆ¶æˆã€‚å®ƒå±•çŽ°äº†é€¼çœŸçš„ç‰©ç†æŠ˜å°„æ•ˆæžœï¼Œé€è¿‡å…¶é€æ˜Žçš„èº«ä½“çœ‹åˆ°çš„èƒŒæ™¯å‘ˆçŽ°å‡ºè½»å¾®çš„æ‰­æ›²ã€‚æ¸…æ™°çš„é•œé¢é«˜å…‰æ²¿ç€å…¶èº«ä½“çš„æ›²çº¿è½®å»“æµåŠ¨ï¼Œè¡¨é¢ä¸Šè¿˜èƒ½çœ‹åˆ°å¾®å¼±è€Œæ¸…æ™°çš„çŽ¯å¢ƒåå°„ï¼Œèµ‹äºˆå…¶ä¸€ç§ç²¾è‡´è€Œæ˜“ç¢Žçš„è´¨æ„Ÿã€‚\n\nå·¦ä¸‹è§’çš„å®«æ ¼ä¸­ï¼Œå…”å­æ¨¡åž‹å‘ˆçŽ°ä¸ºå¸¦æœ‰æ‹‰ä¸çº¹ç†çš„é’›é‡‘å±žæè´¨ã€‚é‡‘å±žè¡¨é¢å…·æœ‰æ˜Žæ˜¾çš„å„å‘å¼‚æ€§åå°„æ•ˆæžœï¼Œå‘ˆçŽ°å‡ºå†·å³»çš„ç°è°ƒé‡‘å±žå…‰æ³½ã€‚é”åˆ©æ˜Žäº®çš„é«˜å…‰å’Œæ·±é‚ƒçš„é˜´å½±å½¢æˆäº†å¼ºçƒˆå¯¹æ¯”ï¼Œç²¾ç¡®åœ°å®šä¹‰äº†å…¶åšå›ºçš„ä¸‰ç»´å½¢æ€ï¼Œå±•çŽ°äº†å·¥ä¸šè®¾è®¡èˆ¬çš„ç¾Žæ„Ÿã€‚\n\nå³ä¸‹è§’çš„å®«æ ¼ä¸­ï¼Œå…”å­æ¨¡åž‹è¦†ç›–ç€ä¸€å±‚æŸ”è½¯æµ“å¯†çš„ç°è‰²æ¯›ç»’ã€‚æ ¹æ ¹åˆ†æ˜Žçš„ç»’æ¯›æ¸…æ™°å¯è§ï¼Œåˆ›é€ å‡ºä¸€ç§æ¸©æš–ã€å¯è§¦æ‘¸çš„è´¨åœ°ã€‚å…‰çº¿ç…§å°„åœ¨ç»’æ¯›çš„æœ«æ¢¢ï¼Œå½¢æˆæŸ”å’Œçš„å…‰æ™•æ•ˆæžœï¼Œè€Œæ¯›ç»’å†…éƒ¨çš„é˜´å½±åˆ™æ˜¾å¾—æ·±é‚ƒè€ŒæŸ”è½¯ï¼Œå±•çŽ°äº†é«˜åº¦å†™å®žçš„æ¯›å‘æ¸²æŸ“æ•ˆæžœã€‚\n\næ•´ä¸ªå››å®«æ ¼ç”±æ¥è‡ªå¤šä¸ªæ–¹å‘çš„ã€æŸ”å’Œå‡åŒ€çš„å½±æ£šç¯å…‰ç…§äº®ï¼Œç¡®ä¿äº†æ¯ç§æè´¨çš„ç»†èŠ‚å’Œç‰¹æ€§éƒ½å¾—åˆ°æ¸…æ™°çš„å±•çŽ°ï¼Œæ²¡æœ‰ä»»ä½•åˆºçœ¼çš„é˜´å½±æˆ–è¿‡æ›çš„é«˜å…‰ã€‚è¿™å¼ å›¾åƒä»¥ä¸€ç§é«˜åº¦å†™å®žçš„3Dæ¸²æŸ“é£Žæ ¼å‘ˆçŽ°ï¼Œå®Œç¾Žåœ°è¯ é‡Šäº†äº§å“å¯è§†åŒ–çš„ç²¾é«“\n</details>\n</td>\n<td>\n<img src=\"./assets/pg_imgs/image6.png\" width=100%><details>\n<summary>Show prompt</summary>\nç”±ä¸€ä¸ªä¸¤è¡Œä¸¤åˆ—çš„ç½‘æ ¼æž„æˆï¼Œå…±åŒ…å«å››ä¸ªç‹¬ç«‹çš„åœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯éƒ½ä»¥ä¸åŒçš„è‰ºæœ¯é£Žæ ¼æç»˜äº†ä¸€ä¸ªå°ç”·å­©ï¼ˆå°æ˜Žï¼‰ä¸€å¤©ä¸­çš„ä¸åŒæ´»åŠ¨ã€‚\n\nå·¦ä¸Šè§’çš„ç¬¬ä¸€ä¸ªåœºæ™¯ï¼Œä»¥è¶…å†™å®žæ‘„å½±é£Žæ ¼å‘ˆçŽ°ã€‚ç”»é¢ä¸»ä½“æ˜¯ä¸€ä¸ªå¤§çº¦8å²çš„ä¸œäºšå°ç”·å­©ï¼Œä»–ç©¿ç€æ•´æ´çš„å°å­¦åˆ¶æœâ€”â€”ä¸€ä»¶ç™½è‰²çŸ­è¢–è¡¬è¡«å’Œè“è‰²çŸ­è£¤ï¼Œè„–å­ä¸Šç³»ç€çº¢é¢†å·¾ã€‚ä»–èƒŒç€ä¸€ä¸ªè“è‰²çš„åŒè‚©ä¹¦åŒ…ï¼Œæ­£èµ°åœ¨åŽ»ä¸Šå­¦çš„è·¯ä¸Šã€‚ä»–ä½äºŽç”»é¢çš„å‰æ™¯åå³ä¾§ï¼Œé¢å¸¦å¾®ç¬‘ï¼Œæ­¥ä¼è½»å¿«ã€‚åœºæ™¯è®¾å®šåœ¨æ¸…æ™¨ï¼ŒæŸ”å’Œçš„é˜³å…‰ä»Žå·¦ä¸Šæ–¹ç…§å°„ä¸‹æ¥ï¼Œåœ¨äººè¡Œé“ä¸ŠæŠ•ä¸‹æ¸…æ™°è€ŒæŸ”å’Œçš„å½±å­ã€‚èƒŒæ™¯æ˜¯ç»¿æ ‘æˆè«çš„è¡—é“å’Œæ¨¡ç³Šå¯è§çš„å­¦æ ¡é“è‰ºå¤§é—¨ï¼Œè¥é€ å‡ºå®é™çš„æ—©æ™¨æ°›å›´ã€‚è¿™å¼ å›¾ç‰‡çš„ç»†èŠ‚è¡¨çŽ°æžä¸ºä¸°å¯Œï¼Œå¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°ç”·å­©å¤´å‘çš„å…‰æ³½ã€è¡£æœçš„è¤¶çš±çº¹ç†ä»¥åŠä¹¦åŒ…çš„å¸†å¸ƒæè´¨ï¼Œå®Œå…¨å±•çŽ°äº†ä¸“ä¸šæ‘„å½±çš„è´¨æ„Ÿã€‚\n\nå³ä¸Šè§’çš„ç¬¬äºŒä¸ªåœºæ™¯ï¼Œé‡‡ç”¨æ—¥å¼èµ›ç’ç’åŠ¨æ¼«é£Žæ ¼ç»˜åˆ¶ã€‚ç”»é¢ä¸­ï¼Œå°ç”·å­©ååœ¨å®¶ä¸­çš„æœ¨è´¨é¤æ¡Œæ—åƒåˆé¥­ã€‚ä»–çš„å½¢è±¡è¢«åŠ¨æ¼«åŒ–ï¼Œæ‹¥æœ‰å¤§è€Œæ˜Žäº®çš„çœ¼ç›å’Œç®€æ´çš„äº”å®˜çº¿æ¡ã€‚ä»–èº«ç©¿ä¸€ä»¶ç®€å•çš„é»„è‰²Tæ¤ï¼Œæ­£ç”¨ç­·å­å¤¹èµ·ç¢—é‡Œçš„ç±³é¥­ã€‚æ¡Œä¸Šæ‘†æ”¾ç€ä¸€ç¢—æ±¤å’Œä¸¤ç›˜å®¶å¸¸èœã€‚èƒŒæ™¯æ˜¯ä¸€ä¸ªæ¸©é¦¨çš„å®¤å†…çŽ¯å¢ƒï¼Œä¸€æ‰‡æ˜Žäº®çš„çª—æˆ·é€è¿›æ­£åˆçš„é˜³å…‰ï¼Œçª—å¤–æ˜¯è“å¤©ç™½äº‘ã€‚æ•´ä¸ªç”»é¢è‰²å½©é²œè‰³ã€é¥±å’Œåº¦é«˜ï¼Œè§’è‰²è½®å»“çº¿æ¸…æ™°æ˜Žç¡®ï¼Œé˜´å½±éƒ¨åˆ†é‡‡ç”¨å¹³æ¶‚çš„è‰²å—å¤„ç†ï¼Œæ˜¯å…¸åž‹çš„èµ›ç’ç’åŠ¨æ¼«é£Žæ ¼ã€‚\n\nå·¦ä¸‹è§’çš„ç¬¬ä¸‰ä¸ªåœºæ™¯ï¼Œä»¥ç»†è…»çš„é“…ç¬”ç´ æé£Žæ ¼å‘ˆçŽ°ã€‚ç”»é¢æç»˜äº†ä¸‹åˆåœ¨æ“åœºä¸Šè¸¢è¶³çƒçš„å°ç”·å­©ã€‚æ•´ä¸ªå›¾åƒç”±ä¸åŒç°åº¦çš„çŸ³å¢¨è‰²è°ƒæž„æˆï¼Œæ²¡æœ‰å…¶ä»–é¢œè‰²ã€‚å°ç”·å­©èº«ç©¿è¿åŠ¨çŸ­è¢–å’ŒçŸ­è£¤ï¼Œèº«ä½“å‘ˆå‰å€¾å§¿æ€ï¼Œå³è„šæ­£è¦è¸¢å‘ä¸€ä¸ªè¶³çƒï¼ŒåŠ¨ä½œå……æ»¡åŠ¨æ„Ÿã€‚èƒŒæ™¯æ˜¯ç©ºæ—·çš„æ“åœºå’Œè¿œå¤„çš„çƒé—¨ï¼Œç”¨ç®€ç»ƒçš„çº¿æ¡å’ŒæŽ’çº¿å‹¾å‹’ã€‚è‰ºæœ¯å®¶é€šè¿‡äº¤å‰æŽ’çº¿å’Œæ¶‚æŠ¹æŠ€å·§æ¥è¡¨çŽ°å…‰å½±å’Œä½“ç§¯æ„Ÿï¼Œè¶³çƒä¸Šçš„é˜´å½±ã€äººç‰©èº«ä¸Šçš„è‚Œè‚‰çº¿æ¡ä»¥åŠåœ°é¢ç²—ç³™çš„è´¨æ„Ÿéƒ½é€šè¿‡é“…ç¬”çš„ç¬”è§¦å¾—åˆ°äº†å……åˆ†çš„å±•çŽ°ã€‚è¿™å¼ é“…ç¬”ç”»çªå‡ºäº†ç´ æçš„å…‰å½±å…³ç³»å’Œçº¿æ¡ç¾Žæ„Ÿã€‚\n\nå³ä¸‹è§’çš„ç¬¬å››ä¸ªåœºæ™¯ï¼Œä»¥æ–‡æ£®ç‰¹Â·æ¢µé«˜çš„åŽå°è±¡æ´¾æ²¹ç”»é£Žæ ¼è¿›è¡Œè¯ é‡Šã€‚ç”»é¢æç»˜äº†å¤œæ™šæ—¶åˆ†ï¼Œå°ç”·å­©ç‹¬è‡ªåœ¨æ²³è¾¹é’“é±¼çš„æ™¯è±¡ã€‚ä»–ååœ¨ä¸€å—å²©çŸ³ä¸Šï¼Œæ‰‹æŒä¸€æ ¹ç®€æ˜“çš„é’“é±¼ç«¿ï¼Œèº«å½±åœ¨æ·±è“è‰²çš„å¤œå¹•ä¸‹æ˜¾å¾—å¾ˆæ¸ºå°ã€‚æ•´ä¸ªç”»é¢çš„è§†è§‰ç„¦ç‚¹æ˜¯å¤©ç©ºå’Œæ°´é¢ï¼Œå¤©ç©ºå¸ƒæ»¡äº†æ—‹è½¬ã€å·æ›²çš„æ˜Ÿäº‘ï¼Œæ˜Ÿæ˜Ÿå’Œæœˆäº®è¢«æç»˜æˆå·¨å¤§ã€å‘å…‰çš„å…‰å›¢ï¼Œä½¿ç”¨äº†åŽšæ¶‚çš„æ²¹ç”»é¢œæ–™ï¼ˆImpastoï¼‰ï¼Œç¬”è§¦ç²—çŠ·è€Œå……æ»¡èƒ½é‡ã€‚æ·±è“ã€äº®é»„å’Œç™½è‰²çš„é¢œæ–™åœ¨ç”»å¸ƒä¸Šç›¸äº’äº¤ç»‡ï¼Œå½¢æˆå¼ºçƒˆçš„è§†è§‰å†²å‡»åŠ›ã€‚æ°´é¢å€’æ˜ ç€å¤©ç©ºä¸­æ‰­æ›²çš„å…‰å½±ï¼Œæ•´ä¸ªåœºæ™¯å……æ»¡äº†æ¢µé«˜ä½œå“ä¸­ç‰¹æœ‰çš„å¼ºçƒˆæƒ…æ„Ÿå’ŒåŠ¨è¡ä¸å®‰çš„ç¾Žæ„Ÿã€‚è¿™å¹…ç”»ä½œæ˜¯å¯¹æ¢µé«˜é£Žæ ¼çš„æ·±åº¦è‡´æ•¬ã€‚\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src=\"./assets/pg_imgs/image7.png\" width=100%><details>\n<summary>Show prompt</summary>\nä»¥å¹³è§†è§†è§’ï¼Œå‘ˆçŽ°äº†ä¸€å¹…å…³äºŽå¦‚ä½•ç”¨ç´ ææŠ€æ³•ç»˜åˆ¶é¹¦é¹‰çš„ä¹å®«æ ¼æ•™å­¦å›¾ã€‚æ•´ä½“æž„å›¾è§„æ•´ï¼Œä¹ä¸ªå¤§å°ä¸€è‡´çš„æ–¹å½¢ç”»æ¡†ä»¥ä¸‰è¡Œä¸‰åˆ—çš„å½¢å¼å‡åŒ€åˆ†å¸ƒåœ¨æµ…ç°è‰²èƒŒæ™¯ä¸Šï¼Œæ¸…æ™°åœ°å±•ç¤ºäº†ä»ŽåŸºæœ¬å½¢çŠ¶åˆ°æœ€ç»ˆæˆå“çš„å…¨è¿‡ç¨‹ã€‚\n\nç¬¬ä¸€è¡Œä»Žå·¦è‡³å³å±•ç¤ºäº†ç»˜ç”»çš„åˆå§‹æ­¥éª¤ã€‚å·¦ä¸Šè§’çš„ç¬¬ä¸€ä¸ªç”»æ¡†ä¸­ï¼Œç”¨ç®€æ´çš„é“…ç¬”çº¿æ¡å‹¾å‹’å‡ºé¹¦é¹‰çš„åŸºæœ¬å‡ ä½•å½¢æ€ï¼šä¸€ä¸ªåœ†å½¢ä»£è¡¨å¤´éƒ¨ï¼Œä¸€ä¸ªç¨å¤§çš„æ¤­åœ†å½¢ä»£è¡¨èº«ä½“ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°å·çš„æ— è¡¬çº¿å­—ä½“æ•°å­—â€œ1â€ã€‚ä¸­é—´çš„ç¬¬äºŒä¸ªç”»æ¡†ä¸­ï¼Œåœ¨åŸºç¡€å½¢æ€ä¸Šæ·»åŠ äº†ä¸‰è§’å½¢çš„é¸Ÿå–™è½®å»“å’Œä¸€æ¡é•¿é•¿çš„å¼§çº¿ä½œä¸ºå°¾å·´çš„é›å½¢ï¼Œå¤´éƒ¨å’Œèº«ä½“çš„è¿žæŽ¥å¤„çº¿æ¡å˜å¾—æ›´åŠ æµç•…ï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ2â€ã€‚å³ä¾§çš„ç¬¬ä¸‰ä¸ªç”»æ¡†ä¸­ï¼Œè¿›ä¸€æ­¥ç²¾ç¡®äº†é¹¦é¹‰çš„æ•´ä½“è½®å»“ï¼Œå‹¾å‹’å‡ºå¤´éƒ¨é¡¶ç«¯çš„ç¾½å† å’Œæ¸…æ™°çš„çœ¼éƒ¨åœ†å½¢è½®å»“ï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ3â€ã€‚\n\nç¬¬äºŒè¡Œä¸“æ³¨äºŽç»“æž„ä¸Žç»†èŠ‚çš„æ·»åŠ ï¼Œæç»˜äº†ç»˜ç”»çš„ä¸­æœŸé˜¶æ®µã€‚å·¦ä¾§çš„ç¬¬å››ä¸ªç”»æ¡†é‡Œï¼Œé¹¦é¹‰çš„èº«ä½“ä¸Šæ·»åŠ äº†ç¿…è†€çš„åŸºæœ¬å½¢çŠ¶ï¼ŒåŒæ—¶åœ¨èº«ä½“ä¸‹æ–¹ç”»å‡ºäº†ä¸€æ ¹ä½œä¸ºæ –æœ¨çš„æ¨ªå‘æ ‘æžï¼Œé¹¦é¹‰çš„çˆªå­åˆæ­¥æ­åœ¨æ ‘æžä¸Šï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ4â€ã€‚ä¸­é—´çš„ç¬¬äº”ä¸ªç”»æ¡†ä¸­ï¼Œå¼€å§‹ç»†åŒ–ç¿…è†€å’Œå°¾éƒ¨çš„ç¾½æ¯›åˆ†ç»„ï¼Œç”¨çŸ­ä¿ƒçš„çº¿æ¡è¡¨çŽ°å‡ºå±‚æ¬¡æ„Ÿï¼Œå¹¶æ¸…æ™°åœ°ç”»å‡ºçˆªå­ç´§æ¡æ ‘æžçš„ç»†èŠ‚ï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ5â€ã€‚å³ä¾§çš„ç¬¬å…­ä¸ªç”»æ¡†é‡Œï¼Œå¼€å§‹ä¸ºé¹¦é¹‰æ·»åŠ åˆæ­¥çš„é˜´å½±ï¼Œä½¿ç”¨äº¤å‰æŽ’çº¿çš„ç´ ææŠ€æ³•åœ¨è…¹éƒ¨ã€ç¿…è†€ä¸‹æ–¹å’Œé¢ˆéƒ¨åˆ¶é€ å‡ºä½“ç§¯æ„Ÿï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ6â€ã€‚\n\nç¬¬ä¸‰è¡Œåˆ™å±•ç¤ºäº†æœ€ç»ˆçš„æ¶¦è‰²ä¸Žå®Œæˆé˜¶æ®µã€‚å·¦ä¸‹è§’çš„ç¬¬ä¸ƒä¸ªç”»æ¡†ä¸­ï¼Œç´ æçš„æŽ’çº¿æ›´åŠ å¯†é›†ï¼Œé˜´å½±å±‚æ¬¡æ›´åŠ ä¸°å¯Œï¼Œç¾½æ¯›çš„çº¹ç†ç»†èŠ‚è¢«ä»”ç»†åˆ»ç”»å‡ºæ¥ï¼Œçœ¼ç ä¹Ÿæ·»åŠ äº†é«˜å…‰ç‚¹ç¼€ï¼Œæ˜¾å¾—ç‚¯ç‚¯æœ‰ç¥žï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ7â€ã€‚ä¸­é—´çš„ç¬¬å…«ä¸ªç”»æ¡†é‡Œï¼Œæç»˜çš„é‡ç‚¹è½¬ç§»åˆ°æ –æœ¨ä¸Šï¼Œå¢žåŠ äº†æ ‘æžçš„çº¹ç†å’ŒèŠ‚ç–¤ç»†èŠ‚ï¼ŒåŒæ—¶æ•´ä½“è°ƒæ•´äº†é¹¦é¹‰èº«ä¸Šçš„å…‰å½±å…³ç³»ï¼Œä½¿ç«‹ä½“æ„Ÿæ›´ä¸ºçªå‡ºï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ8â€ã€‚å³ä¸‹è§’çš„ç¬¬ä¹ä¸ªç”»æ¡†æ˜¯æœ€ç»ˆå®Œæˆå›¾ï¼Œæ‰€æœ‰çº¿æ¡éƒ½ç»è¿‡äº†ç²¾ç‚¼ï¼Œå…‰å½±å¯¹æ¯”å¼ºçƒˆï¼Œé¹¦é¹‰çš„ç¾½æ¯›è´¨æ„Ÿã€æœ¨è´¨æ –æœ¨çš„ç²—ç³™æ„Ÿéƒ½è¡¨çŽ°å¾—æ·‹æ¼“å°½è‡´ï¼Œå‘ˆçŽ°å‡ºä¸€å¹…å®Œæ•´ä¸”ç»†èŠ‚ä¸°å¯Œçš„ç´ æä½œå“ï¼›å³ä¸Šè§’æ ‡æœ‰æ•°å­—â€œ9â€ã€‚\n\næ•´ä¸ªç”»é¢çš„å…‰çº¿å‡åŒ€è€Œæ˜Žäº®ï¼Œæ²¡æœ‰ä»»ä½•ç‰¹å®šçš„å…‰æºæ–¹å‘ï¼Œç¡®ä¿äº†æ¯ä¸ªæ•™å­¦æ­¥éª¤çš„è§†è§‰æ¸…æ™°åº¦ã€‚æ•´ä½“å‘ˆçŽ°å‡ºä¸€ç§æ¸…æ™°ã€æœ‰æ¡ç†çš„æ•°å­—æ’ç”»æ•™ç¨‹é£Žæ ¼ã€‚\n</details>\n</td>\n<td>\n<img src=\"./assets/pg_imgs/image8.png\" width=100%><details>\n<summary>Show prompt</summary>\nä¸€å¼ çŽ°ä»£å¹³é¢è®¾è®¡é£Žæ ¼çš„æµ·æŠ¥å æ®äº†æ•´ä¸ªç”»é¢ï¼Œæž„å›¾ç®€æ´ä¸”ä¸­å¿ƒçªå‡ºã€‚\n\næµ·æŠ¥çš„ä¸»ä½“æ˜¯ä½äºŽç”»é¢æ­£ä¸­å¤®çš„ä¸€åªè…¾è®¯QQä¼é¹…ã€‚è¿™åªä¼é¹…é‡‡ç”¨äº†åœ†æ¶¦å¯çˆ±çš„3Då¡é€šæ¸²æŸ“é£Žæ ¼ï¼Œèº«ä½“ä¸»è¦ä¸ºé¥±æ»¡çš„é»‘è‰²ï¼Œè…¹éƒ¨ä¸ºçº¯ç™½è‰²ã€‚å®ƒçš„çœ¼ç›å¤§è€Œåœ†ï¼Œçœ¼ç¥žå¥½å¥‡åœ°ç›´è§†å‰æ–¹ã€‚é»„è‰²çš„å˜´å·´å°å·§è€Œç«‹ä½“ï¼ŒåŒè„šåŒæ ·ä¸ºé²œæ˜Žçš„é»„è‰²ï¼Œç¨³ç¨³åœ°ç«™ç«‹ç€ã€‚ä¸€æ¡æ ‡å¿—æ€§çš„çº¢è‰²å›´å·¾æ•´é½åœ°ç³»åœ¨å®ƒçš„è„–å­ä¸Šï¼Œå›´å·¾çš„æè´¨å¸¦æœ‰è½»å¾®çš„å¸ƒæ–™è´¨æ„Ÿï¼Œæœ«ç«¯è‡ªç„¶ä¸‹åž‚ã€‚ä¼é¹…çš„æ•´ä½“é€ åž‹å¹²å‡€åˆ©è½ï¼Œè¾¹ç¼˜å…‰æ»‘ï¼Œå‘ˆçŽ°å‡ºä¸€ç§ç²¾è‡´çš„æ•°å­—æ’ç”»è´¨æ„Ÿã€‚\n\næµ·æŠ¥çš„èƒŒæ™¯æ˜¯ä¸€ç§ä»Žä¸Šåˆ°ä¸‹ç”±æµ…è“è‰²å¹³æ»‘è¿‡æ¸¡åˆ°ç™½è‰²çš„æŸ”å’Œæ¸å˜ï¼Œè¥é€ å‡ºä¸€ç§å¼€é˜”ã€æ˜Žäº®çš„ç©ºé—´æ„Ÿã€‚åœ¨ä¼é¹…çš„èº«åŽï¼Œæ•£å¸ƒç€ä¸€äº›æ·¡æ·¡çš„ã€æ¨¡ç³Šçš„åœ†å½¢å…‰æ–‘å’Œå‡ é“æŸ”å’Œçš„æŠ½è±¡å…‰æŸï¼Œä¸ºè¿™ä¸ªç®€çº¦çš„å¹³é¢è®¾è®¡æµ·æŠ¥å¢žæ·»äº†å¾®å¦™çš„æ·±åº¦å’Œç§‘æŠ€æ„Ÿã€‚\n\nç”»é¢çš„åº•éƒ¨åŒºåŸŸæ˜¯æ–‡å­—éƒ¨åˆ†ï¼ŒæŽ’ç‰ˆå±…ä¸­å¯¹é½ã€‚ä¸ŠåŠéƒ¨åˆ†æ˜¯ä¸€è¡Œç¨å¤§çš„é»‘è‰²é»‘ä½“å­—ï¼Œå†…å®¹ä¸ºâ€œHunyuan Image 3.0â€ã€‚ç´§éšå…¶ä¸‹çš„æ˜¯ä¸€è¡Œå­—å·ç•¥å°çš„æ·±ç°è‰²é»‘ä½“å­—ï¼Œå†…å®¹ä¸ºâ€œåŽŸç”Ÿå¤šæ¨¡æ€å¤§æ¨¡åž‹â€ã€‚ä¸¤è¡Œæ–‡å­—æ¸…æ™°æ˜“è¯»ï¼Œä¸Žæ•´ä½“çš„çŽ°ä»£å¹³é¢è®¾è®¡é£Žæ ¼ä¿æŒä¸€è‡´ã€‚\n\næ•´ä½“å…‰çº¿æ˜Žäº®ã€å‡åŒ€ï¼Œæ²¡æœ‰æ˜Žæ˜¾çš„é˜´å½±ï¼Œçªå‡ºäº†ä¼é¹…å’Œæ–‡å­—ä¿¡æ¯ï¼Œç¬¦åˆçŽ°ä»£è®¾è®¡æµ·æŠ¥çš„è§†è§‰è¦æ±‚ã€‚è¿™å¼ å›¾åƒå‘ˆçŽ°äº†çŽ°ä»£ã€ç®€æ´çš„å¹³é¢è®¾è®¡æµ·æŠ¥é£Žæ ¼ã€‚\n</details>\n</td>\n</tr>\n</tbody>\n</table>\n</p>\n\n## ðŸ“Š Evaluation\n\n* ðŸ¤– **SSAE (Machine Evaluation)**   \nSSAE (Structured Semantic Alignment Evaluation) is an intelligent evaluation metric for image-text alignment based on advanced multimodal large language models (MLLMs). We extracted 3500 key points across 12 categories, then used multimodal large language models to automatically evaluate and score by comparing the generated images with these key points based on the visual content of the images. Mean Image Accuracy represents the image-wise average score across all key points, while Global Accuracy directly calculates the average score across all key points.\n\n<p align=\"center\">\n  <img src=\"./assets/ssae_side_by_side_comparison.png\" width=98% alt=\"Human Evaluation with Other Models\">\n</p>\n\n<p align=\"center\">\n  <img src=\"./assets/ssae_side_by_side_heatmap.png\" width=98% alt=\"Human Evaluation with Other Models\">\n</p>\n\n\n* ðŸ‘¥ **GSB (Human Evaluation)** \n\nWe adopted the GSB (Good/Same/Bad) evaluation method commonly used to assess the relative performance between two models from an overall image perception perspective. In total, we utilized 1,000 text prompts, generating an equal number of image samples for all compared models in a single run. For a fair comparison, we conducted inference only once for each prompt, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models. The evaluation was performed by more than 100 professional evaluators. \n\n<p align=\"center\">\n  <img src=\"./assets/gsb.png\" width=98% alt=\"Human Evaluation with Other Models\">\n</p>\n\n\n## ðŸ“š Citation\n\nIf you find HunyuanImage-3.0 useful in your research, please cite our work:\n\n```bibtex\n@article{cao2025hunyuanimage,\n  title={HunyuanImage 3.0 Technical Report},\n  author={Cao, Siyu and Chen, Hangting and Chen, Peng and Cheng, Yiji and Cui, Yutao and Deng, Xinchi and Dong, Ying and Gong, Kipper and Gu, Tianpeng and Gu, Xiusen and others},\n  journal={arXiv preprint arXiv:2509.23951},\n  year={2025}\n}\n```\n\n## ðŸ™ Acknowledgements\n\nWe extend our heartfelt gratitude to the following open-source projects and communities for their invaluable contributions:\n\n* ðŸ¤— [Transformers](https://github.com/huggingface/transformers) - State-of-the-art NLP library\n* ðŸŽ¨ [Diffusers](https://github.com/huggingface/diffusers) - Diffusion models library  \n* ðŸŒ [HuggingFace](https://huggingface.co/) - AI model hub and community\n* âš¡ [FlashAttention](https://github.com/Dao-AILab/flash-attention) - Memory-efficient attention\n* ðŸš€ [FlashInfer](https://github.com/flashinfer-ai/flashinfer) - Optimized inference engine\n\n## ðŸŒŸðŸš€ Github Star History\n\n[![GitHub stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanImage-3.0?style=social)](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)\n[![GitHub forks](https://img.shields.io/github/forks/Tencent-Hunyuan/HunyuanImage-3.0?style=social)](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanImage-3.0&type=Date)](https://www.star-history.com/#Tencent-Hunyuan/HunyuanImage-3.0&Date)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":83009199459,\"storage_bytes\":168599289097,\"files_count\":68,\"spaces_count\":53,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"HunyuanImage3ForCausalMM\"],\"auto_map\":{\"AutoConfig\":\"configuration_hunyuan.HunyuanImage3Config\",\"AutoModel\":\"hunyuan.HunyuanImage3Model\",\"AutoModelForCausalLM\":\"hunyuan.HunyuanImage3ForCausalMM\"},\"model_type\":\"hunyuan_image_3_moe\",\"tokenizer_config\":{\"bos_token\":\"<|startoftext|>\",\"eos_token\":\"<|endoftext|>\",\"pad_token\":\"<pad>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanImage-3.0\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanImage-3.0.git\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:diffusers\",\"source_url\":\"https://github.com/huggingface/diffusers\"},{\"type\":\"has_code\",\"target_id\":\"github:Dao-AILab:flash-attention\",\"source_url\":\"https://github.com/Dao-AILab/flash-attention\"},{\"type\":\"has_code\",\"target_id\":\"github:flashinfer-ai:flashinfer\",\"source_url\":\"https://github.com/flashinfer-ai/flashinfer\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanImage-3.0\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0\"},{\"type\":\"has_code\",\"target_id\":\"github:Tencent-Hunyuan:HunyuanImage-3.0\",\"source_url\":\"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:2509.23951\",\"source_url\":\"https://arxiv.org/abs/2509.23951\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 100,
    "content_hash": "3b5f845b8cf66b8d47c3c990c28d1c27",
    "velocity": null,
    "raw_image_url": "https://huggingface.co/tencent/HunyuanImage-3.0/resolve/main/assets/banner.png",
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/tencent/HunyuanImage-3.0\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:envvi:inkpunk-diffusion",
    "name": "Inkpunk-Diffusion",
    "author": "Envvi",
    "description": "--- license: creativeml-openrail-m language: - en tags: - stable-diffusion - text-to-image - diffusers --- Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use **_nvinkpunk_** in your prompts. We support a Gradio Web UI to run Inkpunk-Diffusion: !output Samples v2 !output Samples v2",
    "tags": [
      "diffusers",
      "stable-diffusion",
      "text-to-image",
      "en",
      "license:creativeml-openrail-m",
      "endpoints_compatible",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 990,
    "downloads": 919,
    "source": "huggingface",
    "source_url": "https://huggingface.co/Envvi/Inkpunk-Diffusion",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: creativeml-openrail-m\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\n- diffusers\n---\n\n# Inkpunk Diffusion\n\nFinetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use **_nvinkpunk_** in your prompts.\n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Inkpunk-Diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/Inkpunk-Diffusion)\n\n# Sample images\n![output Samples v2](https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-v2-samples-1.png)\n![output Samples v2](https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-v2-samples-2.png)",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":28191407827,\"files_count\":23,\"spaces_count\":100,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "creativeml-openrail-m",
    "compliance_status": "approved",
    "quality_score": 50,
    "content_hash": "5c4dc42d4b4a12415442356b880ef693",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/Envvi/Inkpunk-Diffusion\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:manycore-research:spatiallm-llama-1b",
    "name": "SpatialLM-Llama-1B",
    "author": "manycore-research",
    "description": "--- license: llama3.2 library_name: transformers base_model: - meta-llama/Llama-3.2-1B-Instruct --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align=\"center\"> <picture> <source srcset=\"https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/_dK14CT3do8rBG3QrHUjN.png\" media=\"(prefers-color-scheme: dark)\"> <img src=\"https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a...",
    "tags": [
      "transformers",
      "safetensors",
      "spatiallm_llama",
      "text-generation",
      "conversational",
      "base_model:meta-llama/llama-3.2-1b-instruct",
      "license:llama3.2",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "likes": 990,
    "downloads": 176,
    "source": "huggingface",
    "source_url": "https://huggingface.co/manycore-research/SpatialLM-Llama-1B",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: llama3.2\nlibrary_name: transformers\nbase_model:\n  - meta-llama/Llama-3.2-1B-Instruct\n---\n\n# SpatialLM-Llama-1B\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <picture>\n    <source srcset=\"https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/_dK14CT3do8rBG3QrHUjN.png\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/bAZyeIXOMVASHR6-xVlQU.png\" width=\"60%\" alt=\"SpatialLM\"\"/>\n  </picture>\n</div>\n<hr style=\"margin-top: 0; margin-bottom: 8px;\">\n<div align=\"center\" style=\"margin-top: 0; padding-top: 0; line-height: 1;\">\n    <a href=\"https://manycore-research.github.io/SpatialLM\" target=\"_blank\" style=\"margin: 2px;\"><img alt=\"Project\"\n    src=\"https://img.shields.io/badge/ðŸŒ%20Website-SpatialLM-ffc107?color=42a5f5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n    <a href=\"https://github.com/manycore-research/SpatialLM\" target=\"_blank\" style=\"margin: 2px;\"><img alt=\"GitHub\"\n    src=\"https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://huggingface.co/manycore-research/SpatialLM-Llama-1B\" target=\"_blank\" style=\"margin: 2px;\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM%201B-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n    <a href=\"https://huggingface.co/datasets/manycore-research/SpatialLM-Testset\" target=\"_blank\" style=\"margin: 2px;\"><img alt=\"Dataset\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-SpatialLM-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n</div>\n\n## Introduction\n\nSpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.\n\n<div align=\"center\">\n  <video controls autoplay src=\"https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/3bz_jNRCLD2L9uj11HPnP.mp4\" poster=\"https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/euo94dNx28qBNe51_oiB1.png\"></video>\n  <p><i>SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.</i></p>\n</div>\n\n## SpatialLM Models\n\n<div align=\"center\">\n\n|      **Model**      | **Download**                                                                   |\n| :-----------------: | ------------------------------------------------------------------------------ |\n| SpatialLM-Llama-1B  | [ðŸ¤— HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Llama-1B)  |\n| SpatialLM-Qwen-0.5B | [ðŸ¤— HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B) |\n\n</div>\n\n## Usage\n\n### Installation\n\nTested with the following environment:\n\n- Python 3.11\n- Pytorch 2.4.1\n- CUDA Version 12.4\n\n```bash\n# clone the repository\ngit clone https://github.com/manycore-research/SpatialLM.git\ncd SpatialLM\n\n# create a conda environment with cuda 12.4\nconda create -n spatiallm python=3.11\nconda activate spatiallm\nconda install -y nvidia/label/cuda-12.4.0::cuda-toolkit conda-forge::sparsehash\n\n# Install dependencies with poetry\npip install poetry && poetry config virtualenvs.create false --local\npoetry install\npoe install-torchsparse # Building wheel for torchsparse will take a while\n```\n\n### Inference\n\nIn the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications.\nExample preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM), are available in [SpatialLM-Testset](#spatiallm-testset).\n\nDownload an example point cloud:\n\n```bash\nhuggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .\n```\n\nRun inference:\n\n```bash\npython inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM-Llama-1B\n```\n\n### Visualization\n\nUse `rerun` to visualize the point cloud and the predicted structured 3D layout output:\n\n```bash\n# Convert the predicted layout to Rerun format\npython visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd\n\n# Visualize the point cloud and the predicted layout\nrerun scene0000_00.rrd\n```\n\n### Evaluation\n\nTo evaluate the performance of SpatialLM, we provide `eval.py` script that reports the benchmark results on the SpatialLM-Testset in the table below in section [Benchmark Results](#benchmark-results).\n\nDownload the testset:\n\n```bash\nhuggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset\n```\n\nRun evaluation:\n\n```bash\n# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM-Llama-1B model\npython inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM-Llama-1B\n\n# Evaluate the predicted layouts\npython eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv\n```\n\n## SpatialLM Testset\n\nWe provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM). SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.\n\n<div align=\"center\">\n\n|    **Dataset**    | **Download**                                                                       |\n| :---------------: | ---------------------------------------------------------------------------------- |\n| SpatialLM-Testset | [ðŸ¤— Datasets](https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet) |\n\n</div>\n\n## Benchmark Results\n\nBenchmark results on the challenging SpatialLM-Testset are reported in the following table:\n\n<div align=\"center\">\n\n| **Method**       | **SpatialLM-Llama-1B** | **SpatialLM-Qwen-0.5B** |\n| ---------------- | ---------------------- | ----------------------- |\n| **Floorplan**    | **mean IoU**           |                         |\n| wall             | 78.62                  | 74.81                   |\n|                  |                        |                         |\n| **Objects**      | **F1 @.25 IoU (3D)**   |                         |\n| curtain          | 27.35                  | 28.59                   |\n| nightstand       | 57.47                  | 54.39                   |\n| chandelier       | 38.92                  | 40.12                   |\n| wardrobe         | 23.33                  | 30.60                   |\n| bed              | 95.24                  | 93.75                   |\n| sofa             | 65.50                  | 66.15                   |\n| chair            | 21.26                  | 14.94                   |\n| cabinet          | 8.47                   | 8.44                    |\n| dining table     | 54.26                  | 56.10                   |\n| plants           | 20.68                  | 26.46                   |\n| tv cabinet       | 33.33                  | 10.26                   |\n| coffee table     | 50.00                  | 55.56                   |\n| side table       | 7.60                   | 2.17                    |\n| air conditioner  | 20.00                  | 13.04                   |\n| dresser          | 46.67                  | 23.53                   |\n|                  |                        |                         |\n| **Thin Objects** | **F1 @.25 IoU (2D)**   |                         |\n| painting         | 50.04                  | 53.81                   |\n| carpet           | 31.76                  | 45.31                   |\n| tv               | 67.31                  | 52.29                   |\n| door             | 50.35                  | 42.15                   |\n| window           | 45.4                   | 45.9                    |\n\n</div>\n\n## License\n\nSpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license.\nSpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.\n\nAll models are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.\n\n## Citation\n\nIf you find this work useful, please consider citing:\n\n```bibtex\n@misc{spatiallm,\n  title        = {SpatialLM: Large Language Model for Spatial Understanding},\n  author       = {ManyCore Research Team},\n  howpublished = {\\url{https://github.com/manycore-research/SpatialLM}},\n  year         = {2025}\n}\n```\n\n## Acknowledgements\n\nWe would like to thank the following projects that made this work possible:\n\n[Llama3.2](https://github.com/meta-llama) | [Qwen2.5](https://github.com/QwenLM/Qwen2.5) | [Transformers](https://github.com/huggingface/transformers) | [SceneScript](https://github.com/facebookresearch/scenescript) | [TorchSparse](https://github.com/mit-han-lab/torchsparse)\n",
    "meta_json": "{\"pipeline_tag\":\"text-generation\",\"library_name\":\"transformers\",\"framework\":\"transformers\",\"params\":1247355840,\"storage_bytes\":2532929485,\"files_count\":8,\"spaces_count\":0,\"gated\":false,\"private\":false,\"config\":{\"architectures\":[\"SpatialLMLlamaForCausalLM\"],\"model_type\":\"spatiallm_llama\",\"tokenizer_config\":{\"bos_token\":\"<|begin_of_text|>\",\"chat_template\":\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n        {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n        {{- '\\\"parameters\\\": ' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \\\"}\\\" }}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\",\"eos_token\":\"<|eot_id|>\",\"pad_token\":\"<|eot_id|>\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:manycore-research:SpatialLM\\\"\",\"source_url\":\"https://github.com/manycore-research/SpatialLM\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:manycore-research:SpatialLM.git\",\"source_url\":\"https://github.com/manycore-research/SpatialLM.git\"},{\"type\":\"has_code\",\"target_id\":\"github:rmurai0610:MASt3R-SLAM\",\"source_url\":\"https://github.com/rmurai0610/MASt3R-SLAM\"},{\"type\":\"has_code\",\"target_id\":\"github:rmurai0610:MASt3R-SLAM\",\"source_url\":\"https://github.com/rmurai0610/MASt3R-SLAM\"},{\"type\":\"has_code\",\"target_id\":\"github:manycore-research:SpatialLM}},\",\"source_url\":\"https://github.com/manycore-research/SpatialLM}},\"},{\"type\":\"has_code\",\"target_id\":\"github:QwenLM:Qwen2.5\",\"source_url\":\"https://github.com/QwenLM/Qwen2.5\"},{\"type\":\"has_code\",\"target_id\":\"github:huggingface:transformers\",\"source_url\":\"https://github.com/huggingface/transformers\"},{\"type\":\"has_code\",\"target_id\":\"github:facebookresearch:scenescript\",\"source_url\":\"https://github.com/facebookresearch/scenescript\"},{\"type\":\"has_code\",\"target_id\":\"github:mit-han-lab:torchsparse\",\"source_url\":\"https://github.com/mit-han-lab/torchsparse\"}]",
    "canonical_id": null,
    "license_spdx": "llama3.2",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "b398c3deda3dd1ca45c2ddc3839f790c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/manycore-research/SpatialLM-Llama-1B\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:miqudev:miqu-1-70b",
    "name": "miqu-1-70b",
    "author": "miqudev",
    "description": "--- {} --- Leaked from â–„â–„â–„â–‘â–‘ â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ...",
    "tags": [
      "gguf",
      "endpoints_compatible",
      "region:us",
      "conversational"
    ],
    "pipeline_tag": "other",
    "likes": 986,
    "downloads": 592,
    "source": "huggingface",
    "source_url": "https://huggingface.co/miqudev/miqu-1-70b",
    "image_url": null,
    "type": "model",
    "body_content": "---\n{}\n---\n# miqu 70b\n\nLeaked from\n\n\n                                                                     â–„â–„â–„â–‘â–‘\n                                                            â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘\n                                                â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n                                             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n                        â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n             â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n          â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â–‘â–‘â–‘\n                â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n                   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n                        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n                           â–‘â–‘â–‘â–‘â–‘\n\n## Model card\n\nFirst model in the potential series.\n\n## Prompt format: Mistral\n\n```\n<s> [INST] QUERY_1 [/INST] ANSWER_1</s> [INST] QUERY_2 [/INST] ANSWER_2</s>...\n```\n\nBeware that some backends (like llama.cpp) add bos already (by default), so you don't need to prepend it yourself.\n\n## Settings\n\nDO NOT CHANGE ROPE SETTINGS. This model uses high freq base with 32k seen tokens, it should be fine for most tasks.\n\nOnly tested with temp 1 and top_p 0.95 with everything else disabled.\n\n<video src=\"https://cdn-uploads.huggingface.co/production/uploads/65ab93082bf3e0cbbf717850/cIEP5e43VP0k0caRzl16e.mp4\" controls=\"controls\" style=\"max-width: 720px;\">\n</video>",
    "meta_json": "{\"pipeline_tag\":null,\"library_name\":null,\"framework\":null,\"params\":null,\"storage_bytes\":115639116384,\"files_count\":5,\"spaces_count\":0,\"gated\":false,\"private\":false,\"config\":null}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": null,
    "compliance_status": "pending",
    "quality_score": 54.9,
    "content_hash": "a84d44e597feeb2292d93a25501cde5c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/miqudev/miqu-1-70b\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "huggingface:nuigurumi:basil_mix",
    "name": "basil_mix",
    "author": "nuigurumi",
    "description": "--- license: other --- - merged model. - realistic texture and Asian face. - designed to maintain a responsive reaction to danbooru based prompts. - This model and its derivatives(image, merged model) can be freely used for non-profit purposes only. - You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi. - Introducing the model itself is allowed ...",
    "tags": [
      "diffusers",
      "license:other",
      "endpoints_compatible",
      "diffusers:stablediffusionpipeline",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "likes": 984,
    "downloads": 22795,
    "source": "huggingface",
    "source_url": "https://huggingface.co/nuigurumi/basil_mix",
    "image_url": null,
    "type": "model",
    "body_content": "---\nlicense: other\n---\n## Model Description\n\n- merged model.\n- realistic texture and Asian face.\n- designed to maintain a responsive reaction to danbooru based prompts.\n\n## License\n  \n- This model and its derivatives(image, merged model) can be freely used for non-profit purposes only.\n- You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi.\n- Introducing the model itself is allowed for both commercial and non-commercial purposes, but please include the model name and a link to this repository when doing so.\n\n- ã“ã®ãƒ¢ãƒ‡ãƒ«åŠã³ãã®æ´¾ç”Ÿç‰©(ç”Ÿæˆç‰©ã€ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«)ã¯ã€å®Œå…¨ã«éžå–¶åˆ©ç›®çš„ã®ä½¿ç”¨ã«é™ã‚Šã€è‡ªç”±ã«åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n- ã‚ãªãŸãŒåŽå…¥ã‚„å¯„ä»˜ã‚’å¾—ã‚‹ã“ã¨ã®ã§ãã‚‹ã€ã‚‚ã—ãã¯å¾—ã‚‹äºˆå®šã®Webã‚µã‚¤ãƒˆã€ã‚¢ãƒ—ãƒªã€ãã®ä»–ã§ã“ã®ãƒ¢ãƒ‡ãƒ«åŠã³ãã®æ´¾ç”Ÿç‰©ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚åˆ©ç”¨ã—ãŸã„å ´åˆã¯[nuigurumi](https://twitter.com/nuigurumi1_KR)ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚\n- ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã®ç´¹ä»‹ã™ã‚‹ã“ã¨ã¯ã€å–¶åˆ©éžå–¶åˆ©ã‚’å•ã‚ãšè‡ªç”±ã§ã™ã€ãã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«åã¨å½“ãƒªãƒã‚¸ãƒˆãƒªã®ãƒªãƒ³ã‚¯ã‚’ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚\n\n- check [License](https://huggingface.co/nuigurumi/basil_mix/blob/main/License.md)\n  \n  \n  _èª­ã‚€ã®ã‚ã‚“ã©ãã•ã„äººå‘ã‘  \n  å•†ç”¨åˆ©ç”¨ã‚’ã™ã¹ã¦ç¦æ­¢ã—ã¾ã™ã€‚fanboxã‚„patreonãªã©ã®æ”¯æ´ã‚µã‚¤ãƒˆã§ã®ä½¿ç”¨ã‚‚å…¨ã¦ç¦æ­¢ã—ã¾ã™ã€‚  \n  ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«(cilled_re...ã¨ã‹)ã‚‚æ´¾ç”Ÿç‰©ãªã®ã§å•†ç”¨åˆ©ç”¨ç¦æ­¢ã«ãªã‚Šã¾ã™ã€‚ å•†ç”¨åˆ©ç”¨ã‚’ã—ãŸã„ãªã‚‰ç§ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚  \n  ã©ã“ã‹ã§ãƒ¢ãƒ‡ãƒ«ã‚’ç´¹ä»‹ã—ã¦ã„ãŸã ã‘ã‚‹ãªã‚‰ã€ãƒªãƒ³ã‚¯ã‚‚ä½µè¨˜ã—ã¦ãã‚Œã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚_ \n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run basil_mix:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/basil_mix)\n\n\n## Recommendations\n\n- VAE: [vae-ft-mse-840000](https://huggingface.co/stabilityai/sd-vae-ft-mse-original) from StabilityAI\n- Prompting: Simple prompts are better. Large amounts of quality tags and negative prompts can have negative effects.",
    "meta_json": "{\"pipeline_tag\":\"text-to-image\",\"library_name\":\"diffusers\",\"framework\":\"diffusers\",\"params\":null,\"storage_bytes\":17450358760,\"files_count\":21,\"spaces_count\":5,\"gated\":false,\"private\":false,\"config\":{\"diffusers\":{\"_class_name\":\"StableDiffusionPipeline\"}}}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "Other",
    "compliance_status": "approved",
    "quality_score": 49.9,
    "content_hash": "0403d07e994ea57e0c2ff79932b8edfd",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"huggingface\",\"source_url\":\"https://huggingface.co/nuigurumi/basil_mix\",\"fetched_at\":\"2025-12-10T01:31:39.550Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]