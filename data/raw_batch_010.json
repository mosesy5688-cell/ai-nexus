[
  {
    "id": "github:binhnguyennus:awesome-scalability",
    "name": "awesome-scalability",
    "author": "binhnguyennus",
    "description": "An updated and organized reading list for illustrating the patterns of scalable, reliable, and performant large-scale systems. Concepts are explained in the articles of prominent engineers and credible references. Case studies are taken from battle-tested systems that serve millions to billions of users. > Understand your problems: scalability problem (fast for a single user but slow under heavy load) or performance problem (slow for a single user) by reviewing some design principles and chec...",
    "tags": [
      "architecture",
      "awesome",
      "awesome-list",
      "backend",
      "big-data",
      "computer-science",
      "design-patterns",
      "devops",
      "distributed-systems",
      "interview",
      "interview-practice",
      "interview-questions",
      "lists",
      "machine-learning",
      "programming",
      "resources",
      "scalability",
      "system",
      "system-design",
      "web-development"
    ],
    "pipeline_tag": "other",
    "likes": 67100,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/binhnguyennus/awesome-scalability",
    "image_url": null,
    "type": "tool",
    "body_content": "[![Logo](/logo.png)](http://awesome-scalability.com/)\n\nAn updated and organized reading list for illustrating the patterns of scalable, reliable, and performant large-scale systems. Concepts are explained in the articles of prominent engineers and credible references. Case studies are taken from battle-tested systems that serve millions to billions of users.\n\n#### If your system goes slow\n> Understand your problems: scalability problem (fast for a single user but slow under heavy load) or performance problem (slow for a single user) by reviewing some [design principles](#principle) and checking how [scalability](#scalability) and [performance](#performance) problems are solved at tech companies. The section of [intelligence](#intelligence) are created for those who work with data and machine learning at big (data) and deep (learning) scale.\n\n#### If your system goes down\n> \"Even if you lose all one day, you can build all over again if you retain your calm!\" - Thuan Pham, former CTO of Uber. So, keep calm and mind the [availability](#availability) and [stability](#stability) matters! \n\n#### If you are having a system design interview\n> Look at some [interview notes](#interview) and [real-world architectures with completed diagrams](#architecture) to get a comprehensive view before designing your system on whiteboard. You can check some [talks](#talk) of engineers from tech giants to know how they build, scale, and optimize their systems. Good luck!\n\n#### If you are building your dream team\n> The goal of scaling team is not growing team size but increasing team output and value. You can find out how tech companies reach that goal in various aspects: hiring, management, organization, culture, and communication in the [organization](#organization) section.\n\n#### Community power\n\n> Contributions are greatly welcome! You may want to take a look at the [contribution guidelines](CONTRIBUTING.md). If you see a link here that is no longer maintained or is not a good fit, please submit a pull request!\n\n> Many long hours of hard work have gone into this project. If you find it helpful, please share on Facebook, [on Twitter](https://ctt.ec/V8B2p), [on Weibo](http://t.cn/RnjFLCB), or on your chat groups! Knowledge is power, knowledge shared is power multiplied. Thank you!\n\n## Content\n- [Principle](#principle)\n- [Scalability](#scalability)\n- [Availability](#availability)\n- [Stability](#stability)\n- [Performance](#performance)\n- [Intelligence](#intelligence)\n- [Architecture](#architecture)\n- [Interview](#interview)\n- [Organization](#organization)\n- [Talk](#talk)\n- [Book](#book)\n\n## Principle\n* [Lessons from Giant-Scale Services - Eric Brewer, UC Berkeley & Google](https://people.eecs.berkeley.edu/~brewer/papers/GiantScale-IEEE.pdf)\n* [Designs, Lessons and Advice from Building Large Distributed Systems - Jeff Dean, Google](https://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)\n* [How to Design a Good API & Why it Matters - Joshua Bloch, CMU & Google](https://www.infoq.com/presentations/effective-api-design)\n* [On Efficiency, Reliability, Scaling - James Hamilton, VP at AWS](http://mvdirona.com/jrh/work/)\n* [Principles of Chaos Engineering](https://www.usenix.org/conference/srecon17americas/program/presentation/rosenthal)\n* [Finding the Order in Chaos](https://www.usenix.org/conference/srecon16/program/presentation/lueder)\n* [The Twelve-Factor App](https://12factor.net/)\n* [Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)\n* [High Cohesion and Low Coupling](http://www.math-cs.gordon.edu/courses/cs211/lectures-2009/Cohesion,Coupling,MVC.pdf)\n* [Monoliths and Microservices](https://medium.com/@SkyscannerEng/monoliths-and-microservices-8c65708c3dbf)\n* [CAP Theorem and Trade-offs](http://robertgreiner.com/2014/08/cap-theorem-revisited/)\n* [CP Databases and AP Databases](https://blog.andyet.com/2014/10/01/right-database)\n* [Stateless vs Stateful Scalability](http://ithare.com/scaling-stateful-objects/)\t\n* [Scale Up vs Scale Out: Hidden Costs](https://blog.codinghorror.com/scaling-up-vs-scaling-out-hidden-costs/)\n* [ACID and BASE](https://neo4j.com/blog/acid-vs-base-consistency-models-explained/)\n* [Blocking/Non-Blocking and Sync/Async](https://blogs.msdn.microsoft.com/csliu/2009/08/27/io-concept-blockingnon-blocking-vs-syncasync/)\n* [Performance and Scalability of Databases](https://use-the-index-luke.com/sql/testing-scalability)\n* [Database Isolation Levels and Effects on Performance and Scalability](http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html)\n* [The Probability of Data Loss in Large Clusters](https://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html)\n* [Data Access for Highly-Scalable Solutions: Using SQL, NoSQL, and Polyglot Persistence](https://docs.microsoft.com/en-us/previous-versions/msp-n-p/dn271399(v=pandp.10))\n* [SQL vs NoSQL](https://www.upwork.com/hiring/data/sql-vs-nosql-databases-whats-the-difference/)\n* [SQL vs NoSQL - Lesson Learned at Salesforce](https://engineering.salesforce.com/sql-or-nosql-9eaf1d92545b)\n* [NoSQL Databases: Survey and Decision Guidance](https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d)\n* [How Sharding Works](https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6)\n* [Consistent Hashing](http://www.tom-e-white.com/2007/11/consistent-hashing.html)\n* [Consistent Hashing: Algorithmic Tradeoffs](https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8)\n* [Don’t be tricked by the Hashing Trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)\n* [Uniform Consistent Hashing at Netflix](https://medium.com/netflix-techblog/distributing-content-to-open-connect-3e3e391d4dc9)\n* [Eventually Consistent - Werner Vogels, CTO at Amazon](https://www.allthingsdistributed.com/2008/12/eventually_consistent.html)\n* [Cache is King](https://www.stevesouders.com/blog/2012/10/11/cache-is-king/)\n* [Anti-Caching](https://www.the-paper-trail.org/post/2014-06-06-paper-notes-anti-caching/)\n* [Understand Latency](http://highscalability.com/latency-everywhere-and-it-costs-you-sales-how-crush-it)\n* [Latency Numbers Every Programmer Should Know](http://norvig.com/21-days.html#answers)\n* [The Calculus of Service Availability](https://queue.acm.org/detail.cfm?id=3096459&__s=dnkxuaws9pogqdnxmx8i)\n* [Architecture Issues When Scaling Web Applications: Bottlenecks, Database, CPU, IO](http://highscalability.com/blog/2014/5/12/4-architecture-issues-when-scaling-web-applications-bottlene.html)\t\n* [Common Bottlenecks](http://highscalability.com/blog/2012/5/16/big-list-of-20-common-bottlenecks.html)\n* [Life Beyond Distributed Transactions](https://queue.acm.org/detail.cfm?id=3025012)\n* [Relying on Software to Redirect Traffic Reliably at Various Layers](https://www.usenix.org/conference/srecon15/program/presentation/taveira)\n* [Breaking Things on Purpose](https://www.usenix.org/conference/srecon17americas/program/presentation/andrus)\n* [Avoid Over Engineering](https://medium.com/@rdsubhas/10-modern-software-engineering-mistakes-bc67fbef4fc8)\n* [Scalability Worst Practices](https://www.infoq.com/articles/scalability-worst-practices)\n* [Use Solid Technologies - Don’t Re-invent the Wheel - Keep It Simple!](https://medium.com/@DataStax/instagram-engineerings-3-rules-to-a-scalable-cloud-application-architecture-c44afed31406)\n* [Simplicity by Distributing Complexity](https://engineering.zalando.com/posts/2018/01/simplicity-by-distributing-complexity.html)\n* [Why Over-Reusing is Bad](http://tech.transferwise.com/why-over-reusing-is-bad/)\n* [Performance is a Feature](https://blog.codinghorror.com/performance-is-a-feature/)\n* [Make Performance Part of Your Workflow](https://codeascraft.com/2014/12/11/make-performance-part-of-your-workflow/)\n* [The Benefits of Server Side Rendering over Client Side Rendering](https://medium.com/walmartlabs/the-benefits-of-server-side-rendering-over-client-side-rendering-5d07ff2cefe8)\n* [Automate and Abstract: Lessons at Facebook](https://architecht.io/lessons-from-facebook-on-engineering-for-scale-f5716f0afc7a)\n* [AWS Do's and Don'ts](https://8thlight.com/blog/sarah-sunday/2017/09/15/aws-dos-and-donts.html)\n* [(UI) Design Doesn’t Scale - Stanley Wood, Design Director at Spotify](https://medium.com/@hellostanley/design-doesnt-scale-4d81e12cbc3e)\n* [Linux Performance](http://www.brendangregg.com/linuxperf.html)\n* [Building Fast and Resilient Web Applications - Ilya Grigorik](https://www.igvita.com/2016/05/20/building-fast-and-resilient-web-applications/)\n* [Accept Partial Failures, Minimize Service Loss](https://www.usenix.org/conference/srecon17asia/program/presentation/wang_daxin)\n* [Design for Resiliency](http://highscalability.com/blog/2012/12/31/designing-for-resiliency-will-be-so-2013.html)\n* [Design for Self-healing](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/self-healing)\n* [Design for Scaling Out](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/scale-out)\t\n* [Design for Evolution](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/design-for-evolution)\n* [Learn from Mistakes](http://highscalability.com/blog/2013/8/26/reddit-lessons-learned-from-mistakes-made-scaling-to-1-billi.html)\n\n## Scalability\n* [Microservices and Orchestration](https://martinfowler.com/microservices/)\n\t* [Domain-Oriented Microservice Architecture at Uber](https://eng.uber.com/microservice-architecture/)\n\t* [Service Architecture (3 parts: Domain Gateways, Value-Added Services, BFF) at SoundCloud](https://developers.soundcloud.com/blog/service-architecture-3)\n\t* [Container (8 parts) at Riot Games](https://engineering.riotgames.com/news/thinking-inside-container)\n\t* [Containerization at Pinterest](https://medium.com/@Pinterest_Engineering/containerization-at-pinterest-92295347f2f3)\n\t* [Evolution of Container Usage at Netflix](https://medium.com/netflix-techblog/the-evolution-of-container-usage-at-netflix-3abfc096781b)\n\t* [Dockerizing MySQL at Uber](https://eng.uber.com/dockerizing-mysql/)\n\t* [Testing of Microservices at Spotify](https://labs.spotify.com/2018/01/11/testing-of-microservices/)\n\t* [Docker in Production at Treehouse](https://medium.com/treehouse-engineering/lessons-learned-running-docker-in-production-5dce99ece770)\n\t* [Microservice at SoundCloud](https://developers.soundcloud.com/blog/inside-a-soundcloud-microservice)\n\t* [Operate Kubernetes Reliably at Stripe](https://stripe.com/blog/operating-kubernetes)\n\t* [Cross-Cluster Traffic Mirroring with Istio at Trivago](https://tech.trivago.com/2020/06/10/cross-cluster-traffic-mirroring-with-istio/)\n\t* [Agrarian-Scale Kubernetes (3 parts) at New York Times](https://open.nytimes.com/agrarian-scale-kubernetes-part-3-ee459887ed7e)\n\t* [Nanoservices at BBC](https://medium.com/bbc-design-engineering/powering-bbc-online-with-nanoservices-727840ba015b)\n\t* [PowerfulSeal: Testing Tool for Kubernetes Clusters at Bloomberg](https://www.techatbloomberg.com/blog/powerfulseal-testing-tool-kubernetes-clusters/)\n\t* [Conductor: Microservices Orchestrator at Netflix](https://medium.com/netflix-techblog/netflix-conductor-a-microservices-orchestrator-2e8d4771bf40)\n\t* [Docker Containers that Power Over 100.000 Online Shops at Shopify](https://shopifyengineering.myshopify.com/blogs/engineering/docker-at-shopify-how-we-built-containers-that-power-over-100-000-online-shops)\n\t* [Microservice Architecture at Medium](https://medium.engineering/microservice-architecture-at-medium-9c33805eb74f)\n\t* [From bare-metal to Kubernetes at Betabrand](https://boxunix.com/post/bare_metal_to_kube/)\n\t* [Kubernetes at Tinder](https://medium.com/tinder-engineering/tinders-move-to-kubernetes-cda2a6372f44)\n\t* [Kubernetes at Quora](https://www.quora.com/q/quoraengineering/Adopting-Kubernetes-at-Quora)\t\n\t* [Kubernetes Platform at Pinterest](https://medium.com/pinterest-engineering/building-a-kubernetes-platform-at-pinterest-fb3d9571c948)\n\t* [Microservices at Nubank](https://medium.com/building-nubank/microservices-at-nubank-an-overview-2ebcb336c64d)\n\t* [Payment Transaction Management in Microservices at Mercari](https://engineering.mercari.com/en/blog/entry/20210831-2019-06-07-155849/)\n\t* [Service Mesh at Snap](https://eng.snap.com/monolith-to-multicloud-microservices-snap-service-mesh)\n\t* [GRIT: Protocol for Distributed Transactions across Microservices at eBay](https://tech.ebayinc.com/engineering/grit-a-protocol-for-distributed-transactions-across-microservices/)\n\t* [Rubix: Kubernetes at Palantir](https://medium.com/palantir/introducing-rubix-kubernetes-at-palantir-ab0ce16ea42e)\n\t* [CRISP: Critical Path Analysis for Microservice Architectures at Uber](https://eng.uber.com/crisp-critical-path-analysis-for-microservice-architectures/)\n* [Distributed Caching](https://www.wix.engineering/post/scaling-to-100m-to-cache-or-not-to-cache)\n\t* [EVCache: Distributed In-memory Caching at Netflix](https://medium.com/netflix-techblog/caching-for-a-global-netflix-7bcc457012f1)\n\t* [EVCache Cache Warmer Infrastructure at Netflix](https://medium.com/netflix-techblog/cache-warming-agility-for-a-stateful-service-2d3b1da82642)\n\t* [Memsniff: Robust Memcache Traffic Analyzer at Box](https://blog.box.com/blog/introducing-memsniff-robust-memcache-traffic-analyzer/)\n\t* [Caching with Consistent Hashing and Cache Smearing at Etsy](https://codeascraft.com/2017/11/30/how-etsy-caches/)\n\t* [Analysis of Photo Caching at Facebook](https://code.facebook.com/posts/220956754772273/an-analysis-of-facebook-photo-caching/)\n\t* [Cache Efficiency Exercise at Facebook](https://code.facebook.com/posts/964122680272229/web-performance-cache-efficiency-exercise/)\n\t* [tCache: Scalable Data-aware Java Caching at Trivago](http://tech.trivago.com/2015/10/15/tcache/)\n\t* [Pycache: In-process Caching at Quora](https://quoraengineering.quora.com/Pycache-lightning-fast-in-process-caching)\n\t* [Reduce Memcached Memory Usage by 50% at Trivago](http://tech.trivago.com/2017/12/19/how-trivago-reduced-memcached-memory-usage-by-50/)\n\t* [Caching Internal Service Calls at Yelp](https://engineeringblog.yelp.com/2018/03/caching-internal-service-calls-at-yelp.html)\n\t* [Estimating the Cache Efficiency using Big Data at Allegro](https://allegro.tech/2017/01/estimating-the-cache-efficiency-using-big-data.html)\n\t* [Distributed Cache at Zalando](https://engineering.zalando.com/posts/2018/04/distributed-cache-akka-kubernetes.html)\n\t* [Distributed Cache for S3 at ClickHouse](https://clickhouse.com/blog/building-a-distributed-cache-for-s3)\n\t* [Application Data Caching from RAM to SSD at NetFlix](https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690)\n\t* [Tradeoffs of Replicated Cache at Skyscanner](https://medium.com/@SkyscannerEng/the-tradeoffs-of-a-replicated-cache-b6680c722f58)\n\t* [Location Caching with Quadtrees at Yext](http://engblog.yext.com/post/geolocation-caching)\n\t* [Video Metadata Caching at Vimeo](https://medium.com/vimeo-engineering-blog/video-metadata-caching-at-vimeo-a54b25f0b304)\n\t* [Scaling Redis at Twitter](http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html)\n\t* [Scaling Job Queue with Redis at Slack](https://slack.engineering/scaling-slacks-job-queue-687222e9d100)\n\t* [Moving persistent data out of Redis at Github](https://githubengineering.com/moving-persistent-data-out-of-redis/)\n\t* [Storing Hundreds of Millions of Simple Key-Value Pairs in Redis at Instagram](https://engineering.instagram.com/storing-hundreds-of-millions-of-simple-key-value-pairs-in-redis-1091ae80f74c)\n\t* [Redis at Trivago](http://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/)\n\t* [Optimizing Redis Storage at Deliveroo](https://deliveroo.engineering/2017/01/19/optimising-membership-queries.html)\n\t* [Memory Optimization in Redis at Wattpad](http://engineering.wattpad.com/post/23244724794/store-more-stuff-memory-optimization-in-redis)\n\t* [Redis Fleet at Heroku](https://blog.heroku.com/rolling-redis-fleet)\n\t* [Solving Remote Build Cache Misses (2 parts) at SoundCloud](https://developers.soundcloud.com/blog/gradle-remote-build-cache-misses-part-2)\n\t* [Ratings & Reviews (2 parts) at Flipkart](https://blog.flipkart.tech/ratings-reviews-flipkart-part-2-574ab08e75cf)\n\t* [Prefetch Caching of Items at eBay](https://tech.ebayinc.com/engineering/prefetch-caching-of-ebay-items/)\n\t* [Cross-Region Caching Library at Wix](https://www.wix.engineering/post/how-we-built-a-cross-region-caching-library)\n\t* [Improving Distributed Caching Performance and Efficiency at Pinterest](https://medium.com/pinterest-engineering/improving-distributed-caching-performance-and-efficiency-at-pinterest-92484b5fe39b)\n\t* [Standardize and Improve Microservices Caching at DoorDash](https://doordash.engineering/2023/10/19/how-doordash-standardized-and-improved-microservices-caching/)\n    * [HTTP Caching and CDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching)\n        * [Zynga Geo Proxy: Reducing Mobile Game Latency at Zynga](https://www.zynga.com/blogs/engineering/zynga-geo-proxy-reducing-mobile-game-latency)\n        * [Google AMP at Condé Nast](https://technology.condenast.com/story/the-why-and-how-of-google-amp-at-conde-nast)\n        * [A/B Tests on Hosting Infrastructure (CDNs) at Deliveroo](https://deliveroo.engineering/2016/09/19/ab-testing-cdns.html)\n        * [HAProxy with Kubernetes for User-facing Traffic at SoundCloud](https://developers.soundcloud.com/blog/how-soundcloud-uses-haproxy-with-kubernetes-for-user-facing-traffic)\n        * [Bandaid: Service Proxy at Dropbox](https://blogs.dropbox.com/tech/2018/03/meet-bandaid-the-dropbox-service-proxy/)\n\t\t* [Service Workers at Slack](https://slack.engineering/service-workers-at-slack-our-quest-for-faster-boot-times-and-offline-support-3492cf79c88)\n\t\t* [CDN Services at Spotify](https://labs.spotify.com/2020/02/24/how-spotify-aligned-cdn-services-for-a-lightning-fast-streaming-experience/)\n* [Distributed Locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n\t* [Chubby: Lock Service for Loosely Coupled Distributed Systems at Google](https://blog.acolyer.org/2015/02/13/the-chubby-lock-service-for-loosely-coupled-distributed-systems/)\n\t* [Distributed Locking at Uber](https://www.youtube.com/watch?v=MDuagr729aU)\n\t* [Distributed Locks using Redis at GoSquared](https://engineering.gosquared.com/distributed-locks-using-redis)\n\t* [ZooKeeper at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/zookeeper-at-twitter.html)\n\t* [Eliminating Duplicate Queries using Distributed Locking at Chartio](https://chartio.com/blog/eliminating-duplicate-queries-using-distributed-locking/)\n* [Distributed Tracking, Tracing, and Measuring](https://www.oreilly.com/ideas/understanding-the-value-of-distributed-tracing)\n\t* [Zipkin: Distributed Systems Tracing at Twitter](https://blog.twitter.com/engineering/en_us/a/2012/distributed-systems-tracing-with-zipkin.html)\n\t* [Improve Zipkin Traces using Kubernetes Pod Metadata at SoundCloud](https://developers.soundcloud.com/blog/using-kubernetes-pod-metadata-to-improve-zipkin-traces)\n\t* [Canopy: Scalable Distributed Tracing & Analysis at Facebook](https://www.infoq.com/presentations/canopy-scalable-tracing-analytics-facebook)\n\t* [Pintrace: Distributed Tracing at Pinterest](https://medium.com/@Pinterest_Engineering/distributed-tracing-at-pinterest-with-new-open-source-tools-a4f8a5562f6b)\n\t* [XCMetrics: All-in-One Tool for Tracking Xcode Build Metrics at Spotify](https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/)\n\t* [Real-time Distributed Tracing at LinkedIn](https://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency)\t\n\t* [Tracking Service Infrastructure at Scale at Shopify](https://www.usenix.org/conference/srecon17americas/program/presentation/arthorne)\t\n\t* [Distributed Tracing at HelloFresh](https://engineering.hellofresh.com/scaling-hellofresh-distributed-tracing-7b182928247d)\n\t* [Analyzing Distributed Trace Data at Pinterest](https://medium.com/@Pinterest_Engineering/analyzing-distributed-trace-data-6aae58919949)\n\t* [Distributed Tracing at Uber](https://eng.uber.com/distributed-tracing/)\n\t* [JVM Profiler: Tracing Distributed JVM Applications at Uber](https://eng.uber.com/jvm-profiler/)\n\t* [Data Checking at Dropbox](https://www.usenix.org/conference/srecon17asia/program/presentation/mah)\n\t* [Tracing Distributed Systems at Showmax](https://tech.showmax.com/2016/10/tracing-distributed-systems-at-showmax/)\n\t* [osquery Across the Enterprise at Palantir](https://medium.com/@palantir/osquery-across-the-enterprise-3c3c9d13ec55)\n\t* [StatsD at Etsy](https://codeascraft.com/2011/02/15/measure-anything-measure-everything/)\n* [Distributed Scheduling](https://www.csee.umbc.edu/courses/graduate/CMSC621/fall02/lectures/ch11.pdf)\n\t* [Distributed Task Scheduling (3 parts) at PagerDuty](https://www.pagerduty.com/eng/distributed-task-scheduling-3/)\n    * [Building Cron at Google](https://landing.google.com/sre/sre-book/chapters/distributed-periodic-scheduling/)\n    * [Distributed Cron Architecture at Quora](https://quoraengineering.quora.com/Quoras-Distributed-Cron-Architecture)\n    * [Chronos: A Replacement for Cron at Airbnb](https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d)\n    * [Scheduler at Nextdoor](https://engblog.nextdoor.com/we-don-t-run-cron-jobs-at-nextdoor-6f7f9cc62040)\n    * [Peloton: Unified Resource Scheduler for Diverse Cluster Workloads at Uber](https://eng.uber.com/peloton/)\n    * [Fenzo: OSS Scheduler for Apache Mesos Frameworks at Netflix](https://medium.com/netflix-techblog/fenzo-oss-scheduler-for-apache-mesos-frameworks-5c340e77e543)\n    * [Airflow - Workflow Orchestration](https://airflow.apache.org/)\n\t\t* [Airflow at Airbnb](https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8)\n\t\t* [Airflow at Adyen](https://www.adyen.com/knowledge-hub/apache-airflow-at-adyen)\n\t\t* [Airflow at Pandora](https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee)\n        * [Airflow at Robinhood](https://medium.com/robinhood-engineering/why-robinhood-uses-airflow-aed13a9a90c8)\n        * [Airflow at Lyft](https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff)\n        * [Airflow at Drivy](https://drivy.engineering/airflow-architecture/)\n\t\t* [Airflow at Grab](https://engineering.grab.com/experimentation-platform-data-pipeline)\n\t\t* [Airflow at Adobe](https://medium.com/adobetech/adobe-experience-platform-orchestration-service-with-apache-airflow-952203723c0b)\n        * [Auditing Airflow Job Runs at Walmart](https://medium.com/walmartlabs/auditing-airflow-batch-jobs-73b45100045)\n        * [MaaT: DAG-based Distributed Task Scheduler at Alibaba](https://hackernoon.com/meet-maat-alibabas-dag-based-distributed-task-scheduler-7c9cf0c83438)\n        * [boundary-layer: Declarative Airflow Workflows at Etsy](https://www.etsy.com/codeascraft/boundary-layer-declarative-airflow-workflows)\n* [Distributed Monitoring and Alerting](https://www.oreilly.com/ideas/monitoring-distributed-systems)\n\t* [Unicorn: Remediation System at eBay](https://www.ebayinc.com/stories/blogs/tech/unicorn-rheos-remediation-center/)\n\t* [M3: Metrics and Monitoring Platform at Uber](https://eng.uber.com/optimizing-m3/)\n\t* [Athena: Automated Build Health Management System at Dropbox](https://blogs.dropbox.com/tech/2019/05/athena-our-automated-build-health-management-system/)\n\t* [Vortex: Monitoring Server Applications at Dropbox](https://blogs.dropbox.com/tech/2019/11/monitoring-server-applications-with-vortex/)\t\n\t* [Nuage: Cloud Management Service at LinkedIn](https://engineering.linkedin.com/blog/2019/solving-manageability-challenges-with-nuage)\n\t* [Telltale: Application Monitoring at Netflix](https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba)\n\t* [ThirdEye: Monitoring Platform at LinkedIn](https://engineering.linkedin.com/blog/2019/06/smart-alerts-in-thirdeye--linkedins-real-time-monitoring-platfor)\n\t* [Periskop: Exception Monitoring Service at SoundCloud](https://developers.soundcloud.com/blog/periskop-exception-monitoring-service)\n    * [Securitybot: Distributed Alerting Bot at Dropbox](https://blogs.dropbox.com/tech/2017/02/meet-securitybot-open-sourcing-automated-security-at-scale/)\t\n    * [Monitoring System at Alibaba](https://www.usenix.org/conference/srecon18asia/presentation/xinchi)\n    * [Real User Monitoring at Dailymotion](https://medium.com/dailymotion/real-user-monitoring-1948375f8be5)\n    * [Alerting Ecosystem at Uber](https://eng.uber.com/observability-at-scale/)\n\t* [Alerting Framework at Airbnb](https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f)\n\t* [Alerting on Service-Level Objectives (SLOs) at SoundCloud](https://developers.soundcloud.com/blog/alerting-on-slos)\n    * [Job-based Forecasting Workflow for Observability Anomaly Detection at Uber](https://eng.uber.com/observability-anomaly-detection/)\n\t* [Monitoring and Alert System using Graphite and Cabot at HackerEarth](http://engineering.hackerearth.com/2017/03/21/monitoring-and-alert-system-using-graphite-and-cabot/)\n    * [Observability (2 parts) at Twitter](https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-ii.html)\n    * [Distributed Security Alerting at Slack](https://slack.engineering/distributed-security-alerting-c89414c992d6)\n    * [Real-Time News Alerting at Bloomberg](https://www.infoq.com/presentations/news-alerting-bloomberg)\n\t* [Data Pipeline Monitoring System at LinkedIn](https://engineering.linkedin.com/blog/2019/an-inside-look-at-linkedins-data-pipeline-monitoring-system-)\n\t* [Monitoring and Observability at Picnic](https://blog.picnic.nl/monitoring-and-observability-at-picnic-684cefd845c4)\n* [Distributed Security](https://msdn.microsoft.com/en-us/library/cc767123.aspx)\n\t* [Approach to Security at Scale at Dropbox](https://blogs.dropbox.com/tech/2018/02/security-at-scale-the-dropbox-approach/)\n\t* [Aardvark and Repokid: AWS Least Privilege for Distributed, High-Velocity Development at Netflix](https://medium.com/netflix-techblog/introducing-aardvark-and-repokid-53b081bf3a7e)\t\n\t* [LISA: Distributed Firewall at LinkedIn](https://www.slideshare.net/MikeSvoboda/2017-lisa-linkedins-distributed-firewall-dfw)\n\t* [Secure Infrastructure To Store Bitcoin In The Cloud at Coinbase](https://engineering.coinbase.com/how-coinbase-builds-secure-infrastructure-to-store-bitcoin-in-the-cloud-30a6504e40ba)\n\t* [BinaryAlert: Real-time Serverless Malware Detection at Airbnb](https://medium.com/airbnb-engineering/binaryalert-real-time-serverless-malware-detection-ca44370c1b90)\n\t* [Scalable IAM Architecture to Secure Access to 100 AWS Accounts at Segment](https://segment.com/blog/secure-access-to-100-aws-accounts/)\n\t* [OAuth Audit Toolbox at Indeed](http://engineering.indeedblog.com/blog/2018/04/oaudit-toolbox/)\n\t* [Active Directory Password Blacklisting at Yelp](https://engineeringblog.yelp.com/2018/04/ad-password-blacklisting.html)\t\n\t* [Syscall Auditing at Scale at Slack](https://slack.engineering/syscall-auditing-at-scale-e6a3ca8ac1b8)\n\t* [Athenz: Fine-Grained, Role-Based Access Control at Yahoo](https://yahooeng.tumblr.com/post/160481899076/open-sourcing-athenz-fine-grained-role-based)\n\t* [WebAuthn Support for Secure Sign In at Dropbox](https://blogs.dropbox.com/tech/2018/05/introducing-webauthn-support-for-secure-dropbox-sign-in/)\n\t* [Security Development Lifecycle at Slack](https://slack.engineering/moving-fast-and-securing-things-540e6c5ae58a)\n\t* [Unprivileged Container Builds at Kinvolk](https://kinvolk.io/blog/2018/04/towards-unprivileged-container-builds/)\n\t* [Diffy: Differencing Engine for Digital Forensics in the Cloud at Netflix](https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698)\n\t* [Detecting Credential Compromise in AWS at Netflix](https://medium.com/netflix-techblog/netflix-cloud-security-detecting-credential-compromise-in-aws-9493d6fd373a)\n\t* [Scalable User Privacy at Spotify](https://labs.spotify.com/2018/09/18/scalable-user-privacy/)\n\t* [AVA: Audit Web Applications at Indeed](https://engineering.indeedblog.com/blog/2018/09/application-scanning/)\n\t* [TTL as a Service: Automatic Revocation of Stale Privileges at Yelp](https://engineeringblog.yelp.com/2018/11/ttl-as-a-service.html)\n\t* [Enterprise Key Management at Slack](https://slack.engineering/engineering-dive-into-slack-enterprise-key-management-1fce471b178c)\t\n\t* [Scalability and Authentication at Twitch](https://blog.twitch.tv/en/2019/03/15/how-twitch-addresses-scalability-and-authentication/)\n\t* [Edge Authentication and Token-Agnostic Identity Propagation at Netflix](https://netflixtechblog.com/edge-authentication-and-token-agnostic-identity-propagation-514e47e0b602)\n\t* [Hardening Kubernetes Infrastructure with Cilium at Palantir](https://blog.palantir.com/hardening-palantirs-kubernetes-infrastructure-with-cilium-1c40d4c7ef0)\n\t* [Improving Web Vulnerability Management through Automation at Lyft](https://eng.lyft.com/improving-web-vulnerability-management-through-automation-2631570d8415)\n\t* [Clock Skew when Syncing Password Payloads at Drobbox](https://dropbox.tech/application/dropbox-passwords-clock-skew-payload-sync-merge)\n* [Distributed Messaging, Queuing, and Event Streaming](https://arxiv.org/pdf/1704.00411.pdf)\n\t* [Cape: Event Stream Processing Framework at Dropbox](https://blogs.dropbox.com/tech/2017/05/introducing-cape/)\n\t* [Brooklin: Distributed Service for Near Real-Time Data Streaming at LinkedIn](https://engineering.linkedin.com/blog/2019/brooklin-open-source)\n\t* [Samza: Stream Processing System for Latency Insighs at LinkedIn](https://engineering.linkedin.com/blog/2018/04/samza-aeon--latency-insights-for-asynchronous-one-way-flows)\t\n\t* [Bullet: Forward-Looking Query Engine for Streaming Data at Yahoo](https://yahooeng.tumblr.com/post/161855616651/open-sourcing-bullet-yahoos-forward-looking)\n\t* [EventHorizon: Tool for Watching Events Streaming at Etsy](https://codeascraft.com/2018/05/29/the-eventhorizon-saga/)\n\t* [Qmessage: Distributed, Asynchronous Task Queue at Quora](https://quoraengineering.quora.com/Qmessage-Handling-Billions-of-Tasks-Per-Day)\n\t* [Cherami: Message Queue System for Transporting Async Tasks at Uber](https://eng.uber.com/cherami/)\n\t* [Dynein: Distributed Delayed Job Queueing System at Airbnb](https://medium.com/airbnb-engineering/dynein-building-a-distributed-delayed-job-queueing-system-93ab10f05f99)\n\t* [Timestone: Queueing System for Non-Parallelizable Workloads at Netflix](https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f)\n\t* [Messaging Service at Riot Games](https://engineering.riotgames.com/news/riot-messaging-service)\n\t* [Messaging System Model at Dropbox](https://dropbox.tech/infrastructure/infrastructure-messaging-system-model-async-platform-evolution)\n\t* [Debugging Production with Event Logging at Zillow](https://www.zillow.com/engineering/debugging-production-event-logging/)\n\t* [Cross-platform In-app Messaging Orchestration Service at Netflix](https://medium.com/netflix-techblog/building-a-cross-platform-in-app-messaging-orchestration-service-86ba614f92d8)\n\t* [Video Gatekeeper at Netflix](https://medium.com/netflix-techblog/re-architecting-the-video-gatekeeper-f7b0ac2f6b00)\n\t* [Scaling Push Messaging for Millions of Devices at Netflix](https://www.infoq.com/presentations/neflix-push-messaging-scale)\n\t* [Delaying Asynchronous Message Processing with RabbitMQ at Indeed](http://engineering.indeedblog.com/blog/2017/06/delaying-messages/)\t\n\t* [Benchmarking Streaming Computation Engines at Yahoo](https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at)\n\t* [Improving Stream Data Quality With Protobuf Schema Validation at Deliveroo](https://deliveroo.engineering/2019/02/05/improving-stream-data-quality-with-protobuf-schema-validation.html)\n\t* [Scaling Email Infrastructure at Medium](https://medium.engineering/scaling-email-infrastructure-for-medium-digest-254223c883b8)\n\t* [Real-time Messaging at Slack](https://slack.engineering/real-time-messaging/)\n\t* [Event Stream Database at Nike](https://medium.com/nikeengineering/moving-faster-with-aws-by-creating-an-event-stream-database-dedec8ca3eeb)\n\t* [Event Tracking System at Udemy](https://medium.com/udemy-engineering/designing-the-new-event-tracking-system-at-udemy-a45e502216fd)\n    * [Event-Driven Messaging](https://martinfowler.com/articles/201701-event-driven.html)\n        * [Domain-Driven Design at Alibaba](https://medium.com/swlh/creating-coding-excellence-with-domain-driven-design-88f73d2232c3)\n        * [Domain-Driven Design at Weebly](https://medium.com/weebly-engineering/how-to-organize-your-monolith-before-breaking-it-into-services-69cbdb9248b0)\n        * [Domain-Driven Design at Moonpig](https://engineering.moonpig.com/development/modelling-for-domain-driven-design)\n        * [Scaling Event Sourcing for Netflix Downloads](https://www.infoq.com/presentations/netflix-scale-event-sourcing)\n        * [Scaling Event-Sourcing at Jet.com](https://medium.com/@eulerfx/scaling-event-sourcing-at-jet-9c873cac33b8)\n        * [Event Sourcing (2 parts) at eBay](https://www.ebayinc.com/stories/blogs/tech/event-sourcing-in-action-with-ebays-continuous-delivery-team/)\n\t\t* [Event Sourcing at FREE NOW](https://medium.com/inside-freenow/event-sourcing-an-evolutionary-perspective-31e7387aa6f1)\n\t\t* [Scalable content feed using Event Sourcing and CQRS patterns at Brainly](https://medium.com/engineering-brainly/scalable-content-feed-using-event-sourcing-and-cqrs-patterns-e09df98bf977)\n    * [Pub-Sub Messaging](https://aws.amazon.com/pub-sub-messaging/)\n\t\t* [Pulsar: Pub-Sub Messaging at Scale at Yahoo](https://yahooeng.tumblr.com/post/150078336821/open-sourcing-pulsar-pub-sub-messaging-at-scale)\n\t\t* [Wormhole: Pub-Sub System at Facebook](https://code.facebook.com/posts/188966771280871/wormhole-pub-sub-system-moving-data-through-space-and-time/)\n\t\t* [MemQ: Cloud Native Pub-Sub System at Pinterest](https://medium.com/pinterest-engineering/memq-an-efficient-scalable-cloud-native-pubsub-system-4402695dd4e7)\n\t\t* [Pub-Sub in Microservices at Netflix](https://medium.com/netflix-techblog/how-netflix-microservices-tackle-dataset-pub-sub-4a068adcc9a)\n\t* [Kafka - Message Broker](https://martin.kleppmann.com/papers/kafka-debull15.pdf)\t\n\t\t* [Kafka at LinkedIn](https://engineering.linkedin.com/kafka/running-kafka-scale)\n\t\t* [Kafka at Pinterest](https://medium.com/pinterest-engineering/how-pinterest-runs-kafka-at-scale-ff9c6f735be)\n\t\t* [Kafka at Trello](https://tech.trello.com/why-we-chose-kafka/)\t\n\t\t* [Kafka at Salesforce](https://engineering.salesforce.com/how-apache-kafka-inspired-our-platform-events-architecture-2f351fe4cf63)\n\t\t* [Kafka at The New York Times](https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077)\n\t\t* [Kafka at Yelp](https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html)\n\t\t* [Kafka at Criteo](https://medium.com/criteo-labs/upgrading-kafka-on-a-large-infra-3ee99f56e970)\n\t\t* [Kafka on Kubernetes at Shopify](https://shopifyengineering.myshopify.com/blogs/engineering/running-apache-kafka-on-kubernetes-at-shopify)\n\t\t* [Kafka on PaaSTA: Running Kafka on Kubernetes at Yelp (2 parts)](https://engineeringblog.yelp.com/2022/03/kafka-on-paasta-part-two.html)\n\t\t* [Migrating Kafka's Zookeeper with No Downtime at Yelp](https://engineeringblog.yelp.com/2019/01/migrating-kafkas-zookeeper-with-no-downtime.html)\n\t\t* [Reprocessing and Dead Letter Queues with Kafka at Uber](https://eng.uber.com/reliable-reprocessing/)\n\t\t* [Chaperone: Audit Kafka End-to-End at Uber](https://eng.uber.com/chaperone/)\n\t\t* [Finding Kafka throughput limit in infrastructure at Dropbox](https://blogs.dropbox.com/tech/2019/01/finding-kafkas-throughput-limit-in-dropbox-infrastructure/)\n\t\t* [Cost Orchestration at Walmart](https://medium.com/walmartlabs/cost-orchestration-at-walmart-f34918af67c4)\n\t\t* [InfluxDB and Kafka to Scale to Over 1 Million Metrics a Second at Hulu](https://medium.com/hulu-tech-blog/how-hulu-uses-influxdb-and-kafka-to-scale-to-over-1-million-metrics-a-second-1721476aaff5)\n\t\t* [Scaling Kafka to Support Data Growth at PayPal](https://medium.com/paypal-tech/scaling-kafka-to-support-paypals-data-growth-a0b4da420fab)\n\t* [Stream Data Deduplication](https://en.wikipedia.org/wiki/Data_deduplication)\n\t\t* [Exactly-once Semantics with Kafka](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)\n\t\t* [Real-time Deduping at Tapjoy](http://eng.tapjoy.com/blog-list/real-time-deduping-at-scale)\n\t\t* [Deduplication at Segment](https://segment.com/blog/exactly-once-delivery/)\n\t\t* [Deduplication at Mail.Ru](https://medium.com/@andrewsumin/efficient-storage-how-we-went-down-from-50-pb-to-32-pb-99f9c61bf6b4)\n\t\t* [Petabyte Scale Data Deduplication at Mixpanel](https://medium.com/mixpaneleng/petabyte-scale-data-deduplication-mixpanel-engineering-e808c70c99f8)\n* [Distributed Logging](https://blog.codinghorror.com/the-problem-with-logging/)\n\t* [Logging at LinkedIn](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)\n\t* [Scalable and Reliable Log Ingestion at Pinterest](https://medium.com/@Pinterest_Engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754)\n\t* [High-performance Replicated Log Service at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2015/building-distributedlog-twitter-s-high-performance-replicated-log-servic.html)\n\t* [Logging Service with Spark at CERN Accelerator](https://databricks.com/blog/2017/12/14/the-architecture-of-the-next-cern-accelerator-logging-service.html)\n\t* [Logging and Aggregation at Quora](https://quoraengineering.quora.com/Logging-and-Aggregation-at-Quora)\n\t* [Collection and Analysis of Daemon Logs at Badoo](https://badoo.com/techblog/blog/2016/06/06/collection-and-analysis-of-daemon-logs-at-badoo/)\n\t* [Log Parsing with Static Code Analysis at Palantir](https://medium.com/palantir/using-static-code-analysis-to-improve-log-parsing-18f0d1843965)\t\t\n\t* [Centralized Application Logging at eBay](https://tech.ebayinc.com/engineering/low-latency-and-high-throughput-cal-ingress/)\n\t* [Enrich VPC Flow Logs at Hyper Scale to provide Network Insight at Netflix](https://netflixtechblog.com/hyper-scale-vpc-flow-logs-enrichment-to-provide-network-insight-e5f1db02910d)\t\n\t* [BookKeeper: Distributed Log Storage at Yahoo](https://yahooeng.tumblr.com/post/109908973316/bookkeeper-yahoos-distributed-log-storage-is)\n\t* [LogDevice: Distributed Data Store for Logs at Facebook](https://code.facebook.com/posts/357056558062811/logdevice-a-distributed-data-store-for-logs/)\n\t* [LogFeeder: Log Collection System at Yelp](https://engineeringblog.yelp.com/2018/03/introducing-logfeeder.html)\n\t* [DBLog: Generic Change-Data-Capture Framework at Netflix](https://medium.com/netflix-techblog/dblog-a-generic-change-data-capture-framework-69351fb9099b)\t\n* [Distributed Searching](http://nwds.cs.washington.edu/files/nwds/pdf/Distributed-WR.pdf)\n\t* [Search Architecture at Instagram](https://instagram-engineering.com/search-architecture-eeb34a936d3a)\n\t* [Search Architecture at eBay](http://www.cs.otago.ac.nz/homepages/andrew/papers/2017-8.pdf)\n\t* [Search Architecture at Box](https://medium.com/box-tech-blog/scaling-box-search-using-lumos-22d9e0cb4175)\n\t* [Search Discovery Indexing Platform at Coupang](https://medium.com/coupang-tech/the-evolution-of-search-discovery-indexing-platform-fa43e41305f9)\n\t* [Universal Search System at Pinterest](https://medium.com/pinterest-engineering/building-a-universal-search-system-for-pinterest-e4cb03a898d4)\n\t* [Improving Search Engine Efficiency by over 25% at eBay](https://www.ebayinc.com/stories/blogs/tech/making-e-commerce-search-faster/)\t\n\t* [Indexing and Querying Telemetry Logs with Lucene at Palantir](https://medium.com/palantir/indexing-and-querying-telemetry-logs-with-lucene-234c5ce3e5f3)\n\t* [Query Understanding at TripAdvisor](https://www.tripadvisor.com/engineering/query-understanding-at-tripadvisor/)\n\t* [Search Federation Architecture at LinkedIn (2018)](https://engineering.linkedin.com/blog/2018/03/search-federation-architecture-at-linkedin)\n\t* [Search at Slack](https://slack.engineering/search-at-slack-431f8c80619e)\n\t* [Search Engine at DoorDash](https://careersatdoordash.com/blog/introducing-doordashs-in-house-search-engine/)\n\t* [Stability and Scalability for Search at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2022/stability-and-scalability-for-search)\n\t* [Search Service at Twitter (2014)](https://blog.twitter.com/engineering/en_us/a/2014/building-a-complete-tweet-index.html)\n\t* [Autocomplete Search (2 parts) at Traveloka](https://medium.com/traveloka-engineering/high-quality-autocomplete-search-part-2-d5b15bb0dadf)\n\t* [Data-Driven Autocorrection System at Canva](https://product.canva.com/building-a-data-driven-autocorrection-system/)\n\t* [Adapting Search to Indian Phonetics at Flipkart](https://blog.flipkart.tech/adapting-search-to-indian-phonetics-cdbe65259686)\n\t* [Nautilus: Search Engine at Dropbox](https://blogs.dropbox.com/tech/2018/09/architecture-of-nautilus-the-new-dropbox-search-engine/)\n\t* [Galene: Search Architecture of LinkedIn](https://engineering.linkedin.com/search/did-you-mean-galene)\n\t* [Manas: High Performing Customized Search System at Pinterest](https://medium.com/@Pinterest_Engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f)\n\t* [Sherlock: Near Real Time Search Indexing at Flipkart](https://blog.flipkart.tech/sherlock-near-real-time-search-indexing-95519783859d)\n\t* [Nebula: Storage Platform to Build Search Backends at Airbnb](https://medium.com/airbnb-engineering/nebula-as-a-storage-platform-to-build-airbnbs-search-backends-ecc577b05f06)\n\t* [ELK (Elasticsearch, Logstash, Kibana) Stack](https://logz.io/blog/15-tech-companies-chose-elk-stack/)\n\t\t* [Predictions in Real Time with ELK at Uber](https://eng.uber.com/elk/)\n\t\t* [Building a scalable ELK stack at Envato](https://webuild.envato.com/blog/building-a-scalable-elk-stack/)\n\t\t* [ELK at Robinhood](https://robinhood.engineering/taming-elk-4e1349f077c3)\n\t\t* [Scaling Elasticsearch Clusters at Uber](https://www.infoq.com/presentations/uber-elasticsearch-clusters?utm_source=presentations_about_Case_Study&utm_medium=link&utm_campaign=Case_Study)\n\t\t* [Elasticsearch Performance Tuning Practice at eBay](https://www.ebayinc.com/stories/blogs/tech/elasticsearch-performance-tuning-practice-at-ebay/)\n\t\t* [Improve Performance using Elasticsearch Plugins (2 parts) at Tinder](https://medium.com/tinder-engineering/how-we-improved-our-performance-using-elasticsearch-plugins-part-2-b051da2ee85b)\n\t\t* [Elasticsearch at Kickstarter](https://kickstarter.engineering/elasticsearch-at-kickstarter-db3c487887fc)\n\t\t* [Log Parsing with Logstash and Google Protocol Buffers at Trivago](https://tech.trivago.com/2016/01/19/logstash_protobuf_codec/)\n\t\t* [Fast Order Search using Data Pipeline and Elasticsearch at Yelp](https://engineeringblog.yelp.com/2018/06/fast-order-search.html)\n\t\t* [Moving Core Business Search to Elasticsearch at Yelp](https://engineeringblog.yelp.com/2017/06/moving-yelps-core-business-search-to-elasticsearch.html)\n\t\t* [Sharding out Elasticsearch at Vinted](http://engineering.vinted.com/2017/06/05/sharding-out-elasticsearch/)\n\t\t* [Self-Ranking Search with Elasticsearch at Wattpad](http://engineering.wattpad.com/post/146216619727/self-ranking-search-with-elasticsearch-at-wattpad)\n\t\t* [Vulcanizer: a library for operating Elasticsearch at Github](https://github.blog/2019-03-05-vulcanizer-a-library-for-operating-elasticsearch/)\t\n* [Distributed Storage](http://highscalability.com/blog/2011/11/1/finding-the-right-data-solution-for-your-application-in-the.html)\n\t* [In-memory Storage](https://medium.com/@denisanikin/what-an-in-memory-database-is-and-how-it-persists-data-efficiently-f43868cff4c1)\n\t\t* [MemSQL Architecture - The Fast (MVCC, InMem, LockFree, CodeGen) And Familiar (SQL)](http://highscalability.com/blog/2012/8/14/memsql-architecture-the-fast-mvcc-inmem-lockfree-codegen-and.html)\n\t\t* [Optimizing Memcached Efficiency at Quora](https://quoraengineering.quora.com/Optimizing-Memcached-Efficiency)\n\t\t* [Real-Time Data Warehouse with MemSQL on Cisco UCS](https://blogs.cisco.com/datacenter/memsql)\n\t\t* [Moving to MemSQL at Tapjoy](http://eng.tapjoy.com/blog-list/moving-to-memsql)\n\t\t* [MemSQL and Kinesis for Real-time Insights at Disney](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/68131)\n\t\t* [MemSQL to Query Hundreds of Billions of Rows in a Dashboard at Pandora](https://engineering.pandora.com/using-memsql-at-pandora-79a86cb09b57)\n\t* [Object Storage](http://www.datacenterknowledge.com/archives/2013/10/04/object-storage-the-future-of-scale-out)\n\t\t* [Scaling HDFS at Uber](https://eng.uber.com/scaling-hdfs/)\n\t\t* [Reasons for Choosing S3 over HDFS at Databricks](https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html)\n\t\t* [File System on Amazon S3 at Quantcast](https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/)\n\t\t* [Image Recovery at Scale Using S3 Versioning at Trivago](https://tech.trivago.com/2018/09/03/efficient-image-recovery-at-scale-using-amazon-s3-versioning/)\n\t\t* [Cloud Object Store at Yahoo](https://yahooeng.tumblr.com/post/116391291701/yahoo-cloud-object-store-object-storage-at)\n\t\t* [Ambry: Distributed Immutable Object Store at LinkedIn](https://www.usenix.org/conference/srecon17americas/program/presentation/shenoy)\n\t\t* [Dynamometer: Scale Testing HDFS on Minimal Hardware with Maximum Fidelity at LinkedIn](https://engineering.linkedin.com/blog/2018/02/dynamometer--scale-testing-hdfs-on-minimal-hardware-with-maximum)\n\t\t* [Hammerspace: Persistent, Concurrent, Off-heap Storage at Airbnb](https://medium.com/airbnb-engineering/hammerspace-persistent-concurrent-off-heap-storage-3db39bb04472)\n\t\t* [MezzFS: Mounting Object Storage in Media Processing Platform at Netflix](https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba)\t\n\t\t* [Magic Pocket: In-house Multi-exabyte Storage System at Dropbox](https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/)\n* [Relational Databases](https://www.mysql.com/products/cluster/scalability.html)\n\t* [MySQL at Uber](https://www.uber.com/en-SG/blog/mysql-at-uber/)\n\t* [MySQL at Pinterest](https://medium.com/@Pinterest_Engineering/learn-to-stop-using-shiny-new-things-and-love-mysql-3e1613c2ce14)\n\t* [PostgreSQL at Twitch](https://blog.twitch.tv/en/2016/10/11/how-twitch-uses-postgresql-c34aa9e56f58)\n\t* [Scaling MySQL-based Financial Reporting System at Airbnb](https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040)\n\t* [Scaling MySQL at Wix](https://www.wix.engineering/post/scaling-to-100m-mysql-is-a-better-nosql)\n\t* [Building and Deploying MySQL Raft at Meta](https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/)\n\t* [MaxScale (MySQL) Database Proxy at Airbnb](https://medium.com/airbnb-engineering/unlocking-horizontal-scalability-in-our-web-serving-tier-d907449cdbcf)\n\t* [Switching from Postgres to MySQL at Uber](https://www.uber.com/en-NL/blog/postgres-to-mysql-migration/)\n\t* [Handling Growth with Postgres at Instagram](https://engineering.instagram.com/handling-growth-with-postgres-5-tips-from-instagram-d5d7e7ffdfcb)\n\t* [Scaling the Analytics Database (Postgres) at TransferWise](http://tech.transferwise.com/scaling-our-analytics-database/)\n\t* [Updating a 50 Terabyte PostgreSQL Database at Adyen](https://medium.com/adyen/updating-a-50-terabyte-postgresql-database-f64384b799e7)\n\t* [Scaling Database Access for 100s of Billions of Queries per Day at PayPal](https://medium.com/paypal-engineering/scaling-database-access-for-100s-of-billions-of-queries-per-day-paypal-introducing-hera-e192adacda54)\n\t* [Minimizing Read-Write MySQL Downtime at Yelp](https://engineeringblog.yelp.com/2020/11/minimizing-read-write-mysql-downtime.html)\n\t* [Migrating MySQL from 5.6 to 8.0 at Facebook](https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/)\n\t* [Migration from HBase to MyRocks at Quora](https://quoraengineering.quora.com/Migration-from-HBase-to-MyRocks-at-Quora)\n\t* [Replication](https://docs.microsoft.com/en-us/sql/relational-databases/replication/types-of-replication)\n\t\t* [MySQL Parallel Replication (4 parts) at Booking.com](https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-4-annex-under-the-hood-eb456cf8b2fb)\n\t\t* [Mitigating MySQL Replication Lag and Reducing Read Load at Github](https://githubengineering.com/mitigating-replication-lag-and-reducing-read-load-with-freno/)\n\t\t* [Read Consistency with Database Replicas at Shopify](https://shopify.engineering/read-consistency-database-replicas)\n\t\t* [Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift at Yelp](https://engineeringblog.yelp.com/2018/04/black-box-auditing.html)\n\t\t* [Partitioning Main MySQL Database at Airbnb](https://medium.com/airbnb-engineering/how-we-partitioned-airbnb-s-main-database-in-two-weeks-55f7e006ff21)\n\t\t* [Herb: Multi-DC Replication Engine for Schemaless Datastore at Uber](https://eng.uber.com/herb-datacenter-replication/)\n\t* [Sharding](https://quabase.sei.cmu.edu/mediawiki/index.php/Shard_data_set_across_multiple_servers_(Range-based))\n\t\t* [Sharding MySQL at Pinterest](https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f)\n\t\t* [Sharding MySQL at Twilio](https://www.twilio.com/engineering/2014/06/26/how-we-replaced-our-data-pipeline-with-zero-downtime)\n\t\t* [Sharding MySQL at Square](https://medium.com/square-corner-blog/sharding-cash-10280fa3ef3b)\n\t\t* [Sharding MySQL at Quora](https://www.quora.com/q/quoraengineering/MySQL-sharding-at-Quora)\n\t\t* [Sharding Layer of Schemaless Datastore at Uber](https://eng.uber.com/schemaless-rewrite/)\n\t\t* [Sharding & IDs at Instagram](https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c)\n\t\t* [Sharding Postgres at Notion](https://www.notion.so/blog/sharding-postgres-at-notion)\n\t\t* [Sharding Postgres at Figma](https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/)\n\t\t* [Solr: Improving Performance for Batch Indexing at Box](https://blog.box.com/blog/solr-improving-performance-batch-indexing/)\t\n\t\t* [Geosharded Recommendations (3 parts) at Tinder](https://medium.com/tinder-engineering/geosharded-recommendations-part-3-consistency-2d2cb2f0594b)\n\t\t* [Scaling Services with Shard Manager at Facebook](https://engineering.fb.com/production-engineering/scaling-services-with-shard-manager/)\n\t* [Presto the Distributed SQL Query Engine](https://research.fb.com/wp-content/uploads/2019/03/Presto-SQL-on-Everything.pdf?)\n\t\t* [Presto at Pinterest](https://medium.com/@Pinterest_Engineering/presto-at-pinterest-a8bda7515e52)\n\t\t* [Presto Infrastructure at Lyft](https://eng.lyft.com/presto-infrastructure-at-lyft-b10adb9db01)\n\t\t* [Presto at Grab](https://engineering.grab.com/scaling-like-a-boss-with-presto)\n\t\t* [Engineering Data Analytics with Presto and Apache Parquet at Uber](https://eng.uber.com/presto/)\n\t\t* [Data Wrangling at Slack](https://slack.engineering/data-wrangling-at-slack-f2e0ff633b69)\n\t\t* [Presto in Big Data Platform on AWS at Netflix](https://medium.com/netflix-techblog/using-presto-in-our-big-data-platform-on-aws-938035909fd4)\n\t\t* [Presto Auto Scaling at Eventbrite](https://www.eventbrite.com/engineering/big-data-workloads-presto-auto-scaling/)\n\t\t* [Speed Up Presto with Alluxio Local Cache at Uber](https://www.uber.com/en-MY/blog/speed-up-presto-with-alluxio-local-cache/)\n* [NoSQL Databases](https://www.thoughtworks.com/insights/blog/nosql-databases-overview)\n\t* [Key-Value Databases](http://www.cs.ucsb.edu/~agrawal/fall2009/dynamo.pdf)\n\t\t* [DynamoDB at Nike](https://medium.com/nikeengineering/becoming-a-nimble-giant-how-dynamo-db-serves-nike-at-scale-4cc375dbb18e)\n\t\t* [DynamoDB at Segment](https://segment.com/blog/the-million-dollar-eng-problem/)\n\t\t* [DynamoDB at Mapbox](https://blog.mapbox.com/scaling-mapbox-infrastructure-with-dynamodb-streams-d53eabc5e972)\n\t\t* [Manhattan: Distributed Key-Value Database at Twitter](https://blog.twitter.com/engineering/en_us/a/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale.html)\n\t\t* [Sherpa: Distributed NoSQL Key-Value Store at Yahoo](https://yahooeng.tumblr.com/post/120730204806/sherpa-scales-new-heights)\n\t\t* [HaloDB: Embedded Key-Value Storage Engine at Yahoo](https://yahooeng.tumblr.com/post/178262468576/introducing-halodb-a-fast-embedded-key-value)\n\t\t* [MPH: Fast and Compact Immutable Key-Value Stores at Indeed](http://engineering.indeedblog.com/blog/2018/02/indeed-mph/)\n\t\t* [Venice: Distributed Key-Value Database at Linkedin](https://engineering.linkedin.com/blog/2017/02/building-venice-with-apache-helix)\n\t* [Columnar Databases](https://aws.amazon.com/nosql/columnar/)\n\t\t* [Cassandra](http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf)\n\t\t\t* [Cassandra at Instagram](https://www.slideshare.net/DataStax/cassandra-at-instagram-2016)\n\t\t\t* [Storing Images in Cassandra at Walmart](https://medium.com/walmartlabs/building-object-store-storing-images-in-cassandra-walmart-scale-a6b9c02af593)\n\t\t\t* [Storing Messages with Cassandra at Discord](https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7)\n\t\t\t* [Scaling Cassandra Cluster at Walmart](https://medium.com/walmartlabs/avoid-pitfalls-in-scaling-your-cassandra-cluster-lessons-and-remedies-a71ca01f8c04)\n\t\t\t* [Scaling Ad Analytics with Cassandra at Yelp](https://engineeringblog.yelp.com/2016/08/how-we-scaled-our-ad-analytics-with-cassandra.html)\n\t\t\t* [Scaling to 100+ Million Reads/Writes using Spark and Cassandra at Dream11](https://medium.com/dream11-tech-blog/leaderboard-dream11-4efc6f93c23e)\t\t\n\t\t\t* [Moving Food Feed from Redis to Cassandra at Zomato](https://www.zomato.com/blog/how-we-moved-our-food-feed-from-redis-to-cassandra)\n\t\t\t* [Benchmarking Cassandra Scalability on AWS at Netflix](https://medium.com/netflix-techblog/benchmarking-cassandra-scalability-on-aws-over-a-million-writes-per-second-39f45f066c9e)\n\t\t\t* [Service Decomposition at Scale with Cassandra at Intuit QuickBooks](https://quickbooks-engineering.intuit.com/service-decomposition-at-scale-70405ac2f637)\n\t\t\t* [Cassandra for Keeping Counts In Sync at SoundCloud](https://developers.soundcloud.com/blog/keeping-counts-in-sync)\n\t\t\t* [Cassandra Driver Configuration for Improved Performance and Load Balancing at Glassdoor](https://medium.com/glassdoor-engineering/cassandra-driver-configuration-for-improved-performance-and-load-balancing-1b0106ce12bb)\n\t\t\t* [cstar: Cassandra Orchestration Tool at Spotify](https://labs.spotify.com/2018/09/04/introducing-cstar-the-spotify-cassandra-orchestration-tool-now-open-source/)\n\t\t* [HBase](https://hbase.apache.org/)\n\t\t\t* [HBase at Salesforce](https://engineering.salesforce.com/investing-in-big-data-apache-hbase-b9d98661a66b)\n\t\t\t* [HBase in Facebook Messages](https://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919/)\n\t\t\t* [HBase in Imgur Notification](https://blog.imgur.com/2015/09/15/tech-tuesday-imgur-notifications-from-mysql-to-hbase/)\n\t\t\t* [Improving HBase Backup Efficiency at Pinterest](https://medium.com/@Pinterest_Engineering/improving-hbase-backup-efficiency-at-pinterest-86159da4b954)\n\t\t\t* [HBase at Xiaomi](https://www.slideshare.net/HBaseCon/hbase-practice-at-xiaomi)\n\t\t* [Redshift](https://www.allthingsdistributed.com/2018/11/amazon-redshift-performance-optimization.html)\n\t\t\t* [Redshift at GIPHY](https://engineering.giphy.com/scaling-redshift-without-scaling-costs/)\n\t\t\t* [Redshift at Hudl](https://www.hudl.com/bits/the-low-hanging-fruit-of-redshift-performance)\n\t\t\t* [Redshift at Drivy](https://drivy.engineering/redshift_tips_ticks_part_1/)\n\t* [Document Databases](https://msdn.microsoft.com/en-us/magazine/hh547103.aspx)\n\t\t* [eBay: Building Mission-Critical Multi-Data Center Applications with MongoDB](https://www.mongodb.com/blog/post/ebay-building-mission-critical-multi-data-center-applications-with-mongodb)\n\t\t* [MongoDB at Baidu: Multi-Tenant Cluster Storing 200+ Billion Documents across 160 Shards](https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale)\n\t\t* [Migrating Mongo Data at Addepar](https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952)\n\t\t* [The AWS and MongoDB Infrastructure of Parse (acquired by Facebook)](https://medium.baqend.com/parse-is-gone-a-few-secrets-about-their-infrastructure-91b3ab2fcf71)\n\t\t* [Migrating Mountains of Mongo Data at Addepar](https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952)\n\t\t* [Couchbase Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2017/12/couchbase-ecosystem-at-linkedin)\n\t\t* [SimpleDB at Zendesk](https://medium.com/zendesk-engineering/resurrecting-amazon-simpledb-9404034ec506)\n\t\t* [Espresso: Distributed Document Store at LinkedIn](https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store)\n\t* [Graph Databases](https://www.eecs.harvard.edu/margo/papers/systor13-bench/)\n\t\t* [FlockDB: Distributed Graph Database at Twitter](https://blog.twitter.com/engineering/en_us/a/2010/introducing-flockdb.html)\n\t\t* [TAO: Distributed Data Store for the Social Graph at Facebook](https://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/11730-atc13-bronson.pdf)\n\t\t* [Akutan: Distributed Knowledge Graph Store at eBay](https://tech.ebayinc.com/engineering/akutan-a-distributed-knowledge-graph-store/)\n* [Time Series Databases](https://www.influxdata.com/time-series-database/)\n\t* [Beringei: High-performance Time Series Storage Engine at Facebook](https://code.facebook.com/posts/952820474848503/beringei-a-high-performance-time-series-storage-engine/)\n\t* [MetricsDB: TimeSeries Database for storing metrics at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/metricsdb.html)\t\n\t* [Atlas: In-memory Dimensional Time Series Database at Netflix](https://medium.com/netflix-techblog/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a)\n\t* [Heroic: Time Series Database at Spotify](https://labs.spotify.com/2015/11/17/monitoring-at-spotify-introducing-heroic/)\n\t* [Roshi: Distributed Storage System for Time-Series Event at SoundCloud](https://developers.soundcloud.com/blog/roshi-a-crdt-system-for-timestamped-events)\n\t* [Goku: Time Series Database at Pinterest](https://medium.com/@Pinterest_Engineering/goku-building-a-scalable-and-high-performant-time-series-database-system-a8ff5758a181)\n\t* [Scaling Time Series Data Storage (2 parts) at Netflix](https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-ii-d67939655586)\n\t* [Time Series Data Abstraction Layer at Netflix](https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8)\n\t* [Druid - Real-time Analytics Database](https://druid.apache.org/)\n\t\t* [Druid at Airbnb](https://medium.com/airbnb-engineering/druid-airbnb-data-platform-601c312f2a4c)\n\t\t* [Druid at Walmart](https://medium.com/walmartlabs/event-stream-analytics-at-walmart-with-druid-dcf1a37ceda7)\n\t\t* [Druid at eBay](https://tech.ebayinc.com/engineering/monitoring-at-ebay-with-druid/)\n\t\t* [Druid at Netflix](https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06)\n* [Distributed Repositories, Dependencies, and Configurations Management](https://betterexplained.com/articles/intro-to-distributed-version-control-illustrated/)\n\t* [DGit: Distributed Git at Github](https://githubengineering.com/introducing-dgit/)\n\t* [Stemma: Distributed Git Server at Palantir](https://medium.com/@palantir/stemma-distributed-git-server-70afbca0fc29)\n\t* [Configuration Management for Distributed Systems at Flickr](https://code.flickr.net/2016/03/24/configuration-management-for-distributed-systems-using-github-and-cfg4j/)\n\t* [Git Repository at Microsoft](https://blogs.msdn.microsoft.com/bharry/2017/05/24/the-largest-git-repo-on-the-planet/)\n\t* [Solve Git Problem with Large Repositories at Microsoft](https://www.infoq.com/news/2017/02/GVFS)\t\n\t* [Single Repository at Google](https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext)\t\n\t* [Scaling Infrastructure and (Git) Workflow at Adyen](https://medium.com/adyen/from-0-100-billion-scaling-infrastructure-and-workflow-at-adyen-7b63b690dfb6)\t\n\t* [Dotfiles Distribution at Booking.com](https://medium.com/booking-com-infrastructure/dotfiles-distribution-dedb69c66a75)\n\t* [Secret Detector: Preventing Secrets in Source Code at Yelp](https://engineeringblog.yelp.com/2018/06/yelps-secret-detector.html)\n\t* [Managing Software Dependency at Scale at LinkedIn](https://engineering.linkedin.com/blog/2018/09/managing-software-dependency-at-scale)\n\t* [Merging Code in High-velocity Repositories at LinkedIn](https://engineering.linkedin.com/blog/2020/continuous-integration)\n\t* [Dynamic Configuration at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/dynamic-configuration-at-twitter.html)\n\t* [Dynamic Configuration at Mixpanel](https://medium.com/mixpaneleng/dynamic-configuration-at-mixpanel-94bfcf97d6b8)\n\t* [Dynamic Configuration at GoDaddy](https://sg.godaddy.com/engineering/2019/03/06/dynamic-configuration-for-nodejs/)\n\t* [Fleet Management (3 parts) at Spotify](https://engineering.atspotify.com/2023/5/fleet-management-at-spotify-part-3-fleet-wide-refactoring)\n* [Scaling Continuous Integration and Continuous Delivery](https://www.synopsys.com/blogs/software-security/agile-cicd-devops-glossary/)\n\t* [Continuous Integration Stack at Facebook](https://code.fb.com/web/rapid-release-at-massive-scale/)\n\t* [Continuous Integration with Distributed Repositories and Dependencies at Netflix](https://medium.com/netflix-techblog/towards-true-continuous-integration-distributed-repositories-and-dependencies-2a2e3108c051)\n\t* [Continuous Integration and Deployment with Bazel at Dropbox](https://blogs.dropbox.com/tech/2019/12/continuous-integration-and-deployment-with-bazel/)\n\t* [Adopting Bazel for Web at Airbnb](https://medium.com/airbnb-engineering/adopting-bazel-for-web-at-scale-a784b2dbe325)\n\t* [Continuous Deployments at BuzzFeed](https://tech.buzzfeed.com/continuous-deployments-at-buzzfeed-d171f76c1ac4)\n\t* [Screwdriver: Continuous Delivery Build System for Dynamic Infrastructure at Yahoo](https://yahooeng.tumblr.com/post/155765242061/open-sourcing-screwdriver-yahoos-continuous)\n\t* [CI/CD at Betterment](https://www.betterment.com/resources/ci-cd-shortening-the-feedback-loop/)\n\t* [CI/CD at Brainly](https://medium.com/engineering-brainly/ci-cd-at-scale-fdfb0f49e031)\n\t* [Scaling iOS CI with Anka at Shopify](https://engineering.shopify.com/blogs/engineering/scaling-ios-ci-with-anka)\n\t* [Scaling Jira Server at Yelp](https://engineeringblog.yelp.com/2019/04/Scaling-Jira-Server-Administration-For-The-Enterprise.html)\n\t* [Auto-scaling CI/CD cluster at Flexport](https://flexport.engineering/how-flexport-halved-testing-costs-with-an-auto-scaling-ci-cd-cluster-8304297222f)\n\n## Availability\n* [Resilience Engineering: Learning to Embrace Failure](https://queue.acm.org/detail.cfm?id=2371297)\t\n\t* [Resilience Engineering with Project Waterbear at LinkedIn](https://engineering.linkedin.com/blog/2017/11/resilience-engineering-at-linkedin-with-project-waterbear)\n\t* [Resiliency against Traffic Oversaturation at iHeartRadio](https://tech.iheart.com/resiliency-against-traffic-oversaturation-77c5ed92a5fb)\n\t* [Resiliency in Distributed Systems at GO-JEK](https://blog.gojekengineering.com/resiliency-in-distributed-systems-efd30f74baf4)\n\t* [Practical NoSQL Resilience Design Pattern for the Enterprise at eBay](https://www.ebayinc.com/stories/blogs/tech/practical-nosql-resilience-design-pattern-for-the-enterprise/)\n\t* [Ensuring Resilience to Disaster at Quora](https://quoraengineering.quora.com/Ensuring-Quoras-Resilience-to-Disaster)\n\t* [Site Resiliency at Expedia](https://www.infoq.com/presentations/expedia-website-resiliency?utm_source=presentations_about_Case_Study&utm_medium=link&utm_campaign=Case_Study)\n\t* [Resiliency and Disaster Recovery with Kafka at eBay](https://tech.ebayinc.com/engineering/resiliency-and-disaster-recovery-with-kafka/)\n\t* [Disaster Recovery for Multi-Region Kafka at Uber](https://eng.uber.com/kafka/)\n* [Failover](http://cloudpatterns.org/mechanisms/failover_system)\n\t* [The Evolution of Global Traffic Routing and Failover](https://www.usenix.org/conference/srecon16/program/presentation/heady)\n\t* [Testing for Disaster Recovery Failover Testing](https://www.usenix.org/conference/srecon17asia/program/presentation/liu_zehua)\n\t* [Designing a Microservices Architecture for Failure](https://blog.risingstack.com/designing-microservices-architecture-for-failure/)\n\t* [ELB for Automatic Failover at GoSquared](https://engineering.gosquared.com/use-elb-automatic-failover)\n\t* [Eliminate the Database for Higher Availability at American Express](http://americanexpress.io/eliminate-the-database-for-higher-availability/)\n\t* [Failover with Redis Sentinel at Vinted](http://engineering.vinted.com/2015/09/03/failover-with-redis-sentinel/)\n\t* [High-availability SaaS Infrastructure at FreeAgent](http://engineering.freeagent.com/2017/02/06/ha-infrastructure-without-breaking-the-bank/)\n\t* [MySQL High Availability at GitHub](https://github.blog/2018-06-20-mysql-high-availability-at-github/)\n\t* [MySQL High Availability at Eventbrite](https://www.eventbrite.com/engineering/mysql-high-availability-at-eventbrite/)\n\t* [Business Continuity & Disaster Recovery at Walmart](https://medium.com/walmartlabs/business-continuity-disaster-recovery-in-the-microservices-world-ef2adca363df)\n* [Load Balancing](https://blog.vivekpanyam.com/scaling-a-web-service-load-balancing/)\n\t* [Introduction to Modern Network Load Balancing and Proxying](https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236)\n\t* [Top Five (Load Balancing) Scalability Patterns](https://www.f5.com/company/blog/top-five-scalability-patterns)\n\t* [Load Balancing infrastructure to support more than 1.3 billion users at Facebook](https://www.usenix.org/conference/srecon15europe/program/presentation/shuff)\n\t* [DHCPLB: DHCP Load Balancer at Facebook](https://code.facebook.com/posts/1734309626831603/dhcplb-an-open-source-load-balancer/)\n\t* [Katran: Scalable Network Load Balancer at Facebook](https://code.facebook.com/posts/1906146702752923/open-sourcing-katran-a-scalable-network-load-balancer/)\n\t* [Deterministic Aperture: A Distributed, Load Balancing Algorithm at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/daperture-load-balancer.html)\t\n\t* [Load Balancing with Eureka at Netflix](https://medium.com/netflix-techblog/netflix-shares-cloud-load-balancing-and-failover-tool-eureka-c10647ef95e5)\n\t* [Edge Load Balancing at Netflix](https://medium.com/netflix-techblog/netflix-edge-load-balancing-695308b5548c)\n\t* [Zuul 2: Cloud Gateway at Netflix](https://medium.com/netflix-techblog/open-sourcing-zuul-2-82ea476cb2b3)\n\t* [Load Balancing at Yelp](https://engineeringblog.yelp.com/2017/05/taking-zero-downtime-load-balancing-even-further.html)\n\t* [Load Balancing at Github](https://githubengineering.com/introducing-glb/)\n\t* [Consistent Hashing to Improve Load Balancing at Vimeo](https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed)\n\t* [UDP Load Balancing at 500 pixel](https://developers.500px.com/udp-load-balancing-with-keepalived-167382d7ad08)\n\t* [QALM: QoS Load Management Framework at Uber](https://eng.uber.com/qalm/)\t\n\t* [Traffic Steering using Rum DNS at LinkedIn](https://www.usenix.org/conference/srecon17europe/program/presentation/rastogi)\n\t* [Traffic Infrastructure (Edge Network) at Dropbox](https://blogs.dropbox.com/tech/2018/10/dropbox-traffic-infrastructure-edge-network/)\n\t* [Intelligent DNS based load balancing at Dropbox](https://blogs.dropbox.com/tech/2020/01/intelligent-dns-based-load-balancing-at-dropbox/)\n\t* [Monitor DNS systems at Stripe](https://stripe.com/en-sg/blog/secret-life-of-dns)\n\t* [Multi-DNS Architecture (3 parts) at Monday](https://medium.com/monday-engineering/how-and-why-we-migrated-our-dns-from-cloudflare-to-a-multi-dns-architecture-part-3-584a470f4062)\n\t* [Dynamic Anycast DNS Infrastructure at Hulu](https://medium.com/hulu-tech-blog/building-hulus-dynamic-anycast-dns-infrastructure-985a7a11fd30)\n* [Rate Limiting](https://www.keycdn.com/support/rate-limiting/)\n\t* [Rate Limiting for Scaling to Millions of Domains at Cloudflare](https://blog.cloudflare.com/counting-things-a-lot-of-different-things/)\n\t* [Cloud Bouncer: Distributed Rate Limiting at Yahoo](https://yahooeng.tumblr.com/post/111288877956/cloud-bouncer-distributed-rate-limiting-at-yahoo)\n\t* [Scaling API with Rate Limiters at Stripe](https://stripe.com/blog/rate-limiters)\n\t* [Distributed Rate Limiting at Allegro](https://allegro.tech/2017/04/hermes-max-rate.html)\n\t* [Ratequeue: Core Queueing-And-Rate-Limiting System at Twilio](https://www.twilio.com/blog/2017/11/chaos-engineering-ratequeue-ha.html)\n\t* [Quotas Service at Grab](https://engineering.grab.com/quotas-service)\n\t* [Rate Limiting at Figma](https://medium.com/figma-design/an-alternative-approach-to-rate-limiting-f8a06cf7c94c)\t\n* [Autoscaling](https://medium.com/@BotmetricHQ/top-11-hard-won-lessons-learned-about-aws-auto-scaling-5bfe56da755f)\n\t* [Autoscaling Pinterest](https://medium.com/@Pinterest_Engineering/auto-scaling-pinterest-df1d2beb4d64)\n\t* [Autoscaling Based on Request Queuing at Square](https://medium.com/square-corner-blog/autoscaling-based-on-request-queuing-c4c0f57f860f)\n\t* [Autoscaling Jenkins at Trivago](http://tech.trivago.com/2017/02/17/your-definite-guide-for-autoscaling-jenkins/)\n\t* [Autoscaling Pub-Sub Consumers at Spotify](https://labs.spotify.com/2017/11/20/autoscaling-pub-sub-consumers/)\n\t* [Autoscaling Bigtable Clusters based on CPU Load at Spotify](https://labs.spotify.com/2018/12/18/bigtable-autoscaler-saving-money-and-time-using-managed-storage/)\n\t* [Autoscaling AWS Step Functions Activities at Yelp](https://engineeringblog.yelp.com/2019/06/autoscaling-aws-step-functions-activities.html)\n\t* [Scryer: Predictive Auto Scaling Engine at Netflix](https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-a3f8fc922270)\t\n\t* [Bouncer: Simple AWS Auto Scaling Rollovers at Palantir](https://medium.com/palantir/bouncer-simple-aws-auto-scaling-rollovers-c5af601d65d4)\n\t* [Clusterman: Autoscaling Mesos Clusters at Yelp](https://engineeringblog.yelp.com/2019/02/autoscaling-mesos-clusters-with-clusterman.html)\n* [Availability in Globally Distributed Storage Systems at Google](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36737.pdf)\t\n* [NodeJS High Availability at Yahoo](https://yahooeng.tumblr.com/post/68823943185/nodejs-high-availability)\n* [Operations (11 parts) at LinkedIn](https://www.linkedin.com/pulse/introduction-every-day-monday-operations-benjamin-purgason)\n* [Monitoring Powers High Availability for LinkedIn Feed](https://www.usenix.org/conference/srecon17americas/program/presentation/barot)\n* [Supporting Global Events at Facebook](https://code.facebook.com/posts/166966743929963/how-production-engineers-support-global-events-on-facebook/)\n* [High Availability at BlaBlaCar](https://medium.com/blablacar-tech/the-expendables-backends-high-availability-at-blablacar-8cea3b95b26b)\n* [High Availability at Netflix](https://medium.com/@NetflixTechBlog/tips-for-high-availability-be0472f2599c)\n* [High Availability Cloud Infrastructure at Twilio](https://www.twilio.com/engineering/2011/12/12/scaling-high-availablity-infrastructure-in-cloud)\n* [High Availability with Distributed DB on Kubernetes at Airbnb](https://airbnb.tech/infrastructure/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb/)\n* [Automating Datacenter Operations at Dropbox](https://blogs.dropbox.com/tech/2019/01/automating-datacenter-operations-at-dropbox/)\n* [Globalizing Player Accounts at Riot Games](https://technology.riotgames.com/news/globalizing-player-accounts)\n\n## Stability\n* [Circuit Breaker](https://martinfowler.com/bliki/CircuitBreaker.html)\n\t* [Circuit Breaking in Distributed Systems](https://www.infoq.com/presentations/circuit-breaking-distributed-systems)\n\t* [Circuit Breaker for Scaling Containers](https://f5.com/about-us/blog/articles/the-art-of-scaling-containers-circuit-breakers-28919)\n\t* [Lessons in Resilience at SoundCloud](https://developers.soundcloud.com/blog/lessons-in-resilience-at-SoundCloud)\n\t* [Protector: Circuit Breaker for Time Series Databases at Trivago](http://tech.trivago.com/2016/02/23/protector/)\n\t* [Improved Production Stability with Circuit Breakers at Heroku](https://blog.heroku.com/improved-production-stability-with-circuit-breakers)\n\t* [Circuit Breaker at Zendesk](https://medium.com/zendesk-engineering/the-joys-of-circuit-breaking-ee6584acd687)\n\t* [Circuit Breaker at Traveloka](https://medium.com/traveloka-engineering/circuit-breakers-dont-let-your-dependencies-bring-you-down-5ba1c5cf1eec)\n\t* [Circuit Breaker at Shopify](https://shopify.engineering/circuit-breaker-misconfigured)\n* [Timeouts](https://www.javaworld.com/article/2824163/application-performance/stability-patterns-applied-in-a-restful-architecture.html)\n\t* [Fault Tolerance (Timeouts and Retries, Thread Separation, Semaphores, Circuit Breakers) at Netflix](https://medium.com/netflix-techblog/fault-tolerance-in-a-high-volume-distributed-system-91ab4faae74a)\n\t* [Enforce Timeout: A Reliability Methodology at DoorDash](https://doordash.engineering/2018/12/21/enforce-timeout-a-doordash-reliability-methodology/)\n\t* [Troubleshooting a Connection Timeout Issue with tcp_tw_recycle Enabled at eBay](https://www.ebayinc.com/stories/blogs/tech/a-vip-connection-timeout-issue-caused-by-snat-and-tcp-tw-recycle/)\n* [Crash-safe Replication for MySQL at Booking.com](https://medium.com/booking-com-infrastructure/better-crash-safe-replication-for-mysql-a336a69b317f)\n* [Bulkheads: Partition and Tolerate Failure in One Part](https://skife.org/architecture/fault-tolerance/2009/12/31/bulkheads.html)\n* [Steady State: Always Put Logs on Separate Disk](https://docs.microsoft.com/en-us/sql/relational-databases/policy-based-management/place-data-and-log-files-on-separate-drives)\n* [Throttling: Maintain a Steady Pace](http://www.sosp.org/2001/papers/welsh.pdf)\n* [Multi-Clustering: Improving Resiliency and Stability of a Large-scale Monolithic API Service at LinkedIn](https://engineering.linkedin.com/blog/2017/11/improving-resiliency-and-stability-of-a-large-scale-api)\n* [Determinism (4 parts) in League of Legends Server](https://engineering.riotgames.com/news/determinism-league-legends-fixing-divergences)\n\n## Performance\n* [Performance Optimization on OS, Storage, Database, Network](https://stackify.com/application-performance-metrics/)\n\t* [Improving Performance with Background Data Prefetching at Instagram](https://engineering.instagram.com/improving-performance-with-background-data-prefetching-b191acb39898)\n\t* [Fixing Linux filesystem performance regressions at LinkedIn](https://engineering.linkedin.com/blog/2020/fixing-linux-filesystem-performance-regressions)\n\t* [Compression Techniques to Solve Network I/O Bottlenecks at eBay](https://www.ebayinc.com/stories/blogs/tech/how-ebays-shopping-cart-used-compression-techniques-to-solve-network-io-bottlenecks/)\n\t* [Optimizing Web Servers for High Throughput and Low Latency at Dropbox](https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/)\n\t* [Linux Performance Analysis in 60.000 Milliseconds at Netflix](https://medium.com/netflix-techblog/linux-performance-analysis-in-60-000-milliseconds-accc10403c55)\n\t* [Live Downsizing Google Cloud Persistent Disks (PD-SSD) at Mixpanel](https://engineering.mixpanel.com/2018/07/31/live-downsizing-google-cloud-pds-for-fun-and-profit/)\n\t* [Decreasing RAM Usage by 40% Using jemalloc with Python & Celery at Zapier](https://zapier.com/engineering/celery-python-jemalloc/)\n\t* [Reducing Memory Footprint at Slack](https://slack.engineering/reducing-slacks-memory-footprint-4480fec7e8eb)\n\t* [Continuous Load Testing at Slack](https://slack.engineering/continuous-load-testing/)\n\t* [Performance Improvements at Pinterest](https://medium.com/@Pinterest_Engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7)\n\t* [Server Side Rendering at Wix](https://www.youtube.com/watch?v=f9xI2jR71Ms)\n\t* [30x Performance Improvements on MySQLStreamer at Yelp](https://engineeringblog.yelp.com/2018/02/making-30x-performance-improvements-on-yelps-mysqlstreamer.html)\n\t* [Optimizing APIs at Netflix](https://medium.com/netflix-techblog/optimizing-the-netflix-api-5c9ac715cf19)\n\t* [Performance Monitoring with Riemann and Clojure at Walmart](https://medium.com/walmartlabs/performance-monitoring-with-riemann-and-clojure-eafc07fcd375)\n\t* [Performance Tracking Dashboard for Live Games at Zynga](https://www.zynga.com/blogs/engineering/live-games-have-evolving-performance)\n\t* [Optimizing CAL Report Hadoop MapReduce Jobs at eBay](https://www.ebayinc.com/stories/blogs/tech/optimization-of-cal-report-hadoop-mapreduce-job/)\n\t* [Performance Tuning on Quartz Scheduler at eBay](https://www.ebayinc.com/stories/blogs/tech/performance-tuning-on-quartz-scheduler/)\n\t* [Profiling C++ (Part 1: Optimization, Part 2: Measurement and Analysis) at Riot Games](https://engineering.riotgames.com/news/profiling-optimisation)\n\t* [Profiling React Server-Side Rendering at HomeAway](https://medium.com/homeaway-tech-blog/profiling-react-server-side-rendering-to-free-the-node-js-event-loop-7f0fe455a901)\n\t* [Hardware-Assisted Video Transcoding at Dailymotion](https://medium.com/dailymotion-engineering/hardware-assisted-video-transcoding-at-dailymotion-66cd2db448ae)\n\t* [Cross Shard Transactions at 10 Million RPS at Dropbox](https://blogs.dropbox.com/tech/2018/11/cross-shard-transactions-at-10-million-requests-per-second/)\n\t* [API Profiling at Pinterest](https://medium.com/@Pinterest_Engineering/api-profiling-at-pinterest-6fa9333b4961)\n\t* [Pagelets Parallelize Server-side Processing at Yelp](https://engineeringblog.yelp.com/2017/07/generating-web-pages-in-parallel-with-pagelets.html)\n\t* [Improving key expiration in Redis at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/improving-key-expiration-in-redis.html)\n\t* [Ad Delivery Network Performance Optimization with Flame Graphs at MindGeek](https://medium.com/mindgeek-engineering-blog/ad-delivery-network-performance-optimization-with-flame-graphs-bc550cf59cf7)\n\t* [Predictive CPU isolation of containers at Netflix](https://medium.com/netflix-techblog/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7)\n\t* [Improving HDFS I/O Utilization for Efficiency at Uber](https://eng.uber.com/improving-hdfs-i-o-utilization-for-efficiency/)\n\t* [Cloud Jewels: Estimating kWh in the Cloud at Etsy](https://codeascraft.com/2020/04/23/cloud-jewels-estimating-kwh-in-the-cloud/)\n\t* [Unthrottled: Fixing CPU Limits in the Cloud (2 parts) at Indeed](https://engineering.indeedblog.com/blog/2019/12/unthrottled-fixing-cpu-limits-in-the-cloud/)\n* [Performance Optimization by Tuning Garbage Collection](https://confluence.atlassian.com/enterprise/garbage-collection-gc-tuning-guide-461504616.html)\n\t* [Garbage Collection in Java Applications at LinkedIn](https://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications)\n\t* [Garbage Collection in High-Throughput, Low-Latency Machine Learning Services at Adobe](https://medium.com/adobetech/engineering-high-throughput-low-latency-machine-learning-services-7d45edac0271)\n\t* [Garbage Collection in Redux Applications at SoundCloud](https://developers.soundcloud.com/blog/garbage-collection-in-redux-applications)\n\t* [Garbage Collection in Go Application at Twitch](https://blog.twitch.tv/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap-26c2462549a2)\n\t* [Analyzing V8 Garbage Collection Logs at Alibaba](https://www.linux.com/blog/can-nodejs-scale-ask-team-alibaba)\n\t* [Python Garbage Collection for Dropping 50% Memory Growth Per Request at Instagram](https://instagram-engineering.com/copy-on-write-friendly-python-garbage-collection-ad6ed5233ddf)\n\t* [Performance Impact of Removing Out of Band Garbage Collector (OOBGC) at Github](https://githubengineering.com/removing-oobgc/)\n\t* [Debugging Java Memory Leaks at Allegro](https://allegro.tech/2018/05/a-comedy-of-errors-debugging-java-memory-leaks.html)\n\t* [Optimizing JVM at Alibaba](https://www.youtube.com/watch?v=X4tmr3nhZRg)\n\t* [Tuning JVM Memory for Large-scale Services at Uber](https://eng.uber.com/jvm-tuning-garbage-collection/)\n\t* [Solr Performance Tuning at Walmart](https://medium.com/walmartglobaltech/solr-performance-tuning-beb7d0d0f8d9)\n\t* [Memory Tuning a High Throughput Microservice at Flipkart](https://blog.flipkart.tech/memory-tuning-a-high-throughput-microservice-ed57b3e60997)\n* [Performance Optimization on Image, Video, Page Load](https://developers.google.com/web/fundamentals/performance/why-performance-matters/)\n\t* [Optimizing 360 Photos at Scale at Facebook](https://code.facebook.com/posts/129055711052260/optimizing-360-photos-at-scale/)\n\t* [Reducing Image File Size in the Photos Infrastructure at Etsy](https://codeascraft.com/2017/05/30/reducing-image-file-size-at-etsy/)\n\t* [Improving GIF Performance at Pinterest](https://medium.com/@Pinterest_Engineering/improving-gif-performance-on-pinterest-8dad74bf92f1)\n\t* [Optimizing Video Playback Performance at Pinterest](https://medium.com/@Pinterest_Engineering/optimizing-video-playback-performance-caf55ce310d1)\n\t* [Optimizing Video Stream for Low Bandwidth with Dynamic Optimizer at Netflix](https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830)\n\t* [Adaptive Video Streaming at YouTube](https://youtube-eng.googleblog.com/2018/04/making-high-quality-video-efficient.html)\n    * [Reducing Video Loading Time at Dailymotion](https://medium.com/dailymotion/reducing-video-loading-time-fa9c997a2294)\n\t* [Improving Homepage Performance at Zillow](https://www.zillow.com/engineering/improving-homepage-performance/)\n\t* [The Process of Optimizing for Client Performance at Expedia](https://medium.com/expedia-engineering/go-fast-or-go-home-the-process-of-optimizing-for-client-performance-57bb497402e)\n\t* [Web Performance at BBC](https://medium.com/bbc-design-engineering/bbc-world-service-web-performance-26b08f7abfcc)\n* [Performance Optimization by Brotli Compression](https://blogs.akamai.com/2016/02/understanding-brotlis-potential.html)\n\t* [Boosting Site Speed Using Brotli Compression at LinkedIn](https://engineering.linkedin.com/blog/2017/05/boosting-site-speed-using-brotli-compression)\t\n\t* [Brotli at Booking.com](https://medium.com/booking-com-development/bookings-journey-with-brotli-978b249d34f3)\n\t* [Brotli at Treebo](https://tech.treebo.com/a-tale-of-brotli-compression-bcb071d9780a)\n\t* [Deploying Brotli for Static Content at Dropbox](https://dropbox.tech/infrastructure/deploying-brotli-for-static-content)\n\t* [Progressive Enhancement with Brotli at Yelp](https://engineeringblog.yelp.com/2017/07/progressive-enhancement-with-brotli.html)\n\t* [Speeding Up Redis with Compression at DoorDash](https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/)\n* [Performance Optimization on Languages and Frameworks](https://www.techempower.com/benchmarks/)\n\t* [Python at Netflix](https://netflixtechblog.com/python-at-netflix-bba45dae649e)\n\t* [Python at scale (3 parts) at Instagram](https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834)\n\t* [OCaml best practices (2 parts) at Issuu](https://engineering.issuu.com/2018/12/10/our-current-ocaml-best-practices-part-2)\n\t* [PHP at Slack](https://slack.engineering/taking-php-seriously-cf7a60065329)\n\t* [Go at Trivago](https://tech.trivago.com/2020/03/02/why-we-chose-go/)\n\t* [TypeScript at Etsy](https://codeascraft.com/2021/11/08/etsys-journey-to-typescript/)\n\t* [Kotlin for taming state at Etsy](https://www.etsy.com/sg-en/codeascraft/sealed-classes-opened-my-mind)\n\t* [Kotlin at DoorDash](https://doordash.engineering/2022/03/22/how-to-leverage-functional-programming-in-kotlin-to-write-better-cleaner-code/)\n\t* [BPF and Go at Bumble](https://medium.com/bumble-tech/bpf-and-go-modern-forms-of-introspection-in-linux-6b9802682223)\n\t* [Ruby on Rails at GitLab](https://medium.com/gitlab-magazine/why-we-use-ruby-on-rails-to-build-gitlab-601dce4a7a38)\n\t* [Rust in production at Figma](https://medium.com/figma-design/rust-in-production-at-figma-e10a0ec31929)\n\t* [Choosing a Language Stack at WeWork](https://engineering.wework.com/choosing-a-language-stack-cac3726928f6)\n\t* [Switching from Go to Rust at Discord](https://blog.discord.com/why-discord-is-switching-from-go-to-rust-a190bbca2b1f)\n\t* [ASP.NET Core Performance Optimization at Agoda](https://medium.com/agoda-engineering/happy-asp-net-core-performance-optimization-4e21a383d299)\n\t* [Data Race Patterns in Go at Uber](https://eng.uber.com/data-race-patterns-in-go/)\n\t* [Java 21 Virtual Threads at Netflix](https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d)\t\n    \n## Intelligence\n* [Big Data](https://insights.sei.cmu.edu/sei_blog/2017/05/reference-architectures-for-big-data-systems.html)\t\n\t* [Data Platform at Uber](https://eng.uber.com/uber-big-data-platform/)\n\t* [Data Platform at BMW](https://www.unibw.de/code/events-u/jt-2018-workshops/ws3_bigdata_vortrag_widmann.pdf)\n\t* [Data Platform at Netflix](https://www.youtube.com/watch?v=CSDIThSwA7s)\n\t* [Data Platform at Flipkart](https://blog.flipkart.tech/overview-of-flipkart-data-platform-20c6d3e9a196)\n\t* [Data Platform at Coupang](https://medium.com/coupang-tech/evolving-the-coupang-data-platform-308e305a9c45)\n\t* [Data Platform at DoorDash](https://doordash.engineering/2020/09/25/how-doordash-is-scaling-its-data-platform/)\n\t* [Data Platform at Khan Academy](http://engineering.khanacademy.org/posts/khanalytics.htm)\n\t* [Data Infrastructure at Airbnb](https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c)\n\t* [Data Infrastructure at LinkedIn](https://www.infoq.com/presentations/big-data-infrastructure-linkedin)\n\t* [Data Infrastructure at GO-JEK](https://blog.gojekengineering.com/data-infrastructure-at-go-jek-cd4dc8cbd929)\n\t* [Data Ingestion Infrastructure at Pinterest](https://medium.com/@Pinterest_Engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754)\n\t* [Data Analytics Architecture at Pinterest](https://medium.com/@Pinterest_Engineering/behind-the-pins-building-analytics-f7b508cdacab)\n\t* [Data Orchestration Service at Spotify](https://engineering.atspotify.com/2022/03/why-we-switched-our-data-orchestration-service/)\n\t* [Big Data Processing (2 parts) at Spotify](https://labs.spotify.com/2017/10/23/big-data-processing-at-spotify-the-road-to-scio-part-2/)\n\t* [Big Data Processing at Uber](https://cdn.oreillystatic.com/en/assets/1/event/160/Big%20data%20processing%20with%20Hadoop%20and%20Spark%2C%20the%20Uber%20way%20Presentation.pdf)\n\t* [Analytics Pipeline at Lyft](https://cdn.oreillystatic.com/en/assets/1/event/269/Lyft_s%20analytics%20pipeline_%20From%20Redshift%20to%20Apache%20Hive%20and%20Presto%20Presentation.pdf)\n\t* [Analytics Pipeline at Grammarly](https://tech.grammarly.com/blog/building-a-versatile-analytics-pipeline-on-top-of-apache-spark)\n\t* [Analytics Pipeline at Teads](https://medium.com/teads-engineering/give-meaning-to-100-billion-analytics-events-a-day-d6ba09aa8f44)\n\t* [ML Data Pipelines for Real-Time Fraud Prevention at PayPal](https://www.infoq.com/presentations/paypal-ml-fraud-prevention-2018)\n\t* [Big Data Analytics and ML Techniques at LinkedIn](https://cdn.oreillystatic.com/en/assets/1/event/269/Big%20data%20analytics%20and%20machine%20learning%20techniques%20to%20drive%20and%20grow%20business%20Presentation%201.pdf)\n\t* [Self-Serve Reporting Platform on Hadoop at LinkedIn](https://cdn.oreillystatic.com/en/assets/1/event/137/Building%20a%20self-serve%20real-time%20reporting%20platform%20at%20LinkedIn%20Presentation%201.pdf)\n\t* [Privacy-Preserving Analytics and Reporting at LinkedIn](https://engineering.linkedin.com/blog/2019/04/privacy-preserving-analytics-and-reporting-at-linkedin)\n\t* [Analytics Platform for Tracking Item Availability at Walmart](https://medium.com/walmartlabs/how-we-build-a-robust-analytics-platform-using-spark-kafka-and-cassandra-lambda-architecture-70c2d1bc8981)\n\t* [Real-Time Analytics for Mobile App Crashes using Apache Pinot at Uber](https://www.uber.com/en-SG/blog/real-time-analytics-for-mobile-app-crashes/)\n\t* [HALO: Hardware Analytics and Lifecycle Optimization at Facebook](https://code.fb.com/data-center-engineering/hardware-analytics-and-lifecycle-optimization-halo-at-facebook/)\n\t* [RBEA: Real-time Analytics Platform at King](https://techblog.king.com/rbea-scalable-real-time-analytics-king/)\n\t* [AresDB: GPU-Powered Real-time Analytics Engine at Uber](https://eng.uber.com/aresdb/)\n\t* [AthenaX: Streaming Analytics Platform at Uber](https://eng.uber.com/athenax/)\n\t* [Jupiter: Config Driven Adtech Batch Ingestion Platform at Uber](https://www.uber.com/en-SG/blog/jupiter-batch-ingestion-platform/)\n\t* [Delta: Data Synchronization and Enrichment Platform at Netflix](https://medium.com/netflix-techblog/delta-a-data-synchronization-and-enrichment-platform-e82c36a79aee)\n\t* [Keystone: Real-time Stream Processing Platform at Netflix](https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a)\n\t* [Databook: Turning Big Data into Knowledge with Metadata at Uber](https://eng.uber.com/databook/)\n\t* [Amundsen: Data Discovery & Metadata Engine at Lyft](https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9)\n\t* [Maze: Funnel Visualization Platform at Uber](https://eng.uber.com/maze/)\n\t* [Metacat: Making Big Data Discoverable and Meaningful at Netflix](https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520)\n\t* [SpinalTap: Change Data Capture System at Airbnb](https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f)\n\t* [Accelerator: Fast Data Processing Framework at eBay](https://www.ebayinc.com/stories/blogs/tech/announcing-the-accelerator-processing-1-000-000-000-lines-per-second-on-a-single-computer/)\n\t* [Omid: Transaction Processing Platform at Yahoo](https://yahooeng.tumblr.com/post/180867271141/a-new-chapter-for-omid)\n\t* [TensorFlowOnSpark: Distributed Deep Learning on Big Data Clusters at Yahoo](https://yahooeng.tumblr.com/post/157196488076/open-sourcing-tensorflowonspark-distributed-deep)\n\t* [CaffeOnSpark: Distributed Deep Learning on Big Data Clusters at Yahoo](https://yahooeng.tumblr.com/post/139916828451/caffeonspark-open-sourced-for-distributed-deep)\n\t* [Spark on Scala: Analytics Reference Architecture at Adobe](https://medium.com/adobetech/spark-on-scala-adobe-analytics-reference-architecture-7457f5614b4c)\n\t* [Experimentation Platform (2 parts) at Spotify](https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/)\n\t* [Experimentation Platform at Airbnb](https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166)\n\t* [Smart Product Platform at Zalando](https://engineering.zalando.com/posts/2017/10/zalando-smart-product-platform.html)\n\t* [Log Analysis Platform at LINE](https://www.slideshare.net/wyukawa/strata2017-sg)\n\t* [Data Visualisation Platform at Myntra](https://medium.com/myntra-engineering/universal-dashboarding-platform-udp-data-visualisation-platform-at-myntra-5f2522fcf72d)\n\t* [Building and Scaling Data Lineage at Netflix](https://medium.com/netflix-techblog/building-and-scaling-data-lineage-at-netflix-to-improve-data-infrastructure-reliability-and-1a52526a7977)\n\t* [Building a scalable data management system for computer vision tasks at Pinterest](https://medium.com/@Pinterest_Engineering/building-a-scalable-data-management-system-for-computer-vision-tasks-a6dee8f1c580)\n\t* [Structured Data at Etsy](https://codeascraft.com/2019/07/31/an-introduction-to-structured-data-at-etsy/)\n\t* [Scaling a Mature Data Pipeline - Managing Overhead at Airbnb](https://medium.com/airbnb-engineering/scaling-a-mature-data-pipeline-managing-overhead-f34835cbc866)\n\t* [Spark Partitioning Strategies at Airbnb](https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908)\n\t* [Scaling the Hadoop Distributed File System at LinkedIn](https://engineering.linkedin.com/blog/2021/the-exabyte-club--linkedin-s-journey-of-scaling-the-hadoop-distr)\n\t* [Scaling Hadoop YARN cluster beyond 10,000 nodes at LinkedIn](https://engineering.linkedin.com/blog/2021/scaling-linkedin-s-hadoop-yarn-cluster-beyond-10-000-nodes)\n\t* [Scaling Big Data Access Controls at Pinterest](https://medium.com/pinterest-engineering/securely-scaling-big-data-access-controls-at-pinterest-bbc3406a1695)\n* [Distributed Machine Learning](https://www.csie.ntu.edu.tw/~cjlin/talks/bigdata-bilbao.pdf)\n\t* [Machine Learning Platform at Yelp](https://engineeringblog.yelp.com/2020/07/ML-platform-overview.html)\n\t* [Machine Learning Platform at Etsy](https://codeascraft.com/2021/12/21/redesigning-etsys-machine-learning-platform/)\n\t* [Machine Learning Platform at Zalando](https://engineering.zalando.com/posts/2022/04/zalando-machine-learning-platform.html)\n\t* [Scaling AI/ML Infrastructure at Uber](https://www.uber.com/en-SG/blog/scaling-ai-ml-infrastructure-at-uber/)\n\t* [Recommendation System at Lyft](https://eng.lyft.com/the-recommendation-system-at-lyft-67bc9dcc1793)\n\t* [Reinforcement Learning Platform at Lyft](https://eng.lyft.com/lyfts-reinforcement-learning-platform-670f77ff46ec)\n\t* [Platform for Serving Recommendations at Etsy](https://www.etsy.com/sg-en/codeascraft/building-a-platform-for-serving-recommendations-at-etsy)\n\t* [Infrastructure to Run User Forecasts at Spotify](https://engineering.atspotify.com/2022/06/how-we-built-infrastructure-to-run-user-forecasts-at-spotify/)\n\t* [Aroma: Using ML for Code Recommendation at Facebook](https://code.fb.com/developer-tools/aroma/)\n\t* [Flyte: Cloud Native Machine Learning and Data Processing Platform at Lyft](https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59)\n\t* [LyftLearn: ML Model Training Infrastructure built on Kubernetes at Lyft](https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb)\n\t* [Horovod: Open Source Distributed Deep Learning Framework for TensorFlow at Uber](https://eng.uber.com/horovod/)\n\t* [Genie: Gen AI On-Call Copilot at Uber](https://www.uber.com/blog/genie-ubers-gen-ai-on-call-copilot/)\n\t* [COTA: Improving Customer Care with NLP & Machine Learning at Uber](https://eng.uber.com/cota/)\n\t* [Manifold: Model-Agnostic Visual Debugging Tool for Machine Learning at Uber](https://eng.uber.com/manifold/)\t\n\t* [Repo-Topix: Topic Extraction Framework at Github](https://githubengineering.com/topics/)\n\t* [Concourse: Generating Personalized Content Notifications in Near-Real-Time at LinkedIn](https://engineering.linkedin.com/blog/2018/05/concourse--generating-personalized-content-notifications-in-near)\n\t* [Altus Care: Applying a Chatbot to Platform Engineering at eBay](https://www.ebayinc.com/stories/blogs/tech/altus-care-apply-chatbot-to-ebay-platform-engineering/)\n\t* [PyKrylov: Accelerating Machine Learning Research at eBay](https://tech.ebayinc.com/engineering/pykrylov-accelerating-machine-learning-research-at-ebay/)\n\t* [Box Graph: Spontaneous Social Network at Box](https://blog.box.com/blog/box-graph-how-we-built-spontaneous-social-network/)\n\t* [PricingNet: Pricing Modelling with Neural Networks at Skyscanner](https://hackernoon.com/pricingnet-modelling-the-global-airline-industry-with-neural-networks-833844d20ea6)\n\t* [PinText: Multitask Text Embedding System at Pinterest](https://medium.com/pinterest-engineering/pintext-a-multitask-text-embedding-system-in-pinterest-b80ece364555)\n\t* [SearchSage: Learning Search Query Representations at Pinterest](https://medium.com/pinterest-engineering/searchsage-learning-search-query-representations-at-pinterest-654f2bb887fc)\n\t* [Cannes: ML saves $1.7M a year on document previews at Dropbox](https://dropbox.tech/machine-learning/cannes--how-ml-saves-us--1-7m-a-year-on-document-previews)\n\t* [Scaling Gradient Boosted Trees for Click-Through-Rate Prediction at Yelp](https://engineeringblog.yelp.com/2018/01/building-a-distributed-ml-pipeline-part1.html)\t\n\t* [Learning with Privacy at Scale at Apple](https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html)\n\t* [Deep Learning for Image Classification Experiment at Mercari](https://medium.com/mercari-engineering/mercaris-image-classification-experiment-using-deep-learning-9b4e994a18ec)\n\t* [Deep Learning for Frame Detection in Product Images at Allegro](https://allegro.tech/2016/12/deep-learning-for-frame-detection.html)\n\t* [Content-based Video Relevance Prediction at Hulu](https://medium.com/hulu-tech-blog/content-based-video-relevance-prediction-b2c448e14752)\n\t* [Moderating Inappropriate Video Content at Yelp](https://engineeringblog.yelp.com/2024/03/moderating-inappropriate-video-content-at-yelp.html)\n\t* [Improving Photo Selection With Deep Learning at TripAdvisor](http://engineering.tripadvisor.com/improving-tripadvisor-photo-selection-deep-learning/)\n\t* [Personalized Recommendations for Experiences Using Deep Learning at TripAdvisor](https://www.tripadvisor.com/engineering/personalized-recommendations-for-experiences-using-deep-learning/)\n\t* [Personalised Recommender Systems at BBC](https://medium.com/bbc-design-engineering/developing-personalised-recommender-systems-at-the-bbc-e26c5e0c4216)\n\t* [Machine Learning (2 parts) at Condé Nast](https://technology.condenast.com/story/handbag-brand-and-color-detection)\n\t* [Natural Language Processing and Content Analysis (2 parts) at Condé Nast](https://technology.condenast.com/story/natural-language-processing-and-content-analysis-at-conde-nast-part-2-system-architecture)\n\t* [Mapping the World of Music Using Machine Learning (2 parts) at iHeartRadio](https://tech.iheart.com/mapping-the-world-of-music-using-machine-learning-part-2-aa50b6a0304c)\n\t* [Machine Learning to Improve Streaming Quality at Netflix](https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f)\n\t* [Machine Learning to Match Drivers & Riders at GO-JEK](https://blog.gojekengineering.com/how-we-use-machine-learning-to-match-drivers-riders-b06d617b9e5)\n\t* [Improving Video Thumbnails with Deep Neural Nets at YouTube](https://youtube-eng.googleblog.com/2015/10/improving-youtube-video-thumbnails-with_8.html)\n\t* \n\n[Content truncated...]",
    "meta_json": "{\"language\":null,\"stars\":67100,\"forks\":6709,\"watchers\":67100,\"open_issues\":18,\"topics\":[\"architecture\",\"awesome\",\"awesome-list\",\"backend\",\"big-data\",\"computer-science\",\"design-patterns\",\"devops\",\"distributed-systems\",\"interview\",\"interview-practice\",\"interview-questions\",\"lists\",\"machine-learning\",\"programming\",\"resources\",\"scalability\",\"system\",\"system-design\",\"web-development\"],\"default_branch\":\"master\",\"size_kb\":1463,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "2f90e75984b3270efee3d993f4641468",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/binhnguyennus/awesome-scalability\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:labmlai:annotated_deep_learning_paper_implementations",
    "name": "annotated_deep_learning_paper_implementations",
    "author": "labmlai",
    "description": "This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations, The website renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better. !Screenshot We are actively maintaining this repo and adding new implementations almost weekly. for updates. * JAX implementation * Multi-headed attention * Triton Flash Attention * Transformer building blocks * Tran...",
    "tags": [
      "attention",
      "deep-learning",
      "deep-learning-tutorial",
      "gan",
      "literate-programming",
      "lora",
      "machine-learning",
      "neural-networks",
      "optimizers",
      "pytorch",
      "reinforcement-learning",
      "transformer",
      "transformers",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 64739,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
    "image_url": null,
    "type": "tool",
    "body_content": "[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)\n\n# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\nThese implementations are documented with explanations,\n\n[The website](https://nn.labml.ai/index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](https://nn.labml.ai/dqn-light.png)\n\nWe are actively maintaining this repo and adding new \nimplementations almost weekly.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Paper Implementations\n\n#### ✨ [Transformers](https://nn.labml.ai/transformers/index.html)\n\n* [JAX implementation](https://nn.labml.ai/transformers/jax_transformer/index.html)\n* [Multi-headed attention](https://nn.labml.ai/transformers/mha.html)\n* [Triton Flash Attention](https://nn.labml.ai/transformers/flash/index.html)\n* [Transformer building blocks](https://nn.labml.ai/transformers/models.html) \n* [Transformer XL](https://nn.labml.ai/transformers/xl/index.html)\n    * [Relative multi-headed attention](https://nn.labml.ai/transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings](https://nn.labml.ai/transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](https://nn.labml.ai/transformers/alibi/index.html)\n* [RETRO](https://nn.labml.ai/transformers/retro/index.html)\n* [Compressive Transformer](https://nn.labml.ai/transformers/compressive/index.html)\n* [GPT Architecture](https://nn.labml.ai/transformers/gpt/index.html)\n* [GLU Variants](https://nn.labml.ai/transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](https://nn.labml.ai/transformers/knn)\n* [Feedback Transformer](https://nn.labml.ai/transformers/feedback/index.html)\n* [Switch Transformer](https://nn.labml.ai/transformers/switch/index.html)\n* [Fast Weights Transformer](https://nn.labml.ai/transformers/fast_weights/index.html)\n* [FNet](https://nn.labml.ai/transformers/fnet/index.html)\n* [Attention Free Transformer](https://nn.labml.ai/transformers/aft/index.html)\n* [Masked Language Model](https://nn.labml.ai/transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](https://nn.labml.ai/transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](https://nn.labml.ai/transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](https://nn.labml.ai/transformers/vit/index.html)\n* [Primer EZ](https://nn.labml.ai/transformers/primer_ez/index.html)\n* [Hourglass](https://nn.labml.ai/transformers/hour_glass/index.html)\n\n#### ✨ [Low-Rank Adaptation (LoRA)](https://nn.labml.ai/lora/index.html)\n\n#### ✨ [Eleuther GPT-NeoX](https://nn.labml.ai/neox/index.html)\n* [Generate on a 48GB GPU](https://nn.labml.ai/neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](https://nn.labml.ai/neox/samples/finetune.html)\n* [LLM.int8()](https://nn.labml.ai/neox/utils/llm_int8.html)\n\n#### ✨ [Diffusion models](https://nn.labml.ai/diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](https://nn.labml.ai/diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](https://nn.labml.ai/diffusion/stable_diffusion/index.html)\n\n#### ✨ [Generative Adversarial Networks](https://nn.labml.ai/gan/index.html)\n* [Original GAN](https://nn.labml.ai/gan/original/index.html)\n* [GAN with deep convolutional network](https://nn.labml.ai/gan/dcgan/index.html)\n* [Cycle GAN](https://nn.labml.ai/gan/cycle_gan/index.html)\n* [Wasserstein GAN](https://nn.labml.ai/gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](https://nn.labml.ai/gan/stylegan/index.html)\n\n#### ✨ [Recurrent Highway Networks](https://nn.labml.ai/recurrent_highway_networks/index.html)\n\n#### ✨ [LSTM](https://nn.labml.ai/lstm/index.html)\n\n#### ✨ [HyperNetworks - HyperLSTM](https://nn.labml.ai/hypernetworks/hyper_lstm.html)\n\n#### ✨ [ResNet](https://nn.labml.ai/resnet/index.html)\n\n#### ✨ [ConvMixer](https://nn.labml.ai/conv_mixer/index.html)\n\n#### ✨ [Capsule Networks](https://nn.labml.ai/capsule_networks/index.html)\n\n#### ✨ [U-Net](https://nn.labml.ai/unet/index.html)\n\n#### ✨ [Sketch RNN](https://nn.labml.ai/sketch_rnn/index.html)\n\n#### ✨ Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](https://nn.labml.ai/graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](https://nn.labml.ai/graphs/gatv2/index.html)\n\n#### ✨ [Counterfactual Regret Minimization (CFR)](https://nn.labml.ai/cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](https://nn.labml.ai/cfr/kuhn/index.html)\n\n#### ✨ [Reinforcement Learning](https://nn.labml.ai/rl/index.html)\n* [Proximal Policy Optimization](https://nn.labml.ai/rl/ppo/index.html) with\n [Generalized Advantage Estimation](https://nn.labml.ai/rl/ppo/gae.html)\n* [Deep Q Networks](https://nn.labml.ai/rl/dqn/index.html) with\n with [Dueling Network](https://nn.labml.ai/rl/dqn/model.html),\n [Prioritized Replay](https://nn.labml.ai/rl/dqn/replay_buffer.html)\n and Double Q Network.\n\n#### ✨ [Optimizers](https://nn.labml.ai/optimizers/index.html)\n* [Adam](https://nn.labml.ai/optimizers/adam.html)\n* [AMSGrad](https://nn.labml.ai/optimizers/amsgrad.html)\n* [Adam Optimizer with warmup](https://nn.labml.ai/optimizers/adam_warmup.html)\n* [Noam Optimizer](https://nn.labml.ai/optimizers/noam.html)\n* [Rectified Adam Optimizer](https://nn.labml.ai/optimizers/radam.html)\n* [AdaBelief Optimizer](https://nn.labml.ai/optimizers/ada_belief.html)\n* [Sophia-G Optimizer](https://nn.labml.ai/optimizers/sophia.html)\n\n#### ✨ [Normalization Layers](https://nn.labml.ai/normalization/index.html)\n* [Batch Normalization](https://nn.labml.ai/normalization/batch_norm/index.html)\n* [Layer Normalization](https://nn.labml.ai/normalization/layer_norm/index.html)\n* [Instance Normalization](https://nn.labml.ai/normalization/instance_norm/index.html)\n* [Group Normalization](https://nn.labml.ai/normalization/group_norm/index.html)\n* [Weight Standardization](https://nn.labml.ai/normalization/weight_standardization/index.html)\n* [Batch-Channel Normalization](https://nn.labml.ai/normalization/batch_channel_norm/index.html)\n* [DeepNorm](https://nn.labml.ai/normalization/deep_norm/index.html)\n\n#### ✨ [Distillation](https://nn.labml.ai/distillation/index.html)\n\n#### ✨ [Adaptive Computation](https://nn.labml.ai/adaptive_computation/index.html)\n\n* [PonderNet](https://nn.labml.ai/adaptive_computation/ponder_net/index.html)\n\n#### ✨ [Uncertainty](https://nn.labml.ai/uncertainty/index.html)\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](https://nn.labml.ai/uncertainty/evidence/index.html)\n\n#### ✨ [Activations](https://nn.labml.ai/activations/index.html)\n\n* [Fuzzy Tiling Activations](https://nn.labml.ai/activations/fta/index.html)\n\n#### ✨ [Langauge Model Sampling Techniques](https://nn.labml.ai/sampling/index.html)\n* [Greedy Sampling](https://nn.labml.ai/sampling/greedy.html)\n* [Temperature Sampling](https://nn.labml.ai/sampling/temperature.html)\n* [Top-k Sampling](https://nn.labml.ai/sampling/top_k.html)\n* [Nucleus Sampling](https://nn.labml.ai/sampling/nucleus.html)\n\n#### ✨ [Scalable Training/Inference](https://nn.labml.ai/scaling/index.html)\n* [Zero3 memory optimizations](https://nn.labml.ai/scaling/zero3/index.html)\n\n### Installation\n\n```bash\npip install labml-nn\n```\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":64739,\"forks\":6557,\"watchers\":64739,\"open_issues\":27,\"topics\":[\"attention\",\"deep-learning\",\"deep-learning-tutorial\",\"gan\",\"literate-programming\",\"lora\",\"machine-learning\",\"neural-networks\",\"optimizers\",\"pytorch\",\"reinforcement-learning\",\"transformer\",\"transformers\"],\"default_branch\":\"master\",\"size_kb\":156359,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "f2c82de3855bbf3c7c3f855ccf043ecb",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/labmlai/annotated_deep_learning_paper_implementations\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:scikit-learn:scikit-learn",
    "name": "scikit-learn",
    "author": "scikit-learn",
    "description": ".. -*- mode: rst -*- |Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPI| |DOI| |Benchmark| .. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main .. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield :target: https://circleci.com/gh/scikit-learn/scikit-...",
    "tags": [
      "data-analysis",
      "data-science",
      "machine-learning",
      "python",
      "statistics",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 64245,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/scikit-learn/scikit-learn",
    "image_url": null,
    "type": "tool",
    "body_content": ".. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPI| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.11\n.. |NumPyMinVersion| replace:: 1.24.1\n.. |SciPyMinVersion| replace:: 1.10.0\n.. |JoblibMinVersion| replace:: 1.3.0\n.. |ThreadpoolctlMinVersion| replace:: 3.2.0\n.. |MatplotlibMinVersion| replace:: 3.6.1\n.. |Scikit-ImageMinVersion| replace:: 0.22.0\n.. |PandasMinVersion| replace:: 1.5.0\n.. |SeabornMinVersion| replace:: 0.13.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.18.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and Plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PytestMinVersion| installed)::\n\n    pytest sklearn\n\nSee the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https://scikit-learn.org\n- HTML documentation (development version): https://scikit-learn.org/dev/\n- FAQ: https://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\nMain Channels\n^^^^^^^^^^^^^\n\n- **Website**: https://scikit-learn.org\n- **Blog**: https://blog.scikit-learn.org\n- **Mailing list**: https://mail.python.org/mailman/listinfo/scikit-learn\n\nDeveloper & Support\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **GitHub Discussions**: https://github.com/scikit-learn/scikit-learn/discussions\n- **Stack Overflow**: https://stackoverflow.com/questions/tagged/scikit-learn\n- **Discord**: https://discord.gg/h9qyrK8Jc8\n\nSocial Media Platforms\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **LinkedIn**: https://www.linkedin.com/company/scikit-learn\n- **YouTube**: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists\n- **Facebook**: https://www.facebook.com/scikitlearnofficial/\n- **Instagram**: https://www.instagram.com/scikitlearnofficial/\n- **TikTok**: https://www.tiktok.com/@scikit.learn\n- **Bluesky**: https://bsky.app/profile/scikit-learn.org\n- **Mastodon**: https://mastodon.social/@sklearn@fosstodon.org\n\nResources\n^^^^^^^^^\n\n- **Calendar**: https://blog.scikit-learn.org/calendar/\n- **Logos & Branding**: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":64245,\"forks\":26498,\"watchers\":64245,\"open_issues\":2119,\"topics\":[\"data-analysis\",\"data-science\",\"machine-learning\",\"python\",\"statistics\"],\"default_branch\":\"main\",\"size_kb\":178581,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\"},{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\"},{\"type\":\"has_code\",\"target_id\":\"github:astral-sh:ruff\",\"source_url\":\"https://github.com/astral-sh/ruff\"},{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\"},{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\"},{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn.git\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn.git\"},{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\"},{\"type\":\"has_code\",\"target_id\":\"github:scikit-learn:scikit-learn\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\"}]",
    "canonical_id": null,
    "license_spdx": "BSD-3-Clause",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "c8e4e8ec3328becb641499bf8bf8ad92",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/scikit-learn/scikit-learn\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:keras-team:keras",
    "name": "keras",
    "author": "keras-team",
    "description": "Keras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only). Effortlessly build and train models for computer vision, natural language processing, audio processing, timeseries forecasting, recommender systems, etc. - **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras and the availability of easy-to-debug runtimes like PyTorch or JAX eager execution. - **State-of-the-ar...",
    "tags": [
      "data-science",
      "deep-learning",
      "jax",
      "machine-learning",
      "neural-networks",
      "python",
      "pytorch",
      "tensorflow",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 63625,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/keras-team/keras",
    "image_url": null,
    "type": "tool",
    "body_content": "# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`. Additionally,\nThe `openvino` backend is available with support for model inference only.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and macOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n## Backend Compatibility Table\n\nThe following table lists the minimum supported versions of each backend for the latest stable release of Keras (v3.x):\n\n| Backend    | Minimum Supported Version |\n|------------|---------------------------|\n| TensorFlow | 2.16.1                    |\n| JAX        | 0.4.20                    |\n| PyTorch    | 2.1.0                     |\n| OpenVINO   | 2025.3.0                  |\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean Python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a JAX GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after\nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.\n\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":63625,\"forks\":19654,\"watchers\":63625,\"open_issues\":261,\"topics\":[\"data-science\",\"deep-learning\",\"jax\",\"machine-learning\",\"neural-networks\",\"python\",\"pytorch\",\"tensorflow\"],\"default_branch\":\"master\",\"size_kb\":48785,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "da0664abbe8e343dd1002fff06b7bf46",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/keras-team/keras\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:ultralytics:yolov5",
    "name": "yolov5",
    "author": "ultralytics",
    "description": "<div align=\"center\"> <p> <a href=\"https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event\" target=\"_blank\"> <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"Ultralytics YOLO banner\"></a> </p> 中文 | 한국어 | 日本語 | Русский | Deutsch | Français | Español | Português | Türkçe | Tiếng Việt | العربية <div> <a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src...",
    "tags": [
      "coreml",
      "deep-learning",
      "ios",
      "machine-learning",
      "ml",
      "object-detection",
      "onnx",
      "pytorch",
      "tflite",
      "ultralytics",
      "yolo",
      "yolov3",
      "yolov5",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 56310,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/ultralytics/yolov5",
    "image_url": null,
    "type": "tool",
    "body_content": "<div align=\"center\">\n  <p>\n    <a href=\"https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event\" target=\"_blank\">\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"Ultralytics YOLO banner\"></a>\n  </p>\n\n[中文](https://docs.ultralytics.com/zh/) | [한국어](https://docs.ultralytics.com/ko/) | [日本語](https://docs.ultralytics.com/ja/) | [Русский](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Français](https://docs.ultralytics.com/fr/) | [Español](https://docs.ultralytics.com/es) | [Português](https://docs.ultralytics.com/pt/) | [Türkçe](https://docs.ultralytics.com/tr/) | [Tiếng Việt](https://docs.ultralytics.com/vi/) | [العربية](https://docs.ultralytics.com/ar/)\n\n<div>\n    <a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI Testing\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"YOLOv5 Citation\"></a>\n    <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n    <a href=\"https://discord.com/invite/ultralytics\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a> <a href=\"https://community.ultralytics.com/\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a> <a href=\"https://www.reddit.com/r/ultralytics/\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n    <br>\n    <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\n    <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n    <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n  </div>\n  <br>\n\nUltralytics YOLOv5 🚀 is a cutting-edge, state-of-the-art (SOTA) computer vision model developed by [Ultralytics](https://www.ultralytics.com/). Based on the [PyTorch](https://pytorch.org/) framework, YOLOv5 is renowned for its ease of use, speed, and accuracy. It incorporates insights and best practices from extensive research and development, making it a popular choice for a wide range of vision AI tasks, including [object detection](https://docs.ultralytics.com/tasks/detect/), [image segmentation](https://docs.ultralytics.com/tasks/segment/), and [image classification](https://docs.ultralytics.com/tasks/classify/).\n\nWe hope the resources here help you get the most out of YOLOv5. Please browse the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for detailed information, raise an issue on [GitHub](https://github.com/ultralytics/yolov5/issues/new/choose) for support, and join our [Discord community](https://discord.com/invite/ultralytics) for questions and discussions!\n\nTo request an Enterprise License, please complete the form at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"2%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"2%\" alt=\"Ultralytics Discord\"></a>\n</div>\n\n</div>\n<br>\n\n## 🚀 YOLO11: The Next Evolution\n\nWe are excited to announce the launch of **Ultralytics YOLO11** 🚀, the latest advancement in our state-of-the-art (SOTA) vision models! Available now at the [Ultralytics YOLO GitHub repository](https://github.com/ultralytics/ultralytics), YOLO11 builds on our legacy of speed, precision, and ease of use. Whether you're tackling [object detection](https://docs.ultralytics.com/tasks/detect/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [pose estimation](https://docs.ultralytics.com/tasks/pose/), [image classification](https://docs.ultralytics.com/tasks/classify/), or [oriented object detection (OBB)](https://docs.ultralytics.com/tasks/obb/), YOLO11 delivers the performance and versatility needed to excel in diverse applications.\n\nGet started today and unlock the full potential of YOLO11! Visit the [Ultralytics Docs](https://docs.ultralytics.com/) for comprehensive guides and resources:\n\n[![PyPI version](https://badge.fury.io/py/ultralytics.svg)](https://badge.fury.io/py/ultralytics) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics)\n\n```bash\n# Install the ultralytics package\npip install ultralytics\n```\n\n<div align=\"center\">\n  <a href=\"https://www.ultralytics.com/yolo\" target=\"_blank\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png\" alt=\"Ultralytics YOLO Performance Comparison\"></a>\n</div>\n\n## 📚 Documentation\n\nSee the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for full documentation on training, testing, and deployment. See below for quickstart examples.\n\n<details open>\n<summary>Install</summary>\n\nClone the repository and install dependencies in a [**Python>=3.8.0**](https://www.python.org/) environment. Ensure you have [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) installed.\n\n```bash\n# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt\n```\n\n</details>\n\n<details open>\n<summary>Inference with PyTorch Hub</summary>\n\nUse YOLOv5 via [PyTorch Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/) for inference. [Models](https://github.com/ultralytics/yolov5/tree/master/models) are automatically downloaded from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\n\n```python\nimport torch\n\n# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # Default: yolov5s\n\n# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # Example image\n\n# Perform inference (handles batching, resizing, normalization automatically)\nresults = model(img)\n\n# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())\nresults.print()  # Print results to console\nresults.show()  # Display results in a window\nresults.save()  # Save results to runs/detect/exp\n```\n\n</details>\n\n<details>\n<summary>Inference with detect.py</summary>\n\nThe `detect.py` script runs inference on various sources. It automatically downloads [models](https://github.com/ultralytics/yolov5/tree/master/models) from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases) and saves the results to the `runs/detect` directory.\n\n```bash\n# Run inference using a webcam\npython detect.py --weights yolov5s.pt --source 0\n\n# Run inference on a local image file\npython detect.py --weights yolov5s.pt --source img.jpg\n\n# Run inference on a local video file\npython detect.py --weights yolov5s.pt --source vid.mp4\n\n# Run inference on a screen capture\npython detect.py --weights yolov5s.pt --source screen\n\n# Run inference on a directory of images\npython detect.py --weights yolov5s.pt --source path/to/images/\n\n# Run inference on a text file listing image paths\npython detect.py --weights yolov5s.pt --source list.txt\n\n# Run inference on a text file listing stream URLs\npython detect.py --weights yolov5s.pt --source list.streams\n\n# Run inference using a glob pattern for images\npython detect.py --weights yolov5s.pt --source 'path/to/*.jpg'\n\n# Run inference on a YouTube video URL\npython detect.py --weights yolov5s.pt --source 'https://youtu.be/LNwODJXcvt4'\n\n# Run inference on an RTSP, RTMP, or HTTP stream\npython detect.py --weights yolov5s.pt --source 'rtsp://example.com/media.mp4'\n```\n\n</details>\n\n<details>\n<summary>Training</summary>\n\nThe commands below demonstrate how to reproduce YOLOv5 [COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/) results. Both [models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) are downloaded automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases). Training times for YOLOv5n/s/m/l/x are approximately 1/2/4/6/8 days on a single [NVIDIA V100 GPU](https://www.nvidia.com/en-us/data-center/v100/). Using [Multi-GPU training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/) can significantly reduce training time. Use the largest `--batch-size` your hardware allows, or use `--batch-size -1` for YOLOv5 [AutoBatch](https://github.com/ultralytics/yolov5/pull/5092). The batch sizes shown below are for V100-16GB GPUs.\n\n```bash\n# Train YOLOv5n on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5n.yaml --batch-size 128\n\n# Train YOLOv5s on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64\n\n# Train YOLOv5m on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5m.yaml --batch-size 40\n\n# Train YOLOv5l on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5l.yaml --batch-size 24\n\n# Train YOLOv5x on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5x.yaml --batch-size 16\n```\n\n<img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png\" alt=\"YOLOv5 Training Results\">\n\n</details>\n\n<details open>\n<summary>Tutorials</summary>\n\n- **[Train Custom Data](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/)** 🚀 **RECOMMENDED**: Learn how to train YOLOv5 on your own datasets.\n- **[Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)** ☘️: Improve your model's performance with expert tips.\n- **[Multi-GPU Training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/)**: Speed up training using multiple GPUs.\n- **[PyTorch Hub Integration](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/)** 🌟 **NEW**: Easily load models using PyTorch Hub.\n- **[Model Export (TFLite, ONNX, CoreML, TensorRT)](https://docs.ultralytics.com/yolov5/tutorials/model_export/)** 🚀: Convert your models to various deployment formats like [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt).\n- **[NVIDIA Jetson Deployment](https://docs.ultralytics.com/guides/nvidia-jetson/)** 🌟 **NEW**: Deploy YOLOv5 on [NVIDIA Jetson](https://developer.nvidia.com/embedded-computing) devices.\n- **[Test-Time Augmentation (TTA)](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)**: Enhance prediction accuracy with TTA.\n- **[Model Ensembling](https://docs.ultralytics.com/yolov5/tutorials/model_ensembling/)**: Combine multiple models for better performance.\n- **[Model Pruning/Sparsity](https://docs.ultralytics.com/yolov5/tutorials/model_pruning_and_sparsity/)**: Optimize models for size and speed.\n- **[Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/)**: Automatically find the best training hyperparameters.\n- **[Transfer Learning with Frozen Layers](https://docs.ultralytics.com/yolov5/tutorials/transfer_learning_with_frozen_layers/)**: Adapt pretrained models to new tasks efficiently using [transfer learning](https://www.ultralytics.com/glossary/transfer-learning).\n- **[Architecture Summary](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/)** 🌟 **NEW**: Understand the YOLOv5 model architecture.\n- **[Ultralytics HUB Training](https://www.ultralytics.com/hub)** 🚀 **RECOMMENDED**: Train and deploy YOLO models using Ultralytics HUB.\n- **[ClearML Logging](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration/)**: Integrate with [ClearML](https://clear.ml/) for experiment tracking.\n- **[Neural Magic DeepSparse Integration](https://docs.ultralytics.com/yolov5/tutorials/neural_magic_pruning_quantization/)**: Accelerate inference with DeepSparse.\n- **[Comet Logging](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration/)** 🌟 **NEW**: Log experiments using [Comet ML](https://www.comet.com/site/).\n\n</details>\n\n## 🧩 Integrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\n<a href=\"https://docs.ultralytics.com/integrations/\" target=\"_blank\">\n    <img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\" alt=\"Ultralytics active learning integrations\">\n</a>\n<br>\n<br>\n\n<div align=\"center\">\n  <a href=\"https://www.ultralytics.com/hub\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png\" width=\"10%\" alt=\"Ultralytics HUB logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/weights-biases/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png\" width=\"10%\" alt=\"Weights & Biases logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/comet/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"10%\" alt=\"Comet ML logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/neural-magic/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"10%\" alt=\"Neural Magic logo\"></a>\n</div>\n\n|                                                       Ultralytics HUB 🌟                                                        |                                                          Weights & Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |\n| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |\n| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |\n\n## ⭐ Ultralytics HUB\n\nExperience seamless AI development with [Ultralytics HUB](https://www.ultralytics.com/hub) ⭐, the ultimate platform for building, training, and deploying [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) models. Visualize datasets, train [YOLOv5](https://docs.ultralytics.com/models/yolov5/) and [YOLOv8](https://docs.ultralytics.com/models/yolov8/) 🚀 models, and deploy them to real-world applications without writing any code. Transform images into actionable insights using our cutting-edge tools and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\n<a align=\"center\" href=\"https://www.ultralytics.com/hub\" target=\"_blank\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\" alt=\"Ultralytics HUB Platform Screenshot\"></a>\n\n## 🤔 Why YOLOv5?\n\nYOLOv5 is designed for simplicity and ease of use. We prioritize real-world performance and accessibility.\n\n<p align=\"left\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/155040763-93c22a27-347c-4e3c-847a-8094621d3f4e.png\" alt=\"YOLOv5 Performance Chart\"></p>\n<details>\n  <summary>YOLOv5-P5 640 Figure</summary>\n\n<p align=\"left\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/155040757-ce0934a3-06a6-43dc-a979-2edbbd69ea0e.png\" alt=\"YOLOv5 P5 640 Performance Chart\"></p>\n</details>\n<details>\n  <summary>Figure Notes</summary>\n\n- **COCO AP val** denotes the [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) at [Intersection over Union (IoU)](https://www.ultralytics.com/glossary/intersection-over-union-iou) thresholds from 0.5 to 0.95, measured on the 5,000-image [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/) across various inference sizes (256 to 1536 pixels).\n- **GPU Speed** measures the average inference time per image on the [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/) using an [AWS p3.2xlarge V100 instance](https://aws.amazon.com/ec2/instance-types/p4/) with a batch size of 32.\n- **EfficientDet** data is sourced from the [google/automl repository](https://github.com/google/automl) at batch size 8.\n- **Reproduce** these results using the command: `python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n6.pt yolov5s6.pt yolov5m6.pt yolov5l6.pt yolov5x6.pt`\n\n</details>\n\n### Pretrained Checkpoints\n\nThis table shows the performance metrics for various YOLOv5 models trained on the COCO dataset.\n\n| Model                                                                                                                                                                    | Size<br><sup>(pixels) | mAP<sup>val<br>50-95 | mAP<sup>val<br>50 | Speed<br><sup>CPU b1<br>(ms) | Speed<br><sup>V100 b1<br>(ms) | Speed<br><sup>V100 b32<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@640 (B) |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------- | -------------------- | ----------------- | ---------------------------- | ----------------------------- | ------------------------------ | ------------------ | ---------------------- |\n| [YOLOv5n](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt)                                                                                       | 640                   | 28.0                 | 45.7              | **45**                       | **6.3**                       | **0.6**                        | **1.9**            | **4.5**                |\n| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt)                                                                                       | 640                   | 37.4                 | 56.8              | 98                           | 6.4                           | 0.9                            | 7.2                | 16.5                   |\n| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt)                                                                                       | 640                   | 45.4                 | 64.1              | 224                          | 8.2                           | 1.7                            | 21.2               | 49.0                   |\n| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt)                                                                                       | 640                   | 49.0                 | 67.3              | 430                          | 10.1                          | 2.7                            | 46.5               | 109.1                  |\n| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt)                                                                                       | 640                   | 50.7                 | 68.9              | 766                          | 12.1                          | 4.8                            | 86.7               | 205.7                  |\n|                                                                                                                                                                          |                       |                      |                   |                              |                               |                                |                    |                        |\n| [YOLOv5n6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt)                                                                                     | 1280                  | 36.0                 | 54.4              | 153                          | 8.1                           | 2.1                            | 3.2                | 4.6                    |\n| [YOLOv5s6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt)                                                                                     | 1280                  | 44.8                 | 63.7              | 385                          | 8.2                           | 3.6                            | 12.6               | 16.8                   |\n| [YOLOv5m6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt)                                                                                     | 1280                  | 51.3                 | 69.3              | 887                          | 11.1                          | 6.8                            | 35.7               | 50.0                   |\n| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt)                                                                                     | 1280                  | 53.7                 | 71.3              | 1784                         | 15.8                          | 10.5                           | 76.8               | 111.4                  |\n| [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt)<br>+ [[TTA]](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/) | 1280<br>1536          | 55.0<br>**55.8**     | 72.7<br>**72.7**  | 3136<br>-                    | 26.2<br>-                     | 19.4<br>-                      | 140.7<br>-         | 209.8<br>-             |\n\n<details>\n  <summary>Table Notes</summary>\n\n- All checkpoints were trained for 300 epochs using default settings. Nano (n) and Small (s) models use [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml) hyperparameters, while Medium (m), Large (l), and Extra-Large (x) models use [hyp.scratch-high.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-high.yaml).\n- **mAP<sup>val</sup>** values represent single-model, single-scale performance on the [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/).<br>Reproduce using: `python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65`\n- **Speed** metrics are averaged over COCO val images using an [AWS p3.2xlarge V100 instance](https://aws.amazon.com/ec2/instance-types/p4/). Non-Maximum Suppression (NMS) time (~1 ms/image) is not included.<br>Reproduce using: `python val.py --data coco.yaml --img 640 --task speed --batch 1`\n- **TTA** ([Test Time Augmentation](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)) includes reflection and scale augmentations for improved accuracy.<br>Reproduce using: `python val.py --data coco.yaml --img 1536 --iou 0.7 --augment`\n\n</details>\n\n## 🖼️ Segmentation\n\nThe YOLOv5 [release v7.0](https://github.com/ultralytics/yolov5/releases/v7.0) introduced [instance segmentation](https://docs.ultralytics.com/tasks/segment/) models that achieve state-of-the-art performance. These models are designed for easy training, validation, and deployment. For full details, see the [Release Notes](https://github.com/ultralytics/yolov5/releases/v7.0) and explore the [YOLOv5 Segmentation Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb) for quickstart examples.\n\n<details>\n  <summary>Segmentation Checkpoints</summary>\n\n<div align=\"center\">\n<a align=\"center\" href=\"https://www.ultralytics.com/yolo\" target=\"_blank\">\n<img width=\"800\" src=\"https://user-images.githubusercontent.com/61612323/204180385-84f3aca9-a5e9-43d8-a617-dda7ca12e54a.png\" alt=\"YOLOv5 Segmentation Performance Chart\"></a>\n</div>\n\nYOLOv5 segmentation models were trained on the [COCO dataset](https://docs.ultralytics.com/datasets/segment/coco/) for 300 epochs at an image size of 640 pixels using A100 GPUs. Models were exported to [ONNX](https://onnx.ai/) FP32 for CPU speed tests and [TensorRT](https://developer.nvidia.com/tensorrt) FP16 for GPU speed tests. All speed tests were conducted on Google [Colab Pro](https://colab.research.google.com/signup) notebooks for reproducibility.\n\n| Model                                                                                      | Size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Train Time<br><sup>300 epochs<br>A100 (hours) | Speed<br><sup>ONNX CPU<br>(ms) | Speed<br><sup>TRT A100<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@640 (B) |\n| ------------------------------------------------------------------------------------------ | --------------------- | -------------------- | --------------------- | --------------------------------------------- | ------------------------------ | ------------------------------ | ------------------ | ---------------------- |\n| [YOLOv5n-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt) | 640                   | 27.6                 | 23.4                  | 80:17                                         | **62.7**                       | **1.2**                        | **2.0**            | **7.1**                |\n| [YOLOv5s-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt) | 640                   | 37.6                 | 31.7                  | 88:16                                         | 173.3                          | 1.4                            | 7.6                | 26.4                   |\n| [YOLOv5m-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt) | 640                   | 45.0                 | 37.1                  | 108:36                                        | 427.0                          | 2.2                            | 22.0               | 70.8                   |\n| [YOLOv5l-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt) | 640                   | 49.0                 | 39.9                  | 66:43 (2x)                                    | 857.4                          | 2.9                            | 47.9               | 147.7                  |\n| [YOLOv5x-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt) | 640                   | **50.7**             | **41.4**              | 62:56 (3x)                                    | 1579.2                         | 4.5                            | 88.8               | 265.7                  |\n\n- All checkpoints were trained for 300 epochs using the SGD optimizer with `lr0=0.01` and `weight_decay=5e-5` at an image size of 640 pixels, using default settings.<br>Training runs are logged at [https://wandb.ai/glenn-jocher/YOLOv5_v70_official](https://wandb.ai/glenn-jocher/YOLOv5_v70_official).\n- **Accuracy** values represent single-model, single-scale performance on the COCO dataset.<br>Reproduce using: `python segment/val.py --data coco.yaml --weights yolov5s-seg.pt`\n- **Speed** metrics are averaged over 100 inference images using a [Colab Pro A100 High-RAM instance](https://colab.research.google.com/signup). Values indicate inference speed only (NMS adds approximately 1ms per image).<br>Reproduce using: `python segment/val.py --data coco.yaml --weights yolov5s-seg.pt --batch 1`\n- **Export** to ONNX (FP32) and TensorRT (FP16) was performed using `export.py`.<br>Reproduce using: `python export.py --weights yolov5s-seg.pt --include engine --device 0 --half`\n\n</details>\n\n<details>\n  <summary>Segmentation Usage Examples &nbsp;<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/segment/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a></summary>\n\n### Train\n\nYOLOv5 segmentation training supports automatic download of the [COCO128-seg dataset](https://docs.ultralytics.com/datasets/segment/coco8-seg/) via the `--data coco128-seg.yaml` argument. For the full [COCO-segments dataset](https://docs.ultralytics.com/datasets/segment/coco/), download it manually using `bash data/scripts/get_coco.sh --train --val --segments` and then train with `python train.py --data coco.yaml`.\n\n```bash\n# Train on a single GPU\npython segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640\n\n# Train using Multi-GPU Distributed Data Parallel (DDP)\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640 --device 0,1,2,3\n```\n\n### Val\n\nValidate the mask [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) of YOLOv5s-seg on the COCO dataset:\n\n```bash\n# Download COCO validation segments split (780MB, 5000 images)\nbash data/scripts/get_coco.sh --val --segments\n\n# Validate the model\npython segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640\n```\n\n### Predict\n\nUse the pretrained YOLOv5m-seg.pt model to perform segmentation on `bus.jpg`:\n\n```bash\n# Run prediction\npython segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg\n```\n\n```python\n# Load model from PyTorch Hub (Note: Inference support might vary)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5m-seg.pt\")\n```\n\n| ![Zidane Segmentation Example](https://user-images.githubusercontent.com/26833433/203113421-decef4c4-183d-4a0a-a6c2-6435b33bc5d3.jpg) | ![Bus Segmentation Example](https://user-images.githubusercontent.com/26833433/203113416-11fe0025-69f7-4874-a0a6-65d0bfe2999a.jpg) |\n| :-----------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------: |\n\n### Export\n\nExport the YOLOv5s-seg model to ONNX and TensorRT formats:\n\n```bash\n# Export model\npython export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0\n```\n\n</details>\n\n## 🏷️ Classification\n\nYOLOv5 [release v6.2](https://github.com/ultralytics/yolov5/releases/v6.2) introduced support for [image classification](https://docs.ultralytics.com/tasks/classify/) model training, validation, and deployment. Check the [Release Notes](https://github.com/ultralytics/yolov5/releases/v6.2) for details and the [YOLOv5 Classification Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/classify/tutorial.ipynb) for quickstart guides.\n\n<details>\n  <summary>Classification Checkpoints</summary>\n\n<br>\n\nYOLOv5-cls classification models were trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) for 90 epochs using a 4xA100 instance. [ResNet](https://arxiv.org/abs/1512.03385) and [EfficientNet](https://arxiv.org/abs/1905.11946) models were trained alongside under identical settings for comparison. Models were exported to [ONNX](https://onnx.ai/) FP32 (CPU speed tests) and [TensorRT](https://developer.nvidia.com/tensorrt) FP16 (GPU speed tests). All speed tests were run on Google [Colab Pro](https://colab.research.google.com/signup) for reproducibility.\n\n| Model                                                                                              | Size<br><sup>(pixels) | Acc<br><sup>top1 | Acc<br><sup>top5 | Training<br><sup>90 epochs<br>4xA100 (hours) | Speed<br><sup>ONNX CPU<br>(ms) | Speed<br><sup>TensorRT V100<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@224 (B) |\n| -------------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | -------------------------------------------- | ------------------------------ | ----------------------------------- | ------------------ | ---------------------- |\n| [YOLOv5n-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt)         | 224                   | 64.6             | 85.4             | 7:59                                         | **3.3**                        | **0.5**                             | **2.5**            | **0.5**                |\n| [YOLOv5s-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt)         | 224                   | 71.5             | 90.2             | 8:09                                         | 6.6                            | 0.6                                 | 5.4                | 1.4                    |\n| [YOLOv5m-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt)         | 224                   | 75.9             | 92.9             | 10:06                                        | 15.5                           | 0.9                                 | 12.9               | 3.9                    |\n| [YOLOv5l-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt)         | 224                   | 78.0             | 94.0             | 11:56                                        | 26.9                           | 1.4                                 | 26.5               | 8.5                    |\n| [YOLOv5x-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt)         | 224                   | **79.0**         | **94.4**         | 15:04                                        | 54.3                           | 1.8                                 | 48.1               | 15.9                   |\n|                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        |\n| [ResNet18](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet18.pt)               | 224                   | 70.3             | 89.5             | **6:47**                                     | 11.2                           | 0.5                                 | 11.7               | 3.7                    |\n| [ResNet34](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet34.pt)               | 224                   | 73.9             | 91.8             | 8:33                                         | 20.6                           | 0.9                                 | 21.8               | 7.4                    |\n| [ResNet50](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet50.pt)               | 224                   | 76.8             | 93.4             | 11:10                                        | 23.4                           | 1.0                                 | 25.6               | 8.5                    |\n| [ResNet101](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet101.pt)             | 224                   | 78.5             | 94.3             | 17:10                                        | 42.1                           | 1.9                                 | 44.5               | 15.9                   |\n|                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        |\n| [EfficientNet_b0](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b0.pt) | 224                   | 75.1             | 92.4             | 13:03                                        | 12.5                           | 1.3                                 | 5.3                | 1.0                    |\n| [EfficientNet_b1](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b1.pt) | 224                   | 76.4             | 93.2             | 17:04                                        | 14.9                           | 1.6                                 | 7.8                | 1.5                    |\n| [EfficientNet_b2](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b2.pt) | 224                   | 76.6             | 93.4             | 17:10                                        | 15.9                           | 1.6                                 | 9.1                | 1.7                    |\n| [EfficientNet_b3](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b3.pt) | 224                   | 77.7             | 94.0             | 19:19                                        | 18.9                           | 1.9                                 | 12.2               | 2.4                    |\n\n<details>\n  <summary>Table Notes (click to expand)</summary>\n\n- All checkpoints were trained for 90 epochs using the SGD optimizer with `lr0=0.001` and `weight_decay=5e-5` at an image size of 224 pixels, using default settings.<br>Training runs are logged at [https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2](https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2).\n- **Accuracy** values (top-1 and top-5) represent single-model, single-scale performance on the [ImageNet-1k dataset](https://docs.ultralytics.com/datasets/classify/imagenet/).<br>Reproduce using: `python classify/val.py --data ../datasets/imagenet --img 224`\n- **Speed** metrics are averaged over 100 inference images using a Google [Colab Pro V100 High-RAM instance](https://colab.research.google.com/signup).<br>Reproduce using: `python classify/val.py --data ../datasets/imagenet --img 224 --batch 1`\n- **Export** to ONNX (FP32) and TensorRT (FP16) was performed using `export.py`.<br>Reproduce using: `python export.py --weights yolov5s-cls.pt --include engine onnx --imgsz 224`\n\n</details>\n</details>\n\n<details>\n  <summary>Classification Usage Examples &nbsp;<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/classify/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a></summary>\n\n### Train\n\nYOLOv5 classification training supports automatic download for datasets like [MNIST](https://docs.ultralytics.com/datasets/classify/mnist/), [Fashion-MNIST](https://docs.ultralytics.com/datasets/classify/fashion-mnist/), [CIFAR10](https://docs.ultralytics.com/datasets/classify/cifar10/), [CIFAR100](https://docs.ultralytics.com/datasets/classify/cifar100/), [Imagenette](https://docs.ultralytics.com/datasets/classify/imagenette/), [Imagewoof](https://docs.ultralytics.com/datasets/classify/imagewoof/), and [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) using the `--data` argument. For example, start training on MNIST with `--data mnist`.\n\n```bash\n# Train on a single GPU using CIFAR-100 dataset\npython classify/train.py --model yolov5s-cls.pt --data cifar100 --epochs 5 --img 224 --batch 128\n\n# Train using Multi-GPU DDP on ImageNet dataset\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3\n```\n\n### Val\n\nValidate the accuracy of the YOLOv5m-cls model on the ImageNet-1k validation dataset:\n\n```bash\n# Download ImageNet validation split (6.3GB, 50,000 images)\nbash data/scripts/get_imagenet.sh --val\n\n# Validate the model\npython classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224\n```\n\n### Predict\n\nUse the pretrained YOLOv5s-cls.pt model to classify the image `bus.jpg`:\n\n```bash\n# Run prediction\npython classify/predict.py --weights yolov5s-cls.pt --source data/images/bus.jpg\n```\n\n```python\n# Load model from PyTorch Hub\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s-cls.pt\")\n```\n\n### Export\n\nExport trained YOLOv5s-cls, ResNet50, and EfficientNet_b0 models to ONNX and TensorRT formats:\n\n```bash\n# Export models\npython export.py --weights yolov5s-cls.pt resnet50.pt efficientnet_b0.pt --include onnx engine --img 224\n```\n\n</details>\n\n## ☁️ Environments\n\nGet started quickly with our pre-configured environments. Click the icons below for setup details.\n\n<div align=\"center\">\n  <a href=\"https://bit.ly/yolov5-paperspace-notebook\" title=\"Run on Paperspace Gradient\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gradient.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\" title=\"Open in Google Colab\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-colab-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\" title=\"Open in Kaggle\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-kaggle-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://hub.docker.com/r/ultralytics/yolov5\" title=\"Pull Docker Image\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-docker-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/\" title=\"AWS Quickstart Guide\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-aws-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/\" title=\"GCP Quickstart Guide\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gcp-small.png\" width=\"10%\" /></a>\n</div>\n\n## 🤝 Contribute\n\nWe welcome your contributions! Making YOLOv5 accessible and effective is a community effort. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. Share your feedback through the [YOLOv5 Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). Thank you to all our contributors for making YOLOv5 better!\n\n[![Ultralytics open-source contributors](https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png)](https://github.com/ultralytics/yolov5/graphs/contributors)\n\n## 📜 License\n\nUltralytics provides two licensing options to meet different needs:\n\n- **AGPL-3.0 License**: An [OSI-approved](https://opensource.org/license/agpl-v3) open-source license ideal for academic research, personal projects, and testing. It promotes open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/yolov5/blob/master/LICENSE) file for details.\n- **Enterprise License**: Tailored for commercial applications, this license allows seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. For commercial use cases, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## 📧 Contact\n\nFor bug reports and feature requests related to YOLOv5, please visit [GitHub Issues](https://github.com/ultralytics/yolov5/issues). For general questions, discussions, and community support, join our [Discord server](https://discord.com/invite/ultralytics)!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"3%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"3%\" alt=\"Ultralytics Discord\"></a>\n</div>\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":56310,\"forks\":17361,\"watchers\":56310,\"open_issues\":55,\"topics\":[\"coreml\",\"deep-learning\",\"ios\",\"machine-learning\",\"ml\",\"object-detection\",\"onnx\",\"pytorch\",\"tflite\",\"ultralytics\",\"yolo\",\"yolov3\",\"yolov5\"],\"default_branch\":\"master\",\"size_kb\":17378,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:google:automl\",\"source_url\":\"https://github.com/google/automl\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:yolov5\",\"source_url\":\"https://github.com/ultralytics/yolov5\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"}]",
    "canonical_id": null,
    "license_spdx": "AGPL-3.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "9a848e05fd14c29ce842644eef01a8c4",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/ultralytics/yolov5\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:ageitgey:face_recognition",
    "name": "face_recognition",
    "author": "ageitgey",
    "description": "_You can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語._ Recognize and manipulate faces from Python or from the command line with the world's simplest face recognition library. Built using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the Labeled Faces in the Wild benchmark. This also provides a simple command line tool that lets you do face recognition on a folder of images from the...",
    "tags": [
      "face-detection",
      "face-recognition",
      "machine-learning",
      "python",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 55869,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/ageitgey/face_recognition",
    "image_url": null,
    "type": "tool",
    "body_content": "# Face Recognition\n\n_You can also read a translated version of this file [in Chinese 简体中文版](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md) or [in Korean 한국어](https://github.com/ageitgey/face_recognition/blob/master/README_Korean.md) or [in Japanese 日本語](https://github.com/m-i-k-i/face_recognition/blob/master/README_Japanese.md)._\n\nRecognize and manipulate faces from Python or from the command line with\nthe world's simplest face recognition library.\n\nBuilt using [dlib](http://dlib.net/)'s state-of-the-art face recognition\nbuilt with deep learning. The model has an accuracy of 99.38% on the\n[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.\n\nThis also provides a simple `face_recognition` command line tool that lets\nyou do face recognition on a folder of images from the command line!\n\n\n[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)\n[![Build Status](https://github.com/ageitgey/face_recognition/workflows/CI/badge.svg?branch=master&event=push)](https://github.com/ageitgey/face_recognition/actions?query=workflow%3ACI)\n[![Documentation Status](https://readthedocs.org/projects/face-recognition/badge/?version=latest)](http://face-recognition.readthedocs.io/en/latest/?badge=latest)\n\n## Features\n\n#### Find faces in pictures\n\nFind all the faces that appear in a picture:\n\n![](https://cloud.githubusercontent.com/assets/896692/23625227/42c65360-025d-11e7-94ea-b12f28cb34b4.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_locations = face_recognition.face_locations(image)\n```\n\n#### Find and manipulate facial features in pictures\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625282/7f2d79dc-025d-11e7-8728-d8924596f8fa.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n```\n\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff\nlike applying [digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py) (think 'Meitu'):\n\n![](https://cloud.githubusercontent.com/assets/896692/23625283/80638760-025d-11e7-80a2-1d2779f7ccab.png)\n\n#### Identify faces in pictures\n\nRecognize who appears in each photo.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625229/45e049b6-025d-11e7-89cc-8a71cf89e713.png)\n\n```python\nimport face_recognition\nknown_image = face_recognition.load_image_file(\"biden.jpg\")\nunknown_image = face_recognition.load_image_file(\"unknown.jpg\")\n\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)\n```\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\n![](https://cloud.githubusercontent.com/assets/896692/24430398/36f0e3f0-13cb-11e7-8258-4d0c9ce1e419.gif)\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py) for the code.\n\n## Online Demos\n\nUser-contributed shared Jupyter notebook demo (not officially supported): [![Deepnote](https://beta.deepnote.org/buttons/try-in-a-jupyter-notebook.svg)](https://beta.deepnote.org/launch?template=face_recognition)\n\n## Installation\n\n### Requirements\n\n  * Python 3.3+ or Python 2.7\n  * macOS or Linux (Windows not officially supported, but might work)\n\n### Installation Options:\n\n#### Installing on Mac or Linux\n\nFirst, make sure you have dlib already installed with Python bindings:\n\n  * [How to install dlib from source on macOS or Ubuntu](https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf)\n  \nThen, make sure you have cmake installed:  \n \n```brew install cmake```\n\nFinally, install this module from pypi using `pip3` (or `pip2` for Python 2):\n\n```bash\npip3 install face_recognition\n```\n\nAlternatively, you can try this library with [Docker](https://www.docker.com/), see [this section](#deployment).\n\nIf you are having trouble with installation, you can also try out a\n[pre-configured VM](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b).\n\n#### Installing on an Nvidia Jetson Nano board\n\n * [Jetson Nano installation instructions](https://medium.com/@ageitgey/build-a-hardware-based-face-recognition-system-for-150-with-the-nvidia-jetson-nano-and-python-a25cb8c891fd)\n   * Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.\n\n#### Installing on Raspberry Pi 2+\n\n  * [Raspberry Pi 2+ installation instructions](https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65)\n\n#### Installing on FreeBSD\n\n```bash\npkg install graphics/py-face_recognition\n```\n\n#### Installing on Windows\n\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n\n  * [@masoudr's Windows 10 installation guide (dlib + face_recognition)](https://github.com/ageitgey/face_recognition/issues/175#issue-257710508)\n\n#### Installing a pre-configured Virtual Machine image\n\n  * [Download the pre-configured VM image](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b) (for VMware Player or VirtualBox).\n\n## Usage\n\n### Command-Line Interface\n\nWhen you install `face_recognition`, you get two simple command-line \nprograms:\n\n* `face_recognition` - Recognize faces in a photograph or folder full for \n   photographs.\n* `face_detection` - Find faces in a photograph or folder full for photographs.\n\n#### `face_recognition` command line tool\n\nThe `face_recognition` command lets you recognize faces in a photograph or \nfolder full  for photographs.\n\nFirst, you need to provide a folder with one picture of each person you\nalready know. There should be one image file for each person with the\nfiles named according to who is in the picture:\n\n![known](https://cloud.githubusercontent.com/assets/896692/23582466/8324810e-00df-11e7-82cf-41515eba704d.png)\n\nNext, you need a second folder with the files you want to identify:\n\n![unknown](https://cloud.githubusercontent.com/assets/896692/23582465/81f422f8-00df-11e7-8b0d-75364f641f58.png)\n\nThen in you simply run the command `face_recognition`, passing in\nthe folder of known people and the folder (or single image) with unknown\npeople and it tells you who is in each image:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nThere's one line in the output for each face. The data is comma-separated\nwith the filename and the name of the person found.\n\nAn `unknown_person` is a face in the image that didn't match anyone in\nyour folder of known people.\n\n#### `face_detection` command line tool\n\nThe `face_detection` command lets you find the location (pixel coordinatates) \nof any faces in an image.\n\nJust run the command `face_detection`, passing in a folder of images \nto check (or a single image):\n\n```bash\n$ face_detection  ./folder_with_pictures/\n\nexamples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792\n```\n\nIt prints one line for each face that was detected. The coordinates\nreported are the top, right, bottom and left coordinates of the face (in pixels).\n \n##### Adjusting Tolerance / Sensitivity\n\nIf you are getting multiple matches for the same person, it might be that\nthe people in your photos look very similar and a lower tolerance value\nis needed to make face comparisons more strict.\n\nYou can do that with the `--tolerance` parameter. The default tolerance\nvalue is 0.6 and lower numbers make face comparisons more strict:\n\n```bash\n$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nIf you want to see the face distance calculated for each match in order\nto adjust the tolerance setting, you can use `--show-distance true`:\n\n```bash\n$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None\n```\n\n##### More Examples\n\nIf you simply want to know the names of the people in each photograph but don't\ncare about file names, you could do this:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2\n\nBarack Obama\nunknown_person\n```\n\n##### Speeding up Face Recognition\n\nFace recognition can be done in parallel if you have a computer with\nmultiple CPU cores. For example, if your system has 4 CPU cores, you can\nprocess about 4 times as many images in the same amount of time by using\nall your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a `--cpus <number_of_cpu_cores_to_use>` parameter:\n\n```bash\n$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/\n```\n\nYou can also pass in `--cpus -1` to use all CPU cores in your system.\n\n#### Python Module\n\nYou can import the `face_recognition` module and then easily manipulate\nfaces with just a couple of lines of code. It's super easy!\n\nAPI Docs: [https://face-recognition.readthedocs.io](https://face-recognition.readthedocs.io/en/latest/face_recognition.html).\n\n##### Automatically find all the faces in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image)\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via NVidia's CUDA library) is required for good\nperformance with this model. You'll also want to enable CUDA support\nwhen compliling `dlib`.\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image, model=\"cnn\")\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n to try it out.\n\nIf you have a lot of images and a GPU, you can also\n[find faces in batches](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py).\n\n##### Automatically locate the facial features of a person in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n\n# face_landmarks_list is now an array with the locations of each facial feature in each face.\n# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n to try it out.\n\n##### Recognize faces in images and identify who they are\n\n```python\nimport face_recognition\n\npicture_of_me = face_recognition.load_image_file(\"me.jpg\")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\n\n# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!\n\nunknown_picture = face_recognition.load_image_file(\"unknown.jpg\")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\n\n# Now we can see the two face encodings are of the same person with `compare_faces`!\n\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\n\nif results[0] == True:\n    print(\"It's a picture of me!\")\nelse:\n    print(\"It's not a picture of me!\")\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n to try it out.\n\n## Python Code Examples\n\nAll the examples are available [here](https://github.com/ageitgey/face_recognition/tree/master/examples).\n\n\n#### Face Detection\n\n* [Find faces in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n* [Find faces in a photograph (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n* [Find faces in batches of images w/ GPU (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py)\n* [Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/blur_faces_on_webcam.py)\n\n#### Facial Features\n\n* [Identify specific facial features in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n* [Apply (horribly ugly) digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py)\n\n#### Facial Recognition\n\n* [Find and recognize unknown faces in a photograph based on photographs of known people](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n* [Identify and draw boxes around each person in a photo](https://github.com/ageitgey/face_recognition/blob/master/examples/identify_and_draw_boxes_on_faces.py)\n* [Compare faces by numeric face distance instead of only True/False matches](https://github.com/ageitgey/face_recognition/blob/master/examples/face_distance.py)\n* [Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam.py)\n* [Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py)\n* [Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n* [Recognize faces on a Raspberry Pi w/ camera](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_on_raspberry_pi.py)\n* [Run a web service to recognize faces via HTTP (Requires Flask to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/web_service_example.py)\n* [Recognize faces with a K-nearest neighbors classifier](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py)\n* [Train multiple images per person then recognize faces using a SVM](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_svm.py)\n\n## Creating a Standalone Executable\nIf you want to create a standalone executable that can run without the need to install `python` or `face_recognition`, you can use [PyInstaller](https://github.com/pyinstaller/pyinstaller). However, it requires some custom configuration to work with this library. See [this issue](https://github.com/ageitgey/face_recognition/issues/357) for how to do it.\n\n## Articles and Guides that cover `face_recognition`\n\n- My article on how Face Recognition works: [Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)\n  - Covers the algorithms and how they generally work\n- [Face recognition with OpenCV, Python, and deep learning](https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/) by Adrian Rosebrock\n  - Covers how to use face recognition in practice\n- [Raspberry Pi Face Recognition](https://www.pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/) by Adrian Rosebrock\n  - Covers how to use this on a Raspberry Pi\n- [Face clustering with Python](https://www.pyimagesearch.com/2018/07/09/face-clustering-with-python/) by Adrian Rosebrock\n  - Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\n## How Face Recognition Works\n\nIf you want to learn how face location and recognition work instead of\ndepending on a black box library, [read my article](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n\n## Caveats\n\n* The face recognition model is trained on adults and does not work very well on children. It tends to mix\n  up children quite easy using the default comparison threshold of 0.6.\n* Accuracy may vary between ethnic groups. Please see [this wiki page](https://github.com/ageitgey/face_recognition/wiki/Face-Recognition-Accuracy-Problems#question-face-recognition-works-well-with-european-individuals-but-overall-accuracy-is-lower-with-asian-individuals) for more details.\n\n## <a name=\"deployment\">Deployment to Cloud Hosts (Heroku, AWS, etc)</a>\n\nSince `face_recognition` depends on `dlib` which is written in C++, it can be tricky to deploy an app\nusing it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with\n`face_recognition` in a [Docker](https://www.docker.com/) container. With that, you should be able to deploy\nto any service that supports Docker images.\n\nYou can try the Docker image locally by running: `docker-compose up --build`\n\nThere are also [several prebuilt Docker images.](docker/README.md)\n\nLinux users with a GPU (drivers >= 384.81) and [Nvidia-Docker](https://github.com/NVIDIA/nvidia-docker) installed can run the example on the GPU: Open the [docker-compose.yml](docker-compose.yml) file and uncomment the `dockerfile: Dockerfile.gpu` and `runtime: nvidia` lines.\n\n## Having problems?\n\nIf you run into problems, please read the [Common Errors](https://github.com/ageitgey/face_recognition/wiki/Common-Errors) section of the wiki before filing a github issue.\n\n## Thanks\n\n* Many, many thanks to [Davis King](https://github.com/davisking) ([@nulhom](https://twitter.com/nulhom))\n  for creating dlib and for providing the trained facial feature detection and face encoding models\n  used in this library. For more information on the ResNet that powers the face encodings, check out\n  his [blog post](http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html).\n* Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,\n  pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n* Thanks to [Cookiecutter](https://github.com/audreyr/cookiecutter) and the\n  [audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template\n  for making Python project packaging way more tolerable.\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":55869,\"forks\":13707,\"watchers\":55869,\"open_issues\":831,\"topics\":[\"face-detection\",\"face-recognition\",\"machine-learning\",\"python\"],\"default_branch\":\"master\",\"size_kb\":103959,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:m-i-k-i:face_recognition\",\"source_url\":\"https://github.com/m-i-k-i/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:pyinstaller:pyinstaller\",\"source_url\":\"https://github.com/pyinstaller/pyinstaller\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:NVIDIA:nvidia-docker\",\"source_url\":\"https://github.com/NVIDIA/nvidia-docker\"},{\"type\":\"has_code\",\"target_id\":\"github:ageitgey:face_recognition\",\"source_url\":\"https://github.com/ageitgey/face_recognition\"},{\"type\":\"has_code\",\"target_id\":\"github:audreyr:cookiecutter\",\"source_url\":\"https://github.com/audreyr/cookiecutter\"},{\"type\":\"has_code\",\"target_id\":\"github:audreyr:cookiecutter-pypackage\",\"source_url\":\"https://github.com/audreyr/cookiecutter-pypackage\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "215a6d5dc5625c370947928436292b6d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/ageitgey/face_recognition\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:openbb-finance:openbb",
    "name": "OpenBB",
    "author": "OpenBB-finance",
    "description": "<br /> <img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-light.svg?raw=true#gh-light-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\"> <img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\"> <br /> <br /> <a href=\"https://codespaces.new/OpenBB-finance/OpenBB\"> <img src=\"https://github.com/codespaces/badge.svg\" height=\"20\" /> </a> <a target=\"_blank...",
    "tags": [
      "ai",
      "crypto",
      "derivatives",
      "economics",
      "equity",
      "finance",
      "fixed-income",
      "machine-learning",
      "openbb",
      "options",
      "python",
      "quantitative-finance",
      "stocks",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 55300,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/OpenBB-finance/OpenBB",
    "image_url": null,
    "type": "tool",
    "body_content": "<br />\n<img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-light.svg?raw=true#gh-light-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\">\n<img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only\" alt=\"Open Data Platform by OpenBB logo\" width=\"600\">\n<br />\n<br />\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)\n[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)\n<a href=\"https://codespaces.new/OpenBB-finance/OpenBB\">\n  <img src=\"https://github.com/codespaces/badge.svg\" height=\"20\" />\n</a>\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&label=PyPI%20Package)](https://pypi.org/project/openbb/)\n\nOpen Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.\n\nODP operates as the \"connect once, consume everywhere\" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.\n\n<a href=\"https://pro.openbb.co\">\n  <div align=\"center\">\n  <img src=\"https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png\" alt=\"Logo\" width=\"1000\">\n  </div>\n</a>\n\nGet started with: `pip install openbb`\n\n```python\nfrom openbb import obb\noutput = obb.equity.price.historical(\"AAPL\")\ndf = output.to_dataframe()\n```\n\nData integrations available can be found here: <https://docs.openbb.co/python/reference>\n\n---\n\n## OpenBB Workspace\n\nWhile the Open Data Platform provides the open-source data integration foundation, **OpenBB Workspace** offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's \"connect once, consume everywhere\" architecture enables seamless integration between the two.\n\nYou can find OpenBB Workspace at <https://pro.openbb.co>.\n<a href=\"https://pro.openbb.co\">\n  <div align=\"center\">\n  <img src=\"https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png\" alt=\"Logo\" width=\"1000\">\n  </div>\n</a>\n\nData integration:\n\n- You can learn more about adding data to the OpenBB workspace from the [docs](https://docs.openbb.co/workspace) or [this open source repository](https://github.com/OpenBB-finance/backends-for-openbb).\n\nAI Agents integration:\n\n- You can learn more about adding AI agents to the OpenBB workspace from [this open source repository](https://github.com/OpenBB-finance/agents-for-openbb).\n\n### Integrating Open Data Platform to the OpenBB Workspace\n\nConnect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.\n\n#### Run an ODP backend\n\n- Install the packages.\n\n```sh\npip install \"openbb[all]\"\n```\n\n- Start the API server over localhost.\n\n```sh\nopenbb-api\n```\n\nThis will launch a FastAPI server, via Uvicorn, at `127.0.0.1:6900`.\n\nYou can check that it works by going to <http://127.0.0.1:6900>.\n\n#### Integrate the ODP Backend to OpenBB Workspace\n\nSign-in to the [OpenBB Workspace](https://pro.openbb.co/), and follow the following steps:\n\n![CleanShot 2025-05-17 at 09 51 56@2x](https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069)\n\n1. Go to the \"Apps\" tab\n2. Click on \"Connect backend\"\n3. Fill in the form with:\n   Name: Open Data Platform\n   URL: <http://127.0.0.1:6900>\n4. Click on \"Test\". You should get a \"Test successful\" with the number of apps found.\n5. Click on \"Add\".\n\nThat's it.\n\n---\n\n<!-- TABLE OF CONTENTS -->\n<details closed=\"closed\">\n  <summary><h2 style=\"display: inline-block\">Table of Contents</h2></summary>\n  <ol>\n    <li><a href=\"#1-installation\">Installation</a></li>\n    <li><a href=\"#2-contributing\">Contributing</a></li>\n    <li><a href=\"#3-license\">License</a></li>\n    <li><a href=\"#4-disclaimer\">Disclaimer</a></li>\n    <li><a href=\"#5-contacts\">Contacts</a></li>\n    <li><a href=\"#6-star-history\">Star History</a></li>\n    <li><a href=\"#7-contributors\">Contributors</a></li>\n  </ol>\n</details>\n\n## 1. Installation\n\nThe ODP Python Package can be installed from [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`\n\nor by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process, in the [OpenBB Documentation](https://docs.openbb.co/python/installation).\n\n### ODP CLI installation\n\nThe ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.\n\nIt can be installed by running `pip install openbb-cli`\n\nor by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).\n\n## 2. Contributing\n\nThere are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)\n\n### Become a Contributor\n\n- More information on our [Developer Documentation](https://docs.openbb.co/python/developer).\n\n### Create a GitHub ticket\n\nBefore creating a ticket make sure the one you are creating doesn't exist already [among the existing issues](https://github.com/OpenBB-finance/OpenBB/issues)\n\n- [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBug%5D)\n- [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=enhancement&template=enhancement.md&title=%5BIMPROVE%5D)\n- [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=new+feature&template=feature_request.md&title=%5BFR%5D)\n\n### Provide feedback\n\nWe are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.\n\n## 3. License\n\nDistributed under the AGPLv3 License. See\n[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.\n\n## 4. Disclaimer\n\nTrading in financial instruments involves high risks including the risk of losing some, or all, of your investment\namount, and may not be suitable for all investors.\n\nBefore deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.\n\nThe data contained in the Open Data Platform is not necessarily accurate.\n\nOpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.\n\nAll names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.\n\nOur use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.\n\n## 5. Contacts\n\nIf you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`\n\nIf you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`\n\nAny of our social media platforms: [openbb.co/links](https://openbb.co/links)\n\n## 6. Star History\n\nThis is a proxy of our growth and that we are just getting started.\n\nBut for more metrics important to us check [openbb.co/open](https://openbb.co/open).\n\n[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)\n\n## 7. Contributors\n\nOpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.\n\n<a href=\"https://github.com/OpenBB-finance/OpenBB/graphs/contributors\">\n   <img src=\"https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB\" width=\"800\"/>\n</a>\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members\n[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers\n[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&color=blue\n[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues\n[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=yellow\n[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen\n[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=success\n[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed\n[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/DidierRLopes\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":55300,\"forks\":5364,\"watchers\":55300,\"open_issues\":50,\"topics\":[\"ai\",\"crypto\",\"derivatives\",\"economics\",\"equity\",\"finance\",\"fixed-income\",\"machine-learning\",\"openbb\",\"options\",\"python\",\"quantitative-finance\",\"stocks\"],\"default_branch\":\"develop\",\"size_kb\":2383294,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:codespaces:badge.svg\\\"\",\"source_url\":\"https://github.com/codespaces/badge.svg\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:backends-for-openbb\",\"source_url\":\"https://github.com/OpenBB-finance/backends-for-openbb\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:agents-for-openbb\",\"source_url\":\"https://github.com/OpenBB-finance/agents-for-openbb\"},{\"type\":\"has_code\",\"target_id\":\"github:user-attachments:assets\",\"source_url\":\"https://github.com/user-attachments/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB.git`.\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB.git`.\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB.git`.\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB.git`.\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"},{\"type\":\"has_code\",\"target_id\":\"github:OpenBB-finance:OpenBB\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\"}]",
    "canonical_id": null,
    "license_spdx": "NOASSERTION",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "d9fdde35be4412a465184548b06cedaf",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/OpenBB-finance/OpenBB\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:deepfakes:faceswap",
    "name": "faceswap",
    "author": "deepfakes",
    "description": "<p align=\"center\"> <a href=\"https://faceswap.dev\"><img src=\"https://i.imgur.com/zHvjHnb.png\"></img></a> <br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos. </p> <p align=\"center\"> <img src = \"https://i.imgur.com/nWHFLDf.jpg\"></img> </p> <p align=\"center\"> <a href=\"https://www.patreon.com/bePatron?u=23238350\"><img src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\"></img></a> &nbsp;&nbsp;&nbsp;&nbsp;<a href=\"https://discord...",
    "tags": [
      "deep-face-swap",
      "deep-learning",
      "deep-neural-networks",
      "deepface",
      "deepfakes",
      "deeplearning",
      "face-swap",
      "faceswap",
      "fakeapp",
      "machine-learning",
      "myfakeapp",
      "neural-nets",
      "neural-networks",
      "openfaceswap",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 54778,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/deepfakes/faceswap",
    "image_url": null,
    "type": "tool",
    "body_content": "# deepfakes_faceswap\n\n### Important information for **Patreon** and **PayPal** supporters. Please see this forum post: https://forum.faceswap.dev/viewtopic.php?f=14&t=3120\n\n<p align=\"center\">\n  <a href=\"https://faceswap.dev\"><img src=\"https://i.imgur.com/zHvjHnb.png\"></img></a>\n<br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.\n</p>\n<p align=\"center\">\n<img src = \"https://i.imgur.com/nWHFLDf.jpg\"></img>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.patreon.com/bePatron?u=23238350\"><img src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\"></img></a>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"https://discord.gg/FC54sYg\"><img src=\"https://i.imgur.com/gIpztkv.png\"></img></a></p>\n\n<p align=\"center\">\n  <a href=\"https://www.dailymotion.com/video/x810mot\"><img src=\"https://user-images.githubusercontent.com/36920800/178301720-b69841bb-a1ca-4c20-91db-a2a10f5692ca.png\"></img></a>\n<br />Emma Stone/Scarlett Johansson FaceSwap using the Phaze-A model\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=r1jng79a5xc\"><img src=\"https://img.youtube.com/vi/r1jng79a5xc/0.jpg\"></img></a>\n<br />Jennifer Lawrence/Steve Buscemi FaceSwap using the Villain model\n</p>\n\n\n![Build Status](https://github.com/deepfakes/faceswap/actions/workflows/pytest.yml/badge.svg) [![Documentation Status](https://readthedocs.org/projects/faceswap/badge/?version=latest)](https://faceswap.readthedocs.io/en/latest/?badge=latest)\n\nMake sure you check out [INSTALL.md](INSTALL.md) before getting started.\n\n- [deepfakes\\_faceswap](#deepfakes_faceswap)\n    - [Important information for **Patreon** and **PayPal** supporters. Please see this forum post: https://forum.faceswap.dev/viewtopic.php?f=14\\&t=3120](#important-information-for-patreon-and-paypal-supporters-please-see-this-forum-post-httpsforumfaceswapdevviewtopicphpf14t3120)\n- [Manifesto](#manifesto)\n  - [FaceSwap has ethical uses.](#faceswap-has-ethical-uses)\n- [How To setup and run the project](#how-to-setup-and-run-the-project)\n- [Overview](#overview)\n  - [Extract](#extract)\n  - [Train](#train)\n  - [Convert](#convert)\n  - [GUI](#gui)\n- [General notes:](#general-notes)\n- [Help I need support!](#help-i-need-support)\n  - [Discord Server](#discord-server)\n  - [FaceSwap Forum](#faceswap-forum)\n- [Donate](#donate)\n  - [Patreon](#patreon)\n  - [One time Donations](#one-time-donations)\n    - [@torzdf](#torzdf)\n    - [@andenixa](#andenixa)\n- [How to contribute](#how-to-contribute)\n  - [For people interested in the generative models](#for-people-interested-in-the-generative-models)\n  - [For devs](#for-devs)\n  - [For non-dev advanced users](#for-non-dev-advanced-users)\n  - [For end-users](#for-end-users)\n- [About machine learning](#about-machine-learning)\n  - [How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?](#how-does-a-computer-know-how-to-recognizeshape-faces-how-does-machine-learning-work-what-is-a-neural-network)\n\n# Manifesto\n\n## FaceSwap has ethical uses.\n\nWhen faceswapping was first developed and published, the technology was groundbreaking, it was a huge step in AI development. It was also completely ignored outside of academia because the code was confusing and fragmentary. It required a thorough understanding of complicated AI techniques and took a lot of effort to figure it out. Until one individual brought it together into a single, cohesive collection. It ran, it worked, and as is so often the way with new technology emerging on the internet, it was immediately used to create inappropriate content. Despite the inappropriate uses the software was given originally, it was the first AI code that anyone could download, run and learn by experimentation without having a Ph.D. in math, computer theory, psychology, and more. Before \"deepfakes\" these techniques were like black magic, only practiced by those who could understand all of the inner workings as described in esoteric and endlessly complicated books and papers.\n\n\"Deepfakes\" changed all that and anyone could participate in AI development. To us, developers, the release of this code opened up a fantastic learning opportunity. It allowed us to build on ideas developed by others, collaborate with a variety of skilled coders, experiment with AI whilst learning new skills and ultimately contribute towards an emerging technology which will only see more mainstream use as it progresses.\n\nAre there some out there doing horrible things with similar software? Yes. And because of this, the developers have been following strict ethical standards. Many of us don't even use it to create videos, we just tinker with the code to see what it does. Sadly, the media concentrates only on the unethical uses of this software. That is, unfortunately, the nature of how it was first exposed to the public, but it is not representative of why it was created, how we use it now, or what we see in its future. Like any technology, it can be used for good or it can be abused. It is our intention to develop FaceSwap in a way that its potential for abuse is minimized whilst maximizing its potential as a tool for learning, experimenting and, yes, for legitimate faceswapping.\n\nWe are not trying to denigrate celebrities or to demean anyone. We are programmers, we are engineers, we are Hollywood VFX artists, we are activists, we are hobbyists, we are human beings. To this end, we feel that it's time to come out with a standard statement of what this software is and isn't as far as us developers are concerned.\n\n- FaceSwap is not for creating inappropriate content.\n- FaceSwap is not for changing faces without consent or with the intent of hiding its use.\n- FaceSwap is not for any illicit, unethical, or questionable purposes.\n- FaceSwap exists to experiment and discover AI techniques, for social or political commentary, for movies, and for any number of ethical and reasonable uses.\n\nWe are very troubled by the fact that FaceSwap can be used for unethical and disreputable things. However, we support the development of tools and techniques that can be used ethically as well as provide education and experience in AI for anyone who wants to learn it hands-on. We will take a zero tolerance approach to anyone using this software for any unethical purposes and will actively discourage any such uses.\n\n# How To setup and run the project\nFaceSwap is a Python program that will run on multiple Operating Systems including Windows, Linux, and MacOS.\n\nSee [INSTALL.md](INSTALL.md) for full installation instructions. You will need a modern GPU with CUDA support for best performance. Many AMD GPUs are supported through DirectML (Windows) and ROCm (Linux).\n\n# Overview\nThe project has multiple entry points. You will have to:\n - Gather photos and/or videos\n - **Extract** faces from your raw photos\n - **Train** a model on the faces extracted from the photos/videos\n - **Convert** your sources with the model\n\nCheck out [USAGE.md](USAGE.md) for more detailed instructions.\n\n## Extract\nFrom your setup folder, run `python faceswap.py extract`. This will take photos from `src` folder and extract faces into `extract` folder.\n\n## Train\nFrom your setup folder, run `python faceswap.py train`. This will take photos from two folders containing pictures of both faces and train a model that will be saved inside the `models` folder.\n\n## Convert\nFrom your setup folder, run `python faceswap.py convert`. This will take photos from `original` folder and apply new faces into `modified` folder.\n\n## GUI\nAlternatively, you can run the GUI by running `python faceswap.py gui`\n\n# General notes:\n- All of the scripts mentioned have `-h`/`--help` options with arguments that they will accept. You're smart, you can figure out how this works, right?!\n\nNB: there is a conversion tool for video. This can be accessed by running `python tools.py effmpeg -h`. Alternatively, you can use [ffmpeg](https://www.ffmpeg.org) to convert video into photos, process images, and convert images back to the video.\n\n\n**Some tips:**\n\nReusing existing models will train much faster than starting from nothing.\nIf there is not enough training data, start with someone who looks similar, then switch the data.\n\n# Help I need support!\n## Discord Server\nYour best bet is to join the [FaceSwap Discord server](https://discord.gg/FC54sYg) where there are plenty of users willing to help. Please note that, like this repo, this is a SFW Server!\n\n## FaceSwap Forum\nAlternatively, you can post questions in the [FaceSwap Forum](https://faceswap.dev/forum). Please do not post general support questions in this repo as they are liable to be deleted without response.\n\n# Donate\nThe developers work tirelessly to improve and develop FaceSwap. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.\n\n## Patreon\nThe best way to support us is through our Patreon page:\n\n[![become-a-patron](https://c5.patreon.com/external/logo/become_a_patron_button.png)](https://www.patreon.com/bePatron?u=23238350)\n\n## One time Donations\nAlternatively you can give a one off donation to any of our Devs:\n### @torzdf\n There is very little FaceSwap code that hasn't been touched by torzdf. He is responsible for implementing the GUI, FAN aligner, MTCNN detector and porting the Villain, DFL-H128 and DFaker models to FaceSwap, as well as significantly improving many areas of the code.\n\n**Bitcoin:** bc1qpm22suz59ylzk0j7qk5e4c7cnkjmve2rmtrnc6\n\n**Ethereum:** 0xd3e954dC241B87C4E8E1A801ada485DC1d530F01\n\n**Monero:** 45dLrtQZ2pkHizBpt3P3yyJKkhcFHnhfNYPMSnz3yVEbdWm3Hj6Kr5TgmGAn3Far8LVaQf1th2n3DJVTRkfeB5ZkHxWozSX\n\n**Paypal:** [![torzdf](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=JZ8PP3YE9J62L)\n\n### @andenixa\nCreator of the Unbalanced and OHR models, as well as expanding various capabilities within the training process. Andenixa is currently working on new models and will take requests for donations.\n\n**Paypal:** [![andenixa](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=NRVLQYGS6NWTU)\n\n# How to contribute\n\n## For people interested in the generative models\n - Go to the 'faceswap-model' to discuss/suggest/commit alternatives to the current algorithm.\n\n## For devs\n - Read this README entirely\n - Fork the repo\n - Play with it\n - Check issues with the 'dev' tag\n - For devs more interested in computer vision and openCV, look at issues with the 'opencv' tag. Also feel free to add your own alternatives/improvements\n\n## For non-dev advanced users\n - Read this README entirely\n - Clone the repo\n - Play with it\n - Check issues with the 'advuser' tag\n - Also go to the '[faceswap Forum](https://faceswap.dev/forum)' and help others.\n\n## For end-users\n - Get the code here and play with it if you can\n - You can also go to the [faceswap Forum](https://faceswap.dev/forum) and help or get help from others.\n - Be patient. This is a relatively new technology for developers as well. Much effort is already being put into making this program easy to use for the average user. It just takes time!\n - **Notice** Any issue related to running the code has to be opened in the [faceswap Forum](https://faceswap.dev/forum)!\n\n# About machine learning\n\n## How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?\nIt's complicated. Here's a good video that makes the process understandable:\n[![How Machines Learn](https://img.youtube.com/vi/R9OHn5ZF4Uo/0.jpg)](https://www.youtube.com/watch?v=R9OHn5ZF4Uo)\n\nHere's a slightly more in depth video that tries to explain the basic functioning of a neural network:\n[![How Machines Learn](https://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n\ntl;dr: training data + trial and error\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":54778,\"forks\":13428,\"watchers\":54778,\"open_issues\":45,\"topics\":[\"deep-face-swap\",\"deep-learning\",\"deep-neural-networks\",\"deepface\",\"deepfakes\",\"deeplearning\",\"face-swap\",\"faceswap\",\"fakeapp\",\"machine-learning\",\"myfakeapp\",\"neural-nets\",\"neural-networks\",\"openfaceswap\"],\"default_branch\":\"master\",\"size_kb\":203532,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepfakes:faceswap\",\"source_url\":\"https://github.com/deepfakes/faceswap\"}]",
    "canonical_id": null,
    "license_spdx": "GPL-3.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e3d50e138153c437d24c77a13c2a2786",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/deepfakes/faceswap\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:ultralytics:ultralytics",
    "name": "ultralytics",
    "author": "ultralytics",
    "description": "<div align=\"center\"> <p> <a href=\"https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event\" target=\"_blank\"> <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"Ultralytics YOLO banner\"></a> </p> 中文 | 한국어 | 日本語 | Русский | Deutsch | Français | Español | Português | Türkçe | Tiếng Việt | العربية <br> <div> <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img s...",
    "tags": [
      "cli",
      "computer-vision",
      "deep-learning",
      "hub",
      "image-classification",
      "instance-segmentation",
      "machine-learning",
      "object-detection",
      "pose-estimation",
      "python",
      "pytorch",
      "rotated-object-detection",
      "segment-anything",
      "tracking",
      "ultralytics",
      "yolo",
      "yolo-world",
      "yolo11",
      "yolo26",
      "yolov8",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 49718,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/ultralytics/ultralytics",
    "image_url": null,
    "type": "tool",
    "body_content": "<div align=\"center\">\n  <p>\n    <a href=\"https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event\" target=\"_blank\">\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"Ultralytics YOLO banner\"></a>\n  </p>\n\n[中文](https://docs.ultralytics.com/zh/) | [한국어](https://docs.ultralytics.com/ko/) | [日本語](https://docs.ultralytics.com/ja/) | [Русский](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Français](https://docs.ultralytics.com/fr/) | [Español](https://docs.ultralytics.com/es) | [Português](https://docs.ultralytics.com/pt/) | [Türkçe](https://docs.ultralytics.com/tr/) | [Tiếng Việt](https://docs.ultralytics.com/vi/) | [العربية](https://docs.ultralytics.com/ar/) <br>\n\n<div>\n    <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n    <a href=\"https://clickpy.clickhouse.com/dashboard/ultralytics\"><img src=\"https://static.pepy.tech/badge/ultralytics\" alt=\"Ultralytics Downloads\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"Ultralytics YOLO Citation\"></a>\n    <a href=\"https://discord.com/invite/ultralytics\"><img alt=\"Ultralytics Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n    <a href=\"https://community.ultralytics.com/\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n    <a href=\"https://www.reddit.com/r/ultralytics/\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n    <br>\n    <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run Ultralytics on Gradient\"></a>\n    <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Ultralytics In Colab\"></a>\n    <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open Ultralytics In Kaggle\"></a>\n    <a href=\"https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open Ultralytics In Binder\"></a>\n</div>\n</div>\n<br>\n\n[Ultralytics](https://www.ultralytics.com/) creates cutting-edge, state-of-the-art (SOTA) [YOLO models](https://www.ultralytics.com/yolo) built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are **fast**, **accurate**, and **easy to use**. They excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/) tasks.\n\nFind detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!\n\nRequest an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<a href=\"https://docs.ultralytics.com/models/yolo11/\" target=\"_blank\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png\" alt=\"YOLO11 performance plots\">\n</a>\n\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"2%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"2%\" alt=\"Ultralytics Discord\"></a>\n</div>\n\n## 📄 Documentation\n\nSee below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).\n\n<details open>\n<summary>Install</summary>\n\nInstall the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)\n\n```bash\npip install ultralytics\n```\n\nFor alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).\n\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics) [![Ultralytics Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)\n\n</details>\n\n<details open>\n<summary>Usage</summary>\n\n### CLI\n\nYou can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:\n\n```bash\n# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\nThe `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.\n\n### Python\n\nUltralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:\n\n```python\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the COCO8 dataset for 100 epochs\ntrain_results = model.train(\n    data=\"coco8.yaml\",  # Path to dataset configuration file\n    epochs=100,  # Number of training epochs\n    imgsz=640,  # Image size for training\n    device=\"cpu\",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])\n)\n\n# Evaluate the model's performance on the validation set\nmetrics = model.val()\n\n# Perform object detection on an image\nresults = model(\"path/to/image.jpg\")  # Predict on an image\nresults[0].show()  # Display results\n\n# Export the model to ONNX format for deployment\npath = model.export(format=\"onnx\")  # Returns the path to the exported model\n```\n\nDiscover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).\n\n</details>\n\n## ✨ Models\n\nUltralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO11](https://docs.ultralytics.com/models/yolo11/). The tables below showcase YOLO11 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.\n\n<a href=\"https://docs.ultralytics.com/tasks/\" target=\"_blank\">\n    <img width=\"100%\" src=\"https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif\" alt=\"Ultralytics YOLO supported tasks\">\n</a>\n<br>\n<br>\n\n<details open><summary>Detection (COCO)</summary>\n\nExplore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.\n\n| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640                   | 39.5                 | 56.1 ± 0.8                     | 1.5 ± 0.0                           | 2.6                | 6.5               |\n| [YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt) | 640                   | 47.0                 | 90.0 ± 1.2                     | 2.5 ± 0.0                           | 9.4                | 21.5              |\n| [YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) | 640                   | 51.5                 | 183.2 ± 2.0                    | 4.7 ± 0.1                           | 20.1               | 68.0              |\n| [YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt) | 640                   | 53.4                 | 238.6 ± 1.4                    | 6.2 ± 0.1                           | 25.3               | 86.9              |\n| [YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt) | 640                   | 54.7                 | 462.8 ± 6.7                    | 11.3 ± 0.2                          | 56.9               | 194.9             |\n\n- **mAP<sup>val</sup>** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val detect data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Segmentation (COCO)</summary>\n\nRefer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt) | 640                   | 38.9                 | 32.0                  | 65.9 ± 1.1                     | 1.8 ± 0.0                           | 2.9                | 9.7               |\n| [YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt) | 640                   | 46.6                 | 37.8                  | 117.6 ± 4.9                    | 2.9 ± 0.0                           | 10.1               | 33.0              |\n| [YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt) | 640                   | 51.5                 | 41.5                  | 281.6 ± 1.2                    | 6.3 ± 0.1                           | 22.4               | 113.2             |\n| [YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt) | 640                   | 53.4                 | 42.9                  | 344.2 ± 3.2                    | 7.8 ± 0.2                           | 27.6               | 132.2             |\n| [YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt) | 640                   | 54.7                 | 43.8                  | 664.5 ± 3.2                    | 15.8 ± 0.7                          | 62.1               | 296.4             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val segment data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Classification (ImageNet)</summary>\n\nConsult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 224 |\n| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n| [YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt) | 224                   | 70.0             | 89.4             | 5.0 ± 0.3                      | 1.1 ± 0.0                           | 2.8                | 0.5                      |\n| [YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt) | 224                   | 75.4             | 92.7             | 7.9 ± 0.2                      | 1.3 ± 0.0                           | 6.7                | 1.6                      |\n| [YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt) | 224                   | 77.3             | 93.9             | 17.2 ± 0.4                     | 2.0 ± 0.0                           | 11.6               | 4.9                      |\n| [YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt) | 224                   | 78.3             | 94.3             | 23.2 ± 0.3                     | 2.8 ± 0.0                           | 14.1               | 6.2                      |\n| [YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt) | 224                   | 79.5             | 94.9             | 41.4 ± 0.9                     | 3.8 ± 0.0                           | 29.6               | 13.6                     |\n\n- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. <br>Reproduce with `yolo val classify data=path/to/ImageNet device=0`\n- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Pose (COCO)</summary>\n\nSee the [Pose Estimation Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples. These models are trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), focusing on the 'person' class.\n\n| Model                                                                                          | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 ± 0.5                     | 1.7 ± 0.0                           | 2.9                | 7.4               |\n| [YOLO11s-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt) | 640                   | 58.9                  | 86.3               | 90.5 ± 0.6                     | 2.6 ± 0.0                           | 9.9                | 23.1              |\n| [YOLO11m-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt) | 640                   | 64.9                  | 89.4               | 187.3 ± 0.8                    | 4.9 ± 0.1                           | 20.9               | 71.4              |\n| [YOLO11l-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt) | 640                   | 66.1                  | 89.9               | 247.7 ± 1.1                    | 6.4 ± 0.1                           | 26.1               | 90.3              |\n| [YOLO11x-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt) | 640                   | 69.5                  | 91.1               | 488.0 ± 13.9                   | 12.1 ± 0.2                          | 58.8               | 202.8             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on the [COCO Keypoints val2017](https://docs.ultralytics.com/datasets/pose/coco/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val pose data=coco-pose.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val pose data=coco-pose.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Oriented Bounding Boxes (DOTAv1)</summary>\n\nCheck the [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for usage examples. These models are trained on [DOTAv1](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/), including 15 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>test<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt) | 1024                  | 78.4               | 117.6 ± 0.8                    | 4.4 ± 0.0                           | 2.7                | 16.8              |\n| [YOLO11s-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt) | 1024                  | 79.5               | 219.4 ± 4.0                    | 5.1 ± 0.0                           | 9.7                | 57.1              |\n| [YOLO11m-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt) | 1024                  | 80.9               | 562.8 ± 2.9                    | 10.1 ± 0.4                          | 20.9               | 182.8             |\n| [YOLO11l-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt) | 1024                  | 81.0               | 712.5 ± 5.0                    | 13.5 ± 0.6                          | 26.1               | 231.2             |\n| [YOLO11x-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt) | 1024                  | 81.3               | 1408.6 ± 7.7                   | 28.6 ± 1.0                          | 58.8               | 519.1             |\n\n- **mAP<sup>test</sup>** values are for single-model multiscale performance on the [DOTAv1 test set](https://captain-whu.github.io/DOTA/dataset.html). <br>Reproduce by `yolo val obb data=DOTAv1.yaml device=0 split=test` and submit merged results to the [DOTA evaluation server](https://captain-whu.github.io/DOTA/evaluation.html).\n- **Speed** metrics are averaged over [DOTAv1 val images](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10) using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce by `yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu`\n\n</details>\n\n## 🧩 Integrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\n<a href=\"https://docs.ultralytics.com/integrations/\" target=\"_blank\">\n    <img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\" alt=\"Ultralytics active learning integrations\">\n</a>\n<br>\n<br>\n\n<div align=\"center\">\n  <a href=\"https://www.ultralytics.com/hub\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png\" width=\"10%\" alt=\"Ultralytics HUB logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/weights-biases/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png\" width=\"10%\" alt=\"Weights & Biases logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/comet/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"10%\" alt=\"Comet ML logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/neural-magic/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"10%\" alt=\"Neural Magic logo\"></a>\n</div>\n\n|                                                       Ultralytics HUB 🌟                                                        |                                                          Weights & Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |\n| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |\n| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |\n\n## 🌟 Ultralytics HUB\n\nExperience seamless AI with [Ultralytics HUB](https://hub.ultralytics.com/), the all-in-one platform for data visualization, training YOLO models, and deployment—no coding required. Transform images into actionable insights and bring your AI visions to life effortlessly using our cutting-edge platform and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\n<a href=\"https://www.ultralytics.com/hub\" target=\"_blank\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\" alt=\"Ultralytics HUB preview image\"></a>\n\n## 🤝 Contribute\n\nWe thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. We also welcome your feedback—share your experience by completing our [Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). A huge **Thank You** 🙏 to everyone who contributes!\n\n<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 -->\n\n[![Ultralytics open-source contributors](https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png)](https://github.com/ultralytics/ultralytics/graphs/contributors)\n\nWe look forward to your contributions to help make the Ultralytics ecosystem even better!\n\n## 📜 License\n\nUltralytics offers two licensing options to suit different needs:\n\n- **AGPL-3.0 License**: This [OSI-approved](https://opensource.org/license/agpl-v3) open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for full details.\n- **Ultralytics Enterprise License**: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## 📞 Contact\n\nFor bug reports and feature requests related to Ultralytics software, please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues). For questions, discussions, and community support, join our active communities on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/). We're here to help with all things Ultralytics!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"3%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"3%\" alt=\"Ultralytics Discord\"></a>\n</div>\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":49718,\"forks\":9609,\"watchers\":49718,\"open_issues\":301,\"topics\":[\"cli\",\"computer-vision\",\"deep-learning\",\"hub\",\"image-classification\",\"instance-segmentation\",\"machine-learning\",\"object-detection\",\"pose-estimation\",\"python\",\"pytorch\",\"rotated-object-detection\",\"segment-anything\",\"tracking\",\"ultralytics\",\"yolo\",\"yolo-world\",\"yolo11\",\"yolo26\",\"yolov8\"],\"default_branch\":\"main\",\"size_kb\":42181,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:docs\",\"source_url\":\"https://github.com/ultralytics/docs\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:ultralytics\",\"source_url\":\"https://github.com/ultralytics/ultralytics\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"},{\"type\":\"has_code\",\"target_id\":\"github:ultralytics:assets\",\"source_url\":\"https://github.com/ultralytics/assets\"}]",
    "canonical_id": null,
    "license_spdx": "AGPL-3.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "acb4c75428b951472e29f9a0e2f3e609",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/ultralytics/ultralytics\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:avik-jain:100-days-of-ml-code",
    "name": "100-Days-Of-ML-Code",
    "author": "Avik-Jain",
    "description": "100 Days of Machine Learning Coding as proposed by Siraj Raval Get the datasets from here Check out the code from here. <p align=\"center\"> <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg\"> </p> Check out the code from here. <p align=\"center\"> <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg\"> </p> Check out the code from here. <p align=\"center\"> <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Co...",
    "tags": [
      "100-days-of-code-log",
      "100daysofcode",
      "deep-learning",
      "implementation",
      "infographics",
      "linear-algebra",
      "linear-regression",
      "logistic-regression",
      "machine-learning",
      "machine-learning-algorithms",
      "naive-bayes-classifier",
      "python",
      "scikit-learn",
      "siraj-raval",
      "siraj-raval-challenge",
      "support-vector-machines",
      "svm",
      "tutorial"
    ],
    "pipeline_tag": "other",
    "likes": 48980,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/Avik-Jain/100-Days-Of-ML-Code",
    "image_url": null,
    "type": "tool",
    "body_content": "# 100-Days-Of-ML-Code\n\n100 Days of Machine Learning Coding as proposed by [Siraj Raval](https://github.com/llSourcell)\n\nGet the datasets from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/tree/master/datasets)\n\n## Data PreProcessing | Day 1\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data%20PreProcessing.md).\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg\">\n</p>\n\n## Simple Linear Regression | Day 2\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md).\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg\">\n</p>\n\n## Multiple Linear Regression | Day 3\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md).\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.jpg\">\n</p>\n\n## Logistic Regression | Day 4\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%204.jpg\">\n</p>\n\n## Logistic Regression | Day 5\nMoving forward into #100DaysOfMLCode today I dived into the deeper depth of what Logistic Regression actually is and what is the math involved behind it. Learned how cost function is calculated and then how to apply gradient descent algorithm to cost function to minimize the error in prediction.  \nDue to less time I will now be posting an infographic on alternate days.\nAlso if someone wants to help me out in documentaion of code and already has some experince in the field and knows Markdown for github please contact me on LinkedIn :) .\n\n## Implementing Logistic Regression | Day 6\nCheck out the Code [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%206%20Logistic%20Regression.md)\n\n## K Nearest Neighbours | Day 7\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%207.jpg\">\n</p>\n\n## Math Behind Logistic Regression | Day 8 \n\n#100DaysOfMLCode To clear my insights on logistic regression I was searching on the internet for some resource or article and I came across this article (https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) by Saishruthi Swaminathan. \n\nIt gives a detailed description of Logistic Regression. Do check it out.\n\n## Support Vector Machines | Day 9\nGot an intution on what SVM is and how it is used to solve Classification problem.\n\n## SVM and KNN | Day 10\nLearned more about how SVM works and implementing the K-NN algorithm.\n\n## Implementation of K-NN | Day 11  \n\nImplemented the K-NN algorithm for classification. #100DaysOfMLCode \nSupport Vector Machine Infographic is halfway complete. Will update it tomorrow.\n\n## Support Vector Machines | Day 12\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2012.jpg\">\n</p>\n\n## Naive Bayes Classifier | Day 13\n\nContinuing with #100DaysOfMLCode today I went through the Naive Bayes classifier.\nI am also implementing the SVM in python using scikit-learn. Will update the code soon.\n\n## Implementation of SVM | Day 14\nToday I implemented SVM on linearly related data. Used Scikit-Learn library. In Scikit-Learn we have SVC classifier which we use to achieve this task. Will be using kernel-trick on next implementation.\nCheck the code [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2013%20SVM.md).\n\n## Naive Bayes Classifier and Black Box Machine Learning | Day 15\nLearned about different types of naive bayes classifiers. Also started the lectures by [Bloomberg](https://bloomberg.github.io/foml/#home). First one in the playlist was Black Box Machine Learning. It gives the whole overview about prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.\n\n## Implemented SVM using Kernel Trick | Day 16\nUsing Scikit-Learn library implemented SVM algorithm along with kernel function which maps our data points into higher dimension to find optimal hyperplane. \n\n## Started Deep learning Specialization on Coursera | Day 17\nCompleted the whole Week 1 and Week 2 on a single day. Learned Logistic regression as Neural Network. \n\n## Deep learning Specialization on Coursera | Day 18\nCompleted the Course 1 of the deep learning specialization. Implemented a neural net in python.\n\n## The Learning Problem , Professor Yaser Abu-Mostafa | Day 19\nStarted Lecture 1 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. It was basically an introduction to the upcoming lectures. He also explained Perceptron Algorithm.\n\n## Started Deep learning Specialization Course 2 | Day 20\nCompleted the Week 1 of Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.\n\n## Web Scraping | Day 21\nWatched some tutorials on how to do web scraping using Beautiful Soup in order to collect data for building a model.\n\n## Is Learning Feasible? | Day 22\nLecture 2 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. Learned about Hoeffding Inequality.\n\n## Decision Trees | Day 23\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2023.jpg\">\n</p>\n\n## Introduction To Statistical Learning Theory | Day 24\nLec 3 of Bloomberg ML course introduced some of the core concepts like input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces.\n\n## Implementing Decision Trees | Day 25\nCheck the code [here.](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2025%20Decision%20Tree.md)\n\n## Jumped To Brush up Linear Algebra | Day 26\nFound an amazing [channel](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) on youtube 3Blue1Brown. It has a playlist called Essence of Linear Algebra. Started off by completing 4 videos which gave a complete overview of Vectors, Linear Combinations, Spans, Basis Vectors, Linear Transformations and Matrix Multiplication. \n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Jumped To Brush up Linear Algebra | Day 27\nContinuing with the playlist completed next 4 videos discussing topics 3D Transformations, Determinants, Inverse Matrix, Column Space, Null Space and Non-Square Matrices.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Jumped To Brush up Linear Algebra | Day 28\nIn the playlist of 3Blue1Brown completed another 3 videos from the essence of linear algebra. \nTopics covered were Dot Product and Cross Product.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n\n## Jumped To Brush up Linear Algebra | Day 29\nCompleted the whole playlist today, videos 12-14. Really an amazing playlist to refresh the concepts of Linear Algebra.\nTopics covered were the change of basis, Eigenvectors and Eigenvalues, and Abstract Vector Spaces.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Essence of calculus | Day 30\nCompleting the playlist - Essence of Linear Algebra by 3blue1brown a suggestion popped up by youtube regarding a series of videos again by the same channel 3Blue1Brown. Being already impressed by the previous series on Linear algebra I dived straight into it.\nCompleted about 5 videos on topics such as Derivatives, Chain Rule, Product Rule, and derivative of exponential.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Essence of calculus | Day 31\nWatched 2 Videos on topic Implicit Diffrentiation and Limits from the playlist Essence of Calculus.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Essence of calculus | Day 32\nWatched the remaining 4 videos covering topics Like Integration and Higher order derivatives.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Random Forests | Day 33\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2033.jpg\">\n</p>\n\n## Implementing Random Forests | Day 34\nCheck the code [here.](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2034%20Random_Forest.md)\n\n## But what *is* a Neural Network? | Deep learning, chapter 1  | Day 35\nAn Amazing Video on neural networks by 3Blue1Brown youtube channel. This video gives a good understanding of Neural Networks and uses Handwritten digit dataset to explain the concept. \nLink To the [video.](https://www.youtube.com/watch?v=aircAruvnKk&t=7s)\n\n## Gradient descent, how neural networks learn | Deep learning, chapter 2 | Day 36\nPart two of neural networks by 3Blue1Brown youtube channel. This video explains the concepts of Gradient Descent in an interesting way. 169 must watch and highly recommended.\nLink To the [video.](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n\n## What is backpropagation really doing? | Deep learning, chapter 3 | Day 37\nPart three of neural networks by 3Blue1Brown youtube channel. This video mostly discusses the partial derivatives and backpropagation.\nLink To the [video.](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n\n## Backpropagation calculus | Deep learning, chapter 4 | Day 38\nPart four of neural networks by 3Blue1Brown youtube channel. The goal here is to represent, in somewhat more formal terms, the intuition for how backpropagation works and the video moslty discusses the partial derivatives and backpropagation.\nLink To the [video.](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n\n## Deep Learning with Python, TensorFlow, and Keras tutorial | Day 39\nLink To the [video.](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\n\n## Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2 | Day 40\nLink To the [video.](https://www.youtube.com/watch?v=j-3vuBynnOE&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=2)\n\n## Convolutional Neural Networks - Deep Learning basics with Python, TensorFlow and Keras p.3 | Day 41\nLink To the [video.](https://www.youtube.com/watch?v=WvoLTXIjBYU&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=3)\n\n## Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4 | Day 42\nLink To the [video.](https://www.youtube.com/watch?v=BqgTU7_cBnk&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=4)\n\n## K Means Clustering | Day 43\nMoved to Unsupervised Learning and studied about Clustering.\nWorking on my website check it out [avikjain.me](http://www.avikjain.me/)\nAlso found a wonderful animation that can help to easily understand K - Means Clustering [Link](http://shabal.in/visuals/kmeans/6.html)\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2043.jpg\">\n</p>\n\n## K Means Clustering Implementation | Day 44\nImplemented K Means Clustering. Check the code [here.]()\n\n## Digging Deeper | NUMPY  | Day 45\nGot a new book \"Python Data Science HandBook\" by JK VanderPlas Check the Jupyter notebooks [here.](https://github.com/jakevdp/PythonDataScienceHandbook)\n<br>Started with chapter 2 : Introduction to Numpy. Covered topics like Data Types, Numpy arrays and Computations on Numpy arrays.\n<br>Check the code - \n<br>[Introduction to NumPy](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb)\n<br>[Understanding Data Types in Python](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb)\n<br>[The Basics of NumPy Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n<br>[Computation on NumPy Arrays: Universal Functions](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.03-Computation-on-arrays-ufuncs.ipynb)\n\n## Digging Deeper | NUMPY | Day 46\nChapter 2 : Aggregations, Comparisions and Broadcasting\n<br>Link to Notebook:\n<br>[Aggregations: Min, Max, and Everything In Between](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.04-Computation-on-arrays-aggregates.ipynb)\n<br>[Computation on Arrays: Broadcasting](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.05-Computation-on-arrays-broadcasting.ipynb)\n<br>[Comparisons, Masks, and Boolean Logic](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.06-Boolean-Arrays-and-Masks.ipynb)\n\n## Digging Deeper | NUMPY | Day 47\nChapter 2 : Fancy Indexing, sorting arrays, Struchered Data\n<br>Link to Notebook:\n<br>[Fancy Indexing](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.07-Fancy-Indexing.ipynb)\n<br>[Sorting Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)\n<br>[Structured Data: NumPy's Structured Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.09-<br>Structured-Data-NumPy.ipynb)\n\n## Digging Deeper | PANDAS | Day 48\nChapter 3 : Data Manipulation with Pandas\n<br> Covered Various topics like Pandas Objects, Data Indexing and Selection, Operating on Data, Handling Missing Data, Hierarchical Indexing, ConCat and Append.\n<br>Link To the Notebooks:\n<br>[Data Manipulation with Pandas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)\n<br>[Introducing Pandas Objects](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb)\n<br>[Data Indexing and Selection](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02-Data-Indexing-and-Selection.ipynb)\n<br>[Operating on Data in Pandas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.03-Operations-in-Pandas.ipynb)\n<br>[Handling Missing Data](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb)\n<br>[Hierarchical Indexing](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb)\n<br>[Combining Datasets: Concat and Append](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb)\n\n## Digging Deeper | PANDAS | Day 49\nChapter 3: Completed following topics- Merge and Join, Aggregation and grouping and Pivot Tables.\n<br>[Combining Datasets: Merge and Join](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb)\n<br>[Aggregation and Grouping](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb)\n<br>[Pivot Tables](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb)\n\n## Digging Deeper | PANDAS | Day 50\nChapter 3: Vectorized Strings Operations, Working with Time Series\n<br>Links to Notebooks:\n<br>[Vectorized String Operations](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb)\n<br>[Working with Time Series](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb)\n<br>[High-Performance Pandas: eval() and query()](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 51\nChapter 4: Visualization with Matplotlib \nLearned about Simple Line Plots, Simple Scatter Plotsand Density and Contour Plots.\n<br>Links to Notebooks: \n<br>[Visualization with Matplotlib](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n<br>[Simple Line Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb)\n<br>[Simple Scatter Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb)\n<br>[Visualizing Errors](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.03-Errorbars.ipynb)\n<br>[Density and Contour Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 52\nChapter 4: Visualization with Matplotlib \nLearned about Histograms, How to customize plot legends, colorbars, and buliding Multiple Subplots.\n<br>Links to Notebooks: \n<br>[Histograms, Binnings, and Density](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.05-Histograms-and-Binnings.ipynb)\n<br>[Customizing Plot Legends](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.06-Customizing-Legends.ipynb)\n<br>[Customizing Colorbars](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.07-Customizing-Colorbars.ipynb)\n<br>[Multiple Subplots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.08-Multiple-Subplots.ipynb)\n<br>[Text and Annotation](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 53\nChapter 4: Covered Three Dimensional Plotting in Mathplotlib.\n<br>Links to Notebooks:\n<br>[Three-Dimensional Plotting in Matplotlib](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb)\n\n## Hierarchical Clustering | Day 54\nStudied about Hierarchical Clustering.\nCheck out this amazing [Visualization.](https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif)\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2054.jpg\">\n</p>\n",
    "meta_json": "{\"language\":null,\"stars\":48980,\"forks\":11244,\"watchers\":48980,\"open_issues\":66,\"topics\":[\"100-days-of-code-log\",\"100daysofcode\",\"deep-learning\",\"implementation\",\"infographics\",\"linear-algebra\",\"linear-regression\",\"logistic-regression\",\"machine-learning\",\"machine-learning-algorithms\",\"naive-bayes-classifier\",\"python\",\"scikit-learn\",\"siraj-raval\",\"siraj-raval-challenge\",\"support-vector-machines\",\"svm\",\"tutorial\"],\"default_branch\":\"master\",\"size_kb\":10955,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:jakevdp:PythonDataScienceHandbook\",\"source_url\":\"https://github.com/jakevdp/PythonDataScienceHandbook\"},{\"type\":\"has_code\",\"target_id\":\"github:Avik-Jain:100-Days-Of-ML-Code\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "7669026bc307fe862c07d57f7c9b37a2",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/Avik-Jain/100-Days-Of-ML-Code\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:julialang:julia",
    "name": "julia",
    "author": "JuliaLang",
    "description": "<a name=\"logo\"/> <div align=\"center\"> <a href=\"https://julialang.org/\" target=\"_blank\"> <img src=\"doc/src/assets/logo.svg\" alt=\"Julia Logo\" width=\"210\" height=\"142\"></img> </a> </div> <table> <!-- Docs --> <tr> <td>Documentation</td> <td> <a href=\"https://docs.julialang.org\"><img src='https://img.shields.io/badge/docs-v1-blue.svg'/></a> </td> </tr> <!-- Continuous integration To change the badge to point to a different pipeline, it is not sufficient to simply change the part. You need to go t...",
    "tags": [
      "hacktoberfest",
      "hpc",
      "julia",
      "julia-language",
      "julialang",
      "machine-learning",
      "numerical",
      "programming-language",
      "science",
      "scientific",
      "julia"
    ],
    "pipeline_tag": "other",
    "likes": 48065,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/JuliaLang/julia",
    "image_url": null,
    "type": "tool",
    "body_content": "<a name=\"logo\"/>\n<div align=\"center\">\n<a href=\"https://julialang.org/\" target=\"_blank\">\n<img src=\"doc/src/assets/logo.svg\" alt=\"Julia Logo\" width=\"210\" height=\"142\"></img>\n</a>\n</div>\n\n<table>\n    <!-- Docs -->\n    <tr>\n        <td>Documentation</td>\n        <td>\n            <a href=\"https://docs.julialang.org\"><img src='https://img.shields.io/badge/docs-v1-blue.svg'/></a>\n        </td>\n    </tr>\n    <!-- Continuous integration\n    To change the badge to point to a different pipeline, it is not sufficient to simply change the `?branch=` part.\n    You need to go to the Buildkite website and get the SVG URL for the correct pipeline. -->\n    <tr>\n        <td>Continuous integration</td>\n        <td>\n            <a href=\"https://buildkite.com/julialang/julia-master\"><img src='https://badge.buildkite.com/f28e0d28b345f9fad5856ce6a8d64fffc7c70df8f4f2685cd8.svg?branch=master'/></a>\n        </td>\n    </tr>\n    <!-- Coverage -->\n    <tr>\n        <td>Code coverage</td>\n        <td>\n            <a href='https://coveralls.io/github/JuliaLang/julia?branch=master'><img src='https://coveralls.io/repos/github/JuliaLang/julia/badge.svg?branch=master' alt='Coverage Status'/></a>\n            <a href=\"https://codecov.io/gh/JuliaLang/julia\"><img src=\"https://codecov.io/gh/JuliaLang/julia/branch/master/graph/badge.svg?token=TckCRxc7HS\"/></a>\n        </td>\n    </tr>\n</table>\n\n## The Julia Language\n\nJulia is a high-level, high-performance dynamic language for technical\ncomputing. The main homepage for Julia can be found at\n[julialang.org](https://julialang.org/). This is the GitHub\nrepository of Julia source code, including instructions for compiling\nand installing Julia, below.\n\n## Resources\n\n- **Homepage:** <https://julialang.org>\n- **Install:** <https://julialang.org/downloads/>\n- **Source code:** <https://github.com/JuliaLang/julia>\n- **Documentation:** <https://docs.julialang.org>\n- **Packages:** <https://julialang.org/packages/>\n- **Discussion forum:** <https://discourse.julialang.org>\n- **Zulip:** <https://julialang.zulipchat.com/>\n- **Slack:** <https://julialang.slack.com> (get an invite from <https://julialang.org/slack/>)\n- **YouTube:** <https://www.youtube.com/user/JuliaLanguage>\n- **Code coverage:** <https://coveralls.io/r/JuliaLang/julia>\n\nNew developers may find the notes in\n[CONTRIBUTING](https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md)\nhelpful to start contributing to the Julia codebase.\n\n### Learning Julia\n\n- [**Learning resources**](https://julialang.org/learning/)\n\n## Binary Installation\n\nThe recommended way of installing Julia is to use `juliaup` which will install\nthe latest stable `julia` for you and help keep it up to date. It can also let\nyou install and run different Julia versions simultaneously. Instructions for\nthis can be found [here](https://julialang.org/downloads/). If you want to manually\ndownload specific Julia binaries, you can find those on the [Manual Downloads\npage](https://julialang.org/downloads/manual-downloads/). The downloads page also provides\ndetails on the [different tiers of\nsupport](https://julialang.org/downloads/support) for OS and\nplatform combinations.\n\nIf everything works correctly, you will get a `julia` program and when you run\nit in a terminal or command prompt, you will see a Julia banner and an\ninteractive prompt into which you can enter expressions for evaluation. You can\nread about [getting\nstarted](https://docs.julialang.org/en/v1/manual/getting-started/) in the\nmanual.\n\n**Note**: Although some OS package managers provide Julia, such\ninstallations are neither maintained nor endorsed by the Julia\nproject. They may be outdated, broken and/or unmaintained. We\nrecommend you use the official Julia binaries instead.\n\n## Building Julia\n\nFirst, make sure you have all the [required\ndependencies](https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/build.md#required-build-tools-and-external-libraries) installed.\nThen, acquire the source code by cloning the git repository:\n\n    git clone https://github.com/JuliaLang/julia.git\n\nand then use the command prompt to change into the resulting julia directory. By default, you will be building the latest unstable version of\nJulia. However, most users should use the [most recent stable version](https://github.com/JuliaLang/julia/releases)\nof Julia. You can get this version by running:\n\n    git checkout v1.12.2\n\nTo build the `julia` executable, run `make` from within the julia directory.\n\nBuilding Julia requires 2GiB of disk space and approximately 4GiB of virtual memory.\n\n**Note:** The build process will fail badly if any of the build directory's parent directories have spaces or other shell meta-characters such as `$` or `:` in their names (this is due to a limitation in GNU make).\n\nOnce it is built, you can run the `julia` executable. From within the julia directory, run\n\n    ./julia\n\nYour first test of Julia determines whether your build is working\nproperly. From the julia\ndirectory, type `make testall`. You should see output that\nlists a series of running tests; if they complete without error, you\nshould be in good shape to start using Julia.\n\nYou can read about [getting\nstarted](https://docs.julialang.org/en/v1/manual/getting-started/)\nin the manual.\n\nDetailed build instructions, should they be necessary,\nare included in the [build documentation](https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/build.md).\n\n### Uninstalling Julia\n\nBy default, Julia does not install anything outside the directory it was cloned\ninto and `~/.julia`. Julia and the vast majority of Julia packages can be\ncompletely uninstalled by deleting these two directories.\n\n## Source Code Organization\n\nThe Julia source code is organized as follows:\n\n| Directory         | Contents                                                           |\n| -                 | -                                                                  |\n| `base/`           | source code for the Base module (part of Julia's standard library) |\n| `cli/`            | source for the command line interface/REPL                         |\n| `contrib/`        | miscellaneous scripts                                              |\n| `deps/`           | external dependencies                                              |\n| `doc/src/`        | source for the user manual                                         |\n| `etc/`            | contains `startup.jl`                                              |\n| `src/`            | source for Julia language core                                     |\n| `stdlib/`         | source code for other standard library packages                    |\n| `test/`           | test suites                                                        |\n\n## Terminal, Editors and IDEs\n\nThe Julia REPL is quite powerful. See the section in the manual on\n[the Julia REPL](https://docs.julialang.org/en/v1/stdlib/REPL/)\nfor more details.\n\nOn Windows, we highly recommend running Julia in a modern terminal,\nsuch as [Windows Terminal from the Microsoft Store](https://aka.ms/terminal).\n\nSupport for editing Julia is available for many\n[widely used editors](https://github.com/JuliaEditorSupport):\n[Emacs](https://github.com/JuliaEditorSupport/julia-emacs),\n[Vim](https://github.com/JuliaEditorSupport/julia-vim),\n[Sublime Text](https://github.com/JuliaEditorSupport/Julia-sublime), and many\nothers.\n\nFor users who prefer IDEs, we recommend using VS Code with the\n[julia-vscode](https://www.julia-vscode.org/) plugin.\\\nFor notebook users, [Jupyter](https://jupyter.org/) notebook support is available through the\n[IJulia](https://github.com/JuliaLang/IJulia.jl) package, and\nthe [Pluto.jl](https://github.com/fonsp/Pluto.jl) package provides Pluto notebooks.\n",
    "meta_json": "{\"language\":\"Julia\",\"stars\":48065,\"forks\":5684,\"watchers\":48065,\"open_issues\":4916,\"topics\":[\"hacktoberfest\",\"hpc\",\"julia\",\"julia-language\",\"julialang\",\"machine-learning\",\"numerical\",\"programming-language\",\"science\",\"scientific\"],\"default_branch\":\"master\",\"size_kb\":352038,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:julia>\",\"source_url\":\"https://github.com/JuliaLang/julia>\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:julia\",\"source_url\":\"https://github.com/JuliaLang/julia\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:julia\",\"source_url\":\"https://github.com/JuliaLang/julia\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:julia.git\",\"source_url\":\"https://github.com/JuliaLang/julia.git\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:julia\",\"source_url\":\"https://github.com/JuliaLang/julia\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:julia\",\"source_url\":\"https://github.com/JuliaLang/julia\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaEditorSupport:julia-emacs\",\"source_url\":\"https://github.com/JuliaEditorSupport/julia-emacs\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaEditorSupport:julia-vim\",\"source_url\":\"https://github.com/JuliaEditorSupport/julia-vim\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaEditorSupport:Julia-sublime\",\"source_url\":\"https://github.com/JuliaEditorSupport/Julia-sublime\"},{\"type\":\"has_code\",\"target_id\":\"github:JuliaLang:IJulia.jl\",\"source_url\":\"https://github.com/JuliaLang/IJulia.jl\"},{\"type\":\"has_code\",\"target_id\":\"github:fonsp:Pluto.jl\",\"source_url\":\"https://github.com/fonsp/Pluto.jl\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "2a2e0c8d36a12acf05a00f5c18334795",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/JuliaLang/julia\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:pathwaycom:llm-app",
    "name": "llm-app",
    "author": "pathwaycom",
    "description": "<div align=\"center\"> <a href=\"https://trendshift.io/repositories/4400\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4400\" alt=\"pathwaycom%2Fllm-app | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a> !Linux !macOS </div> Pathway's **AI Pipelines** allow you to quickly put in production AI applications that offer **high-accuracy RAG and AI enterprise search at scale** using the most **up-to-date knowledge** available in your data sources. I...",
    "tags": [
      "chatbot",
      "hugging-face",
      "llm",
      "llm-local",
      "llm-prompting",
      "llm-security",
      "llmops",
      "machine-learning",
      "open-ai",
      "pathway",
      "rag",
      "real-time",
      "retrieval-augmented-generation",
      "vector-database",
      "vector-index",
      "jupyter notebook"
    ],
    "pipeline_tag": "other",
    "likes": 47713,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/pathwaycom/llm-app",
    "image_url": null,
    "type": "tool",
    "body_content": "<div align=\"center\">\n\n# Pathway AI Pipelines\n\n<a href=\"https://trendshift.io/repositories/4400\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4400\" alt=\"pathwaycom%2Fllm-app | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=apple&logoColor=white)\n[![chat on Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/pathway)\n[![follow on X](  https://img.shields.io/badge/X-000000?style=for-the-badge&logo=x&logoColor=white)](https://x.com/intent/follow?screen_name=pathway_com)\n</div>\n\nPathway's **AI Pipelines** allow you to quickly put in production AI applications that offer **high-accuracy RAG and AI enterprise search at scale** using the most **up-to-date knowledge** available in your data sources. It provides you ready-to-deploy **LLM (Large Language Model) App Templates**. You can test them on your own machine and deploy on-cloud (GCP, AWS, Azure, Render,...) or on-premises.\n\nThe apps connect and sync (all new data additions, deletions, updates) with data sources on your **file system, Google Drive, Sharepoint, S3, Kafka, PostgreSQL, real-time data APIs**. They come with no infrastructure dependencies that would need a separate setup. They include **built-in data indexing** enabling vector search, hybrid search, and full-text search - all done in-memory, with cache.\n\n\n## Application Templates\n\nThe application templates provided in this repo scale up to **millions of pages of documents**. Some of them are optimized for simplicity, some are optimized for amazing accuracy. Pick the one that suits you best. You can use it out of the box, or change some steps of the pipeline - for example, if you would like to add a new data source, or change a Vector Index into a Hybrid Index, it's just a one-line change. \n\n| Application (template)                                                                           | Description                                                                                                                                                                                                                                                                                                                                                         |\n| --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Question-Answering RAG App`](templates/question_answering_rag/)    | Basic end-to-end RAG app. A question-answering pipeline that uses the GPT model of choice to provide answers to queries to your documents (PDF, DOCX,...) on a live connected data source (files, Google Drive, Sharepoint,...). You can also try out a [demo REST endpoint](https://pathway.com/solutions/rag-pipelines#try-it-out).              |\n| [`Live Document Indexing (Vector Store / Retriever)`](templates/document_indexing/)     | A real-time document indexing pipeline for RAG that acts as a vector store service. It performs live indexing on your documents (PDF, DOCX,...) from a connected data source (files, Google Drive, Sharepoint,...). It can be used with any frontend, or integrated as a retriever backend for a [Langchain](https://pathway.com/blog/langchain-integration) or [Llamaindex](https://pathway.com/blog/llamaindex-pathway) application. You can also try out a [demo REST endpoint](https://pathway.com/solutions/ai-contract-management#try-it-out).         |\n| [`Multimodal RAG pipeline with GPT4o`](templates/multimodal_rag/) | Multimodal RAG using GPT-4o in the parsing stage to index PDFs and other documents from a connected data source files, Google Drive, Sharepoint,...). It is perfect for extracting information from unstructured financial documents in your folders (including charts and tables), updating results as documents change or new ones arrive.|\n| [`Unstructured-to-SQL pipeline + SQL question-answering`](templates/unstructured_to_sql_on_the_fly/) | A RAG example which connects to unstructured financial data sources (financial report PDFs), structures the data into SQL, and loads it into a PostgreSQL table. It also answers natural language user queries to these financial documents by translating them into SQL using an LLM and executing the query on the PostgreSQL table. |\n| [`Adaptive RAG App`](templates/adaptive_rag/) | A RAG application using Adaptive RAG, a technique developed by Pathway to reduce token cost in RAG up to 4x while maintaining accuracy. |\n| [`Private RAG App with Mistral and Ollama`](templates/private_rag/) |  A fully private (local) version of the `question_answering_rag` RAG pipeline using Pathway, Mistral, and Ollama. |\n| [`Slides AI Search App`](templates/slides_ai_search/)                                        | An indexing pipeline for retrieving slides. It performs multi-modal of PowerPoint and PDF and maintains live index of your slides.\"|\n\n\n## How do these AI Pipelines work?\n\nThe apps can be run as **Docker containers**, and expose an **HTTP API** to connect the frontend. To allow quick testing and demos, some app templates also include an optional Streamlit UI which connects to this API. \n\nThe apps rely on the [Pathway Live Data framework](https://github.com/pathwaycom/pathway) for data source synchronization and for serving API requests (Pathway is a standalone Python library with a Rust engine built into it). They bring you a **simple and unified application logic** for back-end, embedding, retrieval, LLM tech stack. There is no need to integrate and maintain separate modules for your Gen AI app: ~Vector Database (e.g. Pinecone/Weaviate/Qdrant) + Cache (e.g. Redis) + API Framework (e.g. Fast API)~. Pathway's default choice of **built-in vector index** is based on the lightning-fast [usearch](https://github.com/unum-cloud/usearch) library, and **hybrid full-text indexes** make use of [Tantivy](https://github.com/quickwit-oss/tantivy) library. Everything works out of the box.\n\n## Getting started\n\nEach of the [App templates](templates/) in this repo contains a README.md with instructions on how to run it.\n\nYou can also find [more ready-to-run code templates](https://pathway.com/developers/templates/) on the Pathway website.\n\n\n## Some visual highlights\n\nEffortlessly extract and organize table and chart data from PDFs, docs, and more with multimodal RAG - in real-time:\n\n![Effortlessly extract and organize table and chart data from PDFs, docs, and more with multimodal RAG - in real-time](https://github.com/pathwaycom/llm-app/blob/main/templates/multimodal_rag/gpt4o_with_pathway_comparison.gif)\n\n(Check out [`Multimodal RAG pipeline with GPT4o`](templates/multimodal_rag/) to see the whole pipeline in the works. You may also check out the [`Unstructured-to-SQL pipeline`](templates/unstructured_to_sql_on_the_fly/) for a minimal example that works with non-multimodal models as well.)\n\n\nAutomated real-time knowledge mining and alerting:\n\n![Automated real-time knowledge mining and alerting](templates/drive_alert/drive_alert_demo.gif)\n\n(Check out the [`Alerting when answers change on Google Drive`](https://github.com/pathwaycom/llm-app/tree/main/templates/drive_alert) app example.)\n\n\n###  Do-it-Yourself Videos\n\n▶️ [An introduction to building LLM apps with Pathway](https://www.youtube.com/watch?v=kcrJSk00duw) - by [Jan Chorowski](https://scholar.google.com/citations?user=Yc94070AAAAJ)\n\n▶️ [Let's build a real-world LLM app in 11 minutes](https://www.youtube.com/watch?v=k1XGo7ts4tI) - by [Pau Labarta Bajo](https://substack.com/@paulabartabajo)\n\n\n## Troubleshooting\n\nTo provide feedback or report a bug, please [raise an issue on our issue tracker](https://github.com/pathwaycom/pathway/issues).\n\n## Contributing\n\nAnyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. If this is your first contribution to a GitHub project, here is a [Get Started Guide](https://docs.github.com/en/get-started/quickstart/contributing-to-projects). \n\nIf you'd like to make a contribution that needs some more work, just raise your hand on the [Pathway Discord server](https://discord.com/invite/pathway) (#get-help) and let us know what you are planning!\n\n## Supported and maintained by\n\n<p align=\"center\">\n  <a href=\"https://github.com/pathwaycom/\"><img src=\"https://pathway.com/logo-light.svg\" alt=\"Pathway\"/></a>\n</p>\n<p align=\"center\">\n  <a href=\"https://pathway.com/solutions/llm-app\">\n    <img src=\"https://img.shields.io/badge/See%20Pathway's%20offering%20for%20AI%20applications-0000FF\" alt=\"See Pathway's offering for AI applications\"/>\n  </a>\n</p>\n",
    "meta_json": "{\"language\":\"Jupyter Notebook\",\"stars\":47713,\"forks\":1221,\"watchers\":47713,\"open_issues\":6,\"topics\":[\"chatbot\",\"hugging-face\",\"llm\",\"llm-local\",\"llm-prompting\",\"llm-security\",\"llmops\",\"machine-learning\",\"open-ai\",\"pathway\",\"rag\",\"real-time\",\"retrieval-augmented-generation\",\"vector-database\",\"vector-index\"],\"default_branch\":\"main\",\"size_kb\":62508,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:pathwaycom:pathway\",\"source_url\":\"https://github.com/pathwaycom/pathway\"},{\"type\":\"has_code\",\"target_id\":\"github:unum-cloud:usearch\",\"source_url\":\"https://github.com/unum-cloud/usearch\"},{\"type\":\"has_code\",\"target_id\":\"github:quickwit-oss:tantivy\",\"source_url\":\"https://github.com/quickwit-oss/tantivy\"},{\"type\":\"has_code\",\"target_id\":\"github:pathwaycom:llm-app\",\"source_url\":\"https://github.com/pathwaycom/llm-app\"},{\"type\":\"has_code\",\"target_id\":\"github:pathwaycom:llm-app\",\"source_url\":\"https://github.com/pathwaycom/llm-app\"},{\"type\":\"has_code\",\"target_id\":\"github:pathwaycom:pathway\",\"source_url\":\"https://github.com/pathwaycom/pathway\"},{\"type\":\"has_code\",\"target_id\":\"github:pathwaycom:\\\"><img\",\"source_url\":\"https://github.com/pathwaycom/\\\"><img\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "7529ed2b036ba89e86deb0619822e284",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/pathwaycom/llm-app\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:gokumohandas:made-with-ml",
    "name": "Made-With-ML",
    "author": "GokuMohandas",
    "description": "<div align=\"center\"> <h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1> Design · Develop · Deploy · Iterate <br> Join 40K+ developers in learning how to responsibly deliver value with ML. <br> </div> <br> <div align=\"center\"> <a target=\"_blank\" href=\"https://madewithml.com/\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp; <a target=\"_blank\" href=\"https://github.com/GokuMohan...",
    "tags": [
      "data-engineering",
      "data-quality",
      "data-science",
      "deep-learning",
      "distributed-ml",
      "distributed-training",
      "llms",
      "machine-learning",
      "mlops",
      "natural-language-processing",
      "python",
      "pytorch",
      "ray",
      "jupyter notebook"
    ],
    "pipeline_tag": "other",
    "likes": 44722,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/GokuMohandas/Made-With-ML",
    "image_url": null,
    "type": "tool",
    "body_content": "<div align=\"center\">\n<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\nDesign · Develop · Deploy · Iterate\n<br>\nJoin 40K+ developers in learning how to responsibly deliver value with ML.\n    <br>\n</div>\n\n<br>\n\n<div align=\"center\">\n    <a target=\"_blank\" href=\"https://madewithml.com/\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/Made-With-ML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&label=Star\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n    <br>\n    🔥&nbsp; Among the <a href=\"https://github.com/GokuMohandas/Made-With-ML\" target=\"_blank\">top ML repositories</a> on GitHub\n</div>\n\n<br>\n<hr>\n\n## Lessons\n\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production-grade ML applications.\n\n- Lessons: https://madewithml.com/\n- Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n\n<a href=\"https://madewithml.com/#course\">\n  <img src=\"https://madewithml.com/static/images/lessons.png\" alt=\"lessons\">\n</a>\n\n## Overview\n\nIn this course, we'll go from experimentation (design + development) to production (deployment + iteration). We'll do this iteratively by motivating the components that will enable us to build a *reliable* production system.\n\n<blockquote>\n  <img width=20 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/640px-YouTube_full-color_icon_%282017%29.svg.png\">&nbsp; Be sure to watch the video below for a quick overview of what we'll be building.\n</blockquote>\n\n<div align=\"center\">\n  <a href=\"https://youtu.be/AWgkt8H8yVo\"><img src=\"https://img.youtube.com/vi/AWgkt8H8yVo/0.jpg\" alt=\"Course overview video\"></a>\n</div>\n\n<br>\n\n- **💡 First principles**: before we jump straight into the code, we develop a first principles understanding for every machine learning concept.\n- **💻 Best practices**: implement software engineering best practices as we develop and deploy our machine learning models.\n- **📈 Scale**: easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.\n- **⚙️ MLOps**: connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.\n- **🚀 Dev to Prod**: learn how to quickly and reliably go from development to production without any changes to our code or infra management.\n- **🐙 CI/CD**: learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.\n\n## Audience\n\nMachine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.\n\n- **👩‍💻 All developers**: whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you'll be developing.\n- **👩‍🎓 College graduates**: learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.\n- **👩‍💼 Product/Leadership**: who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.\n\n## Set up\n\nBe sure to go through the [course](https://madewithml/#course) for a much more detailed walkthrough of the content on this repository. We will have instructions for both local laptop and Anyscale clusters for the sections below, so be sure to toggle the ► dropdown based on what you're using (Anyscale instructions will be toggled on by default). If you do want to run this course with Anyscale, where we'll provide the **structure**, **compute (GPUs)** and **community** to learn everything in one day, join our next upcoming live cohort → [sign up here](https://4190urw86oh.typeform.com/madewithml)!\n\n### Cluster\n\nWe'll start by setting up our cluster with the environment and compute configurations.\n\n<details>\n  <summary>Local</summary><br>\n  Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes. All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  We can create an [Anyscale Workspace](https://docs.anyscale.com/develop/workspaces/get-started) using the [webpage UI](https://console.anyscale.com/o/madewithml/workspaces/add/blank).\n\n  ```md\n  - Workspace name: `madewithml`\n  - Project: `madewithml`\n  - Cluster environment name: `madewithml-cluster-env`\n  # Toggle `Select from saved configurations`\n  - Compute config: `madewithml-cluster-compute-g5.4xlarge`\n  ```\n\n  > Alternatively, we can use the [CLI](https://docs.anyscale.com/reference/anyscale-cli) to create the workspace via `anyscale workspace create ...`\n\n</details>\n\n<details>\n  <summary>Other (cloud platforms, K8s, on-prem)</summary><br>\n\n  If you don't want to do this course locally or via Anyscale, you have the following options:\n\n  - On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported Azure and Aliyun integrations also exist.\n  - On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), via the officially supported KubeRay project.\n  - Deploy Ray manually [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) or onto platforms [not listed here](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/index.html#ref-cluster-setup).\n\n</details>\n\n### Git setup\n\nCreate a repository by following these instructions: [Create a new repository](https://github.com/new) → name it `Made-With-ML` → Toggle `Add a README file` (**very important** as this creates a `main` branch) → Click `Create repository` (scroll down)\n\nNow we're ready to clone the repository that has all of our code:\n\n```bash\ngit clone https://github.com/GokuMohandas/Made-With-ML.git .\n```\n\n### Credentials\n\n```bash\ntouch .env\n```\n```bash\n# Inside .env\nGITHUB_USERNAME=\"CHANGE_THIS_TO_YOUR_USERNAME\"  # ← CHANGE THIS\n```\n```bash\nsource .env\n```\n\n### Virtual environment\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  python3 -m venv venv  # recommend using Python 3.10\n  source venv/bin/activate  # on Windows: venv\\Scripts\\activate\n  python3 -m pip install --upgrade pip setuptools wheel\n  python3 -m pip install -r requirements.txt\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n  > Highly recommend using Python `3.10` and using [pyenv](https://github.com/pyenv/pyenv) (mac) or [pyenv-win](https://github.com/pyenv-win/pyenv-win) (windows).\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Our environment with the appropriate Python version and libraries is already all set for us through the cluster environment we used when setting up our Anyscale Workspace. So we just need to run these commands:\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n</details>\n\n## Notebook\n\nStart by exploring the [jupyter notebook](notebooks/madewithml.ipynb) to interactively walkthrough the core machine learning workloads.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/systems-design/workloads.png\">\n</div>\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start notebook\n  jupyter lab notebooks/madewithml.ipynb\n```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Click on the Jupyter icon &nbsp;<img width=15 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/1200px-Jupyter_logo.svg.png\">&nbsp; at the top right corner of our Anyscale Workspace page and this will open up our JupyterLab instance in a new tab. Then navigate to the `notebooks` directory and open up the `madewithml.ipynb` notebook.\n\n</details>\n\n\n## Scripts\n\nNow we'll execute the same workloads using the clean Python scripts following software engineering best practices (testing, documentation, logging, serving, versioning, etc.) The code we've implemented in our notebook will be refactored into the following scripts:\n\n```bash\nmadewithml\n├── config.py\n├── data.py\n├── evaluate.py\n├── models.py\n├── predict.py\n├── serve.py\n├── train.py\n├── tune.py\n└── utils.py\n```\n\n**Note**: Change the `--num-workers`, `--cpu-per-worker`, and `--gpu-per-worker` input argument values below based on your system's resources. For example, if you're on a local laptop, a reasonable configuration would be `--num-workers 6 --cpu-per-worker 1 --gpu-per-worker 0`.\n\n### Training\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\npython madewithml/train.py \\\n    --experiment-name \"$EXPERIMENT_NAME\" \\\n    --dataset-loc \"$DATASET_LOC\" \\\n    --train-loop-config \"$TRAIN_LOOP_CONFIG\" \\\n    --num-workers 1 \\\n    --cpu-per-worker 3 \\\n    --gpu-per-worker 1 \\\n    --num-epochs 10 \\\n    --batch-size 256 \\\n    --results-fp results/training_results.json\n```\n\n### Tuning\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\nexport INITIAL_PARAMS=\"[{\\\"train_loop_config\\\": $TRAIN_LOOP_CONFIG}]\"\npython madewithml/tune.py \\\n    --experiment-name \"$EXPERIMENT_NAME\" \\\n    --dataset-loc \"$DATASET_LOC\" \\\n    --initial-params \"$INITIAL_PARAMS\" \\\n    --num-runs 2 \\\n    --num-workers 1 \\\n    --cpu-per-worker 3 \\\n    --gpu-per-worker 1 \\\n    --num-epochs 10 \\\n    --batch-size 256 \\\n    --results-fp results/tuning_results.json\n```\n\n### Experiment tracking\n\nWe'll use [MLflow](https://mlflow.org/) to track our experiments and store our models and the [MLflow Tracking UI](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) to view our experiments. We have been saving our experiments to a local directory but note that in an actual production setting, we would have a central location to store all of our experiments. It's easy/inexpensive to spin up your own MLflow server for all of your team members to track their experiments on or use a managed solution like [Weights & Biases](https://wandb.ai/site), [Comet](https://www.comet.ml/), etc.\n\n```bash\nexport MODEL_REGISTRY=$(python -c \"from madewithml import config; print(config.MODEL_REGISTRY)\")\nmlflow server -h 0.0.0.0 -p 8080 --backend-store-uri $MODEL_REGISTRY\n```\n\n<details>\n  <summary>Local</summary><br>\n\n  If you're running this notebook on your local laptop then head on over to <a href=\"http://localhost:8080/\" target=\"_blank\">http://localhost:8080/</a> to view your MLflow dashboard.\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  If you're on <a href=\"https://docs.anyscale.com/develop/workspaces/get-started\" target=\"_blank\">Anyscale Workspaces</a>, then we need to first expose the port of the MLflow server. Run the following command on your Anyscale Workspace terminal to generate the public URL to your MLflow server.\n\n  ```bash\n  APP_PORT=8080\n  echo https://$APP_PORT-port-$ANYSCALE_SESSION_DOMAIN\n  ```\n\n</details>\n\n### Evaluation\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\nexport HOLDOUT_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\npython madewithml/evaluate.py \\\n    --run-id $RUN_ID \\\n    --dataset-loc $HOLDOUT_LOC \\\n    --results-fp results/evaluation_results.json\n```\n```json\n{\n  \"timestamp\": \"June 09, 2023 09:26:18 AM\",\n  \"run_id\": \"6149e3fec8d24f1492d4a4cabd5c06f6\",\n  \"overall\": {\n    \"precision\": 0.9076136428670714,\n    \"recall\": 0.9057591623036649,\n    \"f1\": 0.9046792827719773,\n    \"num_samples\": 191.0\n  },\n...\n```\n\n### Inference\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/predict.py predict \\\n    --run-id $RUN_ID \\\n    --title \"Transfer learning with transformers\" \\\n    --description \"Using transformers for transfer learning on text classification tasks.\"\n```\n```json\n[{\n  \"prediction\": [\n    \"natural-language-processing\"\n  ],\n  \"probabilities\": {\n    \"computer-vision\": 0.0009767753,\n    \"mlops\": 0.0008223939,\n    \"natural-language-processing\": 0.99762577,\n    \"other\": 0.000575123\n  }\n}]\n```\n\n### Serving\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start\n  ray start --head\n  ```\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME=\"llm\"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = \"Transfer learning with transformers\"\n  description = \"Using transformers for transfer learning on text classification tasks.\"\n  json_data = json.dumps({\"title\": title, \"description\": description})\n  requests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n  ```\n\n  ```bash\n  ray stop  # shutdown\n  ```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  In Anyscale Workspaces, Ray is already running so we don't have to manually start/shutdown like we have to do locally.\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME=\"llm\"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = \"Transfer learning with transformers\"\n  description = \"Using transformers for transfer learning on text classification tasks.\"\n  json_data = json.dumps({\"title\": title, \"description\": description})\n  requests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n  ```\n\n</details>\n\n### Testing\n```bash\n# Code\npython3 -m pytest tests/code --verbose --disable-warnings\n\n# Data\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings\n\n# Model\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npytest --run-id=$RUN_ID tests/model --verbose --disable-warnings\n\n# Coverage\npython3 -m pytest tests/code --cov madewithml --cov-report html --disable-warnings  # html report\npython3 -m pytest tests/code --cov madewithml --cov-report term --disable-warnings  # terminal report\n```\n\n## Production\n\nFrom this point onwards, in order to deploy our application into production, we'll need to either be on Anyscale or on a [cloud VM](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index) / [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) cluster you manage yourself (w/ Ray). If not on Anyscale, the commands will be [slightly different](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html) but the concepts will be the same.\n\n> If you don't want to set up all of this yourself, we highly recommend joining our [upcoming live cohort](https://4190urw86oh.typeform.com/madewithml){:target=\"_blank\"} where we'll provide an environment with all of this infrastructure already set up for you so that you just focused on the machine learning.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/jobs_and_services/manual.png\">\n</div>\n\n### Authentication\n\nThese credentials below are **automatically** set for us if we're using Anyscale Workspaces. We **do not** need to set these credentials explicitly on Workspaces but we do if we're running this locally or on a cluster outside of where our Anyscale Jobs and Services are configured to run.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from Anyscale credentials page\n```\n\n### Cluster environment\n\nThe cluster environment determines **where** our workloads will be executed (OS, dependencies, etc.) We've already created this [cluster environment](./deploy/cluster_env.yaml) for us but this is how we can create/update one ourselves.\n\n```bash\nexport CLUSTER_ENV_NAME=\"madewithml-cluster-env\"\nanyscale cluster-env build deploy/cluster_env.yaml --name $CLUSTER_ENV_NAME\n```\n\n### Compute configuration\n\nThe compute configuration determines **what** resources our workloads will be executes on. We've already created this [compute configuration](./deploy/cluster_compute.yaml) for us but this is how we can create it ourselves.\n\n```bash\nexport CLUSTER_COMPUTE_NAME=\"madewithml-cluster-compute-g5.4xlarge\"\nanyscale cluster-compute create deploy/cluster_compute.yaml --name $CLUSTER_COMPUTE_NAME\n```\n\n### Anyscale jobs\n\nNow we're ready to execute our ML workloads. We've decided to combine them all together into one [job](./deploy/jobs/workloads.yaml) but we could have also created separate jobs for each workload (train, evaluate, etc.) We'll start by editing the `$GITHUB_USERNAME` slots inside our [`workloads.yaml`](./deploy/jobs/workloads.yaml) file:\n```yaml\nruntime_env:\n  working_dir: .\n  upload_path: s3://madewithml/$GITHUB_USERNAME/jobs  # <--- CHANGE USERNAME (case-sensitive)\n  env_vars:\n    GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nThe `runtime_env` here specifies that we should upload our current `working_dir` to an S3 bucket so that all of our workers when we execute an Anyscale Job have access to the code to use. The `GITHUB_USERNAME` is used later to save results from our workloads to S3 so that we can retrieve them later (ex. for serving).\n\nNow we're ready to submit our job to execute our ML workloads:\n```bash\nanyscale job submit deploy/jobs/workloads.yaml\n```\n\n### Anyscale Services\n\nAnd after our ML workloads have been executed, we're ready to launch our serve our model to production. Similar to our Anyscale Jobs configs, be sure to change the `$GITHUB_USERNAME` in [`serve_model.yaml`](./deploy/services/serve_model.yaml).\n\n```yaml\nray_serve_config:\n  import_path: deploy.services.serve_model:entrypoint\n  runtime_env:\n    working_dir: .\n    upload_path: s3://madewithml/$GITHUB_USERNAME/services  # <--- CHANGE USERNAME (case-sensitive)\n    env_vars:\n      GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nNow we're ready to launch our service:\n```bash\n# Rollout service\nanyscale service rollout -f deploy/services/serve_model.yaml\n\n# Query\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $SECRET_TOKEN\" -d '{\n  \"title\": \"Transfer learning with transformers\",\n  \"description\": \"Using transformers for transfer learning on text classification tasks.\"\n}' $SERVICE_ENDPOINT/predict/\n\n# Rollback (to previous version of the Service)\nanyscale service rollback -f $SERVICE_CONFIG --name $SERVICE_NAME\n\n# Terminate\nanyscale service terminate --name $SERVICE_NAME\n```\n\n### CI/CD\n\nWe're not going to manually deploy our application every time we make a change. Instead, we'll automate this process using GitHub Actions!\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/cicd.png\">\n</div>\n\n1. Create a new github branch to save our changes to and execute CI/CD workloads:\n```bash\ngit remote set-url origin https://github.com/$GITHUB_USERNAME/Made-With-ML.git  # <-- CHANGE THIS to your username\ngit checkout -b dev\n```\n\n2. We'll start by adding the necessary credentials to the [`/settings/secrets/actions`](https://github.com/GokuMohandas/Made-With-ML/settings/secrets/actions) page of our GitHub repository.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from https://console.anyscale.com/o/madewithml/credentials\n```\n\n3. Now we can make changes to our code (not on `main` branch) and push them to GitHub. But in order to push our code to GitHub, we'll need to first authenticate with our credentials before pushing to our repository:\n\n```bash\ngit config --global user.name $GITHUB_USERNAME  # <-- CHANGE THIS to your username\ngit config --global user.email you@example.com  # <-- CHANGE THIS to your email\ngit add .\ngit commit -m \"\"  # <-- CHANGE THIS to your message\ngit push origin dev\n```\n\nNow you will be prompted to enter your username and password (personal access token). Follow these steps to get personal access token: [New GitHub personal access token](https://github.com/settings/tokens/new) → Add a name → Toggle `repo` and `workflow` → Click `Generate token` (scroll down) → Copy the token and paste it when prompted for your password.\n\n4. Now we can start a PR from this branch to our `main` branch and this will trigger the [workloads workflow](/.github/workflows/workloads.yaml). If the workflow (Anyscale Jobs) succeeds, this will produce comments with the training and evaluation results directly on the PR.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/comments.png\">\n</div>\n\n5. If we like the results, we can merge the PR into the `main` branch. This will trigger the [serve workflow](/.github/workflows/serve.yaml) which will rollout our new service to production!\n\n### Continual learning\n\nWith our CI/CD workflow in place to deploy our application, we can now focus on continually improving our model. It becomes really easy to extend on this foundation to connect to scheduled runs (cron), [data pipelines](https://madewithml.com/courses/mlops/data-engineering/), drift detected through [monitoring](https://madewithml.com/courses/mlops/monitoring/), [online evaluation](https://madewithml.com/courses/mlops/evaluation/#online-evaluation), etc. And we can easily add additional context such as comparing any experiment with what's currently in production (directly in the PR even), etc.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/continual.png\">\n</div>\n\n## FAQ\n\n### Jupyter notebook kernels\n\nIssues with configuring the notebooks with jupyter? By default, jupyter will use the kernel with our virtual environment but we can also manually add it to jupyter:\n```bash\npython3 -m ipykernel install --user --name=venv\n```\nNow we can open up a notebook → Kernel (top menu bar) → Change Kernel → `venv`. To ever delete this kernel, we can do the following:\n```bash\njupyter kernelspec list\njupyter kernelspec uninstall venv\n```\n",
    "meta_json": "{\"language\":\"Jupyter Notebook\",\"stars\":44722,\"forks\":6979,\"watchers\":44722,\"open_issues\":21,\"topics\":[\"data-engineering\",\"data-quality\",\"data-science\",\"deep-learning\",\"distributed-ml\",\"distributed-training\",\"llms\",\"machine-learning\",\"mlops\",\"natural-language-processing\",\"python\",\"pytorch\",\"ray\"],\"default_branch\":\"main\",\"size_kb\":4001,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:GokuMohandas:Made-With-ML\\\"><img\",\"source_url\":\"https://github.com/GokuMohandas/Made-With-ML\\\"><img\"},{\"type\":\"has_code\",\"target_id\":\"github:GokuMohandas:Made-With-ML\\\"\",\"source_url\":\"https://github.com/GokuMohandas/Made-With-ML\\\"\"},{\"type\":\"has_code\",\"target_id\":\"github:GokuMohandas:Made-With-ML\",\"source_url\":\"https://github.com/GokuMohandas/Made-With-ML\"},{\"type\":\"has_code\",\"target_id\":\"github:GokuMohandas:Made-With-ML.git\",\"source_url\":\"https://github.com/GokuMohandas/Made-With-ML.git\"},{\"type\":\"has_code\",\"target_id\":\"github:pyenv:pyenv\",\"source_url\":\"https://github.com/pyenv/pyenv\"},{\"type\":\"has_code\",\"target_id\":\"github:pyenv-win:pyenv-win\",\"source_url\":\"https://github.com/pyenv-win/pyenv-win\"},{\"type\":\"has_code\",\"target_id\":\"github:$GITHUB_USERNAME:Made-With-ML.git\",\"source_url\":\"https://github.com/$GITHUB_USERNAME/Made-With-ML.git\"},{\"type\":\"has_code\",\"target_id\":\"github:GokuMohandas:Made-With-ML\",\"source_url\":\"https://github.com/GokuMohandas/Made-With-ML\"},{\"type\":\"has_code\",\"target_id\":\"github:settings:tokens\",\"source_url\":\"https://github.com/settings/tokens\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "41b7e6cf339dda323e1e82d568443b5b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/GokuMohandas/Made-With-ML\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:microsoft:ai-for-beginners",
    "name": "AI-For-Beginners",
    "author": "microsoft",
    "description": "|!Sketchnote by @girlie_mac https://twitter.com/girlie_mac| |:---:| | AI For Beginners - _Sketchnote by @girlie_mac_ | Explore the world of **Artificial Intelligence** (AI) with our 12-week, 24-lesson curriculum! It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI <!-- CO-OP TRANSLATOR LANGUAGES TABLE START --> Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chines...",
    "tags": [
      "ai",
      "artificial-intelligence",
      "cnn",
      "computer-vision",
      "deep-learning",
      "gan",
      "machine-learning",
      "microsoft-for-beginners",
      "nlp",
      "rnn",
      "jupyter notebook"
    ],
    "pipeline_tag": "other",
    "likes": 44142,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/microsoft/AI-For-Beginners",
    "image_url": null,
    "type": "tool",
    "body_content": "[![GitHub license](https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg)](https://github.com/microsoft/AI-For-Beginners/blob/main/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/AI-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/AI-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/AI-For-Beginners/stargazers/)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD)\n[![Gitter](https://badges.gitter.im/Microsoft/ai-for-beginners.svg)](https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\n# Artificial Intelligence for Beginners - A Curriculum\n\n|![Sketchnote by @girlie_mac https://twitter.com/girlie_mac](./lessons/sketchnotes/ai-overview.png)|\n|:---:|\n| AI For Beginners - _Sketchnote by [@girlie_mac](https://twitter.com/girlie_mac)_ |\n\nExplore the world of **Artificial Intelligence** (AI) with our 12-week, 24-lesson curriculum!  It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI\n\n\n### 🌐 Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE START -->\n[Arabic](./translations/ar/README.md) | [Bengali](./translations/bn/README.md) | [Bulgarian](./translations/bg/README.md) | [Burmese (Myanmar)](./translations/my/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Croatian](./translations/hr/README.md) | [Czech](./translations/cs/README.md) | [Danish](./translations/da/README.md) | [Dutch](./translations/nl/README.md) | [Estonian](./translations/et/README.md) | [Finnish](./translations/fi/README.md) | [French](./translations/fr/README.md) | [German](./translations/de/README.md) | [Greek](./translations/el/README.md) | [Hebrew](./translations/he/README.md) | [Hindi](./translations/hi/README.md) | [Hungarian](./translations/hu/README.md) | [Indonesian](./translations/id/README.md) | [Italian](./translations/it/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Lithuanian](./translations/lt/README.md) | [Malay](./translations/ms/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Norwegian](./translations/no/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Polish](./translations/pl/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Romanian](./translations/ro/README.md) | [Russian](./translations/ru/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Slovak](./translations/sk/README.md) | [Slovenian](./translations/sl/README.md) | [Spanish](./translations/es/README.md) | [Swahili](./translations/sw/README.md) | [Swedish](./translations/sv/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Tamil](./translations/ta/README.md) | [Thai](./translations/th/README.md) | [Turkish](./translations/tr/README.md) | [Ukrainian](./translations/uk/README.md) | [Urdu](./translations/ur/README.md) | [Vietnamese](./translations/vi/README.md)\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE END -->\n\n**If you wish to have additional translations languages supported are listed [here](https://github.com/Azure/co-op-translator/blob/main/getting_started/supported-languages.md)**\n\n## Join the Community\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\n## What you will learn\n\n**[Mindmap of the Course](http://soshnikov.com/courses/ai-for-beginners/mindmap.html)**\n\nIn this curriculum, you will learn:\n\n* Different approaches to Artificial Intelligence, including the \"good old\" symbolic approach with **Knowledge Representation** and reasoning ([GOFAI](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)).\n* **Neural Networks** and **Deep Learning**, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - [TensorFlow](http://Tensorflow.org) and [PyTorch](http://pytorch.org).\n* **Neural Architectures** for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.\n* Less popular AI approaches, such as **Genetic Algorithms** and **Multi-Agent Systems**.\n\nWhat we will not cover in this curriculum:\n\n> [Find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)\n\n* Business cases of using **AI in Business**. Consider taking [Introduction to AI for business users](https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum) learning path on Microsoft Learn, or [AI Business School](https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum), developed in cooperation with [INSEAD](https://www.insead.edu/).\n* **Classic Machine Learning**, which is well described in our [Machine Learning for Beginners Curriculum](http://github.com/Microsoft/ML-for-Beginners).\n* Practical AI applications built using **[Cognitive Services](https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum)**. For this, we recommend that you start with modules Microsoft Learn for [vision](https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum), [natural language processing](https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum), **[Generative AI with Azure OpenAI Service](https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum)** and others.\n* Specific ML **Cloud Frameworks**, such as [Azure Machine Learning](https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum), [Microsoft Fabric](https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum), or [Azure Databricks](https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum). Consider using [Build and operate machine learning solutions with Azure Machine Learning](https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-77998-bethanycheum) and [Build and Operate Machine Learning Solutions with Azure Databricks](https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-77998-bethanycheum) learning paths.\n* **Conversational AI** and **Chat Bots**. There is a separate [Create conversational AI solutions](https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-77998-bethanycheum) learning path, and you can also refer to [this blog post](https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/) for more detail.\n* **Deep Mathematics** behind deep learning. For this, we would recommend [Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618) by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/).\n\nFor a gentle introduction to _AI in the Cloud_ topics you may consider taking the [Get started with artificial intelligence on Azure](https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-77998-bethanycheum) Learning Path.\n\n# Content\n\n|     |                                                                 Lesson Link                                                                  |                                           PyTorch/Keras/TensorFlow                                          | Lab                                                            |\n| :-: | :------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------ |\n| 0  |                                 [Course Setup](./lessons/0-course-setup/setup.md)                                 |                      [Setup Your Development Environment](./lessons/0-course-setup/how-to-run.md)                       |   |\n| I  |               [**Introduction to AI**](./lessons/1-Intro/README.md)      | | |\n| 01  |       [Introduction and History of AI](./lessons/1-Intro/README.md)       |           -                            | -  |\n| II |              **Symbolic AI**              |\n| 02  |       [Knowledge Representation and Expert Systems](./lessons/2-Symbolic/README.md)       |            [Expert Systems](./lessons/2-Symbolic/Animals.ipynb) /  [Ontology](./lessons/2-Symbolic/FamilyOntology.ipynb) /[Concept Graph](./lessons/2-Symbolic/MSConceptGraph.ipynb)                             |  |\n| III |                        [**Introduction to Neural Networks**](./lessons/3-NeuralNetworks/README.md) |||\n| 03  |                [Perceptron](./lessons/3-NeuralNetworks/03-Perceptron/README.md)                 |                       [Notebook](./lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb)                      | [Lab](./lessons/3-NeuralNetworks/03-Perceptron/lab/README.md) |\n| 04  |                   [Multi-Layered Perceptron and Creating our own Framework](./lessons/3-NeuralNetworks/04-OwnFramework/README.md)                   |        [Notebook](./lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb)        | [Lab](./lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md) |\n| 05  |            [Intro to Frameworks (PyTorch/TensorFlow) and Overfitting](./lessons/3-NeuralNetworks/05-Frameworks/README.md)             |           [PyTorch](./lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb) / [Keras](./lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) / [TensorFlow](./lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb)             | [Lab](./lessons/3-NeuralNetworks/05-Frameworks/lab/README.md) |\n| IV  |            [**Computer Vision**](./lessons/4-ComputerVision/README.md)             | [PyTorch](https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-77998-cacaste) / [TensorFlow](https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-77998-cacaste)| [Explore Computer Vision on Microsoft Azure](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum) |\n| 06  |            [Intro to Computer Vision. OpenCV](./lessons/4-ComputerVision/06-IntroCV/README.md)             |           [Notebook](./lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb)         | [Lab](./lessons/4-ComputerVision/06-IntroCV/lab/README.md) |\n| 07  |            [Convolutional Neural Networks](./lessons/4-ComputerVision/07-ConvNets/README.md) &  [CNN Architectures](./lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md)             |           [PyTorch](./lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb) /[TensorFlow](./lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb)             | [Lab](./lessons/4-ComputerVision/07-ConvNets/lab/README.md) |\n| 08  |            [Pre-trained Networks and Transfer Learning](./lessons/4-ComputerVision/08-TransferLearning/README.md) and [Training Tricks](./lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md)             |           [PyTorch](./lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb) / [TensorFlow](./lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb)             | [Lab](./lessons/4-ComputerVision/08-TransferLearning/lab/README.md) |\n| 09  |            [Autoencoders and VAEs](./lessons/4-ComputerVision/09-Autoencoders/README.md)             |           [PyTorch](./lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb) / [TensorFlow](./lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)             |  |\n| 10  |            [Generative Adversarial Networks & Artistic Style Transfer](./lessons/4-ComputerVision/10-GANs/README.md)             |           [PyTorch](./lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb) / [TensorFlow](./lessons/4-ComputerVision/10-GANs/GANTF.ipynb)             |  |\n| 11  |            [Object Detection](./lessons/4-ComputerVision/11-ObjectDetection/README.md)             |         [TensorFlow](./lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb)             | [Lab](./lessons/4-ComputerVision/11-ObjectDetection/lab/README.md) |\n| 12  |            [Semantic Segmentation. U-Net](./lessons/4-ComputerVision/12-Segmentation/README.md)             |           [PyTorch](./lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb) / [TensorFlow](./lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb)             |  |\n| V  |            [**Natural Language Processing**](./lessons/5-NLP/README.md)             | [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) /[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-77998-cacaste) | [Explore Natural Language Processing on Microsoft Azure](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)|\n| 13  |            [Text Representation. Bow/TF-IDF](./lessons/5-NLP/13-TextRep/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)             | |\n| 14  |            [Semantic word embeddings. Word2Vec and GloVe](./lessons/5-NLP/14-Embeddings/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)             |  |\n| 15  |            [Language Modeling. Training your own embeddings](./lessons/5-NLP/15-LanguageModeling/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)             | [Lab](./lessons/5-NLP/15-LanguageModeling/lab/README.md) |\n| 16  |            [Recurrent Neural Networks](./lessons/5-NLP/16-RNN/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/16-RNN/RNNTF.ipynb)             |  |\n| 17  |            [Generative Recurrent Networks](./lessons/5-NLP/17-GenerativeNetworks/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)             | [Lab](./lessons/5-NLP/17-GenerativeNetworks/lab/README.md) |\n| 18  |            [Transformers. BERT.](./lessons/5-NLP/18-Transformers/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb) /[TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb)             |  |\n| 19  |            [Named Entity Recognition](./lessons/5-NLP/19-NER/README.md)             |           [TensorFlow](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/19-NER/NER-TF.ipynb)             | [Lab](./lessons/5-NLP/19-NER/lab/README.md) |\n| 20  |            [Large Language Models, Prompt Programming and Few-Shot Tasks](./lessons/5-NLP/20-LangModels/README.md)             |           [PyTorch](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb) | |\n| VI |            **Other AI Techniques** || |\n| 21  |            [Genetic Algorithms](./lessons/6-Other/21-GeneticAlgorithms/README.md)             |           [Notebook](./lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb) | |\n| 22  |            [Deep Reinforcement Learning](./lessons/6-Other/22-DeepRL/README.md)             |           [PyTorch](./lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb) /[TensorFlow](./lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)             | [Lab](./lessons/6-Other/22-DeepRL/lab/README.md) |\n| 23  |            [Multi-Agent Systems](./lessons/6-Other/23-MultiagentSystems/README.md)             |  | |\n| VII |            **AI Ethics** | | |\n| 24  |            [AI Ethics and Responsible AI](./lessons/7-Ethics/README.md)             |           [Microsoft Learn: Responsible AI Principles](https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-77998-cacaste) | |\n| IX  |            **Extras** | | |\n| 25  |            [Multi-Modal Networks, CLIP and VQGAN](./lessons/X-Extras/X1-MultiModal/README.md)             |           [Notebook](./lessons/X-Extras/X1-MultiModal/Clip.ipynb)    | |\n\n## Each lesson contains\n\n* Pre-reading material\n* Executable Jupyter Notebooks, which are often specific to the framework (**PyTorch** or **TensorFlow**). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).\n* **Labs** available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.\n* Some sections contain links to [**MS Learn**](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum) modules that cover related topics.\n\n## Getting Started\n\n### 🎯 New to AI? Start Here!\n\nIf you're completely new to AI and want quick, hands-on examples, check out our [**Beginner-Friendly Examples**](./examples/README.md)! These include:\n\n- 🌟 **Hello AI World** - Your first AI program (pattern recognition)\n- 🧠 **Simple Neural Network** - Build a neural network from scratch  \n- 🖼️ **Image Classifier** - Classify images with detailed comments\n- 💬 **Text Sentiment** - Analyze positive/negative text\n\nThese examples are designed to help you understand AI concepts before diving into the full curriculum.\n\n### 📚 Full Curriculum Setup\n\n- We have created a [setup lesson](./lessons/0-course-setup/setup.md) to help you with setting up your development environment. - For Educators, we have created a [curricula setup lesson](./lessons/0-course-setup/for-teachers.md) for you too!\n- How to [Run the code in a VSCode or a Codepace](./lessons/0-course-setup/how-to-run.md)\n\nFollow these steps:\n\nFork the Repository: Click on the \"Fork\" button at the top-right corner of this page.\n\nClone the Repository: `git clone https://github.com/microsoft/AI-For-Beginners.git`\n\nDon't forget to star (🌟) this repo to find it easier later.\n\n## Meet other Learners\n\nJoin our [official AI Discord server](https://aka.ms/genai-discord?WT.mc_id=academic-105485-bethanycheum) to meet and network with other learners taking this course and get support.\n\nIf you have product feedback or questions whilst building visit our [Azure AI Foundry Developer Forum](https://aka.ms/foundry/forum)\n\n## Quizzes \n\n> **A note about quizzes**: All quizzes are contained in the Quiz-app folder in etc\\quiz-app, or [Online Here](https://ff-quizzes.netlify.app/) They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the `quiz-app` folder. They are gradually being localized.\n\n## Help Wanted\n\nDo you have suggestions or found spelling or code errors? Raise an issue or create a pull request.\n\n## Special Thanks\n\n* **✍️ Primary Author:** [Dmitry Soshnikov](http://soshnikov.com), PhD\n* **🔥 Editor:** [Jen Looper](https://twitter.com/jenlooper), PhD\n* **🎨 Sketchnote illustrator:** [Tomomi Imura](https://twitter.com/girlie_mac)\n* **✅ Quiz Creator:** [Lateefah Bello](https://github.com/CinnamonXI), [MLSA](https://studentambassadors.microsoft.com/)\n* **🙏 Core Contributors:** [Evgenii Pishchik](https://github.com/Pe4enIks)\n\n## Other Curricula\n\nOur team produces other curricula! Check out:\n\n<!-- CO-OP TRANSLATOR OTHER COURSES START -->\n### Azure / Edge / MCP / Agents\n[![AZD for Beginners](https://img.shields.io/badge/AZD%20for%20Beginners-0078D4?style=for-the-badge&labelColor=E5E7EB&color=0078D4)](https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Edge AI for Beginners](https://img.shields.io/badge/Edge%20AI%20for%20Beginners-00B8E4?style=for-the-badge&labelColor=E5E7EB&color=00B8E4)](https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![MCP for Beginners](https://img.shields.io/badge/MCP%20for%20Beginners-009688?style=for-the-badge&labelColor=E5E7EB&color=009688)](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI Agents for Beginners](https://img.shields.io/badge/AI%20Agents%20for%20Beginners-00C49A?style=for-the-badge&labelColor=E5E7EB&color=00C49A)](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Generative AI Series\n[![Generative AI for Beginners](https://img.shields.io/badge/Generative%20AI%20for%20Beginners-8B5CF6?style=for-the-badge&labelColor=E5E7EB&color=8B5CF6)](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (.NET)](https://img.shields.io/badge/Generative%20AI%20(.NET)-9333EA?style=for-the-badge&labelColor=E5E7EB&color=9333EA)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (Java)](https://img.shields.io/badge/Generative%20AI%20(Java)-C084FC?style=for-the-badge&labelColor=E5E7EB&color=C084FC)](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (JavaScript)](https://img.shields.io/badge/Generative%20AI%20(JavaScript)-E879F9?style=for-the-badge&labelColor=E5E7EB&color=E879F9)](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Core Learning\n[![ML for Beginners](https://img.shields.io/badge/ML%20for%20Beginners-22C55E?style=for-the-badge&labelColor=E5E7EB&color=22C55E)](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n[![Data Science for Beginners](https://img.shields.io/badge/Data%20Science%20for%20Beginners-84CC16?style=for-the-badge&labelColor=E5E7EB&color=84CC16)](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI for Beginners](https://img.shields.io/badge/AI%20for%20Beginners-A3E635?style=for-the-badge&labelColor=E5E7EB&color=A3E635)](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n[![Cybersecurity for Beginners](https://img.shields.io/badge/Cybersecurity%20for%20Beginners-F97316?style=for-the-badge&labelColor=E5E7EB&color=F97316)](https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung)\n[![Web Dev for Beginners](https://img.shields.io/badge/Web%20Dev%20for%20Beginners-EC4899?style=for-the-badge&labelColor=E5E7EB&color=EC4899)](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n[![IoT for Beginners](https://img.shields.io/badge/IoT%20for%20Beginners-14B8A6?style=for-the-badge&labelColor=E5E7EB&color=14B8A6)](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n[![XR Development for Beginners](https://img.shields.io/badge/XR%20Development%20for%20Beginners-38BDF8?style=for-the-badge&labelColor=E5E7EB&color=38BDF8)](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Copilot Series\n[![Copilot for AI Paired Programming](https://img.shields.io/badge/Copilot%20for%20AI%20Paired%20Programming-FACC15?style=for-the-badge&labelColor=E5E7EB&color=FACC15)](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n[![Copilot for C#/.NET](https://img.shields.io/badge/Copilot%20for%20C%23/.NET-FBBF24?style=for-the-badge&labelColor=E5E7EB&color=FBBF24)](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n[![Copilot Adventure](https://img.shields.io/badge/Copilot%20Adventure-FDE68A?style=for-the-badge&labelColor=E5E7EB&color=FDE68A)](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n<!-- CO-OP TRANSLATOR OTHER COURSES END -->\n\n## Getting Help\n\nIf you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It's a supportive community where questions are welcome and knowledge is shared freely.\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nIf you have product feedback or errors while building visit:\n\n[![Microsoft Foundry Developer Forum](https://img.shields.io/badge/GitHub-Microsoft_Foundry_Developer_Forum-blue?style=for-the-badge&logo=github&color=000000&logoColor=fff)](https://aka.ms/foundry/forum)\n",
    "meta_json": "{\"language\":\"Jupyter Notebook\",\"stars\":44142,\"forks\":8800,\"watchers\":44142,\"open_issues\":7,\"topics\":[\"ai\",\"artificial-intelligence\",\"cnn\",\"computer-vision\",\"deep-learning\",\"gan\",\"machine-learning\",\"microsoft-for-beginners\",\"nlp\",\"rnn\"],\"default_branch\":\"main\",\"size_kb\":1003297,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:Azure:co-op-translator\",\"source_url\":\"https://github.com/Azure/co-op-translator\"},{\"type\":\"has_code\",\"target_id\":\"github:Microsoft:ML-for-Beginners\",\"source_url\":\"http://github.com/Microsoft/ML-for-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AI-For-Beginners.git`\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners.git`\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:AZD-for-beginners\",\"source_url\":\"https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:edgeai-for-beginners\",\"source_url\":\"https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:mcp-for-beginners\",\"source_url\":\"https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:ai-agents-for-beginners\",\"source_url\":\"https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:generative-ai-for-beginners\",\"source_url\":\"https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:Generative-AI-for-beginners-dotnet\",\"source_url\":\"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:generative-ai-for-beginners-java\",\"source_url\":\"https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:generative-ai-with-javascript\",\"source_url\":\"https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:Security-101\",\"source_url\":\"https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:xr-development-for-beginners\",\"source_url\":\"https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:mastering-github-copilot-for-dotnet-csharp-developers\",\"source_url\":\"https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst\"},{\"type\":\"has_code\",\"target_id\":\"github:microsoft:CopilotAdventures\",\"source_url\":\"https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst\"}]",
    "canonical_id": null,
    "license_spdx": "MIT",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "992d1f99892cc34bff66034d85178ee1",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/microsoft/AI-For-Beginners\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:aymericdamien:tensorflow-examples",
    "name": "TensorFlow-Examples",
    "author": "aymericdamien",
    "description": "This tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 & v2. It is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional 'raw' TensorFlow implementations, you can also find the latest TensorFlow API practices (such as , , , ...). **Update (05/16/2020):** Moving all default examples to TF2. For TF v1 examples: check here. ...",
    "tags": [
      "deep-learning",
      "examples",
      "machine-learning",
      "python",
      "tensorflow",
      "tutorial",
      "jupyter notebook"
    ],
    "pipeline_tag": "other",
    "likes": 43766,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/aymericdamien/TensorFlow-Examples",
    "image_url": null,
    "type": "tool",
    "body_content": "# TensorFlow Examples\n\nThis tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 & v2.\n\nIt is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional 'raw' TensorFlow implementations, you can also find the latest TensorFlow API practices (such as `layers`, `estimator`, `dataset`, ...).\n\n**Update (05/16/2020):** Moving all default examples to TF2. For TF v1 examples: [check here](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1).\n\n## Tutorial index\n\n#### 0 - Prerequisite\n- [Introduction to Machine Learning](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/ml_introduction.ipynb).\n- [Introduction to MNIST Dataset](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb).\n\n#### 1 - Introduction\n- **Hello World** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/1_Introduction/helloworld.ipynb)). Very simple example to learn how to print \"hello world\" using TensorFlow 2.0+.\n- **Basic Operations** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/1_Introduction/basic_operations.ipynb)). A simple example that cover TensorFlow 2.0+ basic operations.\n\n#### 2 - Basic Models\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/linear_regression.ipynb)). Implement a Linear Regression with TensorFlow 2.0+.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/logistic_regression.ipynb)). Implement a Logistic Regression with TensorFlow 2.0+.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/word2vec.ipynb)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow 2.0+.\n- **GBDT (Gradient Boosted Decision Trees)** ([notebooks](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/gradient_boosted_trees.ipynb)). Implement a Gradient Boosted Decision Trees with TensorFlow 2.0+ to predict house value using Boston Housing dataset.\n\n#### 3 - Neural Networks\n##### Supervised\n\n- **Simple Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network.ipynb)). Use TensorFlow 2.0 'layers' and 'model' API to build a simple neural network to classify MNIST digits dataset.\n- **Simple Neural Network (low-level)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network_raw.ipynb)). Raw implementation of a simple neural network to classify MNIST digits dataset.\n- **Convolutional Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network.ipynb)). Use TensorFlow 2.0+ 'layers' and 'model' API to build a convolutional neural network to classify MNIST digits dataset.\n- **Convolutional Neural Network (low-level)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb)). Raw implementation of a convolutional neural network to classify MNIST digits dataset.\n- **Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/recurrent_network.ipynb)). Build a recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0 'layers' and 'model' API.\n- **Bi-directional Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0+ 'layers' and 'model' API.\n- **Dynamic Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb)). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of variable length, using TensorFlow 2.0+ 'layers' and 'model' API.\n\n##### Unsupervised\n- **Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/autoencoder.ipynb)). Build an auto-encoder to encode an image to a lower dimension and re-construct it.\n- **DCGAN (Deep Convolutional Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/dcgan.ipynb)). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.\n\n#### 4 - Utilities\n- **Save and Restore a model** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/save_restore_model.ipynb)). Save and Restore a model with TensorFlow 2.0+.\n- **Build Custom Layers & Modules** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/build_custom_layers.ipynb)). Learn how to build your own layers / modules and integrate them into TensorFlow 2.0+ Models.\n- **Tensorboard** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/tensorboard.ipynb)). Track and visualize neural network computation graph, metrics, weights and more using TensorFlow 2.0+ tensorboard.\n\n#### 5 - Data Management\n- **Load and Parse data** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb)). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...).\n- **Build and Load TFRecords** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/tfrecords.ipynb)). Convert data into TFRecords format, and load them with TensorFlow 2.0+.\n- **Image Transformation (i.e. Image Augmentation)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/image_transformation.ipynb)). Apply various image augmentation techniques with TensorFlow 2.0+, to generate distorted images for training.\n\n#### 6 - Hardware\n- **Multi-GPU Training** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/6_Hardware/multigpu_training.ipynb)). Train a convolutional neural network with multiple GPUs on CIFAR-10 dataset.\n\n## TensorFlow v1\n\nThe tutorial index for TF v1 is available here: [TensorFlow v1.15 Examples](tensorflow_v1). Or see below for a list of the examples.\n\n## Dataset\nSome examples require MNIST dataset for training and testing. Don't worry, this dataset will automatically be downloaded when running examples.\nMNIST is a database of handwritten digits, for a quick description of that dataset, you can check [this notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb).\n\nOfficial Website: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).\n\n## Installation\n\nTo download all the examples, simply clone this repository:\n```\ngit clone https://github.com/aymericdamien/TensorFlow-Examples\n```\n\nTo run them, you also need the latest version of TensorFlow. To install it:\n```\npip install tensorflow\n```\n\nor (with GPU support):\n```\npip install tensorflow_gpu\n```\n\nFor more details about TensorFlow installation, you can check [TensorFlow Installation Guide](https://www.tensorflow.org/install/)\n\n\n## TensorFlow v1 Examples - Index\n\nThe tutorial index for TF v1 is available here: [TensorFlow v1.15 Examples](tensorflow_v1).\n\n#### 0 - Prerequisite\n- [Introduction to Machine Learning](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/ml_introduction.ipynb).\n- [Introduction to MNIST Dataset](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb).\n\n#### 1 - Introduction\n- **Hello World** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/1_Introduction/helloworld.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/1_Introduction/helloworld.py)). Very simple example to learn how to print \"hello world\" using TensorFlow.\n- **Basic Operations** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/1_Introduction/basic_operations.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-examples/Examples/blob/master/tensorflow_v1/1_Introduction/basic_operations.py)). A simple example that cover TensorFlow basic operations.\n- **TensorFlow Eager API basics** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/1_Introduction/basic_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/1_Introduction/basic_eager_api.py)). Get started with TensorFlow's Eager API.\n\n#### 2 - Basic Models\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/linear_regression.py)). Implement a Linear Regression with TensorFlow.\n- **Linear Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/linear_regression_eager_api.py)). Implement a Linear Regression using TensorFlow's Eager API.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/logistic_regression.py)). Implement a Logistic Regression with TensorFlow.\n- **Logistic Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/logistic_regression_eager_api.py)). Implement a Logistic Regression using TensorFlow's Eager API.\n- **Nearest Neighbor** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/nearest_neighbor.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/nearest_neighbor.py)). Implement Nearest Neighbor algorithm with TensorFlow.\n- **K-Means** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/kmeans.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/kmeans.py)). Build a K-Means classifier with TensorFlow.\n- **Random Forest** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/random_forest.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/random_forest.py)). Build a Random Forest classifier with TensorFlow.\n- **Gradient Boosted Decision Tree (GBDT)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/gradient_boosted_decision_tree.py)). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/word2vec.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/word2vec.py)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow.\n\n#### 3 - Neural Networks\n##### Supervised\n\n- **Simple Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/3_NeuralNetworks/notebooks/neural_network_raw.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_raw.py)). Build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. Raw TensorFlow implementation.\n- **Simple Neural Network (tf.layers/estimator api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network.py)). Use TensorFlow 'layers' and 'estimator' API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.\n- **Simple Neural Network (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_eager_api.py)). Use TensorFlow Eager API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.\n- **Convolutional Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network_raw.py)). Build a convolutional neural network to classify MNIST digits dataset. Raw TensorFlow implementation.\n- **Convolutional Neural Network (tf.layers/estimator api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network.py)). Use TensorFlow 'layers' and 'estimator' API to build a convolutional neural network to classify MNIST digits dataset.\n- **Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/recurrent_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/recurrent_network.py)). Build a recurrent neural network (LSTM) to classify MNIST digits dataset.\n- **Bi-directional Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/bidirectional_rnn.py)). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset.\n- **Dynamic Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/dynamic_rnn.py)). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of different length.\n\n##### Unsupervised\n- **Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/autoencoder.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/autoencoder.py)). Build an auto-encoder to encode an image to a lower dimension and re-construct it.\n- **Variational Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/variational_autoencoder.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/variational_autoencoder.py)). Build a variational auto-encoder (VAE), to encode and generate images from noise.\n- **GAN (Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/gan.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/gan.py)). Build a Generative Adversarial Network (GAN) to generate images from noise.\n- **DCGAN (Deep Convolutional Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/dcgan.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/dcgan.py)). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.\n\n#### 4 - Utilities\n- **Save and Restore a model** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/save_restore_model.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/save_restore_model.py)). Save and Restore a model with TensorFlow.\n- **Tensorboard - Graph and loss visualization** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/tensorboard_basic.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/tensorboard_basic.py)). Use Tensorboard to visualize the computation Graph and plot the loss.\n- **Tensorboard - Advanced visualization** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/tensorboard_advanced.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/tensorboard_advanced.py)). Going deeper into Tensorboard; visualize the variables, gradients, and more...\n\n#### 5 - Data Management\n- **Build an image dataset** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py)). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.\n- **TensorFlow Dataset API** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py)). Introducing TensorFlow Dataset API for optimizing the input data pipeline.\n- **Load and Parse data** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb)). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).\n- **Build and Load TFRecords** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tfrecords.ipynb)). Convert data into TFRecords format, and load them.\n- **Image Transformation (i.e. Image Augmentation)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/image_transformation.ipynb)). Apply various image augmentation techniques, to generate distorted images for training.\n\n#### 6 - Multi GPU\n- **Basic Operations on multi-GPU** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_basics.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/6_MultiGPU/multigpu_basics.py)). A simple example to introduce multi-GPU in TensorFlow.\n- **Train a Neural Network on multi-GPU** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_cnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/6_MultiGPU/multigpu_cnn.py)). A clear and simple TensorFlow implementation to train a convolutional neural network on multiple GPUs.\n\n## More Examples\nThe following examples are coming from [TFLearn](https://github.com/tflearn/tflearn), a library that provides a simplified interface for TensorFlow. You can have a look, there are many [examples](https://github.com/tflearn/tflearn/tree/master/examples) and [pre-built operations and layers](http://tflearn.org/doc_index/#api).\n\n### Tutorials\n- [TFLearn Quickstart](https://github.com/tflearn/tflearn/blob/master/tutorials/intro/quickstart.md). Learn the basics of TFLearn through a concrete machine learning task. Build and train a deep neural network classifier.\n\n### Examples\n- [TFLearn Examples](https://github.com/tflearn/tflearn/blob/master/examples). A large collection of examples using TFLearn.\n\n",
    "meta_json": "{\"language\":\"Jupyter Notebook\",\"stars\":43766,\"forks\":14834,\"watchers\":43766,\"open_issues\":226,\"topics\":[\"deep-learning\",\"examples\",\"machine-learning\",\"python\",\"tensorflow\",\"tutorial\"],\"default_branch\":\"master\",\"size_kb\":10242,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:aymericdamien:TensorFlow-Examples\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\"},{\"type\":\"has_code\",\"target_id\":\"github:tflearn:tflearn\",\"source_url\":\"https://github.com/tflearn/tflearn\"},{\"type\":\"has_code\",\"target_id\":\"github:tflearn:tflearn\",\"source_url\":\"https://github.com/tflearn/tflearn\"},{\"type\":\"has_code\",\"target_id\":\"github:tflearn:tflearn\",\"source_url\":\"https://github.com/tflearn/tflearn\"},{\"type\":\"has_code\",\"target_id\":\"github:tflearn:tflearn\",\"source_url\":\"https://github.com/tflearn/tflearn\"}]",
    "canonical_id": null,
    "license_spdx": "NOASSERTION",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "f2fb1f9be1421e79b60fc7ba27bde347",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/aymericdamien/TensorFlow-Examples\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:apache:airflow",
    "name": "airflow",
    "author": "apache",
    "description": "Apache Airflow - A platform to programmatically author, schedule, and monitor workflows",
    "tags": [
      "airflow",
      "apache",
      "apache-airflow",
      "automation",
      "dag",
      "data-engineering",
      "data-integration",
      "data-orchestrator",
      "data-pipelines",
      "data-science",
      "elt",
      "etl",
      "machine-learning",
      "mlops",
      "orchestration",
      "python",
      "scheduler",
      "workflow",
      "workflow-engine",
      "workflow-orchestration",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 43479,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/apache/airflow",
    "image_url": null,
    "type": "tool",
    "body_content": "",
    "meta_json": "{\"language\":\"Python\",\"stars\":43479,\"forks\":16078,\"watchers\":43479,\"open_issues\":1676,\"topics\":[\"airflow\",\"apache\",\"apache-airflow\",\"automation\",\"dag\",\"data-engineering\",\"data-integration\",\"data-orchestrator\",\"data-pipelines\",\"data-science\",\"elt\",\"etl\",\"machine-learning\",\"mlops\",\"orchestration\",\"python\",\"scheduler\",\"workflow\",\"workflow-engine\",\"workflow-orchestration\"],\"default_branch\":\"main\",\"size_kb\":509504,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 40,
    "content_hash": "81cea8902bc646917b186e7ff5cdbbb3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/apache/airflow\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:streamlit:streamlit",
    "name": "streamlit",
    "author": "streamlit",
    "description": "<br> <img src=\"https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png\" alt=\"Streamlit logo\" style=\"margin-top:50px\"></img> **A faster way to build and share data apps.** Streamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our Community Cloud platform to deploy, manage, and share your app. - **Simple and P...",
    "tags": [
      "data-analysis",
      "data-science",
      "data-visualization",
      "deep-learning",
      "developer-tools",
      "machine-learning",
      "python",
      "streamlit",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 42603,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/streamlit/streamlit",
    "image_url": null,
    "type": "tool",
    "body_content": "<br>\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png\" alt=\"Streamlit logo\" style=\"margin-top:50px\"></img>\n\n# Welcome to Streamlit 👋\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you're all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif\" alt=\"Streamlit Hello\" width=500 href=\"none\"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src=\"https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif\" width=300 alt=\"Little example\"></img>\n\n### Give me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n<table border=\"0\">\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/widgets\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936099-12c16f8c-7fe4-44b1-889a-1ac9ee6a1b44.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/data/st.dataframe\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215110064-5eb4e294-8f30-4933-9563-0275230e52b5.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/charts\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215174472-bca8a0d7-cf4b-4268-9c3b-8c03dad50bcd.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/layout\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936149-a35c35be-0d96-4c63-8c6a-1c4b52aa8f60.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/concepts/multipage-apps\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215173883-eae0de69-7c1d-4d78-97d0-3bc1ab865e5b.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://streamlit.io/gallery\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215109229-6ae9111f-e5c1-4f0b-b3a2-87a79268ccc9.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>Input widgets</td>\n    <td>Dataframes</td>\n    <td>Charts</td>\n    <td>Layout</td>\n    <td>Multi-page apps</td>\n    <td>Fun</td>\n  </tr>\n</table>\n\n\nOur vibrant creators community also extends Streamlit capabilities using  🧩 [Streamlit Components](https://streamlit.io/components).\n\n## Get inspired\n\nThere's so much you can build with Streamlit:\n- 🤖  [LLMs & chatbot apps](https://streamlit.io/gallery?category=llms)\n- 🧬  [Science & technology apps](https://streamlit.io/gallery?category=science-technology)\n- 💬  [NLP & language apps](https://streamlit.io/gallery?category=nlp-language)\n- 🏦  [Finance & business apps](https://streamlit.io/gallery?category=finance-business)\n- 🗺  [Geography & society apps](https://streamlit.io/gallery?category=geography-society)\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)** 🎈\n\n## Community Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup). <br><br>\n<img src=\"https://user-images.githubusercontent.com/7164864/214965336-64500db3-0d79-4a20-8052-2dda883902d2.gif\" width=\"400\"></img>\n\n## Resources\n\n- Explore our [docs](https://docs.streamlit.io) to learn how Streamlit works.\n- Ask questions and get help in our [community forum](https://discuss.streamlit.io).\n- Read our [blog](https://blog.streamlit.io) for tips from developers and creators.\n- Extend Streamlit's capabilities by installing or creating your own [Streamlit Components](https://streamlit.io/components).\n- Help others find and play with your app by using the Streamlit GitHub badge in your repository:\n```markdown\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](URL_TO_YOUR_APP)\n```\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/roadmap)\n\n## Contribute\n\n🎉 Thanks for your interest in helping improve Streamlit! 🎉\n\nBefore contributing, please read our guidelines here: https://github.com/streamlit/streamlit/wiki/Contributing\n\n## License\n\nStreamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":42603,\"forks\":3949,\"watchers\":42603,\"open_issues\":1298,\"topics\":[\"data-analysis\",\"data-science\",\"data-visualization\",\"deep-learning\",\"developer-tools\",\"machine-learning\",\"python\",\"streamlit\"],\"default_branch\":\"develop\",\"size_kb\":791137,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:streamlit:streamlit\",\"source_url\":\"https://github.com/streamlit/streamlit\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "a0699b965473363ec49d99eceeed15bc",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/streamlit/streamlit\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:deepspeedai:deepspeed",
    "name": "DeepSpeed",
    "author": "deepspeedai",
    "description": "<div align=\"center\"> <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\"> <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\"> </div> * [2025/10] We hosted the Ray x DeepSpeed Meetup at Anyscale. We shared our most recent work on SuperOffload, ZenFlow, Muon Optimizer Support, Arctic Long Sequence Training and DeepCompile. Please find the meetup slides here. * [2025/10] SuperOffload: Unleashing the Power of Large-Scale LLM...",
    "tags": [
      "billion-parameters",
      "compression",
      "data-parallelism",
      "deep-learning",
      "gpu",
      "inference",
      "machine-learning",
      "mixture-of-experts",
      "model-parallelism",
      "pipeline-parallelism",
      "pytorch",
      "trillion-parameters",
      "zero",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 40955,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/deepspeedai/DeepSpeed",
    "image_url": null,
    "type": "model",
    "body_content": "[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/DeepSpeedAI)](https://twitter.com/intent/follow?screen_name=DeepSpeedAI)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue)](https://twitter.com/DeepSpeedAI_JP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n[![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/deepspeedworkspace/shared_invite/zt-3a8pjd8dd-PCj2hMvR4Y2syPwVnjEoww)\n\n\n<div align=\"center\">\n <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n\n* [2025/10] We hosted the [Ray x DeepSpeed Meetup](https://luma.com/3wctqteh) at Anyscale. We shared our most recent work on SuperOffload, ZenFlow, Muon Optimizer Support, Arctic Long Sequence Training and DeepCompile. Please find the meetup slides [here](https://docs.google.com/presentation/d/1eM3mY6oW9GYkRy1Xz0iOnbbEr5T1t0JJXOM5BKtR-Ks/edit?slide=id.g38615d6b4c2_0_87#slide=id.g38615d6b4c2_0_87).\n\n* [2025/10] [SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips](https://pytorch.org/blog/superoffload-unleashing-the-power-of-large-scale-llm-training-on-superchips/)\n\n* [2025/10] [Study of ZenFlow and ZeRO offload performance with DeepSpeed CPU core binding](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/zenflow-corebinding/README.md)\n\n* [2025/08] [ZenFlow: Stall-Free Offloading Engine for LLM Training](https://pytorch.org/blog/zenflow-stall-free-offloading-engine-for-llm-training/)\n\n* [2025/06] [Arctic Long Sequence Training (ALST) with DeepSpeed: Scalable And Efficient Training For Multi-Million Token Sequences](https://www.snowflake.com/en/engineering-blog/arctic-long-sequence-training-multi-million-token-ai/)\n\n* [2025/06] [DeepNVMe: Affordable I/O scaling for Deep Learning Applications](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepnvme/06-2025/README.md)\n\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the 'more news' section -->\n<details>\n<!-- NOTE: Maintain only three items in 'more news' section -->\n <summary>More news</summary>\n <ul>\n\n   <li>[2025/04] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md\">DeepCompile: Unlocking Compiler Optimization for Distributed Training</a></li>\n\n   <li>[2025/03] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md\">DeepSpeed AutoTP: Automatic Tensor Parallel Training of Hugging Face models</a></li>\n\n<li>[2024/12] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md\">Ulysses-Offload: Democratizing Long Context LLM Training</a></li>\n\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training\n\n***[DeepSpeed](https://www.deepspeed.ai/) enabled the world's most powerful language models (at the time of this writing) such as [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. DeepSpeed offers a confluence of [system innovations](https://www.deepspeed.ai/training/), that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations include ZeRO, ZeRO-Infinity, 3D-Parallelism, Ulysses Sequence Parallelism, DeepSpeed-MoE, etc.\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed was an important part of Microsoft’s\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src=\"docs/assets/images/transformers-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/transformers-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src=\"docs/assets/images/accelerate-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/accelerate-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src=\"docs/assets/images/lightning-light.svg#gh-light-mode-only\" width=\"200px\"><img src=\"docs/assets/images/lightning-dark.svg#gh-dark-mode-only\" width=\"200px\"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src=\"docs/assets/images/mosaicml.svg\" width=\"200px\"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src=\"docs/assets/images/determined.svg\" width=\"225px\"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src=\"https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg\" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch-latest-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml)  [![nv-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n| Tecorigin   | Scalable Data Analytics Accelerator | sdaa             | Yes | No                 |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nMany DeepSpeed features are supported on Windows for both training and inference. You can read more about this in the original blog post [here](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md). Among features that are currently not supported are async io (AIO) and GDS (which does not support Windows).\n1. Install PyTorch, such as pytorch 2.3+cu121.\n2. Install Visual C++ build tools, such as VS2022 C++ x64/x86 build tools.\n3. Launch Cmd console with Administrator permissions for creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt for Visual Studio 2022 with administrator permissions.\n4. Run `build_win.bat` to build wheel in `dist` folder.\n\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# CI funding\n\nThis being an open source project we rely on others to provide us resources for CI hardware. At this moment Modal is kindly supporting our GPU CI runs by funding the hardware for us. Modal is an AI infrastructure platform for inference, fine-tuning, batch jobs and more. Get started with $30/mo in free credits today at https://modal.com. We have been getting an amazing support from Modal's team and will surely recommend them to your business.\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href=\"https://github.com/deepspeedai/DeepSpeed/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/DeepSpeed&r=\"  width=\"800px\"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and [ICS 2023](https://dl.acm.org/doi/10.1145/3577193.3593704).\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n34. Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He. (2025) Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences [arXiv:2506.13996](https://arxiv.org/abs/2506.13996)\n35. Tingfeng Lan, Yusen Wu, Bin Ma, Zhaoyuan Su, Rui Yang, Tekin Bicer, Masahiro Tanaka, Olatunji Ruwase, Dong Li, Yue Cheng. (2025) ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates [arXiv:2505.12242](https://arxiv.org/abs/2505.12242)\n36. Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang. (2026) SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips [arxiv](https://arxiv.org/abs/2509.21271), [ASPLOS 2026](https://www.asplos-conference.org/asplos2026)\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":40955,\"forks\":4656,\"watchers\":40955,\"open_issues\":1242,\"topics\":[\"billion-parameters\",\"compression\",\"data-parallelism\",\"deep-learning\",\"gpu\",\"inference\",\"machine-learning\",\"mixture-of-experts\",\"model-parallelism\",\"pipeline-parallelism\",\"pytorch\",\"trillion-parameters\",\"zero\"],\"default_branch\":\"master\",\"size_kb\":243597,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":true}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:THUDM:GLM-130B\",\"source_url\":\"https://github.com/THUDM/GLM-130B\"},{\"type\":\"has_code\",\"target_id\":\"github:yandex:YaLM-100B\",\"source_url\":\"https://github.com/yandex/YaLM-100B\"},{\"type\":\"has_code\",\"target_id\":\"github:EleutherAI:gpt-neox\",\"source_url\":\"https://github.com/EleutherAI/gpt-neox\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:Ascend:Ascend-CI\",\"source_url\":\"https://github.com/Ascend/Ascend-CI\"},{\"type\":\"has_code\",\"target_id\":\"github:Ascend:Ascend-CI\",\"source_url\":\"https://github.com/Ascend/Ascend-CI\"},{\"type\":\"has_code\",\"target_id\":\"github:ROCm-Developer-Tools:HIPCC\",\"source_url\":\"https://github.com/ROCm-Developer-Tools/HIPCC\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"},{\"type\":\"has_code\",\"target_id\":\"github:deepspeedai:DeepSpeed\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "5f4f07e99e5164d833b03b8c97512c2c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/deepspeedai/DeepSpeed\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:gradio-app:gradio",
    "name": "gradio",
    "author": "gradio-app",
    "description": "<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE OR TEMPLATES AND THEN RUN SCRIPT. --> <div align=\"center\"> <a href=\"https://gradio.app\"> <img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=350> </a> </div> <div align=\"center\"> <span> <a href=\"https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&th...",
    "tags": [
      "data-analysis",
      "data-science",
      "data-visualization",
      "deep-learning",
      "deploy",
      "gradio",
      "gradio-interface",
      "interface",
      "machine-learning",
      "models",
      "python",
      "python-notebook",
      "ui",
      "ui-components",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 40870,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/gradio-app/gradio",
    "image_url": null,
    "type": "model",
    "body_content": "<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align=\"center\">\n<a href=\"https://gradio.app\">\n<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=350>\n</a>\n</div>\n\n<div align=\"center\">\n<span>\n<a href=\"https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light\" alt=\"Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt\" style=\"width: 150px; height: 54px;\" width=\"150\" height=\"54\" /></a>\n<a href=\"https://trendshift.io/repositories/2145\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2145\" alt=\"gradio-app%2Fgradio | Trendshift\" style=\"width: 150px; height: 55px;\" width=\"150\" height=\"55\"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [中文](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps — in Python\n\n\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create your own demo, so let's get started 💫\n\n\n### Installation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install --upgrade gradio\n```\n\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n### Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. \n\nNow, run your code. If you've written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. You can also enable <strong>vibe mode</strong> by using the <code>--vibe</code> flag, e.g. <code>gradio --vibe app.py</code>, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. Learn more in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. \n\n> [!TIP]\n > For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`\"textbox\"`) or an instance of the class (`gr.Textbox()`).\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\n### Sharing Your Demo\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter 🚀\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\n👉 &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n### An Overview of Gradio\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets you build demos quickly with Gradio. But what else does Gradio include?\n\n#### Custom Demos with `gr.Blocks`\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction — still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n#### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! It's an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!\n\n### What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let's dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n### Gradio Sketch\n\nYou can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).\n\n## Questions?\n\nIf you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a ⭐ on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src=\"readme_files/huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"readme_files/python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"readme_files/fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"readme_files/encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"readme_files/svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"readme_files/vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"readme_files/pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"readme_files/tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n[<img src=\"readme_files/storybook.svg\" alt=\"storybook\" height=40>](https://storybook.js.org/)\n[<img src=\"readme_files/chromatic.svg\" alt=\"chromatic\" height=40>](https://www.chromatic.com/)\n\n## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## Citation\n\nAlso check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.\n\n```\n@article{abid2019gradio,\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\n  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\n  journal = {arXiv preprint arXiv:1906.02569},\n  year = {2019},\n}\n```\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":40870,\"forks\":3182,\"watchers\":40870,\"open_issues\":433,\"topics\":[\"data-analysis\",\"data-science\",\"data-visualization\",\"deep-learning\",\"deploy\",\"gradio\",\"gradio-interface\",\"interface\",\"machine-learning\",\"models\",\"python\",\"python-notebook\",\"ui\",\"ui-components\"],\"default_branch\":\"main\",\"size_kb\":312577,\"archived\":false,\"fork\":false,\"has_wiki\":true,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"},{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"},{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"},{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"},{\"type\":\"has_code\",\"target_id\":\"github:AUTOMATIC1111:stable-diffusion-webui\",\"source_url\":\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\"},{\"type\":\"has_code\",\"target_id\":\"github:gradio-app:gradio\",\"source_url\":\"https://github.com/gradio-app/gradio\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e002e1b5a98a4ec77bf4ee340f62aa9c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/gradio-app/gradio\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "github:ray-project:ray",
    "name": "ray",
    "author": "ray-project",
    "description": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png .. image:: https://readthedocs.org/projects/ray/badge/?version=master :target: http://docs.ray.io/en/master/?badge=master .. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue :target: https://www.ray.io/join-slack .. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue :target: https://discuss.ray.io/ .. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=soc...",
    "tags": [
      "data-science",
      "deep-learning",
      "deployment",
      "distributed",
      "hyperparameter-optimization",
      "hyperparameter-search",
      "large-language-models",
      "llm",
      "llm-inference",
      "llm-serving",
      "machine-learning",
      "optimization",
      "parallel",
      "python",
      "pytorch",
      "ray",
      "reinforcement-learning",
      "rllib",
      "serving",
      "tensorflow",
      "python"
    ],
    "pipeline_tag": "other",
    "likes": 40237,
    "downloads": 0,
    "source": "github",
    "source_url": "https://github.com/ray-project/ray",
    "image_url": null,
    "type": "model",
    "body_content": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://x.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable and Programmable Serving\n\nOr more about `Ray Core`_ and its key abstractions:\n\n- `Tasks`_: Stateless functions executed in the cluster.\n- `Actors`_: Stateful worker processes created in the cluster.\n- `Objects`_: Immutable values accessible across the cluster.\n\nLearn more about Monitoring and Debugging:\n\n- Monitor Ray apps and clusters with the `Ray Dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.\n- Debug Ray apps with the `Ray Distributed Debugger <https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.\n\nRay runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing\n`ecosystem of community integrations`_.\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/ray-overview/installation.html>`__.\n\n.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html\n.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html\n.. _`Workflow`: https://docs.ray.io/en/latest/workflows/\n.. _`Train`: https://docs.ray.io/en/latest/train/train.html\n.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html\n.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html\n\n\nWhy Ray?\n--------\n\nToday's ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n\nRay is a unified way to scale Python and AI applications from a laptop to a cluster.\n\nWith Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Ray Architecture whitepaper`_\n- `Exoshuffle: large-scale data shuffle in Ray`_\n- `Ownership: a distributed futures system for fine-grained tasks`_\n- `RLlib paper`_\n- `Tune paper`_\n\n*Older documents:*\n\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `Ray Architecture v1 whitepaper`_\n\n.. _`Ray AI Libraries`: https://docs.ray.io/en/latest/ray-air/getting-started.html\n.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html\n.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html\n.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html\n.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview\n.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview\n.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072\n.. _`Ownership: a distributed futures system for fine-grained tasks`: https://www.usenix.org/system/files/nsdi21-wang.pdf\n.. _`Ray paper`: https://arxiv.org/abs/1712.05889\n.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924\n.. _`RLlib paper`: https://arxiv.org/abs/1712.09381\n.. _`Tune paper`: https://arxiv.org/abs/1807.05118\n\nGetting Involved\n----------------\n\n.. list-table::\n   :widths: 25 50 25 25\n   :header-rows: 1\n\n   * - Platform\n     - Purpose\n     - Estimated Response Time\n     - Support Level\n   * - `Discourse Forum`_\n     - For discussions about development and questions about usage.\n     - < 1 day\n     - Community\n   * - `GitHub Issues`_\n     - For reporting bugs and filing feature requests.\n     - < 2 days\n     - Ray OSS Team\n   * - `Slack`_\n     - For collaborating with other Ray users.\n     - < 2 days\n     - Community\n   * - `StackOverflow`_\n     - For asking questions about how to use Ray.\n     - 3-5 days\n     - Community\n   * - `Meetup Group`_\n     - For learning about Ray projects and best practices.\n     - Monthly\n     - Ray DevRel\n   * - `Twitter`_\n     - For staying up-to-date on new features.\n     - Daily\n     - Ray DevRel\n\n.. _`Discourse Forum`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`Twitter`: https://x.com/raydistributed\n.. _`Slack`: https://www.ray.io/join-slack?utm_source=github&utm_medium=ray_readme&utm_campaign=getting_involved\n",
    "meta_json": "{\"language\":\"Python\",\"stars\":40237,\"forks\":6987,\"watchers\":40237,\"open_issues\":3207,\"topics\":[\"data-science\",\"deep-learning\",\"deployment\",\"distributed\",\"hyperparameter-optimization\",\"hyperparameter-search\",\"large-language-models\",\"llm\",\"llm-inference\",\"llm-serving\",\"machine-learning\",\"optimization\",\"parallel\",\"python\",\"pytorch\",\"ray\",\"reinforcement-learning\",\"rllib\",\"serving\",\"tensorflow\"],\"default_branch\":\"master\",\"size_kb\":647025,\"archived\":false,\"fork\":false,\"has_wiki\":false,\"has_pages\":false}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:ray-project:ray\",\"source_url\":\"https://github.com/ray-project/ray\"},{\"type\":\"has_code\",\"target_id\":\"github:ray-project:ray\",\"source_url\":\"https://github.com/ray-project/ray\"},{\"type\":\"has_code\",\"target_id\":\"github:ray-project:ray\",\"source_url\":\"https://github.com/ray-project/ray\"}]",
    "canonical_id": null,
    "license_spdx": "Apache-2.0",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "4e810944c5ced749b319403d13f5b309",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"github\",\"source_url\":\"https://github.com/ray-project/ray\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07834v1",
    "name": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "author": "Yi-Chuan Huang",
    "description": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervisi...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07834v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
    "meta_json": "{\"arxiv_id\":\"2512.07834v1\",\"authors\":[\"Yi-Chuan Huang\",\"Jiewen Chan\",\"Hao-Jen Chien\",\"Yu-Lun Liu\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:59:58Z\",\"updated_date\":\"2025-12-08T18:59:58Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "1d4e7ecae76da7ea387735354b90dcd5",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07834v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07833v1",
    "name": "Relational Visual Similarity",
    "author": "Thao Nguyen",
    "description": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on pe...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07833v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
    "meta_json": "{\"arxiv_id\":\"2512.07833v1\",\"authors\":[\"Thao Nguyen\",\"Sicheng Mo\",\"Krishna Kumar Singh\",\"Yilin Wang\",\"Jing Shi\",\"Nicholas Kolkin\",\"Eli Shechtman\",\"Yong Jae Lee\",\"Yuheng Li\"],\"categories\":[\"cs.CV\",\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:59:56Z\",\"updated_date\":\"2025-12-08T18:59:56Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c27356fbcc1c9f858192be037f820fb0",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07833v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07832v1",
    "name": "Do Generalisation Results Generalise?",
    "author": "Matteo Boglioni",
    "description": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evalua...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07832v1",
    "image_url": null,
    "type": "paper",
    "body_content": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.",
    "meta_json": "{\"arxiv_id\":\"2512.07832v1\",\"authors\":[\"Matteo Boglioni\",\"Andrea Sgobbi\",\"Gabriel Tavernini\",\"Francesco Rita\",\"Marius Mosbach\",\"Tiago Pimentel\"],\"categories\":[\"cs.CL\",\"cs.LG\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:59:51Z\",\"updated_date\":\"2025-12-08T18:59:51Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e771fcbba05ba9c3298956868faa6e7d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07832v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07831v1",
    "name": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "author": "Jiehui Huang",
    "description": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skelet...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07831v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
    "meta_json": "{\"arxiv_id\":\"2512.07831v1\",\"authors\":[\"Jiehui Huang\",\"Yuechen Zhang\",\"Xu He\",\"Yuan Gao\",\"Zhi Cen\",\"Bin Xia\",\"Yan Zhou\",\"Xin Tao\",\"Pengfei Wan\",\"Jiaya Jia\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:59:01Z\",\"updated_date\":\"2025-12-08T18:59:01Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:dvlab-research:UnityVideo\",\"source_url\":\"https://github.com/dvlab-research/UnityVideo\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "716adaafee7610a97ee3e0956f275205",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07831v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07829v1",
    "name": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "author": "Yuan Gao",
    "description": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent space...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07829v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",
    "meta_json": "{\"arxiv_id\":\"2512.07829v1\",\"authors\":[\"Yuan Gao\",\"Chen Chen\",\"Tianrong Chen\",\"Jiatao Gu\"],\"categories\":[\"cs.CV\",\"cs.AI\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:57:26Z\",\"updated_date\":\"2025-12-08T18:57:26Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "e1d5d2f99ac286eaa47a53811d609e60",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07829v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07828v1",
    "name": "The Adoption and Usage of AI Agents: Early Evidence from Perplexity",
    "author": "Jeremy Yang",
    "description": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:econ.GN"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07828v1",
    "image_url": null,
    "type": "paper",
    "body_content": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity &amp; Workflow and Learning &amp; Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.",
    "meta_json": "{\"arxiv_id\":\"2512.07828v1\",\"authors\":[\"Jeremy Yang\",\"Noah Yonack\",\"Kate Zyskowski\",\"Denis Yarats\",\"Johnny Ho\",\"Jerry Ma\"],\"categories\":[\"cs.LG\",\"econ.GN\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:56:10Z\",\"updated_date\":\"2025-12-08T18:56:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:econ.GN\",\"source_url\":\"https://arxiv.org/abs/econ.GN\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "9829925347d13613338c7688079256aa",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07828v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07827v1",
    "name": "An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning",
    "author": "Lukas Johannes Möller",
    "description": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platfo...",
    "tags": [
      "arxiv:cs.CR",
      "arxiv:cs.DC",
      "arxiv:cs.LG",
      "deep learning"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07827v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.",
    "meta_json": "{\"arxiv_id\":\"2512.07827v1\",\"authors\":[\"Lukas Johannes Möller\"],\"categories\":[\"cs.CR\",\"cs.DC\",\"cs.LG\"],\"primary_category\":\"cs.CR\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:55:26Z\",\"updated_date\":\"2025-12-08T18:55:26Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CR\",\"source_url\":\"https://arxiv.org/abs/cs.CR\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.DC\",\"source_url\":\"https://arxiv.org/abs/cs.DC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "e57fceef699c3dcfe8fe6eb9ea769639",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07827v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07826v1",
    "name": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing",
    "author": "Haoyang He",
    "description": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07826v1",
    "image_url": null,
    "type": "paper",
    "body_content": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.",
    "meta_json": "{\"arxiv_id\":\"2512.07826v1\",\"authors\":[\"Haoyang He\",\"Jie Wang\",\"Jiangning Zhang\",\"Zhucun Xue\",\"Xingyuan Bu\",\"Qiangpeng Yang\",\"Shilei Wen\",\"Lei Xie\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:55:07Z\",\"updated_date\":\"2025-12-08T18:55:07Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:lewandofskee:OpenVE.\",\"source_url\":\"https://github.com/lewandofskee/OpenVE.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "882d32a4af109aae1a452218ab8cac3b",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07826v1\",\"fetched_at\":\"2025-12-10T01:31:39.557Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07820v1",
    "name": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces",
    "author": "Prithila Angkan",
    "description": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subj...",
    "tags": [
      "arxiv:cs.HC",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07820v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.",
    "meta_json": "{\"arxiv_id\":\"2512.07820v1\",\"authors\":[\"Prithila Angkan\",\"Amin Jalali\",\"Paul Hungler\",\"Ali Etemad\"],\"categories\":[\"cs.HC\",\"cs.LG\"],\"primary_category\":\"cs.HC\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:54:11Z\",\"updated_date\":\"2025-12-08T18:54:11Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.HC\",\"source_url\":\"https://arxiv.org/abs/cs.HC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c282ed5a5f975663739529a1f66ad3f6",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07820v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07818v1",
    "name": "Provable Long-Range Benefits of Next-Token Prediction",
    "author": "Xinyuan Cao",
    "description": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from th...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "arxiv:stat.ML"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07818v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.",
    "meta_json": "{\"arxiv_id\":\"2512.07818v1\",\"authors\":[\"Xinyuan Cao\",\"Santosh S. Vempala\"],\"categories\":[\"cs.LG\",\"cs.AI\",\"stat.ML\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:51:54Z\",\"updated_date\":\"2025-12-08T18:51:54Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "dba509222c47258062ed5a1f06c0cc49",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07818v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07814v1",
    "name": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach",
    "author": "Hua Yang",
    "description": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being ...",
    "tags": [
      "arxiv:cs.SE",
      "arxiv:cs.AI",
      "arxiv:cs.CR"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07814v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",
    "meta_json": "{\"arxiv_id\":\"2512.07814v1\",\"authors\":[\"Hua Yang\",\"Alejandro Velasco\",\"Sen Fang\",\"Bowen Xu\",\"Denys Poshyvanyk\"],\"categories\":[\"cs.SE\",\"cs.AI\",\"cs.CR\"],\"primary_category\":\"cs.SE\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:47:40Z\",\"updated_date\":\"2025-12-08T18:47:40Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.SE\",\"source_url\":\"https://arxiv.org/abs/cs.SE\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CR\",\"source_url\":\"https://arxiv.org/abs/cs.CR\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "1163d9c0bbfa3f6960391fb0e0561e64",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07814v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07810v1",
    "name": "Auditing Games for Sandbagging",
    "author": "Jordan Taylor",
    "description": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate ...",
    "tags": [
      "arxiv:cs.AI"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07810v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .",
    "meta_json": "{\"arxiv_id\":\"2512.07810v1\",\"authors\":[\"Jordan Taylor\",\"Sid Black\",\"Dillon Bowen\",\"Thomas Read\",\"Satvik Golechha\",\"Alex Zelenka-Martin\",\"Oliver Makins\",\"Connor Kissane\",\"Kola Ayonrinde\",\"Jacob Merizian\",\"Samuel Marks\",\"Chris Cundy\",\"Joseph Bloom\"],\"categories\":[\"cs.AI\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:44:44Z\",\"updated_date\":\"2025-12-08T18:44:44Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:AI-Safety-Institute:sandbagging_auditing_games\",\"source_url\":\"https://github.com/AI-Safety-Institute/sandbagging_auditing_games\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "afc7f6f747cc0d36f1c6a317d5ccda7d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07810v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07808v1",
    "name": "LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout",
    "author": "M. A. Farooq",
    "description": "Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and effic...",
    "tags": [
      "arxiv:quant-ph",
      "arxiv:cs.LG",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07808v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.",
    "meta_json": "{\"arxiv_id\":\"2512.07808v1\",\"authors\":[\"M. A. Farooq\",\"G. Di Guglielmo\",\"A. Rajagopala\",\"N. Tran\",\"V. A. Chhabria\",\"A. Arora\"],\"categories\":[\"quant-ph\",\"cs.LG\"],\"primary_category\":\"quant-ph\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:41:13Z\",\"updated_date\":\"2025-12-08T18:41:13Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:quant-ph\",\"source_url\":\"https://arxiv.org/abs/quant-ph\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c6edfe435733845eb3a20553802f4e50",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07808v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07807v1",
    "name": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes",
    "author": "Shai Krakovsky",
    "description": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distill...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.GR",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07807v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.",
    "meta_json": "{\"arxiv_id\":\"2512.07807v1\",\"authors\":[\"Shai Krakovsky\",\"Gal Fiebelman\",\"Sagie Benaim\",\"Hadar Averbuch-Elor\"],\"categories\":[\"cs.CV\",\"cs.GR\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:39:58Z\",\"updated_date\":\"2025-12-08T18:39:58Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.GR\",\"source_url\":\"https://arxiv.org/abs/cs.GR\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c34f896afdf7ce07a35a32bfe34ed2a8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07807v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07806v1",
    "name": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "author": "Gyeongjin Kang",
    "description": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-...",
    "tags": [
      "arxiv:cs.CV",
      "transformer"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07806v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
    "meta_json": "{\"arxiv_id\":\"2512.07806v1\",\"authors\":[\"Gyeongjin Kang\",\"Seungkwon Yang\",\"Seungtae Nam\",\"Younggeun Lee\",\"Jungwoo Kim\",\"Eunbyung Park\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:39:27Z\",\"updated_date\":\"2025-12-08T18:39:27Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "388f29d1e4739ea2817d1c25a1f5426c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07806v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07805v1",
    "name": "Group Representational Position Encoding",
    "author": "Yifan Zhang",
    "description": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.AI",
      "arxiv:cs.CL"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07805v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",
    "meta_json": "{\"arxiv_id\":\"2512.07805v1\",\"authors\":[\"Yifan Zhang\",\"Zixiang Chen\",\"Yifeng Liu\",\"Zhen Qin\",\"Huizhuo Yuan\",\"Kangping Xu\",\"Yang Yuan\",\"Quanquan Gu\",\"Andrew Chi-Chih Yao\"],\"categories\":[\"cs.LG\",\"cs.AI\",\"cs.CL\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:39:13Z\",\"updated_date\":\"2025-12-08T18:39:13Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:model-architectures:GRAPE.\",\"source_url\":\"https://github.com/model-architectures/GRAPE.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "d8c60fab11fde77c02880c8999100d49",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07805v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07802v1",
    "name": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
    "author": "Zhaochong An",
    "description": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot contex...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07802v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
    "meta_json": "{\"arxiv_id\":\"2512.07802v1\",\"authors\":[\"Zhaochong An\",\"Menglin Jia\",\"Haonan Qiu\",\"Zijian Zhou\",\"Xiaoke Huang\",\"Zhiheng Liu\",\"Weiming Ren\",\"Kumara Kahatapitiya\",\"Ding Liu\",\"Sen He\",\"Chenyang Zhang\",\"Tao Xiang\",\"Fanny Yang\",\"Serge Belongie\",\"Tian Xie\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:32:24Z\",\"updated_date\":\"2025-12-08T18:32:24Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c5c0d584768439b80aaf008d611f889d",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07802v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07801v1",
    "name": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support",
    "author": "Raunak Jain",
    "description": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where me...",
    "tags": [
      "arxiv:cs.CL",
      "arxiv:cs.AI",
      "arxiv:cs.HC",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07801v1",
    "image_url": null,
    "type": "paper",
    "body_content": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.",
    "meta_json": "{\"arxiv_id\":\"2512.07801v1\",\"authors\":[\"Raunak Jain\",\"Mudita Khurana\"],\"categories\":[\"cs.CL\",\"cs.AI\",\"cs.HC\",\"cs.LG\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:30:41Z\",\"updated_date\":\"2025-12-08T18:30:41Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.HC\",\"source_url\":\"https://arxiv.org/abs/cs.HC\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 70,
    "content_hash": "204dc38b794e42fc113fb3f1dffdee12",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07801v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07796v1",
    "name": "Large Causal Models from Large Language Models",
    "author": "Sridhar Mahadevan",
    "description": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is metho...",
    "tags": [
      "arxiv:cs.AI",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07796v1",
    "image_url": null,
    "type": "paper",
    "body_content": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.",
    "meta_json": "{\"arxiv_id\":\"2512.07796v1\",\"authors\":[\"Sridhar Mahadevan\"],\"categories\":[\"cs.AI\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:28:04Z\",\"updated_date\":\"2025-12-08T18:28:04Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 65,
    "content_hash": "531f7ebe36fcb1cfa3eefbab83bf874c",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07796v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07795v1",
    "name": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning",
    "author": "Nearchos Potamitis",
    "description": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.CL",
      "arxiv:cs.LG",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07795v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .",
    "meta_json": "{\"arxiv_id\":\"2512.07795v1\",\"authors\":[\"Nearchos Potamitis\",\"Lars Klein\",\"Akhil Arora\"],\"categories\":[\"cs.AI\",\"cs.CL\",\"cs.LG\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:26:58Z\",\"updated_date\":\"2025-12-08T18:26:58Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:au-clan:ReasonBench\",\"source_url\":\"https://github.com/au-clan/ReasonBench\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "726865b3b29a194d363bad1d81714df8",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07795v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07783v1",
    "name": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "author": "Charlie Zhang",
    "description": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguit...",
    "tags": [
      "arxiv:cs.CL",
      "language"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07783v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.",
    "meta_json": "{\"arxiv_id\":\"2512.07783v1\",\"authors\":[\"Charlie Zhang\",\"Graham Neubig\",\"Xiang Yue\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:12:10Z\",\"updated_date\":\"2025-12-08T18:12:10Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "d994a1def15b9793436a9efab09cfa03",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07783v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07782v1",
    "name": "GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory",
    "author": "Jiaxu Liu",
    "description": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanish...",
    "tags": [
      "arxiv:cs.LG",
      "attention"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07782v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\\underline{Gated} (\\underline{F}lash) \\underline{W}indowed \\underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.",
    "meta_json": "{\"arxiv_id\":\"2512.07782v1\",\"authors\":[\"Jiaxu Liu\",\"Yuhe Bai\",\"Christos-Savvas Bouganis\"],\"categories\":[\"cs.LG\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T18:11:06Z\",\"updated_date\":\"2025-12-08T18:11:06Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "5c1c36ff1097e95225f17cb2fe822c79",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07782v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07777v1",
    "name": "Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?",
    "author": "Karin de Langis",
    "description": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasonin...",
    "tags": [
      "arxiv:cs.CL",
      "llm"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07777v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.",
    "meta_json": "{\"arxiv_id\":\"2512.07777v1\",\"authors\":[\"Karin de Langis\",\"Püren Öncel\",\"Ryan Peters\",\"Andrew Elfenbein\",\"Laura Kristen Allen\",\"Andreas Schramm\",\"Dongyeop Kang\"],\"categories\":[\"cs.CL\"],\"primary_category\":\"cs.CL\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:58:43Z\",\"updated_date\":\"2025-12-08T17:58:43Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CL\",\"source_url\":\"https://arxiv.org/abs/cs.CL\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "c59964cc8722dc4d76e2410fa812c040",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07777v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07776v1",
    "name": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring",
    "author": "Maximilian Schall",
    "description": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primat...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07776v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species",
    "meta_json": "{\"arxiv_id\":\"2512.07776v1\",\"authors\":[\"Maximilian Schall\",\"Felix Leonard Knöfel\",\"Noah Elias König\",\"Jan Jonas Kubeler\",\"Maximilian von Klinski\",\"Joan Wilhelm Linnemann\",\"Xiaoshi Liu\",\"Iven Jelle Schlegelmilch\",\"Ole Woyciniuk\",\"Alexandra Schild\",\"Dante Wasmuht\",\"Magdalena Bermejo Espinet\",\"German Illera Basas\",\"Gerard de Melo\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:58:20Z\",\"updated_date\":\"2025-12-08T17:58:20Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "d5b5f9663177a8bcf70e307a9deaac54",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07776v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07770v1",
    "name": "Distribution-informed Online Conformal Prediction",
    "author": "Dongjian Hu",
    "description": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Throu...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.LG"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07770v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.",
    "meta_json": "{\"arxiv_id\":\"2512.07770v1\",\"authors\":[\"Dongjian Hu\",\"Junxi Wu\",\"Shu-Tao Xia\",\"Changliang Zou\"],\"categories\":[\"stat.ML\",\"cs.LG\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:51:49Z\",\"updated_date\":\"2025-12-08T17:51:49Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "4966125e448a0d1a6b5acff52956a5b3",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07770v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07766v1",
    "name": "Formalized Hopfield Networks and Boltzmann Machines",
    "author": "Matteo Cipollina",
    "description": "Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We the...",
    "tags": [
      "arxiv:cs.LG",
      "arxiv:cs.LO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07766v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.",
    "meta_json": "{\"arxiv_id\":\"2512.07766v1\",\"authors\":[\"Matteo Cipollina\",\"Michail Karatarakis\",\"Freek Wiedijk\"],\"categories\":[\"cs.LG\",\"cs.LO\"],\"primary_category\":\"cs.LG\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:48:31Z\",\"updated_date\":\"2025-12-08T17:48:31Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LO\",\"source_url\":\"https://arxiv.org/abs/cs.LO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 75,
    "content_hash": "86926a9f2151f0e2e7f0a17f6861b463",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07766v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07761v1",
    "name": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models",
    "author": "Xiqiao Xiong",
    "description": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinfor...",
    "tags": [
      "arxiv:cs.AI",
      "arxiv:cs.LG",
      "language",
      "reinforcement"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07761v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.",
    "meta_json": "{\"arxiv_id\":\"2512.07761v1\",\"authors\":[\"Xiqiao Xiong\",\"Ouxiang Li\",\"Zhuo Liu\",\"Moxin Li\",\"Wentao Shi\",\"Fuli Feng\",\"Xiangnan He\"],\"categories\":[\"cs.AI\",\"cs.LG\"],\"primary_category\":\"cs.AI\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:42:59Z\",\"updated_date\":\"2025-12-08T17:42:59Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:xxiqiao:RL-MTJail.\",\"source_url\":\"https://github.com/xxiqiao/RL-MTJail.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.AI\",\"source_url\":\"https://arxiv.org/abs/cs.AI\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "4d30dd0f225810764a301d4847a48383",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07761v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07760v1",
    "name": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification",
    "author": "Menglin Wang",
    "description": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-...",
    "tags": [
      "arxiv:cs.CV"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07760v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.",
    "meta_json": "{\"arxiv_id\":\"2512.07760v1\",\"authors\":[\"Menglin Wang\",\"Xiaojin Gong\",\"Jiachen Li\",\"Genlin Ji\"],\"categories\":[\"cs.CV\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:42:28Z\",\"updated_date\":\"2025-12-08T17:42:28Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "4af2e4a421eaf512d49735e7765dc9aa",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07760v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07756v1",
    "name": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction",
    "author": "Mayank Anand",
    "description": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow strea...",
    "tags": [
      "arxiv:cs.CV",
      "arxiv:cs.RO"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07756v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.",
    "meta_json": "{\"arxiv_id\":\"2512.07756v1\",\"authors\":[\"Mayank Anand\",\"Ujair Alam\",\"Surya Prakash\",\"Priya Shukla\",\"Gora Chand Nandi\",\"Domenec Puig\"],\"categories\":[\"cs.CV\",\"cs.RO\"],\"primary_category\":\"cs.CV\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:39:34Z\",\"updated_date\":\"2025-12-08T17:39:34Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"has_code\",\"target_id\":\"github:AnandMayank:UltrasODM.\",\"source_url\":\"https://github.com/AnandMayank/UltrasODM.\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.CV\",\"source_url\":\"https://arxiv.org/abs/cs.CV\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.RO\",\"source_url\":\"https://arxiv.org/abs/cs.RO\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "86724cfb2d769add68b3b7e6a9f100ce",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07756v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  },
  {
    "id": "arxiv:2512.07755v1",
    "name": "Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion",
    "author": "Brenda Anague",
    "description": "Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarc...",
    "tags": [
      "arxiv:stat.ML",
      "arxiv:cs.LG",
      "neural"
    ],
    "pipeline_tag": "other",
    "likes": 0,
    "downloads": 0,
    "source": "arxiv",
    "source_url": "https://arxiv.org/abs/2512.07755v1",
    "image_url": null,
    "type": "paper",
    "body_content": "Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.",
    "meta_json": "{\"arxiv_id\":\"2512.07755v1\",\"authors\":[\"Brenda Anague\",\"Bamdad Hosseini\",\"Issa Karambal\",\"Jean Medard Ngnotchouye\"],\"categories\":[\"stat.ML\",\"cs.LG\"],\"primary_category\":\"stat.ML\",\"pdf_url\":null,\"published_date\":\"2025-12-08T17:38:49Z\",\"updated_date\":\"2025-12-08T17:38:49Z\"}",
    "assets_json": "[]",
    "relations_json": "[{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:stat.ML\",\"source_url\":\"https://arxiv.org/abs/stat.ML\"},{\"type\":\"based_on_paper\",\"target_id\":\"arxiv:cs.LG\",\"source_url\":\"https://arxiv.org/abs/cs.LG\"}]",
    "canonical_id": null,
    "license_spdx": "arXiv",
    "compliance_status": "approved",
    "quality_score": 80,
    "content_hash": "f9df4b2ce006c305869788b4f3de81ce",
    "velocity": null,
    "raw_image_url": null,
    "source_trail": "[{\"source_platform\":\"arxiv\",\"source_url\":\"https://arxiv.org/abs/2512.07755v1\",\"fetched_at\":\"2025-12-10T01:31:39.558Z\",\"adapter_version\":\"3.2.0\"}]",
    "commercial_slots": null,
    "notebooklm_summary": null,
    "velocity_score": 0,
    "last_commercial_at": null
  }
]