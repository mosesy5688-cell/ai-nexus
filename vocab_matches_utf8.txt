3411:    "readme": "# llm.c\n\nLLMs in simple, pure C/CUDA with no need for 245MB of PyTorch or 107MB of cPython. Current focus is on pretraining, in particular reproducing the [GPT-2](https://github.com/openai/gpt-2) and [GPT-3](https://arxiv.org/abs/2005.14165) miniseries, along with a parallel PyTorch reference implementation in [train_gpt2.py](train_gpt2.py). You'll recognize this file as a slightly tweaked [nanoGPT](https://github.com/karpathy/nanoGPT), an earlier project of mine. Currently, llm.c is a bit faster than PyTorch Nightly (by about 7%). In addition to the bleeding edge mainline code in [train_gpt2.cu](train_gpt2.cu), we have a simple reference CPU fp32 implementation in ~1,000 lines of clean code in one file [train_gpt2.c](train_gpt2.c). I'd like this repo to only maintain C and CUDA code. Ports to other languages or repos are very welcome, but should be done in separate repos, and I am happy to link to them below in the \"notable forks\" section. Developer coordination happens in the [Discussions](https://github.com/karpathy/llm.c/discussions) and on Discord, either the `#llmc` channel on the [Zero to Hero](https://discord.gg/3zy8kqD9Cp) channel, or on `#llmdotc` on [GPU MODE](https://discord.gg/gpumode) Discord.\n\n## quick start\n\nThe best introduction to the llm.c repo today is reproducing the GPT-2 (124M) model. [Discussion #481](https://github.com/karpathy/llm.c/discussions/481) steps through this in detail. We can reproduce other models from the GPT-2 and GPT-3 series in both llm.c and in the parallel implementation of PyTorch. Have a look at the [scripts README](scripts/README.md).\n\ndebugging tip: when you run the `make` command to build the binary, modify it by replacing `-O3` with `-g` so you can step through the code in your favorite IDE (e.g. vscode).\n\n## quick start (1 GPU, fp32 only)\n\nIf you won't be training on multiple nodes, aren't interested in mixed precision, and are interested in learning CUDA, the fp32 (legacy) files might be of interest to you. These are files that were \"checkpointed\" early in the history of llm.c and frozen in time. They are simpler, more portable, and possibly easier to understand. Run the 1 GPU, fp32 code like this:\n\n```bash\nchmod u+x ./dev/download_starter_pack.sh\n./dev/download_starter_pack.sh\nmake train_gpt2fp32cu\n./train_gpt2fp32cu\n```\n\nThe download_starter_pack.sh script is a quick & easy way to get started and it downloads a bunch of .bin files that help get you off the ground. These contain: 1) the GPT-2 124M model saved in fp32, in bfloat16, 2) a \"debug state\" used in unit testing (a small batch of data, and target activations and gradients), 3) the GPT-2 tokenizer, and 3) the tokenized [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset. Alternatively, instead of running the .sh script, you can re-create these artifacts manually as follows:\n\n```bash\npip install -r requirements.txt\npython dev/data/tinyshakespeare.py\npython train_gpt2.py\n```\n\n## quick start (CPU)\n\nThe \"I am so GPU poor that I don't even have one GPU\" section. You can still enjoy seeing llm.c train! But you won't go too far. Just like the fp32 version above, the CPU version is an even earlier checkpoint in the history of llm.c, back when it was just a simple reference implementation in C. For example, instead of training from scratch, you can finetune a GPT-2 small (124M) to output Shakespeare-like text, as an example:\n\n```bash\nchmod u+x ./dev/download_starter_pack.sh\n./dev/download_starter_pack.sh\nmake train_gpt2\nOMP_NUM_THREADS=8 ./train_gpt2\n```\n\nIf you'd prefer to avoid running the starter pack script, then as mentioned in the previous section you can reproduce the exact same .bin files and artifacts by running `python dev/data/tinyshakespeare.py` and then `python train_gpt2.py`.\n\nThe above lines (1) download an already tokenized [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset and download the GPT-2 (124M) weights, (3) init from them in C and train for 40 steps on tineshakespeare with AdamW (using batch size 4, context length only 64), evaluate validation loss, and sample some text. Honestly, unless you have a beefy CPU (and can crank up the number of OMP threads in the launch command), you're not going to get that far on CPU training LLMs, but it might be a good demo/reference. The output looks like this on my MacBook Pro (Apple Silicon M3 Max):\n\n```\n[GPT-2]\nmax_seq_len: 1024\nvocab_size: 50257\nnum_layers: 12\nnum_heads: 12\nchannels: 768\nnum_parameters: 124439808\ntrain dataset num_batches: 1192\nval dataset num_batches: 128\nnum_activations: 73323776\nval loss 5.252026\nstep 0: train loss 5.356189 (took 1452.121000 ms)\nstep 1: train loss 4.301069 (took 1288.673000 ms)\nstep 2: train loss 4.623322 (took 1369.394000 ms)\nstep 3: train loss 4.600470 (took 1290.761000 ms)\n... (trunctated) ...\nstep 39: train loss 3.970751 (took 1323.779000 ms)\nval loss 4.107781\ngenerating:\n---\nCome Running Away,\nGreater conquer\nWith the Imperial blood\nthe heaviest host of the gods\ninto this wondrous world beyond.\nI will not back thee, for how sweet after birth\nNetflix against repounder,\nwill not\nflourish against the earlocks of\nAllay\n---\n```\n\n## datasets\n\nThe data files inside `/dev/data/(dataset).py` are responsible for downloading, tokenizing and saving the tokens to .bin files, readable easily from C. So for example when you run:\n\n```bash\npython dev/data/tinyshakespeare.py\n```\n\nWe download and tokenize the [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset. The output of this looks like this:\n\n```\nwriting 32,768 tokens to ./dev/data/tinyshakespeare/tiny_shakespeare_val.bin\nwriting 305,260 tokens to ./dev/data/tinyshakespeare/tiny_shakespeare_train.bin\n```\n\nThe .bin files contain a short header (1024 bytes) and then a stream of tokens in uint16, indicating the token ids with the GPT-2 tokenizer. More datasets are available in `/dev/data`.\n\n## test\n\nI am also attaching a simple unit test for making sure our C code agrees with the PyTorch code. On the CPU as an example, compile and run with:\n\n```bash\nmake test_gpt2\n./test_gpt2\n```\n\nThis now loads the `gpt2_124M_debug_state.bin` file that gets written by train_gpt2.py, runs a forward pass, compares the logits and loss with the PyTorch reference implementation, then it does 10 iterations of training with Adam and makes sure the losses match PyTorch. To test the GPU version we run:\n\n```bash\n# fp32 test (cudnn not supported)\nmake test_gpt2cu PRECISION=FP32 && ./test_gpt2cu\n# mixed precision cudnn test\nmake test_gpt2cu USE_CUDNN=1 && ./test_gpt2cu\n```\n\nThis tests both the fp32 path and the mixed precision path. The test should pass and print `overall okay: 1`.\n\n## tutorial\n\nI attached a very small tutorial here, in [doc/layernorm/layernorm.md](doc/layernorm/layernorm.md). It's a simple, step-by-step guide to implementing a single layer of the GPT-2 model, the layernorm layer. This is a good starting point to understand how the layers are implemented in C.\n\n**flash attention**. As of May 1, 2024 we use the Flash Attention from cuDNN. Because cuDNN bloats the compile time from a few seconds to ~minute and this code path is right now very new, this is disabled by default. You can enable it by compiling like this:\n\n```bash\nmake train_gpt2cu USE_CUDNN=1\n```\n\nThis will try to compile with cudnn and run it. You have to have cuDNN installed on your system. The [cuDNN installation instructions](https://developer.nvidia.com/cudnn) with apt-get will grab the default set of cuDNN packages. For a minimal setup, the cuDNN dev package is sufficient, e.g. on Ubuntu 22.04 for CUDA 12.x:\n\n```bash\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install libcudnn9-dev-cuda-12\n```\n\nOn top of this you need the [cuDNN frontend](https://github.com/NVIDIA/cudnn-frontend/tree/main), but this is just header files. Simply clone the repo to your disk. The Makefile currently looks for it in either your home directory or the current directory. If you have put it elsewhere, add `CUDNN_FRONTEND_PATH=/path/to/your/cudnn-frontend/include` to the `make` command-line.\n\n## multi-GPU training\n\nMake sure you install MPI and NCCL, e.g. on Linux:\n\n```bash\nsudo apt install openmpi-bin openmpi-doc libopenmpi-dev\n```\n\nFor NCCL follow the instructions from the [official website](https://developer.nvidia.com/nccl/nccl-download) (e.g. network installer)\n\nand then:\n\n```bash\nmake train_gpt2cu\nmpirun -np <number of GPUs> ./train_gpt2cu\n```\n\nor simply run one of our scripts under `./scripts/`.\n\n## multi-node training\n\nMake sure you've installed `NCCL` following instructions from [multi-GPU](#multi-gpu-training) section.\n\nThere are 3 ways we currently support that allow you to run multi-node training:\n1) Use OpenMPI to exchange nccl id and initialize NCCL. See e.g. `./scripts/multi_node/run_gpt2_124M_mpi.sh` script for details.\n2) Use shared file system to init NCCL. See `./scripts/multi_node/run_gpt2_124M_fs.sbatch` script for details.\n3) Use TCP sockets to init NCCL. See `./scripts/multi_node/run_gpt2_124M_tcp.sbatch` script for details.\n\nNote:\n* If you're running in a slurm environment and your slurm doesn't support PMIx (which we assume will be a common situation given that `slurm-wlm` dropped PMIx support) you will have to use FS (2) or TCP (3) approach. To test whether your slurm supports PMIx run: `srun --mpi=list` and see whether you get `pmix` in the output.\n* If you don't have slurm set up, you can kick off a multi-node run using `mpirun` - MPI (1).\n\nNone of these 3 methods is superior, we just offer you options so that you can run in your specific environment.\n\n## experiments / sweeps\n\nJust as an example process to sweep learning rates on a machine with 4 GPUs on TinyStories. Run a shell script `sweep.sh` (after you of course `chmod u+x sweep.sh`):\n\n```bash\n#!/bin/bash\n\nlearning_rates=(3e-5 1e-4 3e-4 1e-3)\n\nfor i in {0..3}; do\n    export CUDA_VISIBLE_DEVICES=$i\n    screen -dmS \"tr$i\" bash -c \"./train_gpt2cu -i data/TinyStories -v 250 -s 250 -g 144 -l ${learning_rates[$i]} -o stories$i.log\"\ndone\n\n# you can bring these down with\n# screen -ls | grep -E \"tr[0-3]\" | cut -d. -f1 | xargs -I {} screen -X -S {} quit\n```\n\nThis example opens up 4 screen sessions and runs the four commands with different LRs. This writes the log files `stories$i.log` with all the losses, which you can plot as you wish in Python. A quick example of how to parse and plot these logfiles is in [dev/vislog.ipynb](dev/vislog.ipynb).\n\n## repo\n\nA few more words on what I want this repo to be:\n\nFirst, I want `llm.c` to be a place for education. E.g. our `dev/cuda` folder is a place for a library of kernels for all the layers that are manually hand-written and very well documented, starting from very simple kernels all the way to more complex / faster kernels. If you have a new kernel with various different tradeoffs, please feel free to contribute it here.\n\nThat said, I also want `llm.c` to be very fast too, even practically useful to train networks. E.g. to start, we should be able to reproduce the big GPT-2 (1.6B) training run. This requires that we incorporate whatever fastest kernels there are, including the use of libraries such as cuBLAS, cuBLASLt, CUTLASS, cuDNN, etc. I also think doing so serves an educational purpose to establish an expert upper bound, and a unit of measurement, e.g. you could say that your manually written kernels are 80% of cuBLAS speed, etc. Then you can choose to do a super fast run, or you can choose to \"drag and drop\" whatever manual kernels you wish to use, and run with those.\n\nHowever, as a constraint, I want to keep the mainline `llm.c` in the root folder simple and readable. If there is a PR that e.g. improves performance by 2% but it \"costs\" 500 lines of complex C code, and maybe an exotic 3rd party dependency, I may reject the PR because the complexity is not worth it. As a concrete example - making cuBLAS for matmuls the default in the root training loop is a no-brainer: it makes the mainline code much faster, it is a single line of interpretable code, and it is a very common dependency. On the side of this, we can have manual implementations that can compete with cuBLAS in `dev/cuda`.\n\nLastly, I will be a lot more sensitive to complexity in the root folder of the project, which contains the main / default files of the project. In comparison, the `dev/` folder is a bit more of a scratch space for us to develop a library of kernels or classes and share useful or related or educational code, and some of this code could be ok to be (locally) complex.\n\n## notable forks\n\n- AMD support\n  - [llm.c](https://github.com/anthonix/llm.c) by @[anthonix](https://github.com/anthonix): support for AMD devices, such as the 7900 XTX\n\n- C#\n  - [llm.cs](https://github.com/azret/llm.cs) by @[azret](https://github.com/azret): a C# port of this project\n  - [Llm.cs](https://github.com/nietras/Llm.cs) by @[nietras](https://github.com/nietras): a C# port of this project with focus on easy to get started on any platform. Clone and run 鉁匼n\n- CUDA C++\n  - [llm.cpp](https://github.com/gevtushenko/llm.c) by @[gevtushenko](https://github.com/gevtushenko): a port of this project using the [CUDA C++ Core Libraries](https://github.com/NVIDIA/cccl)\n     - A presentation this fork was covered in [this lecture](https://www.youtube.com/watch?v=WiB_3Csfj_Q) in the [GPU MODE Discord Server](https://discord.gg/cudamode)\n\n- C++/CUDA\n  - [llm.cpp](https://github.com/zhangpiu/llm.cpp/tree/master/llmcpp) by @[zhangpiu](https://github.com/zhangpiu): a port of this project using the [Eigen](https://gitlab.com/libeigen/eigen), supporting CPU/CUDA.\n\n- WebGPU C++\n  - [gpu.cpp](https://github.com/AnswerDotAI/gpu.cpp) by @[austinvhuang](https://github.com/austinvhuang): a library for portable GPU compute in C++ using native WebGPU. Aims to be a general-purpose library, but also porting llm.c kernels to WGSL.\n  \n- C++\n  - [llm.cpp](https://github.com/GaoYusong/llm.cpp) by @[GaoYusong](https://github.com/GaoYusong): a port of this project featuring a C++ single-header [tinytorch.hpp](https://github.com/GaoYusong/llm.cpp/blob/main/tinytorch.hpp) library\n\n- Go\n  - [llm.go](https://github.com/joshcarp/llm.go) by @[joshcarp](https://github.com/joshcarp): a Go port of this project\n\n- Java\n  - [llm.java](https://github.com/harryjackson/llm.java) by @[harryjackson](https://github.com/harryjackson): a Java port of this project\n\n- Metal\n  - [llm.metal](https://github.com/regrettable-username/llm.metal) by @[regrettable-username](https://github.com/regrettable-username): LLM training in simple, raw C/Metal Shading Language\n\n- Mojo\n  - [llm.馃敟](https://github.com/dorjeduck/llm.mojo) by @[dorjeduck](https://github.com/dorjeduck): a Mojo port of this project\n\n- OpenCL\n  - [llm.c](https://github.com/krrishnarraj/llm.c) by @[krrishnarraj](https://github.com/krrishnarraj): an OpenCL port of this project\n\n- Rust\n  -  [llm.rs](https://github.com/yijunyu/llm.rs) by @[Yijun Yu](https://github.com/yijunyu): a Rust rewrite with the aim to have same performance\n  -  [llm.rs](https://github.com/ToJen/llm.rs) by @[ToJen](https://github.com/ToJen): a Rust port of this project\n\n- Swift\n  - [llm.swift](https://github.com/otabuzzman/llm.swift) by @[otabuzzman](https://github.com/otabuzzman): a Swift port of this project\n\n- Zig\n  - [llm.zig](https://github.com/Saimirbaci/llm.zig) by @[saimirbaci](https://github.com/Saimirbaci): a Zig port of this project\n \n- Habana Gaudi2\n  - [llm.tpc](https://github.com/abhilash1910/llm.tpc) by @[abhilash1910](https://github.com/abhilash1910): a Habana Gaudi2 port of this project \n\n- Nim\n  - [llm.nim](https://github.com/Vindaar/llm.nim) by @[Vindaar](https://github.com/Vindaar): a Nim port of this project\n\n## discussions\n\nWays of organizing development:\n\n- Experiencing a concrete issue with the repo? Use [Issues](https://github.com/karpathy/llm.c/issues).\n- Have some code to contribute? Open a [PR](https://github.com/karpathy/llm.c/pulls)\n- Chat about the repo, ask questions, etc.? Look at [Discussions](https://github.com/karpathy/llm.c/discussions).\n- Something faster? I created a new `#llmc` channel on my [Zero to Hero Discord channel](https://discord.gg/3zy8kqD9Cp).\n\n## license\n\nMIT\n",
9135:    "readme": "# RWKV: Parallelizable RNN with Transformer-level LLM Performance (pronounced as \"RwaKuv\" (r蕦kuv in IPA), from 4 major params: R W K V)\n\nRWKV website: https://rwkv.com (with 90+ RWKV-related papers)\n\nRWKV twitter: https://twitter.com/BlinkDL_AI (lastest news)\n\nRWKV discord: https://discord.gg/bDSBUMeFpc (9k+ members)\n\nRWKV-7 \"Goose\" is the strongest **linear-time** & **constant-space** (no kv-cache) & **attention-free** & 100% RNN architecture on this planet at this moment, suitable for LLM and multimodal applications and more (see [rwkv.com](https://rwkv.com)).\n\n**Important**: Use PreLN LayerNorm (instead of RMSNorm) for RWKV. I think it's related to better initial state, because I am not using trainable initial state (found it useless when using LayerNorm).\n\nRWKV-7 is a [meta-in-context learner](https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-v7.png), test-time-training its state on the context via in-context gradient descent at every token.\n\nRWKV is a [Linux Foundation AI project](https://lfaidata.foundation/projects/rwkv/), so totally free. RWKV runtime is [already in Windows & Office](https://x.com/BlinkDL_AI/status/1831012419508019550).\n\nYou are welcome to ask the RWKV community (such as [RWKV discord](https://discord.gg/bDSBUMeFpc)) for advice on upgrading your attention/ssm models to rwkv7 models :)\n\n**Improving RNNs (RWKV-8 and beyond)**: https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-8.md\n\n**Efficient inference project**: https://github.com/BlinkDL/Albatross\n\n**RWKV APP**: https://github.com/RWKV-APP/RWKV_APP (local inference for Android / iOS)\n\n**Simplified RWKV-7 training demo**: https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/train_temp/rwkv7_train_simplified.py\n\n===\n\n**Please use https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7/train_temp as RWKV-7 reference implementation**. The default config only requires 1 GPU with 10G VRAM (you can reduce bsz if you have less VRAM), so it's easy to test.\n\nNote FLA RWKV-7 is NOT aligned with reference implementation yet, and you will get less performance.\n\nThis is because RWKV-7 is the whole model with carefully set stuffs, including different init / wd / lr for each parameter, so it's readily scalable and very stable (spike-free).\n\nBut the price to pay is there is no good simple \"RWKV-7 layer\" because a pytorch layer can't make sure itself is using correct init and hyperparameters.\n\nSo if you need to use RWKV-7 for another task, please study train_temp code (only several hundred lines) and change it to suit you.\n\n===\n\n<img src=\"RWKV-8-ROSA.png\">\n\n===\n\nRWKV-7 can do math. See https://github.com/BlinkDL/RWKV-LM/blob/main/Research/rwkv7-g0-7.2b.md for details.\n\n<img width=\"555\" height=\"784\" alt=\"image\" src=\"https://github.com/user-attachments/assets/095b4576-962f-4274-ae1a-855406ec76c1\" />\n\nHistory of RWKV (from v1 to v7): [https://wiki.rwkv.com](https://wiki.rwkv.com/) (note: AI-written. might contain errors)\n\n<img src=\"RWKV-v7-niah.png\">\n\nGradio Demo 1: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1\n\nGradio Demo 2: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2\n\nWebGPU Demo: https://cryscan.github.io/web-rwkv-puzzles/#/chat\n\nLatest RWKV weights: https://huggingface.co/BlinkDL\n\n===\n\nRWKV-Runner GUI: https://github.com/josStorer/RWKV-Runner/releases\n\nAi00 Server: https://github.com/Ai00-X/ai00_server\n\nRWKV pip pkg: https://pypi.org/project/rwkv/\n\nPEFT (Lora etc.): https://github.com/JL-er/RWKV-PEFT\n\nRLHF: https://github.com/OpenMOSE/RWKV-LM-RLHF\n\n400+ RWKV projects: https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories\n\n**Faster RWKV-7 kernels**: https://github.com/johanwind/wind_rwkv\n\n===\n\nRWKV-5/6 Eagle/Finch paper: https://arxiv.org/abs/2404.05892\n\nChat demo code: https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py\n\n**RWKV-7 demo code**: https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7\n\nhttps://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/rwkv_v7_demo.py (GPT-like mode)\n\nhttps://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/rwkv_v7_demo_rnn.py (RNN mode)\n\nhttps://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v7/rwkv_v7_demo_fast.py (Both mode, fastest)\n\nRWKV-6 demo code: https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/rwkv_v6_demo.py\n\nRWKV-6 demo code: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v6_demo.py\n\n## HOW TO TRAIN RWKV-7/6/5 on MiniPile (1.5G tokens) ##\n\nFor reference, use python 3.10+, torch 2.5+, cuda 12.4+, latest deepspeed, but **keep pytorch-lightning==1.9.5**\n\n### Note: seems deepspeed 0.17.x is buggy (worse loss or divergence). Use 0.16.8 for reference (maybe --layerwise_lr 0 can fix it)\n\n**Train RWKV-7:**\n```\n# you can use latest torch + latest cuda (not limited to cu121)\npip install torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu121\npip install pytorch-lightning==1.9.5 deepspeed wandb ninja --upgrade\n\n# train RWKV-7\ncd RWKV-v7/train_temp/ \n\n# download minipile .bin .idx to train_temp/data first (check demo-training-prepare.sh)\n# this will generate the initial weight rwkv-init.pth in out/....../\nsh ./demo-training-prepare.sh\n\n# this will load rwkv-init.pth and train the model. you may want to log in to wandb first\nsh ./demo-training-run.sh\n\nyour out/....../train_log.txt should have losses similar to:\n0 4.875856 131.0863 0.00059975 2025-04-24 02:23:42.481256 0\n1 4.028621 56.1834 0.00059899 2025-04-24 02:28:16.674463 1\n2 3.801625 44.7739 0.00059773 2025-04-24 02:32:51.059568 2\n3 3.663070 38.9808 0.00059597 2025-04-24 02:37:25.409892 3\n4 3.578974 35.8368 0.00059371 2025-04-24 02:41:59.711315 4\n5 3.510906 33.4786 0.00059096 2025-04-24 02:46:33.990839 5\n6 3.462345 31.8917 0.00058771 2025-04-24 02:51:08.378331 6\n7 3.412196 30.3318 0.00058399 2025-04-24 02:55:42.927474 7\n8 3.376724 29.2747 0.00057978 2025-04-24 03:00:17.504665 8\n9 3.336911 28.1321 0.00057511 2025-04-24 03:04:52.006063 9\n10 3.313411 27.4787 0.00056999 2025-04-24 03:09:27.563336 10\n11 3.295895 27.0016 0.00056441 2025-04-24 03:14:01.786079 11\n```\n\nRWKV-7 weight example for 1.5B (L24-D2048, vocab 65536):\n| name                | shape         | comment      | initialization  |\n|---------------------|---------------|--------------|-----------------|\n| emb.weight          | [65536, 2048] | wdecay       | see code        |\n| blocks.0.ln0.weight | [2048]        | for layer 0  | 1               |\n| blocks.0.ln0.bias   | [2048]        | for layer 0  | 0               |\n|                     |               |              |                 |\n| blocks.*.ln1.weight | [2048]        |              | 1               |\n| blocks.*.ln1.bias   | [2048]        |              | 0               |\n| blocks.*.att.x_r    | [1, 1, 2048]  |              | see code        |\n| blocks.*.att.x_w    | [1, 1, 2048]  |              | see code        |\n| blocks.*.att.x_k    | [1, 1, 2048]  |              | see code        |\n| blocks.*.att.x_v    | [1, 1, 2048]  |              | see code        |\n| blocks.*.att.x_a    | [1, 1, 2048]  |              | see code        |\n| blocks.*.att.x_g    | [1, 1, 2048]  |              | see code        |\n| blocks.*.att.w0     | [1, 1, 2048]  | lr 2x        | see code        |\n| blocks.*.att.w1     | [2048, 96]    |              | 0               |\n| blocks.*.att.w2     | [96, 2048]    |              | see code        |\n| blocks.*.att.a0     | [1, 1, 2048]  |              | 0               |\n| blocks.*.att.a1     | [2048, 96]    |              | 0               |\n| blocks.*.att.a2     | [96, 2048]    |              | see code        |\n| blocks.*.att.v0     | [1, 1, 2048]  | for layer 1+ | 1               |\n| blocks.*.att.v1                | [2048, 64]   | for layer 1+ | 0         |\n| blocks.*.att.v2                | [64, 2048]   | for layer 1+ | see code  |\n| blocks.*.att.g1                | [2048, 256]  |              | 0         |\n| blocks.*.att.g2                | [256, 2048]  |              | see code  |\n| blocks.*.att.k_k               | [1, 1, 2048] |              | 1         |\n| blocks.*.att.k_a               | [1, 1, 2048] |              | 1         |\n| blocks.*.att.r_k               | [32, 64]     |              | 0         |\n| blocks.*.att.receptance.weight | [2048, 2048] | wdecay       | see code  |\n| blocks.*.att.key.weight        | [2048, 2048] | wdecay       | see code  |\n| blocks.*.att.value.weight      | [2048, 2048] | wdecay       | see code  |\n| blocks.*.att.output.weight     | [2048, 2048] | wdecay       | 0         |\n| blocks.*.att.ln_x.weight       | [2048]       |              | see code  |\n| blocks.*.att.ln_x.bias         | [2048]       |              | 0         |\n|                                |              |              |           |\n| blocks.*.ln2.weight            | [2048]       |              | 1         |\n| blocks.*.ln2.bias              | [2048]       |              | 0         |\n| blocks.*.ffn.x_k               | [1, 1, 2048] |              | see code  |\n| blocks.*.ffn.key.weight        | [8192, 2048] | wdecay       | see code  |\n| blocks.*.ffn.value.weight      | [2048, 8192] | wdecay       | 0         |\n|                                |              |              |           |\n| ln_out.weight | [2048]        |        | 1         |\n| ln_out.bias   | [2048]        |        | 0         |\n| head.weight   | [65536, 2048] | wdecay | see code  |\n\nTrain RWKV-6: use /RWKV-v5/ and use --my_testing \"x060\" in demo-training-prepare.sh and demo-training-run.sh\n\nYour loss curve should look almost exactly the same as this, with the same ups and downs (if you use the same bsz & config):\n\n<img src=\"RWKV-v5-minipile.png\" width=\"500\">\n\nYou can run your model using https://pypi.org/project/rwkv/ (use \"rwkv_vocab_v20230424\" instead of \"20B_tokenizer.json\")\n\nUse https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/make_data.py to prepare binidx data from jsonl, and compute \"--my_exit_tokens\" and \"--magic_prime\".\n\nUse https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/compute_magic_prime.py to compute \"--my_exit_tokens\" and \"--magic_prime\" for existing binidx.\n\nMuch faster tokenizer of large data: https://github.com/cahya-wirawan/json2bin https://github.com/cahya-wirawan/rwkv-tokenizer https://github.com/m8than/RWKV-World-Tokenizer-CPP\n\nThe \"epoch\" in train.py is \"mini-epoch\" (not real epoch. only for convenience), and 1 mini-epoch = 40320 * ctx_len tokens.\n\nFor example, if your binidx has 1498226207 tokens and ctxlen=4096, set \"--my_exit_tokens 1498226207\" (this will override epoch_count), and it will be 1498226207/(40320 * 4096) = 9.07 miniepochs. The trainer will auto-exit after \"--my_exit_tokens\" tokens. Set \"--magic_prime\" to the largest 3n+2 prime smaller than datalen/ctxlen-1 (= 1498226207/4096-1 = 365776), which is \"--magic_prime 365759\" in this case.\n\nsimple: prepare SFT jsonl => repeat your SFT data 3 or 4 times in make_data.py. more repetition leads to overfitting.\n\nadvanced: repeat your SFT data 3 or 4 times in your jsonl (note make_data.py will shuffle all jsonl items) => add some base data (such as slimpajama) to your jsonl => and only repeat 1 times in make_data.py.\n\n**Fix training spikes**: see the \"Fixing RWKV-6 Spikes\" part on this page. \n\nOr use RWKV-7 (much better). RWKV-7 is very stable and spike-free (verified for 0.1/0.4/1.5/2.9b):\n<img src=\"RWKV-v7-loss.png\" width=\"500\">\n\n**Simple inference for RWKV-6**: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v6_demo.py\n\n**Simple inference for RWKV-5**: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v5_demo.py\n\n**Note: In [state = kv + w * state] everything must be in fp32 because w can be very close to 1. So we can keep state and w in fp32, and convert kv to fp32.**\n\nlm_eval: https://github.com/BlinkDL/ChatRWKV/blob/main/run_lm_eval.py\n\n**Tips for small model / small data**: When I train RWKV music models, I use deep & narrow (such as L29-D512) dimensions, and apply wd and dropout (such as wd=2 dropout=0.02). Note RWKV-LM dropout is very effective - use 1/4 of your usual value.\n\n## HOW TO TRAIN RWKV-7 on Pile (332G tokens) ##\n\nSee https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/demo-training-prepare-v7-pile.sh and https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/demo-training-run-v7-pile.sh\n\nGet these files first:\n\npile_20B_tokenizer_text_document.bin (664230651068 bytes)\n\npile_20B_tokenizer_text_document.idx (4212099722 bytes)\n\n### HOW TO FINETUNE RWKV-5 MODELS ###\n\nUse .jsonl format for your data (see https://huggingface.co/BlinkDL/rwkv-5-world for formats).\n\nUse https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/make_data.py to tokenizer it using World tokenizer into binidx, suitable for finetuning World models.\n\nRename the base checkpoint in your model folder to rwkv-init.pth, and change the training commands to use --n_layer 32 --n_embd 4096 --vocab_size 65536 --lr_init 1e-5 --lr_final 1e-5 for 7B.\n\n0.1B = --n_layer 12 --n_embd 768 // 0.4B = --n_layer 24 --n_embd 1024 // 1.5B = --n_layer 24 --n_embd 2048 // 3B = --n_layer 32 --n_embd 2560 // 7B = --n_layer 32 --n_embd 4096\n\n### State-tuning (tuning the initial state. zero inference overhead)\n\nCurrently unoptimized implementation, takes same vram as full SFT\n\n```--train_type \"states\" --load_partial 1 --lr_init 1 --lr_final 0.01 --warmup_steps 10 (yes, use very high LR)```\n\nuse rwkv 0.8.26+ to auto-load the trained \"time_state\" \n\n### Initializing RWKV 5/6 Models ###\n\nWhen you train RWKV from scratch, try my initialization for best performance. Check generate_init_weight() of src/model.py:\n```\nemb.weight => nn.init.uniform_(a=-1e-4, b=1e-4)\n(Note ln0 of block0 is the layernorm for emb.weight)\nhead.weight => nn.init.orthogonal_(gain=0.5*sqrt(n_vocab / n_embd))\n\natt.receptance.weight => nn.init.orthogonal_(gain=1)\natt.key.weight => nn.init.orthogonal_(gain=0.1)\natt.value.weight => nn.init.orthogonal_(gain=1)\natt.gate.weight => nn.init.orthogonal_(gain=0.1)\natt.output.weight => zero\n\natt.ln_x.weight (groupnorm) => ((1 + layer_id) / total_layers) ** 0.7\n\nffn.key.weight => nn.init.orthogonal_(gain=1)\nffn.value.weight => zero\nffn.receptance.weight => zero\n```\n!!! If you are using positional embedding, maybe it's better to remove block.0.ln0 and use default initialization for emb.weight instead of my uniform_(a=-1e-4, b=1e-4) !!!\n\n### Fixing RWKV-6 Spikes ###\n\n0. upgrade to RWKV-7. It's very stable.\n\n1. when training from scratch, add \"k = k * torch.clamp(w, max=0).exp()\" before \"RUN_CUDA_RWKV6(r, k, v, w, u)\", and remember to change your inference code too. you will see faster convergence.\n\n2. use \"--adam_eps 1e-18\"\n\n3. \"--beta2 0.95\" if you see spikes\n\n4. in trainer.py do \"lr = lr * (0.01 + 0.99 * trainer.global_step / w_step)\" (originally 0.2 + 0.8), and \"--warmup_steps 20\"\n\n5. \"--weight_decay 0.1\" leads to better final loss if you are training lots of data. set lr_final to 1/100 of lr_init when doing this.\n\n## Introducing RWKV\n\nRWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1. You can use the \"GPT\" mode to quickly compute the hidden state for the \"RNN\" mode.\n\nSo it's combining the best of RNN and transformer - **great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding** (using the final hidden state).\n\n**All latest RWKV weights:** https://huggingface.co/BlinkDL\n\n**HF-compatible RWKV weights:** https://huggingface.co/RWKV\n\n```python\nos.environ[\"RWKV_JIT_ON\"] = '1'\nos.environ[\"RWKV_CUDA_ON\"] = '0' # if '1' then use CUDA kernel for seq mode (much faster)\nfrom rwkv.model import RWKV                         # pip install rwkv\nmodel = RWKV(model='/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040', strategy='cuda fp16')\n\nout, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json\nprint(out.detach().cpu().numpy())                   # get logits\nout, state = model.forward([187, 510], None)\nout, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)\nout, state = model.forward([310, 247], state)\nprint(out.detach().cpu().numpy())                   # same result as above\n```\n\nnanoRWKV: https://github.com/BlinkDL/nanoRWKV (does not require custom CUDA kernel to train, works for any GPU/CPU)\n\n**Cool Community RWKV Projects**:\n\nAll (400+) RWKV projects: https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories\n\nhttps://github.com/OpenGVLab/Vision-RWKV Vision RWKV\n\nhttps://github.com/feizc/Diffusion-RWKV Diffusion RWKV\n\nhttps://github.com/cgisky1980/ai00_rwkv_server Fastest WebGPU inference (nVidia/AMD/Intel)\n\nhttps://github.com/cryscan/web-rwkv backend for ai00_rwkv_server\n\nhttps://github.com/saharNooby/rwkv.cpp Fast CPU/cuBLAS/CLBlast inference: int4/int8/fp16/fp32\n\nhttps://github.com/JL-er/RWKV-PEFT lora/pissa/Qlora/Qpissa/state tuning\n\nhttps://github.com/RWKV/RWKV-infctx-trainer Infctx trainer\n\nhttps://github.com/daquexian/faster-rwkv\n\nhttps://github.com/mlc-ai/mlc-llm/pull/1275\n\nhttps://github.com/TheRamU/Fay/blob/main/README_EN.md Digital Assistant with RWKV\n\nhttps://github.com/harrisonvanderbyl/rwkv-cpp-cuda Fast GPU inference with cuda/amd/vulkan\n\n**RWKV v6 in 250 lines** (with tokenizer too): https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v6_demo.py\n\n**RWKV v5 in 250 lines** (with tokenizer too): https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v5_demo.py\n\n**RWKV v4 in 150 lines** (model, inference, text generation): https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\n\n**RWKV v4 preprint** https://arxiv.org/abs/2305.13048\n\n**RWKV v4 introduction, and in 100 lines of numpy**: https://johanwind.github.io/2023/03/23/rwkv_overview.html https://johanwind.github.io/2023/03/23/rwkv_details.html\n\n![RWKV-7](RWKV-v7.png)\n\n![MQAR](Research/RWKV-6-MQAR.png)\n\n![RWKV-paper](RWKV-paper.png)\n\nRWKV v6 illustrated:\n\n![RWKV-v6](rwkv-x060.png)\n\n![RWKV-v5-benchmark-1](RWKV-v5-benchmark-1.png)\n\nA cool paper (Spiking Neural Network) using RWKV: https://github.com/ridgerchu/SpikeGPT\n\nYou are welcome to join the RWKV discord https://discord.gg/bDSBUMeFpc to build upon it. We have plenty of potential compute (A100 40Gs) now (thanks to Stability and EleutherAI), so if you have interesting ideas I can run them.\n\n![RWKV-eval2](RWKV-eval2.png)\n\nRWKV [loss vs token position] for 10000 ctx4k+ documents in Pile. RWKV 1B5-4k is mostly flat after ctx1500, but 3B-4k and 7B-4k and 14B-4k have some slopes, and they are getting better. This debunks the old view that RNNs cannot model long ctxlens. We can predict that RWKV 100B will be great, and RWKV 1T is probably all you need :)\n\n![RWKV-ctxlen](RWKV-ctxlen.png)\n\nChatRWKV with RWKV 14B ctx8192:\n\n![RWKV-chat](RWKV-chat.png)\n\nI believe RNN is a better candidate for fundamental models, because: (1) It's more friendly for ASICs (no kv cache). (2) It's more friendly for RL. (3) When we write, our brain is more similar to RNN. (4) The universe is like an RNN too (because of locality). Transformers are non-local models.\n\nRWKV-3 1.5B on A40 (tf32) = always 0.015 sec/token, tested using simple pytorch code (no CUDA), GPU utilization 45%, VRAM 7823M\n\nGPT2-XL 1.3B on A40 (tf32) = 0.032 sec/token (for ctxlen 1000), tested using HF, GPU utilization 45% too (interesting), VRAM 9655M\n\nTraining speed: (new training code) RWKV-4 14B BF16 ctxlen4096 = 114K tokens/s on 8x8 A100 80G (ZERO2+CP). (old training code) RWKV-4 1.5B BF16 ctxlen1024 = 106K tokens/s on 8xA100 40G.\n\nI am doing image experiments too (For example: https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder) and RWKV will be able to do txt2img diffusion :) My idea: 256x256 rgb image -> 32x32x13bit latents -> apply RWKV to compute transition probability for each of the 32x32 grid -> pretend the grids are independent and \"diffuse\" using these probabilities.\n\nSmooth training - no loss spikes! (lr & bsz change around 15G tokens)\n![RWKV-loss](RWKV-loss.png)\n\n![RWKV-eval](RWKV-eval.png)\n\nAll of the trained models will be open-source. Inference is very fast (only matrix-vector multiplications, no matrix-matrix multiplications) even on CPUs, so you can even run a LLM on your phone.\n\nHow it works: RWKV gathers information to a number of channels, which are also decaying with different speeds as you move to the next token. It's very simple once you understand it.\n\n**RWKV is parallelizable because the time-decay of each channel is data-independent (and trainable)**. For example, in usual RNN you can adjust the time-decay of a channel from say 0.8 to 0.5 (these are called \"gates\"), while in RWKV you simply move the information from a W-0.8-channel to a W-0.5-channel to achieve the same effect. Moreover, you can fine-tune RWKV into a non-parallelizable RNN (then you can use outputs of later layers of the previous token) if you want extra performance.\n\n![RWKV-formula](RWKV-formula.png)\n\nHere are some of my TODOs. Let's work together :)\n\n* HuggingFace integration (check https://github.com/huggingface/transformers/issues/17230\n), and optimized CPU & iOS & Android & WASM & WebGL inference. RWKV is a RNN and very friendly for edge devices. Let's make it possible to run a LLM on your phone. \n\n* Test it on bidirectional & MLM tasks, and image & audio & video tokens. I think RWKV can support Encoder-Decoder via this: for each decoder token, use a learned mixture of [decoder previous hidden state] & [encoder final hidden state]. Hence all decoder tokens will have access to the encoder output.\n\n* Now training RWKV-4a with one single tiny extra attention (just a few extra lines comparing with RWKV-4) to further improve some difficult zeroshot tasks (such as LAMBADA) for smaller models. See https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829\n\nUser feedback:\n> *I've so far toyed around the character-based model on our relatively small pre-training dataset (around 10GB of text), and the results are extremely good - similar ppl to models taking much, much longer to train.*\n\n> *dear god rwkv is fast. i switched to another tab after starting training it from scratch & when i returned it was emitting plausible english & maori words, i left to go microwave some coffee & when i came back it was producing fully grammatically correct sentences.*\n\nTweet from Sepp Hochreiter (thank you!): https://twitter.com/HochreiterSepp/status/1524270961314484227\n\nYou can find me (BlinkDL) in the EleutherAI Discord too: https://www.eleuther.ai/get-involved/\n\n![RWKV-demo](RWKV-demo.png)\n\n## Quick start\n\n**IMPORTANT: Use deepspeed==0.7.0 pytorch-lightning==1.9.5 torch==1.13.1+cu117 and cuda 11.7.1 or 11.7 (note torch2 + deepspeed has weird bugs and hurts model performance)**\n\nUse https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo (latest code, compatible with v4).\n\nHere is a great prompt for testing Q&A of LLMs. Works for any model: (found by minimizing ChatGPT ppls for RWKV 1.5B)\n```python\nprompt = f'\\nQ & A\\n\\nQuestion:\\n{qq}\\n\\nDetailed Expert Answer:\\n' # let the model generate after this\n```\n\n### Inference\n\n**Run RWKV-4 Pile models:** Download models from https://huggingface.co/BlinkDL. Set TOKEN_MODE = 'pile' in run.py and run it. It's fast even on CPU (the default mode).\n\n**Colab for RWKV-4 Pile 1.5B**: https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM\n\nRun RWKV-4 Pile models in your browser (and onnx version): see this issue https://github.com/BlinkDL/RWKV-LM/issues/7\n\nRWKV-4 Web Demo: https://josephrocca.github.io/rwkv-v4-web/demo/ (note: only greedy sampling for now)\n\nFor the old RWKV-2: see the release here for a 27M params model on enwik8 with 0.72 BPC(dev). Run run.py in https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN. You can even run it in your browser: https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng https://blinkdl.github.io/AI-Writer/eng/ (this is using tf.js WASM single-thread mode).\n\n### Training / Fine-tuning\n\npip install deepspeed==0.7.0 // pip install pytorch-lightning==1.9.5 // torch 1.13.1+cu117\n\nNOTE: add weight decay (0.1 or 0.01) and dropout (0.1 or 0.01) when training on small amt of data. try x=x+dropout(att(x)) x=x+dropout(ffn(x)) x=dropout(x+att(x)) x=dropout(x+ffn(x)) etc.\n\n**Training RWKV-4 from scratch:** run train.py, which by default is using the enwik8 dataset (unzip https://data.deepai.org/enwik8.zip).\n\nYou will be training the \"GPT\" version because it's paralleziable and faster to train. RWKV-4 can extrapolate, so training with ctxLen 1024 can work for ctxLen of 2500+. You can fine-tune the model with longer ctxLen and it can quickly adapt to longer ctxLens.\n\n**Fine-tuning RWKV-4 Pile models:** use 'prepare-data.py' in https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3 to tokenize .txt into train.npy data. Then use https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/train.py to train it.\n\nRead the inference code in src/model.py and try using the final hidden state锛?xx .aa .bb) as a faithful sentence embedding for other tasks. Probably you should begin with .xx and .aa/.bb (.aa divided by .bb).\n\nColab for fine-tuning RWKV-4 Pile models: https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb\n\n**Large corpus:** Use https://github.com/Abel2076/json2binidx_tool to convert .jsonl into .bin and .idx\n\nThe jsonl format sample (one line for each document):\n```\n{\"text\": \"This is the first document.\"}\n{\"text\": \"Hello\\nWorld\"}\n{\"text\": \"1+1=2\\n1+2=3\\n2+2=4\"}\n```\ngenerated by code like this:\n```\nss = json.dumps({\"text\": text}, ensure_ascii=False)\nout.write(ss + \"\\n\")\n```\n\n**Infinite ctxlen training (WIP):** https://github.com/Blealtan/RWKV-LM-LoRA/tree/dev-infctx\n\n### How to use RWKV hidden state as text embedding\n\nConsider RWKV 14B. The state has 200 vectors, that is, 5 vectors for each block: fp16 (xx), fp32 (aa), fp32 (bb), fp32 (pp), fp16 (xx).\n\nDo not avg pool because different vectors (xx aa bb pp xx) in the state have very different meanings and ranges. You can probably remove pp.\n\nI suggest firstly collect the mean+stdev statistics of each channel of each vector, and normalize all of them (note: the normalization should be data-indepedent and collected from various texts). Then train a linear classifer.\n\n## Towards RWKV-5 (just to record some new ideas)\n\n### Lastest Design\n\nRWKV-5 is multi-head and here shows one head. There is also a LayerNorm for each head (hence actually GroupNorm).\n\n$`\n\\begin{array}{|l|l|l|}\n\\hline & \\text { RWKV-4 with real-valued } k \\,\\&\\, v \\,\\&\\, u \\,\\&\\, w & \\text { RWKV-5 with matrix-valued } \\mathrm{k}^{\\dagger} \\mathrm{v} \\,\\&\\, \\mathrm{u} \\,\\&\\, \\mathrm{w} \\\\\n\\hline \\mathrm{y}_0 & \\mathrm{r}_0 \\frac{\\mathrm{uk}_0 \\mathrm{v}_0}{\\mathrm{uk}_0} & \\mathrm{r}_0\\left(\\mathrm{uk}_0^{\\dagger} \\mathrm{v}_0\\right) \\\\\n\\hline \\mathrm{y}_1 & \\mathrm{r}_1 \\frac{\\mathrm{uk}_1 \\mathrm{v}_1+\\mathrm{k}_0 \\mathrm{v}_0}{\\mathrm{uk}_1+\\mathrm{k}_0} & \\mathrm{r}_1\\left(\\mathrm{uk}_1^{\\dagger} \\mathrm{v}_1+\\mathrm{k}_0^{\\dagger} \\mathrm{v}_0\\right) \\\\\n\\hline \\mathrm{y}_2 & \\mathrm{r}_2 \\frac{\\mathrm{uk}_2 \\mathrm{v}_2+\\mathrm{k}_1 \\mathrm{v}_1+\\mathrm{wk}_0 \\mathrm{v}_0}{\\mathrm{uk}_2+\\mathrm{k}_1+\\mathrm{wk}_0} & \\mathrm{r}_2\\left(\\mathrm{uk}_2^{\\dagger} \\mathrm{v}_2+\\mathrm{k}_1^{\\dagger} \\mathrm{v}_1+\\mathrm{wk}_0^{\\dagger} \\mathrm{v}_0\\right) \\\\\n\\hline \\mathrm{y}_3 & \\mathrm{r}_3 \\frac{\\mathrm{uk}_3 \\mathrm{v}_3+\\mathrm{k}_2 \\mathrm{v}_2+\\mathrm{wk}_1 \\mathrm{v}_1+\\mathrm{w}^2 \\mathrm{k}_0 \\mathrm{v}_0}{\\mathrm{uk}_3+\\mathrm{k}_2+\\mathrm{wk}_1+\\mathrm{w}^2 \\mathrm{k}_0} & \\mathrm{r}_3\\left(\\mathrm{uk}_3^{\\dagger} \\mathrm{v}_3+\\mathrm{k}_2^{\\dagger} \\mathrm{v}_2+\\mathrm{wk}_1^{\\dagger} \\mathrm{v}_1+\\mathrm{w}^2 \\mathrm{k}_0^{\\dagger} \\mathrm{v}_0\\right) \\\\\n\\hline\n\\end{array}`$\n\n$`\\left[\\begin{array}{ll}\n\\mathrm{y}_{20} & \\cdots \\mathrm{y}_{2 \\mathrm{c}}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n\\mathrm{r}_{20} & \\cdots & \\mathrm{r}_{2 \\mathrm{c}}\n\\end{array}\\right]`$\n$`\\left(\\left[\\begin{array}{ccc}\n\\mathrm{u}_{00} & \\cdots & \\mathrm{u}_{0 \\mathrm{c}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathrm{u}_{\\mathrm{c} 0} & \\cdots & \\mathrm{u}_{\\mathrm{cc}}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\mathrm{k}_{20} \\mathrm{v}_{20} & \\cdots & \\mathrm{k}_{20} \\mathrm{v}_{2 \\mathrm{c}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathrm{k}_{2 \\mathrm{c}} \\mathrm{v}_{20} & \\cdots & \\mathrm{k}_{2 \\mathrm{c}} \\mathrm{v}_{2 \\mathrm{c}}\n\\end{array}\\right]+\\left[\\begin{array}{ccc}\n\\mathrm{k}_{10} \\mathrm{v}_{10} & \\cdots & \\mathrm{k}_{10} \\mathrm{v}_{1 \\mathrm{c}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathrm{k}_{1 \\mathrm{c}} \\mathrm{v}_{10} & \\cdots & \\mathrm{k}_{1 \\mathrm{c}} \\mathrm{v}_{1 \\mathrm{c}}\n\\end{array}\\right]+\\left[\\begin{array}{ccc}\n\\mathrm{w}_{00} & \\cdots & \\mathrm{w}_{0 \\mathrm{c}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathrm{w}_{\\mathrm{c} 0} & \\cdots & \\mathrm{w}_{\\mathrm{cc}}\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\mathrm{k}_{00} \\mathrm{v}_{00} & \\cdots & \\mathrm{k}_{00} \\mathrm{v}_{0 c} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathrm{k}_{0 \\mathrm{c}} \\mathrm{v}_{00} & \\cdots & \\mathrm{k}_{0 \\mathrm{c}} \\mathrm{v}_{0 c}\n\\end{array}\\right]\n\\right)`$\n\n### RWKV-6\n\nDynamic Mix & Dynamic Decay. Example (do this for both TimeMix & ChannelMix):\n```\nTIME_MIX_EXTRA_DIM = 32\nself.time_mix_k_w1 = nn.Parameter(torch.empty(args.n_embd, TIME_MIX_EXTRA_DIM).uniform_(-0.01, 0.01))\nself.time_mix_k_w2 = nn.Parameter(torch.zeros(TIME_MIX_EXTRA_DIM, args.n_embd))\nself.time_mix_v_w1 = nn.Parameter(torch.empty(args.n_embd, TIME_MIX_EXTRA_DIM).uniform_(-0.01, 0.01))\nself.time_mix_v_w2 = nn.Parameter(torch.zeros(TIME_MIX_EXTRA_DIM, args.n_embd))\nself.time_mix_r_w1 = nn.Parameter(torch.empty(args.n_embd, TIME_MIX_EXTRA_DIM).uniform_(-0.01, 0.01))\nself.time_mix_r_w2 = nn.Parameter(torch.zeros(TIME_MIX_EXTRA_DIM, args.n_embd))\nself.time_mix_g_w1 = nn.Parameter(torch.empty(args.n_embd, TIME_MIX_EXTRA_DIM).uniform_(-0.01, 0.01))\nself.time_mix_g_w2 = nn.Parameter(torch.zeros(TIME_MIX_EXTRA_DIM, args.n_embd))\n...\ntime_mix_k = self.time_mix_k.view(1,1,-1) + (x @ self.time_mix_k_w1) @ self.time_mix_k_w2\ntime_mix_v = self.time_mix_v.view(1,1,-1) + (x @ self.time_mix_v_w1) @ self.time_mix_v_w2\ntime_mix_r = self.time_mix_r.view(1,1,-1) + (x @ self.time_mix_r_w1) @ self.time_mix_r_w2\ntime_mix_g = self.time_mix_g.view(1,1,-1) + (x @ self.time_mix_g_w1) @ self.time_mix_g_w2\n\nxx = self.time_shift(x)\nxk = x * time_mix_k + xx * (1 - time_mix_k)\nxv = x * time_mix_v + xx * (1 - time_mix_v)\nxr = x * time_mix_r + xx * (1 - time_mix_r)\nxg = x * time_mix_g + xx * (1 - time_mix_g)\n```\n\n![RWKV-v6](RWKV-v6.png)\n\n### RWKV-7\n\nUse parallelized mode to quickly generate the state, then use a finetuned full RNN (the layers of token n can use outputs of all layer of token n-1) for sequential generation.\n\n### Some old ideas\n\n1. Now time decay is like 0.999^T (0.999 is learnable). Change it to something like (0.999^T + 0.1) where 0.1 is learnable too. The 0.1 part will be kept forever. Or, A^T + B^T + C = fast-decay + slow-decay + constant. Can even use different formulas (for example, K^2 instead of e^K for a decay component, or, without normalization).\n\n2. Use complex-valued decay (so, rotation instead of decay) in some channels.\n\n3. Inject some trainable and extrapolatable positional encoding?\n\n4. Aside from 2d rotation, we can try other Lie groups such as 3d rotation ( SO(3) ). Non-abelian RWKV lol.\n\n5. RWKV might be great on analog devices (search for Analog Matrix-vector multiplication & Photonic Matrix-vector multiplication). The RNN mode is very hardware-friendly (processing-in-memory). Can be a SNN too (https://github.com/ridgerchu/SpikeGPT). I wonder if it can be optimized for quantum computation.\n\n6. Trainable initial hidden state (xx aa bb pp xx).\n\n7. Layerwise (or even row/column-wise, elementwise) LR, and test Lion optimizer.\n\n### Vision Tasks\n\n1. I find it's good to add a 2d pos encoding:\n```\nself.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))\nself.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))\n...\nx = x + pos_emb_x + pos_emb_y\n```\n\n2. In a BPE langauge model, it's the best to use [tokenShift of 1 token] (you can mix more tokens in a char-level English model). However you can try [tokenShift of N (or N-1) (or N+1) tokens] if the image size is N x N, because that will be like mixing [the token above the current positon (or the token above the to-be-predicted positon)] with [current token]. You can use try different tokenShift styles for \"ATT\" & \"FFN\", or mixing different tokenShift styles - such as mixing [token A] with [token A-1] and [token A-(N-1)] etc.\n\n### Misc\n\nMaybe we can improve memorization by simply repeating the context (I guess 2 times is enough). Example:  Reference -> Reference(again) -> Question -> Answer\n\n#### Idea: Bytes-aware Embedding\n\nThe idea is to make sure each token in vocab understand its length and raw UTF-8 bytes.\n\nLet a = max(len(token)) for all token in vocab. Define AA : float[a][d_emb]\n\nLet b = max(len_in_utf8_bytes(token)) for all token in vocab. Define BB : float[b][256][d_emb]\n\nFor each token X in vocab, let [x0, x1, ..., xn] be its raw UTF-8 bytes. We will add some extra values to its embedding EMB(X):\n\nEMB(X) += AA[len(X)] + BB[0][x0] + BB[1][x1] + ... + BB[n][xn] (note: AA BB are learnable weights)\n\n* We can do this for the final Linear(d_emb, n_vocab) projection too.\n* We can use some small networks to generate AA and BB, for some extra regularization (for example, BB[m][xi] and BB[n][xi] should be related).\n\n#### Old Idea\n\nI have an idea to improve tokenization. We can hardcode some channels to have meanings. Example:\n\nChannel 0 = \"space\"\n\nChannel 1 = \"capitalize first letter\"\n\nChannel 2 = \"capitalize all letters\"\n\nTherefore:\n\nEmbedding of \"abc\":  [0, 0, 0, x0, x1, x2 , ..]\n\nEmbedding of \" abc\":  [1, 0, 0, x0, x1, x2, ..]\n\nEmbedding of \" Abc\":  [1, 1, 0, x0, x1, x2, ..]\n\nEmbedding of \"ABC\": [0, 0, 1, x0, x1, x2, ...]\n\n......\n\nso they will share most of the embedding. And we can rapidly compute the output probability of all variations of \"abc\".\n\nNote: the above method is assuming that p(\" xyz\") / p(\"xyz\") is the same for any \"xyz\", which can be wrong.\n\nBetter: define emb_space emb_capitalize_first emb_capitalize_all to be a function of emb.\n\nMaybe the Best: let 'abc' ' abc' etc. to share the last 90% of their embeddings.\n\nAt this moment, all our tokenizers spend too many items to represent all variations of 'abc' ' abc' ' Abc' etc. Moreover the model cannot discover that these are actually similar if some of these variations are rare in the dataset. The method here can improve this. I plan to test this in a new version of RWKV.\n\n#### Idea: Better Initial States\n\nExample (single-round Q & A):\n\n1. Generate the final state of all wiki documents.\n\n2. For any user Q, find the best wiki document, and use its final state as the initial state.\n\n3. Train a model to directly generate the optimal initial state for any user Q.\n\nHowever this can be a bit more tricky for multi-round Q & A :)\n\n## How it works\n\nRWKV is inspired by Apple's AFT (https://arxiv.org/abs/2105.14103).\n\nMoreover it's using a number of my tricks, such as:\n\n* SmallInitEmb: https://github.com/BlinkDL/SmallInitEmb (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).\n\n* Token-shift: https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing (applicable to all transformers), especially helpful for char-level models.\n\n* Head-QK: https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens (applicable to all transformers). Note: it's helpful, but I disabled it in the Pile model to keep it 100% RNN.\n\n* Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer.\n\n* Better initilization: I init most of the matrices to ZERO (see RWKV_Init in https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py).\n\n* You can transfer some parameters from a small model to a large model (note: I sort & smooth them too), for faster and better convergence (see https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/).\n\n* My CUDA kernel: https://github.com/BlinkDL/RWKV-CUDA to speedup training.\n\n## The pseudocode (execution from top to bottom):\n\n![RWKV-v2-RNN](RWKV-v2-RNN.png)\n\nThe a b c d factors work together to build a time-decay curve: [X, 1, W, W^2, W^3, ...].\n\nWrite out the formulas for \"token at pos 2\" and \"token at pos 3\" and you will get the idea:\n* a and b: EMAs of kv and k.\n* c and d: these are a and b combined with \"self-attention\".\n\nkv / k is the memory mechanism. The token with high k can be remembered for a long duration, if W is close to 1 in the channel.\n\nThe R-gate is important for performance. k = info strength of this token (to be passed to future tokens). r = whether to apply the info to this token.\n\n## RWKV-3 improvements\n\nUse different trainable TimeMix factors for R / K / V in SA and FF layers. Example:\n```python\nxx = self.time_shift(x)\nxk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\nxv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\nxr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n```\n\nUse preLN instead of postLN (more stable & faster convergence):\n```python\nif self.layer_id == 0:\n\tx = self.ln0(x)\nx = x + self.att(self.ln1(x))\nx = x + self.ffn(self.ln2(x))\n```\n\n## Explaining the code for RWKV-3 GPT mode\n\n### The GPT mode - overview\n\nThe building blocks of RWKV-3 GPT mode are similar to that of a usual preLN GPT.\n\nThe only difference is an extra LN after embedding. Note you can absorb this LN into the embedding after finishing the training.\n```python\nx = self.emb(idx)  # input: idx = token indices\nx = self.ln_emb(x) # extra LN after embedding\nx = x + self.att_0(self.ln_att_0(x)) # preLN\nx = x + self.ffn_0(self.ln_ffn_0(x))\n...\nx = x + self.att_n(self.ln_att_n(x))\nx = x + self.ffn_n(self.ln_ffn_n(x))\nx = self.ln_head(x) # final LN before projection\nx = self.head(x)    # output: x = logits\n```\nIt is important to initialize emb to tiny values, such as nn.init.uniform_(a=-1e-4, b=1e-4), to utilize my trick https://github.com/BlinkDL/SmallInitEmb.\n\nFor the 1.5B RWKV-3, I use Adam (no wd, no dropout) optimizer on 8 * A100 40G.\n\nbatchSz = 32 * 896, ctxLen = 896. I am using tf32 so the batchSz is a bit small. \n\nFor the first 15B tokens, LR is fixed at 3e-4, and beta=(0.9, 0.99).\n\nThen I set beta=(0.9, 0.999), and do an exponential decay of LR, reaching 1e-5 at 332B tokens.\n\n### The GPT mode - ATT block\n\nThe RWKV-3 does not have any attention in the usual sense, but we will call this block ATT anyway.\n```python\nB, T, C = x.size() # x = (Batch,Time,Channel)\n\n# Mix x with the previous timestep to produce xk, xv, xr\nxx = self.time_shift(x) # self.time_shift = nn.ZeroPad2d((0,0,1,-1))\nxk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\nxv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\nxr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n\n# Use xk, xv, xr to produce k, v, r\nk = self.key(xk).transpose(-1, -2)\nv = self.value(xv).transpose(-1, -2)\nr = self.receptance(xr)\nk = torch.clamp(k, max=60) # clamp k to avoid overflow\nk = torch.exp(k)\nkv = k * v\n\n# Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]\nself.time_w = torch.cat([torch.exp(self.time_decay) * self.time_curve.to(x.device), self.time_first], dim=-1)\nw = torch.exp(self.time_w)\n\n# Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero\nif RUN_DEVICE == 'cuda':\n    wkv = TimeX.apply(w, kv, B,C,T, 0)\n    wk = TimeX.apply(w, k, B,C,T, K_EPS)\nelse:\n    w = w[:,-T:].unsqueeze(1)\n    wkv = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(kv), w, groups=C)\n    wk = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(k), w, groups=C) + K_EPS\n\n# The RWKV formula\nrwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)\nrwkv = self.output(rwkv) # final output projection\n```\n\nThe self.key, self.receptance, self.output matrices are all initialized to zero.\n\nThe time_mix, time_decay, time_first vectors are transferred from a smaller trained model (note: I sort & smooth them too).\n\n### The GPT mode - FFN block\n\nThe FFN block has three tricks comparing with the usual GPT:\n\n1. My time_mix trick.\n\n2. The sqReLU from the Primer paper.\n\n3. An extra receptance-gate (similar to the receptance-gate in ATT block).\n```python\n# Mix x with the previous timestep to produce xk, xr\nxx = self.time_shift(x)\nxk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\nxr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n\n# The usual FFN operation\nk = self.key(xk)\nk = torch.square(torch.relu(k)) # from the Primer paper\nkv = self.value(k)\n\n# Apply an extra receptance-gate to kv\nrkv = torch.sigmoid(self.receptance(xr)) * kv\nreturn rkv\n```\nThe self.value, self.receptance matrices are all initialized to zero.\n\n## RWKV-4 improvements\n\n![RWKV-v3-plan](RWKV-v3-plan.png)\n\n## From GPT to RWKV (the formulas)\n\nLet F[t] be the system state at t.\n\nLet x[t] be the new external input at t.\n\nIn GPT, predicting F[t+1] requires considering F[0], F[1], .. F[t]. So it takes O(T^2) to generate a length T sequence.\n\nThe **simplified formula** for GPT:\n\n![F[\\mathrm{t}+1]=\\frac{\\sum_{\\mathrm{i}=0}^{\\mathrm{t}} \\exp (\\mathbf{Q}x[\\mathrm{t}] * \\mathbf{K}F[\\mathrm{i}]) \\cdot(\\mathbf{V}F[\\mathrm{i}])}{\\sum_{\\mathrm{i}=0}^{\\mathrm{t}} \\exp (\\mathbf{Q}x[\\mathrm{t}] * \\mathbf{K}F[\\mathrm{i}])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D)\n\nIt's very capable in theory, however that **does not mean we can fully utilize its capability with usual optimizers**. I suspect the loss landscape is too difficult for our current methods.\n\nCompare with the **simplified formula** for RWKV (the parallel mode, looks similar to Apple's AFT):\n\n![F[\\mathrm{t}+1]=\\sigma(\\mathbf{R}x[\\mathrm{t}]) \\cdot \\frac{\\sum_{\\mathrm{i}=0}^{\\mathrm{t}} \\exp (\\mathbf{W} \\cdot(\\mathrm{t}-\\mathrm{i})) \\cdot \\exp (\\mathbf{K}F[\\mathrm{i}]) \\cdot(\\mathbf{V}F[\\mathrm{i}])}{\\sum_{\\mathrm{i}=0}^{\\mathrm{t}} \\exp (\\mathbf{W} \\cdot(\\mathrm{t}-\\mathrm{i})) \\cdot \\exp (\\mathbf{K }F[\\mathrm{i}])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D)\n\nThe R, K, V are trainable matrices, and W is a trainable vector (time-decay factor for each channel).\n\nIn GPT, the contribution of F[i] to F[t+1] is weighted by ![ \\exp (\\mathbf{Q}x[\\mathrm{t}] * \\mathbf{K}F[\\mathrm{i}]) ](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+).\n\nIn RWKV-2, the contribution of F[i] to F[t+1] is weighted by ![\\sigma(\\mathbf{R}x[\\mathrm{t}]) \\cdot \\exp (\\mathbf{W} \\cdot(\\mathrm{t}-\\mathrm{i})) \\cdot \\exp (\\mathbf{K}F[\\mathrm{i}]) ](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+).\n* The ![\\sigma](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma) is a non-linearity and we can use sigmoid. \n* Note ![\\sigma(\\mathbf{R}x[\\mathrm{t}])](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29) is not in the denominator, and I call R the \"receptance\".\n* The ![\\exp (\\mathbf{W} \\cdot(\\mathrm{t}-\\mathrm{i}))](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29) is the time-decay factor. I proposed the same idea (scaling the attention by distance) in Aug 2020 and called it the \"time-weighting\" (check the commit history of https://github.com/BlinkDL/minGPT-tuned).\n\nHere comes the punchline: we can rewrite it into a RNN (recursive formula). Note:\n\n![F[1]=\\sigma(\\mathbf{R }x[0]) \\cdot \\frac{ \\exp (\\mathbf{K }F[0]) \\cdot(\\mathbf{V }F[0])}{\\exp (\\mathbf{K }F[0])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D)\n\n![F[2]=\\sigma(\\mathbf{R }x[1]) \\cdot \\frac{ \\exp (\\mathbf{K }F[1]) \\cdot(\\mathbf{V }F[1])+\\exp (\\mathbf{W} ) \\cdot \\exp (\\mathbf{K }F[0]) \\cdot(\\mathbf{V }F[0])}{ \\exp (\\mathbf{K }F[1])+\\exp (\\mathbf{W} ) \\cdot \\exp (\\mathbf{K }F[0])}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D)\n\nTherefore it's straightforward to verify:\n\n![F[t+1]=\\sigma(\\mathbf{R }x[t]) \\cdot \\frac{\\exp (\\mathbf{K}F[\\mathrm{t}]) \\cdot(\\mathbf{V}F[\\mathrm{t}])+\\exp (\\mathbf{W}) \\cdot A[\\mathrm{t}]}{ \\exp (\\mathbf{K}F[\\mathrm{t}])+\\exp (\\mathbf{W}) \\cdot B[\\mathrm{t}]}](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D)\n\nwhere A[t] and B[t] are the numerator and denominator of the previous step, respectively.\n\nI believe RWKV is performant because W is like repeatedly applying a diagonal matrix. Note (P^{-1} D P)^n = P^{-1} D^n P, so it is similar to repeatedly applying a general diagonalizable matrix.\n\nMoreover it's possible to turn it into a continuous ODE (a bit similar to State Space Models). I will write about it later.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=BlinkDL/RWKV-LM&type=Date)](https://star-history.com/#BlinkDL/RWKV-LM&Date)\n\n## Multimodal ideas\n\nI have an idea for [text --> 32x32 RGB image] using a LM (transformer, RWKV, etc.). Will test it soon.\n\nFirstly, LM loss (instead of L2 loss), so the image will not be blurry.\n\nSecondly, color quantization. For example, only allowing 8 levels for R/G/B. Then the image vocab size is 8x8x8 = 512 (for each pixel), instead of 2^24.\nTherefore, a 32x32 RGB image = a len1024 sequence of vocab512 (image tokens), which is a typical input for usual LMs.\n(Later we can use diffusion models to upsample and generate RGB888 images. We might be able to use a LM for this too.)\n\nThirdly, 2D positional embeddings that are easy for the model to understand.\nFor example, add one-hot X & Y coords to the first 64(=32+32) channels. Say if the pixel is at x=8, y=20, then we will add 1 to channel 8 and channel 52 (=32+20).\nMoreover probably we can add the float X & Y coords (normalized to 0~1 range) to another 2 channels. And other periodic pos. encoding might help too (will test). \n\nFinally, RandRound when doing the color quantization in the DataLoader.\nFor example, if the float level is 4.578, then there is a 57.8% chance to use 5, and (1-57.8%) chance to use 4.\nAnd we can allow both 4 and 5 in the prediction, but the loss will be higher if the prediction is 4.\n\nMulti-task training might help too. I will try this dataset format:\n[TxtFirst] [Desc of Img (txt tokens)] [Img] [img tokens]\nand sometimes\n[ImgFirst] [img tokens] [Txt] [Desc of Img (txt tokens)]\n... the order of the imgs should be randomized in the DataLoader, and [TxtFirst] [ImgFirst] [Img] [Txt] are special tokens\nand do random sampling of the full dataset. So sometimes the model will see the img tokens first and then the corresponding txt tokens, which is a [img -> txt] task. And the model will see some partial imgs and partial txts. I think a char-level LM might help the model to write correct text on images.\n\n## How to sample a large dataset (for training)\n\nI am using a trick to sample the Pile deterministically yet randomly enough.\n\nLet's say the pile has x chunks (a chunk = ctx_len tokens).\n\npick a prime number p just less than x, and make sure p = 2 (mod 3).\n\nUse (step * step * step) mod p to sample it. Add some bias to step for extra randomness.\n\n## The top-p-x sampling method (for inference)\n\nWe propose a new sampling method called top-p-x:\n\nit's like top-p, and the only difference is you also keep all tokens whose prob > x.\n\nTry x = 0.01 first.\n\n## Better Learning Rate Schedule via Variantional Method of Loss Curve\n\nI propose a simple new method to find better LR schedules. The method is cost-efficient and practical for large LMs. The takeaway is we can model the loss curve dynamics (phenomenology) w.r.t. the LR, and a nice closed-form LR curve can be directly computed from it using variantional method. Moreover we can predict the final loss with reasonable accuracy.\n\nUPDATE: In \"Conclusion 1.\", use the best-fitting regime (ignore the initial steps where our approximations break down) to fit the parameters.\n\nTry this: fixed lr for 1 hr, then exponential decay to 0.2 * lr in 12 hrs, and choose the t=[1hr, 13hr] segment.\n\nIn the last three plots, black = predicted loss curve of the new LR schedule, blue = original (unoptimized) real loss curve, orange = new LR schedule.\n\n![better_lr_schedule](Research/better_lr_schedule.png)\n\n# RWKV v1\n\nWe propose the RWKV language model, with alternating time-mix and channel-mix layers:\n\n<img src=\n\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_%7Bu%7D+%26%26%5Ctextbf%7BW%7D_%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_d+%26%26%5Ctextbf%7BW%7D_%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D_%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A\" \nalt=\"\\begin{align*}\n\\text{Time-mix :} && \\text{TM}_{t,c} &&=&&\\text{sigmoid}(\\text{R}_{t,c}) &&\\cdot&& &&\\textstyle\\sum_{u} &&\\textbf{W}_{t,u,c} &&\\cdot&& \\text{softmax}_t(\\text{K}_{u,c}) &&\\cdot&& \\text{V}_{u,c}\\\\\n\\text{Channel-mix :} && \\text{CM}_{t,c} &&=&&\\text{sigmoid}(\\text{R}_{t,c}) &&\\cdot&& &&\\textstyle\\sum_d &&\\textbf{W}_{c,d} &&\\cdot&& \\text{gelu}(\\text{K}_{t,d}) &&\\cdot&& \\text{V}_{t,d}\n\\end{align*}\n\">\n\n* The R, K, V are generated by linear transforms of input, and W is parameter. The idea of RWKV is to decompose attention into R(target) * W(src, target) * K(src). So we can call R \"receptance\", and sigmoid means it's in 0~1 range.\n\n* The Time-mix is similar to AFT (https://arxiv.org/abs/2105.14103). There are two differences.\n\n(1) We changed the normalization (denominator). For masked language models, we define:\n\n<img src=\n\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29%7D%7B%5Csum_%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D_%7Bv%2Cc%7D%29%7D\" \nalt=\"\\text{softmax}_t(\\text{K}_{u,c}) = \\frac{\\exp(\\text{K}_{u,c})}{\\sum_{v \\leq t}\\exp(\\text{K}_{v,c})}\">\n\n**(UPDATE: We are using the original AFT normalization in v2)**\n \nInitialize K and R matrices (and the output projection matrix) to ZERO for fast & stable convergence.\n \n(2) We decompose W_{t,u,c} and introduce multi-head W (here h is the corresponding head of c):\n\n<img src=\n\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W_%7Bt%2Cu%2Cc%7D%3Df_h%28t-u%29%5Ccdot+%5Calpha_h%28u%29+%5Ccdot+%5Cbeta_h%28t%29\" \nalt=\"W_{t,u,c}=f_h(t-u)\\cdot \\alpha_h(u) \\cdot \\beta_h(t)\">\n\nMoreover we multiply the final output of Time-mix layer by 纬(t). The reason for the 伪 尾 纬 factors, is because the context size is smaller when t is small, and this can be compensated using the 伪 尾 纬 factors.\n\n**(UPDATE: We remove 伪 尾 纬 factors in v2-RNN and restrict W to be of a simple form and hence able to rewrite it as RNN)**\n\n* The Channel-mix is similar to GeGLU (https://arxiv.org/abs/2002.05202) with an extra R factor. Initialize R and W matrices to ZERO for fast & stable convergence.\n\n* Finally, we add extra token-shift (time-shift mixing) as in (https://github.com/BlinkDL/minGPT-tuned).\n\n# Token-shift (time-shift mixing)\n\nThe token-shift explicitly uses (half the channels of this token) & (half the channels of prev token) to generate all vectors (QKV, RWKV, ...).\n\n```\nself.time_shift = nn.ZeroPad2d((0,0,1,-1))\n\nx = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\n```\n\nDividing channels by 2 and shift-1 works great for char-level English and char-level Chinese LM.\n\nHowever for BPE-level English LM, it's only effective if your embedding is large enough (at least 1024 - so the usual small L12-D768 model is not enough).\n\nMy theory on the effectiveness of token-shift:\n\nWhen we train a GPT, the hidden representation of a token has to accomplish two different objects:\n\n1. Predict the next token. Sometimes this is easy (obvious next token).\n\n2. Collect all previous context info, so later tokens can use it. This is always hard.\n\nThe shifted channels can focus on (2), so we have good propagation of info. It's like some kind of residual connection, or a small RNN inside the transformer.\n\nYou can use token-shift in usual QKV self-attention too. I looked at the weights, and found V really likes the shifted channels, less so for Q. Makes sense if you think about it. I also found you may want to use less mixing in higher layers.\n\np.s. There is a MHA_pro model in this repo with strong performance. Give it a try :)\n\n# The Head-QK Trick: learning to copy and avoid tokens\n\nIn usual transformer, a small model has difficulty copying tokens (such as person names) in the context. We add extra Q & K to the final output such that the model can directly copy (or avoid) tokens in the context. Afterwards the model will teach itself NER (named entity recognition) if you look at the learned weights.\n```\nq = self.head_q(x)[:,:T,:] # projecting to 256-d\nk = self.head_k(x)[:,:T,:] # projecting to 256-d\nc = (q @ k.transpose(-2, -1)) * (1.0 / 256)\nc = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)\nc = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()       \nx = self.head(x) + c\n```\nNote: when a token occurs multiple times in the context, it might be better to use max(prob) instead of sum(prob).\n\n# The top-a sampling method\n\nWe also propose a new sampling method called top-a (as in src/utils.py):\n\n(1) Find the max probability p_max after softmax.\n\n(2) Remove all entries whose probability is lower than 0.2 * pow(p_max, 2). So it's adaptive, hence \"top-a\".\n\n(3) Feel free to tune the 0.2 and 2 factor. Tune 0.2 first.\n\nThe idea of top-a:\n1. If max_prob=0.9, then remove all tokens with prob < 0.162 (so, removing all alternatives)\n2. If max_prob=0.5, then remove all tokens with prob < 0.05  (so, allowing more choices)\n3. If max_prob=0.1, then remove all tokens with prob < 0.002 (so, allowing lots of possibilities)\n\n```\nprobs = F.softmax(logits, dim=-1)\n\nlimit = torch.pow(torch.max(probs), 2) * 0.02\nlogits[probs < limit] = -float('Inf')\n```\n\n# Performance\n\nCharacter-level loss on simplebooks-92 dataset https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n\n![RWKV-vs-MHA](RWKV-vs-MHA.png)\n\nGray: usual MHA+Rotary+GeGLU - performance not as good. 17.2M params.\n\nRed: RWKV (\"linear\" attention) - VRAM friendly - quite faster when ctx window is long - good performance. 16.6M params.\n\nGreen: MHA+Rotary+GeGLU+Token_shift. 17.2M params.\n\nBlue: MHA_pro (MHA with various tweaks & RWKV-type-FFN) - slow - needs more VRAM - good performance. 16.6M params.\n\n```\n@software{peng_bo_2021_5196578,\n  author       = {PENG Bo},\n  title        = {BlinkDL/RWKV-LM: 0.01},\n  month        = aug,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {0.01},\n  doi          = {10.5281/zenodo.5196577},\n  url          = {https://doi.org/10.5281/zenodo.5196577}\n}\n```\n\n# Initialization\n\nWe use careful initialization for RWKV to get fast convergence - orthogonal matrices with proper scaling, and special time_w curves. Check model.py for details.\n\nSome learned time_w examples:\n\n![RWKV-time-w](RWKV-time-w.png)\n",
12250:    "readme": "# minbpe\n\nMinimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is \"byte-level\" because it runs on UTF-8 encoded strings.\n\nThis algorithm was popularized for LLMs by the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the associated GPT-2 [code release](https://github.com/openai/gpt-2) from OpenAI. [Sennrich et al. 2015](https://arxiv.org/abs/1508.07909) is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.\n\nThere are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:\n\n1. [minbpe/base.py](minbpe/base.py): Implements the `Tokenizer` class, which is the base class. It contains the `train`, `encode`, and `decode` stubs, save/load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.\n2. [minbpe/basic.py](minbpe/basic.py): Implements the `BasicTokenizer`, the simplest implementation of the BPE algorithm that runs directly on text.\n3. [minbpe/regex.py](minbpe/regex.py): Implements the `RegexTokenizer` that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.\n4. [minbpe/gpt4.py](minbpe/gpt4.py): Implements the `GPT4Tokenizer`. This class is a light wrapper around the `RegexTokenizer` (2, above) that exactly reproduces the tokenization of GPT-4 in the [tiktoken](https://github.com/openai/tiktoken) library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations.\n\nFinally, the script [train.py](train.py) trains the two major tokenizers on the input text [tests/taylorswift.txt](tests/taylorswift.txt) (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.\n\nAll of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file.\n\n## quick start\n\nAs the simplest example, we can reproduce the [Wikipedia article on BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) as follows:\n\n```python\nfrom minbpe import BasicTokenizer\ntokenizer = BasicTokenizer()\ntext = \"aaabdaaabac\"\ntokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges\nprint(tokenizer.encode(text))\n# [258, 100, 258, 97, 99]\nprint(tokenizer.decode([258, 100, 258, 97, 99]))\n# aaabdaaabac\ntokenizer.save(\"toy\")\n# writes two files: toy.model (for loading) and toy.vocab (for viewing)\n```\n\nAccording to Wikipedia, running bpe on the input string: \"aaabdaaabac\" for 3 merges results in the string: \"XdXac\" where  X=ZY, Y=ab, and Z=aa. The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their [ASCII](https://www.asciitable.com) values). Then when (a,a) is merged to Z, Z will become 256. Likewise Y will become 257 and X 258. So we start with the 256 bytes, and do 3 merges to get to the result above, with the expected output of [258, 100, 258, 97, 99].\n\n## inference: GPT-4 comparison\n\nWe can verify that the `RegexTokenizer` has feature parity with the GPT-4 tokenizer from [tiktoken](https://github.com/openai/tiktoken) as follows:\n\n```python\ntext = \"hello123!!!? (鞎堧厱頃橃劯鞖?) 馃槈\"\n\n# tiktoken\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\")\nprint(enc.encode(text))\n# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]\n\n# ours\nfrom minbpe import GPT4Tokenizer\ntokenizer = GPT4Tokenizer()\nprint(tokenizer.encode(text))\n# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]\n```\n\n(you'll have to `pip install tiktoken` to run). Under the hood, the `GPT4Tokenizer` is just a light wrapper around `RegexTokenizer`, passing in the merges and the special tokens of GPT-4. We can also ensure the special tokens are handled correctly:\n\n```python\ntext = \"<|endoftext|>hello world\"\n\n# tiktoken\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\")\nprint(enc.encode(text, allowed_special=\"all\"))\n# [100257, 15339, 1917]\n\n# ours\nfrom minbpe import GPT4Tokenizer\ntokenizer = GPT4Tokenizer()\nprint(tokenizer.encode(text, allowed_special=\"all\"))\n# [100257, 15339, 1917]\n```\n\nNote that just like tiktoken, we have to explicitly declare our intent to use and parse special tokens in the call to encode. Otherwise this can become a major footgun, unintentionally tokenizing attacker-controlled data (e.g. user prompts) with special tokens. The `allowed_special` parameter can be set to \"all\", \"none\", or a list of special tokens to allow.\n\n## training\n\nUnlike tiktoken, this code allows you to train your own tokenizer. In principle and to my knowledge, if you train the `RegexTokenizer` on a large dataset with a vocabulary size of 100K, you would reproduce the GPT-4 tokenizer.\n\nThere are two paths you can follow. First, you can decide that you don't want the complexity of splitting and preprocessing text with regex patterns, and you also don't care for special tokens. In that case, reach for the `BasicTokenizer`. You can train it, and then encode and decode for example as follows:\n\n```python\nfrom minbpe import BasicTokenizer\ntokenizer = BasicTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=4096)\ntokenizer.encode(\"hello world\") # string -> tokens\ntokenizer.decode([1000, 2000, 3000]) # tokens -> string\ntokenizer.save(\"mymodel\") # writes mymodel.model and mymodel.vocab\ntokenizer.load(\"mymodel.model\") # loads the model back, the vocab is just for vis\n```\n\nIf you instead want to follow along with OpenAI did for their text tokenizer, it's a good idea to adopt their approach of using regex pattern to split the text by categories. The GPT-4 pattern is a default with the `RegexTokenizer`, so you'd simple do something like:\n\n```python\nfrom minbpe import RegexTokenizer\ntokenizer = RegexTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=32768)\ntokenizer.encode(\"hello world\") # string -> tokens\ntokenizer.decode([1000, 2000, 3000]) # tokens -> string\ntokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab\ntokenizer.load(\"tok32k.model\") # loads the model back from disk\n```\n\nWhere, of course, you'd want to change around the vocabulary size depending on the size of your dataset.\n\n**Special tokens**. Finally, you might wish to add special tokens to your tokenizer. Register these using the `register_special_tokens` function. For example if you train with vocab_size of 32768, then the first 256 tokens are raw byte tokens, the next 32768-256 are merge tokens, and after those you can add the special tokens. The last \"real\" merge token will have id of 32767 (vocab_size - 1), so your first special token should come right after that, with an id of exactly 32768. So:\n\n```python\nfrom minbpe import RegexTokenizer\ntokenizer = RegexTokenizer()\ntokenizer.train(very_long_training_string, vocab_size=32768)\ntokenizer.register_special_tokens({\"<|endoftext|>\": 32768})\ntokenizer.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n```\n\nYou can of course add more tokens after that as well, as you like. Finally, I'd like to stress that I tried hard to keep the code itself clean, readable and hackable. You should not have feel scared to read the code and understand how it works. The tests are also a nice place to look for more usage examples. That reminds me:\n\n## tests\n\nWe use the pytest library for tests. All of them are located in the `tests/` directory. First `pip install pytest` if you haven't already, then:\n\n```bash\n$ pytest -v .\n```\n\nto run the tests. (-v is verbose, slightly prettier).\n\n## community extensions\n\n* [gnp/minbpe-rs](https://github.com/gnp/minbpe-rs): A Rust implementation of `minbpe` providing (near) one-to-one correspondence with the Python version\n\n## exercise\n\nFor those trying to study BPE, here is the advised progression exercise for how you can build your own minbpe step by step. See [exercise.md](exercise.md).\n\n## lecture\n\nI built the code in this repository in this [YouTube video](https://www.youtube.com/watch?v=zduSFxRajkE). You can also find this lecture in text form in [lecture.md](lecture.md).\n\n## todos\n\n- write a more optimized Python version that could run over large files and big vocabs\n- write an even more optimized C or Rust version (think through)\n- rename GPT4Tokenizer to GPTTokenizer and support GPT-2/GPT-3/GPT-3.5 as well?\n- write a LlamaTokenizer similar to GPT4Tokenizer (i.e. attempt sentencepiece equivalent)\n\n## License\n\nMIT\n",
23616:    "readme": "---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: transformers\ninference: false\nthumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\ntags:\n- gpt\n- llm\n- large language model\n- open-source\ndatasets:\n- h2oai/openassistant_oasst1\n- h2oai/openassistant_oasst1_h2ogpt\n---\n# h2oGPT Model Card\n## Summary\n\nH2O.ai's `h2ogpt-oasst1-512-20b` is a 20 billion parameter instruction-following large language model licensed for commercial use.\n\n- Base model: [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b)\n- Fine-tuning dataset: [h2oai/openassistant_oasst1](https://huggingface.co/datasets/h2oai/openassistant_oasst1) and [h2oai/openassistant_oasst1_h2ogpt](https://huggingface.co/datasets/h2oai/openassistant_oasst1_h2ogpt)\n- Data-prep and fine-tuning code: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n- Training logs: [zip](https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b/blob/main/gpt-neox-20b.openassistant_oasst1.json.6.0_epochs.5a14ea8b3794c0d60476fc262d0a297f98dd712d.1013.zip) and [zip](https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b/blob/main/h2ogpt-oasst1-512-20b.h2oaiopenassistant_oasst1_h2ogpt.2_epochs.fcaae7ef70600de8c97c9b38cb3f0075467cdad1.3.zip)\n\n## Chatbot\n\n- Run your own chatbot: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n[![H2O.ai GitHub](https://user-images.githubusercontent.com/6147661/232930822-e7170e4d-8aa1-4f7a-ad70-ece9cdd8b0cb.png)](https://github.com/h2oai/h2ogpt)\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the `transformers` and `accelerate` libraries installed.\n\n```bash\npip install transformers==4.28.1\npip install accelerate==0.18.0\n```\n\n```python\nimport torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\nAlternatively, if you prefer to not use `trust_remote_code=True` you can download [instruct_pipeline.py](https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b/blob/main/h2oai_pipeline.py),\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\n\n```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n## Model Architecture\n\n```\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50432, 6144)\n    (layers): ModuleList(\n      (0-43): 44 x GPTNeoXLayer(\n        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n        (attention): GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n          (query_key_value): Linear(in_features=6144, out_features=18432, bias=True)\n          (dense): Linear(in_features=6144, out_features=6144, bias=True)\n        )\n        (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=6144, out_features=24576, bias=True)\n          (dense_4h_to_h): Linear(in_features=24576, out_features=6144, bias=True)\n          (act): FastGELUActivation()\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n  )\n  (embed_out): Linear(in_features=6144, out_features=50432, bias=False)\n)\n```\n\n## Model Configuration\n\n```json\nGPTNeoXConfig {\n  \"_name_or_path\": \"h2oai/h2ogpt-oasst1-512-20b\",\n  \"architectures\": [\n    \"GPTNeoXForCausalLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 0,\n  \"custom_pipeline\": {\n    \"text-generation\": {\n      \"impl\": \"h2oai_pipeline.H2OTextGenerationPipeline\",\n      \"pt\": \"AutoModelForCausalLM\"\n    }\n  },\n  \"custom_pipelines\": {\n    \"text-generation\": {\n      \"impl\": \"h2oai_pipeline.H2OTextGenerationPipeline\",\n      \"pt\": \"AutoModelForCausalLM\"\n    }\n  },\n  \"eos_token_id\": 0,\n  \"hidden_act\": \"gelu_fast\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 6144,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 24576,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"gpt_neox\",\n  \"num_attention_heads\": 64,\n  \"num_hidden_layers\": 44,\n  \"rotary_emb_base\": 10000,\n  \"rotary_pct\": 0.25,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.28.1\",\n  \"use_cache\": true,\n  \"use_parallel_residual\": true,\n  \"vocab_size\": 50432\n}\n\n```\n\n## Model Validation\n\nModel validation results using [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n\n\n[eval source code](https://github.com/h2oai/h2ogpt/issues/35#issuecomment-1521119301)\n\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|hellaswag    |      0|acc     |0.5419|卤  |0.0050|\n|             |       |acc_norm|0.7259|卤  |0.0045|\n|boolq        |      1|acc     |0.7125|卤  |0.0079|\n|piqa         |      0|acc     |0.7742|卤  |0.0098|\n|             |       |acc_norm|0.7775|卤  |0.0097|\n|openbookqa   |      0|acc     |0.2800|卤  |0.0201|\n|             |       |acc_norm|0.4000|卤  |0.0219|\n|arc_challenge|      0|acc     |0.3993|卤  |0.0143|\n|             |       |acc_norm|0.4420|卤  |0.0145|\n|winogrande   |      0|acc     |0.6614|卤  |0.0133|\n|arc_easy     |      0|acc     |0.7327|卤  |0.0091|\n|             |       |acc_norm|0.6894|卤  |0.0095|\n\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.\n",
24505:    "readme": "---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: transformers\ninference: false\nthumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\ntags:\n- gpt\n- llm\n- large language model\n- open-source\ndatasets:\n- h2oai/openassistant_oasst1_h2ogpt_graded\n---\n# h2oGPT Model Card\n## Summary\n\nH2O.ai's `h2ogpt-oasst1-512-12b` is a 12 billion parameter instruction-following large language model licensed for commercial use.\n\n- Base model: [EleutherAI/pythia-12b](https://huggingface.co/EleutherAI/pythia-12b)\n- Fine-tuning dataset: [h2oai/openassistant_oasst1_h2ogpt_graded](https://huggingface.co/datasets/h2oai/openassistant_oasst1_h2ogpt_graded)\n- Data-prep and fine-tuning code: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n- Training logs: [zip](https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b/blob/main/pythia-12b-deduped.h2oaiopenassistant_oasst1_h2ogpt_graded.3_epochs.2ccf687ea3f3f3775a501838e81c1a0066430455.4.zip)\n\n## Chatbot\n\n- Run your own chatbot: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n[![H2O.ai GitHub](https://user-images.githubusercontent.com/6147661/232930822-e7170e4d-8aa1-4f7a-ad70-ece9cdd8b0cb.png)](https://github.com/h2oai/h2ogpt)\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the `transformers` and `accelerate` libraries installed.\n\n```bash\npip install transformers==4.28.1\npip install accelerate==0.18.0\n```\n\n```python\nimport torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-512-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\nAlternatively, if you prefer to not use `trust_remote_code=True` you can download [instruct_pipeline.py](https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b/blob/main/h2oai_pipeline.py),\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\n\n```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-512-12b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n## Model Architecture\n\n```\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50688, 5120)\n    (layers): ModuleList(\n      (0-35): 36 x GPTNeoXLayer(\n        (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n        (attention): GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n          (query_key_value): Linear(in_features=5120, out_features=15360, bias=True)\n          (dense): Linear(in_features=5120, out_features=5120, bias=True)\n        )\n        (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=5120, out_features=20480, bias=True)\n          (dense_4h_to_h): Linear(in_features=20480, out_features=5120, bias=True)\n          (act): GELUActivation()\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n  )\n  (embed_out): Linear(in_features=5120, out_features=50688, bias=False)\n)\n```\n\n## Model Configuration\n\n```json\nGPTNeoXConfig {\n  \"_name_or_path\": \"h2oai/h2ogpt-oasst1-512-12b\",\n  \"architectures\": [\n    \"GPTNeoXForCausalLM\"\n  ],\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.1,\n  \"custom_pipelines\": {\n    \"text-generation\": {\n      \"impl\": \"h2oai_pipeline.H2OTextGenerationPipeline\",\n      \"pt\": \"AutoModelForCausalLM\"\n    }\n  },\n  \"eos_token_id\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 5120,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 20480,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"gpt_neox\",\n  \"num_attention_heads\": 40,\n  \"num_hidden_layers\": 36,\n  \"rotary_emb_base\": 10000,\n  \"rotary_pct\": 0.25,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.30.0.dev0\",\n  \"use_cache\": true,\n  \"use_parallel_residual\": true,\n  \"vocab_size\": 50688\n}\n\n```\n\n## Model Validation\n\nModel validation results using [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n\n[eval source code](https://github.com/h2oai/h2ogpt/issues/125#issuecomment-1548239108)\n\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.3157|卤  |0.0136|\n|             |       |acc_norm|0.3507|卤  |0.0139|\n|arc_easy     |      0|acc     |0.6932|卤  |0.0095|\n|             |       |acc_norm|0.6225|卤  |0.0099|\n|boolq        |      1|acc     |0.6685|卤  |0.0082|\n|hellaswag    |      0|acc     |0.5140|卤  |0.0050|\n|             |       |acc_norm|0.6803|卤  |0.0047|\n|openbookqa   |      0|acc     |0.2900|卤  |0.0203|\n|             |       |acc_norm|0.3740|卤  |0.0217|\n|piqa         |      0|acc     |0.7682|卤  |0.0098|\n|             |       |acc_norm|0.7661|卤  |0.0099|\n|winogrande   |      0|acc     |0.6369|卤  |0.0135|\n\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.\n",
25928:    "readme": "---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: transformers\ninference: false\nthumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\ntags:\n- gpt\n- llm\n- large language model\n- open-source\ndatasets:\n- h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v1\n- h2oai/openassistant_oasst1_h2ogpt\n- h2oai/h2ogpt-fortune2000-personalized\n- h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v3\n---\n# h2oGPT Model Card\n## Summary\n\nH2O.ai's `h2ogpt-oig-oasst1-512-6_9b` is a 6.9 billion parameter instruction-following large language model licensed for commercial use.\n\n- Base model: [EleutherAI/pythia-6.9b](https://huggingface.co/EleutherAI/pythia-6.9b)\n- Fine-tuning dataset: [h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v1](https://huggingface.co/datasets/h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v1) and [h2oai/openassistant_oasst1_h2ogpt](https://huggingface.co/datasets/h2oai/openassistant_oasst1_h2ogpt) and [h2oai/h2ogpt-fortune2000-personalized](https://huggingface.co/datasets/h2oai/h2ogpt-fortune2000-personalized) and [h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v3](https://huggingface.co/datasets/h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v3)\n- Data-prep and fine-tuning code: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n- Training logs: [zip](https://huggingface.co/h2oai/h2ogpt-oig-oasst1-512-6.9b/blob/main/pythia-6.9b.h2ogpt-oig-oasst1-instruct-cleaned-v1.json.1_epochs.5fc91911bc2bfaaf3b6c2de577c4b0ae45a07a4a.7.zip) and [zip](https://huggingface.co/h2oai/h2ogpt-oig-oasst1-512-6.9b/blob/main/h2ogpt-oig-oasst1-512-6.9b.h2oaiopenassistant_oasst1_h2ogpt.2_epochs.e35e2e06e0af2f7dceac2e16e3646c90ccce4ec0.1.zip) and [zip](https://huggingface.co/h2oai/h2ogpt-oig-oasst1-512-6.9b/blob/main/h2ogpt-oig-oasst1-512-6.9b.h2oaih2ogpt-oig-oasst1-instruct-cleaned-v3.1_epochs.e48f9debb0d2bd8d866fa5668bbbb51c317c553c.1.zip)\n\n## Chatbot\n\n- Run your own chatbot: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n[![H2O.ai GitHub](https://user-images.githubusercontent.com/6147661/232930822-e7170e4d-8aa1-4f7a-ad70-ece9cdd8b0cb.png)](https://github.com/h2oai/h2ogpt)\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the `transformers` and `accelerate` libraries installed.\n\n```bash\npip install transformers==4.28.1\npip install accelerate==0.18.0\n```\n\n```python\nimport torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oig-oasst1-512-6_9b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\nAlternatively, if you prefer to not use `trust_remote_code=True` you can download [instruct_pipeline.py](https://huggingface.co/h2oai/h2ogpt-oig-oasst1-512-6.9b/blob/main/h2oai_pipeline.py),\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\n\n```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oig-oasst1-512-6_9b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oig-oasst1-512-6_9b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type='human_bot')\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n## Model Architecture\n\n```\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50432, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x GPTNeoXLayer(\n        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (attention): GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n        )\n        (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n          (act): GELUActivation()\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n  )\n  (embed_out): Linear(in_features=4096, out_features=50432, bias=False)\n)\n```\n\n## Model Configuration\n\n```json\nGPTNeoXConfig {\n  \"_name_or_path\": \"h2oai/h2ogpt-oig-oasst1-512-6_9b\",\n  \"architectures\": [\n    \"GPTNeoXForCausalLM\"\n  ],\n  \"bos_token_id\": 0,\n  \"custom_pipeline\": {\n    \"text-generation\": {\n      \"impl\": \"h2oai_pipeline.H2OTextGenerationPipeline\",\n      \"pt\": \"AutoModelForCausalLM\"\n    }\n  },\n  \"eos_token_id\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"gpt_neox\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"rotary_emb_base\": 10000,\n  \"rotary_pct\": 0.25,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.28.1\",\n  \"use_cache\": true,\n  \"use_parallel_residual\": true,\n  \"vocab_size\": 50432\n}\n\n```\n\n## Model Validation\n\nModel validation results using [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n\n[eval source code](https://github.com/h2oai/h2ogpt/issues/125#issue-1702311702)\n\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_easy     |      0|acc     |0.6591|卤  |0.0097|\n|             |       |acc_norm|0.6178|卤  |0.0100|\n|arc_challenge|      0|acc     |0.3174|卤  |0.0136|\n|             |       |acc_norm|0.3558|卤  |0.0140|\n|openbookqa   |      0|acc     |0.2540|卤  |0.0195|\n|             |       |acc_norm|0.3580|卤  |0.0215|\n|winogrande   |      0|acc     |0.6069|卤  |0.0137|\n|piqa         |      0|acc     |0.7486|卤  |0.0101|\n|             |       |acc_norm|0.7546|卤  |0.0100|\n|hellaswag    |      0|acc     |0.4843|卤  |0.0050|\n|             |       |acc_norm|0.6388|卤  |0.0048|\n|boolq        |      1|acc     |0.6193|卤  |0.0085|\n\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.\n",
26244:    "readme": "---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: transformers\ninference: false\nthumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\ntags:\n- gpt\n- llm\n- large language model\n- open-source\ndatasets:\n- h2oai/openassistant_oasst1_h2ogpt_graded\n---\n# h2oGPT Model Card\n## Summary\n\nH2O.ai's `h2ogpt-oasst1-falcon-40b` is a 40 billion parameter instruction-following large language model licensed for commercial use.\n\n- Base model: [tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b)\n- Fine-tuning dataset: [h2oai/openassistant_oasst1_h2ogpt_graded](https://huggingface.co/datasets/h2oai/openassistant_oasst1_h2ogpt_graded)\n- Data-prep and fine-tuning code: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n- Training logs: [zip](https://huggingface.co/h2oai/h2ogpt-oasst1-falcon-40b/blob/main/falcon-40b.h2oaiopenassistant_oasst1_h2ogpt_graded.3_epochs.2e023709e9a36283986d136e66cb94e0bd7e6452.8.zip)\n- Paper: [arxiv.org/abs/2306.08161](https://arxiv.org/abs/2306.08161)\n\n## Chatbot\n\n- Run your own chatbot: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n[![H2O.ai GitHub](https://user-images.githubusercontent.com/6147661/232930822-e7170e4d-8aa1-4f7a-ad70-ece9cdd8b0cb.png)](https://github.com/h2oai/h2ogpt)\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the following libraries installed.\n\n```bash\npip install transformers==4.29.2\npip install accelerate==0.19.0\npip install torch==2.0.1\npip install einops==0.6.1\n```\n\n```python\nimport torch\nfrom transformers import pipeline, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-falcon-40b\", padding_side=\"left\")\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-falcon-40b\", tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type=\"human_bot\")\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\nAlternatively, if you prefer to not use `trust_remote_code=True` you can download [instruct_pipeline.py](https://huggingface.co/h2oai/h2ogpt-oasst1-falcon-40b/blob/main/h2oai_pipeline.py),\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\n\n```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-falcon-40b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-falcon-40b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type=\"human_bot\")\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n## Model Architecture\n\n```\nRWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 8192)\n    (h): ModuleList(\n      (0-59): 60 x DecoderLayer(\n        (ln_attn): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n        (ln_mlp): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value): Linear(in_features=8192, out_features=9216, bias=False)\n          (dense): Linear(in_features=8192, out_features=8192, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=8192, out_features=32768, bias=False)\n          (act): GELU(approximate='none')\n          (dense_4h_to_h): Linear(in_features=32768, out_features=8192, bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=8192, out_features=65024, bias=False)\n)\n```\n\n## Model Configuration\n\n```json\nRWConfig {\n  \"_name_or_path\": \"h2oai/h2ogpt-oasst1-falcon-40b\",\n  \"alibi\": false,\n  \"apply_residual_connection_post_layernorm\": false,\n  \"architectures\": [\n    \"RWForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"auto_map\": {\n    \"AutoConfig\": \"tiiuae/falcon-40b--configuration_RW.RWConfig\",\n    \"AutoModel\": \"tiiuae/falcon-40b--modelling_RW.RWModel\",\n    \"AutoModelForCausalLM\": \"tiiuae/falcon-40b--modelling_RW.RWForCausalLM\",\n    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-40b--modelling_RW.RWForQuestionAnswering\",\n    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-40b--modelling_RW.RWForSequenceClassification\",\n    \"AutoModelForTokenClassification\": \"tiiuae/falcon-40b--modelling_RW.RWForTokenClassification\"\n  },\n  \"bias\": false,\n  \"bos_token_id\": 11,\n  \"custom_pipelines\": {\n    \"text-generation\": {\n      \"impl\": \"h2oai_pipeline.H2OTextGenerationPipeline\",\n      \"pt\": \"AutoModelForCausalLM\"\n    }\n  },\n  \"eos_token_id\": 11,\n  \"hidden_dropout\": 0.0,\n  \"hidden_size\": 8192,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"RefinedWeb\",\n  \"n_head\": 128,\n  \"n_head_kv\": 8,\n  \"n_layer\": 60,\n  \"parallel_attn\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.30.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 65024\n}\n\n```\n\n## Model Validation\n\nModel validation results using [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n\n[eval source code](https://github.com/h2oai/h2ogpt/issues/216#issuecomment-1579573101)\n\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.5196|卤  |0.0146|\n|             |       |acc_norm|0.5461|卤  |0.0145|\n|arc_easy     |      0|acc     |0.8190|卤  |0.0079|\n|             |       |acc_norm|0.7799|卤  |0.0085|\n|boolq        |      1|acc     |0.8514|卤  |0.0062|\n|hellaswag    |      0|acc     |0.6485|卤  |0.0048|\n|             |       |acc_norm|0.8314|卤  |0.0037|\n|openbookqa   |      0|acc     |0.3860|卤  |0.0218|\n|             |       |acc_norm|0.4880|卤  |0.0224|\n|piqa         |      0|acc     |0.8194|卤  |0.0090|\n|             |       |acc_norm|0.8335|卤  |0.0087|\n|winogrande   |      0|acc     |0.7751|卤  |0.0117|\n\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.\n",
28188:    "readme": "---\nlicense: other\nlanguage:\n- en\nlibrary_name: transformers\ninference: false\nthumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\ntags:\n- gpt\n- llm\n- large language model\n- open-source\ndatasets:\n- h2oai/openassistant_oasst1_h2ogpt_graded\n---\n# h2oGPT Model Card\n## Summary\n\nH2O.ai's `h2ogpt-research-oasst1-llama-65b` is a 65 billion parameter instruction-following large language model (NOT licensed for commercial use).\n\n- Base model: [decapoda-research/llama-65b-hf](https://huggingface.co/decapoda-research/llama-65b-hf)\n- Fine-tuning dataset: [h2oai/openassistant_oasst1_h2ogpt_graded](https://huggingface.co/datasets/h2oai/openassistant_oasst1_h2ogpt_graded)\n- Data-prep and fine-tuning code: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n- Training logs: [zip](https://huggingface.co/h2oai/h2ogpt-research-oasst1-llama-65b/blob/main/llama-65b-hf.h2oaiopenassistant_oasst1_h2ogpt_graded.1_epochs.113510499324f0f007cbec9d9f1f8091441f2469.3.zip)\n\n## Chatbot\n\n- Run your own chatbot: [H2O.ai GitHub](https://github.com/h2oai/h2ogpt)\n[![H2O.ai GitHub](https://user-images.githubusercontent.com/6147661/232930822-e7170e4d-8aa1-4f7a-ad70-ece9cdd8b0cb.png)](https://github.com/h2oai/h2ogpt)\n\n## Usage\n\nTo use the model with the `transformers` library on a machine with GPUs, first make sure you have the following libraries installed.\n\n```bash\npip install transformers==4.29.2\npip install accelerate==0.19.0\npip install torch==2.0.1\npip install einops==0.6.1\n```\n\n```python\nimport torch\nfrom transformers import pipeline, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-research-oasst1-llama-65b\", padding_side=\"left\")\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-research-oasst1-llama-65b\", tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type=\"human_bot\")\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\nAlternatively, if you prefer to not use `trust_remote_code=True` you can download [instruct_pipeline.py](https://huggingface.co/h2oai/h2ogpt-research-oasst1-llama-65b/blob/main/h2oai_pipeline.py),\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\n\n```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-research-oasst1-llama-65b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-research-oasst1-llama-65b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type=\"human_bot\")\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n## Model Architecture\n\n```\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 8192, padding_idx=31999)\n    (layers): ModuleList(\n      (0-79): 80 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n          (k_proj): Linear(in_features=8192, out_features=8192, bias=False)\n          (v_proj): Linear(in_features=8192, out_features=8192, bias=False)\n          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n)\n```\n\n## Model Configuration\n\n```json\nLlamaConfig {\n  \"_name_or_path\": \"h2oai/h2ogpt-research-oasst1-llama-65b\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"bos_token_id\": 0,\n  \"custom_pipelines\": {\n    \"text-generation\": {\n      \"impl\": \"h2oai_pipeline.H2OTextGenerationPipeline\",\n      \"pt\": \"AutoModelForCausalLM\"\n    }\n  },\n  \"eos_token_id\": 1,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 8192,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 22016,\n  \"max_position_embeddings\": 2048,\n  \"max_sequence_length\": 2048,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 64,\n  \"num_hidden_layers\": 80,\n  \"pad_token_id\": -1,\n  \"rms_norm_eps\": 1e-05,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.30.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n```\n\n## Model Validation\n\nModel validation results using [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n\nTBD\n\n\n## Disclaimer\n\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\n\n- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\n- Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\n- Use at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\n- Ethical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\n- Reporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\n- Changes to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\n\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.\n",
