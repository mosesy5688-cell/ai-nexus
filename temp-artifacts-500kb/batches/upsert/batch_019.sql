/* LOGS:
Downloading image for github-datawhalechina-pumpkin-book from https://github.com/datawhalechina.png
Image converted to WebP: data/images/github-datawhalechina-pumpkin-book.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-modular-modular', 'github--modular--modular', 'modular', 'modular', '<div align="center"> <img src="https://modular-assets.s3.amazonaws.com/images/modular_github_logo_bg.png"> [About Modular] | [Get started] | [API docs] | [Contributing] | [Changelog] </div> [About Modular]: https://www.modular.com/ [Get started]: https://docs.modular.com/max/get-started [API docs]: https://docs.modular.com/max/api [Contributing]: ./CONTRIBUTING.md [Changelog]: https://docs.modular.com/max/changelog --- [Join us next Thursday, December 11th][dec-meetup] at Modular''s Los Altos ...', '["ai","language","machine-learning","max","modular","mojo","programming-language","mojo"]', 'other', 25312, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/modular/modular","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n    <img src="https://modular-assets.s3.amazonaws.com/images/modular_github_logo_bg.png">\n\n  [About Modular] | [Get started] | [API docs] | [Contributing] | [Changelog]\n</div>\n\n[About Modular]: https://www.modular.com/\n[Get started]: https://docs.modular.com/max/get-started\n[API docs]: https://docs.modular.com/max/api\n[Contributing]: ./CONTRIBUTING.md\n[Changelog]: https://docs.modular.com/max/changelog\n\n---\n[Join us next Thursday, December 11th][dec-meetup] at Modular''s Los Altos\noffices for a [Modular Meetup][meetup-group] going inside the MAX platform!\n\n# Modular Platform\n\n> A unified platform for AI development and deployment, including **MAX**ðŸ§‘â€ðŸš€ and\n**Mojo**ðŸ”¥.\n\nThe Modular Platform is an open and fully-integrated suite of AI libraries\nand tools that accelerates model serving and scales GenAI deployments. It\nabstracts away hardware complexity so you can run the most popular open\nmodels with industry-leading GPU and CPU performance without any code changes.\n\n![](https://docs.modular.com/images/modular-container-stack.png?20250513)\n\n## Get started\n\nYou don''t need to clone this repo.\n\nYou can install Modular as a `pip` or `conda` package and then start an\nOpenAI-compatible endpoint with a model of your choice.\n\nTo get started with the Modular Platform and serve a model using the MAX\nframework, see [the quickstart guide](https://docs.modular.com/max/get-started).\n\n> [!NOTE]\n> **Nightly vs. stable releases**\n> If you cloned the repo and want a stable release, run\n  `git checkout modular/vX.X` to match the version.\n> The `main` branch tracks nightly builds, while the `stable` branch matches\n  the latest released version.\n\nAfter your model endpoint is up and running, you can start sending the model\ninference requests using\n[our OpenAI-compatible REST API](https://docs.modular.com/max/api/serve).\n\nTry running hundreds of other models from\n[our model repository](https://builds.modular.com/?category=models).\n\n## Deploy our container\n\nThe MAX container is our Kubernetes-compatible Docker container for convenient\ndeployment, which uses the MAX framework''s built-in inference server. We have\nseparate containers for NVIDIA and AMD GPU environments, and a unified container\nthat works with both.\n\nFor example, you can start a container for an NVIDIA GPU with this command:\n\n```sh\ndocker run --gpus=1 \\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\n    -p 8000:8000 \\n    modular/max-nvidia-full:latest \\n    --model-path google/gemma-3-27b-it\n```\n\nFor more information, see our [MAX container\ndocs](https://docs.modular.com/max/container) or the [Modular Docker Hub\nrepository](https://hub.docker.com/u/modular).\n\n## About the repo\n\nWe''re constantly open-sourcing more of the Modular Platform and you can find\nall of it in here. As of May, 2025, this repo includes over 450,000 lines of\ncode from over 6000 contributors, providing developers with production-grade\nreference implementations and tools to extend the Modular Platform with new\nalgorithms, operations, and hardware targets. It is quite likely **the world''s\nlargest repository of open source CPU and GPU kernels**!\n\nHighlights include:\n\n- Mojo standard library: [/mojo/stdlib](mojo/stdlib)\n- MAX GPU and CPU kernels: [/max/kernels](max/kernels) (Mojo kernels)\n- MAX inference server: [/max/serve](max/serve) (OpenAI-compatible endpoint)\n- MAX model pipelines: [/max/pipelines](max/pipelines) (Python-based graphs)\n- Code example: [/examples](examples)\n\nThis repo has two major branches:\n\n- The [`main`](https://github.com/modular/modular/tree/main) branch, which is\nin sync with the nightly build and subject to new bugs. Use this branch for\n[contributions](./CONTRIBUTING.md), or if you [installed the nightly\nbuild](https://docs.modular.com/max/packages).\n\n- The [`stable`](https://github.com/modular/modular/tree/stable) branch, which\nis in sync with the last stable released version of Mojo. Use the examples in\nhere if you [installed the stable\nbuild](https://docs.modular.com/max/packages).\n\n## News & Announcements\n\n**[2025/11]** [Modular Platform 25.7][25.7] provides a fully open MAX Python\nAPI, expanded hardware support for NVIDIA Grace superchips, improved Mojo GPU\nprogramming experience, and much more.\n\n**[2025/11]** We met with the community at\n[PyTorch 2025 + the LLVM Developers'' Meeting][pytorch-llvm] to solicit\ncommunity input into how the Modular platform can reduce fragmentation and\nprovide a unified AI stack.\n\n**[2025/09]** [Modular raises $250M][funding] to scale AI''s unified compute\nlayer, bringing total funding to $380M at a $1.6B valuation.\n\n**[2025/09]** [Modular Platform 25.6][25.6] delivers a unified compute layer\nspanning from laptops to datacenter GPUs, with industry-leading throughput on\nNVIDIA Blackwell (B200) and AMD MI355X.\n\n**[2025/08]** [Modular Platform 25.5][25.5] introduces Large Scale Batch\nInference through a partnership with SF Compute + open source launch of the\nMAX Graph API and more.\n\n**[2025/08]** We hosted our [Los Altos Meetup][la-meetup] featuring talks from\nChris Lattner on democratizing AI compute and Inworld AI on production voice AI.\n\n**[2025/06]** [AMD partnership announced][amd] â€” Modular Platform now generally\navailable across AMD''s MI300 and MI325 GPU portfolio.\n\n**[2025/06]** [Modular Hack Weekend][hack-weekend] brought developers together\nto build custom kernels, model architectures, and PyTorch custom ops with\nMojo and MAX.\n\n**[2025/05]** Over 100 engineers gathered at AGI House for our first\n[GPU Kernel Hackathon][hackathon], featuring talks from Modular and\nAnthropic engineers.\n\n[25.7]: https://www.modular.com/blog/modular-25-7-faster-inference-safer-gpu-programming-and-a-more-unified-developer-experience\n[pytorch-llvm]: https://www.modular.com/blog/pytorch-and-llvm-in-2025-keeping-up-with-ai-innovation\n[25.6]: https://www.modular.com/blog/modular-25-6-unifying-the-latest-gpus-from-nvidia-amd-and-apple\n[25.5]: https://www.modular.com/blog/modular-platform-25-5\n[la-meetup]: https://lu.ma/modular-aug-meetup\n[amd]: https://www.modular.com/blog/modular-x-amd-unleashing-ai-performance-on-amd-gpus\n[hack-weekend]: https://www.meetup.com/modular-meetup-group/events/308311461/\n[hackathon]: https://www.modular.com/blog/modverse-48\n[dec-meetup]: https://www.meetup.com/modular-meetup-group/events/311998048/\n[meetup-group]: https://www.meetup.com/modular-meetup-group/\n\n---\n\n## Community & Events\n\nWe host regular meetups, hackathons, and community calls. Join us!\n\n| Channel               | Link                                |\n|-----------------------|-------------------------------------|\n| ðŸ’¬ Discord            | [discord.gg/modular][discord]       |\n| ðŸ’¬ Forum              | [forum.modular.com][forum]          |\n| ðŸ“… Meetup Group       | [meetup.com/modular-meetup-group][meetup-group] |\n| ðŸŽ¥ Community Meetings | Recordings on [YouTube][youtube]    |\n\n**Upcoming events** will be posted on our [Meetup page][meetup-group] and\n[Discord][discord].\n\n[discord]: https://discord.gg/modular\n[forum]: https://forum.modular.com/\n[youtube]: https://www.youtube.com/@modularinc\n\n## Contribute\n\nThanks for your interest in contributing to this repository!\n\nWe accept contributions to the [Mojo standard library](./mojo), [MAX AI\nkernels](./max/kernels), code examples, and Mojo docs, but currently not to any\nother parts of the repository.\n\nPlease see the [Contribution Guide](./CONTRIBUTING.md) for instructions.\n\nWe also welcome your bug reports.  If you have a bug, please [file an issue\nhere](https://github.com/modular/modular/issues/new/choose).\n\n## Contact us\n\nIf you''d like to chat with the team and other community members, please send a\nmessage to our [Discord channel](https://discord.gg/modular) and [our\nforum board](https://forum.modular.com/).\n\n## License\n\nThis repository and its contributions are licensed under the Apache License\nv2.0 with LLVM Exceptions (see the LLVM [License](https://llvm.org/LICENSE.txt)).\nModular, MAX and Mojo usage and distribution are licensed under the\n[Modular Community License](https://www.modular.com/legal/community).\n\n### Third party licenses\n\nYou are entirely responsible for checking and validating the licenses of\nthird parties (i.e. Huggingface) for related software and libraries that are downloaded.\n\n## Thanks to our contributors\n\n<a href="https://github.com/modular/modular/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=modular/modular" />\n</a>\n', '{"language":"Mojo","stars":25312,"forks":2736,"watchers":25312,"open_issues":788,"topics":["ai","language","machine-learning","max","modular","mojo","programming-language"],"default_branch":"main","size_kb":132873,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"},{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"},{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"},{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"}]', NULL, 'NOASSERTION', 'approved', 65, 'ee1744892ede74755510c105f3daf067', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-modular-modular from https://github.com/modular.png
Image converted to WebP: data/images/github-modular-modular.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-shap-shap', 'github--shap--shap', 'shap', 'shap', '<p align="center"> <img src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/shap_header.svg" width="800" /> </p> --- !License !Tests !Downloads **SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations). <!--**SHAP (SHapley Additive exP...', '["deep-learning","explainability","gradient-boosting","interpretability","machine-learning","shap","shapley","jupyter notebook"]', 'other', 24810, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/shap/shap","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/shap_header.svg" width="800" />\n</p>\n\n---\n[![PyPI](https://img.shields.io/pypi/v/shap)](https://pypi.org/project/shap/)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/shap)](https://anaconda.org/conda-forge/shap)\n![License](https://img.shields.io/github/license/shap/shap)\n![Tests](https://github.com/shap/shap/actions/workflows/run_tests.yml/badge.svg)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/shap/shap/master)\n[![Documentation Status](https://readthedocs.org/projects/shap/badge/?version=latest)](https://shap.readthedocs.io/en/latest/?badge=latest)\n![Downloads](https://img.shields.io/pypi/dm/shap)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/shap)](https://pypi.org/pypi/shap/)\n\n\n**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](#citations) for details and citations).\n\n<!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).-->\n\n\n\n## Install\n\nSHAP can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n\n<pre>\npip install shap\n<i>or</i>\nconda install -c conda-forge shap\n</pre>\n\n## Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n\nWhile SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our [Nature MI paper](https://rdcu.be/b0z70)). Fast C++ implementations are supported for *XGBoost*, *LightGBM*, *CatBoost*, *scikit-learn* and *pyspark* tree models:\n\n```python\nimport xgboost\nimport shap\n\n# train an XGBoost model\nX, y = shap.datasets.california()\nmodel = xgboost.XGBRegressor().fit(X, y)\n\n# explain the model''s predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\nexplainer = shap.Explainer(model)\nshap_values = explainer(X)\n\n# visualize the first prediction''s explanation\nshap.plots.waterfall(shap_values[0])\n```\n\n<p align="center">\n  <img width="616" src="./docs/artwork/california_waterfall.png" />\n</p>\n\nThe above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our [Nature BME paper](https://rdcu.be/baVbR)):\n\n```python\n# visualize the first prediction''s explanation with a force plot\nshap.plots.force(shap_values[0])\n```\n\n<p align="center">\n  <img width="811" src="./docs/artwork/california_instance.png" />\n</p>\n\nIf we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):\n\n```python\n# visualize all the training set predictions\nshap.plots.force(shap_values[:500])\n```\n\n<p align="center">\n  <img width="811" src="./docs/artwork/california_dataset.png" />\n</p>\n\nTo understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature''s responsibility for a change in the model output, the plot below represents the change in predicted house price as the latitude changes. Vertical dispersion at a single value of latitude represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the `color` argument the scatter plot will pick the best feature to color by. In this case it picks longitude.\n\n```python\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, "Latitude"], color=shap_values)\n```\n\n<p align="center">\n  <img width="544" src="./docs/artwork/california_scatter.png" />\n</p>\n\n\nTo get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that higher median incomes increases the predicted home price.\n\n```python\n# summarize the effects of all the features\nshap.plots.beeswarm(shap_values)\n```\n\n<p align="center">\n  <img width="583" src="./docs/artwork/california_beeswarm.png" />\n</p>\n\nWe can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n\n```python\nshap.plots.bar(shap_values)\n```\n\n<p align="center">\n  <img width="570" src="./docs/artwork/california_global_bar.png" />\n</p>\n\n## Natural language example (transformers)\n\nSHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:\n\n```python\nimport transformers\nimport shap\n\n# load a transformers pipeline model\nmodel = transformers.pipeline(''sentiment-analysis'', return_all_scores=True)\n\n# explain the model on two sample inputs\nexplainer = shap.Explainer(model)\nshap_values = explainer(["What a great movie! ...if you have no taste."])\n\n# visualize the first prediction''s explanation for the POSITIVE output class\nshap.plots.text(shap_values[0, :, "POSITIVE"])\n```\n\n<p align="center">\n  <img width="811" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/sentiment_analysis_plot.png" />\n</p>\n\n## Deep learning example with DeepExplainer (TensorFlow/Keras models)\n\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):\n\n```python\n# ...include code from https://github.com/keras-team/keras/blob/master/examples/demo_mnist_convnet.py\n\nimport shap\nimport numpy as np\n\n# select a set of background examples to take an expectation over\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n\n# explain predictions of the model on four images\ne = shap.DeepExplainer(model, background)\n# ...or pass tensors directly\n# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\nshap_values = e.shap_values(x_test[1:5])\n\n# plot the feature attributions\nshap.image_plot(shap_values, -x_test[1:5])\n```\n\n<p align="center">\n  <img width="820" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/mnist_image_plot.png" />\n</p>\n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model''s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the ''zero'' image the blank middle is important, while for the ''four'' image the lack of a connection on top makes it a four instead of a nine.\n\n\n## Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)\n\nExpected gradients combines ideas from [Integrated Gradients](https://arxiv.org/abs/1703.01365), SHAP, and [SmoothGrad](https://arxiv.org/abs/1706.03825) into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.\n\n```python\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nimport keras.backend as K\nimport numpy as np\nimport json\nimport shap\n\n# load pre-trained model and choose two images to explain\nmodel = VGG16(weights=''imagenet'', include_top=True)\nX,y = shap.datasets.imagenet50()\nto_explain = X[[39,41]]\n\n# load the ImageNet class names\nurl = "https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"\nfname = shap.datasets.cache(url)\nwith open(fname) as f:\n    class_names = json.load(f)\n\n# explain how the input to the 7th layer of the model explains the top two classes\ndef map2layer(x, layer):\n    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n    return K.get_session().run(model.layers[layer].input, feed_dict)\ne = shap.GradientExplainer(\n    (model.layers[7].input, model.layers[-1].output),\n    map2layer(X, 7),\n    local_smoothing=0 # std dev of smoothing noise\n)\nshap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n\n# get the names for the classes\nindex_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n\n# plot the explanations\nshap.image_plot(shap_values, to_explain, index_names)\n```\n\n<p align="center">\n  <img width="500" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/gradient_imagenet_plot.png" />\n</p>\n\nPredictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using `ranked_outputs=2` we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).\n\n## Model agnostic example with KernelExplainer (explains any function)\n\nKernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.\n\n```python\nimport sklearn\nimport shap\nfrom sklearn.model_selection import train_test_split\n\n# print the JS visualization code to the notebook\nshap.initjs()\n\n# train a SVM classifier\nX_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\nsvm = sklearn.svm.SVC(kernel=''rbf'', probability=True)\nsvm.fit(X_train, Y_train)\n\n# use Kernel SHAP to explain test set predictions\nexplainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")\nshap_values = explainer.shap_values(X_test, nsamples=100)\n\n# plot the SHAP values for the Setosa output of the first instance\nshap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")\n```\n<p align="center">\n  <img width="810" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/iris_instance.png" />\n</p>\n\nThe above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.\n\nIf we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:\n\n```python\n# plot the SHAP values for the Setosa output of all instances\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link="logit")\n```\n<p align="center">\n  <img width="813" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/iris_dataset.png" />\n</p>\n\n## SHAP Interaction Values\n\nSHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with `shap.TreeExplainer(model).shap_interaction_values(X)`. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):\n\n<p align="center">\n  <img width="483" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/nhanes_age_sex_interaction.png" />\n</p>\n\n## Sample notebooks\n\nThe notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.\n\n### TreeExplainer\n\nAn implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.\n\n- [**NHANES survival model with XGBoost and SHAP interaction values**](https://shap.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and `shap` to uncover complex risk factor relationships.\n\n- [**Census income classification with LightGBM**](https://shap.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html) - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using `shap`.\n\n- [**League of Legends Win Prediction with XGBoost**](https://shap.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html) - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.\n\n### DeepExplainer\n\nAn implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.\n\n- [**MNIST Digit classification with Keras**](https://shap.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using `shap`.\n\n- [**Keras LSTM for IMDB Sentiment Classification**](https://shap.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using `shap`.\n\n### GradientExplainer\n\nAn implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.\n\n- [**Explain an Intermediate Layer of VGG16 on ImageNet**](https://shap.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html) - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.\n\n### LinearExplainer\n\nFor a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.\n\n- [**Sentiment Analysis with Logistic Regression**](https://shap.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html) - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.\n\n### KernelExplainer\n\nAn implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.\n\n- [**Census income classification with scikit-learn**](https://shap.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html) - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using `shap`.\n\n- [**ImageNet VGG16 Model with Keras**](https://shap.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html) - Explain the classic VGG16 convolutional neural network''s predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.\n\n- [**Iris classification**](https://shap.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html) - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using `shap`.\n\n## Documentation notebooks\n\nThese notebooks comprehensively demonstrate how to use specific functions and objects.\n\n- [`shap.decision_plot` and `shap.multioutput_decision_plot`](https://shap.github.io/shap/notebooks/plots/decision_plot.html)\n\n- [`shap.dependence_plot`](https://shap.github.io/shap/notebooks/plots/dependence_plot.html)\n\n## Methods Unified by SHAP\n\n1. *LIME:* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n\n2. *Shapley sampling values:* Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.\n\n3. *DeepLIFT:* Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).\n\n4. *QII:* Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.\n\n5. *Layer-wise relevance propagation:* Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.\n\n6. *Shapley regression values:* Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.\n\n7. *Tree interpreter:* Saabas, Ando. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/\n\n## Citations\n\nThe algorithms and visualizations used in this package came primarily out of research in [Su-In Lee''s lab](https://suinlee.cs.washington.edu) at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):\n\n- For general use of SHAP you can read/cite our [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) ([bibtex](https://raw.githubusercontent.com/shap/shap/master/docs/references/shap_nips.bib)).\n- For TreeExplainer you can read/cite our [Nature Machine Intelligence paper](https://www.nature.com/articles/s42256-019-0138-9) ([bibtex](https://raw.githubusercontent.com/shap/shap/master/docs/references/tree_explainer.bib); [free access](https://rdcu.be/b0z70)).\n- For GPUTreeExplainer you can read/cite [this article](https://arxiv.org/abs/2010.13972).\n- For `force_plot` visualizations and medical applications you can read/cite our [Nature Biomedical Engineering paper](https://www.nature.com/articles/s41551-018-0304-0) ([bibtex](https://raw.githubusercontent.com/shap/shap/master/docs/references/nature_bme.bib); [free access](https://rdcu.be/baVbR)).\n\n<img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=189147091855991&ev=PageView&noscript=1" />\n', '{"language":"Jupyter Notebook","stars":24810,"forks":3459,"watchers":24810,"open_issues":605,"topics":["deep-learning","explainability","gradient-boosting","interpretability","machine-learning","shap","shapley"],"default_branch":"master","size_kb":294073,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:shap:shap","source_url":"https://github.com/shap/shap"},{"type":"has_code","target_id":"github:keras-team:keras","source_url":"https://github.com/keras-team/keras"}]', NULL, 'MIT', 'approved', 80, '8b73442bec6eea99b132b4cd0ae99a7d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-shap-shap from https://github.com/shap.png
Image converted to WebP: data/images/github-shap-shap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mxgmn-WaveFunctionCollapse', 'github--mxgmn--wavefunctioncollapse', 'WaveFunctionCollapse', 'mxgmn', 'This program generates bitmaps that are locally similar to the input bitmap. <p align="center"><img alt="main collage" src="images/wfc.png"></p> <p align="center"><img alt="main gif" src="images/wfc.gif"></p> Local similarity means that * (C1) The output should contain only those NxN patterns of pixels that are present in the input. * (Weak C2) Distribution of NxN patterns in the input should be similar to the distribution of NxN patterns over a sufficiently large number of outputs. In other ...', '["algorithm","csharp","gamedev","machine-learning","procedural-generation","wfc","c#"]', 'other', 24482, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mxgmn/WaveFunctionCollapse","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# WaveFunctionCollapse\nThis program generates bitmaps that are locally similar to the input bitmap.\n<p align="center"><img alt="main collage" src="images/wfc.png"></p>\n<p align="center"><img alt="main gif" src="images/wfc.gif"></p>\n\nLocal similarity means that\n\n* (C1) The output should contain only those NxN patterns of pixels that are present in the input.\n* (Weak C2) Distribution of NxN patterns in the input should be similar to the distribution of NxN patterns over a sufficiently large number of outputs. In other words, probability to meet a particular pattern in the output should be close to the density of such patterns in the input.\n\nIn the examples a typical value of N is 3.\n<p align="center"><img alt="local similarity" src="images/patterns.png"></p>\n\nWFC initializes output bitmap in a completely unobserved state, where each pixel value is in superposition of colors of the input bitmap (so if the input was black & white then the unobserved states are shown in different shades of grey). The coefficients in these superpositions are real numbers, not complex numbers, so it doesn''t do the actual quantum mechanics, but it was inspired by QM. Then the program goes into the observation-propagation cycle:\n\n* On each observation step an NxN region is chosen among the unobserved which has the lowest Shannon entropy. This region''s state then collapses into a definite state according to its coefficients and the distribution of NxN patterns in the input.\n* On each propagation step new information gained from the collapse on the previous step propagates through the output.\n\nOn each step the number of non-zero coefficients decreases and in the end we have a completely observed state, the wave function has collapsed.\n\nIt may happen that during propagation all the coefficients for a certain pixel become zero. That means that the algorithm has run into a contradiction and can not continue. The problem of determining whether a certain bitmap allows other nontrivial bitmaps satisfying condition (C1) is NP-hard, so it''s impossible to create a fast solution that always finishes. In practice, however, the algorithm runs into contradictions surprisingly rarely.\n\nWave Function Collapse algorithm has been implemented in\n[C++](https://github.com/math-fehr/fast-wfc),\n[Python](https://github.com/ikarth/wfc_2019f),\n[Kotlin](https://github.com/j-roskopf/WFC),\n[Rust](https://github.com/Elwqnn/wfc),\n[Julia](https://github.com/roberthoenig/WaveFunctionCollapse.jl),\n[Go](https://github.com/shawnridgeway/wfc),\n[Haxe](https://github.com/Mitim-84/WFC-Gen),\n[Java](https://github.com/sjcasey21/wavefunctioncollapse),\n[Clojure](https://github.com/sjcasey21/wavefunctioncollapse-clj),\n[Free Pascal](https://github.com/PascalCorpsman/mini_projects/tree/main/miniprojects/Wave_function_collapse/Overlap_model),\n[Dart](https://github.com/rick-dalley/wfc),\n[p5js](https://github.com/D-T-666/wave-function-collapse-p5),\n[JavaScript](https://github.com/kchapelier/wavefunctioncollapse)\nand adapted to [Unity](https://selfsame.itch.io/unitywfc),\n[Unreal Engine 5](https://docs.unrealengine.com/5.0/en-US/BlueprintAPI/WaveFunctionCollapse/)\nand [Houdini](https://www.sidefx.com/tutorials/wfc-dungeon-generator/).\nYou can [build WFC from source](https://github.com/mxgmn/WaveFunctionCollapse#how-to-build),\ndownload an official [release](https://github.com/mxgmn/WaveFunctionCollapse/releases) for Windows,\ndownload an interactive graphical version from [itch.io](https://exutumno.itch.io/wavefunctioncollapse)\nor [run it in the browser](http://www.kchapelier.com/wfc-example/overlapping-model.html).\nWFC generates levels in [Bad North](https://www.badnorth.com/),\n[Caves of Qud](https://store.steampowered.com/app/333640/Caves_of_Qud/),\n[Dead Static Drive](https://twitter.com/deadstaticdrive),\n[Townscaper](https://store.steampowered.com/app/1291340/Townscaper/),\n[Matrix Awakens](https://www.youtube.com/watch?v=usJrcwN6T4I),\n[several](https://arcadia-clojure.itch.io/proc-skater-2016)\n[smaller](https://arcadia-clojure.itch.io/swapland)\n[games](https://marian42.itch.io/wfc) and many prototypes.\nIt led to [new](https://escholarship.org/uc/item/3rm1w0mn)\n[research](https://hal.inria.fr/hal-01706539v3/document).\nFor [more](https://twitter.com/OskSta/status/784847588893814785)\n[related](https://twitter.com/dwtw/status/810166761270243328)\n[work](https://github.com/mewo2/oisin),\n[explanations](https://trasevol.dog/2017/09/01/di19/),\n[interactive demos](http://oskarstalberg.com/game/wave/wave.html),\n[guides](https://www.dropbox.com/s/zeiat1w8zre9ro8/Knots%20breakdown.png?dl=0),\n[tutorials](http://www.procjam.com/tutorials/wfc/)\nand [examples](https://twitter.com/ExUtumno/status/895684431477747715)\nsee the [ports, forks and spinoffs section](https://github.com/mxgmn/WaveFunctionCollapse#notable-ports-forks-and-spinoffs).\n\nWatch a video demonstration of WFC algorithm on YouTube: [https://youtu.be/DOQTr2Xmlz0](https://youtu.be/DOQTr2Xmlz0)\n\n## Algorithm\n1. Read the input bitmap and count NxN patterns.\n    1. (optional) Augment pattern data with rotations and reflections.\n2. Create an array with the dimensions of the output (called "wave" in the source). Each element of this array represents a state of an NxN region in the output. A state of an NxN region is a superposition of NxN patterns of the input with boolean coefficients (so a state of a pixel in the output is a superposition of input colors with real coefficients). False coefficient means that the corresponding pattern is forbidden, true coefficient means that the corresponding pattern is not yet forbidden.\n3. Initialize the wave in the completely unobserved state, i.e. with all the boolean coefficients being true.\n4. Repeat the following steps:\n    1. Observation:\n        1. Find a wave element with the minimal nonzero entropy. If there is no such elements (if all elements have zero or undefined entropy) then break the cycle (4) and go to step (5).\n        2. Collapse this element into a definite state according to its coefficients and the distribution of NxN patterns in the input.\n    2. Propagation: propagate information gained on the previous observation step.\n5. By now all the wave elements are either in a completely observed state (all the coefficients except one being zero) or in the contradictory state (all the coefficients being zero). In the first case return the output. In the second case finish the work without returning anything.\n\n## Tilemap generation\nThe simplest nontrivial case of the algorithm is when NxN=1x2 (well, NxM). If we simplify it even further by storing not the probabilities of pairs of colors but the probabilities of colors themselves, we get what we call a "simple tiled model". The propagation phase in this model is just adjacency constraint propagation. It''s convenient to initialize the simple tiled model with a list of tiles and their adjacency data (adjacency data can be viewed as a large set of very small samples) rather than a sample bitmap.\n<p align="center"><a href="http://i.imgur.com/jIctSoT.gifv"><img src="images/tile.gif"/></a></p>\n<!--<p align="center">\n  <a href="images/tile.gif">GIF</a> |\n  <a href="http://i.imgur.com/jIctSoT.gifv">GIFV</a>\n</p>-->\n\nLists of all the possible pairs of adjacent tiles in practical tilesets can be quite long, so I implemented a symmetry system for tiles to shorten the enumeration. In this system each tile should be assigned with its symmetry type.\n<p align="center"><img alt="symmetries" src="images/symmetry-system.png"></p>\n\nNote that the tiles have the same symmetry type as their assigned letters (or, in other words, actions of the \ndihedral group D4 are isomorphic for tiles and their corresponding letters). With this system it''s enough to enumerate pairs of adjacent tiles only up to symmetry, which makes lists of adjacencies for tilesets with many symmetrical tiles (even the summer tileset, despite drawings not being symmetrical the system considers such tiles to be symmetrical) several times shorter.\n<p align="center">\n<img alt="knots" src="images/knots.png">\n<img alt="tiled rooms" src="images/rooms.png">\n<img alt="circuit 1" src="images/circuit-1.png">\n<img alt="circuit 2" src="images/circuit-2.png">\n<img alt="circles" src="images/circles.png">\n<img alt="castle" src="images/castle.png">\n<img alt="summer 1" src="images/summer-1.png">\n<img alt="summer 2" src="images/summer-2.png">\n</p>\n\nNote that the unrestrained knot tileset (with all 5 tiles being allowed) is not interesting for WFC, because you can''t run into a situation where you can''t place a tile. We call tilesets with this property "easy". Without special heuristics easy tilesets don''t produce interesting global arrangements, because correlations of tiles in easy tilesets quickly fall off with a distance. Many easy tilesets can be found on [Guy Walker''s website](http://cr31.co.uk/stagecast/wang/tiles_e.html). Consider the "Dual" 2-edge tileset there. How can it generate knots (without t-junctions, not easy) while being easy? The answer is, it can only generate a narrow class of knots, it can''t produce an arbitrary knot.\n\nNote also that Circuit, Summer and Rooms tilesets are non-Wang. That is, their adjacency data cannot be induced from edge labels. For example, in Circuit two Corners cannot be adjacent, yet they can be connected with a Connection tile, and diagonal tracks cannot change direction.\n\n## Higher dimensions\nWFC algorithm in higher dimensions works completely the same way as in dimension 2, though performance becomes an issue. These voxel models were generated with N=2 overlapping tiled model using 5x5x5 and 5x5x2 blocks and additional heuristics (height, density, curvature, ...).\n<p align="center"><img alt="voxels" src="images/castles-3d.png"></p>\n\nHigher resolution screenshots: [1](http://i.imgur.com/0bsjlBY.png), [2](http://i.imgur.com/GduN0Vr.png), [3](http://i.imgur.com/IEOsbIy.png).\n\n[MarkovJunior](https://github.com/mxgmn/MarkovJunior) repository contains an implementation of the 3d simple tiled model with many [tilesets](https://github.com/mxgmn/MarkovJunior/tree/main/resources/tilesets) and [examples](https://github.com/mxgmn/MarkovJunior/blob/main/images/top-1764.png).\n\n## Constrained synthesis\nWFC algorithm supports constraints. Therefore, it can be easily combined with other generative algorithms or with manual creation.\n\nHere is WFC autocompleting a level started by a human:\n<p align="center"><a href="http://i.imgur.com/X3aNDUv.gifv"><img src="images/constrained.gif"/></a></p>\n<!--<p align="center">\n  <a href="images/constrained.gif">GIF</a> |\n  <a href="http://i.imgur.com/X3aNDUv.gifv">GIFV</a>\n</p>-->\n\n[ConvChain](https://github.com/mxgmn/ConvChain) algorithm satisfies the strong version of the condition (C2): the limit distribution of NxN patterns in the outputs it is producing is exactly the same as the distributions of patterns in the input. However, ConvChain doesn''t satisfy (C1): it often produces noticeable defects. It makes sense to run ConvChain first to get a well-sampled configuration and then run WFC to correct local defects. This is similar to a common strategy in optimization: first run a Monte-Carlo method to find a point close to a global optimum and then run a gradient descent from that point for greater accuracy.\n\nP. F. Harrison''s [texture synthesis](https://github.com/mxgmn/TextureSynthesis) algorithm is significantly faster than WFC, but it has trouble with long correlations (for example, it''s difficult for this algorithm to synthesize brick wall textures with correctly aligned bricks). But this is exactly where WFC shines, and Harrison''s algorithm supports constraints. It makes sense first to generate a perfect brick wall blueprint with WFC and then run a constrained texture synthesis algorithm on that blueprint.\n\n## Comments\nWhy the minimal entropy heuristic? I noticed that when humans draw something they often follow the [minimal entropy heuristic](images/lowest-entropy-heuristic.gif) themselves. That''s why the algorithm is so enjoyable to watch.\n\nThe overlapping model relates to the simple tiled model the same way higher order Markov chains relate to order one Markov chains.\n\nWFC''s propagation phase is very similar to the loopy belief propagation algorithm. In fact, I first programmed belief propagation, but then switched to constraint propagation with a saved stationary distribution, because BP is significantly slower without a massive parallelization (on a CPU) and didn''t produce significantly better results in my problems.\n\nNote that the "Simple Knot" and "Trick Knot" samples have 3 colors, not 2.\n\nOne of the dimensions can be time. In particular, d-dimensional WFC captures the behaviour of any (d-1)-dimensional cellular automata.\n\n## Used work\n1. Alexei A. Efros and Thomas K. Leung, [Texture Synthesis by Non-parametric Sampling](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/papers/efros-iccv99.pdf), 1999. WaveFunctionCollapse is a [texture synthesis](https://en.wikipedia.org/wiki/Texture_synthesis) algorithm. Compared to the earlier texture synthesis algorithms, WFC guarantees that the output contains only those NxN patterns that are present in the input. This makes WFC perfect for level generation in games and pixel art, and less suited for large full-color textures.\n2. Paul C. Merrell, [Model Synthesis](http://graphics.stanford.edu/~pmerrell/thesis.pdf), 2009. Merrell derives adjacency constraints between tiles from an example model and generates a new larger model with the AC-3 algorithm. We generalize his approach to work with NxN overlapping patterns of tiles instead of individual tiles. This allows to use a single image as the input to the algorithm. By varying N, we can make the output look more like the input or less. We introduce the [lowest entropy heuristic](images/lowest-entropy-heuristic.gif) that removes the [directional bias](images/directional-bias.png) in generated results, is defined for irregular grids and is better suited for [pre-constrained problems](images/constrained.gif). We implement a tile symmetry system to reduce the sizes of inputs. We visualize partially observed states, either with [color averaging](images/wfc.gif) or [per-voxel voting](https://twitter.com/ExUtumno/status/900395635412787202). Merrell also introduced a method of incrementally modifying the model in parts to reduce the failure rate (which we don''t use here). Recently the author created a [page](https://paulmerrell.org/model-synthesis/) for model synthesis and published [code](https://github.com/merrell42/model-synthesis).\n3. Alan K. Mackworth, [Consistency in Networks of Relations](https://www.cs.ubc.ca/~mack/Publications/AI77.pdf), 1977. WFC translates a texture synthesis problem into a constraint satisfaction problem. Currently it uses the [AC-4 algorithm](http://www.cs.utah.edu/~tch/CS4300/resources/AC4.pdf) by Roger Mohr and Thomas C. Henderson, 1986.\n4. Paul F. Harrison, [Image Texture Tools](http://logarithmic.net/pfh-files/thesis/dissertation.pdf), 2005. WFC was also influenced by the declarative texture synthesis chapter of Paul Harrison''s dissertation. The author defines adjacency data of tiles by labeling their borders and uses backtracking search to fill the tilemap. A [demonstration of the algorithm](https://logarithmic.net/ghost.xhtml) is available on the web.\n\n## How to build\nWFC is a console application that depends only on the standard library. Get [.NET Core](https://www.microsoft.com/net/download) for Windows, Linux or macOS and run\n```\ndotnet run --configuration Release WaveFunctionCollapse.csproj\n```\nGenerated results are saved into the `output` folder. Edit `samples.xml` to change model parameters.\n\nAlternatively, use build instructions from the community for various platforms from the [relevant issue](https://github.com/mxgmn/WaveFunctionCollapse/issues/3). Casey Marshall made a [pull request](https://github.com/mxgmn/WaveFunctionCollapse/pull/18) that makes using the program with the command line more convenient and includes snap packaging.\n\n## Notable ports, forks and spinoffs\n* Emil Ernerfeldt made a [C++ port](https://github.com/emilk/wfc).\n* [Max Aller](https://github.com/nanodeath) made a Kotlin (JVM) library, [Kollapse](https://gitlab.com/nanodeath/kollapse). Joseph Roskopf made a line by line Kotlin [port](https://github.com/j-roskopf/WFC) of the optimized 2018 version. Edwin Jakobs made a [Kotlin library](https://github.com/edwinRNDR/wfc) that supports [3d examples](https://www.youtube.com/watch?v=g4Ih8wxBh1E).\n* [Kevin Chapelier](https://github.com/kchapelier) made a [JavaScript port](http://www.kchapelier.com/wfc-example/overlapping-model.html).\n* Oskar StÃ¥lberg programmed a 3d tiled model, a 2d tiled model for irregular grids on a sphere and is building beautiful 3d tilesets for them: [1](https://twitter.com/OskSta/status/787319655648100352), [2](https://twitter.com/OskSta/status/784847588893814785), [3](https://twitter.com/OskSta/status/784847933686575104), [4](https://twitter.com/OskSta/status/784848286272327680), [5](https://twitter.com/OskSta/status/793545297376972801), [6](https://twitter.com/OskSta/status/793806535898136576), [7](https://twitter.com/OskSta/status/802496920790777856), [8](https://twitter.com/OskSta/status/804291629561577472), [9](https://twitter.com/OskSta/status/806856212260278272), [10](https://twitter.com/OskSta/status/806904557502464000), [11](https://twitter.com/OskSta/status/818857408848130048), [12](https://twitter.com/OskSta/status/832633189277409280), [13](https://twitter.com/OskSta/status/851170356530475008), [14](https://twitter.com/OskSta/status/858301207936458752), [15](https://twitter.com/OskSta/status/863019585162932224).\n* [Joseph Parker](https://github.com/selfsame) adapted [WFC to Unity](https://selfsame.itch.io/unitywfc) and used it generate skateparks in the [Proc Skater 2016](https://arcadia-clojure.itch.io/proc-skater-2016) game, [fantastic plateaus](https://twitter.com/jplur_/status/929482200034226176) in the 2017 game [Swapland](https://arcadia-clojure.itch.io/swapland) and [platform levels](https://twitter.com/jplur_/status/1053458654454865921) in the 2018 game [Bug with a Gun](https://selfsame.itch.io/bug-with-a-gun).\n* [Martin O''Leary](https://github.com/mewo2) applied a [WFC-like algorithm](https://github.com/mewo2/oisin) to poetry generation: [1](https://twitter.com/mewo2/status/789167437518217216), [2](https://twitter.com/mewo2/status/789177702620114945), [3](https://twitter.com/mewo2/status/789187174683987968), [4](https://twitter.com/mewo2/status/789897712372183041).\n* [Nick Nenov](https://github.com/NNNenov) made a [3d voxel tileset](https://twitter.com/NNNenov/status/789903180226301953) based on my Castle tileset. Nick uses text output option in the tiled model to reconstruct 3d models in Cinema 4D.\n* Sean Leffler implemented the [overlapping model in Rust](https://github.com/sdleffler/collapse).\n* rid5x is making an [OCaml version of WFC](https://twitter.com/rid5x/status/782442620459114496).\n* I made an [interactive version](https://twitter.com/ExUtumno/status/798571284342837249) of the overlapping model, you can download the GUI executable from the [WFC itch.io page](https://exutumno.itch.io/wavefunctioncollapse).\n* [Brian Bucklew](https://github.com/unormal) built a level generation pipeline that applies WFC in multiple passes for the [Caves of Qud](http://store.steampowered.com/app/333640) game: [1](https://twitter.com/unormal/status/805987523596091392), [2](https://twitter.com/unormal/status/808566029387448320), [3](https://twitter.com/unormal/status/808523056259993601), [4](https://twitter.com/unormal/status/808523493994364928), [5](https://twitter.com/unormal/status/808519575264497666), [6](https://twitter.com/unormal/status/808519216185876480), [7](https://twitter.com/unormal/status/808795396508123136), [8](https://twitter.com/unormal/status/808860105093632001), [9](https://twitter.com/unormal/status/809637856432033792), [10](https://twitter.com/unormal/status/810239794433425408), [11](https://twitter.com/unormal/status/811034574973243393), [12](https://twitter.com/unormal/status/811720423419314176), [13](https://twitter.com/unormal/status/811034037259276290), [14](https://twitter.com/unormal/status/810971337309224960), [15](https://twitter.com/unormal/status/811405368777723909), [16](https://twitter.com/ptychomancer/status/812053801544757248), [17](https://twitter.com/unormal/status/812159308263788544), [18](https://twitter.com/unormal/status/812158749838340096), [19](https://twitter.com/unormal/status/814569437181476864), [20](https://twitter.com/unormal/status/814570383189876738), [21](https://twitter.com/unormal/status/819725864623603712), [22](https://twitter.com/unormal/status/984719207156862976).\n* [Danny Wynne](https://github.com/dannywynne) implemented a [3d tiled model](https://twitter.com/dwtw/status/810166761270243328).\n* Arvi Teikari programmed a [texture synthesis algorithm with the entropy heuristic](http://www.hempuli.com/blogblog/archives/1598) in Lua. Headchant [ported](https://github.com/headchant/iga) it to work with LÃ–VE.\n* Isaac Karth made a [Python port](https://github.com/ikarth/wfc_python) of the overlapping model.\n* Oskar StÃ¥lberg made an [interactive version](http://oskarstalberg.com/game/wave/wave.html) of the tiled model that runs in the browser.\n* [Matt Rix](https://github.com/MattRix) implemented a 3d tiled model ([1](https://twitter.com/MattRix/status/869403586664570880), [2](https://twitter.com/MattRix/status/870999185167962113), [3](https://twitter.com/MattRix/status/871054734018453505), [4](https://twitter.com/MattRix/status/871056805761359872)) and made a 3-dimensional tiled model where one of the dimensions is time ([1](https://twitter.com/MattRix/status/872674537799913472), [2](https://twitter.com/MattRix/status/872648369625325568), [3](https://twitter.com/MattRix/status/872645716660891648), [4](https://twitter.com/MattRix/status/872641331956518914), [5](https://twitter.com/MattRix/status/979020989181890560)).\n* [Nick Nenov](https://github.com/NNNenov) made a [visual guide](https://www.dropbox.com/s/zeiat1w8zre9ro8/Knots%20breakdown.png?dl=0) to the tile symmetry system.\n* [Isaac Karth](https://github.com/ikarth) and [Adam M. Smith](https://github.com/rndmcnlly) wrote a [paper](https://ieeexplore.ieee.org/document/9421370) ([open access link](https://escholarship.org/uc/item/3rm1w0mn)) in which they examine the role of backtracking and different possible heuristics in WFC, experiment with global constraints and combine WFC with VQ-VAE. Earlier in 2017, the authors wrote a [workshop paper](https://adamsmith.as/papers/wfc_is_constraint_solving_in_the_wild.pdf) where they formulate WFC as an ASP problem, use general constraint solver [clingo](https://github.com/potassco/clingo) to generate bitmaps, trace WFC''s history and give a detailed explanation of the algorithm.\n* Sylvain Lefebvre made a [C++ implementation](https://github.com/sylefeb/VoxModSynth) of 3d model synthesis, described the thought process of designing a sample and provided an example where adjacency constraints ensure that the output is connected (walkable).\n* I generalized 3d WFC to work with cube symmetry group and made a tileset that generates [Escheresque scenes](https://twitter.com/ExUtumno/status/895684431477747715).\n* There are many ways to visualize partially observed wave states. In the code, color values of possible options are averaged to produce the resulting color. Oskar StÃ¥lberg [shows](https://twitter.com/OskSta/status/863019585162932224) partially observed states as semi-transparent boxes, where the box is bigger for a state with more options. In the voxel setting I [visualize](https://twitter.com/ExUtumno/status/900395635412787202) wave states with per-voxel voting.\n* Remy Devaux implemented the tiled model in PICO-8 and wrote an [article](https://trasevol.dog/2017/09/01/di19/) about generation of coherent data with an explanation of WFC.\n* For the upcoming game [Bad North](https://www.badnorth.com/) Oskar StÃ¥lberg [uses](https://twitter.com/OskSta/status/917405214638006273) a heuristic that tries to select such tiles\nthat the resulting observed zone is navigable at each step.\n* William Manning [implemented](https://github.com/heyx3/easywfc) the overlapping model in C# with the primary goal of making code readable, and provided it with WPF GUI.\n* [Joseph Parker](https://gist.github.com/selfsame) wrote a WFC [tutorial](http://www.procjam.com/tutorials/wfc/) for Procjam 2017.\n* [Aman Tiwari](https://github.com/aman-tiwari) formulated the connectivity constraint as an [ASP problem](https://gist.github.com/aman-tiwari/8a7b874cb1fd1270adc203b2af293f4c) for clingo.\n* Matvey Khokhlov programmed a [3d overlapping model](https://github.com/MatveyK/Kazimir).\n* [Sylvain Lefebvre](https://github.com/sylefeb), [Li-Yi Wei](https://github.com/1iyiwei) and [Connelly Barnes](https://github.com/connellybarnes) are [investigating](https://hal.archives-ouvertes.fr/hal-01706539/) the possibility of hiding information inside textures. They made a [tool](https://members.loria.fr/Sylvain.Lefebvre/infotexsyn/) that can encode text messages as WFC tilings and decode them back. This technique allows to use WFC tilings as QR codes.\n* [Mathieu Fehr](https://github.com/math-fehr) and [Nathanael Courant](https://github.com/Ekdohibs) significantly [improved](https://github.com/math-fehr/fast-wfc) the running time of WFC, by an order of magnitude for the overlapping model. I [integrated](https://github.com/mxgmn/WaveFunctionCollapse/commit/fad1066b5000f7e9fbda0ef81bbea56799686670) their improvements into the code.\n* Vasu Mahesh [ported](https://github.com/vasumahesh1/WFC_WebGL) 3d tiled model to TypeScript, made a new tileset and [visualised](https://vasumahesh1.github.io/WFC_WebGL) the generation process in WebGL.\n* [Hwanhee Kim](https://github.com/greentec) experimented with 3d WFC and created/adapted many voxel tilesets: [1](https://twitter.com/greentecq/status/1025348928634408960), [2](https://twitter.com/greentecq/status/1004068394553913344), [3](https://twitter.com/greentecq/status/1005835830802305024), [4](https://twitter.com/greentecq/status/1022851327041265664), [5](https://twitter.com/greentecq/status/1011351814216736769), [6](https://twitter.com/greentecq/status/1008210550944387077), [7](https://twitter.com/greentecq/status/1006390606875070464), [8](https://twitter.com/greentecq/status/1015182718810841088).\n* Oskar StÃ¥lberg gave a [talk](https://www.youtube.com/watch?v=0bcZb-SsnrA) about level generation in Bad North at the Everything Procedural Conference 2018.\n* I [wrote](https://twitter.com/ExUtumno/status/1024314661951467521) about how to generate (approximately) unbiased paths between 2 points with WFC and other algorithms. I [implemented](https://github.com/mxgmn/MarkovJunior/blob/main/models/TilePath.xml) this method in MarkovJunior.\n* [Isaac Karth](https://github.com/ikarth) and [Adam M. Smith](https://github.com/rndmcnlly) published a [preprint](https://arxiv.org/abs/1809.04432) where they describe a system based on WFC that learns from both positive and negative examples, and discuss it in a general context of dialogs with example-driven generators.\n* Brendan Anthony [uses](https://steamcommunity.com/games/314230/announcements/detail/3369147113795750369) WFC to generate wall decorations in the game [Rodina](https://store.steampowered.com/app/314230/Rodina/).\n* Tim Kong implemented the [overlapping model in Haxe](https://github.com/Mitim-84/WFC-Gen).\n* In order to generate connected structures, Boris the Brave applied the [chiseling method](https://www.boristhebrave.com/2018/04/28/random-paths-via-chiseling) to WFC. He published a [library](https://boristhebrave.github.io/DeBroglie) that supports hex grids, additional constraints and backtracking.\n* [Marian Kleineberg](https://github.com/marian42) [created](https://twitter.com/marian42_/status/1061785383057440768) an [infinite city generator](https://marian42.itch.io/wfc) based on the tiled model for Procjam 2018. He wrote an [article](https://marian42.de/article/wfc) describing his approaches to setting adjacencies, backtracking and the online variation of WFC.\n* Sol Bekic [programmed](https://github.com/s-ol/gpWFC) the tiled model that runs on GPU using PyOpenCL. Instead of keeping a queue of nodes to propagate from, it propagates from every node on the grid in parallel.\n* Wouter van Oortmerssen [implemented](https://github.com/aardappel/lobster/commit/703f67472bfd80c26bb626e1d5c22ec91047da98) the tiled model in a single C++ function, with a structure similar to a priority queue for faster observation.\n* Robert Hoenig [implemented](https://github.com/roberthoenig/WaveFunctionCollapse.jl) the overlapping model in Julia, with an option to propagate constraints only locally.\n* [Edwin Jakobs](https://github.com/edwinRNDR) applied WFC to [style transfer](https://twitter.com/voorbeeld/status/1073874337248239616) and [dithering](https://twitter.com/voorbeeld/status/1073875725499985926).\n* Breanna Baltaxe-Admony [applied](https://github.com/bbaltaxe/wfc-piano-roll) WFC to music generation.\n* Shawn Ridgeway made a [Go port](https://github.com/shawnridgeway/wfc).\n* For the Global Game Jam 2019, [Andy Wallace](https://github.com/andymasteroffish) made a [game](http://andymakesgames.tumblr.com/post/182363131350/global-game-jam-2019-maureens-chaotic-dungeon) in which the player can interact with WFC-based level generator by resetting portions of the level with various weapons.\n* Stephen Sherratt wrote a [detailed explanation](https://gridbugs.org/wave-function-collapse/) of the overlapping model and made a [Rust library](https://github.com/stevebob/wfc). For the 7DRL Challenge 2019 he made a roguelike [Get Well Soon](https://gridbugs.org/get-well-soon/) that [uses](https://gridbugs.org/7drl2019-day1/) WFC to generate levels.\n* Florian Drux created a [generalization](https://github.com/lamelizard/GraphWaveFunctionCollapse/blob/master/thesis.pdf) that works on graphs with arbitrary local structure and [implemented](https://github.com/lamelizard/GraphWaveFunctionCollapse) it in Python.\n* Bob Burrough [discovered](https://twitter.com/ExUtumno/status/1119996185199116289) a percolation-like phase transition in one of the tilesets that manifests in spiking contradiction rate.\n* Oskar StÃ¥lberg combined WFC with marching cubes on irregular grids and made a town building toy [Townscaper](https://store.steampowered.com/app/1291340/Townscaper/) based on it: [1](https://twitter.com/OskSta/status/1164926304640229376), [2](https://twitter.com/OskSta/status/1168168400155267072), [3](https://twitter.com/OskSta/status/1181464374839521280), [4](https://twitter.com/OskSta/status/1189109278361165825), [5](https://twitter.com/OskSta/status/1189902695303458816), [6](https://www.youtube.com/watch?v=1hqt8JkYRdI). Oskar gave a number of talks and interviews about the mixed initiative town generation in Townscaper: [EPC2021](https://www.youtube.com/watch?v=NOJYZYqY6_M), [SGC21](https://www.youtube.com/watch?v=Uxeo9c-PX-w), [Konsoll 2021](https://www.youtube.com/watch?v=5xrRTOikBBg), [AI and Games](https://www.youtube.com/watch?v=_1fvJ5sHh6A).\n* In his Rust roguelike tutorial, [Herbert Wolverson](https://github.com/thebracket) wrote a [chapter](http://bfnightly.bracketproductions.com/rustbook/chapter_33.html) about implementing the WFC algorithm from scratch.\n* At the [Game Developers Conference 2019](https://www.youtube.com/watch?v=AdCgi9E90jw) and the [Roguelike Celebration 2019](https://www.youtube.com/watch?v=fnFj3dOKcIQ), [Brian Bucklew](https://github.com/unormal) gave talks about WFC and how Freehold Games uses it to generate levels in [Caves of Qud](https://store.steampowered.com/app/333640/Caves_of_Qud/). The talks discuss problems with overfitting and homogeny, level connectedness and combining WFC with constructive procgen methods.\n* [Boris the Brave](https://github.com/boristhebrave) published a [commercial Unity asset](https://assetstore.unity.com/packages/tools/modeling/tessera-procedural-tile-based-generator-155425) based on the tiled model.\n* Steven Casey ported WFC to [Java](https://github.com/sjcasey21/wavefunctioncollapse) and [Clojure](https://github.com/sjcasey21/wavefunctioncollapse-clj).\n* NuÃ±o de la Serna implemented the 3d tiled model in an [openFrameworks addon](https://github.com/action-script/ofxWFC3D) that supports tiles with no symmetries.\n* [Paul Ambrosiussen](https://github.com/Ambrosiussen) [integrated](https://github.com/sideeffects/SideFXLabs) the overlapping model into Houdini and gave a [talk](https://vimeo.com/400993662) about the algorithm and his implementation at Houdini HIVE 2020.\n* Keijiro Takahashi [implemented](https://github.com/keijiro/WfcMaze) a 3d tiled model and generated Escheresque scenes with it: [1](https://twitter.com/_kzr/status/1248993799960838144), [2](https://twitter.com/_kzr/status/1248990065327345664), [3](https://twitter.com/_kzr/status/1248884103274827777), [4](https://twitter.com/_kzr/status/1248268624495689728), [5](https://twitter.com/_kzr/status/1249348597549682689).\n* Simon Verstraete published a [tutorial](https://www.sidefx.com/tutorials/wfc-dungeon-generator/) about generating game levels for Unreal Engine 4 using the Houdini WFC tool: [0](https://www.youtube.com/watch?v=-5_FIqTDuzc), [1](https://www.youtube.com/watch?v=c06bSBYsFT8), [2](https://www.youtube.com/watch?v=u4NCs1F6zf8), [3](https://www.youtube.com/watch?v=YDpVUl213yo), [4](https://www.youtube.com/watch?v=ldcsvGuoW24).\n* [Ã‰lie Michel](https://github.com/eliemichel) posted a [twitter thread](https://twitter.com/exppad/status/1267045322116734977) that explains the relationship between the overlapping and the tiled models.\n* [Lionel Radisson](https://github.com/MAKIO135) published an interactive [Observable notebook](https://observablehq.com/@makio135/super-mario-wfc) that generates Mario and Zelda-like levels with the overlapping model: [1](https://twitter.com/MAKIO135/status/1271187284424040449), [2](https://twitter.com/MAKIO135/status/1268308728782045184), [3](https://twitter.com/MAKIO135/status/1271015222321561600), [4](https://twitter.com/MAKIO135/status/1271113760472694784).\n* Åukasz Jakubowski, Maciej Kaszlewicz, PaweÅ‚ Kroll and Stefan Radziuk [implemented](https://github.com/ic-pcg/waveFunctionCollapse) the tiled model in C.\n* [Ivan Donchevskii](https://github.com/yvvan) published a [commercial Unreal Engine plugin](https://www.unrealengine.com/marketplace/en-US/product/procedural-environment-generator-wfc) based on the tiled model.\n* [JÃ¡n PerneckÃ½](https://github.com/janper) and [JÃ¡n TÃ³th](https://github.com/yanchith) published a [Grasshopper plugin](https://github.com/subdgtl/Monoceros) that extends the tiled model.\n* Krystian Samp made a [single-file overlapping WFC library in C](https://github.com/krychu/wfc).\n* [Gerald Krystian](https://github.com/amarcolina) made an [interactive tool](https://amarcolina.github.io/WFC-Explorer/) that explores the tiled model where tile adjacencies are induced from edge labels.\n* DeepMind open-ended learning team [used](https://arxiv.org/abs/2107.12808) WFC to generate arenas for reinforcement learning agents.\n* Oskar StÃ¥lberg [made](https://twitter.com/OskSta/status/1447483550257799171) an island generator that combines triangle and quad tiles and uses a custom observation heuristic that doesn''t produce local minimums.\n* [Boris the Brave](https://github.com/boristhebrave) [applied](https://www.boristhebrave.com/2021/11/08/infinite-modifying-in-blocks/) [Paul Merrell''s](https://github.com/merrell42) modifying in blocks technique to the lazy generation of unbounded tile configurations. Marian Kleineberg has [implemented](https://marian42.de/article/infinite-wfc) this method into his [infinite city generator](https://github.com/marian42/wavefunctioncollapse).\n* Vladimir PleskonjiÄ‡ created a [single-header WFC library in C](https://github.com/vplesko/libwfc), accompanied by a CLI tool and a basic GUI tool.\n* Rick Dalley [ported](https://github.com/rick-dalley/wfc) WFC to Dart.\n* Elwann Guillemot [implemented](https://github.com/Elwqnn/wfc) the overlapping model in Rust and made a GUI.\n\n## Credits\nCircles tileset is taken from [Mario Klingemann](https://twitter.com/quasimondo/status/778196128957403136). FloorPlan tileset is taken from [Lingdong Huang](https://github.com/LingDong-/ndwfc). Summer tiles were drawn by Hermann Hillmann. Cat overlapping sample is taken from the Nyan Cat video, Water + Forest + Mountains samples are taken from Ultima IV, 3Bricks sample is taken from Dungeon Crawl Stone Soup, Qud sample was made by Brian Bucklew, MagicOffice + Spirals samples - by rid5x, ColoredCity + Link + Link 2 + Mazelike + RedDot + SmileCity samples - by Arvi Teikari, Wall sample - by Arcaniax, NotKnot + Sand + Wrinkles samples - by Krystian Samp, Circle sample - by Noah Buddy. The rest of the examples and tilesets were made by me. Idea of generating integrated circuits was suggested to me by [Moonasaur](https://twitter.com/Moonasaur/status/759890746350731264) and their style was taken from Zachtronics'' [Ruckingenur II](http://www.zachtronics.com/ruckingenur-ii/). Voxel models were rendered in [MagicaVoxel](http://ephtracy.github.io/).\n<p align="center"><img alt="second collage" src="images/wfc-2.png"></p>\n<p align="center"><img alt="voxel perspective" src="images/castle-3d.png"></p>\n', '{"language":"C#","stars":24482,"forks":1305,"watchers":24482,"open_issues":9,"topics":["algorithm","csharp","gamedev","machine-learning","procedural-generation","wfc"],"default_branch":"master","size_kb":31099,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:math-fehr:fast-wfc","source_url":"https://github.com/math-fehr/fast-wfc"},{"type":"has_code","target_id":"github:ikarth:wfc_2019f","source_url":"https://github.com/ikarth/wfc_2019f"},{"type":"has_code","target_id":"github:j-roskopf:WFC","source_url":"https://github.com/j-roskopf/WFC"},{"type":"has_code","target_id":"github:Elwqnn:wfc","source_url":"https://github.com/Elwqnn/wfc"},{"type":"has_code","target_id":"github:roberthoenig:WaveFunctionCollapse.jl","source_url":"https://github.com/roberthoenig/WaveFunctionCollapse.jl"},{"type":"has_code","target_id":"github:shawnridgeway:wfc","source_url":"https://github.com/shawnridgeway/wfc"},{"type":"has_code","target_id":"github:Mitim-84:WFC-Gen","source_url":"https://github.com/Mitim-84/WFC-Gen"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse","source_url":"https://github.com/sjcasey21/wavefunctioncollapse"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse-clj","source_url":"https://github.com/sjcasey21/wavefunctioncollapse-clj"},{"type":"has_code","target_id":"github:PascalCorpsman:mini_projects","source_url":"https://github.com/PascalCorpsman/mini_projects"},{"type":"has_code","target_id":"github:rick-dalley:wfc","source_url":"https://github.com/rick-dalley/wfc"},{"type":"has_code","target_id":"github:D-T-666:wave-function-collapse-p5","source_url":"https://github.com/D-T-666/wave-function-collapse-p5"},{"type":"has_code","target_id":"github:kchapelier:wavefunctioncollapse","source_url":"https://github.com/kchapelier/wavefunctioncollapse"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse#how-to-build"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:mewo2:oisin","source_url":"https://github.com/mewo2/oisin"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse#notable-ports-forks-and-spinoffs"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:mxgmn:ConvChain","source_url":"https://github.com/mxgmn/ConvChain"},{"type":"has_code","target_id":"github:mxgmn:TextureSynthesis","source_url":"https://github.com/mxgmn/TextureSynthesis"},{"type":"has_code","target_id":"github:merrell42:model-synthesis","source_url":"https://github.com/merrell42/model-synthesis"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:emilk:wfc","source_url":"https://github.com/emilk/wfc"},{"type":"has_code","target_id":"github:j-roskopf:WFC","source_url":"https://github.com/j-roskopf/WFC"},{"type":"has_code","target_id":"github:edwinRNDR:wfc","source_url":"https://github.com/edwinRNDR/wfc"},{"type":"has_code","target_id":"github:mewo2:oisin","source_url":"https://github.com/mewo2/oisin"},{"type":"has_code","target_id":"github:sdleffler:collapse","source_url":"https://github.com/sdleffler/collapse"},{"type":"has_code","target_id":"github:headchant:iga","source_url":"https://github.com/headchant/iga"},{"type":"has_code","target_id":"github:ikarth:wfc_python","source_url":"https://github.com/ikarth/wfc_python"},{"type":"has_code","target_id":"github:potassco:clingo","source_url":"https://github.com/potassco/clingo"},{"type":"has_code","target_id":"github:sylefeb:VoxModSynth","source_url":"https://github.com/sylefeb/VoxModSynth"},{"type":"has_code","target_id":"github:heyx3:easywfc","source_url":"https://github.com/heyx3/easywfc"},{"type":"has_code","target_id":"github:MatveyK:Kazimir","source_url":"https://github.com/MatveyK/Kazimir"},{"type":"has_code","target_id":"github:math-fehr:fast-wfc","source_url":"https://github.com/math-fehr/fast-wfc"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:vasumahesh1:WFC_WebGL","source_url":"https://github.com/vasumahesh1/WFC_WebGL"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:Mitim-84:WFC-Gen","source_url":"https://github.com/Mitim-84/WFC-Gen"},{"type":"has_code","target_id":"github:s-ol:gpWFC","source_url":"https://github.com/s-ol/gpWFC"},{"type":"has_code","target_id":"github:aardappel:lobster","source_url":"https://github.com/aardappel/lobster"},{"type":"has_code","target_id":"github:roberthoenig:WaveFunctionCollapse.jl","source_url":"https://github.com/roberthoenig/WaveFunctionCollapse.jl"},{"type":"has_code","target_id":"github:bbaltaxe:wfc-piano-roll","source_url":"https://github.com/bbaltaxe/wfc-piano-roll"},{"type":"has_code","target_id":"github:shawnridgeway:wfc","source_url":"https://github.com/shawnridgeway/wfc"},{"type":"has_code","target_id":"github:stevebob:wfc","source_url":"https://github.com/stevebob/wfc"},{"type":"has_code","target_id":"github:lamelizard:GraphWaveFunctionCollapse","source_url":"https://github.com/lamelizard/GraphWaveFunctionCollapse"},{"type":"has_code","target_id":"github:lamelizard:GraphWaveFunctionCollapse","source_url":"https://github.com/lamelizard/GraphWaveFunctionCollapse"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse","source_url":"https://github.com/sjcasey21/wavefunctioncollapse"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse-clj","source_url":"https://github.com/sjcasey21/wavefunctioncollapse-clj"},{"type":"has_code","target_id":"github:action-script:ofxWFC3D","source_url":"https://github.com/action-script/ofxWFC3D"},{"type":"has_code","target_id":"github:sideeffects:SideFXLabs","source_url":"https://github.com/sideeffects/SideFXLabs"},{"type":"has_code","target_id":"github:keijiro:WfcMaze","source_url":"https://github.com/keijiro/WfcMaze"},{"type":"has_code","target_id":"github:ic-pcg:waveFunctionCollapse","source_url":"https://github.com/ic-pcg/waveFunctionCollapse"},{"type":"has_code","target_id":"github:subdgtl:Monoceros","source_url":"https://github.com/subdgtl/Monoceros"},{"type":"has_code","target_id":"github:krychu:wfc","source_url":"https://github.com/krychu/wfc"},{"type":"has_code","target_id":"github:marian42:wavefunctioncollapse","source_url":"https://github.com/marian42/wavefunctioncollapse"},{"type":"has_code","target_id":"github:vplesko:libwfc","source_url":"https://github.com/vplesko/libwfc"},{"type":"has_code","target_id":"github:rick-dalley:wfc","source_url":"https://github.com/rick-dalley/wfc"},{"type":"has_code","target_id":"github:Elwqnn:wfc","source_url":"https://github.com/Elwqnn/wfc"},{"type":"has_code","target_id":"github:LingDong-:ndwfc","source_url":"https://github.com/LingDong-/ndwfc"}]', NULL, 'NOASSERTION', 'approved', 80, '397164620b4dc068e141f924d31c3ad3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mxgmn-WaveFunctionCollapse from https://github.com/mxgmn.png
Image converted to WebP: data/images/github-mxgmn-WaveFunctionCollapse.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-fastai-fastbook', 'github--fastai--fastbook', 'fastbook', 'fastai', 'English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese These notebooks cover an introduction to deep learning, fastai, and PyTorch. fastai is a layered API for deep learning; for more information, see the fastai paper. Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards. A selection of chapters is available to read online here. The notebooks in this repo are used for a MOOC and form the basis of this book, whi...', '["book","data-science","deep-learning","fastai","machine-learning","notebooks","python","jupyter notebook"]', 'other', 24128, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/fastai/fastbook","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[English](./README.md) / [Spanish](./README_es.md) / [Korean](./README_ko.md) / [Chinese](./README_zh.md) / [Bengali](./README_bn.md) / [Indonesian](./README_id.md) / [Italian](./README_it.md) / [Portuguese](./README_pt.md) / [Vietnamese](./README_vn.md) / [Japanese](./README_ja.md)\n\n# The fastai book\n\nThese notebooks cover an introduction to deep learning, [fastai](https://docs.fast.ai/), and [PyTorch](https://pytorch.org/). fastai is a layered API for deep learning; for more information, see [the fastai paper](https://www.mdpi.com/2078-2489/11/2/108). Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards. A selection of chapters is available to [read online here](https://fastai.github.io/fastbook2e/).\n\nThe notebooks in this repo are used for [a MOOC](https://course.fast.ai) and form the basis of [this book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527), which is currently available for purchase. It does not have the same GPL restrictions that are on this repository.\n\nThe code in the notebooks and python `.py` files is covered by the GPL v3 license; see the LICENSE file for details. The remainder (including all markdown cells in the notebooks and other prose) is not licensed for any redistribution or change of format or medium, other than making copies of the notebooks or forking this repo for your own private use. No commercial or broadcast use is allowed. We are making these materials freely available to help you learn deep learning, so please respect our copyright and these restrictions.\n\nIf you see someone hosting a copy of these materials somewhere else, please let them know that their actions are not allowed and may lead to legal action. Moreover, they would be hurting the community because we''re not likely to release additional materials in this way if people ignore our copyright.\n\n## Colab\n\nInstead of cloning this repo and opening it on your machine, you can read and work with the notebooks using [Google Colab](https://research.google.com/colaboratory/). This is the recommended approach for folks who are just getting started -- there''s no need to set up a Python development environment on your own machine, since you can just work directly in your web-browser.\n\nYou can open any chapter of the book in Colab by clicking on one of these links: [Introduction to Jupyter](https://colab.research.google.com/github/fastai/fastbook/blob/master/app_jupyter.ipynb) | [Chapter 1, Intro](https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb) | [Chapter 2, Production](https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb) | [Chapter 3, Ethics](https://colab.research.google.com/github/fastai/fastbook/blob/master/03_ethics.ipynb) | [Chapter 4, MNIST Basics](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb) | [Chapter 5, Pet Breeds](https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb) | [Chapter 6, Multi-Category](https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb) | [Chapter 7, Sizing and TTA](https://colab.research.google.com/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb) | [Chapter 8, Collab](https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb) | [Chapter 9, Tabular](https://colab.research.google.com/github/fastai/fastbook/blob/master/09_tabular.ipynb) | [Chapter 10, NLP](https://colab.research.google.com/github/fastai/fastbook/blob/master/10_nlp.ipynb) | [Chapter 11, Mid-Level API](https://colab.research.google.com/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb) | [Chapter 12, NLP Deep-Dive](https://colab.research.google.com/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb) | [Chapter 13, Convolutions](https://colab.research.google.com/github/fastai/fastbook/blob/master/13_convolutions.ipynb) | [Chapter 14, Resnet](https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb) | [Chapter 15, Arch Details](https://colab.research.google.com/github/fastai/fastbook/blob/master/15_arch_details.ipynb) | [Chapter 16, Optimizers and Callbacks](https://colab.research.google.com/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb) | [Chapter 17, Foundations](https://colab.research.google.com/github/fastai/fastbook/blob/master/17_foundations.ipynb) | [Chapter 18, GradCAM](https://colab.research.google.com/github/fastai/fastbook/blob/master/18_CAM.ipynb) | [Chapter 19, Learner](https://colab.research.google.com/github/fastai/fastbook/blob/master/19_learner.ipynb) | [Chapter 20, conclusion](https://colab.research.google.com/github/fastai/fastbook/blob/master/20_conclusion.ipynb)\n\n\n## Contributions\n\nIf you make any pull requests to this repo, then you are assigning copyright of that work to Jeremy Howard and Sylvain Gugger. (Additionally, if you are making small edits to spelling or text, please specify the name of the file and a very brief description of what you''re fixing. It''s difficult for reviewers to know which corrections have already been made. Thank you.)\n\n## Citations\n\nIf you wish to cite the book, you may use the following:\n\n```\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O''Reilly Media, Incorporated}\n}\n```\n\n', '{"language":"Jupyter Notebook","stars":24128,"forks":9310,"watchers":24128,"open_issues":178,"topics":["book","data-science","deep-learning","fastai","machine-learning","notebooks","python"],"default_branch":"master","size_kb":86034,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'NOASSERTION', 'approved', 65, '754e31513688e2f1cfaae6bbf6de219f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-fastai-fastbook from https://github.com/fastai.png
Image converted to WebP: data/images/github-fastai-fastbook.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-srbhr-Resume-Matcher', 'github--srbhr--resume-matcher', 'Resume-Matcher', 'srbhr', '<div align="center"> ð™¹ðš˜ðš’ðš— ð™³ðš’ðšœðšŒðš˜ðš›ðš âœ¦ ðš†ðšŽðš‹ðšœðš’ðšðšŽ âœ¦ ð™·ðš˜ðš  ðšðš˜ ð™¸ðš—ðšœðšðšŠðš•ðš• âœ¦ ð™²ðš˜ðš—ðšðš›ðš’ðš‹ðšžðšðš˜ðš›ðšœ âœ¦ ð™³ðš˜ðš—ðšŠðšðšŽ âœ¦ ðšƒðš ðš’ðšðšðšŽðš›/ðš‡ âœ¦ ð™»ðš’ðš—ðš”ðšŽðšð™¸ðš— **Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands. Hoping to make this, **VS Code for making ...', '["applicant-tracking-system","ats","hacktoberfest","machine-learning","natural-language-processing","nextjs","python","resume","resume-builder","resume-parser","text-similarity","typescript","vector-search","word-embeddings","python"]', 'other', 24091, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/srbhr/Resume-Matcher","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n[![Resume Matcher](assets/page_2.png)](https://www.resumematcher.fyi)\n\n# Resume Matcher\n\n[ð™¹ðš˜ðš’ðš— ð™³ðš’ðšœðšŒðš˜ðš›ðš](https://dsc.gg/resume-matcher) âœ¦ [ðš†ðšŽðš‹ðšœðš’ðšðšŽ](https://resumematcher.fyi) âœ¦ [ð™·ðš˜ðš  ðšðš˜ ð™¸ðš—ðšœðšðšŠðš•ðš•](#how-to-install) âœ¦ [ð™²ðš˜ðš—ðšðš›ðš’ðš‹ðšžðšðš˜ðš›ðšœ](#contributors) âœ¦ [ð™³ðš˜ðš—ðšŠðšðšŽ](#support-the-development-by-donating) âœ¦ [ðšƒðš ðš’ðšðšðšŽðš›/ðš‡](https://twitter.com/ssrbhr) âœ¦ [ð™»ðš’ðš—ðš”ðšŽðšð™¸ðš—](https://www.linkedin.com/company/resume-matcher/)\n\n**Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.\n\nHoping to make this, **VS Code for making resumes**.\n\n</div>\n\n<br>\n\n<div align="center">\n\n![Stars](https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&style=for-the-badge&color=c20a71)\n![Apache 2.0](https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&style=for-the-badge&color=c20a71) ![Forks](https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&style=for-the-badge&color=c20a71) ![version](https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&logo=LinkedIn&style=for-the-badge&color=c20a71)\n\n[![Discord](https://img.shields.io/discord/1122069176962531400?labelColor=black&logo=discord&logoColor=c20a71&style=for-the-badge&color=c20a71)](https://dsc.gg/resume-matcher) [![Website](https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&style=for-the-badge&color=c20a71)](https://resumematcher.fyi) [![LinkedIn](https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&logo=LinkedIn&style=for-the-badge&color=c20a71)](https://www.linkedin.com/company/resume-matcher/)\n\n<a href="https://trendshift.io/repositories/565" target="_blank"><img src="https://trendshift.io/api/badge/repositories/565" alt="srbhr%2FResume-Matcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)\n\n</div>\n\n> \[!IMPORTANT]\n>\n> This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the `main` branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [Discord](https://dsc.gg/resume-matcher) server.\n\n## Getting started with Resume Matcher\n\nResume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.\n\nWe''re actively working on improving the platform, building towards a **VS Code for making resumes**, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.\n\n> Join our [Discord](https://dsc.gg/resume-matcher) community ðŸ‘‡\n[![Discord](assets/resume_matcher_discord.png)](https://dsc.gg/resume-matcher)\n\n> Follow us on [LinkedIn](https://www.linkedin.com/company/resume-matcher/) âœ¨\n[![LinkedIn](assets/resume_matcher_linkedin.png)](https://www.linkedin.com/company/resume-matcher/)\n\n> â­ Star Resume Matcher to support the development and get updates on GitHub.\n![Star Resume Matcher](assets/star_resume_matcher.png)\n\n## Key Features\n\n![resume_matcher_features](assets/resume_matcher_features.png)\n\n- **Works locally**: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.\n- **ATS Compatibility**: Get a detailed analysis of your resume''s compatibility with ATS systems.\n- **Instant Match Score**: Upload resume & job description for a quick match score and key improvement areas.\n- **Keyword Optimizer**: Align your resume with job keywords and identify critical content gaps.\n- **Guided Improvements**: Get clear suggestions to make your resume stand out.\n\n### Roadmap\n\nIf you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our [Discord](https://dsc.gg/resume-matcher) server.\n\n- Visual keyword highlighting.\n- AI Canvas, which can help to craft impactful, metric-driven resume content.\n- Multi-job description optimization.\n\n## How to Install\n\n![Installation](assets/how_to_install_resumematcher.png)\n\nFollow the instructions in the [SETUP.md](SETUP.md) file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.\n\nThe project is built using:\n\n- FastAPI for the backend.\n- Next.js for the frontend.\n- Ollama for local AI model serving.\n- Tailwind CSS for styling.\n- SQLite for the database.\n\n| Technology   | Info/Version                               |\n|--------------|---------------------------------------|\n| Python      | 3.12+                   |\n| Next.js      | 15+                   |\n| Ollama       |        0.6.7        |\n\n## Join Us and Contribute\n\n![how to contribute](assets/how_to_contribute.png)\n\nWe welcome contributions from everyone! Whether you''re a developer, designer, or just someone who wants to help out. All the contributors are listed in the [about page](https://resumematcher.fyi/about) on our website and on the GitHub Readme here.\n\nCheck out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our [Discord](https://dsc.gg/resume-matcher) server.\n\n## Contributors\n\n![Contributors](assets/contributors.png)\n\n<a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher" />\n</a>\n\n## Support the Development by Donating\n\n![donate](assets/supporting_resume_matcher.png)\n\nIf you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.\n\n| Platform  | Link                                   |\n|-----------|----------------------------------------|\n| GitHub    | [![GitHub Sponsors](https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&color=c20a71&labelColor=black&logo=github)](https://github.com/sponsors/srbhr) |\n| Buy Me a Coffee | [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&color=c20a72&logoColor=white)](https://www.buymeacoffee.com/srbhr) |\n\n<details>\n  <summary><kbd>Star History</kbd></summary>\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=srbhr/resume-matcher&theme=dark&type=Date">\n    <img width="100%" src="https://api.star-history.com/svg?repos=srbhr/resume-matcher&theme=dark&type=Date">\n  </picture>\n</details>\n\n## Resume Matcher is a part of [Vercel Open Source Program](https://vercel.com/oss)\n\n![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)\n', '{"language":"Python","stars":24091,"forks":4517,"watchers":24091,"open_issues":56,"topics":["applicant-tracking-system","ats","hacktoberfest","machine-learning","natural-language-processing","nextjs","python","resume","resume-builder","resume-parser","text-similarity","typescript","vector-search","word-embeddings"],"default_branch":"main","size_kb":113620,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:srbhr:Resume-Matcher","source_url":"https://github.com/srbhr/Resume-Matcher"},{"type":"has_code","target_id":"github:sponsors:srbhr","source_url":"https://github.com/sponsors/srbhr"}]', NULL, 'Apache-2.0', 'approved', 65, 'f5948eb2f1d07780dc7cdadd06c9ec29', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-srbhr-Resume-Matcher from https://github.com/srbhr.png
Image converted to WebP: data/images/github-srbhr-Resume-Matcher.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-trekhleb-homemade-machine-learning', 'github--trekhleb--homemade-machine-learning', 'homemade-machine-learning', 'trekhleb', '> ðŸ‡ºðŸ‡¦ UKRAINE IS BEING ATTACKED BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED. > - Help Ukraine via: > - Serhiy Prytula Charity Foundation > - Come Back Alive Charity Foundation > - National Bank of Ukraine > - More info on war.ukraine.ua and MFA of Ukraine <hr/> > _Read this in other languages:_ _EspaÃ±ol_ > _You might be interested in:_ > - _Homemade GPT â€¢ JS_ > - _Interactive Machine Learning Experiments_ _For Octave/MatLab version of this repository p...', '["algorithm","jupyter","jupyter-notebook","machine-learning","machine-learning-algorithms","machinelearning","python","jupyter notebook"]', 'other', 23787, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/trekhleb/homemade-machine-learning","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Homemade Machine Learning\n\n> ðŸ‡ºðŸ‡¦ UKRAINE [IS BEING ATTACKED](https://war.ukraine.ua/) BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED.\n> - Help Ukraine via:\n>   - [Serhiy Prytula Charity Foundation](https://prytulafoundation.org/en/)\n>   - [Come Back Alive Charity Foundation](https://savelife.in.ua/en/donate-en/)\n>   - [National Bank of Ukraine](https://bank.gov.ua/en/news/all/natsionalniy-bank-vidkriv-spetsrahunok-dlya-zboru-koshtiv-na-potrebi-armiyi)\n> - More info on [war.ukraine.ua](https://war.ukraine.ua/) and [MFA of Ukraine](https://twitter.com/MFA_Ukraine)\n\n<hr/>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/trekhleb/homemade-machine-learning/master?filepath=notebooks)\n\n> _Read this in other languages:_ [_EspaÃ±ol_](README.es-ES.md)\n\n\n> _You might be interested in:_\n> - _[Homemade GPT â€¢ JS](https://github.com/trekhleb/homemade-gpt-js)_\n> - _[Interactive Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments)_\n\n_For Octave/MatLab version of this repository please check [machine-learning-octave](https://github.com/trekhleb/machine-learning-octave) project._\n\n> This repository contains examples of popular machine learning algorithms implemented in **Python** with mathematics behind them being explained. Each algorithm has interactive **Jupyter Notebook** demo that allows you to play with training data, algorithms configurations and immediately see the results, charts and predictions **right in your browser**. In most cases the explanations are based on [this great machine learning course](https://www.coursera.org/learn/machine-learning) by Andrew Ng.\n\nThe purpose of this repository is _not_ to implement machine learning algorithms by using 3<sup>rd</sup> party library one-liners _but_ rather to practice implementing these algorithms from scratch and get better understanding of the mathematics behind each algorithm. That''s why all algorithms implementations are called "homemade" and not intended to be used for production.\n\n## Supervised Learning\n\nIn supervised learning we have a set of training data as an input and a set of labels or "correct answers" for each training set as an output. Then we''re training our model (machine learning algorithm parameters) to map the input to the output correctly (to do correct prediction). The ultimate purpose is to find such model parameters that will successfully continue correct _inputâ†’output_ mapping (predictions) even for new input examples.\n\n### Regression\n\nIn regression problems we do real value predictions. Basically we try to draw a line/plane/n-dimensional plane along the training examples.\n\n_Usage examples: stock price forecast, sales analysis, dependency of any number, etc._\n\n#### ðŸ¤– Linear Regression\n\n- ðŸ“— [Math | Linear Regression](homemade/linear_regression) - theory and links for further readings\n- âš™ï¸ [Code | Linear Regression](homemade/linear_regression/linear_regression.py) - implementation example\n- â–¶ï¸ [Demo | Univariate Linear Regression](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/univariate_linear_regression_demo.ipynb) - predict `country happiness` score by `economy GDP`\n- â–¶ï¸ [Demo | Multivariate Linear Regression](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/multivariate_linear_regression_demo.ipynb) - predict `country happiness` score by `economy GDP` and `freedom index`\n- â–¶ï¸ [Demo | Non-linear Regression](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/non_linear_regression_demo.ipynb) - use linear regression with _polynomial_ and _sinusoid_ features to predict non-linear dependencies\n\n### Classification\n\nIn classification problems we split input examples by certain characteristic.\n\n_Usage examples: spam-filters, language detection, finding similar documents, handwritten letters recognition, etc._\n\n#### ðŸ¤– Logistic Regression\n\n- ðŸ“— [Math | Logistic Regression](homemade/logistic_regression) - theory and links for further readings\n- âš™ï¸ [Code | Logistic Regression](homemade/logistic_regression/logistic_regression.py) - implementation example\n- â–¶ï¸ [Demo | Logistic Regression (Linear Boundary)](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_linear_boundary_demo.ipynb) - predict Iris flower `class` based on `petal_length` and `petal_width`\n- â–¶ï¸ [Demo | Logistic Regression (Non-Linear Boundary)](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_non_linear_boundary_demo.ipynb) - predict microchip `validity` based on `param_1` and `param_2`\n- â–¶ï¸ [Demo | Multivariate Logistic Regression | MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_demo.ipynb) - recognize handwritten digits from `28x28` pixel images\n- â–¶ï¸ [Demo | Multivariate Logistic Regression | Fashion MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_fashion_demo.ipynb) - recognize clothes types from `28x28` pixel images\n\n## Unsupervised Learning\n\nUnsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.\n\n### Clustering\n\nIn clustering problems we split the training examples by unknown characteristics. The algorithm itself decides what characteristic to use for splitting.\n\n_Usage examples: market segmentation, social networks analysis, organize computing clusters, astronomical data analysis, image compression, etc._\n\n#### ðŸ¤– K-means Algorithm\n\n- ðŸ“— [Math | K-means Algorithm](homemade/k_means) - theory and links for further readings\n- âš™ï¸ [Code | K-means Algorithm](homemade/k_means/k_means.py) - implementation example\n- â–¶ï¸ [Demo | K-means Algorithm](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/k_means/k_means_demo.ipynb) - split Iris flowers into clusters based on `petal_length` and `petal_width`\n\n### Anomaly Detection\n\nAnomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.\n\n_Usage examples: intrusion detection, fraud detection, system health monitoring, removing anomalous data from the dataset etc._\n\n#### ðŸ¤– Anomaly Detection using Gaussian Distribution\n\n- ðŸ“— [Math | Anomaly Detection using Gaussian Distribution](homemade/anomaly_detection) - theory and links for further readings\n- âš™ï¸ [Code | Anomaly Detection using Gaussian Distribution](homemade/anomaly_detection/gaussian_anomaly_detection.py) - implementation example\n- â–¶ï¸ [Demo | Anomaly Detection](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/anomaly_detection/anomaly_detection_gaussian_demo.ipynb) - find anomalies in server operational parameters like `latency` and `threshold`\n\n## Neural Network (NN)\n\nThe neural network itself isn''t an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.\n\n_Usage examples: as a substitute of all other algorithms in general, image recognition, voice recognition, image processing (applying specific style), language translation, etc._\n\n#### ðŸ¤– Multilayer Perceptron (MLP)\n\n- ðŸ“— [Math | Multilayer Perceptron](homemade/neural_network) - theory and links for further readings\n- âš™ï¸ [Code | Multilayer Perceptron](homemade/neural_network/multilayer_perceptron.py) - implementation example\n- â–¶ï¸ [Demo | Multilayer Perceptron | MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_demo.ipynb) - recognize handwritten digits from `28x28` pixel images\n- â–¶ï¸ [Demo | Multilayer Perceptron | Fashion MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_fashion_demo.ipynb) - recognize the type of clothes from `28x28` pixel images\n\n## Machine Learning Map\n\n![Machine Learning Map](images/machine-learning-map.png)\n\nThe source of the following machine learning topics map is [this wonderful blog post](https://vas3k.ru/blog/machine_learning/)\n\n## Prerequisites\n\n#### Installing Python\n\nMake sure that you have [Python installed](https://realpython.com/installing-python/) on your machine.\n\nYou might want to use [venv](https://docs.python.org/3/library/venv.html) standard Python library\nto create virtual environments and have Python, `pip` and all dependent packages to be installed and \nserved from the local project directory to avoid messing with system wide packages and their \nversions.\n\n#### Installing Dependencies\n\nInstall all dependencies that are required for the project by running:\n\n```bash\npip install -r requirements.txt\n```\n\n#### Launching Jupyter Locally\n\nAll demos in the project may be run directly in your browser without installing Jupyter locally. But if you want to launch [Jupyter Notebook](http://jupyter.org/) locally you may do it by running the following command from the root folder of the project:\n\n```bash\njupyter notebook\n```\nAfter this Jupyter Notebook will be accessible by `http://localhost:8888`.\n\n#### Launching Jupyter Remotely\n\nEach algorithm section contains demo links to [Jupyter NBViewer](http://nbviewer.jupyter.org/). This is fast online previewer for Jupyter notebooks where you may see demo code, charts and data right in your browser without installing anything locally. In case if you want to _change_ the code and _experiment_ with demo notebook you need to launch the notebook in [Binder](https://mybinder.org/). You may do it by simply clicking the _"Execute on Binder"_ link in top right corner of the NBViewer.\n\n![](./images/binder-button-place.png)\n\n## Datasets\n\nThe list of datasets that is being used for Jupyter Notebook demos may be found in [data folder](data).\n\n## Supporting the project\n\nYou may support this project via â¤ï¸ï¸ [GitHub](https://github.com/sponsors/trekhleb) or â¤ï¸ï¸ [Patreon](https://www.patreon.com/trekhleb).\n\n## Author\n\n- [@trekhleb](https://trekhleb.dev)\n', '{"language":"Jupyter Notebook","stars":23787,"forks":4113,"watchers":23787,"open_issues":27,"topics":["algorithm","jupyter","jupyter-notebook","machine-learning","machine-learning-algorithms","machinelearning","python"],"default_branch":"master","size_kb":14467,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:trekhleb:homemade-gpt-js","source_url":"https://github.com/trekhleb/homemade-gpt-js"},{"type":"has_code","target_id":"github:trekhleb:machine-learning-experiments","source_url":"https://github.com/trekhleb/machine-learning-experiments"},{"type":"has_code","target_id":"github:trekhleb:machine-learning-octave","source_url":"https://github.com/trekhleb/machine-learning-octave"},{"type":"has_code","target_id":"github:sponsors:trekhleb","source_url":"https://github.com/sponsors/trekhleb"}]', NULL, 'MIT', 'approved', 80, 'd50e2035d9a282bdc312875f4af44825', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-trekhleb-homemade-machine-learning from https://github.com/trekhleb.png
Image converted to WebP: data/images/github-trekhleb-homemade-machine-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deepset-ai-haystack', 'github--deepset-ai--haystack', 'haystack', 'deepset-ai', '<div align="center"> <a href="https://haystack.deepset.ai/"><img src="https://raw.githubusercontent.com/deepset-ai/haystack/main/images/banner.png" alt="Green logo of a stylized white ''H'' with the text ''Haystack, by deepset.'' Abstract green and yellow diagrams in the background."></a> | | | | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...', '["agent","agents","ai","gemini","generative-ai","gpt-4","information-retrieval","large-language-models","llm","machine-learning","nlp","orchestration","python","pytorch","question-answering","rag","retrieval-augmented-generation","semantic-search","summarization","transformers","mdx"]', 'other', 23557, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deepset-ai/haystack","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n  <a href="https://haystack.deepset.ai/"><img src="https://raw.githubusercontent.com/deepset-ai/haystack/main/images/banner.png" alt="Green logo of a stylized white ''H'' with the text ''Haystack, by deepset.''Â Abstract green and yellow diagrams in the background."></a>\n\n|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| CI/CD   | [![Tests](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml) [![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy) [![Coverage Status](https://coveralls.io/repos/github/deepset-ai/haystack/badge.svg?branch=main)](https://coveralls.io/github/deepset-ai/haystack?branch=main) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) |\n| Docs    | [![Website](https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai)](https://docs.haystack.deepset.ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Package | [![PyPI](https://img.shields.io/pypi/v/haystack-ai)](https://pypi.org/project/haystack-ai/) ![PyPI - Downloads](https://img.shields.io/pypi/dm/haystack-ai?color=blue&logo=pypi&logoColor=gold) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/haystack-ai?logo=python&logoColor=gold) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/haystack-ai.svg)](https://anaconda.org/conda-forge/haystack-ai) [![GitHub](https://img.shields.io/github/license/deepset-ai/haystack?color=blue)](LICENSE) [![License Compliance](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml) |\n| Meta    | [![Discord](https://img.shields.io/discord/993534733298450452?logo=discord)](https://discord.com/invite/xYvH6drSmA) [![Twitter Follow](https://img.shields.io/twitter/follow/haystack_ai)](https://twitter.com/haystack_ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n</div>\n\n[Haystack](https://haystack.deepset.ai/) is an end-to-end LLM framework that allows you to build applications powered by\nLLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG),\ndocument search, question answering or answer generation, Haystack can orchestrate state-of-the-art embedding models\nand LLMs into pipelines to build end-to-end NLP applications and solve your use case.\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Documentation](#documentation)\n- [Features](#features)\n- [Use Cases](#features)\n- [Hayhooks (REST API Deployment)](#-tip-1)\n- [Haystack Enterprise](#haystack-enterprise-best-practices-and-expert-support)\n- [deepset Studio](#-deepset-studio-your-development-environment-for-haystack)\n- [Telemetry](#telemetry)\n- [ðŸ–– Community](#-community)\n- [Contributing to Haystack](#contributing-to-haystack)\n- [Who Uses Haystack](#who-uses-haystack)\n\n\n## Installation\n\nThe simplest way to get Haystack is via pip:\n\n```sh\npip install haystack-ai\n```\n\nInstall from the `main` branch to try the newest features:\n```sh\npip install git+https://github.com/deepset-ai/haystack.git@main\n```\n\nHaystack supports multiple installation methods including Docker images. For a comprehensive guide please refer\nto the [documentation](https://docs.haystack.deepset.ai/docs/installation).\n\n## Documentation\n\nIf you''re new to the project, check out ["What is Haystack?"](https://haystack.deepset.ai/overview/intro) then go\nthrough the ["Get Started Guide"](https://haystack.deepset.ai/overview/quick-start) and build your first LLM application\nin a matter of minutes. Keep learning with the [tutorials](https://haystack.deepset.ai/tutorials). For more advanced\nuse cases, or just to get some inspiration, you can browse our Haystack recipes in the\n[Cookbook](https://haystack.deepset.ai/cookbook).\n\nAt any given point, hit the [documentation](https://docs.haystack.deepset.ai/docs/intro) to learn more about Haystack, what can it do for you and the technology behind.\n\n## Features\n\n- **Technology agnostic:** Allow users the flexibility to decide what vendor or technology they want and make it easy to switch out any component for another. Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models or models hosted on Azure, Bedrock and SageMaker.\n- **Explicit:** Make it transparent how different moving parts can â€œtalkâ€ to each other so it''s easier to fit your tech stack and use case.\n- **Flexible:** Haystack provides all tooling in one place: database access, file conversion, cleaning, splitting, training, eval, inference, and more. And whenever custom behavior is desirable, it''s easy to create custom components.\n- **Extensible:** Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\n\nSome examples of what you can do with Haystack:\n\n-   Build **retrieval augmented generation (RAG)** by making use of one of the available vector databases and customizing your LLM interaction, the sky is the limit ðŸš€\n-   Perform Question Answering **in natural language** to find granular answers in your documents.\n-   Perform **semantic search** and retrieve documents according to meaning.\n-   Build applications that can make complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.\n-   Scale to millions of docs using retrievers and production-scale components.\n-   Use **off-the-shelf models** or **fine-tune** them to your data.\n-   Use **user feedback** to evaluate, benchmark, and continuously improve your models.\n\n> [!TIP]\n>\n> Would you like to deploy and serve Haystack pipelines as REST APIs yourself? [Hayhooks](https://github.com/deepset-ai/hayhooks) provides a simple way to wrap your pipelines with custom logic and expose them via HTTP endpoints, including OpenAI-compatible chat completion endpoints and compatibility with fully-featured chat interfaces like [open-webui](https://openwebui.com/).\n\n## Haystack Enterprise: Best Practices and Expert Support\n\nGet expert support from the Haystack team, build faster with enterprise-grade templates, and scale securely with deployment guides for cloud and on-prem environments - all with **Haystack Enterprise**. Read more about it our [announcement post](https://haystack.deepset.ai/blog/announcing-haystack-enterprise).\n\nðŸ‘‰ [Get Haystack Enterprise](https://www.deepset.ai/products-and-services/haystack-enterprise?utm_source=github.com&utm_medium=referral&utm_campaign=haystack_enterprise)\n\n## deepset Studio: Your Development Environment for Haystack\n\nUse **deepset Studio** to visually create, deploy, and test your Haystack pipelines. Learn more about it in our [announcement post](https://haystack.deepset.ai/blog/announcing-studio).\n\n![studio](https://github.com/user-attachments/assets/e4f09746-20b5-433e-8261-eca224ac23b3)\n\nðŸ‘‰ [Sign up](https://landing.deepset.ai/deepset-studio-signup)!\n\n> [!TIP]\n><img src="https://github.com/deepset-ai/haystack/raw/main/images/deepset-platform-logo-alternative.jpeg"  width=20%>\n>\n> Are you looking for a managed solution that benefits from Haystack? [deepset AI Platform](https://www.deepset.ai/products-and-services/deepset-ai-platform?utm_campaign=developer-relations&utm_source=haystack&utm_medium=readme) is our fully managed, end-to-end platform to integrate LLMs with your data, which uses Haystack for the LLM pipelines architecture.\n\n## Telemetry\n\nHaystack collects **anonymous** usage statistics of pipeline components. We receive an event every time these components are initialized. This way, we know which components are most relevant to our community.\n\nRead more about telemetry in Haystack or how you can opt out in [Haystack docs](https://docs.haystack.deepset.ai/docs/telemetry).\n\n## ðŸ–– Community\n\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues). We regularly check these and you can expect a quick response. If you''d like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://discord.com/invite/VBpFzsgRVF). We also check [ð• (Twitter)](https://twitter.com/haystack_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n## Contributing to Haystack\n\nWe are very open to the community''s contributions - be it a quick fix of a typo, or a completely new feature! You don''t need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nThere are several ways you can contribute to Haystack:\n- Contribute to the main Haystack project\n- Contribute an integration on [haystack-core-integrations](https://github.com/deepset-ai/haystack-core-integrations)\n- Contribute to the documentation in [haystack/docs-website](https://github.com/deepset-ai/haystack/tree/main/docs-website)\n\n> [!TIP]\n>ðŸ‘‰ **[Check out the full list of issues that are open to contributions](https://github.com/orgs/deepset-ai/projects/14)**\n\n## Who Uses Haystack\n\nHere''s a list of projects and companies using Haystack. Are you also using Haystack? Open a PR or [tell us your story](https://forms.gle/Mm3G1aEST3GAH2rn8).\n\n- Tech & AI Innovators: [Apple](https://www.apple.com/), [Meta](https://www.meta.com/about), [Databricks](https://www.databricks.com/), [NVIDIA](https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/), [PostHog](https://github.com/PostHog/max-ai#readme)\n- Public Sector: [German Federal Ministry of Research, Technology, and Space (BMFTR)](https://www.deepset.ai/case-studies/german-federal-ministry-research-technology-space-bmftr), [PD, Baden-WÃ¼rttemberg State](https://www.pd-g.de/)\n- Enterprise & Telecom: [Alcatel-Lucent](https://www.al-enterprise.com/), [Intel](https://github.com/intel/open-domain-question-and-answer#readme), [NOS Portugal](https://www.nos.pt/en/welcome), [TELUS Agriculture & Consumer Goods](https://www.telus.com/agcg/en)\n- Aerospace & Hardware: [Airbus](https://www.deepset.ai/case-studies/airbus), [Infineon](https://www.infineon.com/), [LEGO](https://github.com/larsbaunwall/bricky#readme)\n- Media & Entertainment: [Netflix](https://netflix.com), [Comcast](https://arxiv.org/html/2405.00801v2), [Zeit Online](https://www.deepset.ai/case-studies/zeit-online), [Rakuten](https://www.rakuten.com/)\n- Legal & Publishing: [Manz](https://www.deepset.ai/case-studies/manz), [Oxford University Press](https://corp.oup.com/)\n- Startups & Research: [YPulse](https://www.deepset.ai/case-studies/ypulse), [BetterUp](https://www.betterup.com/), [Intel Labs](https://github.com/IntelLabs/fastRAG#readme)\n', '{"language":"MDX","stars":23557,"forks":2510,"watchers":23557,"open_issues":118,"topics":["agent","agents","ai","gemini","generative-ai","gpt-4","information-retrieval","large-language-models","llm","machine-learning","nlp","orchestration","python","pytorch","question-answering","rag","retrieval-augmented-generation","semantic-search","summarization","transformers"],"default_branch":"main","size_kb":55022,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:python:mypy","source_url":"https://github.com/python/mypy"},{"type":"has_code","target_id":"github:astral-sh:ruff","source_url":"https://github.com/astral-sh/ruff"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack.git@main","source_url":"https://github.com/deepset-ai/haystack.git@main"},{"type":"has_code","target_id":"github:deepset-ai:hayhooks","source_url":"https://github.com/deepset-ai/hayhooks"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack-core-integrations","source_url":"https://github.com/deepset-ai/haystack-core-integrations"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:orgs:deepset-ai","source_url":"https://github.com/orgs/deepset-ai"},{"type":"has_code","target_id":"github:PostHog:max-ai","source_url":"https://github.com/PostHog/max-ai#readme"},{"type":"has_code","target_id":"github:intel:open-domain-question-and-answer","source_url":"https://github.com/intel/open-domain-question-and-answer#readme"},{"type":"has_code","target_id":"github:larsbaunwall:bricky","source_url":"https://github.com/larsbaunwall/bricky#readme"},{"type":"has_code","target_id":"github:IntelLabs:fastRAG","source_url":"https://github.com/IntelLabs/fastRAG#readme"}]', NULL, 'Apache-2.0', 'approved', 80, 'f53722665093d18dc16da5a1ed3c1003', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-deepset-ai-haystack from https://github.com/deepset-ai.png
Image converted to WebP: data/images/github-deepset-ai-haystack.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-PaddlePaddle-Paddle', 'github--paddlepaddle--paddle', 'Paddle', 'PaddlePaddle', '<p align="center"> <img align="center" src="doc/imgs/logo.png", width=1600> <p> -------------------------------------------------------------------------------- English | ç®€ä½“ä¸­æ–‡ | æ—¥æœ¬èªž !X (formerly Twitter) URL Welcome to the PaddlePaddle GitHub. PaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning ...', '["deep-learning","distributed-training","efficiency","machine-learning","neural-network","paddlepaddle","python","scalability","c++"]', 'other', 23483, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/PaddlePaddle/Paddle","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n<img align="center" src="doc/imgs/logo.png", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\nEnglish | [ç®€ä½“ä¸­æ–‡](./README_cn.md) | [æ—¥æœ¬èªž](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/ä¸­æ–‡æ–‡æ¡£-æœ€æ–°-brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n![X (formerly Twitter) URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2FPaddlePaddle)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.\nPaddlePaddle originates from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 23.33 million developers, 760,000 companies and generating 1,100,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n## Installation\n\n### Latest PaddlePaddle Release: 3.2\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https://github.com/PaddlePaddle/Paddle/releases) to track the latest features of PaddlePaddle.\n\n### Install Latest Stable Release or Nightly Release\n\nFor detailed information about installation, please view [Quick Install](https://www.paddlepaddle.org.cn/install/quick)\n\n## **PaddlePaddle New Generation Framework 3.2**\n\n* **Unified Dynamic/Static Graphs and Automatic Parallelism**\n\n    By requiring only minimal tensor partitioning annotations based on a single-card configuration, PaddlePaddle automatically discovers the most efficient distributed parallel strategy. This significantly reduces the costs of industrial development and training, enabling developers to focus more intently on model and algorithm innovation.\n\n* **Integrated Training and Inference for Large Models**\n\n    The same framework supports both training and inference, achieving code reuse and seamless integration between these stages. This provides a unified development experience and maximum training efficiency for the entire large model workflow, offering the industry a superior development experience.\n\n* **High-Order Differentiation for Scientific Computing**\n\n    Provides capabilities such as high-order automatic differentiation, complex number operations, Fourier transforms, compilation optimization, and distributed training support. It facilitates scientific exploration in fields including mathematics, mechanics, materials science, meteorology, and biology, substantially improving the speed of solving differential equations.\n\n* **Neural Network Compiler**\n\n    Adopting an integrated framework design, it supports efficient training and flexible inference for diverse models, including generative and scientific computing models. It achieves an effective balance between computational flexibility and high performance, significantly lowering performance optimization costs.\n\n* **Heterogeneous Multi-Chip Adaptation**\n    Features a mature and complete unified adaptation solution for multiple hardware types. Through standardized interfaces, it abstracts the variations in development interfaces across different chip software stacks, realizing a pluggable architecture.\n\n## Documentation\n\nWe provide [English](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html) and\n[Chinese](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html) documentation.\n\n* [Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n* [Practice](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  So far you have already been familiar with Fluid. And the next step should be building a more efficient model or inventing your original Operator.\n\n* [API Reference](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   Our new API enables much shorter programs.\n\n* [How to Contribute](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   We appreciate your contributions!\n\n## Open Source Community\n\n* [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): bug reports, feature requests, install issues, usage issues, etc.\n* Many of our contribution events offer varying levels of mentorship from experienced community members, please check the events in the pinned issues, and consider attending.\n* Community Blog: <https://pfcc.blog/>\n* See more details about PaddlePaddle community at [community](https://github.com/PaddlePaddle/community).\n\n## Copyright and License\n\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n', '{"language":"C++","stars":23483,"forks":5902,"watchers":23483,"open_issues":1590,"topics":["deep-learning","distributed-training","efficiency","machine-learning","neural-network","paddlepaddle","python","scalability"],"default_branch":"develop","size_kb":529465,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:community","source_url":"https://github.com/PaddlePaddle/community"}]', NULL, 'Apache-2.0', 'approved', 65, '50f8dfa8a0a834f2cf87314efdf28731', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-PaddlePaddle-Paddle from https://github.com/PaddlePaddle.png
Image converted to WebP: data/images/github-PaddlePaddle-Paddle.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mlflow-mlflow', 'github--mlflow--mlflow', 'mlflow', 'mlflow', '<h1 align="center" style="border-bottom: none"> <a href="https://mlflow.org/"> <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" /> </a> </h1> <h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2> MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observabilit...', '["agentops","agents","ai","ai-governance","apache-spark","evaluation","langchain","llm-evaluation","llmops","machine-learning","ml","mlflow","mlops","model-management","observability","open-source","openai","prompt-engineering","python"]', 'other', 23202, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mlflow/mlflow","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ðŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ðŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ðŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ðŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ðŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ðŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ðŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ðŸŽ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ðŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ðŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ðŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ðŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ðŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ðŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Tracing (Observability) ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n### Evaluating LLMs, Prompts, and Agents ([Doc](https://mlflow.org/docs/latest/genai/eval-monitor/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport os\nimport openai\nimport mlflow\nfrom mlflow.genai.scorers import Correctness, Guidelines\n\nclient = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n# 1. Define a simple QA dataset\ndataset = [\n    {\n        "inputs": {"question": "Can MLflow manage prompts?"},\n        "expectations": {"expected_response": "Yes!"},\n    },\n    {\n        "inputs": {"question": "Can MLflow create a taco for my lunch?"},\n        "expectations": {\n            "expected_response": "No, unfortunately, MLflow is not a taco maker."\n        },\n    },\n]\n\n\n# 2. Define a prediction function to generate responses\ndef predict_fn(question: str) -> str:\n    response = client.chat.completions.create(\n        model="gpt-4o-mini", messages=[{"role": "user", "content": question}]\n    )\n    return response.choices[0].message.content\n\n\n# 3.Run the evaluation\nresults = mlflow.genai.evaluate(\n    data=dataset,\n    predict_fn=predict_fn,\n    scorers=[\n        # Built-in LLM judge\n        Correctness(),\n        # Custom criteria using LLM judge\n        Guidelines(name="is_english", guidelines="The answer must be in English"),\n    ],\n)\n```\n\nNavigate to the "Evaluations" tab in the MLflow UI to find the evaluation results.\n\n### Tracking Model Training ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow''s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow''s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow server\n```\n\n## ðŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ðŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ðŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Joel Robin P](https://github.com/joelrobin18)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '{"language":"Python","stars":23202,"forks":5044,"watchers":23202,"open_issues":2114,"topics":["agentops","agents","ai","ai-governance","apache-spark","evaluation","langchain","llm-evaluation","llmops","machine-learning","ml","mlflow","mlops","model-management","observability","open-source","openai","prompt-engineering"],"default_branch":"master","size_kb":1226357,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"}]', NULL, 'Apache-2.0', 'approved', 80, '21ddb2a844f8d94892ee86f846b2ef77', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mlflow-mlflow from https://github.com/mlflow.png
Image converted to WebP: data/images/github-mlflow-mlflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-sebastianruder-NLP-progress', 'github--sebastianruder--nlp-progress', 'NLP-progress', 'sebastianruder', '- Automatic speech recognition - CCG - Common sense - Constituency parsing - Coreference resolution - Data-to-Text Generation - Dependency parsing - Dialogue - Domain adaptation - Entity linking - Grammatical error correction - Information extraction - Intent Detection and Slot Filling - Keyphrase Extraction and Generation - Language modeling - Lexical normalization - Machine translation - Missing elements - Multi-task learning - Multi-modal - Named entity recognition - Natural language infer...', '["dialogue","machine-learning","machine-translation","named-entity-recognition","natural-language-processing","nlp-tasks","python"]', 'other', 22973, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/sebastianruder/NLP-progress","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Tracking Progress in Natural Language Processing\n\n## Table of contents\n\n### English\n\n- [Automatic speech recognition](english/automatic_speech_recognition.md)\n- [CCG](english/ccg.md)\n- [Common sense](english/common_sense.md)\n- [Constituency parsing](english/constituency_parsing.md)\n- [Coreference resolution](english/coreference_resolution.md)\n- [Data-to-Text Generation](english/data_to_text_generation.md)\n- [Dependency parsing](english/dependency_parsing.md)\n- [Dialogue](english/dialogue.md)\n- [Domain adaptation](english/domain_adaptation.md)\n- [Entity linking](english/entity_linking.md)\n- [Grammatical error correction](english/grammatical_error_correction.md)\n- [Information extraction](english/information_extraction.md)\n- [Intent Detection and Slot Filling](english/intent_detection_slot_filling.md) \n- [Keyphrase Extraction and Generation](english/keyphrase_extraction_generation.md)\n- [Language modeling](english/language_modeling.md)\n- [Lexical normalization](english/lexical_normalization.md)\n- [Machine translation](english/machine_translation.md)\n- [Missing elements](english/missing_elements.md)\n- [Multi-task learning](english/multi-task_learning.md)\n- [Multi-modal](english/multimodal.md)\n- [Named entity recognition](english/named_entity_recognition.md)\n- [Natural language inference](english/natural_language_inference.md)\n- [Part-of-speech tagging](english/part-of-speech_tagging.md)\n- [Paraphrase Generation](english/paraphrase-generation.md)\n- [Question answering](english/question_answering.md)\n- [Relation prediction](english/relation_prediction.md)\n- [Relationship extraction](english/relationship_extraction.md)\n- [Semantic textual similarity](english/semantic_textual_similarity.md)\n- [Semantic parsing](english/semantic_parsing.md)\n- [Semantic role labeling](english/semantic_role_labeling.md)\n- [Sentiment analysis](english/sentiment_analysis.md)\n- [Shallow syntax](english/shallow_syntax.md)\n- [Simplification](english/simplification.md)\n- [Stance detection](english/stance_detection.md)\n- [Summarization](english/summarization.md)\n- [Taxonomy learning](english/taxonomy_learning.md)\n- [Temporal processing](english/temporal_processing.md)\n- [Text classification](english/text_classification.md)\n- [Word sense disambiguation](english/word_sense_disambiguation.md)\n\n### Vietnamese\n\n- [Dependency parsing](vietnamese/vietnamese.md#dependency-parsing)\n- [Intent detection and Slot filling](vietnamese/vietnamese.md#intent-detection-and-slot-filling)\n- [Machine translation](vietnamese/vietnamese.md#machine-translation)\n- [Named entity recognition](vietnamese/vietnamese.md#named-entity-recognition)\n- [Part-of-speech tagging](vietnamese/vietnamese.md#part-of-speech-tagging)\n- [Semantic parsing](vietnamese/vietnamese.md#semantic-parsing)\n- [Word segmentation](vietnamese/vietnamese.md#word-segmentation)\n\n### Hindi\n\n- [Chunking](hindi/hindi.md#chunking)\n- [Part-of-speech tagging](hindi/hindi.md#part-of-speech-tagging)\n- [Machine Translation](hindi/hindi.md#machine-translation)\n\n### Chinese\n\n- [Entity linking](chinese/chinese.md#entity-linking)\n- [Chinese word segmentation](chinese/chinese_word_segmentation.md)\n- [Question answering](chinese/question_answering.md)\n\nFor more tasks, datasets and results in Chinese, check out the [Chinese NLP](https://chinesenlp.xyz/#/) website.\n\n### French\n\n- [Question answering](french/question_answering.md)\n- [Summarization](french/summarization.md)\n\n### Russian\n\n- [Question answering](russian/question_answering.md)\n- [Sentiment Analysis](russian/sentiment-analysis.md)\n- [Summarization](russian/summarization.md)\n\n### Spanish\n\n- [Named Entity Recognition](spanish/named_entity_recognition.md)\n- [Entity linking](spanish/entity_linking.md#entity-linking)\n- [Summarization](spanish/summarization.md)\n\n### Portuguese\n\n- [Question Answering](portuguese/question_answering.md)\n\n### Korean\n\n- [Question Answering](korean/question_answering.md)\n\n### Nepali\n\n- [Machine Translation](nepali/nepali.md#machine-translation)\n\n### Bengali\n- [Part-of-speech Tagging](bengali/part_of_speech_tagging.md)\n- [Emotion Detection](bengali/emotion_detection.md)\n- [Sentiment Analysis](bengali/sentiment_analysis.md)\n\n### Persian\n- [Named entity recognition](persian/named_entity_recognition.md)\n- [Natural language inference](persian/natural_language_inference.md)\n- [Summarization](persian/summarization.md)\n\n### Turkish\n\n- [Summarization](turkish/summarization.md)\n\n### German\n\n- [Question Answering](german/question_answering.md)\n- [Summarization](german/summarization.md)\n\n### Arabic\n- [Language modeling](arabic/language_modeling.md)\n\n\nThis document aims to track the progress in Natural Language Processing (NLP) and give an overview\nof the state-of-the-art (SOTA) across the most common NLP tasks and their corresponding datasets.\n\nIt aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech tagging\nas well as more recent ones such as reading comprehension and natural language inference. The main objective\nis to provide the reader with a quick overview of benchmark datasets and the state-of-the-art for their\ntask of interest, which serves as a stepping stone for further research. To this end, if there is a \nplace where results for a task are already published and regularly maintained, such as a public leaderboard,\nthe reader will be pointed there.\n\nIf you want to find this document again in the future, just go to [`nlpprogress.com`](https://nlpprogress.com/)\nor [`nlpsota.com`](http://nlpsota.com/) in your browser.\n\n### Contributing\n\n#### Guidelines\n\n**Results** &nbsp; Results reported in published papers are preferred; an exception may be made for influential preprints.\n\n**Datasets** &nbsp; Datasets should have been used for evaluation in at least one published paper besides \nthe one that introduced the dataset.\n\n**Code** &nbsp; We recommend to add a link to an implementation \nif available. You can add a `Code` column (see below) to the table if it does not exist.\nIn the `Code` column, indicate an official implementation with [Official](http://link_to_implementation).\nIf an unofficial implementation is available, use [Link](http://link_to_implementation) (see below).\nIf no implementation is available, you can leave the cell empty.\n\n#### Adding a new result\n\nIf you would like to add a new result, you can just click on the small edit button in the top-right\ncorner of the file for the respective task (see below).\n\n![Click on the edit button to add a file](img/edit_file.png)\n\nThis allows you to edit the file in Markdown. Simply add a row to the corresponding table in the\nsame format. Make sure that the table stays sorted (with the best result on top). \nAfter you''ve made your change, make sure that the table still looks ok by clicking on the\n"Preview changes" tab at the top of the page. If everything looks good, go to the bottom of the page,\nwhere you see the below form. \n\n![Fill out the file change information](img/propose_file_change.png)\n\nAdd a name for your proposed change, an optional description, indicate that you would like to\n"Create a new branch for this commit and start a pull request", and click on "Propose file change".\n\n#### Adding a new dataset or task\n\nFor adding a new dataset or task, you can also follow the steps above. Alternatively, you can fork the repository.\nIn both cases, follow the steps below:\n\n1. If your task is completely new, create a new file and link to it in the table of contents above.\n2. If not, add your task or dataset to the respective section of the corresponding file (in alphabetical order).\n3. Briefly describe the dataset/task and include relevant references. \n4. Describe the evaluation setting and evaluation metric.\n5. Show how an annotated example of the dataset/task looks like.\n6. Add a download link if available.\n7. Copy the below table and fill in at least two results (including the state-of-the-art)\n  for your dataset/task (change Score to the metric of your dataset). If your dataset/task\n  has multiple metrics, add them to the right of `Score`.\n1. Submit your change as a pull request.\n  \n| Model           | Score  |  Paper / Source | Code |\n| ------------- | :-----:| --- | --- |\n|  |  |  | |\n\n\n### Wish list\n\nThese are tasks and datasets that are still missing:\n\n- Bilingual dictionary induction\n- Discourse parsing\n- Keyphrase extraction\n- Knowledge base population (KBP)\n- More dialogue tasks\n- Semi-supervised learning\n- Frame-semantic parsing (FrameNet full-sentence analysis)\n\n### Exporting into a structured format\n\nYou can extract all the data into a structured, machine-readable JSON format with parsed tasks, descriptions and SOTA tables. \n\nThe instructions are in [structured/README.md](structured/README.md).\n\n### Instructions for building the site locally\n\nInstructions for building the website locally using Jekyll can be found [here](jekyll_instructions.md).\n\n\n', '{"language":"Python","stars":22973,"forks":3622,"watchers":22973,"open_issues":40,"topics":["dialogue","machine-learning","machine-translation","named-entity-recognition","natural-language-processing","nlp-tasks"],"default_branch":"master","size_kb":1399,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[]', NULL, 'MIT', 'approved', 65, '0f9ab7689787985f284bafc31ef1ac60', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-sebastianruder-NLP-progress from https://github.com/sebastianruder.png
Image converted to WebP: data/images/github-sebastianruder-NLP-progress.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-lukasmasuch-best-of-ml-python', 'github--lukasmasuch--best-of-ml-python', 'best-of-ml-python', 'lukasmasuch', '<!-- markdownlint-disable --> <h1 align="center"> Best-of Machine Learning with Python <br> </h1> <p align="center"> <strong>ðŸ†&nbsp; A ranked list of awesome machine learning Python libraries. Updated weekly.</strong> </p> <p align="center"> <a href="https://github.com/ml-tooling/best-of" title="Best-of-badge"><img src="http://bit.ly/3o3EHNN"></a> <a href="#Contents" title="Project Count"><img src="https://img.shields.io/badge/projects-920-blue.svg?color=5ac4bf"></a> <a href="#Contribution" ...', '["automl","chatgpt","data-analysis","data-science","data-visualization","data-visualizations","deep-learning","gpt","gpt-3","jax","keras","machine-learning","ml","nlp","python","pytorch","scikit-learn","tensorflow","transformer"]', 'other', 22916, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/lukasmasuch/best-of-ml-python","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!-- markdownlint-disable -->\n<h1 align="center">\n    Best-of Machine Learning with Python\n    <br>\n</h1>\n\n<p align="center">\n    <strong>ðŸ†&nbsp; A ranked list of awesome machine learning Python libraries. Updated weekly.</strong>\n</p>\n\n<p align="center">\n    <a href="https://github.com/ml-tooling/best-of" title="Best-of-badge"><img src="http://bit.ly/3o3EHNN"></a>\n    <a href="#Contents" title="Project Count"><img src="https://img.shields.io/badge/projects-920-blue.svg?color=5ac4bf"></a>\n    <a href="#Contribution" title="Contributions are welcome"><img src="https://img.shields.io/badge/contributions-welcome-green.svg"></a>\n    <a href="https://github.com/ml-tooling/best-of-ml-python/releases" title="Best-of Updates"><img src="https://img.shields.io/github/release-date/ml-tooling/best-of-ml-python?color=green&label=updated"></a>\n    <a href="https://mltooling.substack.com/subscribe" title="Subscribe to newsletter"><img src="http://bit.ly/2Md9rxM"></a>\n    <a href="https://twitter.com/mltooling" title="Follow on Twitter"><img src="https://img.shields.io/twitter/follow/mltooling.svg?style=social&label=Follow"></a>\n</p>\n\nThis curated list contains 920 awesome open-source projects with a total of 5.1M stars grouped into 34 categories. All projects are ranked by a project-quality score, which is calculated based on various metrics automatically collected from GitHub and different package managers. If you like to add or update projects, feel free to open an [issue](https://github.com/ml-tooling/best-of-ml-python/issues/new/choose), submit a [pull request](https://github.com/ml-tooling/best-of-ml-python/pulls), or directly edit the [projects.yaml](https://github.com/ml-tooling/best-of-ml-python/edit/main/projects.yaml). Contributions are very welcome!\n\n---\n\n<p align="center">\n     ðŸ§™â€â™‚ï¸&nbsp; Discover other <a href="https://best-of.org">best-of lists</a> or create <a href="https://github.com/best-of-lists/best-of/blob/main/create-best-of-list.md">your own</a>.<br>\n    ðŸ“«&nbsp; Subscribe to our <a href="https://mltooling.substack.com/subscribe">newsletter</a> for updates and trending projects.\n</p>\n\n---\n\n\n## Contents\n\n- [Machine Learning Frameworks](#machine-learning-frameworks) _64 projects_\n- [Data Visualization](#data-visualization) _55 projects_\n- [Text Data & NLP](#text-data--nlp) _103 projects_\n- [Image Data](#image-data) _64 projects_\n- [Graph Data](#graph-data) _36 projects_\n- [Audio Data](#audio-data) _29 projects_\n- [Geospatial Data](#geospatial-data) _22 projects_\n- [Financial Data](#financial-data) _25 projects_\n- [Time Series Data](#time-series-data) _29 projects_\n- [Medical Data](#medical-data) _19 projects_\n- [Tabular Data](#tabular-data) _6 projects_\n- [Optical Character Recognition](#optical-character-recognition) _12 projects_\n- [Data Containers & Structures](#data-containers--structures) _1 projects_\n- [Data Loading & Extraction](#data-loading--extraction) _1 projects_\n- [Web Scraping & Crawling](#web-scraping--crawling) _1 projects_\n- [Data Pipelines & Streaming](#data-pipelines--streaming) _2 projects_\n- [Distributed Machine Learning](#distributed-machine-learning) _36 projects_\n- [Hyperparameter Optimization & AutoML](#hyperparameter-optimization--automl) _52 projects_\n- [Reinforcement Learning](#reinforcement-learning) _23 projects_\n- [Recommender Systems](#recommender-systems) _17 projects_\n- [Privacy Machine Learning](#privacy-machine-learning) _7 projects_\n- [Workflow & Experiment Tracking](#workflow--experiment-tracking) _40 projects_\n- [Model Serialization & Deployment](#model-serialization--deployment) _20 projects_\n- [Model Interpretability](#model-interpretability) _55 projects_\n- [Vector Similarity Search (ANN)](#vector-similarity-search-ann) _13 projects_\n- [Probabilistics & Statistics](#probabilistics--statistics) _24 projects_\n- [Adversarial Robustness](#adversarial-robustness) _9 projects_\n- [GPU & Accelerator Utilities](#gpu--accelerator-utilities) _20 projects_\n- [Tensorflow Utilities](#tensorflow-utilities) _16 projects_\n- [Jax Utilities](#jax-utilities) _3 projects_\n- [Sklearn Utilities](#sklearn-utilities) _19 projects_\n- [Pytorch Utilities](#pytorch-utilities) _32 projects_\n- [Database Clients](#database-clients) _1 projects_\n- [Others](#others) _66 projects_\n\n## Explanation\n- ðŸ¥‡ðŸ¥ˆðŸ¥‰&nbsp; Combined project-quality score\n- â­ï¸&nbsp; Star count from GitHub\n- ðŸ£&nbsp; New project _(less than 6 months old)_\n- ðŸ’¤&nbsp; Inactive project _(6 months no activity)_\n- ðŸ’€&nbsp; Dead project _(12 months no activity)_\n- ðŸ“ˆðŸ“‰&nbsp; Project is trending up or down\n- âž•&nbsp; Project was recently added\n- â—ï¸&nbsp; Warning _(e.g. missing/risky license)_\n- ðŸ‘¨â€ðŸ’»&nbsp; Contributors count from GitHub\n- ðŸ”€&nbsp; Fork count from GitHub\n- ðŸ“‹&nbsp; Issue count from GitHub\n- â±ï¸&nbsp; Last update timestamp on package manager\n- ðŸ“¥&nbsp; Download count from package manager\n- ðŸ“¦&nbsp; Number of dependent projects\n- <img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13">&nbsp; Tensorflow related project\n- <img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13">&nbsp; Sklearn related project\n- <img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13">&nbsp; PyTorch related project\n- <img src="https://git.io/JLy1X" style="display:inline;" width="13" height="13">&nbsp; MxNet related project\n- <img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13">&nbsp; Apache Spark related project\n- <img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13">&nbsp; Jupyter related project\n- <img src="https://git.io/JLy1M" style="display:inline;" width="13" height="13">&nbsp; PaddlePaddle related project\n- <img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13">&nbsp; Pandas related project\n- <img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13">&nbsp; Jax related project\n\n<br>\n\n## Machine Learning Frameworks\n\n<a href="#contents"><img align="right" width="15" height="15" src="https://git.io/JtehR" alt="Back to top"></a>\n\n_General-purpose machine learning and deep learning frameworks._\n\n<details><summary><b><a href="https://github.com/tensorflow/tensorflow">Tensorflow</a></b> (ðŸ¥‡56 Â·  â­ 200K) - An Open Source Machine Learning Framework for Everyone. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/tensorflow/tensorflow) (ðŸ‘¨â€ðŸ’» 5K Â· ðŸ”€ 75K Â· ðŸ“¦ 540K Â· ðŸ“‹ 42K - 4% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/tensorflow/tensorflow\n	```\n- [PyPi](https://pypi.org/project/tensorflow) (ðŸ“¥ 26M / month Â· ðŸ“¦ 9.6K Â· â±ï¸ 13.08.2025):\n	```\n	pip install tensorflow\n	```\n- [Conda](https://anaconda.org/conda-forge/tensorflow) (ðŸ“¥ 6M Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge tensorflow\n	```\n- [Docker Hub](https://hub.docker.com/r/tensorflow/tensorflow) (ðŸ“¥ 81M Â· â­ 2.8K Â· â±ï¸ 30.10.2025):\n	```\n	docker pull tensorflow/tensorflow\n	```\n</details>\n<details><summary><b><a href="https://github.com/pytorch/pytorch">PyTorch</a></b> (ðŸ¥‡56 Â·  â­ 94K) - Tensors and Dynamic neural networks in Python with strong GPU.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pytorch/pytorch) (ðŸ‘¨â€ðŸ’» 6K Â· ðŸ”€ 26K Â· ðŸ“¥ 110K Â· ðŸ“¦ 830K Â· ðŸ“‹ 56K - 30% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/pytorch/pytorch\n	```\n- [PyPi](https://pypi.org/project/torch) (ðŸ“¥ 70M / month Â· ðŸ“¦ 30K Â· â±ï¸ 15.10.2025):\n	```\n	pip install torch\n	```\n- [Conda](https://anaconda.org/pytorch/pytorch) (ðŸ“¥ 29M Â· â±ï¸ 25.03.2025):\n	```\n	conda install -c pytorch pytorch\n	```\n</details>\n<details><summary><b><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a></b> (ðŸ¥‡53 Â·  â­ 64K) - scikit-learn: machine learning in Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/scikit-learn/scikit-learn) (ðŸ‘¨â€ðŸ’» 3.4K Â· ðŸ”€ 26K Â· ðŸ“¥ 1.1K Â· ðŸ“¦ 1.3M Â· ðŸ“‹ 12K - 17% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/scikit-learn/scikit-learn\n	```\n- [PyPi](https://pypi.org/project/scikit-learn) (ðŸ“¥ 140M / month Â· ðŸ“¦ 35K Â· â±ï¸ 09.09.2025):\n	```\n	pip install scikit-learn\n	```\n- [Conda](https://anaconda.org/conda-forge/scikit-learn) (ðŸ“¥ 40M Â· â±ï¸ 09.09.2025):\n	```\n	conda install -c conda-forge scikit-learn\n	```\n</details>\n<details><summary><b><a href="https://github.com/keras-team/keras">Keras</a></b> (ðŸ¥‡50 Â·  â­ 64K) - Deep Learning for humans. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/keras-team/keras) (ðŸ‘¨â€ðŸ’» 1.4K Â· ðŸ”€ 20K Â· ðŸ“¦ 300K Â· ðŸ“‹ 13K - 2% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/keras-team/keras\n	```\n- [PyPi](https://pypi.org/project/keras) (ðŸ“¥ 19M / month Â· ðŸ“¦ 2K Â· â±ï¸ 27.10.2025):\n	```\n	pip install keras\n	```\n- [Conda](https://anaconda.org/conda-forge/keras) (ðŸ“¥ 4.5M Â· â±ï¸ 28.10.2025):\n	```\n	conda install -c conda-forge keras\n	```\n</details>\n<details><summary><b><a href="https://github.com/dmlc/xgboost">XGBoost</a></b> (ðŸ¥‡46 Â·  â­ 28K) - Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/dmlc/xgboost) (ðŸ‘¨â€ðŸ’» 670 Â· ðŸ”€ 8.8K Â· ðŸ“¥ 20K Â· ðŸ“¦ 170K Â· ðŸ“‹ 5.6K - 8% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/dmlc/xgboost\n	```\n- [PyPi](https://pypi.org/project/xgboost) (ðŸ“¥ 31M / month Â· ðŸ“¦ 2.9K Â· â±ï¸ 21.10.2025):\n	```\n	pip install xgboost\n	```\n- [Conda](https://anaconda.org/conda-forge/xgboost) (ðŸ“¥ 6.6M Â· â±ï¸ 16.09.2025):\n	```\n	conda install -c conda-forge xgboost\n	```\n</details>\n<details><summary><b><a href="https://github.com/PaddlePaddle/Paddle">PaddlePaddle</a></b> (ðŸ¥‡46 Â·  â­ 23K) - PArallel Distributed Deep LEarning: Machine Learning.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1M" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/PaddlePaddle/Paddle) (ðŸ‘¨â€ðŸ’» 1.5K Â· ðŸ”€ 5.9K Â· ðŸ“¥ 15K Â· ðŸ“¦ 8.8K Â· ðŸ“‹ 20K - 8% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/PaddlePaddle/Paddle\n	```\n- [PyPi](https://pypi.org/project/paddlepaddle) (ðŸ“¥ 1.6M / month Â· ðŸ“¦ 280 Â· â±ï¸ 30.10.2025):\n	```\n	pip install paddlepaddle\n	```\n</details>\n<details><summary><b><a href="https://github.com/jax-ml/jax">jax</a></b> (ðŸ¥‡45 Â·  â­ 34K) - Composable transformations of Python+NumPy programs: differentiate,.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/jax-ml/jax) (ðŸ‘¨â€ðŸ’» 980 Â· ðŸ”€ 3.2K Â· ðŸ“¦ 47K Â· ðŸ“‹ 6.6K - 24% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/google/jax\n	```\n- [PyPi](https://pypi.org/project/jax) (ðŸ“¥ 12M / month Â· ðŸ“¦ 3.1K Â· â±ï¸ 15.10.2025):\n	```\n	pip install jax\n	```\n- [Conda](https://anaconda.org/conda-forge/jaxlib) (ðŸ“¥ 3.2M Â· â±ï¸ 06.10.2025):\n	```\n	conda install -c conda-forge jaxlib\n	```\n</details>\n<details><summary><b><a href="https://github.com/Lightning-AI/pytorch-lightning">pytorch-lightning</a></b> (ðŸ¥‡45 Â·  â­ 30K) - Pretrain, finetune ANY AI model of ANY size on 1 or.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/Lightning-AI/pytorch-lightning) (ðŸ‘¨â€ðŸ’» 1K Â· ðŸ”€ 3.6K Â· ðŸ“¥ 15K Â· ðŸ“¦ 48K Â· ðŸ“‹ 7.4K - 11% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/Lightning-AI/lightning\n	```\n- [PyPi](https://pypi.org/project/pytorch-lightning) (ðŸ“¥ 9.8M / month Â· ðŸ“¦ 1.8K Â· â±ï¸ 05.09.2025):\n	```\n	pip install pytorch-lightning\n	```\n- [Conda](https://anaconda.org/conda-forge/pytorch-lightning) (ðŸ“¥ 1.7M Â· â±ï¸ 05.09.2025):\n	```\n	conda install -c conda-forge pytorch-lightning\n	```\n</details>\n<details><summary><b><a href="https://github.com/statsmodels/statsmodels">StatsModels</a></b> (ðŸ¥‡45 Â·  â­ 11K) - Statsmodels: statistical modeling and econometrics in Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/statsmodels/statsmodels) (ðŸ‘¨â€ðŸ’» 470 Â· ðŸ”€ 3.3K Â· ðŸ“¥ 36 Â· ðŸ“¦ 180K Â· ðŸ“‹ 5.8K - 50% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/statsmodels/statsmodels\n	```\n- [PyPi](https://pypi.org/project/statsmodels) (ðŸ“¥ 24M / month Â· ðŸ“¦ 5.6K Â· â±ï¸ 07.07.2025):\n	```\n	pip install statsmodels\n	```\n- [Conda](https://anaconda.org/conda-forge/statsmodels) (ðŸ“¥ 22M Â· â±ï¸ 01.10.2025):\n	```\n	conda install -c conda-forge statsmodels\n	```\n</details>\n<details><summary><b><a href="https://github.com/apache/spark">PySpark</a></b> (ðŸ¥ˆ44 Â·  â­ 42K) - Apache Spark Python API. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/apache/spark) (ðŸ‘¨â€ðŸ’» 3.3K Â· ðŸ”€ 29K Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/apache/spark\n	```\n- [PyPi](https://pypi.org/project/pyspark) (ðŸ“¥ 47M / month Â· ðŸ“¦ 2.1K Â· â±ï¸ 30.10.2025):\n	```\n	pip install pyspark\n	```\n- [Conda](https://anaconda.org/conda-forge/pyspark) (ðŸ“¥ 4.2M Â· â±ï¸ 08.09.2025):\n	```\n	conda install -c conda-forge pyspark\n	```\n</details>\n<details><summary><b><a href="https://github.com/microsoft/LightGBM">LightGBM</a></b> (ðŸ¥ˆ42 Â·  â­ 18K) - A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT,.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/microsoft/LightGBM) (ðŸ‘¨â€ðŸ’» 330 Â· ðŸ”€ 3.9K Â· ðŸ“¥ 310K Â· ðŸ“¦ 56K Â· ðŸ“‹ 3.6K - 12% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/microsoft/LightGBM\n	```\n- [PyPi](https://pypi.org/project/lightgbm) (ðŸ“¥ 11M / month Â· ðŸ“¦ 1.6K Â· â±ï¸ 15.02.2025):\n	```\n	pip install lightgbm\n	```\n- [Conda](https://anaconda.org/conda-forge/lightgbm) (ðŸ“¥ 4.1M Â· â±ï¸ 20.10.2025):\n	```\n	conda install -c conda-forge lightgbm\n	```\n</details>\n<details><summary><b><a href="https://github.com/catboost/catboost">Catboost</a></b> (ðŸ¥ˆ42 Â·  â­ 8.6K) - A fast, scalable, high performance Gradient Boosting on Decision.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/catboost/catboost) (ðŸ‘¨â€ðŸ’» 1.4K Â· ðŸ”€ 1.2K Â· ðŸ“¥ 460K Â· ðŸ“¦ 19 Â· ðŸ“‹ 2.5K - 25% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/catboost/catboost\n	```\n- [PyPi](https://pypi.org/project/catboost) (ðŸ“¥ 5.1M / month Â· ðŸ“¦ 650 Â· â±ï¸ 13.04.2025):\n	```\n	pip install catboost\n	```\n- [Conda](https://anaconda.org/conda-forge/catboost) (ðŸ“¥ 2.2M Â· â±ï¸ 09.08.2025):\n	```\n	conda install -c conda-forge catboost\n	```\n</details>\n<details><summary><b><a href="https://github.com/fastai/fastai">Fastai</a></b> (ðŸ¥ˆ41 Â·  â­ 28K) - The fastai deep learning library. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/fastai/fastai) (ðŸ‘¨â€ðŸ’» 680 Â· ðŸ”€ 7.6K Â· ðŸ“¦ 23K Â· ðŸ“‹ 1.9K - 14% open Â· â±ï¸ 26.10.2025):\n\n	```\n	git clone https://github.com/fastai/fastai\n	```\n- [PyPi](https://pypi.org/project/fastai) (ðŸ“¥ 640K / month Â· ðŸ“¦ 340 Â· â±ï¸ 26.10.2025):\n	```\n	pip install fastai\n	```\n</details>\n<details><summary><b><a href="https://github.com/apache/flink">PyFlink</a></b> (ðŸ¥ˆ39 Â·  â­ 25K) - Apache Flink Python API. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/apache/flink) (ðŸ‘¨â€ðŸ’» 2.1K Â· ðŸ”€ 14K Â· ðŸ“¦ 21 Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/apache/flink\n	```\n- [PyPi](https://pypi.org/project/apache-flink) (ðŸ“¥ 450K / month Â· ðŸ“¦ 38 Â· â±ï¸ 28.10.2025):\n	```\n	pip install apache-flink\n	```\n</details>\n<details><summary><b><a href="https://github.com/google/flax">Flax</a></b> (ðŸ¥ˆ38 Â·  â­ 6.9K) - Flax is a neural network library for JAX that is designed for.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/google/flax) (ðŸ‘¨â€ðŸ’» 280 Â· ðŸ”€ 740 Â· ðŸ“¥ 61 Â· ðŸ“¦ 15K Â· ðŸ“‹ 1.3K - 33% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/google/flax\n	```\n- [PyPi](https://pypi.org/project/flax) (ðŸ“¥ 2M / month Â· ðŸ“¦ 740 Â· â±ï¸ 25.09.2025):\n	```\n	pip install flax\n	```\n- [Conda](https://anaconda.org/conda-forge/flax) (ðŸ“¥ 130K Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge flax\n	```\n</details>\n<details><summary><b><a href="https://github.com/pytorch/ignite">Ignite</a></b> (ðŸ¥ˆ36 Â·  â­ 4.7K) - High-level library to help with training and evaluating neural.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pytorch/ignite) (ðŸ‘¨â€ðŸ’» 1K Â· ðŸ”€ 660 Â· ðŸ“¦ 3.9K Â· ðŸ“‹ 1.4K - 10% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/pytorch/ignite\n	```\n- [PyPi](https://pypi.org/project/pytorch-ignite) (ðŸ“¥ 170K / month Â· ðŸ“¦ 120 Â· â±ï¸ 30.10.2025):\n	```\n	pip install pytorch-ignite\n	```\n- [Conda](https://anaconda.org/pytorch/ignite) (ðŸ“¥ 250K Â· â±ï¸ 16.10.2025):\n	```\n	conda install -c pytorch ignite\n	```\n</details>\n<details><summary><b><a href="https://github.com/arogozhnikov/einops">einops</a></b> (ðŸ¥ˆ35 Â·  â­ 9.2K) - Flexible and powerful tensor operations for readable and reliable code.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/arogozhnikov/einops) (ðŸ‘¨â€ðŸ’» 34 Â· ðŸ”€ 380 Â· ðŸ“¦ 82K Â· ðŸ“‹ 200 - 17% open Â· â±ï¸ 12.08.2025):\n\n	```\n	git clone https://github.com/arogozhnikov/einops\n	```\n- [PyPi](https://pypi.org/project/einops) (ðŸ“¥ 15M / month Â· ðŸ“¦ 2.6K Â· â±ï¸ 09.02.2025):\n	```\n	pip install einops\n	```\n- [Conda](https://anaconda.org/conda-forge/einops) (ðŸ“¥ 470K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge einops\n	```\n</details>\n<details><summary><b><a href="https://github.com/ivy-llc/ivy">ivy</a></b> (ðŸ¥ˆ34 Â·  â­ 14K) - Convert Machine Learning Code Between Frameworks. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/ivy-llc/ivy) (ðŸ‘¨â€ðŸ’» 1.5K Â· ðŸ”€ 5.6K Â· ðŸ“‹ 17K - 5% open Â· â±ï¸ 10.10.2025):\n\n	```\n	git clone https://github.com/unifyai/ivy\n	```\n- [PyPi](https://pypi.org/project/ivy) (ðŸ“¥ 33K / month Â· ðŸ“¦ 16 Â· â±ï¸ 16.06.2025):\n	```\n	pip install ivy\n	```\n</details>\n<details><summary><b><a href="https://github.com/jina-ai/serve">Jina</a></b> (ðŸ¥ˆ33 Â·  â­ 22K Â· ðŸ’¤) - Build multimodal AI applications with cloud-native stack. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/jina-ai/serve) (ðŸ‘¨â€ðŸ’» 180 Â· ðŸ”€ 2.2K Â· â±ï¸ 24.03.2025):\n\n	```\n	git clone https://github.com/jina-ai/jina\n	```\n- [PyPi](https://pypi.org/project/jina) (ðŸ“¥ 120K / month Â· ðŸ“¦ 29 Â· â±ï¸ 24.03.2025):\n	```\n	pip install jina\n	```\n- [Conda](https://anaconda.org/conda-forge/jina-core) (ðŸ“¥ 110K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge jina-core\n	```\n- [Docker Hub](https://hub.docker.com/r/jinaai/jina) (ðŸ“¥ 1.8M Â· â­ 9 Â· â±ï¸ 24.03.2025):\n	```\n	docker pull jinaai/jina\n	```\n</details>\n<details><summary><b><a href="https://github.com/mlpack/mlpack">mlpack</a></b> (ðŸ¥ˆ33 Â·  â­ 5.5K) - mlpack: a fast, header-only C++ machine learning library. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/mlpack/mlpack) (ðŸ‘¨â€ðŸ’» 340 Â· ðŸ”€ 1.7K Â· ðŸ“‹ 1.7K - 1% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/mlpack/mlpack\n	```\n- [PyPi](https://pypi.org/project/mlpack) (ðŸ“¥ 4.7K / month Â· ðŸ“¦ 6 Â· â±ï¸ 22.05.2025):\n	```\n	pip install mlpack\n	```\n- [Conda](https://anaconda.org/conda-forge/mlpack) (ðŸ“¥ 410K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge mlpack\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/thinc">Thinc</a></b> (ðŸ¥ˆ33 Â·  â­ 2.9K Â· ðŸ’¤) - A refreshing functional take on deep learning, compatible with your.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/explosion/thinc) (ðŸ‘¨â€ðŸ’» 67 Â· ðŸ”€ 280 Â· ðŸ“¥ 2K Â· ðŸ“¦ 70K Â· ðŸ“‹ 160 - 14% open Â· â±ï¸ 07.03.2025):\n\n	```\n	git clone https://github.com/explosion/thinc\n	```\n- [PyPi](https://pypi.org/project/thinc) (ðŸ“¥ 17M / month Â· ðŸ“¦ 160 Â· â±ï¸ 04.04.2025):\n	```\n	pip install thinc\n	```\n- [Conda](https://anaconda.org/conda-forge/thinc) (ðŸ“¥ 3.9M Â· â±ï¸ 06.07.2025):\n	```\n	conda install -c conda-forge thinc\n	```\n</details>\n<details><summary><b><a href="https://github.com/ludwig-ai/ludwig">Ludwig</a></b> (ðŸ¥‰32 Â·  â­ 12K Â· ðŸ’¤) - Low-code framework for building custom LLMs, neural networks,.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ludwig-ai/ludwig) (ðŸ‘¨â€ðŸ’» 160 Â· ðŸ”€ 1.2K Â· ðŸ“¦ 340 Â· ðŸ“‹ 1.1K - 4% open Â· â±ï¸ 17.10.2024):\n\n	```\n	git clone https://github.com/ludwig-ai/ludwig\n	```\n- [PyPi](https://pypi.org/project/ludwig) (ðŸ“¥ 3.8K / month Â· ðŸ“¦ 6 Â· â±ï¸ 30.07.2024):\n	```\n	pip install ludwig\n	```\n</details>\n<details><summary><b><a href="https://github.com/skorch-dev/skorch">skorch</a></b> (ðŸ¥‰32 Â·  â­ 6.1K) - A scikit-learn compatible neural network library that wraps.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/skorch-dev/skorch) (ðŸ‘¨â€ðŸ’» 68 Â· ðŸ”€ 400 Â· ðŸ“¦ 1.7K Â· ðŸ“‹ 540 - 12% open Â· â±ï¸ 23.10.2025):\n\n	```\n	git clone https://github.com/skorch-dev/skorch\n	```\n- [PyPi](https://pypi.org/project/skorch) (ðŸ“¥ 150K / month Â· ðŸ“¦ 110 Â· â±ï¸ 08.08.2025):\n	```\n	pip install skorch\n	```\n- [Conda](https://anaconda.org/conda-forge/skorch) (ðŸ“¥ 810K Â· â±ï¸ 08.08.2025):\n	```\n	conda install -c conda-forge skorch\n	```\n</details>\n<details><summary><b><a href="https://github.com/google-deepmind/sonnet">Sonnet</a></b> (ðŸ¥‰31 Â·  â­ 9.9K) - TensorFlow-based neural network library. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/google-deepmind/sonnet) (ðŸ‘¨â€ðŸ’» 61 Â· ðŸ”€ 1.3K Â· ðŸ“¦ 1.5K Â· ðŸ“‹ 190 - 16% open Â· â±ï¸ 04.08.2025):\n\n	```\n	git clone https://github.com/deepmind/sonnet\n	```\n- [PyPi](https://pypi.org/project/dm-sonnet) (ðŸ“¥ 35K / month Â· ðŸ“¦ 19 Â· â±ï¸ 02.01.2024):\n	```\n	pip install dm-sonnet\n	```\n- [Conda](https://anaconda.org/conda-forge/sonnet) (ðŸ“¥ 47K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge sonnet\n	```\n</details>\n<details><summary><b><a href="https://github.com/google-deepmind/dm-haiku">Haiku</a></b> (ðŸ¥‰31 Â·  â­ 3.1K Â· ðŸ“‰) - JAX-based neural network library. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/google-deepmind/dm-haiku) (ðŸ‘¨â€ðŸ’» 90 Â· ðŸ”€ 260 Â· ðŸ“¦ 2.6K Â· ðŸ“‹ 250 - 29% open Â· â±ï¸ 29.09.2025):\n\n	```\n	git clone https://github.com/deepmind/dm-haiku\n	```\n- [PyPi](https://pypi.org/project/dm-haiku) (ðŸ“¥ 260K / month Â· ðŸ“¦ 200 Â· â±ï¸ 18.09.2025):\n	```\n	pip install dm-haiku\n	```\n- [Conda](https://anaconda.org/conda-forge/dm-haiku) (ðŸ“¥ 44K Â· â±ï¸ 19.09.2025):\n	```\n	conda install -c conda-forge dm-haiku\n	```\n</details>\n<details><summary><b><a href="https://github.com/ROCm/tensorflow-upstream">tensorflow-upstream</a></b> (ðŸ¥‰31 Â·  â­ 700) - TensorFlow ROCm port. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ROCm/tensorflow-upstream) (ðŸ‘¨â€ðŸ’» 5K Â· ðŸ”€ 100 Â· ðŸ“¥ 31 Â· ðŸ“‹ 400 - 3% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/ROCmSoftwarePlatform/tensorflow-upstream\n	```\n- [PyPi](https://pypi.org/project/tensorflow-rocm) (ðŸ“¥ 1.7K / month Â· ðŸ“¦ 9 Â· â±ï¸ 10.01.2024):\n	```\n	pip install tensorflow-rocm\n	```\n</details>\n<details><summary><b><a href="https://github.com/geomstats/geomstats">Geomstats</a></b> (ðŸ¥‰30 Â·  â­ 1.4K) - Computations and statistics on manifolds with geometric structures. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/geomstats/geomstats) (ðŸ‘¨â€ðŸ’» 97 Â· ðŸ”€ 260 Â· ðŸ“¦ 150 Â· ðŸ“‹ 570 - 36% open Â· â±ï¸ 06.10.2025):\n\n	```\n	git clone https://github.com/geomstats/geomstats\n	```\n- [PyPi](https://pypi.org/project/geomstats) (ðŸ“¥ 15K / month Â· ðŸ“¦ 12 Â· â±ï¸ 09.09.2024):\n	```\n	pip install geomstats\n	```\n- [Conda](https://anaconda.org/conda-forge/geomstats) (ðŸ“¥ 8.2K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge geomstats\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyRiemann/pyRiemann">pyRiemann</a></b> (ðŸ¥‰28 Â·  â­ 700) - Machine learning for multivariate data through the Riemannian.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pyRiemann/pyRiemann) (ðŸ‘¨â€ðŸ’» 38 Â· ðŸ”€ 170 Â· ðŸ“¦ 480 Â· ðŸ“‹ 110 - 2% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/pyRiemann/pyRiemann\n	```\n- [PyPi](https://pypi.org/project/pyriemann) (ðŸ“¥ 75K / month Â· ðŸ“¦ 31 Â· â±ï¸ 23.07.2025):\n	```\n	pip install pyriemann\n	```\n- [Conda](https://anaconda.org/conda-forge/pyriemann) (ðŸ“¥ 16K Â· â±ï¸ 23.07.2025):\n	```\n	conda install -c conda-forge pyriemann\n	```\n</details>\n<details><summary><b><a href="https://github.com/numenta/nupic-legacy">NuPIC</a></b> (ðŸ¥‰27 Â·  â­ 6.4K Â· ðŸ’¤) - Numenta Platform for Intelligent Computing is an implementation of.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/numenta/nupic-legacy) (ðŸ‘¨â€ðŸ’» 120 Â· ðŸ”€ 1.5K Â· ðŸ“¥ 26 Â· ðŸ“¦ 21 Â· ðŸ“‹ 1.8K - 25% open Â· â±ï¸ 03.12.2024):\n\n	```\n	git clone https://github.com/numenta/nupic\n	```\n- [PyPi](https://pypi.org/project/nupic) (ðŸ“¥ 510 / month Â· â±ï¸ 01.09.2016):\n	```\n	pip install nupic\n	```\n</details>\n<details><summary><b><a href="https://github.com/determined-ai/determined">Determined</a></b> (ðŸ¥‰26 Â·  â­ 3.2K Â· ðŸ’¤) - Determined is an open-source machine learning.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/determined-ai/determined) (ðŸ‘¨â€ðŸ’» 120 Â· ðŸ”€ 360 Â· ðŸ“¥ 7.8K Â· ðŸ“‹ 450 - 22% open Â· â±ï¸ 20.03.2025):\n\n	```\n	git clone https://github.com/determined-ai/determined\n	```\n- [PyPi](https://pypi.org/project/determined) (ðŸ“¥ 33K / month Â· ðŸ“¦ 4 Â· â±ï¸ 19.03.2025):\n	```\n	pip install determined\n	```\n</details>\n<details><summary><b><a href="https://github.com/sony/nnabla">Neural Network Libraries</a></b> (ðŸ¥‰26 Â·  â­ 2.8K) - Neural Network Libraries. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/sony/nnabla) (ðŸ‘¨â€ðŸ’» 76 Â· ðŸ”€ 340 Â· ðŸ“¥ 1K Â· ðŸ“‹ 95 - 36% open Â· â±ï¸ 29.08.2025):\n\n	```\n	git clone https://github.com/sony/nnabla\n	```\n- [PyPi](https://pypi.org/project/nnabla) (ðŸ“¥ 1.6K / month Â· ðŸ“¦ 44 Â· â±ï¸ 29.05.2024):\n	```\n	pip install nnabla\n	```\n</details>\n<details><summary><b><a href="https://github.com/deepinv/deepinv">deepinv</a></b> (ðŸ¥‰26 Â·  â­ 540) - DeepInverse: a PyTorch library for solving imaging inverse problems.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/deepinv/deepinv) (ðŸ‘¨â€ðŸ’» 53 Â· ðŸ”€ 120 Â· ðŸ“¥ 24 Â· ðŸ“¦ 23 Â· ðŸ“‹ 350 - 33% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/deepinv/deepinv\n	```\n- [PyPi](https://pypi.org/project/deepinv) (ðŸ“¥ 2.4K / month Â· â±ï¸ 08.10.2025):\n	```\n	pip install deepinv\n	```\n</details>\n<details><summary><b><a href="https://github.com/towhee-io/towhee">Towhee</a></b> (ðŸ¥‰23 Â·  â­ 3.4K Â· ðŸ’¤) - Towhee is a framework that is dedicated to making neural data.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/towhee-io/towhee) (ðŸ‘¨â€ðŸ’» 38 Â· ðŸ”€ 260 Â· ðŸ“¥ 2.7K Â· ðŸ“‹ 670 - 0% open Â· â±ï¸ 18.10.2024):\n\n	```\n	git clone https://github.com/towhee-io/towhee\n	```\n- [PyPi](https://pypi.org/project/towhee) (ðŸ“¥ 1.3K / month Â· â±ï¸ 04.12.2023):\n	```\n	pip install towhee\n	```\n</details>\n<details><summary><b><a href="https://github.com/nubank/fklearn">fklearn</a></b> (ðŸ¥‰22 Â·  â­ 1.5K) - fklearn: Functional Machine Learning. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/nubank/fklearn) (ðŸ‘¨â€ðŸ’» 56 Â· ðŸ”€ 170 Â· ðŸ“¦ 16 Â· ðŸ“‹ 64 - 60% open Â· â±ï¸ 23.04.2025):\n\n	```\n	git clone https://github.com/nubank/fklearn\n	```\n- [PyPi](https://pypi.org/project/fklearn) (ðŸ“¥ 750 / month Â· â±ï¸ 26.02.2025):\n	```\n	pip install fklearn\n	```\n</details>\n<details><summary><b><a href="https://github.com/run-house/kubetorch">Runhouse</a></b> (ðŸ¥‰21 Â·  â­ 1.1K) - Distribute and run AI workloads magically in Python, like PyTorch.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/run-house/kubetorch) (ðŸ‘¨â€ðŸ’» 16 Â· ðŸ”€ 41 Â· ðŸ“¥ 79 Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/run-house/runhouse\n	```\n- [PyPi](https://pypi.org/project/runhouse) (ðŸ“¥ 4.5K / month Â· ðŸ“¦ 1 Â· â±ï¸ 10.03.2025):\n	```\n	pip install runhouse\n	```\n</details>\n<details><summary><b><a href="https://github.com/neoml-lib/neoml">NeoML</a></b> (ðŸ¥‰19 Â·  â­ 790) - Machine learning framework for both deep learning and traditional.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/neoml-lib/neoml) (ðŸ‘¨â€ðŸ’» 41 Â· ðŸ”€ 130 Â· ðŸ“¦ 2 Â· ðŸ“‹ 91 - 40% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/neoml-lib/neoml\n	```\n- [PyPi](https://pypi.org/project/neoml) (ðŸ“¥ 190 / month Â· â±ï¸ 26.12.2023):\n	```\n	pip install neoml\n	```\n</details>\n<details><summary><b><a href="https://github.com/serengil/chefboost">chefboost</a></b> (ðŸ¥‰19 Â·  â­ 480) - A Lightweight Decision Tree Framework supporting regular algorithms:.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/serengil/chefboost) (ðŸ‘¨â€ðŸ’» 7 Â· ðŸ”€ 100 Â· ðŸ“¦ 72 Â· â±ï¸ 09.07.2025):\n\n	```\n	git clone https://github.com/serengil/chefboost\n	```\n- [PyPi](https://pypi.org/project/chefboost) (ðŸ“¥ 770 / month Â· â±ï¸ 30.10.2024):\n	```\n	pip install chefboost\n	```\n</details>\n<details><summary><b><a href="https://github.com/Xtra-Computing/thundergbm">ThunderGBM</a></b> (ðŸ¥‰18 Â·  â­ 710 Â· ðŸ’¤) - ThunderGBM: Fast GBDTs and Random Forests on GPUs. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/Xtra-Computing/thundergbm) (ðŸ‘¨â€ðŸ’» 12 Â· ðŸ”€ 88 Â· ðŸ“¦ 4 Â· ðŸ“‹ 81 - 48% open Â· â±ï¸ 19.03.2025):\n\n	```\n	git clone https://github.com/Xtra-Computing/thundergbm\n	```\n- [PyPi](https://pypi.org/project/thundergbm) (ðŸ“¥ 220 / month Â· â±ï¸ 19.09.2022):\n	```\n	pip install thundergbm\n	```\n</details>\n<details><summary>Show 26 hidden projects...</summary>\n\n- <b><a href="https://github.com/davisking/dlib">dlib</a></b> (ðŸ¥ˆ40 Â·  â­ 14K) - A toolkit for making real world machine learning and data analysis.. <code><a href="https://tldrlegal.com/search?q=BSL-1.0">â—ï¸BSL-1.0</a></code>\n- <b><a href="https://github.com/apache/mxnet">MXNet</a></b> (ðŸ¥ˆ38 Â·  â­ 21K Â· ðŸ’€) - Lightweight, Portable, Flexible Distributed/Mobile Deep.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1X" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/Theano/Theano">Theano</a></b> (ðŸ¥ˆ37 Â·  â­ 10K Â· ðŸ’€) - Theano was a Python library that allows you to define, optimize, and.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/mindsdb/mindsdb">MindsDB</a></b> (ðŸ¥ˆ33 Â·  â­ 37K) - Federated query engine for AI - The only MCP Server youll ever need. <code><a href="https://tldrlegal.com/search?q=ICU">â—ï¸ICU</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/VowpalWabbit/vowpal_wabbit">Vowpal Wabbit</a></b> (ðŸ¥ˆ33 Â·  â­ 8.6K Â· ðŸ’€) - Vowpal Wabbit is a machine learning system which pushes the.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/chainer/chainer">Chainer</a></b> (ðŸ¥ˆ33 Â·  â­ 5.9K Â· ðŸ’€) - A flexible framework of neural networks for deep learning. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/apple/turicreate">Turi Create</a></b> (ðŸ¥‰32 Â·  â­ 11K Â· ðŸ’€) - Turi Create simplifies the development of custom machine.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/tensorpack/tensorpack">tensorpack</a></b> (ðŸ¥‰32 Â·  â­ 6.3K Â· ðŸ’€) - A Neural Net Training Interface on TensorFlow, with.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/tflearn/tflearn">TFlearn</a></b> (ðŸ¥‰31 Â·  â­ 9.6K Â· ðŸ’€) - Deep learning library featuring a higher-level API for TensorFlow. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/clab/dynet">dyNET</a></b> (ðŸ¥‰31 Â·  â­ 3.4K Â· ðŸ’€) - DyNet: The Dynamic Neural Network Toolkit. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/microsoft/CNTK">CNTK</a></b> (ðŸ¥‰29 Â·  â­ 18K Â· ðŸ’€) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/Lasagne/Lasagne">Lasagne</a></b> (ðŸ¥‰28 Â·  â­ 3.9K Â· ðŸ’€) - Lightweight library to build and train neural networks in Theano. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/shogun-toolbox/shogun">SHOGUN</a></b> (ðŸ¥‰26 Â·  â­ 3.1K Â· ðŸ’€) - Unified and efficient Machine Learning. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/amaiya/ktrain">ktrain</a></b> (ðŸ¥‰26 Â·  â­ 1.3K Â· ðŸ’€) - ktrain is a Python library that makes deep learning and AI.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/itdxer/neupy">NeuPy</a></b> (ðŸ¥‰25 Â·  â­ 740 Â· ðŸ’€) - NeuPy is a Tensorflow based python library for prototyping and building.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/aksnzhy/xlearn">xLearn</a></b> (ðŸ¥‰24 Â·  â­ 3.1K Â· ðŸ’€) - High performance, easy-to-use, and scalable machine learning (ML).. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/georgia-tech-db/evadb">EvaDB</a></b> (ðŸ¥‰24 Â·  â­ 2.7K Â· ðŸ’€) - Database system for AI-powered apps. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/NervanaSystems/neon">neon</a></b> (ðŸ¥‰22 Â·  â­ 3.9K Â· ðŸ’€) - Intel Nervana reference deep learning framework committed to best.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/Xtra-Computing/thundersvm">ThunderSVM</a></b> (ðŸ¥‰22 Â·  â­ 1.6K Â· ðŸ’€) - ThunderSVM: A Fast SVM Library on GPUs and CPUs. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/pytorchbearer/torchbearer">Torchbearer</a></b> (ðŸ¥‰22 Â·  â­ 640 Â· ðŸ’€) - torchbearer: A model fitting library for PyTorch. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/XiaoMi/mace">mace</a></b> (ðŸ¥‰21 Â·  â­ 5K Â· ðŸ’€) - MACE is a deep learning inference framework optimized for mobile.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/google/neural-tangents">Neural Tangents</a></b> (ðŸ¥‰21 Â·  â­ 2.4K Â· ðŸ’€) - Fast and Easy Infinite Neural Networks in Python. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/google/objax">Objax</a></b> (ðŸ¥‰20 Â·  â­ 770 Â· ðŸ’€) - Objax is a machine learning framework that provides an Object.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/poets-ai/elegy">elegy</a></b> (ðŸ¥‰19 Â·  â­ 480 Â· ðŸ’€) - A High Level API for Deep Learning in JAX. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/facebookresearch/StarSpace">StarSpace</a></b> (ðŸ¥‰16 Â·  â­ 4K Â· ðŸ’€) - Learning embeddings for classification, retrieval and ranking. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/HenryNdubuaku/nanodl">nanodl</a></b> (ðŸ¥‰14 Â·  â­ 300 Â· ðŸ’€) - A Jax-based library for building transformers, includes.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code>\n</details>\n<br>\n\n## Data Visualization\n\n<a href="#contents"><img align="right" width="15" height="15" src="https://git.io/JtehR" alt="Back to top"></a>\n\n_General-purpose and task-specific data visualization libraries._\n\n<details><summary><b><a href="https://github.com/matplotlib/matplotlib">Matplotlib</a></b> (ðŸ¥‡49 Â·  â­ 22K) - matplotlib: plotting with Python. <code>â—Unlicensed</code></summary>\n\n- [GitHub](https://github.com/matplotlib/matplotlib) (ðŸ‘¨â€ðŸ’» 1.9K Â· ðŸ”€ 8.1K Â· ðŸ“¦ 1.9M Â· ðŸ“‹ 11K - 14% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/matplotlib/matplotlib\n	```\n- [PyPi](https://pypi.org/project/matplotlib) (ðŸ“¥ 120M / month Â· ðŸ“¦ 68K Â· â±ï¸ 09.10.2025):\n	```\n	pip install matplotlib\n	```\n- [Conda](https://anaconda.org/conda-forge/matplotlib) (ðŸ“¥ 33M Â· â±ï¸ 15.10.2025):\n	```\n	conda install -c conda-forge matplotlib\n	```\n</details>\n<details><summary><b><a href="https://github.com/plotly/plotly.py">Plotly</a></b> (ðŸ¥‡47 Â·  â­ 18K) - The interactive graphing library for Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/plotly/plotly.py) (ðŸ‘¨â€ðŸ’» 300 Â· ðŸ”€ 2.7K Â· ðŸ“¥ 550 Â· ðŸ“¦ 460K Â· ðŸ“‹ 3.3K - 21% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/plotly/plotly.py\n	```\n- [PyPi](https://pypi.org/project/plotly) (ðŸ“¥ 37M / month Â· ðŸ“¦ 9.7K Â· â±ï¸ 02.10.2025):\n	```\n	pip install plotly\n	```\n- [Conda](https://anaconda.org/conda-forge/plotly) (ðŸ“¥ 12M Â· â±ï¸ 03.10.2025):\n	```\n	conda install -c conda-forge plotly\n	```\n- [npm](https://www.npmjs.com/package/plotlywidget) (ðŸ“¥ 2.8K / month Â· ðŸ“¦ 9 Â· â±ï¸ 12.01.2021):\n	```\n	npm install plotlywidget\n	```\n</details>\n<details><summary><b><a href="https://github.com/plotly/dash">dash</a></b> (ðŸ¥‡45 Â·  â­ 24K) - Data Apps & Dashboards for Python. No JavaScript Required. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/plotly/dash) (ðŸ‘¨â€ðŸ’» 190 Â· ðŸ”€ 2.2K Â· ðŸ“¥ 120 Â· ðŸ“¦ 89K Â· ðŸ“‹ 2.1K - 27% open Â· â±ï¸ 21.10.2025):\n\n	```\n	git clone https://github.com/plotly/dash\n	```\n- [PyPi](https://pypi.org/project/dash) (ðŸ“¥ 5.5M / month Â· ðŸ“¦ 1.9K Â· â±ï¸ 22.10.2025):\n	```\n	pip install dash\n	```\n- [Conda](https://anaconda.org/conda-forge/dash) (ðŸ“¥ 2.1M Â· â±ï¸ 11.08.2025):\n	```\n	conda install -c conda-forge dash\n	```\n</details>\n<details><summary><b><a href="https://github.com/bokeh/bokeh">Bokeh</a></b> (ðŸ¥‡45 Â·  â­ 20K) - Interactive Data Visualization in the browser, from Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/bokeh/bokeh) (ðŸ‘¨â€ðŸ’» 720 Â· ðŸ”€ 4.2K Â· ðŸ“¦ 100K Â· ðŸ“‹ 8.1K - 10% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/bokeh/bokeh\n	```\n- [PyPi](https://pypi.org/project/bokeh) (ðŸ“¥ 5M / month Â· ðŸ“¦ 2.2K Â· â±ï¸ 13.10.2025):\n	```\n	pip install bokeh\n	```\n- [Conda](https://anaconda.org/conda-forge/bokeh) (ðŸ“¥ 18M Â· â±ï¸ 30.08.2025):\n	```\n	conda install -c conda-forge bokeh\n	```\n</details>\n<details><summary><b><a href="https://github.com/mwaskom/seaborn">Seaborn</a></b> (ðŸ¥‡42 Â·  â­ 14K) - Statistical data visualization in Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/mwaskom/seaborn) (ðŸ‘¨â€ðŸ’» 220 Â· ðŸ”€ 2K Â· ðŸ“¥ 510 Â· ðŸ“¦ 700K Â· ðŸ“‹ 2.6K - 6% open Â· â±ï¸ 10.07.2025):\n\n	```\n	git clone https://github.com/mwaskom/seaborn\n	```\n- [PyPi](https://pypi.org/project/seaborn) (ðŸ“¥ 31M / month Â· ðŸ“¦ 11K Â· â±ï¸ 25.01.2024):\n	```\n	pip install seaborn\n	```\n- [Conda](https://anaconda.org/conda-forge/seaborn) (ðŸ“¥ 15M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge seaborn\n	```\n</details>\n<details><summary><b><a href="https://github.com/vega/altair">Altair</a></b> (ðŸ¥‡41 Â·  â­ 10K) - Declarative visualization library for Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/vega/altair) (ðŸ‘¨â€ðŸ’» 180 Â· ðŸ”€ 800 Â· ðŸ“¥ 280 Â· ðŸ“¦ 240K Â· ðŸ“‹ 2.1K - 6% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/altair-viz/altair\n	```\n- [PyPi](https://pypi.org/project/altair) (ðŸ“¥ 37M / month Â· ðŸ“¦ 920 Â· â±ï¸ 23.11.2024):\n	```\n	pip install altair\n	```\n- [Conda](https://anaconda.org/conda-forge/altair) (ðŸ“¥ 3M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge altair\n	```\n</details>\n<details><summary><b><a href="https://github.com/voxel51/fiftyone">FiftyOne</a></b> (ðŸ¥ˆ39 Â·  â­ 10K) - Visualize, create, and debug image and video datasets.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/voxel51/fiftyone) (ðŸ‘¨â€ðŸ’» 160 Â· ðŸ”€ 680 Â· ðŸ“¦ 1K Â· ðŸ“‹ 1.8K - 35% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/voxel51/fiftyone\n	```\n- [PyPi](https://pypi.org/project/fiftyone) (ðŸ“¥ 170K / month Â· ðŸ“¦ 36 Â· â±ï¸ 20.10.2025):\n	```\n	pip install fiftyone\n	```\n</details>\n<details><summary><b><a href="https://github.com/xflr6/graphviz">Graphviz</a></b> (ðŸ¥ˆ39 Â·  â­ 1.8K) - Simple Python interface for Graphviz. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/xflr6/graphviz) (ðŸ‘¨â€ðŸ’» 24 Â· ðŸ”€ 220 Â· ðŸ“¦ 95K Â· ðŸ“‹ 190 - 6% open Â· â±ï¸ 26.10.2025):\n\n	```\n	git clone https://github.com/xflr6/graphviz\n	```\n- [PyPi](https://pypi.org/project/graphviz) (ðŸ“¥ 26M / month Â· ðŸ“¦ 3.2K Â· â±ï¸ 15.06.2025):\n	```\n	pip install graphviz\n	```\n- [Conda](https://anaconda.org/anaconda/python-graphviz) (ðŸ“¥ 59K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c anaconda python-graphviz\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyvista/pyvista">PyVista</a></b> (ðŸ¥ˆ38 Â·  â­ 3.3K) - 3D plotting and mesh analysis through a streamlined interface for.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pyvista/pyvista) (ðŸ‘¨â€ðŸ’» 190 Â· ðŸ”€ 590 Â· ðŸ“¥ 960 Â· ðŸ“¦ 5.2K Â· ðŸ“‹ 2K - 35% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/pyvista/pyvista\n	```\n- [PyPi](https://pypi.org/project/pyvista) (ðŸ“¥ 1M / month Â· ðŸ“¦ 820 Â· â±ï¸ 26.08.2025):\n	```\n	pip install pyvista\n	```\n- [Conda](https://anaconda.org/conda-forge/pyvista) (ðŸ“¥ 810K Â· â±ï¸ 10.10.2025):\n	```\n	conda install -c conda-forge pyvista\n	```\n</details>\n<details><summary><b><a href="https://github.com/holoviz/holoviews">HoloViews</a></b> (ðŸ¥ˆ38 Â·  â­ 2.8K) - With Holoviews, your data visualizes itself. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/holoviz/holoviews) (ðŸ‘¨â€ðŸ’» 150 Â· ðŸ”€ 410 Â· ðŸ“¦ 17K Â· ðŸ“‹ 3.4K - 31% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/holoviz/holoviews\n	```\n- [PyPi](https://pypi.org/project/holoviews) (ðŸ“¥ 820K / month Â· ðŸ“¦ 490 Â· â±ï¸ 29.10.2025):\n	```\n	pip install holoviews\n	```\n- [Conda](https://anaconda.org/conda-forge/holoviews) (ðŸ“¥ 2.4M Â· â±ï¸ 25.06.2025):\n	```\n	conda install -c conda-forge holoviews\n	```\n- [npm](https://www.npmjs.com/package/@pyviz/jupyterlab_pyviz) (ðŸ“¥ 380 / month Â· ðŸ“¦ 7 Â· â±ï¸ 20.06.2025):\n	```\n	npm install @pyviz/jupyterlab_pyviz\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyecharts/pyecharts">pyecharts</a></b> (ðŸ¥ˆ37 Â·  â­ 16K) - Python Echarts Plotting Library. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pyecharts/pyecharts) (ðŸ‘¨â€ðŸ’» 45 Â· ðŸ”€ 2.9K Â· ðŸ“¥ 75 Â· ðŸ“¦ 5.5K Â· ðŸ“‹ 1.9K - 0% open Â· â±ï¸ 10.10.2025):\n\n	```\n	git clone https://github.com/pyecharts/pyecharts\n	```\n- [PyPi](https://pypi.org/project/pyecharts) (ðŸ“¥ 530K / month Â· ðŸ“¦ 280 Â· â±ï¸ 10.10.2025):\n	```\n	pip install pyecharts\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyqtgraph/pyqtgraph">PyQtGraph</a></b> (ðŸ¥ˆ37 Â·  â­ 4.2K) - Fast data visualization and GUI tools for scientific / engineering.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/pyqtgraph/pyqtgraph) (ðŸ‘¨â€ðŸ’» 300 Â· ðŸ”€ 1.1K Â· ðŸ“¦ 13K Â· ðŸ“‹ 1.4K - 31% open Â· â±ï¸ 02.10.2025):\n\n	```\n	git clone https://github.com/pyqtgraph/pyqtgraph\n	```\n- [PyPi](https://pypi.org/project/pyqtgraph) (ðŸ“¥ 560K / month Â· ðŸ“¦ 1K Â· â±ï¸ 29.04.2024):\n	```\n	pip install pyqtgraph\n	```\n- [Conda](https://anaconda.org/conda-forge/pyqtgraph) (ðŸ“¥ 880K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge pyqtgraph\n	```\n</details>\n<details><summary><b><a href="https://github.com/ydataai/ydata-profiling">pandas-profiling</a></b> (ðŸ¥ˆ35 Â·  â­ 13K) - 1 Line of code data quality profiling & exploratory.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ydataai/ydata-profiling) (ðŸ‘¨â€ðŸ’» 140 Â· ðŸ”€ 1.7K Â· ðŸ“¥ 490 Â· ðŸ“¦ 6.9K Â· ðŸ“‹ 850 - 30% open Â· â±ï¸ 19.09.2025):\n\n	```\n	git clone https://github.com/ydataai/pandas-profiling\n	```\n- [PyPi](https://pypi.org/project/pandas-profiling) (ðŸ“¥ 330K / month Â· ðŸ“¦ 180 Â· â±ï¸ 03.02.2023):\n	```\n	pip install pandas-profiling\n	```\n- [Conda](https://anaconda.org/conda-forge/pandas-profiling) (ðŸ“¥ 590K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge pandas-profiling\n	```\n</details>\n<details><summary><b><a href="https://github.com/has2k1/plotnine">plotnine</a></b> (ðŸ¥ˆ35 Â·  â­ 4.4K) - A Grammar of Graphics for Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/has2k1/plotnine) (ðŸ‘¨â€ðŸ’» 110 Â· ðŸ”€ 240 Â· ðŸ“¦ 13K Â· ðŸ“‹ 750 - 10% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/has2k1/plotnine\n	```\n- [PyPi](https://pypi.org/project/plotnine) (ðŸ“¥ 2.2M / month Â· ðŸ“¦ 400 Â· â±ï¸ 15.07.2025):\n	```\n	pip install plotnine\n	```\n- [Conda](https://anaconda.org/conda-forge/plotnine) (ðŸ“¥ 560K Â· â±ï¸ 15.07.2025):\n	```\n	conda install -c conda-forge plotnine\n	```\n</details>\n<details><summary><b><a href="https://github.com/SciTools/cartopy">cartopy</a></b> (ðŸ¥ˆ35 Â·  â­ 1.5K) - Cartopy - a cartographic python library with matplotlib support. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/SciTools/cartopy) (ðŸ‘¨â€ðŸ’» 140 Â· ðŸ”€ 390 Â· ðŸ“¦ 8.1K Â· ðŸ“‹ 1.3K - 23% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/SciTools/cartopy\n	```\n- [PyPi](https://pypi.org/project/cartopy) (ðŸ“¥ 810K / month Â· ðŸ“¦ 970 Â· â±ï¸ 01.08.2025):\n	```\n	pip install cartopy\n	```\n- [Conda](https://anaconda.org/conda-forge/cartopy) (ðŸ“¥ 5.6M Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge cartopy\n	```\n</details>\n<details><summary><b><a href="https://github.com/vispy/vispy">VisPy</a></b> (ðŸ¥ˆ34 Â·  â­ 3.5K Â· ðŸ“‰) - High-performance interactive 2D/3D data visualization library. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/vispy/vispy) (ðŸ‘¨â€ðŸ’» 210 Â· ðŸ”€ 620 Â· ðŸ“¦ 2.1K Â· ðŸ“‹ 1.5K - 25% open Â· â±ï¸ 13.10.2025):\n\n	```\n	git clone https://github.com/vispy/vispy\n	```\n- [PyPi](https://pypi.org/project/vispy) (ðŸ“¥ 190K / month Â· ðŸ“¦ 200 Â· â±ï¸ 19.05.2025):\n	```\n	pip install vispy\n	```\n- [Conda](https://anaconda.org/conda-forge/vispy) (ðŸ“¥ 980K Â· â±ï¸ 30.08.2025):\n	```\n	conda install -c conda-forge vispy\n	```\n- [npm](https://www.npmjs.com/package/vispy) (ðŸ“¥ 12 / month Â· ðŸ“¦ 3 Â· â±ï¸ 15.03.2020):\n	```\n	npm install vispy\n	```\n</details>\n<details><summary><b><a href="https://github.com/holoviz/datashader">datashader</a></b> (ðŸ¥ˆ34 Â·  â­ 3.5K) - Quickly and accurately render even the largest data. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/holoviz/datashader) (ðŸ‘¨â€ðŸ’» 63 Â· ðŸ”€ 380 Â· ðŸ“¦ 6.3K Â· ðŸ“‹ 620 - 24% open Â· â±ï¸ 09.10.2025):\n\n	```\n	git clone https://github.com/holoviz/datashader\n	```\n- [PyPi](https://pypi.org/project/datashader) (ðŸ“¥ 280K / month Â· ðŸ“¦ 250 Â· â±ï¸ 05.08.2025):\n	```\n	pip install datashader\n	```\n- [Conda](https://anaconda.org/conda-forge/datashader) (ðŸ“¥ 1.6M Â· â±ï¸ 05.08.2025):\n	```\n	conda install -c conda-forge datashader\n	```\n</details>\n<details><summary><b><a href="https://github.com/JetBrains/lets-plot">lets-plot</a></b> (ðŸ¥ˆ34 Â·  â­ 1.7K) - Multiplatform plotting library based on the Grammar of Graphics. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/JetBrains/lets-plot) (ðŸ‘¨â€ðŸ’» 21 Â· ðŸ”€ 54 Â· ðŸ“¥ 3.4K Â· ðŸ“¦ 190 Â· ðŸ“‹ 740 - 21% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/JetBrains/lets-plot\n	```\n- [PyPi](https://pypi.org/project/lets-plot) (ðŸ“¥ 120K / month Â· ðŸ“¦ 16 Â· â±ï¸ 12.09.2025):\n	```\n	pip install lets-plot\n	```\n</details>\n<details><summary><b><a href="https://github.com/amueller/word_cloud">wordcloud</a></b> (ðŸ¥ˆ33 Â·  â­ 10K) - A little word cloud generator in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/amueller/word_cloud) (ðŸ‘¨â€ðŸ’» 75 Â· ðŸ”€ 2.3K Â· ðŸ“¦ 21 Â· ðŸ“‹ 560 - 24% open Â· â±ï¸ 31.08.2025):\n\n	```\n	git clone https://github.com/amueller/word_cloud\n	```\n- [PyPi](https://pypi.org/project/wordcloud) (ðŸ“¥ 2M / month Â· ðŸ“¦ 550 Â· â±ï¸ 10.11.2024):\n	```\n	pip install wordcloud\n	```\n- [Conda](https://anaconda.org/conda-forge/wordcloud) (ðŸ“¥ 790K Â· â±ï¸ 03.09.2025):\n	```\n	conda install -c conda-forge wordcloud\n	```\n</details>\n<details><summary><b><a href="https://github.com/perspective-dev/perspective">Perspective</a></b> (ðŸ¥ˆ33 Â·  â­ 9.5K) - A data visualization and analytics component, especially.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/perspective-dev/perspective) (ðŸ‘¨â€ðŸ’» 100 Â· ðŸ”€ 1.2K Â· ðŸ“¥ 12K Â· ðŸ“¦ 190 Â· ðŸ“‹ 890 - 12% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/finos/perspective\n	```\n- [PyPi](https://pypi.org/project/perspective-python) (ðŸ“¥ 17K / month Â· ðŸ“¦ 31 Â· â±ï¸ 28.10.2025):\n	```\n	pip install perspective-python\n	```\n- [Conda](https://anaconda.org/conda-forge/perspective) (ðŸ“¥ 2.4M Â· â±ï¸ 28.10.2025):\n	```\n	conda install -c conda-forge perspective\n	```\n- [npm](https://www.npmjs.com/package/@finos/perspective-jupyterlab) (ðŸ“¥ 600 / month Â· ðŸ“¦ 6 Â· â±ï¸ 03.09.2025):\n	```\n	npm install @finos/perspective-jupyterlab\n	```\n</details>\n<details><summary><b><a href="https://github.com/lmcinnes/umap">UMAP</a></b> (ðŸ¥ˆ33 Â·  â­ 8K) - Uniform Manifold Approximation and Projection. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/lmcinnes/umap) (ðŸ‘¨â€ðŸ’» 140 Â· ðŸ”€ 850 Â· ðŸ“¦ 1 Â· ðŸ“‹ 860 - 59% open Â· â±ï¸ 26.10.2025):\n\n	```\n	git clone https://github.com/lmcinnes/umap\n	```\n- [PyPi](https://pypi.org/project/umap-learn) (ðŸ“¥ 2.7M / month Â· ðŸ“¦ 1.3K Â· â±ï¸ 03.07.2025):\n	```\n	pip install umap-learn\n	```\n- [Conda](https://anaconda.org/conda-forge/umap-learn) (ðŸ“¥ 3.2M Â· â±ï¸ 03.07.2025):\n	```\n	conda install -c conda-forge umap-learn\n	```\n</details>\n<details><summary><b><a href="https://github.com/holoviz/hvplot">hvPlot</a></b> (ðŸ¥ˆ32 Â·  â­ 1.3K) - A high-level plotting API for pandas, dask, xarray, and networkx built.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/holoviz/hvplot) (ðŸ‘¨â€ðŸ’» 52 Â· ðŸ”€ 110 Â· ðŸ“¦ 7.3K Â· ðŸ“‹ 940 - 41% open Â· â±ï¸ 24.10.2025):\n\n	```\n	git clone https://github.com/holoviz/hvplot\n	```\n- [PyPi](https://pypi.org/project/hvplot) (ðŸ“¥ 310K / month Â· ðŸ“¦ 270 Â· â±ï¸ 29.08.2025):\n	```\n	pip install hvplot\n	```\n- [Conda](https://anaconda.org/conda-forge/hvplot) (ðŸ“¥ 860K Â· â±ï¸ 04.09.2025):\n	```\n	conda install -c conda-forge hvplot\n	```\n</details>\n<details><summary><b><a href="https://github.com/mpld3/mpld3">mpld3</a></b> (ðŸ¥‰31 Â·  â­ 2.4K Â· ðŸ“‰) - An interactive data visualization tool which brings matplotlib.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/mpld3/mpld3) (ðŸ‘¨â€ðŸ’» 54 Â· ðŸ”€ 360 Â· ðŸ“¦ 7.6K Â· ðŸ“‹ 370 - 59% open Â· â±ï¸ 27.07.2025):\n\n	```\n	git clone https://github.com/mpld3/mpld3\n	```\n- [PyPi](https://pypi.org/project/mpld3) (ðŸ“¥ 440K / month Â· ðŸ“¦ 160 Â· â±ï¸ 27.07.2025):\n	```\n	pip install mpld3\n	```\n- [Conda](https://anaconda.org/conda-forge/mpld3) (ðŸ“¥ 280K Â· â±ï¸ 28.07.2025):\n	```\n	conda install -c conda-forge mpld3\n	```\n- [npm](https://www.npmjs.com/package/mpld3) (ðŸ“¥ 900 / month Â· ðŸ“¦ 11 Â· â±ï¸ 27.07.2025):\n	```\n	npm install mpld3\n	```\n</details>\n<details><summary><b><a href="https://github.com/bqplot/bqplot">bqplot</a></b> (ðŸ¥‰30 Â·  â­ 3.7K) - Plotting library for IPython/Jupyter notebooks. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/bqplot/bqplot) (ðŸ‘¨â€ðŸ’» 66 Â· ðŸ”€ 480 Â· ðŸ“¦ 62 Â· ðŸ“‹ 650 - 42% open Â· â±ï¸ 25.08.2025):\n\n	```\n	git clone https://github.com/bqplot/bqplot\n	```\n- [PyPi](https://pypi.org/project/bqplot) (ðŸ“¥ 230K / month Â· ðŸ“¦ 110 Â· â±ï¸ 21.05.2025):\n	```\n	pip install bqplot\n	```\n- [Conda](https://anaconda.org/conda-forge/bqplot) (ðŸ“¥ 1.9M Â· â±ï¸ 02.09.2025):\n	```\n	conda install -c conda-forge bqplot\n	```\n- [npm](https://www.npmjs.com/package/bqplot) (ðŸ“¥ 3K / month Â· ðŸ“¦ 21 Â· â±ï¸ 03.09.2025):\n	```\n	npm install bqplot\n	```\n</details>\n<details><summary><b><a href="https://github.com/man-group/dtale">D-Tale</a></b> (ðŸ¥‰29 Â·  â­ 5K) - Visualizer for pandas data structures. <code><a href="https://tldrlegal.com/search?q=LGPL-2.1">â—ï¸LGPL-2.1</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/man-group/dtale) (ðŸ‘¨â€ðŸ’» 31 Â· ðŸ”€ 430 Â· ðŸ“¦ 1.5K Â· ðŸ“‹ 610 - 10% open Â· â±ï¸ 30.07.2025):\n\n	```\n	git clone https://github.com/man-group/dtale\n	```\n- [PyPi](https://pypi.org/project/dtale) (ðŸ“¥ 31K / month Â· ðŸ“¦ 53 Â· â±ï¸ 30.07.2025):\n	```\n	pip install dtale\n	```\n- [Conda](https://anaconda.org/conda-forge/dtale) (ðŸ“¥ 480K Â· â±ï¸ 30.07.2025):\n	```\n	conda install -c conda-forge dtale\n	```\n</details>\n<details><summary><b><a href="https://github.com/pavlin-policar/openTSNE">openTSNE</a></b> (ðŸ¥‰29 Â·  â­ 1.6K Â· ðŸ“ˆ) - Extensible, parallel implementations of t-SNE. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/pavlin-policar/openTSNE) (ðŸ‘¨â€ðŸ’» 14 Â· ðŸ”€ 170 Â· ðŸ“¦ 1.1K Â· ðŸ“‹ 150 - 2% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/pavlin-policar/openTSNE\n	```\n- [PyPi](https://pypi.org/project/opentsne) (ðŸ“¥ 58K / month Â· ðŸ“¦ 69 Â· â±ï¸ 27.10.2025):\n	```\n	pip install opentsne\n	```\n- [Conda](https://anaconda.org/conda-forge/opentsne) (ðŸ“¥ 500K Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge opentsne\n	```\n</details>\n<details><summary><b><a href="https://github.com/predict-idlab/plotly-resampler">Plotly-Resampler</a></b> (ðŸ¥‰27 Â·  â­ 1.2K) - Visualize large time series data with plotly.py. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/predict-idlab/plotly-resampler) (ðŸ‘¨â€ðŸ’» 14 Â· ðŸ”€ 74 Â· ðŸ“¦ 2K Â· ðŸ“‹ 190 - 32% open Â· â±ï¸ 03.09.2025):\n\n	```\n	git clone https://github.com/predict-idlab/plotly-resampler\n	```\n- [PyPi](https://pypi.org/project/plotly-resampler) (ðŸ“¥ 370K / month Â· ðŸ“¦ 38 Â· â±ï¸ 29.08.2025):\n	```\n	pip install plotly-resampler\n	```\n- [Conda](https://anaconda.org/conda-forge/plotly-resampler) (ðŸ“¥ 140K Â· â±ï¸ 09.10.2025):\n	```\n	conda install -c conda-forge plotly-resampler\n	```\n</details>\n<details><summary><b><a href="https://github.com/ContextLab/hypertools">HyperTools</a></b> (ðŸ¥‰26 Â·  â­ 1.9K) - A Python toolbox for gaining geometric insights into high-dimensional.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/ContextLab/hypertools) (ðŸ‘¨â€ðŸ’» 23 Â· ðŸ”€ 160 Â· ðŸ“¥ 73 Â· ðŸ“¦ 510 Â· ðŸ“‹ 200 - 34% open Â· â±ï¸ 10.07.2025):\n\n	```\n	git clone https://github.com/ContextLab/hypertools\n	```\n- [PyPi](https://pypi.org/project/hypertools) (ðŸ“¥ 1.1K / month Â· ðŸ“¦ 2 Â· â±ï¸ 09.07.2025):\n	```\n	pip install hypertools\n	```\n</details>\n<details><summary><b><a href="https://github.com/tensorflow/data-validation">data-validation</a></b> (ðŸ¥‰25 Â·  â­ 780) - Library for exploring and validating machine learning.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/tensorflow/data-validation) (ðŸ‘¨â€ðŸ’» 30 Â· ðŸ”€ 180 Â· ðŸ“¥ 1K Â· ðŸ“‹ 190 - 20% open Â· â±ï¸ 23.06.2025):\n\n	```\n	git clone https://github.com/tensorflow/data-validation\n	```\n- [PyPi](https://pypi.org/project/tensorflow-data-validation) (ðŸ“¥ 150K / month Â· ðŸ“¦ 32 Â· â±ï¸ 09.06.2025):\n	```\n	pip install tensorflow-data-validation\n	```\n</details>\n<details><summary><b><a href="https://github.com/spotify/chartify">Chartify</a></b> (ðŸ¥‰24 Â·  â­ 3.6K Â· ðŸ’¤) - Python library that makes it easy for data scientists to create.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/spotify/chartify) (ðŸ‘¨â€ðŸ’» 27 Â· ðŸ”€ 340 Â· ðŸ“¦ 83 Â· ðŸ“‹ 86 - 62% open Â· â±ï¸ 16.10.2024):\n\n	```\n	git clone https://github.com/spotify/chartify\n	```\n- [PyPi](https://pypi.org/project/chartify) (ðŸ“¥ 1.2K / month Â· ðŸ“¦ 9 Â· â±ï¸ 16.10.2024):\n	```\n	pip install chartify\n	```\n- [Conda](https://anaconda.org/conda-forge/chartify) (ðŸ“¥ 40K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge chartify\n	```\n</details>\n<details><summary><b><a href="https://github.com/ing-bank/popmon">Popmon</a></b> (ðŸ¥‰22 Â·  â­ 510) - Monitor the stability of a Pandas or Spark dataframe. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ing-bank/popmon) (ðŸ‘¨â€ðŸ’» 19 Â· ðŸ”€ 36 Â· ðŸ“¥ 280 Â· ðŸ“¦ 22 Â· ðŸ“‹ 57 - 28% open Â· â±ï¸ 04.09.2025):\n\n	```\n	git clone https://github.com/ing-bank/popmon\n	```\n- [PyPi](https://pypi.org/project/popmon) (ðŸ“¥ 3.4K / month Â· ðŸ“¦ 4 Â· â±ï¸ 04.09.2025):\n	```\n	pip install popmon\n	```\n</details>\n<details><summary><b><a href="https://github.com/vega/ipyvega">vega</a></b> (ðŸ¥‰22 Â·  â­ 390 Â· ðŸ’¤) - IPython/Jupyter notebook module for Vega and Vega-Lite. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/vega/ipyvega) (ðŸ‘¨â€ðŸ’» 15 Â· ðŸ”€ 65 Â· ðŸ“¦ 4 Â· ðŸ“‹ 110 - 14% open Â· â±ï¸ 01.01.2025):\n\n	```\n	git clone https://github.com/vega/ipyvega\n	```\n- [PyPi](https://pypi.org/project/vega) (ðŸ“¥ 26K / month Â· ðŸ“¦ 17 Â· â±ï¸ 25.09.2024):\n	```\n	pip install vega\n	```\n- [Conda](https://anaconda.org/conda-forge/vega) (ðŸ“¥ 940K Â· â±ï¸ 04.10.2025):\n	```\n	conda install -c conda-forge vega\n	```\n</details>\n<details><summary><b><a href="https://github.com/vega/vegafusion">vegafusion</a></b> (ðŸ¥‰21 Â·  â­ 390) - Serverside scaling for Vega and Altair visualizations. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/vega/vegafusion) (ðŸ‘¨â€ðŸ’» 6 Â· ðŸ”€ 26 Â· ðŸ“¥ 6.6K Â· ðŸ“‹ 150 - 36% open Â· â±ï¸ 29.09.2025):\n\n	```\n	git clone https://github.com/vegafusion/vegafusion\n	```\n- [PyPi](https://pypi.org/project/vegafusion-jupyter) (ðŸ“¥ 770 / month Â· ðŸ“¦ 2 Â· â±ï¸ 09.05.2024):\n	```\n	pip install vegafusion-jupyter\n	```\n- [Conda](https://anaconda.org/conda-forge/vegafusion-python-embed) (ðŸ“¥ 520K Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge vegafusion-python-embed\n	```\n- [npm](https://www.npmjs.com/package/vegafusion-jupyter) (ðŸ“¥ 1.9K / month Â· ðŸ“¦ 3 Â· â±ï¸ 09.05.2024):\n	```\n	npm install vegafusion-jupyter\n	```\n</details>\n<details><summary>Show 22 hidden projects...</summary>\n\n- <b><a href="https://github.com/ResidentMario/missingno">missingno</a></b> (ðŸ¥‰30 Â·  â­ 4.2K Â· ðŸ’€) - Missing data visualization module for Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/PAIR-code/facets">Facets Overview</a></b> (ðŸ¥‰28 Â·  â­ 7.4K Â· ðŸ’€) - Visualizations for machine learning datasets. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/santosjorge/cufflinks">Cufflinks</a></b> (ðŸ¥‰28 Â·  â­ 3.1K Â· ðŸ’€) - Productivity Tools for Plotly + Pandas. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/jupyter-widgets/pythreejs">pythreejs</a></b> (ðŸ¥‰27 Â·  â­ 980 Â· ðŸ’€) - A Jupyter - Three.js bridge. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/fbdesignpro/sweetviz">Sweetviz</a></b> (ðŸ¥‰26 Â·  â­ 3.1K Â· ðŸ’€) - Visualize and compare datasets, target values and associations, with.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/AutoViML/AutoViz">AutoViz</a></b> (ðŸ¥‰26 Â·  â­ 1.9K Â· ðŸ’€) - Automatically Visualize any dataset, any size with a single line.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/tpvasconcelos/ridgeplot">ridgeplot</a></b> (ðŸ¥‰26 Â·  â­ 240) - Beautiful ridgeline plots in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/adamerose/PandasGUI">PandasGUI</a></b> (ðŸ¥‰24 Â·  â­ 3.3K) - A GUI for Pandas DataFrames. <code><a href="https://tldrlegal.com/search?q=MIT-0">â—ï¸MIT-0</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/facebookresearch/hiplot">HiPlot</a></b> (ðŸ¥‰24 Â·  â­ 2.8K Â· ðŸ’€) - HiPlot makes understanding high dimensional data easy. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/marcharper/python-ternary">python-ternary</a></b> (ðŸ¥‰24 Â·  â­ 770 Â· ðŸ’€) - Ternary plotting library for python with matplotlib. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/DmitryUlyanov/Multicore-TSNE">Multicore-TSNE</a></b> (ðŸ¥‰23 Â·  â­ 1.9K Â· ðŸ’€) - Parallel t-SNE implementation with Python and Torch.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/PatrikHlobil/Pandas-Bokeh">Pandas-Bokeh</a></b> (ðŸ¥‰22 Â·  â­ 890 Â· ðŸ’€) - Bokeh Plotting Backend for Pandas and GeoPandas. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/nicolaskruchten/jupyter_pivottablejs">pivottablejs</a></b> (ðŸ¥‰21 Â·  â­ 710 Â· ðŸ’€) - Dragndrop Pivot Tables and Charts for Jupyter/IPython.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/leotac/joypy">joypy</a></b> (ðŸ¥‰21 Â·  â­ 600 Â· ðŸ’€) - Joyplots in Python with matplotlib & pandas. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/gyli/PyWaffle">PyWaffle</a></b> (ðŸ¥‰21 Â·  â­ 600 Â· ðŸ’€) - Make Waffle Charts in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/sosuneko/PDPbox">PDPbox</a></b> (ðŸ¥‰20 Â·  â­ 860 Â· ðŸ’€) - python partial dependence plot toolbox. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/t-makaro/animatplot">animatplot</a></b> (ðŸ¥‰18 Â·  â­ 410 Â· ðŸ’€) - A python package for animating plots build on matplotlib. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/beringresearch/ivis">ivis</a></b> (ðŸ¥‰18 Â·  â­ 340 Â· ðŸ’€) - Dimensionality reduction in very large datasets using Siamese.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/altair-viz/pdvega">pdvega</a></b> (ðŸ¥‰16 Â·  â­ 340 Â· ðŸ’€) - Interactive plotting for Pandas using Vega-Lite. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/Zsailer/nx_altair">nx-altair</a></b> (ðŸ¥‰16 Â·  â­ 230 Â· ðŸ’€) - Draw interactive NetworkX graphs with Altair. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/data-describe/data-describe">data-describe</a></b> (ðŸ¥‰15 Â·  â­ 300 Â· ðŸ’€) - datadescribe: Pythonic EDA Accelerator for Data Science. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/biovault/nptsne">nptsne</a></b> (ðŸ¥‰11 Â·  â­ 33 Â· ðŸ’€) - nptsne is a numpy compatible python binary package that offers a.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n</details>\n<br>\n\n## Text Data & NLP\n\n<a href="#contents"><img align="right" width="15" height="15" src="https://git.io/JtehR" alt="Back to top"></a>\n\n_Libraries for processing, cleaning, manipulating, and analyzing text data as well as libraries for NLP tasks such as language detection, fuzzy matching, classification, seq2seq learning, conversational AI, keyword extraction, and translation._\n\n<details><summary><b><a href="https://github.com/huggingface/transformers">transformers</a></b> (ðŸ¥‡54 Â·  â­ 150K) - Transformers: the model-definition framework for.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/huggingface/transformers) (ðŸ‘¨â€ðŸ’» 3.6K Â· ðŸ”€ 31K Â· ðŸ“¦ 400K Â· ðŸ“‹ 19K - 11% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/huggingface/transformers\n	```\n- [PyPi](https://pypi.org/project/transformers) (ðŸ“¥ 93M / month Â· ðŸ“¦ 11K Â· â±ï¸ 14.10.2025):\n	```\n	pip install transformers\n	```\n- [Conda](https://anaconda.org/conda-forge/transformers) (ðŸ“¥ 3.3M Â· â±ï¸ 14.10.2025):\n	```\n	conda install -c conda-forge transformers\n	```\n</details>\n<details><summary><b><a href="https://github.com/nltk/nltk">nltk</a></b> (ðŸ¥‡47 Â·  â­ 14K) - Suite of libraries and programs for symbolic and statistical natural.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/nltk/nltk) (ðŸ‘¨â€ðŸ’» 480 Â· ðŸ”€ 3K Â· ðŸ“¦ 410K Â· ðŸ“‹ 1.9K - 14% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/nltk/nltk\n	```\n- [PyPi](https://pypi.org/project/nltk) (ðŸ“¥ 42M / month Â· ðŸ“¦ 6.3K Â· â±ï¸ 01.10.2025):\n	```\n	pip install nltk\n	```\n- [Conda](https://anaconda.org/conda-forge/nltk) (ðŸ“¥ 3.4M Â· â±ï¸ 01.10.2025):\n	```\n	conda install -c conda-forge nltk\n	```\n</details>\n<details><summary><b><a href="https://github.com/BerriAI/litellm">litellm</a></b> (ðŸ¥‡45 Â·  â­ 30K Â· ðŸ“‰) - Python SDK, Proxy Server (LLM Gateway) to call 100+.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code>o</code> <code>t</code> <code>h</code> <code>e</code> <code>r</code> <code>s</code></summary>\n\n- [GitHub](https://github.com/BerriAI/litellm) (ðŸ‘¨â€ðŸ’» 960 Â· ðŸ”€ 4.5K Â· ðŸ“¥ 800 Â· ðŸ“¦ 17K Â· ðŸ“‹ 7.8K - 17% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/BerriAI/litellm\n	```\n- [PyPi](https://pypi.org/project/litellm) (ðŸ“¥ 34M / month Â· ðŸ“¦ 1.9K Â· â±ï¸ 29.10.2025):\n	```\n	pip install litellm\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/spaCy">spaCy</a></b> (ðŸ¥‡43 Â·  â­ 33K Â· ðŸ“ˆ) - Industrial-strength Natural Language Processing (NLP) in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/explosion/spaCy) (ðŸ‘¨â€ðŸ’» 780 Â· ðŸ”€ 4.5K Â· ðŸ“¥ 4.9K Â· ðŸ“¦ 140K Â· ðŸ“‹ 5.8K - 3% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/explosion/spaCy\n	```\n- [PyPi](https://pypi.org/project/spacy) (ðŸ“¥ 17M / month Â· ðŸ“¦ 3.2K Â· â±ï¸ 23.05.2025):\n	```\n	pip install spacy\n	```\n- [Conda](https://anaconda.org/conda-forge/spacy) (ðŸ“¥ 6.5M Â· â±ï¸ 06.07.2025):\n	```\n	conda install -c conda-forge spacy\n	```\n</details>\n<details><summary><b><a href="https://github.com/huggingface/sentence-transformers">sentence-transformers</a></b> (ðŸ¥‡42 Â·  â­ 18K) - State-of-the-Art Text Embeddings. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/huggingface/sentence-transformers) (ðŸ‘¨â€ðŸ’» 240 Â· ðŸ”€ 2.7K Â· ðŸ“¦ 120K Â· ðŸ“‹ 2.5K - 51% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/UKPLab/sentence-transformers\n	```\n- [PyPi](https://pypi.org/project/sentence-transformers) (ðŸ“¥ 17M / month Â· ðŸ“¦ 3.7K Â· â±ï¸ 22.10.2025):\n	```\n	pip install sentence-transformers\n	```\n- [Conda](https://anaconda.org/conda-forge/sentence-transformers) (ðŸ“¥ 1M Â· â±ï¸ 22.10.2025):\n	```\n	conda install -c conda-forge sentence-transformers\n	```\n</details>\n<details><summary><b><a href="https://github.com/piskvorky/gensim">gensim</a></b> (ðŸ¥‡42 Â·  â­ 16K) - Topic Modelling for Humans. <code><a href="https://tldrlegal.com/search?q=LGPL-2.1">â—ï¸LGPL-2.1</a></code></summary>\n\n- [GitHub](https://github.com/piskvorky/gensim) (ðŸ‘¨â€ðŸ’» 460 Â· ðŸ”€ 4.4K Â· ðŸ“¥ 6.4K Â· ðŸ“¦ 78K Â· ðŸ“‹ 1.9K - 21% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/RaRe-Technologies/gensim\n	```\n- [PyPi](https://pypi.org/project/gensim) (ðŸ“¥ 5.2M / month Â· ðŸ“¦ 1.6K Â· â±ï¸ 18.10.2025):\n	```\n	pip install gensim\n	```\n- [Conda](https://anaconda.org/conda-forge/gensim) (ðŸ“¥ 1.8M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge gensim\n	```\n</details>\n<details><summary><b><a href="https://github.com/google/sentencepiece">sentencepiece</a></b> (ðŸ¥‡42 Â·  â­ 11K) - Unsupervised text tokenizer for Neural Network-based text.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/google/sentencepiece) (ðŸ‘¨â€ðŸ’» 100 Â· ðŸ”€ 1.3K Â· ðŸ“¥ 110K Â· ðŸ“¦ 120K Â· ðŸ“‹ 800 - 3% open Â· â±ï¸ 04.10.2025):\n\n	```\n	git clone https://github.com/google/sentencepiece\n	```\n- [PyPi](https://pypi.org/project/sentencepiece) (ðŸ“¥ 31M / month Â· ðŸ“¦ 2.4K Â· â±ï¸ 12.08.2025):\n	```\n	pip install sentencepiece\n	```\n- [Conda](https://anaconda.org/conda-forge/sentencepiece) (ðŸ“¥ 1.7M Â· â±ï¸ 22.09.2025):\n	```\n	conda install -c conda-forge sentencepiece\n	```\n</details>\n<details><summary><b><a href="https://github.com/huggingface/tokenizers">Tokenizers</a></b> (ðŸ¥‡40 Â·  â­ 10K) - Fast State-of-the-Art Tokenizers optimized for Research and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/huggingface/tokenizers) (ðŸ‘¨â€ðŸ’» 130 Â· ðŸ”€ 970 Â· ðŸ“¥ 86 Â· ðŸ“¦ 180K Â· ðŸ“‹ 1.1K - 9% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/huggingface/tokenizers\n	```\n- [PyPi](https://pypi.org/project/tokenizers) (ðŸ“¥ 81M / month Â· ðŸ“¦ 1.7K Â· â±ï¸ 19.09.2025):\n	```\n	pip install tokenizers\n	```\n- [Conda](https://anaconda.org/conda-forge/tokenizers) (ðŸ“¥ 3.6M Â· â±ï¸ 19.09.2025):\n	```\n	conda install -c conda-forge tokenizers\n	```\n</details>\n<details><summary><b><a href="https://github.com/NVIDIA-NeMo/NeMo">NeMo</a></b> (ðŸ¥‡38 Â·  â­ 16K) - A scalable generative AI framework built for researchers and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/NVIDIA-NeMo/NeMo) (ðŸ‘¨â€ðŸ’» 460 Â· ðŸ”€ 3.2K Â· ðŸ“¥ 520K Â· ðŸ“¦ 21 Â· ðŸ“‹ 2.8K - 4% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/NVIDIA/NeMo\n	```\n- [PyPi](https://pypi.org/project/nemo-toolkit) (ðŸ“¥ 810K / month Â· ðŸ“¦ 18 Â· â±ï¸ 27.10.2025):\n	```\n	pip install nemo-toolkit\n	```\n</details>\n<details><summary><b><a href="https://github.com/deepset-ai/haystack">haystack</a></b> (ðŸ¥‡37 Â·  â­ 23K) - AI orchestration framework to build customizable, production-ready.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/deepset-ai/haystack) (ðŸ‘¨â€ðŸ’» 310 Â· ðŸ”€ 2.5K Â· ðŸ“¦ 1.3K Â· ðŸ“‹ 4.1K - 2% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/deepset-ai/haystack\n	```\n- [PyPi](https://pypi.org/project/haystack) (ðŸ“¥ 7.4K / month Â· ðŸ“¦ 5 Â· â±ï¸ 15.12.2021):\n	```\n	pip install haystack\n	```\n</details>\n<details><summary><b><a href="https://github.com/comet-ml/opik">Opik</a></b> (ðŸ¥‡37 Â·  â­ 15K) - Debug, evaluate, and monitor your LLM applications, RAG systems, and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/comet-ml/opik) (ðŸ‘¨â€ðŸ’» 81 Â· ðŸ”€ 1.1K Â· ðŸ“¦ 17 Â· ðŸ“‹ 540 - 29% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/comet-ml/opik\n	```\n- [PyPi](https://pypi.org/project/opik) (ðŸ“¥ 850K / month Â· ðŸ“¦ 34 Â· â±ï¸ 29.10.2025):\n	```\n	pip install opik\n	```\n</details>\n<details><summary><b><a href="https://github.com/gunthercox/ChatterBot">ChatterBot</a></b> (ðŸ¥‡37 Â·  â­ 14K) - ChatterBot is a machine learning, conversational dialog engine for.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/gunthercox/ChatterBot) (ðŸ‘¨â€ðŸ’» 110 Â· ðŸ”€ 4.5K Â· ðŸ“¦ 6.5K Â· ðŸ“‹ 1.7K - 6% open Â· â±ï¸ 25.10.2025):\n\n	```\n	git clone https://github.com/gunthercox/ChatterBot\n	```\n- [PyPi](https://pypi.org/project/chatterbot) (ðŸ“¥ 20K / month Â· ðŸ“¦ 19 Â· â±ï¸ 16.10.2025):\n	```\n	pip install chatterbot\n	```\n</details>\n<details><summary><b><a href="https://github.com/flairNLP/flair">flair</a></b> (ðŸ¥‡37 Â·  â­ 14K) - A very simple framework for state-of-the-art Natural Language Processing.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/flairNLP/flair) (ðŸ‘¨â€ðŸ’» 280 Â· ðŸ”€ 2.1K Â· ðŸ“¦ 4.1K Â· ðŸ“‹ 2.4K - 1% open Â· â±ï¸ 12.06.2025):\n\n	```\n	git clone https://github.com/flairNLP/flair\n	```\n- [PyPi](https://pypi.org/project/flair) (ðŸ“¥ 180K / month Â· ðŸ“¦ 160 Â· â±ï¸ 05.02.2025):\n	```\n	pip install flair\n	```\n- [Conda](https://anaconda.org/conda-forge/python-flair) (ðŸ“¥ 49K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge python-flair\n	```\n</details>\n<details><summary><b><a href="https://github.com/sloria/TextBlob">TextBlob</a></b> (ðŸ¥‡37 Â·  â­ 9.5K) - Simple, Pythonic, text processing--Sentiment analysis, part-of-speech.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/sloria/TextBlob) (ðŸ‘¨â€ðŸ’» 37 Â· ðŸ”€ 1.2K Â· ðŸ“¥ 140 Â· ðŸ“¦ 60K Â· ðŸ“‹ 280 - 25% open Â· â±ï¸ 18.10.2025):\n\n	```\n	git clone https://github.com/sloria/TextBlob\n	```\n- [PyPi](https://pypi.org/project/textblob) (ðŸ“¥ 1.5M / month Â· ðŸ“¦ 400 Â· â±ï¸ 13.01.2025):\n	```\n	pip install textblob\n	```\n- [Conda](https://anaconda.org/conda-forge/textblob) (ðŸ“¥ 340K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge textblob\n	```\n</details>\n<details><summary><b><a href="https://github.com/facebookresearch/fairseq">fairseq</a></b> (ðŸ¥ˆ36 Â·  â­ 32K) - Facebook AI Research Sequence-to-Sequence Toolkit written in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/facebookresearch/fairseq) (ðŸ‘¨â€ðŸ’» 430 Â· ðŸ”€ 6.6K Â· ðŸ“¥ 440 Â· ðŸ“¦ 4.4K Â· ðŸ“‹ 4.4K - 30% open Â· â±ï¸ 30.09.2025):\n\n	```\n	git clone https://github.com/facebookresearch/fairseq\n	```\n- [PyPi](https://pypi.org/project/fairseq) (ðŸ“¥ 77K / month Â· ðŸ“¦ 120 Â· â±ï¸ 27.06.2022):\n	```\n	pip install fairseq\n	```\n- [Conda](https://anaconda.org/conda-forge/fairseq) (ðŸ“¥ 170K Â· â±ï¸ 02.10.2025):\n	```\n	conda install -c conda-forge fairseq\n	```\n</details>\n<details><summary><b><a href="https://github.com/stanfordnlp/stanza">stanza</a></b> (ðŸ¥ˆ36 Â·  â­ 7.6K) - Stanford NLP Python library for tokenization, sentence segmentation,.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/stanfordnlp/stanza) (ðŸ‘¨â€ðŸ’» 72 Â· ðŸ”€ 920 Â· ðŸ“¦ 4.1K Â· ðŸ“‹ 950 - 10% open Â· â±ï¸ 05.10.2025):\n\n	```\n	git clone https://github.com/stanfordnlp/stanza\n	```\n- [PyPi](https://pypi.org/project/stanza) (ðŸ“¥ 770K / month Â· ðŸ“¦ 240 Â· â±ï¸ 05.10.2025):\n	```\n	pip install stanza\n	```\n- [Conda](https://anaconda.org/stanfordnlp/stanza) (ðŸ“¥ 9K Â· â±ï¸ 25.03.2025):\n	```\n	conda install -c stanfordnlp stanza\n	```\n</details>\n<details><summary><b><a href="https://github.com/qdrant/qdrant">qdrant</a></b> (ðŸ¥ˆ35 Â·  â­ 27K) - Qdrant - High-performance, massive-scale Vector Database and Vector.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/qdrant/qdrant) (ðŸ‘¨â€ðŸ’» 140 Â· ðŸ”€ 1.9K Â· ðŸ“¥ 500K Â· ðŸ“¦ 120 Â· ðŸ“‹ 1.6K - 22% open Â· â±ï¸ 30.09.2025):\n\n	```\n	git clone https://github.com/qdrant/qdrant\n	```\n</details>\n<details><summary><b><a href="https://github.com/JohnSnowLabs/spark-nlp">spark-nlp</a></b> (ðŸ¥ˆ35 Â·  â­ 4.1K) - State of the Art Natural Language Processing. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/JohnSnowLabs/spark-nlp) (ðŸ‘¨â€ðŸ’» 120 Â· ðŸ”€ 730 Â· ðŸ“¦ 620 Â· ðŸ“‹ 910 - 2% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/JohnSnowLabs/spark-nlp\n	```\n- [PyPi](https://pypi.org/project/spark-nlp) (ðŸ“¥ 1M / month Â· ðŸ“¦ 39 Â· â±ï¸ 22.10.2025):\n	```\n	pip install spark-nlp\n	```\n</details>\n<details><summary><b><a href="https://github.com/RasaHQ/rasa">Rasa</a></b> (ðŸ¥ˆ34 Â·  â­ 21K) - Open source machine learning framework to automate text- and voice-.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/RasaHQ/rasa) (ðŸ‘¨â€ðŸ’» 600 Â· ðŸ”€ 4.9K Â· ðŸ“‹ 6.8K - 2% open Â· â±ï¸ 26.08.2025):\n\n	```\n	git clone https://github.com/RasaHQ/rasa\n	```\n- [PyPi](https://pypi.org/project/rasa) (ðŸ“¥ 110K / month Â· ðŸ“¦ 60 Â· â±ï¸ 14.01.2025):\n	```\n	pip install rasa\n	```\n</details>\n<details><summary><b><a href="https://github.com/tensorflow/text">TensorFlow Text</a></b> (ðŸ¥ˆ34 Â·  â­ 1.3K) - Making text a first-class citizen in TensorFlow. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/tensorflow/text) (ðŸ‘¨â€ðŸ’» 190 Â· ðŸ”€ 360 Â· ðŸ“¦ 10K Â· ðŸ“‹ 370 - 53% open Â· â±ï¸ 18.08.2025):\n\n	```\n	git clone https://github.com/tensorflow/text\n	```\n- [PyPi](https://pypi.org/project/tensorflow-text) (ðŸ“¥ 6.8M / month Â· ðŸ“¦ 230 Â· â±ï¸ 04.04.2025):\n	```\n	pip install tensorflow-text\n	```\n</details>\n<details><summary><b><a href="https://github.com/snowballstem/snowball">snowballstemmer</a></b> (ðŸ¥ˆ34 Â·  â­ 810) - Snowball compiler and stemming algorithms. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/snowballstem/snowball) (ðŸ‘¨â€ðŸ’» 41 Â· ðŸ”€ 190 Â· ðŸ“¦ 11 Â· ðŸ“‹ 120 - 17% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/snowballstem/snowball\n	```\n- [PyPi](https://pypi.org/project/snowballstemmer) (ðŸ“¥ 24M / month Â· ðŸ“¦ 550 Â· â±ï¸ 09.05.2025):\n	```\n	pip install snowballstemmer\n	```\n- [Conda](https://anaconda.org/conda-forge/snowballstemmer) (ðŸ“¥ 11M Â· â±ï¸ 20.05.2025):\n	```\n	conda install -c conda-forge snowballstemmer\n	```\n</details>\n<details><summary><b><a href="https://github.com/pytorch/text">torchtext</a></b> (ðŸ¥ˆ32 Â·  â­ 3.6K) - Models, data loaders and abstractions for language processing,.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pytorch/text) (ðŸ‘¨â€ðŸ’» 160 Â· ðŸ”€ 810 Â· ðŸ“‹ 850 - 38% open Â· â±ï¸ 10.09.2025):\n\n	```\n	git clone https://github.com/pytorch/text\n	```\n- [PyPi](https://pypi.org/project/torchtext) (ðŸ“¥ 730K / month Â· ðŸ“¦ 280 Â· â±ï¸ 24.04.2024):\n	```\n	pip install torchtext\n	```\n</details>\n<details><summary><b><a href="https://github.com/jamesturk/jellyfish">jellyfish</a></b> (ðŸ¥ˆ32 Â·  â­ 2.2K) - a python library for doing approximate and phonetic matching of strings. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/jamesturk/jellyfish) (ðŸ‘¨â€ðŸ’» 37 Â· ðŸ”€ 160 Â· ðŸ“¦ 15K Â· â±ï¸ 11.10.2025):\n\n	```\n	git clone https://github.com/jamesturk/jellyfish\n	```\n- [PyPi](https://pypi.org/project/jellyfish) (ðŸ“¥ 8.6M / month Â· ðŸ“¦ 320 Â· â±ï¸ 11.10.2025):\n	```\n	pip install jellyfish\n	```\n- [Conda](https://anaconda.org/conda-forge/jellyfish) (ðŸ“¥ 1.7M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge jellyfish\n	```\n</details>\n<details><summary><b><a href="https://github.com/deeppavlov/DeepPavlov">DeepPavlov</a></b> (ðŸ¥ˆ31 Â·  â­ 6.9K Â· ðŸ’¤) - An open source library for deep learning end-to-end.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/deeppavlov/DeepPavlov) (ðŸ‘¨â€ðŸ’» 78 Â· ðŸ”€ 1.2K Â· ðŸ“¦ 440 Â· ðŸ“‹ 640 - 4% open Â· â±ï¸ 26.11.2024):\n\n	```\n	git clone https://github.com/deepmipt/DeepPavlov\n	```\n- [PyPi](https://pypi.org/project/deeppavlov) (ðŸ“¥ 11K / month Â· ðŸ“¦ 4 Â· â±ï¸ 12.08.2024):\n	```\n	pip install deeppavlov\n	```\n</details>\n<details><summary><b><a href="https://github.com/rspeer/python-ftfy">ftfy</a></b> (ðŸ¥ˆ31 Â·  â­ 4K Â· ðŸ’¤) - Fixes mojibake and other glitches in Unicode text, after the fact. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/rspeer/python-ftfy) (ðŸ‘¨â€ðŸ’» 22 Â· ðŸ”€ 120 Â· ðŸ“¥ 100 Â· ðŸ“¦ 33K Â· ðŸ“‹ 150 - 7% open Â· â±ï¸ 30.10.2024):\n\n	```\n	git clone https://github.com/rspeer/python-ftfy\n	```\n- [PyPi](https://pypi.org/project/ftfy) (ðŸ“¥ 11M / month Â· ðŸ“¦ 570 Â· â±ï¸ 26.10.2024):\n	```\n	pip install ftfy\n	```\n- [Conda](https://anaconda.org/conda-forge/ftfy) (ðŸ“¥ 380K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge ftfy\n	```\n</details>\n<details><summary><b><a href="https://github.com/allenai/scispacy">SciSpacy</a></b> (ðŸ¥ˆ31 Â·  â­ 1.9K) - A full spaCy pipeline and models for scientific/biomedical documents. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/allenai/scispacy) (ðŸ‘¨â€ðŸ’» 38 Â· ðŸ”€ 240 Â· ðŸ“¦ 1.3K Â· ðŸ“‹ 330 - 11% open Â· â±ï¸ 01.10.2025):\n\n	```\n	git clone https://github.com/allenai/scispacy\n	```\n- [PyPi](https://pypi.org/project/scispacy) (ðŸ“¥ 42K / month Â· ðŸ“¦ 50 Â· â±ï¸ 01.10.2025):\n	```\n	pip install scispacy\n	```\n</details>\n<details><summary><b><a href="https://github.com/cltk/cltk">CLTK</a></b> (ðŸ¥ˆ31 Â·  â­ 870 Â· ðŸ“‰) - The Classical Language Toolkit. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/cltk/cltk) (ðŸ‘¨â€ðŸ’» 120 Â· ðŸ”€ 340 Â· ðŸ“¥ 160 Â· ðŸ“¦ 300 Â· ðŸ“‹ 580 - 0% open Â· â±ï¸ 21.10.2025):\n\n	```\n	git clone https://github.com/cltk/cltk\n	```\n- [PyPi](https://pypi.org/project/cltk) (ðŸ“¥ 14K / month Â· ðŸ“¦ 17 Â· â±ï¸ 21.10.2025):\n	```\n	pip install cltk\n	```\n</details>\n<details><summary><b><a href="https://github.com/dwyl/english-words">english-words</a></b> (ðŸ¥ˆ29 Â·  â­ 12K Â· ðŸ’¤) - A text file containing 479k English words for all your.. <code><a href="http://bit.ly/3rvuUlR">Unlicense</a></code></summary>\n\n- [GitHub](https://github.com/dwyl/english-words) (ðŸ‘¨â€ðŸ’» 34 Â· ðŸ”€ 2K Â· ðŸ“¦ 2 Â· ðŸ“‹ 170 - 75% open Â· â±ï¸ 06.01.2025):\n\n	```\n	git clone https://github.com/dwyl/english-words\n	```\n- [PyPi](https://pypi.org/project/english-words) (ðŸ“¥ 78K / month Â· ðŸ“¦ 15 Â· â±ï¸ 14.08.2025):\n	```\n	pip install english-words\n	```\n</details>\n<details><summary><b><a href="https://github.com/argilla-io/argilla">rubrix</a></b> (ðŸ¥ˆ29 Â·  â­ 4.7K) - Argilla is a collaboration tool for AI engineers and domain experts.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/argilla-io/argilla) (ðŸ‘¨â€ðŸ’» 110 Â· ðŸ”€ 460 Â· ðŸ“¦ 3.1K Â· ðŸ“‹ 2.2K - 0% open Â· â±ï¸ 05.08.2025):\n\n	```\n	git clone https://github.com/recognai/rubrix\n	```\n- [PyPi](https://pypi.org/project/rubrix) (ðŸ“¥ 1.2K / month Â· â±ï¸ 24.10.2022):\n	```\n	pip install rubrix\n	```\n- [Conda](https://anaconda.org/conda-forge/rubrix) (ðŸ“¥ 52K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge rubrix\n	```\n</details>\n<details><summary><b><a href="https://github.com/dedupeio/dedupe">Dedupe</a></b> (ðŸ¥ˆ29 Â·  â­ 4.4K Â· ðŸ“ˆ) - A python library for accurate and scalable fuzzy matching, record.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/dedupeio/dedupe) (ðŸ‘¨â€ðŸ’» 72 Â· ðŸ”€ 560 Â· ðŸ“¦ 370 Â· ðŸ“‹ 820 - 9% open Â· â±ï¸ 29.07.2025):\n\n	```\n	git clone https://github.com/dedupeio/dedupe\n	```\n- [PyPi](https://pypi.org/project/dedupe) (ðŸ“¥ 59K / month Â· ðŸ“¦ 19 Â· â±ï¸ 15.08.2024):\n	```\n	pip install dedupe\n	```\n- [Conda](https://anaconda.org/conda-forge/dedupe) (ðŸ“¥ 130K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge dedupe\n	```\n</details>\n<details><summary><b><a href="https://github.com/life4/textdistance">TextDistance</a></b> (ðŸ¥ˆ28 Â·  â­ 3.5K) - Compute distance between sequences. 30+ algorithms, pure python.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/life4/textdistance) (ðŸ‘¨â€ðŸ’» 18 Â· ðŸ”€ 260 Â· ðŸ“¥ 1.1K Â· ðŸ“¦ 8.8K Â· â±ï¸ 18.04.2025):\n\n	```\n	git clone https://github.com/life4/textdistance\n	```\n- [PyPi](https://pypi.org/project/textdistance) (ðŸ“¥ 1.3M / month Â· ðŸ“¦ 99 Â· â±ï¸ 16.07.2024):\n	```\n	pip install textdistance\n	```\n- [Conda](https://anaconda.org/conda-forge/textdistance) (ðŸ“¥ 970K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge textdistance\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/spacy-transformers">spacy-transformers</a></b> (ðŸ¥ˆ28 Â·  â­ 1.4K) - Use pretrained transformers like BERT, XLNet and GPT-2.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code>spacy</code></summary>\n\n- [GitHub](https://github.com/explosion/spacy-transformers) (ðŸ‘¨â€ðŸ’» 23 Â· ðŸ”€ 170 Â· ðŸ“¥ 610 Â· ðŸ“¦ 2.4K Â· â±ï¸ 26.05.2025):\n\n	```\n	git clone https://github.com/explosion/spacy-transformers\n	```\n- [PyPi](https://pypi.org/project/spacy-transformers) (ðŸ“¥ 270K / month Â· ðŸ“¦ 110 Â· â±ï¸ 26.05.2025):\n	```\n	pip install spacy-transformers\n	```\n- [Conda](https://anaconda.org/conda-forge/spacy-transformers) (ðŸ“¥ 140K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge spacy-transformers\n	```\n</details>\n<details><summary><b><a href="https://github.com/unitaryai/detoxify">detoxify</a></b> (ðŸ¥‰26 Â·  â­ 1.1K) - Trained models & code to predict toxic comments on all 3 Jigsaw.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/unitaryai/detoxify) (ðŸ‘¨â€ðŸ’» 14 Â· ðŸ”€ 130 Â· ðŸ“¥ 1.9M Â· ðŸ“¦ 980 Â· ðŸ“‹ 67 - 55% open Â· â±ï¸ 29.07.2025):\n\n	```\n	git clone https://github.com/unitaryai/detoxify\n	```\n- [PyPi](https://pypi.org/project/detoxify) (ðŸ“¥ 140K / month Â· ðŸ“¦ 30 Â· â±ï¸ 01.02.2024):\n	```\n	pip install detoxify\n	```\n</details>\n<details><summary><b><a href="https://github.com/JasonKessler/scattertext">scattertext</a></b> (ðŸ¥‰25 Â·  â­ 2.3K) - Beautiful visualizations of how language differs among document.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/JasonKessler/scattertext) (ðŸ‘¨â€ðŸ’» 14 Â· ðŸ”€ 290 Â· ðŸ“¦ 670 Â· ðŸ“‹ 100 - 22% open Â· â±ï¸ 29.04.2025):\n\n	```\n	git clone https://github.com/JasonKessler/scattertext\n	```\n- [PyPi](https://pypi.org/project/scattertext) (ðŸ“¥ 7.5K / month Â· ðŸ“¦ 5 Â· â±ï¸ 23.09.2024):\n	```\n	pip install scattertext\n	```\n- [Conda](https://anaconda.org/conda-forge/scattertext) (ðŸ“¥ 140K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge scattertext\n	```\n</details>\n<details><summary><b><a href="https://github.com/google-research/text-to-text-transfer-transformer">T5</a></b> (ðŸ¥‰24 Â·  â­ 6.4K) - Code for the paper Exploring the Limits of Transfer Learning with a.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) (ðŸ‘¨â€ðŸ’» 61 Â· ðŸ”€ 780 Â· ðŸ“‹ 450 - 23% open Â· â±ï¸ 28.04.2025):\n\n	```\n	git clone https://github.com/google-research/text-to-text-transfer-transformer\n	```\n- [PyPi](https://pypi.org/project/t5) (ðŸ“¥ 83K / month Â· ðŸ“¦ 2 Â· â±ï¸ 18.10.2021):\n	```\n	pip install t5\n	```\n</details>\n<details><summary><b><a href="https://github.com/zjunlp/DeepKE">DeepKE</a></b> (ðŸ¥‰24 Â·  â­ 4.2K) - [EMNLP 2022] An Open Toolkit for Knowledge Graph Extraction and.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/zjunlp/DeepKE) (ðŸ‘¨â€ðŸ’» 34 Â· ðŸ”€ 730 Â· ðŸ“¦ 25 Â· â±ï¸ 19.07.2025):\n\n	```\n	git clone https://github.com/zjunlp/deepke\n	```\n- [PyPi](https://pypi.org/project/deepke) (ðŸ“¥ 950 / month Â· â±ï¸ 21.09.2023):\n	```\n	pip install deepke\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/sense2vec">sense2vec</a></b> (ðŸ¥‰24 Â·  â­ 1.7K) - Contextually-keyed word vectors. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/explosion/sense2vec) (ðŸ‘¨â€ðŸ’» 20 Â· ðŸ”€ 240 Â· ðŸ“¥ 73K Â· ðŸ“¦ 470 Â· ðŸ“‹ 120 - 20% open Â· â±ï¸ 23.04.2025):\n\n	```\n	git clone https://github.com/explosion/sense2vec\n	```\n- [PyPi](https://pypi.org/project/sense2vec) (ðŸ“¥ 3.4K / month Â· ðŸ“¦ 13 Â· â±ï¸ 19.04.2021):\n	```\n	pip install sense2vec\n	```\n- [Conda](https://anaconda.org/conda-forge/sense2vec) (ðŸ“¥ 67K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge sense2vec\n	```\n</details>\n<details><summary><b><a href="https://github.com/IndicoDataSolutions/finetune">finetune</a></b> (ðŸ¥‰23 Â·  â­ 720) - Scikit-learn style model finetuning for NLP. <code><a href="http://bit.ly/3postzC">MPL-2.0</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/IndicoDataSolutions/finetune) (ðŸ‘¨â€ðŸ’» 24 Â· ðŸ”€ 79 Â· ðŸ“¦ 16 Â· ðŸ“‹ 190 - 39% open Â· â±ï¸ 21.10.2025):\n\n	```\n	git clone https://github.com/IndicoDataSolutions/finetune\n	```\n- [PyPi](https://pypi.org/project/finetune) (ðŸ“¥ 2.7K / month Â· ðŸ“¦ 2 Â· â±ï¸ 29.09.2023):\n	```\n	pip install finetune\n	```\n</details>\n<details><summary><b><a href="https://github.com/EricFillion/happy-transformer">happy-transformer</a></b> (ðŸ¥‰23 Â·  â­ 540 Â· ðŸ’¤) - Happy Transformer makes it easy to fine-tune and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code>huggingface</code></summary>\n\n- [GitHub](https://github.com/EricFillion/happy-transformer) (ðŸ‘¨â€ðŸ’» 14 Â· ðŸ”€ 69 Â· ðŸ“¦ 330 Â· ðŸ“‹ 130 - 16% open Â· â±ï¸ 22.03.2025):\n\n	```\n	git clone https://github.com/EricFillion/happy-transformer\n	```\n- [PyPi](https://pypi.org/project/happytransformer) (ðŸ“¥ 2.7K / month Â· ðŸ“¦ 5 Â· â±ï¸ 05.08.2023):\n	```\n	pip install happytransformer\n	```\n</details>\n<details><summary><b><a href="https://github.com/awslabs/sockeye">Sockeye</a></b> (ðŸ¥‰21 Â·  â­ 1.2K Â· ðŸ’¤) - Sequence-to-sequence framework with a focus on Neural.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1X" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/awslabs/sockeye) (ðŸ‘¨â€ðŸ’» 60 Â· ðŸ”€ 320 Â· ðŸ“¥ 21 Â· ðŸ“‹ 310 - 3% open Â· â±ï¸ 24.10.2024):\n\n	```\n	git clone https://github.com/awslabs/sockeye\n	```\n- [PyPi](https://pypi.org/project/sockeye) (ðŸ“¥ 580 / month Â· â±ï¸ 03.03.2023):\n	```\n	pip install sockeye\n	```\n</details>\n<details><summary><b><a href="https://github.com/unum-cloud/UForm">UForm</a></b> (ðŸ¥‰21 Â·  â­ 1.2K) - Pocket-Sized Multimodal AI for content understanding and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/unum-cloud/UForm) (ðŸ‘¨â€ðŸ’» 21 Â· ðŸ”€ 76 Â· ðŸ“¥ 710 Â· ðŸ“¦ 36 Â· ðŸ“‹ 39 - 38% open Â· â±ï¸ 03.09.2025):\n\n	```\n	git clone https://github.com/unum-cloud/uform\n	```\n- [PyPi](https://pypi.org/project/uform) (ðŸ“¥ 490 / month Â· ðŸ“¦ 2 Â· â±ï¸ 03.09.2025):\n	```\n	pip install uform\n	```\n</details>\n<details><summary><b><a href="https://github.com/webis-de/small-text">small-text</a></b> (ðŸ¥‰20 Â·  â­ 630) - Active Learning for Text Classification in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/webis-de/small-text) (ðŸ‘¨â€ðŸ’» 10 Â· ðŸ”€ 76 Â· ðŸ“¦ 34 Â· ðŸ“‹ 74 - 28% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/webis-de/small-text\n	```\n- [PyPi](https://pypi.org/project/small-text) (ðŸ“¥ 390 / month Â· â±ï¸ 17.08.2025):\n	```\n	pip install small-text\n	```\n- [Conda](https://anaconda.org/conda-forge/small-text) (ðŸ“¥ 19K Â· â±ï¸ 17.08.2025):\n	```\n	conda install -c conda-forge small-text\n	```\n</details>\n<details><summary><b><a href="https://github.com/dsfsi/textaugment">textaugment</a></b> (ðŸ¥‰19 Â·  â­ 430) - TextAugment: Text Augmentation Library. <code><a href="http://bit.ly/34MBw', '{"language":null,"stars":22916,"forks":3046,"watchers":22916,"open_issues":33,"topics":["automl","chatgpt","data-analysis","data-science","data-visualization","data-visualizations","deep-learning","gpt","gpt-3","jax","keras","machine-learning","ml","nlp","python","pytorch","scikit-learn","tensorflow","transformer"],"default_branch":"main","size_kb":21391,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ml-tooling:best-of\"","source_url":"https://github.com/ml-tooling/best-of\""},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:best-of-lists:best-of","source_url":"https://github.com/best-of-lists/best-of"},{"type":"has_code","target_id":"github:tensorflow:tensorflow\">Tensorflow<","source_url":"https://github.com/tensorflow/tensorflow\">Tensorflow<"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:pytorch:pytorch\">PyTorch<","source_url":"https://github.com/pytorch/pytorch\">PyTorch<"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn\">scikit-learn<","source_url":"https://github.com/scikit-learn/scikit-learn\">scikit-learn<"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:keras-team:keras\">Keras<","source_url":"https://github.com/keras-team/keras\">Keras<"},{"type":"has_code","target_id":"github:keras-team:keras","source_url":"https://github.com/keras-team/keras"},{"type":"has_code","target_id":"github:keras-team:keras","source_url":"https://github.com/keras-team/keras"},{"type":"has_code","target_id":"github:dmlc:xgboost\">XGBoost<","source_url":"https://github.com/dmlc/xgboost\">XGBoost<"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle\">PaddlePaddle<","source_url":"https://github.com/PaddlePaddle/Paddle\">PaddlePaddle<"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:jax-ml:jax\">jax<","source_url":"https://github.com/jax-ml/jax\">jax<"},{"type":"has_code","target_id":"github:jax-ml:jax","source_url":"https://github.com/jax-ml/jax"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning\">pytorch-lightning<","source_url":"https://github.com/Lightning-AI/pytorch-lightning\">pytorch-lightning<"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning","source_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:statsmodels:statsmodels\">StatsModels<","source_url":"https://github.com/statsmodels/statsmodels\">StatsModels<"},{"type":"has_code","target_id":"github:statsmodels:statsmodels","source_url":"https://github.com/statsmodels/statsmodels"},{"type":"has_code","target_id":"github:statsmodels:statsmodels","source_url":"https://github.com/statsmodels/statsmodels"},{"type":"has_code","target_id":"github:apache:spark\">PySpark<","source_url":"https://github.com/apache/spark\">PySpark<"},{"type":"has_code","target_id":"github:apache:spark","source_url":"https://github.com/apache/spark"},{"type":"has_code","target_id":"github:apache:spark","source_url":"https://github.com/apache/spark"},{"type":"has_code","target_id":"github:microsoft:LightGBM\">LightGBM<","source_url":"https://github.com/microsoft/LightGBM\">LightGBM<"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:catboost:catboost\">Catboost<","source_url":"https://github.com/catboost/catboost\">Catboost<"},{"type":"has_code","target_id":"github:catboost:catboost","source_url":"https://github.com/catboost/catboost"},{"type":"has_code","target_id":"github:catboost:catboost","source_url":"https://github.com/catboost/catboost"},{"type":"has_code","target_id":"github:fastai:fastai\">Fastai<","source_url":"https://github.com/fastai/fastai\">Fastai<"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:apache:flink\">PyFlink<","source_url":"https://github.com/apache/flink\">PyFlink<"},{"type":"has_code","target_id":"github:apache:flink","source_url":"https://github.com/apache/flink"},{"type":"has_code","target_id":"github:apache:flink","source_url":"https://github.com/apache/flink"},{"type":"has_code","target_id":"github:google:flax\">Flax<","source_url":"https://github.com/google/flax\">Flax<"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:pytorch:ignite\">Ignite<","source_url":"https://github.com/pytorch/ignite\">Ignite<"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:arogozhnikov:einops\">einops<","source_url":"https://github.com/arogozhnikov/einops\">einops<"},{"type":"has_code","target_id":"github:arogozhnikov:einops","source_url":"https://github.com/arogozhnikov/einops"},{"type":"has_code","target_id":"github:arogozhnikov:einops","source_url":"https://github.com/arogozhnikov/einops"},{"type":"has_code","target_id":"github:ivy-llc:ivy\">ivy<","source_url":"https://github.com/ivy-llc/ivy\">ivy<"},{"type":"has_code","target_id":"github:ivy-llc:ivy","source_url":"https://github.com/ivy-llc/ivy"},{"type":"has_code","target_id":"github:unifyai:ivy","source_url":"https://github.com/unifyai/ivy"},{"type":"has_code","target_id":"github:jina-ai:serve\">Jina<","source_url":"https://github.com/jina-ai/serve\">Jina<"},{"type":"has_code","target_id":"github:jina-ai:serve","source_url":"https://github.com/jina-ai/serve"},{"type":"has_code","target_id":"github:jina-ai:jina","source_url":"https://github.com/jina-ai/jina"},{"type":"has_code","target_id":"github:mlpack:mlpack\">mlpack<","source_url":"https://github.com/mlpack/mlpack\">mlpack<"},{"type":"has_code","target_id":"github:mlpack:mlpack","source_url":"https://github.com/mlpack/mlpack"},{"type":"has_code","target_id":"github:mlpack:mlpack","source_url":"https://github.com/mlpack/mlpack"},{"type":"has_code","target_id":"github:explosion:thinc\">Thinc<","source_url":"https://github.com/explosion/thinc\">Thinc<"},{"type":"has_code","target_id":"github:explosion:thinc","source_url":"https://github.com/explosion/thinc"},{"type":"has_code","target_id":"github:explosion:thinc","source_url":"https://github.com/explosion/thinc"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig\">Ludwig<","source_url":"https://github.com/ludwig-ai/ludwig\">Ludwig<"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig","source_url":"https://github.com/ludwig-ai/ludwig"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig","source_url":"https://github.com/ludwig-ai/ludwig"},{"type":"has_code","target_id":"github:skorch-dev:skorch\">skorch<","source_url":"https://github.com/skorch-dev/skorch\">skorch<"},{"type":"has_code","target_id":"github:skorch-dev:skorch","source_url":"https://github.com/skorch-dev/skorch"},{"type":"has_code","target_id":"github:skorch-dev:skorch","source_url":"https://github.com/skorch-dev/skorch"},{"type":"has_code","target_id":"github:google-deepmind:sonnet\">Sonnet<","source_url":"https://github.com/google-deepmind/sonnet\">Sonnet<"},{"type":"has_code","target_id":"github:google-deepmind:sonnet","source_url":"https://github.com/google-deepmind/sonnet"},{"type":"has_code","target_id":"github:deepmind:sonnet","source_url":"https://github.com/deepmind/sonnet"},{"type":"has_code","target_id":"github:google-deepmind:dm-haiku\">Haiku<","source_url":"https://github.com/google-deepmind/dm-haiku\">Haiku<"},{"type":"has_code","target_id":"github:google-deepmind:dm-haiku","source_url":"https://github.com/google-deepmind/dm-haiku"},{"type":"has_code","target_id":"github:deepmind:dm-haiku","source_url":"https://github.com/deepmind/dm-haiku"},{"type":"has_code","target_id":"github:ROCm:tensorflow-upstream\">tensorflow-upstream<","source_url":"https://github.com/ROCm/tensorflow-upstream\">tensorflow-upstream<"},{"type":"has_code","target_id":"github:ROCm:tensorflow-upstream","source_url":"https://github.com/ROCm/tensorflow-upstream"},{"type":"has_code","target_id":"github:ROCmSoftwarePlatform:tensorflow-upstream","source_url":"https://github.com/ROCmSoftwarePlatform/tensorflow-upstream"},{"type":"has_code","target_id":"github:geomstats:geomstats\">Geomstats<","source_url":"https://github.com/geomstats/geomstats\">Geomstats<"},{"type":"has_code","target_id":"github:geomstats:geomstats","source_url":"https://github.com/geomstats/geomstats"},{"type":"has_code","target_id":"github:geomstats:geomstats","source_url":"https://github.com/geomstats/geomstats"},{"type":"has_code","target_id":"github:pyRiemann:pyRiemann\">pyRiemann<","source_url":"https://github.com/pyRiemann/pyRiemann\">pyRiemann<"},{"type":"has_code","target_id":"github:pyRiemann:pyRiemann","source_url":"https://github.com/pyRiemann/pyRiemann"},{"type":"has_code","target_id":"github:pyRiemann:pyRiemann","source_url":"https://github.com/pyRiemann/pyRiemann"},{"type":"has_code","target_id":"github:numenta:nupic-legacy\">NuPIC<","source_url":"https://github.com/numenta/nupic-legacy\">NuPIC<"},{"type":"has_code","target_id":"github:numenta:nupic-legacy","source_url":"https://github.com/numenta/nupic-legacy"},{"type":"has_code","target_id":"github:numenta:nupic","source_url":"https://github.com/numenta/nupic"},{"type":"has_code","target_id":"github:determined-ai:determined\">Determined<","source_url":"https://github.com/determined-ai/determined\">Determined<"},{"type":"has_code","target_id":"github:determined-ai:determined","source_url":"https://github.com/determined-ai/determined"},{"type":"has_code","target_id":"github:determined-ai:determined","source_url":"https://github.com/determined-ai/determined"},{"type":"has_code","target_id":"github:sony:nnabla\">Neural","source_url":"https://github.com/sony/nnabla\">Neural"},{"type":"has_code","target_id":"github:sony:nnabla","source_url":"https://github.com/sony/nnabla"},{"type":"has_code","target_id":"github:sony:nnabla","source_url":"https://github.com/sony/nnabla"},{"type":"has_code","target_id":"github:deepinv:deepinv\">deepinv<","source_url":"https://github.com/deepinv/deepinv\">deepinv<"},{"type":"has_code","target_id":"github:deepinv:deepinv","source_url":"https://github.com/deepinv/deepinv"},{"type":"has_code","target_id":"github:deepinv:deepinv","source_url":"https://github.com/deepinv/deepinv"},{"type":"has_code","target_id":"github:towhee-io:towhee\">Towhee<","source_url":"https://github.com/towhee-io/towhee\">Towhee<"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:nubank:fklearn\">fklearn<","source_url":"https://github.com/nubank/fklearn\">fklearn<"},{"type":"has_code","target_id":"github:nubank:fklearn","source_url":"https://github.com/nubank/fklearn"},{"type":"has_code","target_id":"github:nubank:fklearn","source_url":"https://github.com/nubank/fklearn"},{"type":"has_code","target_id":"github:run-house:kubetorch\">Runhouse<","source_url":"https://github.com/run-house/kubetorch\">Runhouse<"},{"type":"has_code","target_id":"github:run-house:kubetorch","source_url":"https://github.com/run-house/kubetorch"},{"type":"has_code","target_id":"github:run-house:runhouse","source_url":"https://github.com/run-house/runhouse"},{"type":"has_code","target_id":"github:neoml-lib:neoml\">NeoML<","source_url":"https://github.com/neoml-lib/neoml\">NeoML<"},{"type":"has_code","target_id":"github:neoml-lib:neoml","source_url":"https://github.com/neoml-lib/neoml"},{"type":"has_code","target_id":"github:neoml-lib:neoml","source_url":"https://github.com/neoml-lib/neoml"},{"type":"has_code","target_id":"github:serengil:chefboost\">chefboost<","source_url":"https://github.com/serengil/chefboost\">chefboost<"},{"type":"has_code","target_id":"github:serengil:chefboost","source_url":"https://github.com/serengil/chefboost"},{"type":"has_code","target_id":"github:serengil:chefboost","source_url":"https://github.com/serengil/chefboost"},{"type":"has_code","target_id":"github:Xtra-Computing:thundergbm\">ThunderGBM<","source_url":"https://github.com/Xtra-Computing/thundergbm\">ThunderGBM<"},{"type":"has_code","target_id":"github:Xtra-Computing:thundergbm","source_url":"https://github.com/Xtra-Computing/thundergbm"},{"type":"has_code","target_id":"github:Xtra-Computing:thundergbm","source_url":"https://github.com/Xtra-Computing/thundergbm"},{"type":"has_code","target_id":"github:davisking:dlib\">dlib<","source_url":"https://github.com/davisking/dlib\">dlib<"},{"type":"has_code","target_id":"github:apache:mxnet\">MXNet<","source_url":"https://github.com/apache/mxnet\">MXNet<"},{"type":"has_code","target_id":"github:Theano:Theano\">Theano<","source_url":"https://github.com/Theano/Theano\">Theano<"},{"type":"has_code","target_id":"github:mindsdb:mindsdb\">MindsDB<","source_url":"https://github.com/mindsdb/mindsdb\">MindsDB<"},{"type":"has_code","target_id":"github:VowpalWabbit:vowpal_wabbit\">Vowpal","source_url":"https://github.com/VowpalWabbit/vowpal_wabbit\">Vowpal"},{"type":"has_code","target_id":"github:chainer:chainer\">Chainer<","source_url":"https://github.com/chainer/chainer\">Chainer<"},{"type":"has_code","target_id":"github:apple:turicreate\">Turi","source_url":"https://github.com/apple/turicreate\">Turi"},{"type":"has_code","target_id":"github:tensorpack:tensorpack\">tensorpack<","source_url":"https://github.com/tensorpack/tensorpack\">tensorpack<"},{"type":"has_code","target_id":"github:tflearn:tflearn\">TFlearn<","source_url":"https://github.com/tflearn/tflearn\">TFlearn<"},{"type":"has_code","target_id":"github:clab:dynet\">dyNET<","source_url":"https://github.com/clab/dynet\">dyNET<"},{"type":"has_code","target_id":"github:microsoft:CNTK\">CNTK<","source_url":"https://github.com/microsoft/CNTK\">CNTK<"},{"type":"has_code","target_id":"github:Lasagne:Lasagne\">Lasagne<","source_url":"https://github.com/Lasagne/Lasagne\">Lasagne<"},{"type":"has_code","target_id":"github:shogun-toolbox:shogun\">SHOGUN<","source_url":"https://github.com/shogun-toolbox/shogun\">SHOGUN<"},{"type":"has_code","target_id":"github:amaiya:ktrain\">ktrain<","source_url":"https://github.com/amaiya/ktrain\">ktrain<"},{"type":"has_code","target_id":"github:itdxer:neupy\">NeuPy<","source_url":"https://github.com/itdxer/neupy\">NeuPy<"},{"type":"has_code","target_id":"github:aksnzhy:xlearn\">xLearn<","source_url":"https://github.com/aksnzhy/xlearn\">xLearn<"},{"type":"has_code","target_id":"github:georgia-tech-db:evadb\">EvaDB<","source_url":"https://github.com/georgia-tech-db/evadb\">EvaDB<"},{"type":"has_code","target_id":"github:NervanaSystems:neon\">neon<","source_url":"https://github.com/NervanaSystems/neon\">neon<"},{"type":"has_code","target_id":"github:Xtra-Computing:thundersvm\">ThunderSVM<","source_url":"https://github.com/Xtra-Computing/thundersvm\">ThunderSVM<"},{"type":"has_code","target_id":"github:pytorchbearer:torchbearer\">Torchbearer<","source_url":"https://github.com/pytorchbearer/torchbearer\">Torchbearer<"},{"type":"has_code","target_id":"github:XiaoMi:mace\">mace<","source_url":"https://github.com/XiaoMi/mace\">mace<"},{"type":"has_code","target_id":"github:google:neural-tangents\">Neural","source_url":"https://github.com/google/neural-tangents\">Neural"},{"type":"has_code","target_id":"github:google:objax\">Objax<","source_url":"https://github.com/google/objax\">Objax<"},{"type":"has_code","target_id":"github:poets-ai:elegy\">elegy<","source_url":"https://github.com/poets-ai/elegy\">elegy<"},{"type":"has_code","target_id":"github:facebookresearch:StarSpace\">StarSpace<","source_url":"https://github.com/facebookresearch/StarSpace\">StarSpace<"},{"type":"has_code","target_id":"github:HenryNdubuaku:nanodl\">nanodl<","source_url":"https://github.com/HenryNdubuaku/nanodl\">nanodl<"},{"type":"has_code","target_id":"github:matplotlib:matplotlib\">Matplotlib<","source_url":"https://github.com/matplotlib/matplotlib\">Matplotlib<"},{"type":"has_code","target_id":"github:matplotlib:matplotlib","source_url":"https://github.com/matplotlib/matplotlib"},{"type":"has_code","target_id":"github:matplotlib:matplotlib","source_url":"https://github.com/matplotlib/matplotlib"},{"type":"has_code","target_id":"github:plotly:plotly.py\">Plotly<","source_url":"https://github.com/plotly/plotly.py\">Plotly<"},{"type":"has_code","target_id":"github:plotly:plotly.py","source_url":"https://github.com/plotly/plotly.py"},{"type":"has_code","target_id":"github:plotly:plotly.py","source_url":"https://github.com/plotly/plotly.py"},{"type":"has_code","target_id":"github:plotly:dash\">dash<","source_url":"https://github.com/plotly/dash\">dash<"},{"type":"has_code","target_id":"github:plotly:dash","source_url":"https://github.com/plotly/dash"},{"type":"has_code","target_id":"github:plotly:dash","source_url":"https://github.com/plotly/dash"},{"type":"has_code","target_id":"github:bokeh:bokeh\">Bokeh<","source_url":"https://github.com/bokeh/bokeh\">Bokeh<"},{"type":"has_code","target_id":"github:bokeh:bokeh","source_url":"https://github.com/bokeh/bokeh"},{"type":"has_code","target_id":"github:bokeh:bokeh","source_url":"https://github.com/bokeh/bokeh"},{"type":"has_code","target_id":"github:mwaskom:seaborn\">Seaborn<","source_url":"https://github.com/mwaskom/seaborn\">Seaborn<"},{"type":"has_code","target_id":"github:mwaskom:seaborn","source_url":"https://github.com/mwaskom/seaborn"},{"type":"has_code","target_id":"github:mwaskom:seaborn","source_url":"https://github.com/mwaskom/seaborn"},{"type":"has_code","target_id":"github:vega:altair\">Altair<","source_url":"https://github.com/vega/altair\">Altair<"},{"type":"has_code","target_id":"github:vega:altair","source_url":"https://github.com/vega/altair"},{"type":"has_code","target_id":"github:altair-viz:altair","source_url":"https://github.com/altair-viz/altair"},{"type":"has_code","target_id":"github:voxel51:fiftyone\">FiftyOne<","source_url":"https://github.com/voxel51/fiftyone\">FiftyOne<"},{"type":"has_code","target_id":"github:voxel51:fiftyone","source_url":"https://github.com/voxel51/fiftyone"},{"type":"has_code","target_id":"github:voxel51:fiftyone","source_url":"https://github.com/voxel51/fiftyone"},{"type":"has_code","target_id":"github:xflr6:graphviz\">Graphviz<","source_url":"https://github.com/xflr6/graphviz\">Graphviz<"},{"type":"has_code","target_id":"github:xflr6:graphviz","source_url":"https://github.com/xflr6/graphviz"},{"type":"has_code","target_id":"github:xflr6:graphviz","source_url":"https://github.com/xflr6/graphviz"},{"type":"has_code","target_id":"github:pyvista:pyvista\">PyVista<","source_url":"https://github.com/pyvista/pyvista\">PyVista<"},{"type":"has_code","target_id":"github:pyvista:pyvista","source_url":"https://github.com/pyvista/pyvista"},{"type":"has_code","target_id":"github:pyvista:pyvista","source_url":"https://github.com/pyvista/pyvista"},{"type":"has_code","target_id":"github:holoviz:holoviews\">HoloViews<","source_url":"https://github.com/holoviz/holoviews\">HoloViews<"},{"type":"has_code","target_id":"github:holoviz:holoviews","source_url":"https://github.com/holoviz/holoviews"},{"type":"has_code","target_id":"github:holoviz:holoviews","source_url":"https://github.com/holoviz/holoviews"},{"type":"has_code","target_id":"github:pyecharts:pyecharts\">pyecharts<","source_url":"https://github.com/pyecharts/pyecharts\">pyecharts<"},{"type":"has_code","target_id":"github:pyecharts:pyecharts","source_url":"https://github.com/pyecharts/pyecharts"},{"type":"has_code","target_id":"github:pyecharts:pyecharts","source_url":"https://github.com/pyecharts/pyecharts"},{"type":"has_code","target_id":"github:pyqtgraph:pyqtgraph\">PyQtGraph<","source_url":"https://github.com/pyqtgraph/pyqtgraph\">PyQtGraph<"},{"type":"has_code","target_id":"github:pyqtgraph:pyqtgraph","source_url":"https://github.com/pyqtgraph/pyqtgraph"},{"type":"has_code","target_id":"github:pyqtgraph:pyqtgraph","source_url":"https://github.com/pyqtgraph/pyqtgraph"},{"type":"has_code","target_id":"github:ydataai:ydata-profiling\">pandas-profiling<","source_url":"https://github.com/ydataai/ydata-profiling\">pandas-profiling<"},{"type":"has_code","target_id":"github:ydataai:ydata-profiling","source_url":"https://github.com/ydataai/ydata-profiling"},{"type":"has_code","target_id":"github:ydataai:pandas-profiling","source_url":"https://github.com/ydataai/pandas-profiling"},{"type":"has_code","target_id":"github:has2k1:plotnine\">plotnine<","source_url":"https://github.com/has2k1/plotnine\">plotnine<"},{"type":"has_code","target_id":"github:has2k1:plotnine","source_url":"https://github.com/has2k1/plotnine"},{"type":"has_code","target_id":"github:has2k1:plotnine","source_url":"https://github.com/has2k1/plotnine"},{"type":"has_code","target_id":"github:SciTools:cartopy\">cartopy<","source_url":"https://github.com/SciTools/cartopy\">cartopy<"},{"type":"has_code","target_id":"github:SciTools:cartopy","source_url":"https://github.com/SciTools/cartopy"},{"type":"has_code","target_id":"github:SciTools:cartopy","source_url":"https://github.com/SciTools/cartopy"},{"type":"has_code","target_id":"github:vispy:vispy\">VisPy<","source_url":"https://github.com/vispy/vispy\">VisPy<"},{"type":"has_code","target_id":"github:vispy:vispy","source_url":"https://github.com/vispy/vispy"},{"type":"has_code","target_id":"github:vispy:vispy","source_url":"https://github.com/vispy/vispy"},{"type":"has_code","target_id":"github:holoviz:datashader\">datashader<","source_url":"https://github.com/holoviz/datashader\">datashader<"},{"type":"has_code","target_id":"github:holoviz:datashader","source_url":"https://github.com/holoviz/datashader"},{"type":"has_code","target_id":"github:holoviz:datashader","source_url":"https://github.com/holoviz/datashader"},{"type":"has_code","target_id":"github:JetBrains:lets-plot\">lets-plot<","source_url":"https://github.com/JetBrains/lets-plot\">lets-plot<"},{"type":"has_code","target_id":"github:JetBrains:lets-plot","source_url":"https://github.com/JetBrains/lets-plot"},{"type":"has_code","target_id":"github:JetBrains:lets-plot","source_url":"https://github.com/JetBrains/lets-plot"},{"type":"has_code","target_id":"github:amueller:word_cloud\">wordcloud<","source_url":"https://github.com/amueller/word_cloud\">wordcloud<"},{"type":"has_code","target_id":"github:amueller:word_cloud","source_url":"https://github.com/amueller/word_cloud"},{"type":"has_code","target_id":"github:amueller:word_cloud","source_url":"https://github.com/amueller/word_cloud"},{"type":"has_code","target_id":"github:perspective-dev:perspective\">Perspective<","source_url":"https://github.com/perspective-dev/perspective\">Perspective<"},{"type":"has_code","target_id":"github:perspective-dev:perspective","source_url":"https://github.com/perspective-dev/perspective"},{"type":"has_code","target_id":"github:finos:perspective","source_url":"https://github.com/finos/perspective"},{"type":"has_code","target_id":"github:lmcinnes:umap\">UMAP<","source_url":"https://github.com/lmcinnes/umap\">UMAP<"},{"type":"has_code","target_id":"github:lmcinnes:umap","source_url":"https://github.com/lmcinnes/umap"},{"type":"has_code","target_id":"github:lmcinnes:umap","source_url":"https://github.com/lmcinnes/umap"},{"type":"has_code","target_id":"github:holoviz:hvplot\">hvPlot<","source_url":"https://github.com/holoviz/hvplot\">hvPlot<"},{"type":"has_code","target_id":"github:holoviz:hvplot","source_url":"https://github.com/holoviz/hvplot"},{"type":"has_code","target_id":"github:holoviz:hvplot","source_url":"https://github.com/holoviz/hvplot"},{"type":"has_code","target_id":"github:mpld3:mpld3\">mpld3<","source_url":"https://github.com/mpld3/mpld3\">mpld3<"},{"type":"has_code","target_id":"github:mpld3:mpld3","source_url":"https://github.com/mpld3/mpld3"},{"type":"has_code","target_id":"github:mpld3:mpld3","source_url":"https://github.com/mpld3/mpld3"},{"type":"has_code","target_id":"github:bqplot:bqplot\">bqplot<","source_url":"https://github.com/bqplot/bqplot\">bqplot<"},{"type":"has_code","target_id":"github:bqplot:bqplot","source_url":"https://github.com/bqplot/bqplot"},{"type":"has_code","target_id":"github:bqplot:bqplot","source_url":"https://github.com/bqplot/bqplot"},{"type":"has_code","target_id":"github:man-group:dtale\">D-Tale<","source_url":"https://github.com/man-group/dtale\">D-Tale<"},{"type":"has_code","target_id":"github:man-group:dtale","source_url":"https://github.com/man-group/dtale"},{"type":"has_code","target_id":"github:man-group:dtale","source_url":"https://github.com/man-group/dtale"},{"type":"has_code","target_id":"github:pavlin-policar:openTSNE\">openTSNE<","source_url":"https://github.com/pavlin-policar/openTSNE\">openTSNE<"},{"type":"has_code","target_id":"github:pavlin-policar:openTSNE","source_url":"https://github.com/pavlin-policar/openTSNE"},{"type":"has_code","target_id":"github:pavlin-policar:openTSNE","source_url":"https://github.com/pavlin-policar/openTSNE"},{"type":"has_code","target_id":"github:predict-idlab:plotly-resampler\">Plotly-Resampler<","source_url":"https://github.com/predict-idlab/plotly-resampler\">Plotly-Resampler<"},{"type":"has_code","target_id":"github:predict-idlab:plotly-resampler","source_url":"https://github.com/predict-idlab/plotly-resampler"},{"type":"has_code","target_id":"github:predict-idlab:plotly-resampler","source_url":"https://github.com/predict-idlab/plotly-resampler"},{"type":"has_code","target_id":"github:ContextLab:hypertools\">HyperTools<","source_url":"https://github.com/ContextLab/hypertools\">HyperTools<"},{"type":"has_code","target_id":"github:ContextLab:hypertools","source_url":"https://github.com/ContextLab/hypertools"},{"type":"has_code","target_id":"github:ContextLab:hypertools","source_url":"https://github.com/ContextLab/hypertools"},{"type":"has_code","target_id":"github:tensorflow:data-validation\">data-validation<","source_url":"https://github.com/tensorflow/data-validation\">data-validation<"},{"type":"has_code","target_id":"github:tensorflow:data-validation","source_url":"https://github.com/tensorflow/data-validation"},{"type":"has_code","target_id":"github:tensorflow:data-validation","source_url":"https://github.com/tensorflow/data-validation"},{"type":"has_code","target_id":"github:spotify:chartify\">Chartify<","source_url":"https://github.com/spotify/chartify\">Chartify<"},{"type":"has_code","target_id":"github:spotify:chartify","source_url":"https://github.com/spotify/chartify"},{"type":"has_code","target_id":"github:spotify:chartify","source_url":"https://github.com/spotify/chartify"},{"type":"has_code","target_id":"github:ing-bank:popmon\">Popmon<","source_url":"https://github.com/ing-bank/popmon\">Popmon<"},{"type":"has_code","target_id":"github:ing-bank:popmon","source_url":"https://github.com/ing-bank/popmon"},{"type":"has_code","target_id":"github:ing-bank:popmon","source_url":"https://github.com/ing-bank/popmon"},{"type":"has_code","target_id":"github:vega:ipyvega\">vega<","source_url":"https://github.com/vega/ipyvega\">vega<"},{"type":"has_code","target_id":"github:vega:ipyvega","source_url":"https://github.com/vega/ipyvega"},{"type":"has_code","target_id":"github:vega:ipyvega","source_url":"https://github.com/vega/ipyvega"},{"type":"has_code","target_id":"github:vega:vegafusion\">vegafusion<","source_url":"https://github.com/vega/vegafusion\">vegafusion<"},{"type":"has_code","target_id":"github:vega:vegafusion","source_url":"https://github.com/vega/vegafusion"},{"type":"has_code","target_id":"github:vegafusion:vegafusion","source_url":"https://github.com/vegafusion/vegafusion"},{"type":"has_code","target_id":"github:ResidentMario:missingno\">missingno<","source_url":"https://github.com/ResidentMario/missingno\">missingno<"},{"type":"has_code","target_id":"github:PAIR-code:facets\">Facets","source_url":"https://github.com/PAIR-code/facets\">Facets"},{"type":"has_code","target_id":"github:santosjorge:cufflinks\">Cufflinks<","source_url":"https://github.com/santosjorge/cufflinks\">Cufflinks<"},{"type":"has_code","target_id":"github:jupyter-widgets:pythreejs\">pythreejs<","source_url":"https://github.com/jupyter-widgets/pythreejs\">pythreejs<"},{"type":"has_code","target_id":"github:fbdesignpro:sweetviz\">Sweetviz<","source_url":"https://github.com/fbdesignpro/sweetviz\">Sweetviz<"},{"type":"has_code","target_id":"github:AutoViML:AutoViz\">AutoViz<","source_url":"https://github.com/AutoViML/AutoViz\">AutoViz<"},{"type":"has_code","target_id":"github:tpvasconcelos:ridgeplot\">ridgeplot<","source_url":"https://github.com/tpvasconcelos/ridgeplot\">ridgeplot<"},{"type":"has_code","target_id":"github:adamerose:PandasGUI\">PandasGUI<","source_url":"https://github.com/adamerose/PandasGUI\">PandasGUI<"},{"type":"has_code","target_id":"github:facebookresearch:hiplot\">HiPlot<","source_url":"https://github.com/facebookresearch/hiplot\">HiPlot<"},{"type":"has_code","target_id":"github:marcharper:python-ternary\">python-ternary<","source_url":"https://github.com/marcharper/python-ternary\">python-ternary<"},{"type":"has_code","target_id":"github:DmitryUlyanov:Multicore-TSNE\">Multicore-TSNE<","source_url":"https://github.com/DmitryUlyanov/Multicore-TSNE\">Multicore-TSNE<"},{"type":"has_code","target_id":"github:PatrikHlobil:Pandas-Bokeh\">Pandas-Bokeh<","source_url":"https://github.com/PatrikHlobil/Pandas-Bokeh\">Pandas-Bokeh<"},{"type":"has_code","target_id":"github:nicolaskruchten:jupyter_pivottablejs\">pivottablejs<","source_url":"https://github.com/nicolaskruchten/jupyter_pivottablejs\">pivottablejs<"},{"type":"has_code","target_id":"github:leotac:joypy\">joypy<","source_url":"https://github.com/leotac/joypy\">joypy<"},{"type":"has_code","target_id":"github:gyli:PyWaffle\">PyWaffle<","source_url":"https://github.com/gyli/PyWaffle\">PyWaffle<"},{"type":"has_code","target_id":"github:sosuneko:PDPbox\">PDPbox<","source_url":"https://github.com/sosuneko/PDPbox\">PDPbox<"},{"type":"has_code","target_id":"github:t-makaro:animatplot\">animatplot<","source_url":"https://github.com/t-makaro/animatplot\">animatplot<"},{"type":"has_code","target_id":"github:beringresearch:ivis\">ivis<","source_url":"https://github.com/beringresearch/ivis\">ivis<"},{"type":"has_code","target_id":"github:altair-viz:pdvega\">pdvega<","source_url":"https://github.com/altair-viz/pdvega\">pdvega<"},{"type":"has_code","target_id":"github:Zsailer:nx_altair\">nx-altair<","source_url":"https://github.com/Zsailer/nx_altair\">nx-altair<"},{"type":"has_code","target_id":"github:data-describe:data-describe\">data-describe<","source_url":"https://github.com/data-describe/data-describe\">data-describe<"},{"type":"has_code","target_id":"github:biovault:nptsne\">nptsne<","source_url":"https://github.com/biovault/nptsne\">nptsne<"},{"type":"has_code","target_id":"github:huggingface:transformers\">transformers<","source_url":"https://github.com/huggingface/transformers\">transformers<"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:nltk:nltk\">nltk<","source_url":"https://github.com/nltk/nltk\">nltk<"},{"type":"has_code","target_id":"github:nltk:nltk","source_url":"https://github.com/nltk/nltk"},{"type":"has_code","target_id":"github:nltk:nltk","source_url":"https://github.com/nltk/nltk"},{"type":"has_code","target_id":"github:BerriAI:litellm\">litellm<","source_url":"https://github.com/BerriAI/litellm\">litellm<"},{"type":"has_code","target_id":"github:BerriAI:litellm","source_url":"https://github.com/BerriAI/litellm"},{"type":"has_code","target_id":"github:BerriAI:litellm","source_url":"https://github.com/BerriAI/litellm"},{"type":"has_code","target_id":"github:explosion:spaCy\">spaCy<","source_url":"https://github.com/explosion/spaCy\">spaCy<"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:huggingface:sentence-transformers\">sentence-transformers<","source_url":"https://github.com/huggingface/sentence-transformers\">sentence-transformers<"},{"type":"has_code","target_id":"github:huggingface:sentence-transformers","source_url":"https://github.com/huggingface/sentence-transformers"},{"type":"has_code","target_id":"github:UKPLab:sentence-transformers","source_url":"https://github.com/UKPLab/sentence-transformers"},{"type":"has_code","target_id":"github:piskvorky:gensim\">gensim<","source_url":"https://github.com/piskvorky/gensim\">gensim<"},{"type":"has_code","target_id":"github:piskvorky:gensim","source_url":"https://github.com/piskvorky/gensim"},{"type":"has_code","target_id":"github:RaRe-Technologies:gensim","source_url":"https://github.com/RaRe-Technologies/gensim"},{"type":"has_code","target_id":"github:google:sentencepiece\">sentencepiece<","source_url":"https://github.com/google/sentencepiece\">sentencepiece<"},{"type":"has_code","target_id":"github:google:sentencepiece","source_url":"https://github.com/google/sentencepiece"},{"type":"has_code","target_id":"github:google:sentencepiece","source_url":"https://github.com/google/sentencepiece"},{"type":"has_code","target_id":"github:huggingface:tokenizers\">Tokenizers<","source_url":"https://github.com/huggingface/tokenizers\">Tokenizers<"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:NVIDIA-NeMo:NeMo\">NeMo<","source_url":"https://github.com/NVIDIA-NeMo/NeMo\">NeMo<"},{"type":"has_code","target_id":"github:NVIDIA-NeMo:NeMo","source_url":"https://github.com/NVIDIA-NeMo/NeMo"},{"type":"has_code","target_id":"github:NVIDIA:NeMo","source_url":"https://github.com/NVIDIA/NeMo"},{"type":"has_code","target_id":"github:deepset-ai:haystack\">haystack<","source_url":"https://github.com/deepset-ai/haystack\">haystack<"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:comet-ml:opik\">Opik<","source_url":"https://github.com/comet-ml/opik\">Opik<"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:gunthercox:ChatterBot\">ChatterBot<","source_url":"https://github.com/gunthercox/ChatterBot\">ChatterBot<"},{"type":"has_code","target_id":"github:gunthercox:ChatterBot","source_url":"https://github.com/gunthercox/ChatterBot"},{"type":"has_code","target_id":"github:gunthercox:ChatterBot","source_url":"https://github.com/gunthercox/ChatterBot"},{"type":"has_code","target_id":"github:flairNLP:flair\">flair<","source_url":"https://github.com/flairNLP/flair\">flair<"},{"type":"has_code","target_id":"github:flairNLP:flair","source_url":"https://github.com/flairNLP/flair"},{"type":"has_code","target_id":"github:flairNLP:flair","source_url":"https://github.com/flairNLP/flair"},{"type":"has_code","target_id":"github:sloria:TextBlob\">TextBlob<","source_url":"https://github.com/sloria/TextBlob\">TextBlob<"},{"type":"has_code","target_id":"github:sloria:TextBlob","source_url":"https://github.com/sloria/TextBlob"},{"type":"has_code","target_id":"github:sloria:TextBlob","source_url":"https://github.com/sloria/TextBlob"},{"type":"has_code","target_id":"github:facebookresearch:fairseq\">fairseq<","source_url":"https://github.com/facebookresearch/fairseq\">fairseq<"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"},{"type":"has_code","target_id":"github:stanfordnlp:stanza\">stanza<","source_url":"https://github.com/stanfordnlp/stanza\">stanza<"},{"type":"has_code","target_id":"github:stanfordnlp:stanza","source_url":"https://github.com/stanfordnlp/stanza"},{"type":"has_code","target_id":"github:stanfordnlp:stanza","source_url":"https://github.com/stanfordnlp/stanza"},{"type":"has_code","target_id":"github:qdrant:qdrant\">qdrant<","source_url":"https://github.com/qdrant/qdrant\">qdrant<"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp\">spark-nlp<","source_url":"https://github.com/JohnSnowLabs/spark-nlp\">spark-nlp<"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp","source_url":"https://github.com/JohnSnowLabs/spark-nlp"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp","source_url":"https://github.com/JohnSnowLabs/spark-nlp"},{"type":"has_code","target_id":"github:RasaHQ:rasa\">Rasa<","source_url":"https://github.com/RasaHQ/rasa\">Rasa<"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:tensorflow:text\">TensorFlow","source_url":"https://github.com/tensorflow/text\">TensorFlow"},{"type":"has_code","target_id":"github:tensorflow:text","source_url":"https://github.com/tensorflow/text"},{"type":"has_code","target_id":"github:tensorflow:text","source_url":"https://github.com/tensorflow/text"},{"type":"has_code","target_id":"github:snowballstem:snowball\">snowballstemmer<","source_url":"https://github.com/snowballstem/snowball\">snowballstemmer<"},{"type":"has_code","target_id":"github:snowballstem:snowball","source_url":"https://github.com/snowballstem/snowball"},{"type":"has_code","target_id":"github:snowballstem:snowball","source_url":"https://github.com/snowballstem/snowball"},{"type":"has_code","target_id":"github:pytorch:text\">torchtext<","source_url":"https://github.com/pytorch/text\">torchtext<"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:jamesturk:jellyfish\">jellyfish<","source_url":"https://github.com/jamesturk/jellyfish\">jellyfish<"},{"type":"has_code","target_id":"github:jamesturk:jellyfish","source_url":"https://github.com/jamesturk/jellyfish"},{"type":"has_code","target_id":"github:jamesturk:jellyfish","source_url":"https://github.com/jamesturk/jellyfish"},{"type":"has_code","target_id":"github:deeppavlov:DeepPavlov\">DeepPavlov<","source_url":"https://github.com/deeppavlov/DeepPavlov\">DeepPavlov<"},{"type":"has_code","target_id":"github:deeppavlov:DeepPavlov","source_url":"https://github.com/deeppavlov/DeepPavlov"},{"type":"has_code","target_id":"github:deepmipt:DeepPavlov","source_url":"https://github.com/deepmipt/DeepPavlov"},{"type":"has_code","target_id":"github:rspeer:python-ftfy\">ftfy<","source_url":"https://github.com/rspeer/python-ftfy\">ftfy<"},{"type":"has_code","target_id":"github:rspeer:python-ftfy","source_url":"https://github.com/rspeer/python-ftfy"},{"type":"has_code","target_id":"github:rspeer:python-ftfy","source_url":"https://github.com/rspeer/python-ftfy"},{"type":"has_code","target_id":"github:allenai:scispacy\">SciSpacy<","source_url":"https://github.com/allenai/scispacy\">SciSpacy<"},{"type":"has_code","target_id":"github:allenai:scispacy","source_url":"https://github.com/allenai/scispacy"},{"type":"has_code","target_id":"github:allenai:scispacy","source_url":"https://github.com/allenai/scispacy"},{"type":"has_code","target_id":"github:cltk:cltk\">CLTK<","source_url":"https://github.com/cltk/cltk\">CLTK<"},{"type":"has_code","target_id":"github:cltk:cltk","source_url":"https://github.com/cltk/cltk"},{"type":"has_code","target_id":"github:cltk:cltk","source_url":"https://github.com/cltk/cltk"},{"type":"has_code","target_id":"github:dwyl:english-words\">english-words<","source_url":"https://github.com/dwyl/english-words\">english-words<"},{"type":"has_code","target_id":"github:dwyl:english-words","source_url":"https://github.com/dwyl/english-words"},{"type":"has_code","target_id":"github:dwyl:english-words","source_url":"https://github.com/dwyl/english-words"},{"type":"has_code","target_id":"github:argilla-io:argilla\">rubrix<","source_url":"https://github.com/argilla-io/argilla\">rubrix<"},{"type":"has_code","target_id":"github:argilla-io:argilla","source_url":"https://github.com/argilla-io/argilla"},{"type":"has_code","target_id":"github:recognai:rubrix","source_url":"https://github.com/recognai/rubrix"},{"type":"has_code","target_id":"github:dedupeio:dedupe\">Dedupe<","source_url":"https://github.com/dedupeio/dedupe\">Dedupe<"},{"type":"has_code","target_id":"github:dedupeio:dedupe","source_url":"https://github.com/dedupeio/dedupe"},{"type":"has_code","target_id":"github:dedupeio:dedupe","source_url":"https://github.com/dedupeio/dedupe"},{"type":"has_code","target_id":"github:life4:textdistance\">TextDistance<","source_url":"https://github.com/life4/textdistance\">TextDistance<"},{"type":"has_code","target_id":"github:life4:textdistance","source_url":"https://github.com/life4/textdistance"},{"type":"has_code","target_id":"github:life4:textdistance","source_url":"https://github.com/life4/textdistance"},{"type":"has_code","target_id":"github:explosion:spacy-transformers\">spacy-transformers<","source_url":"https://github.com/explosion/spacy-transformers\">spacy-transformers<"},{"type":"has_code","target_id":"github:explosion:spacy-transformers","source_url":"https://github.com/explosion/spacy-transformers"},{"type":"has_code","target_id":"github:explosion:spacy-transformers","source_url":"https://github.com/explosion/spacy-transformers"},{"type":"has_code","target_id":"github:unitaryai:detoxify\">detoxify<","source_url":"https://github.com/unitaryai/detoxify\">detoxify<"},{"type":"has_code","target_id":"github:unitaryai:detoxify","source_url":"https://github.com/unitaryai/detoxify"},{"type":"has_code","target_id":"github:unitaryai:detoxify","source_url":"https://github.com/unitaryai/detoxify"},{"type":"has_code","target_id":"github:JasonKessler:scattertext\">scattertext<","source_url":"https://github.com/JasonKessler/scattertext\">scattertext<"},{"type":"has_code","target_id":"github:JasonKessler:scattertext","source_url":"https://github.com/JasonKessler/scattertext"},{"type":"has_code","target_id":"github:JasonKessler:scattertext","source_url":"https://github.com/JasonKessler/scattertext"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer\">T5<","source_url":"https://github.com/google-research/text-to-text-transfer-transformer\">T5<"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer"},{"type":"has_code","target_id":"github:zjunlp:DeepKE\">DeepKE<","source_url":"https://github.com/zjunlp/DeepKE\">DeepKE<"},{"type":"has_code","target_id":"github:zjunlp:DeepKE","source_url":"https://github.com/zjunlp/DeepKE"},{"type":"has_code","target_id":"github:zjunlp:deepke","source_url":"https://github.com/zjunlp/deepke"},{"type":"has_code","target_id":"github:explosion:sense2vec\">sense2vec<","source_url":"https://github.com/explosion/sense2vec\">sense2vec<"},{"type":"has_code","target_id":"github:explosion:sense2vec","source_url":"https://github.com/explosion/sense2vec"},{"type":"has_code","target_id":"github:explosion:sense2vec","source_url":"https://github.com/explosion/sense2vec"},{"type":"has_code","target_id":"github:IndicoDataSolutions:finetune\">finetune<","source_url":"https://github.com/IndicoDataSolutions/finetune\">finetune<"},{"type":"has_code","target_id":"github:IndicoDataSolutions:finetune","source_url":"https://github.com/IndicoDataSolutions/finetune"},{"type":"has_code","target_id":"github:IndicoDataSolutions:finetune","source_url":"https://github.com/IndicoDataSolutions/finetune"},{"type":"has_code","target_id":"github:EricFillion:happy-transformer\">happy-transformer<","source_url":"https://github.com/EricFillion/happy-transformer\">happy-transformer<"},{"type":"has_code","target_id":"github:EricFillion:happy-transformer","source_url":"https://github.com/EricFillion/happy-transformer"},{"type":"has_code","target_id":"github:EricFillion:happy-transformer","source_url":"https://github.com/EricFillion/happy-transformer"},{"type":"has_code","target_id":"github:awslabs:sockeye\">Sockeye<","source_url":"https://github.com/awslabs/sockeye\">Sockeye<"},{"type":"has_code","target_id":"github:awslabs:sockeye","source_url":"https://github.com/awslabs/sockeye"},{"type":"has_code","target_id":"github:awslabs:sockeye","source_url":"https://github.com/awslabs/sockeye"},{"type":"has_code","target_id":"github:unum-cloud:UForm\">UForm<","source_url":"https://github.com/unum-cloud/UForm\">UForm<"},{"type":"has_code","target_id":"github:unum-cloud:UForm","source_url":"https://github.com/unum-cloud/UForm"},{"type":"has_code","target_id":"github:unum-cloud:uform","source_url":"https://github.com/unum-cloud/uform"},{"type":"has_code","target_id":"github:webis-de:small-text\">small-text<","source_url":"https://github.com/webis-de/small-text\">small-text<"},{"type":"has_code","target_id":"github:webis-de:small-text","source_url":"https://github.com/webis-de/small-text"},{"type":"has_code","target_id":"github:webis-de:small-text","source_url":"https://github.com/webis-de/small-text"},{"type":"has_code","target_id":"github:dsfsi:textaugment\">textaugment<","source_url":"https://github.com/dsfsi/textaugment\">textaugment<"},{"type":"has_code","target_id":"github:dsfsi:textaugment","source_url":"https://github.com/dsfsi/textaugment"},{"type":"has_code","target_id":"github:dsfsi:textaugment","source_url":"https://github.com/dsfsi/textaugment"},{"type":"has_code","target_id":"github:facebookresearch:vizseq\">VizSeq<","source_url":"https://github.com/facebookresearch/vizseq\">VizSeq<"},{"type":"has_code","target_id":"github:facebookresearch:vizseq","source_url":"https://github.com/facebookresearch/vizseq"},{"type":"has_code","target_id":"github:facebookresearch:vizseq","source_url":"https://github.com/facebookresearch/vizseq"},{"type":"has_code","target_id":"github:allenai:allennlp\">AllenNLP<","source_url":"https://github.com/allenai/allennlp\">AllenNLP<"},{"type":"has_code","target_id":"github:facebookresearch:fastText\">fastText<","source_url":"https://github.com/facebookresearch/fastText\">fastText<"},{"type":"has_code","target_id":"github:OpenNMT:OpenNMT-py\">OpenNMT<","source_url":"https://github.com/OpenNMT/OpenNMT-py\">OpenNMT<"},{"type":"has_code","target_id":"github:facebookresearch:ParlAI\">ParlAI<","source_url":"https://github.com/facebookresearch/ParlAI\">ParlAI<"},{"type":"has_code","target_id":"github:seatgeek:fuzzywuzzy\">fuzzywuzzy<","source_url":"https://github.com/seatgeek/fuzzywuzzy\">fuzzywuzzy<"},{"type":"has_code","target_id":"github:miso-belica:sumy\">Sumy<","source_url":"https://github.com/miso-belica/sumy\">Sumy<"},{"type":"has_code","target_id":"github:undertheseanlp:underthesea\">underthesea<","source_url":"https://github.com/undertheseanlp/underthesea\">underthesea<"},{"type":"has_code","target_id":"github:makcedward:nlpaug\">nlpaug<","source_url":"https://github.com/makcedward/nlpaug\">nlpaug<"},{"type":"has_code","target_id":"github:cjhutto:vaderSentiment\">vaderSentiment<","source_url":"https://github.com/cjhutto/vaderSentiment\">vaderSentiment<"},{"type":"has_code","target_id":"github:chartbeat-labs:textacy\">textacy<","source_url":"https://github.com/chartbeat-labs/textacy\">textacy<"},{"type":"has_code","target_id":"github:DerwenAI:pytextrank\">PyTextRank<","source_url":"https://github.com/DerwenAI/pytextrank\">PyTextRank<"},{"type":"has_code","target_id":"github:bee-san:Ciphey\">Ciphey<","source_url":"https://github.com/bee-san/Ciphey\">Ciphey<"},{"type":"has_code","target_id":"github:fastnlp:fastNLP\">fastNLP<","source_url":"https://github.com/fastnlp/fastNLP\">fastNLP<"},{"type":"has_code","target_id":"github:aboSamoor:polyglot\">polyglot<","source_url":"https://github.com/aboSamoor/polyglot\">polyglot<"},{"type":"has_code","target_id":"github:vi3k6i5:flashtext\">flashtext<","source_url":"https://github.com/vi3k6i5/flashtext\">flashtext<"},{"type":"has_code","target_id":"github:saffsd:langid.py\">langid<","source_url":"https://github.com/saffsd/langid.py\">langid<"},{"type":"has_code","target_id":"github:nipunsadvilkar:pySBD\">pySBD<","source_url":"https://github.com/nipunsadvilkar/pySBD\">pySBD<"},{"type":"has_code","target_id":"github:huggingface:neuralcoref\">neuralcoref<","source_url":"https://github.com/huggingface/neuralcoref\">neuralcoref<"},{"type":"has_code","target_id":"github:dmlc:gluon-nlp\">GluonNLP<","source_url":"https://github.com/dmlc/gluon-nlp\">GluonNLP<"}]', NULL, 'CC-BY-SA-4.0', 'approved', 80, '8533104a823a49264fc8c7b17b4f17b2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-lukasmasuch-best-of-ml-python from https://github.com/lukasmasuch.png
Image converted to WebP: data/images/github-lukasmasuch-best-of-ml-python.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-spmallick-learnopencv', 'github--spmallick--learnopencv', 'learnopencv', 'spmallick', 'This repository contains code for Computer Vision, Deep learning, and AI research articles shared on our blog LearnOpenCV.com. Want to become an expert in AI? AI Courses by OpenCV is a great place to start. <a href="https://opencv.org/courses/"> <p align="center"> <img src="https://learnopencv.com/wp-content/uploads/2023/01/AI-Courses-By-OpenCV-Github.png"> </p> </a> | Blog Post | Code| | ------------- |:-------------| | SAM-3: Whatâ€™s New, How It Works, and Why It Matters | Code | | Image-GS:...', '["ai","computer-vision","computervision","deep-learning","deep-neural-networks","deeplearning","machine-learning","opencv","opencv-cpp","opencv-library","opencv-python","opencv-tutorial","opencv3","jupyter notebook"]', 'other', 22546, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/spmallick/learnopencv","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# LearnOpenCV\n\nThis repository contains code for Computer Vision, Deep learning, and AI research articles shared on our blog [LearnOpenCV.com](https://www.LearnOpenCV.com).\n\nWant to become an expert in AI? [AI Courses by OpenCV](https://opencv.org/courses/) is a great place to start.\n\n<a href="https://opencv.org/courses/">\n\n<p align="center">\n<img src="https://learnopencv.com/wp-content/uploads/2023/01/AI-Courses-By-OpenCV-Github.png">\n</p>\n</a>\n\n## List of Blog Posts\n\n| Blog Post | Code|\n| ------------- |:-------------|\n| [SAM-3: Whatâ€™s New, How It Works, and Why It Matters](https://learnopencv.com/sam-3-whats-new/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SAM-3) |\n| [Image-GS: Adaptive Image Reconstruction using 2D Gaussians](https://learnopencv.com/image-gs-image-reconstruction-using-2d-gaussians/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image_GS_Adaptive_Image_Reconstruction_using_2D_Gaussians) |\n| [Ultimate Guide to Vector Databases and RAG Pipeline](https://learnopencv.com/vector-db-and-rag-pipeline-for-document-rag/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Ultimate_Guide_to_Vector_Databases_and_RAG_pipeline) |\n|[What Makes DeepSeek OCR So Powerful](https://learnopencv.com/what-makes-deepseek-ocr-so-powerful/)|[Code](https://github.com/spmallick/learnopencv/tree/master/What-Makes-DeepSeek-OCR-So-Powerful)|\n| [2D Gaussian Splatting: Geometrically Accurate Radiance Field Reconstruction](https://learnopencv.com/2d-gaussian-splatting-2dgs/) | [Code](https://github.com/spmallick/learnopencv/tree/master/2D_Gaussian_Splatting_Geometrically_Accurate_Radiance_Field_Reconstruction) |\n| [TRM: Tiny Recursive Models](https://learnopencv.com/trm-tiny-ai-models-outsmarting-giants-on-complex-puzzles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TRM) |\n|[Deploying ML Models on Arduino: From Blink to Think]()|[Code](https://github.com/spmallick/learnopencv/tree/master/Deploying-ML-Models-on-Arduino-From-Blink-to-Think)|\n| [VideoRAG: Redefining Long-Context Video Comprehension](https://learnopencv.com/videorag-long-context-video-comprehension/) | |\n| [AI Agent in Action: Automating Desktop Tasks with VLMs](https://learnopencv.com/build-ai-agents-using-vlm/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Local-VLM-Agents-in-Action-GUI-Automation-with-Moondream3-and-Gemini) |\n| [Top VLM Evaluation Metrics for Optimal Performance Analysis](https://learnopencv.com/vlm-evaluation-metrics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VLM_Evaluation_Metrics) |\n|[Getting Started with VLM on Jetson Nano](https://learnopencv.com/vlm-on-jetson-nano/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-with-VLM-on-Jetson-Nano)|\n| [VLM on Edge: Worth the Hype or Just a Novelty?](https://learnopencv.com/vlm-on-edge-devices/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VLM-on-Edge-Worth-the-Hype-or-Just-a-Novelty) |\n| [AnomalyCLIP : Harnessing CLIP for Weakly-Supervised Video Anomaly Recognition](https://learnopencv.com/anomalyclip-video-anomaly-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AnomalyCLIP_Harnessing_CLIP_for_Weakly_Supervised_Video_Anomaly_Recognition) |\n| [AI_for_Video_Understanding_From_Content_Moderation_to_Summarization](https://learnopencv.com/ai-for-video-understanding/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AI_for_Video_Understanding_From_Content_Moderation_to_Summarization) |\n| [Video-RAG: Training-Free Retrieval for Long-Video LVLMs](https://learnopencv.com/video-rag-for-long-videos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Video-RAG_Training_Free_Retrieval_for_Long_Video_LVLMs) |\n| [Object Detection and Spatial Understanding with VLMs ft. Qwen2.5-VL](https://learnopencv.com/object-detection-with-vlms-ft-qwen2-5-vl/) | [Code](https://github.com/spmallick/learnopencv/tree/master/object-detection-with-vlms) |\n| [LangGraph: Building Self-Correcting RAG Agent for Code Generation](https://learnopencv.com/langgraph-self-correcting-agent-code-generation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LangGraph_Building_Self_Correcting_RAG_Agent_for_Code_Generation) |\n| [Inside Sinusoidal Position Embeddings: A Sense of Order](https://learnopencv.com/sinusoidal-position-embeddings/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Sinusoidal_Position_Embeddings) |\n| [Inside RoPE: Rotary Magic into Position Embeddings](https://learnopencv.com/rope-position-embeddings/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inside_RoPE_Position_Embeddings) |\n| [SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving](https://learnopencv.com/simlingo-vision-language-action-model-for-autonomous-driving/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving) |\n| [FineTuning Gemma 3n for Medical VQA on ROCOv2](https://learnopencv.com/finetuning-gemma-3n-medical-vqa/) | [Code](https://github.com/spmallick/learnopencv/tree/master/finetuning-gemma3n) |\n| [SmolLM3 Blueprint: SOTA 3B-Parameter LLM](https://learnopencv.com/smollm3-explained/) | |\n| [LangGraph-A-Visual-Automation-and-Summarization-Pipeline](https://learnopencv.com/langgraph-building-a-visual-web-browser-agent/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LangGraph-A-Visual-Automation-and-Summarization-Pipeline) |\n| [Fine-Tuning AnomalyCLIP: Class-Agnostic Zero-Shot Anomaly Detection](https://learnopencv.com/fine-tuning-anomalyclip-medical-anomaly-clip/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-AnomalyCLIP) |\n| [SigLIP 2: DeepMindâ€™s Multilingual Vision-Language Model](https://learnopencv.com/siglip-2-deepminds-multilingual-vision-language-model/) | |\n| [MedGemma: Googleâ€™s Medico VLM for Clinical QA, Imaging, and More](https://learnopencv.com/medgemma-explained/) | [Code](https://github.com/spmallick/learnopencv/tree/master/medgemma) |\n| [Nanonets-OCR-s: Enabling Rich, Structured Markdown for Document Understanding](https://learnopencv.com/nanonets-ocr-s/) | |\n| [Optimizing VJEPA-2: Tackling Latency & Context in Real-Time Video Classification Scripts](https://learnopencv.com/optimizing-vjepa-2-in-real-time-video-classification/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VJEPA-2-Video-Classification) |\n| [V-JEPA 2: Metaâ€™s Breakthrough in AI for the Physical World](https://learnopencv.com/?p=73731&preview_id=73731&preview_nonce=beb70ccf8e&preview=true#heading-7) | [Code](https://github.com/spmallick/learnopencv/tree/master/V-JEPA-2) |\n| [NVIDIA Cosmos Reason1: Video Understanding](https://learnopencv.com/cosmos-reason-vlm-video-vqa/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Cosmos-Reason1-Video-Understanding) |\n| [GR00T N1.5 Explained](https://learnopencv.com/gr00t-n1_5-explained/) |  |\n| [LLaVA](https://learnopencv.com/llava-training-a-visual-assistant/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LLaVA) |\n| [SmolVLA: Affordable & Efficient VLA Robotics on Consumer GPUs](https://learnopencv.com/smolvla-lerobot-vision-language-action-model/) | [Code](https://github.com/spmallick/learnopencv/tree/master/smolvla) |\n| [Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection](https://learnopencv.com/fine-tuning-grounding-dino/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Grounding-DINO-Open-Vocabulary-Object-Detection) |\n| [Getting Started with Qwen3 â€“ The Thinking Expert](https://learnopencv.com/qwen3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qwen3) |\n| [Inside the GPU: A Comprehensive Guide to Modern Graphics Architecture](https://learnopencv.com/modern-gpu-architecture-explained/) | |\n| [Distributed Parallel Training: PyTorch](https://learnopencv.com/distributed-parallel-training-pytorch-multi-gpu-setup/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Distributed-Training-PyTorch) |\n| [MONAI: The Definitive Framework for Medical Imaging Powered by PyTorch](https://learnopencv.com/monai-medical-imaging-pytorch/) | |\n| [SANA-Sprint: The One-Step Revolution in High-Quality AI Image Synthesis](https://learnopencv.com/sana-sprint-the-one-step-revolution-in-high-quality-ai-image-synthesis/) | |\n| [FramePack-Video-Diffusion-but-feels-like-Image-Diffusion](https://learnopencv.com/framepack-video-diffusion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FramePack-Video-Diffusion-but-feels-like-Image-Diffusion) |\n| [Model Weights File Formats in Machine Learning](https://learnopencv.com/model-weights-file-formats-in-machine-learning/) | |\n| [Unsloth: A Guide from Basics to Fine-Tuning Vision Models](https://learnopencv.com/unsloth-guide-efficient-llm-fine-tuning/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Unsloth_A_Guide_From_Basics_to_Fine_Tuning_Vision_Models) |\n| [Iterative Closest Point (ICP) Algorithm Explained](https://learnopencv.com/iterative-closest-point-icp-explained/) | [Code](https://github.com/spmallick/learnopencv/blob/master/Iterative-Closest-Point-ICP) |\n| [MedSAM2 Explained: One Prompt to Segment Anything in Medical Imaging](https://learnopencv.com/medsam2-explained/) | [Code](https://github.com/spmallick/learnopencv/blob/master/medsam2-explained) |\n| [Batch Normalization and Dropout as Regularizers](https://learnopencv.com/batch-normalization-and-dropout-as-regularizers/) | |\n| [DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model](https://learnopencv.com/dinov2-self-supervised-vision-transformer/) | [Code](https://github.com/spmallick/learnopencv/blob/master/DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model) |\n| [Beginner''s Guide to Embedding Models](https://learnopencv.com/embedding-models-explained/) | |\n| [MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors](https://learnopencv.com/mast3r-slam-realtime-dense-slam-explained/) | [Code](https://github.com/spmallick/learnopencv/blob/master/MASt3R-SLAM) |\n| [Google''s A2A Protocol](https://learnopencv.com/googles-a2a-protocol-heres-what-you-need-to-know/) | |\n| [Nvidia SANA : Faster Image Generation](https://learnopencv.com/nvidia-sana-image-generation-model/) | |\n| [Fine-tuning RF-DETR](https://learnopencv.com/rf-detr-object-detection/) | [Code](https://github.com/spmallick/learnopencv/blob/master/Fine-tuning-RF-DETR) |\n| [Qwen2.5-Omni: A Real-Time Multimodal AI](https://learnopencv.com/qwen2.5-omni/) | |\n| [Vision Language Action Models: Robotic Control](https://learnopencv.com/vision-language-action-models-lerobot-policy/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Vision-Language-Action-Models) |\n| [Fine-Tuning Gemma 3 VLM using QLoRA for LaTeX-OCR Dataset](https://learnopencv.com/fine-tuning-gemma-3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Gemma-3-VLM-using-QLoRA-for-LaTeX-OCR-Dataset) |\n| [ComfyUI](https://learnopencv.com/introduction-to-comfyui-for-stable-diffusion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ComfyUI) |\n| [Gemma-3: A Comprehensive Introduction](https://learnopencv.com/gemma-3/) | |\n| [YOLO11 on Raspberry Pi: Optimizing Object Detection for Edge Devices](https://learnopencv.com/yolo11-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/tree/master/yolo11-on-raspberry-pi) |\n| [VGGT: Visual Geometry Grounded Transformer â€“ For Dense 3D Reconstruction](https://learnopencv.com/vggt-visual-geometry-grounded-transformer-3d-reconstruction/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VGGT-3D-Reconstruction) |\n| [DDIM: The Faster, Improved Version of DDPM for Efficient AI Image Generation](https://learnopencv.com/understanding-ddim/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DDIM-The-Faster-Improved-Version-of-DDPM-for-Efficient-AI-Image-Generation) |\n| [Introduction to Model Context Protocol (MCP)](https://learnopencv.com/introduction-to-model-context-protocol/) | |\n| [MASt3R and MASt3R-SfM Explanation: Image Matching and 3D Reconstruction](https://learnopencv.com/mast3r-sfm-grounding-image-matching-3d/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MASt3R-SfM-3D-Reconstruction-Image-Matching) |\n| [MatAnyone Explained: Consistent Memory for Better Video Matting](https://learnopencv.com/matanyone-for-better-video-matting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MatAnyone-Explained-Consistent-Memory-for-Better-Video-Matting) |\n| [GraphRAG: For Medical Document Analysis](https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Graphrag-Medical-Document-Analysis) |\n| [OmniParser: Vision Based GUI Agent](https://learnopencv.com/omniparser-vision-based-gui-agent/) | |\n| [Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet](https://learnopencv.com/fine-tuning-yolov12/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet) |\n| [FineTuning RetinaNet for Wildlife Detection with PyTorch: A Step-by-Step Tutorial](https://learnopencv.com/finetuning-retinanet) | [Code](https://github.com/spmallick/learnopencv/tree/master/finetuning-retinanet) |\n| [DUSt3R: Geometric 3D Vision Made Easy :  Explanation and Results](https://learnopencv.com/dust3r-geometric-3d-vision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DUSt3R-Dense-3D-Reconstruction) |\n| [YOLOv12: Attention Meets Speed](https://learnopencv.com/yolov12) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv12) |\n| [Video Generation: A Diffusion based approach](https://learnopencv.com/video-generation-models/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Video-Generation-A-Diffusion-based-approach) |\n| [Agentic AI: A Comprehensive Introduction](https://learnopencv.com/agentic-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Agentic-AI-A-Comprehensive-Introduction) |\n| [Finetuning SAM2 for Leaf Disease Segmentation](https://learnopencv.com/finetuning-sam2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/finetuning-sam2) |\n| [Object Insertion in Gaussian Splatting: Paper Explained and Training Code for MCMC and Bilateral Grid](https://learnopencv.com/object-insertion-in-gaussian-splatting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Insertion-in-Gaussian-Splatting) |\n| [Depth Pro: Sharp Monocular Metric Depth](https://learnopencv.com/depth-pro-monocular-metric-depth) | [Code](https://github.com/spmallick/learnopencv/tree/master/DepthPro-Monocular-Metric-Depth) |\n| [Fine-tuning-Stable-Diffusion-3_5-UI-images](https://learnopencv.com/fine-tuning-stable-diffusion-3-5m/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Stable-Diffusion-3_5-UI-images) |\n| [SimSiam: Streamlining SSL with Stop-Gradient Mechanism](https://learnopencv.com/simsiam/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SimSiam-Streamlining-SSL-with-Stop-Gradient-Mechanism) |\n| [Image Captioning using ResNet and LSTM](https://learnopencv.com/image-captioning/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Captioning-using-ResNet-and-LSTM) |\n| [Molmo VLM: Paper Explanation and Demo](https://learnopencv.com/molmo-vlm) | [Code](https://github.com/spmallick/learnopencv/tree/master/Molmo-VLM-SAM2) |\n| [3D Gaussian Splatting Paper Explanation: Training Custom Datasets with NeRF-Studio Gsplats](https://learnopencv.com/3d-gaussian-splatting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/3D-Gaussian-Splatting-Code) |\n| [FLUX Image Generation: Experimenting with the Parameters](https://learnopencv.com/flux-ai-image-generator/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Flux-Image-Generation) |\n| [Contrastive-Learning-SimCLR-and-BYOL(With Code Example)](https://learnopencv.com/contrastive-learning-simclr-and-byol-with-code-example/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Contrastive-Learning-SimCLR-and-BYOL) |\n| [The Annotated NeRF : Training on Custom Dataset from Scratch in Pytorch](https://learnopencv.com/annotated-nerf-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Annotated-NeRF) |\n| [Stable Diffusion 3 and 3.5: Paper Explanation and Inference](https://learnopencv.com/stable-diffusion-3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-3) |\n| [LightRAG - Legal Document Analysis](https://learnopencv.com/lightrag/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LightRAG-Legal) |\n| [NVIDIA AI Summit 2024 â€“ India Overview](https://learnopencv.com/nvidia-ai-summit-2024-india-overview/) | |\n| [Introduction to Speech to Speech: Most Efficient Form of NLP](https://learnopencv.com/speech-to-speech/) | [Code](https://github.com/spmallick/learnopencv/tree/master/speech-to-speech) |\n| [Training 3D U-Net for Brain Tumor Segmentation (BraTS-GLI)](https://learnopencv.com/3d-u-net-brats/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Training_3D_U-Net_Brain_Tumor_Seg) |\n| [DETR: Overview and Inference](https://learnopencv.com/detr-overview-and-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DETR-Overview_and_Inference) |\n| [YOLO11: Faster Than You Can Imagine!](https://learnopencv.com/yolo11/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO11) |\n| [Exploring DINO: Self-Supervised Transformers for Road Segmentation with ResNet50 and U-Net](https://learnopencv.com/fine-tune-dino-self-supervised-learning-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Exploring-DINO-dino-road-segmentation) |\n| [Sapiens: Foundation for Human Vision Models by Meta](https://learnopencv.com/sapiens-human-vision-models) | [Code](https://github.com/spmallick/learnopencv/tree/master/Sapiens-Human-Vision-Model-Meta) |\n| [Multimodal RAG with ColPali and Gemini](https://learnopencv.com/multimodal-rag-with-colpali) | [Code](https://github.com/spmallick/learnopencv/tree/master/Multimodal-RAG-with-ColPali-Gemini) |\n| [Building Autonomous Vehicle in Carla: Path Following with PID Control & ROS 2](https://learnopencv.com/pid-controller-ros-2-carla/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Building_Autonomous_Vehicle_in_Carla_Path_Following_with_PID_Control_ROS2) |\n| [Handwritten Text Recognition using OCR](https://learnopencv.com/handwritten-text-recognition-using-ocr/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Handwritten_Text_Recognition_using_OCR) |\n| [Training CLIP from Sratch for Image Retrieval](https://learnopencv.com/clip-model) | [Code](https://github.com/spmallick/learnopencv/tree/master/Training-CLIP-from-Scratch-for-Image-Retrieval) |\n| [Introduction to LiDAR SLAM: LOAM and LeGO-LOAM Paper and Code Explanation with ROS 2 Implementation](https://learnopencv.com/lidar-slam-with-ros2) | [Code](https://github.com/spmallick/learnopencv/tree/master/LeGO-LOAM-ROS2) |\n| [Recommendation System using Vector Search](https://learnopencv.com/recommendation-system-using-vector-search) | [Code](https://github.com/spmallick/learnopencv/tree/master/Recommendation-System-using-Vector-Search) |\n| [Fine Tuning Whisper on Custom Dataset](https://learnopencv.com/fine-tuning-whisper-on-custom-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Whisper-on-Custom-Dataset) |\n| [SAM 2 â€“ Promptable Segmentation for Images and Videos](https://learnopencv.com/sam-2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SAM_2_Segment_Anything_Model_2) |\n| [Introduction to Feature Matching Using Neural Networks](https://learnopencv.com/feature-matching/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Feature-Matching-Using-Neural-Networks) |\n| [Introduction to ROS2 (Robot Operating System 2): Tutorial on ROS2 Working, DDS, ROS1 RMW, Topics, Nodes, Publisher, Subscriber in Python](https://learnopencv.com/robot-operating-system-introduction) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-ROS2-in-python) |\n| [CVPR 2024 Research Papers - Part- 2](https://learnopencv.com/cvpr-2024-research-papers) | [Code](https://github.com/spmallick/learnopencv/tree/master/cvpr-2024-research-papers-part2) |\n| [CVPR 2024: An Overview and Key Papers](https://learnopencv.com/cvpr2024/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CVPR-2024) |\n| [Object Detection on Edge Device - OAK-D-Lite](https://learnopencv.com/object-detection-on-edge-device) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Detection-on-Edge-Devices) |\n| [Fine-Tuning YOLOv10 Models on Custom Dataset](https://learnopencv.com/fine-tuning-yolov10/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv10-Models-Custom-Dataset) |\n| [ROS2 and Carla Setup Guide for Ubuntu 22.04](https://learnopencv.com/ros2-and-carla-setup-guide/) |  |\n| [Understanding Visual SLAM for Robotics Perception: Building Monocular SLAM from Scratch in Python](https://learnopencv.com/monocular-slam-in-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Monocular%20SLAM%20for%20Robotics%20implementation%20in%20python) |\n| [Enhancing Image Segmentation using U2-Net: An Approach to Efficient Background Removal](https://learnopencv.com/u2-net-image-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-Background-Removal-using-U2-Net) |\n| [YOLOv10: The Dual-Head OG of YOLO Series](https://learnopencv.com/yolov10/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv10) |\n| [Fine-tuning Faster R-CNN on Sea Rescue Dataset](https://learnopencv.com/fine-tuning-faster-r-cnn/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Faster-R-CNN-on-SeaRescue-Dataset) |\n| [Mastering Recommendation System: A Complete Guide](https://learnopencv.com/recommendation-system/) | |\n| [Automatic Speech Recognition with Diarization : Speech-to-Text](https://learnopencv.com/automatic-speech-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Automatic-Speech-Recognition-with-Diarization-Speech-to-Text) |\n| [Building MobileViT Image Classification Model from Scratch In Keras 3](https://learnopencv.com/mobilevit-keras-3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Building%20MobileViT%20from%20Scratch%20in%20Keras%203) |\n| [SDXL Inpainting: Fusing Image Inpainting with Stable Diffusion](https://learnopencv.com/sdxl-inpainting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SDXL-inpainting) |\n| [YOLOv9 Instance Segmentation on Medical Dataset](https://learnopencv.com/yolov9-instance-segmentation-on-medical-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Instance-Segmentation-on-Medical-Dataset) |\n| [A Comprehensive Guide to Robotics](https://learnopencv.com/a-comprehensive-guide-to-robotics/) | |\n| [Integrating Gradio with OpenCV DNN](https://learnopencv.com/integrating-gradio-with-opencv-dnn/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Integrating-Gradio-with-OpenCV-DNN) |\n| [Fine-Tuning YOLOv9 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov9/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv9-Models-Custom-Dataset) |\n| [Dreambooth using Diffusers](https://learnopencv.com/dreambooth-using-diffusers/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Dreambooth_using_Diffusers) |\n| [Introduction to Hugging Face Diffusers](https://learnopencv.com/hugging-face-diffusers/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction_to_Diffusers) |\n| [Introduction to Ultralytics Explorer API](https://learnopencv.com/ultralytics-explorer-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Ultralytics-Explorer-API) |\n| [YOLOv9: Advancing the YOLO Legacy](https://learnopencv.com/yolov9-advancing-the-yolo-legacy/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Advancing-the-YOLO-Legacy) |\n| [Fine-Tuning LLMs using PEFT](https://learnopencv.com/fine-tuning-llms-using-peft/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-LLMs-using-PEFT) |\n| [Depth Anything: Accelerating Monocular Depth Perception](https://learnopencv.com/deciphering-llms/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Depth-Anything) |\n| [Deciphering LLMs: From Transformers to Quantization](https://learnopencv.com/deciphering-llms/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Deciphering-LLMs) |\n| [YOLO Loss Function Part 2: GFL and VFL Loss](https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part2) |\n| [YOLOv8-Object-Tracking-and-Counting-with-OpenCV](https://learnopencv.com/yolov8-object-tracking-and-counting-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv8-Object-Tracking-and-Counting-with-OpenCV) |\n| [Stereo Vision in ADAS: Pioneering Depth Perception Beyond LiDAR](https://learnopencv.com/adas-stereo-vision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ADAS-Stereo-Vision) |\n| [YOLO Loss Function Part 1: SIoU and Focal Loss](https://learnopencv.com/yolo-loss-function-siou-focal-loss/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part1) |\n| [Moving Object Detection with OpenCV](https://learnopencv.com/moving-object-detection-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Moving-Object-Detection-with-OpenCV) |\n| [Integrating ADAS with Keypoint Feature Pyramid Network for 3D LiDAR Object Detection](https://learnopencv.com/3d-lidar-object-detection/) | [Code](https://www.dropbox.com/scl/fi/3n1s68jtfkjmw2f5e5ctv/3D-LiDAR-Object-Detection.zip?rlkey=d8q6xvlxis4oxso4qki87omvc&dl=1) |\n| [Mastering All YOLO Models from YOLOv1 to YOLO-NAS: Papers Explained (2024)](https://learnopencv.com/mastering-all-yolo-models) | |\n| [GradCAM: Enhancing Neural Network Interpretability in the Realm of Explainable AI](https://learnopencv.com/intro-to-gradcam/) | [Code](https://www.dropbox.com/scl/fo/3p3sg5fnvhrvi9vp00i0w/h?rlkey=1x01uz5o7esex7p6c8r534iyn&dl=1) |\n| [Text Summarization using T5: Fine-Tuning and Building Gradio App](https://learnopencv.com/text-summarization-using-t5/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Text-Summarization-using-T5-Fine-Tuning-and-Building-Gradio-App) |\n| [3D LiDAR Visualization using Open3D: A Case Study on 2D KITTI Depth Frames for Autonomous Driving](https://learnopencv.com/3d-lidar-visualization/) | [Code](https://github.com/spmallick/learnopencv/tree/master/3D-LiDAR-Perception) |\n| [Fine Tuning T5: Text2Text Transfer Transformer for Building a Stack Overflow Tag Generator](https://learnopencv.com/fine-tuning-t5/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-T5-Text2Text-Transformer-for-Strack-Overflow-Tag-Generation) |\n| [SegFormer ðŸ¤— : Fine-Tuning for Improved Lane Detection in Autonomous Vehicles](https://learnopencv.com/segformer-fine-tuning-for-lane-detection) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-SegFormer-For-Lane-Detection) |\n| [Fine-Tuning BERT using Hugging Face Transformers](https://learnopencv.com/fine-tuning-bert) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-BERT-using-Hugging-Face-Transformers) |\n| [YOLO-NAS Pose](https://learnopencv.com/yolo-nas-pose) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS-Pose) |\n| [BERT: Bidirectional Encoder Representations from Transformers](https://learnopencv.com/bert-bidirectional-encoder-representations-from-transformers/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BERT-Bidirectional-Encoder-Representations-from-Transformers) |\n| [Comparing KerasCV YOLOv8 Models on the Global Wheat Data 2020](https://learnopencv.com/comparing-kerascv-yolov8-models/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Comparing-KerasCV-YOLOv8-Models-on-the-Global-Wheat-Data-2020) |\n| [Top 5 AI papers of September 2023](https://learnopencv.com/top-5-ai-papers-of-september-2023/) | |\n| [Empowering Drivers: The Rise and Role of Advanced Driver Assistance Systems](https://learnopencv.com/advanced-driver-assistance-systems/) | |\n| [Semantic Segmentation using KerasCV DeepLabv3+](https://learnopencv.com/kerascv-deeplabv3-plus-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Semantic-Segmentation-using-KerasCV-with-DeepLabv3-Plus) |\n| [Object Detection using KerasCV YOLOv8](https://learnopencv.com/object-detection-using-kerascv-yolov8/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-KerasCV-YOLOv8) |\n| [Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation](https://learnopencv.com/animal-pose-estimation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-YOLOv8-Pose-Models-for-Animal-Pose-Estimation) |\n| [Top 5 AI papers of August 2023](https://learnopencv.com/top-5-ai-papers-of-august-2023/) | |\n| [Fine Tuning TrOCR - Training TrOCR to Recognize Curved Text](https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-TrOCR) |\n| [TrOCR - Getting Started with Transformer Based OCR](https://learnopencv.com/trocr-getting-started-with-transformer-based-ocr/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TrOCR-Getting-Started-with-Transformer-Based-OCR) |\n| [Facial Emotion Recognition](https://learnopencv.com/facial-emotion-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Facial-Emotion-Recognition) |\n| [Object Keypoint Similarity in Keypoint Detection](https://learnopencv.com/object-keypoint-similarity/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Keypoint-Similarity-in-Keypoint-Detection) |\n| [Real Time Deep SORT with Torchvision Detectors](https://learnopencv.com/real-time-deep-sort-with-torchvision-detectors/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Real_Time_Deep_SORT_using_Torchvision_Detectors) |\n| [Top 5 AI papers of July 2023](https://learnopencv.com/top-5-ai-papers-of-july-2023/) | |\n| [Medical Image Segmentation](https://learnopencv.com/medical-image-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Medical-Image-Segmentation-Using-HuggingFace-&-PyTorch) |\n| [Weighted Boxes Fusion in Object Detection: A Comparison with Non-Maximum Suppression](https://learnopencv.com/weighted-boxes-fusion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Weighted-Boxes-Fusion-in-Object-Detection) |\n| [Medical Multi-label Classification with PyTorch & Lightning](https://learnopencv.com/medical-multi-label/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Medical_Multi-label_Classification_with_PyTorch_&_Lightning) |\n| [Getting Started with PaddlePaddle: Exploring Object Detection, Segmentation, and Keypoints](https://learnopencv.com/paddlepaddle/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-PaddlePaddle) |\n| [Drone Programming With Computer Vision A Beginners Guide](https://learnopencv.com/drone-programming-with-computer-vision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Drone-Programming-With-Computer-Vision-A-Beginners-Guide) |\n| [How to Build a Pip Installable Package & Upload to PyPi](https://learnopencv.com/building-pip-installable-package-pypi/) | |\n| [IoU Loss Functions for Faster & More Accurate Object Detection](https://learnopencv.com/iou-loss-functions-object-detection/) | |\n| [Exploring Slicing Aided Hyper Inference for Small Object Detection](https://learnopencv.com/slicing-aided-hyper-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Exploring-Slicing-Aided-Hyper-Inference) |\n| [Advancements in Face Recognition Models, Toolkit and Datasets](https://learnopencv.com/face-recognition-models/) | |\n| [Train YOLO NAS on Custom Dataset](https://learnopencv.com/train-yolo-nas-on-custom-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Train-YOLO-NAS-on-Custom-Dataset) |\n| [Train YOLOv8 Instance Segmentation on Custom Data](https://learnopencv.com/train-yolov8-instance-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-Instance-Segmentation-on-Custom-Data) |\n| [YOLO-NAS: New Object Detection Model Beats YOLOv6 & YOLOv8](https://learnopencv.com/yolo-nas/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS_Introduction) |\n| [Segment Anything â€“ A Foundation Model for Image Segmentation](https://learnopencv.com/segment-anything/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Segment-Anything-A-Foundation-Model-for-Image-Segmentation) |\n|[Build a Video to Slides Converter Application using the Power of Background Estimation and Frame Differencing in OpenCV](https://learnopencv.com/video-to-slides-converter-using-background-subtraction/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Build-a-Video-to-Slides-Converter-Application-using-the-Power-of-Background-Estimation-and-Frame-Differencing-in-OpenCV)|\n|[A Closer Look at CVAT: Perfecting Your Annotations](https://learnopencv.com/a-closer-look-at-cvat-perfecting-your-annotations/)|[YouTube](https://www.youtube.com/watch?v=yxX_0-zr-2U&list=PLfYPZalDvZDLvFhjuflhrxk_lLplXUqqB)|\n| [ControlNet - Achieving Superior Image Generation Results](https://learnopencv.com/controlnet/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ControlNet-Achieving-Superior-Image-Generation-Results) |\n| [InstructPix2Pix - Edit Images With Prompts](https://learnopencv.com/instructpix2pix/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstructPix2Pix-Edit-Images-With-Prompts) |\n| [NVIDIA Spring GTC 2023 Day 4: Ending on a High Note with Top Moments from the Finale!](https://learnopencv.com/nvidia-spring-gtc-2023-day-4/) | |\n| [NVIDIA Spring GTC 2023 Day 3: Digging deeper into Deep Learning, Semiconductors & more!](https://learnopencv.com/nvidia-spring-gtc-2023-day-3-digging-deeper-into-deep-learning-semiconductors-more/) | |\n| [NVIDIA Spring GTC 2023 Day 2: Jensenâ€™s keynote & the iPhone moment of AI is here!](https://learnopencv.com/nvidia-spring-gtc-2023-day-2-jensens-keynote-the-iphone-moment-of-ai-is-here/) | |\n| [NVIDIA Spring GTC 2023 Day 1: Welcome to the future!](https://learnopencv.com/nvidia-spring-gtc-2023-day-1-highlights-welcome-to-the-future/) | |\n| [NVIDIA GTC Spring 2023 Curtain Raiser](https://learnopencv.com/nvidia-gtc-spring-2023-curtain-raiser/) | |\n| [Stable Diffusion - A New Paradigm in Generative AI](https://learnopencv.com/stable-diffusion-generative-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-A-New-Paradigm-in-Generative-AI) |\n| [OpenCV Face Recognition â€“ Does Face Recognition Work on AI-Generated Images?](https://learnopencv.com/opencv-face-recognition-api/) | |\n|[An In-Depth Guide to Denoising Diffusion Probabilistic Models â€“ From Theory to Implementation](https://learnopencv.com/denoising-diffusion-probabilistic-models/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Guide-to-training-DDPMs-from-Scratch)|\n|[From Pixels to Paintings: The Rise of Midjourney AI Art](https://learnopencv.com/rise-of-midjourney-ai-art/)| |\n|[Mastering DALLÂ·E 2: A Breakthrough in AI Art Generation](https://learnopencv.com/mastering-dall-e-2/)| |\n|[Top 10 AI Art Generation Tools using Diffusion Models](https://learnopencv.com/ai-art-generation-tools/)| |\n|[The Future of Image Recognition is Here: PyTorch Vision Transformer](https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Vision_Transformer_PyTorch)|\n|[Understanding Attention Mechanism in Transformer Neural Networks](https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Attention_Mechanism_Introduction)|\n| [Deploying a Deep Learning Model using Hugging Face Spaces and Gradio](https://learnopencv.com/deploy-deep-learning-model-huggingface-spaces/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Deploying-a-Deep-Learning-Model-using-Hugging-Face-Spaces-and-Gradio) |\n| [Train YOLOv8 on Custom Dataset â€“ A Complete Tutorial](https://learnopencv.com/train-yolov8-on-custom-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-on-Custom-Dataset-A-Complete-Tutorial) |\n| [Introduction to Diffusion Models for Image Generation](https://learnopencv.com/image-generation-using-diffusion-models/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Diffusion-Models-for-Image-Generation) |\n| [Building An Automated Image Annotation Tool: PyOpenAnnotate](https://learnopencv.com/building-automated-image-annotation-tool-pyopenannotate/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Building-An-Automated-Image-Annotation-Tool-PyOpenAnnotate/) |\n| [Ultralytics YOLOv8: State-of-the-Art YOLO Models](https://learnopencv.com/ultralytics-yolov8/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Ultralytics-YOLOv8-State-of-the-Art-YOLO-Models) |\n| [Getting Started with YOLOv5 Instance Segmentation](https://learnopencv.com/getting-started-with-yolov5-instance-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-with-YOLOv5-Instance-Segmentation) |\n|[The Ultimate Guide To DeepLabv3 - With PyTorch Inference](https://learnopencv.com/deeplabv3-ultimate-guide/)|[Code](https://github.com/spmallick/learnopencv/tree/master/The-ultimate-guide-to-deeplabv3)|\n|[AI Fitness Trainer using MediaPipe: Squats Analysis](https://learnopencv.com/ai-fitness-trainer-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/AI-Fitness-Trainer-Using-MediaPipe-Analyzing-Squats)|\n|[YoloR - Paper Explanation & Inference -An In-Depth Analysis](https://learnopencv.com/yolor-paper-explanation-inference-an-in-depth-analysis/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YoloR-paper-explanation-analysis)|\n|[Roadmap To an Automated Image Annotation Tool Using Python](https://learnopencv.com/automated-image-annotation-tool-using-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Roadmap-To-an-Automated-Image-Annotation-Tool-Using-Python)|\n|[Performance Comparison of YOLO Object Detection Models â€“ An Intensive Study](https://learnopencv.com/performance-comparison-of-yolo-models/)||\n|[FCOS - Anchor Free Object Detection Explained](https://learnopencv.com/fcos-anchor-free-object-detection-explained/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FCOS-Inference-using-PyTorch)|\n| [YOLOv6 Custom Dataset Training â€“ Underwater Trash Detection](https://learnopencv.com/yolov6-custom-dataset-training/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Custom-Dataset-Training-Underwater-Trash-Detection) |\n|[What is EXIF Data in Images?](https://www.learnopencv.com/what-is-exif-data-in-images/)|[Code](https://github.com/spmallick/learnopencv/tree/master/What-is-EXIF-Data-in-Images)|\n|[t-SNE: T-Distributed Stochastic Neighbor Embedding Explained](https://learnopencv.com/t-sne-t-distributed-stochastic-neighbor-embedding-explained/)|[Code](https://github.com/spmallick/learnopencv/tree/master/t-SNE-with-Tensorboard)|\n|[CenterNet: Objects as Points â€“ Anchor-free Object Detection Explained](https://learnopencv.com/centernet-anchor-free-object-detection-explained/)|[Code](https://github.com/spmallick/learnopencv/tree/master/centernet-with-tf-hub)|\n|[YOLOv7 Pose vs MediaPipe in Human Pose Estimation](https://learnopencv.com/yolov7-pose-vs-mediapipe-in-human-pose-estimation/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Pose-vs-MediaPipe-in-Human-Pose-Estimation)|\n|[YOLOv6 Object Detection â€“ Paper Explanation and Inference](https://learnopencv.com/yolov6-object-detection/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Object-Detection-Paper-Explanation-and-Inference)|\n|[YOLOX Object Detector Paper Explanation and Custom Training](https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training)|\n|[Driver Drowsiness Detection Using Mediapipe In Python](https://learnopencv.com/driver-drowsiness-detection-using-mediapipe-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Driver-Drowsiness-detection-using-Mediapipe-in-Python)|\n|[GTC 2022 Big Bang AI announcements: Everything you need to know](https://learnopencv.com/gtc-2022-big-bang-ai-announcements-everything-you-need-to-know/)||\n|[NVIDIA GTC 2022 : The most important AI event this Fall](https://learnopencv.com/nvidia-gtc-2022-the-most-important-ai-event-this-fall/)||\n|[Object Tracking and Reidentification with FairMOT](https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Tracking-and-Reidentification-with-FairMOT) |\n|[What is Face Detection? â€“ The Ultimate Guide for 2022](https://learnopencv.com/what-is-face-detection-the-ultimate-guide/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Face-Detection-Ultimate-Guide) |\n|[Document Scanner: Custom Semantic Segmentation using PyTorch-DeepLabV3](https://learnopencv.com/custom-document-segmentation-using-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Document-Scanner-Custom-Semantic-Segmentation-using-PyTorch-DeepLabV3)|\n|[Fine Tuning YOLOv7 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv7)|\n|[Center Stage for Zoom Calls using MediaPipe](https://learnopencv.com/Center-Stage-for-zoom-call-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/CenterStage)|\n|[Mean Average Precision (mAP) in Object Detection](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/)||\n|[YOLOv7 Object Detection Paper Explanation and Inference](https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Object-Detection-Paper-Explanation-and-Inference)|\n|[Pothole Detection using YOLOv4 and Darknet](https://learnopencv.com/pothole-detection-using-yolov4-and-darknet/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pothole-Detection-using-YOLOv4-and-Darknet)|\n|[Automatic Document Scanner using OpenCV](https://learnopencv.com/automatic-document-scanner-using-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Automatic-Document-Scanner)|\n|[Demystifying GPU architectures for deep learning: Part 2](https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning-part-2/)|[Code](https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA)|\n|[Demystifying GPU Architectures For Deep Learning](https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA)|\n|[Intersection-over-Union(IoU)-in-Object-Detection-and-Segmentation](https://learnopencv.com/intersection-over-unioniou-in-object-detection-and-segmentation/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Intersection-over-Union-IoU-in-Object-Detection-and-Segmentation)|\n|[Understanding Multiple Object Tracking using DeepSORT](https://learnopencv.com/understanding-multiple-object-tracking-using-deepsort/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Understanding-Multiple-Object-Tracking-using-DeepSORT)|\n|[Optical Character Recognition using PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Optical-Character-Recognition-using-PaddleOCR)|\n|[Gesture Control in Zoom Call using Mediapipe](https://learnopencv.com/gesture-control-in-zoom-call-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/zoom-gestures)|\n|[A Deep Dive into Tensorflow Model Optimization](https://learnopencv.com/deep-dive-into-tensorflow-model-optimization-toolkit/)|[Code](https://github.com/spmallick/learnopencv/tree/master/A-Deep-Dive-into-Tensorflow-Model-Optimization)|\n|[DepthAI Pipeline Overview: Creating a Complex Pipeline](https://learnopencv.com/depthai-pipeline-overview-creating-a-complex-pipeline/)|[Code](https://github.com/spmallick/learnopencv/tree/master/OAK-DepthAi-Pipeline-Overview)|\n|[TensorFlow Lite Model Maker: Create Models for On-Device Machine Learning](https://learnopencv.com/tensorflow-lite-model-maker-create-models-for-on-device-machine-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Tensorflow-Lite-Model-Maker-Create-Models-for-On-Device-ML)|\n|[TensorFlow Lite: Model Optimization for On Device Machine Learning](https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Lite-Model-Optimization-for-On-Device-MachineLearning)|\n|[Object detection with depth measurement using pre-trained models with OAK-D](https://learnopencv.com/object-detection-with-depth-measurement-with-oak-d/)|[Code](https://github.com/spmallick/learnopencv/tree/master/OAK-Object-Detection-with-Depth)|\n|[Custom Object Detection Training using YOLOv5](https://learnopencv.com/custom-object-detection-training-using-yolov5/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Custom-Object-Detection-Training-using-YOLOv5)|\n|[Object Detection using Yolov5 and OpenCV DNN (C++/Python)](https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python)|\n|[Create Snapchat/Instagram filters using Mediapipe](https://learnopencv.com/create-snapchat-instagram-filters-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Create-AR-filters-using-Mediapipe)|\n|[AUTOSAR C++ compliant deep learning inference with TensorRT](https://learnopencv.com/autosar-c-compliant-deep-learning-inference-with-tensorrt/)|[Code](https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_cpp)|\n|[NVIDIA GTC 2022 Day 4 Highlights: Meet the new Jetson Orin](https://learnopencv.com/nvidia-gtc-2022-day-4-highlights-meet-the-new-jetson-orin/)||\n|[NVIDIA GTC 2022 Day 3 Highlights: Deep Dive into Hopper architecture](https://learnopencv.com/nvidia-gtc-2022-day-3-highlights-deep-dive-into-hopper-architecture/)||\n|[NVIDIA GTC 2022 Day 2 Highlights: Jensenâ€™s Keynote](https://learnopencv.com/nvidia-gtc-2022-day-2-highlights/)||\n|[NVIDIA GTC 2022 Day 1 Highlights: Brilliant Start](https://learnopencv.com/gtc-day-1-highlights/)||\n|[Automatic License Plate Recognition using Python](https://learnopencv.com/automatic-license-plate-recognition-using-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ALPR)|\n|[Building a Poor Body Posture Detection and Alert System using MediaPipe](https://learnopencv.com/building-a-body-posture-analysis-system-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Posture-analysis-system-using-MediaPipe-Pose)|\n|[Introduction to MediaPipe](https://learnopencv.com/introduction-to-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-MediaPipe)|\n|[Disparity Estimation using Deep Learning](https://learnopencv.com/disparity-estimation-using-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Disparity-Estimation-Using-Deep-Learning)|\n|[How to build Chrome Dino game bot using OpenCV Feature Matching](https://learnopencv.com/how-to-build-chrome-dino-game-bot-using-opencv-feature-matching/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Chrome-Dino-Bot-using-OpenCV-feature-matching)|\n|[Top 10 Sources to Find Computer Vision and AI Models](https://learnopencv.com/top-10-sources-to-find-computer-vision-and-ai-models/)||\n|[Multi-Attribute and Graph-based Object Detection](https://learnopencv.com/multi-attribute-and-graph-based-object-detection/)||\n|[Plastic Waste Detection with Deep Learning](https://learnopencv.com/plastic-waste-detection-with-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Plastic-Waste-Detection-with-Deep-Learning)|\n|[Ensemble Deep Learning-based Defect Classification and Detection in SEM Images](https://learnopencv.com/ensemble-deep-learning-based-defect-classification-and-detection-in-sem-images/)||\n|[Building Industrial embedded deep learning inference pipelines with TensorRT](https://learnopencv.com/building-industrial-embedded-deep-learning-inference-pipelines-with-tensorrt/)|[Code](https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_python)|\n|[Transfer Learning for Medical Images](https://learnopencv.com/transfer-learning-for-medical-images/)||\n|[Stereo Vision and Depth Estimation using OpenCV AI Kit](https://learnopencv.com/stereo-vision-and-depth-estimation-using-opencv-ai-kit/)|[Code](https://github.com/spmallick/learnopencv/tree/master/oak-getting-started)|\n|[Introduction to OpenCV AI Kit and DepthAI](https://learnopencv.com/introduction-to-opencv-ai-kit-and-depthai/)|[Code](https://github.com/spmallick/learnopencv/tree/master/oak-getting-started)|\n|[WeChat QR Code Scanner in OpenCV](https://learnopencv.com/wechat-qr-code-scanner-in-opencv)|[Code](https://github.com/spmallick/learnopencv/tree/master/WeChat-QRCode-Scanner-OpenCV)|\n|[AI behind the Diwali 2021 â€˜Not just a Cadbury adâ€™](https://learnopencv.com/ai-behind-the-diwali-2021-not-just-a-cadbury-ad/)| |\n|[Model Selection and Benchmarking with Modelplace.AI](https://learnopencv.com/model-selection-and-benchmarking-with-modelplace-ai/)|[Model Zoo](https://modelplace.ai/)|\n|[Real-time style transfer in a zoom meeting](https://learnopencv.com/real-time-style-transfer-in-a-zoom-meeting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/style-transfer-zoom)|\n| [Introduction to OpenVino Deep Learning Workbench](https://learnopencv.com/introduction-to-openvino-deep-learning-workbench/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-OpenVino-Deep-Learning-Workbench) |\n| [Running OpenVino Models on Intel Integrated GPU](https://learnopencv.com/running-openvino-models-on-intel-integrated-gpu/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Running-OpenVino-Models-on-Intel-Integrated-GPU) |\n|[Post Training Quantization with OpenVino Toolkit](https://learnopencv.com/post-training-quantization-with-openvino-toolkit/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Post-Training-Quantization-with-OpenVino-Toolkit)|\n|[Introduction to Intel OpenVINO Toolkit](https://learnopencv.com/introduction-to-intel-openvino-toolkit/)||\n|[Human Action Recognition using Detectron2 and LSTM](https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Human-Action-Recognition-Using-Detectron2-And-Lstm)|\n|[Pix2Pix:Image-to-Image Translation in PyTorch & TensorFlow](https://learnopencv.com/paired-image-to-image-translation-pix2pix/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Image-to-Image-Translation-with-GAN)|\n|[Conditional GAN (cGAN) in PyTorch and TensorFlow](https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Conditional-GAN-PyTorch-TensorFlow)|\n|[Deep Convolutional GAN in PyTorch and TensorFlow](https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Deep-Convolutional-GAN)|\n|[Introduction to Generative Adversarial Networks (GANs)](https://learnopencv.com/introduction-to-generative-adversarial-networks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Intro-to-Generative-Adversarial-Network)|\n|[Human Pose Estimation using Keypoint RCNN in PyTorch](https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Keypoint-RCNN)|\n|[Non Maximum Suppression: Theory and Implementation in PyTorch](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch)|[Code](https://github.com/spmallick/learnopencv/tree/master/Non-Maximum-Suppression)|\n|[MRNet â€“ The Multi-Task Approach](https://learnopencv.com/mrnet-multitask-approach/)| [Code](https://github.com/spmallick/learnopencv/tree/master/MRnet-MultiTask-Approach) |\n|[Generative and Discriminative Models](https://learnopencv.com/generative-and-discriminative-models/)| |\n|[Playing Chrome''s T-Rex Game with Facial Gestures](https://learnopencv.com/playing-chromes-t-rex-game-with-facial-gestures/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-Chrome-TRex-Game-with-Facial-Gestures) |\n|[Variational Autoencoder in TensorFlow](https://learnopencv.com/variational-autoencoder-in-tensorflow/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Variational-Autoencoder-TensorFlow) |\n|[Autoencoder in TensorFlow 2: Beginnerâ€™s Guide](https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Autoencoder-in-TensorFlow) |\n|[Deep Learning with OpenCV DNN Module: A Definitive Guide](https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Deep-Learning-with-OpenCV-DNN-Module) |\n|[Depth perception using stereo camera (Python/C++)](https://learnopencv.com/depth-perception-using-stereo-camera-python-c/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Depth-Perception-Using-Stereo-Camera) |\n|[Contour Detection using OpenCV (Python/C++)](https://learnopencv.com/contour-detection-using-opencv-python-c/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Contour-Detection-using-OpenCV) |\n|[Super Resolution in OpenCV](https://learnopencv.com/super-resolution-in-opencv/)| [Code](https://github.com/spmallick/learnopencv/blob/master/Super-Resolution-in-OpenCV) |\n|[Improving Illumination in Night Time Images](https://learnopencv.com/improving-illumination-in-night-time-images/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Improving-Illumination-in-Night-Time-Images) |\n|[Video Classification and Human Activity Recognition](https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/video-classification-and-human-activity-recognition) |\n|[How to use OpenCV DNN Module with Nvidia GPU on Windows](https://learnopencv.com/how-to-use-opencv-dnn-module-with-nvidia-gpu-on-windows) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Windows) |\n|[How to use OpenCV DNN Module with NVIDIA GPUs](https://learnopencv.com/opencv-dnn-with-gpu-support/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Linux) |\n|[Code OpenCV in Visual Studio](https://learnopencv.com/code-opencv-in-visual-studio/) | |\n|[Install OpenCV on Windows â€“ C++ / Python](https://learnopencv.com/install-opencv-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Install-OpenCV-Windows-exe) |\n|[Face Recognition with ArcFace](https://www.learnopencv.com/face-recognition-with-arcface/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Face-Recognition-with-ArcFace)|\n|[Background Subtraction with OpenCV and BGS Libraries](https://www.learnopencv.com/background-subtraction-with-opencv-and-bgs-libraries/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Background-Subtraction) |\n|[RAFT: Optical Flow estimation using Deep Learning](https://learnopencv.com/optical-flow-using-deep-learning-raft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT)|\n|[Making A Low-Cost Stereo Camera Using OpenCV](https://www.learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/stereo-camera)|\n|[Optical Flow in OpenCV (C++/Python)](https://www.learnopencv.com/optical-flow-in-opencv)|[Code](https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-in-OpenCV)|\n|[Introduction to Epipolar Geometry and Stereo Vision](https://www.learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/)|[Code](https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision)|\n|[Classification With Localization: Convert any keras Classifier to a Detector](https://www.learnopencv.com/classification-with-localization/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Classification-with-localization-convert-any-keras-classifier-into-a-detector/README.md) |\n|[Photoshop Filters in OpenCV](https://www.learnopencv.com/photoshop-filters-in-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Photoshop-Filters-in-OpenCV)|\n|[Tetris Game using OpenCV Python](https://www.learnopencv.com/tetris-with-opencv-python)|[Code](https://github.com/spmallick/learnopencv/tree/master/Tetris)|\n|[Image Classification with OpenCV for Android](https://www.learnopencv.com/image-classification-with-opencv-for-android/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android) |\n|[Image Classification with OpenCV Java](https://www.learnopencv.com/image-classification-with-opencv-java)|[Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java) |\n|[PyTorch to Tensorflow Model Conversion](https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion) |\n|[Snake Game with OpenCV Python](https://www.learnopencv.com/snake-game-with-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SnakeGame) |\n|[Stanford MRNet Challenge: Classifying Knee MRIs](https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/)|[Code](https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model) |\n|[Experiment Logging with TensorBoard and wandb](https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging) |\n|[Understanding Lens Distortion](https://www.learnopencv.com/understanding-lens-distortion/)|[Code](https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion) |\n|[Image Matting with state-of-the-art Method â€œF, B, Alpha Mattingâ€](https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FBAMatting) |\n|[Bag Of Tricks For Image Classification - Let''s check if it is working or not](https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification) |\n|[Getting Started with OpenCV CUDA Module](https://www.learnopencv.com/getting-started-opencv-cuda-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module) |\n|[Training a Custom Object Detector with DLIB & Making Gesture Controlled Applications](https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib) |\n|[How To Run Inference Using TensorRT C++ API](https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP) |\n|[Using Facial Landmarks for Overlaying Faces with Medical Masks](https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay) |\n|[Tensorboard with PyTorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning) |\n|[Otsu''s Thresholding with OpenCV](https://www.learnopencv.com/otsu-thresholding-with-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/otsu-method) |\n|[PyTorch-to-CoreML-model-conversion](https://www.learnopencv.com/pytorch-to-coreml-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion) |\n|[Playing Rock, Paper, Scissors with AI](https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI) |\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[CNN Fully Convolutional Image Classification with TensorFlow](https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow) | [Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification) |\n|[How to convert a model from PyTorch to TensorRT and speed up inference](https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT) |\n|[Efficient image loading](https://www.learnopencv.com/efficient-image-loading/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading) |\n|[Graph Convolutional Networks: Model Relations In Data](https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data)|\n|[Getting Started with Federated Learning with PyTorch and PySyft](https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro)|\n|[Creating a Virtual Pen & Eraser](http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser) |\n|[Getting Started with PyTorch Lightning](https://www.learnopencv.com/getting-started-with-pytorch-lightning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning)|\n|[Multi-Label Image Classification with PyTorch: Image Tagging](https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging)|\n|[Funny Mirrors Using OpenCV](https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/)|[code](https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors)|\n|[t-SNE for ResNet feature visualization](https://www.learnopencv.com/t-sne-for-feature-visualization/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TSNE)|\n|[Multi-Label Image Classification with Pytorch](https://www.learnopencv.com/multi-label-image-classification-with-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification)|\n|[CNN Receptive Field Computation Using Backprop](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop)|\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[Augmented Reality using AruCo Markers in OpenCV(C++ and Python)](https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/) |[Code](https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers)|\n|[Fully Convolutional Image Classification on Arbitrary Sized Image](https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification)|\n|[Camera Calibration using OpenCV](https://www.learnopencv.com/camera-calibration-using-opencv/) |[Code](https://github.com/spmallick/learnopencv/tree/master/CameraCalibration)|\n|[Geometry of Image Formation](https://www.learnopencv.com/geometry-of-image-formation/) ||\n|[Ensuring Training Reproducibility in Pytorch](https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch) ||\n|[Gaze Tracking](https://www.learnopencv.com/gaze-tracking/) ||\n|[Simple Background Estimation in Videos Using OpenCV](https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation)|\n|[Applications of Foreground-Background separation with Semantic Segmentation](https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg) |\n|[EfficientNet: Theory + Code](https://www.learnopencv.com/efficientnet-theory-code) | [Code](https://github.com/spmallick/learnopencv/tree/master/EfficientNet) |\n|[PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch](https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/) | [Code](./PyTorch-Mask-RCNN) |\n|[PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN) |\n|[PyTorch for Beginners: Semantic Segmentation using torchvision](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision) |\n|[PyTorch for Beginners: Comparison of pre-trained models for Image Classification](https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb) |\n|[PyTorch for Beginners: Basics](https://www.learnopencv.com/pytorch-for-beginners-basics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb) |\n|[PyTorch Model Inference using ONNX and Caffe2](https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2) |\n|[Image Classification Using Transfer Learning in PyTorch](https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch) |\n|[Hangman: Creating games in OpenCV](https://www.learnopencv.com/hangman-creating-games-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hangman) |\n|[Image Inpainting with OpenCV (C++/Python)](https://www.learnopencv.com/image-inpainting-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting) |\n|[Hough Transform with OpenCV (C++/Python)](https://www.learnopencv.com/hough-transform-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hough-Transform) |\n|[Xeus-Cling: Run C++ code in Jupyter Notebook](https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/) | [Code](https://github.com/spmallick/learnopencv/tree/master/XeusCling) |\n|[Gender & Age Classification using OpenCV Deep Learning ( C++/Python )](https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AgeGender) |\n|[Invisibility Cloak using Color Detection and Segmentation with OpenCV](https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak) |\n|[Fast Image Downloader for Open Images V4 (Python)](https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/) | [Code](https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages) |\n|[Deep Learning based Text Detection Using OpenCV (C++/Python)](https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST) |\n|[Video Stabilization Using Point Feature Matching in OpenCV](https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoStabilization) |\n|[Training YOLOv3 : Deep Learning based Custom Object Detector](https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector ) |\n|[Using OpenVINO with OpenCV](https://www.learnopencv.com/using-openvino-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV) |\n|[Duplicate Search on Quora Dataset](https://www.learnopencv.com/duplicate-search-on-quora-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search) |\n|[Shape Matching using Hu Moments (C++/Python)](https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HuMoments) |\n|[Install OpenCV 4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-red-hat.sh) |\n|[Install OpenCV 4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-red-hat.sh) |\n|[Install OpenCV 4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh) |\n|[Install OpenCV 3.4.4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-raspberry-pi.sh) |\n|[Install OpenCV 3.4.4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-macos.sh) |\n|[OpenCV QR Code Scanner (C++ and Python)](https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV) |\n|[Install OpenCV 3.4.4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3) |\n|[Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh) |\n|[Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh) |\n|[Universal Sentence Encoder](https://www.learnopencv.com/universal-sentence-encoder) | [Code](https://github.com/spmallick/learnopencv/blob/master/Universal-Sentence-Encoder) |\n|[Install OpenCV 4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-raspberry-pi.sh) |\n|[Install OpenCV 4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4) |\n|[Face Detection â€“ Dlib, OpenCV, and Deep Learning ( C++ / Python )](https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceDetectionComparison)|\n|[Hand Keypoint Detection using Deep Learning and OpenCV](https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HandPose)|\n|[Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)](https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN) |\n|[Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh) |\n|[Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh) |\n|[Multi-Person Pose Estimation in OpenCV using OpenPose](https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person) |\n|[Heatmap for Logo Detection using OpenCV (Python)](https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/heatmap)|\n|[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO)|\n|[Convex Hull using OpenCV in Python and C++](https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ConvexHull)|\n|[MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)](https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker) |\n|[Convolutional Neural Network based Image Colorization using OpenCV](https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colorization)|\n|[SVM using scikit-learn](https://www.learnopencv.com/svm-using-scikit-learn-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[GOTURN: Deep Learning based Object Tracking](https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/) | [Code](https://github.com/spmallick/learnopencv/tree/master/GOTURN)|\n|[Find the Center of a Blob (Centroid) using OpenCV (C++/Python)](https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CenterofBlob)|\n|[Support Vector Machines (SVM)](https://www.learnopencv.com/support-vector-machines-svm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[Batch Normalization in Deep Networks](https://www.learnopencv.com/batch-normalization-in-deep-networks/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BatchNormalization)|\n|[Deep Learning based Character Classification using Synthetic Dataset](https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CharClassification)|\n|[Image Quality Assessment : BRISQUE](https://www.learnopencv.com/image-quality-assessment-brisque/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageMetrics)|\n|[Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)||\n|[Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV](https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/OCR)|\n|[Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )](https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose)|\n|[Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)](https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/)| |\n|[How to convert your OpenCV C++ code into a Python module](https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/pymodule)|\n|[CV4Faces : Best Project Award 2018](https://www.learnopencv.com/cv4faces-best-project-award-2018/)| |\n|[Facemark : Facial Landmark Detection using OpenCV](https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection)|\n|[Image Alignment (Feature Based) using OpenCV (C++/Python)](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased)|\n|[Barcode and QR code Scanner using ZBar and OpenCV](https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner)|\n|[Keras Tutorial : Fine-tuning using pre-trained models](https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning)|\n|[OpenCV Transparent API](https://www.learnopencv.com/opencv-transparent-api/)| |\n|[Face Reconstruction using EigenFaces (C++/Python)](https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces) |\n|[Eigenface using OpenCV (C++/Python)](https://www.learnopencv.com/eigenface-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/EigenFace)|\n|[Principal Component Analysis](https://www.learnopencv.com/principal-component-analysis/)| |\n|[Keras Tutorial : Transfer Learning using pre-trained models](https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning) |\n|[Keras Tutorial : Using pre-trained Imagenet models](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models) |\n|[Technical Aspects of a Digital SLR](https://www.learnopencv.com/technical-aspects-of-a-digital-slr/) | |\n|[Using Harry Potter interactive wand with OpenCV to create magic](https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/)| |\n|[Install OpenCV 3 and Dlib on Windows ( Python only )](https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/)| |\n|[Image Classification using Convolutional Neural Networks in Keras](https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR)|\n|[Understanding Autoencoders using Tensorflow (Python)](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder)|\n|[Best Project Award : Computer Vision for Faces](https://www.learnopencv.com/best-project-award-computer-vision-for-faces/) | |\n|[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)      | |\n|[Image Classification using Feedforward Neural Network in Keras](https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/)      | [Code](https://github.com/kromydas/learnopencv/tree/master/Keras-MLP-MNIST-Classification)|\n|[Exposure Fusion using OpenCV (C++/Python)](https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/ExposureFusion)|\n|[Understanding Feedforward Neural Networks](https://www.learnopencv.com/understanding-feedforward-neural-networks/)      | |\n|[High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)](http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python)      | [Code](https://github.com/spmallick/learnopencv/tree/master/hdr)|\n|[Deep learning using Keras â€“ The Basics](http://www.learnopencv.com/deep-learning-using-keras-the-basics)      | [Code](https://github.com/kromydas/learnopencv/tree/master/Keras-Linear-Regression)|\n|[Selective Search for Object Detection (C++ / Python)](http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch) |\n|[Installing Deep Learning Frameworks on Ubuntu with CUDA support](http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/) | |\n|[Parallel Pixel Access in OpenCV using forEach](http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/) | [Code](https://github.com/spmallick/learnopencv/tree/master/forEach) |\n|[cvui: A GUI lib built on top of OpenCV drawing primitives](http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/) | [Code](https://github.com/spmallick/learnopencv/tree/master/UI-cvui) |\n|[Install Dlib on Windows](http://www.learnopencv.com/install-dlib-on-windows/) | |\n|[Install Dlib on Ubuntu](http://www.learnopencv.com/install-dlib-on-ubuntu/) | |\n|[Install OpenCV3 on Ubuntu](http://www.learnopencv.com/install-opencv3-on-ubuntu/) | |\n|[Read, Write and Display a video using OpenCV ( C++/ Python )](http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay) |\n|[Install Dlib on MacOS](http://www.learnopencv.com/install-dlib-on-macos/) | |\n|[Install OpenCV 3 on MacOS](http://www.learnopencv.com/install-opencv3-on-macos/) | |\n|[Install OpenCV 3 on Windows](http://www.learnopencv.com/install-opencv3-on-windows/) | |\n|[Get OpenCV Build Information ( getBuildInformation )](http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/) | |\n|[Color spaces in OpenCV (C++ / Python)](http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ColorSpaces)|\n|[Neural Networks : A 30,000 Feet View for Beginners](http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/) | |\n|[Alpha Blending using OpenCV (C++ / Python)](http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AlphaBlending) |\n|[User stories : How readers of this blog are applying their knowledge to build applications](http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/) | |\n|[How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?](http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/) | |\n|[Automatic Red Eye Remover using OpenCV (C++ / Python)](http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover) |\n|[Bias-Variance Tradeoff in Machine Learning](http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/) | |\n|[Embedded Computer Vision: Which device should you choose?](http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/) | |\n|[Object Tracking using OpenCV (C++/Python)](http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/tracking) |\n|[Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/digits-classification) |\n|[Training a better Haar and LBP cascade based Eye Detector using OpenCV](http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/) | |\n|[Deep Learning Book Gift Recipients](http://www.learnopencv.com/deep-learning-book-gift-recipients/) | |\n|[Minified OpenCV Haar and LBP Cascades](http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector)|\n|[Deep Learning Book Gift](http://www.learnopencv.com/deep-learning-book-gift/) | |\n|[Histogram of Oriented Gradients](http://www.learnopencv.com/histogram-of-oriented-gradients/) | |\n|[Image Recognition and Object Detection : Part 1](http://www.learnopencv.com/image-recognition-and-object-detection-part1/) | |\n|[Head Pose Estimation using OpenCV and Dlib](http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HeadPose) |\n|[Live CV : A Computer Vision Coding Application](http://www.learnopencv.com/live-cv/) | |\n|[Approximate Focal Length for Webcams and Cell Phone Cameras](http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/) | |\n|[Configuring Qt for OpenCV on OSX](http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qt-test) |\n|[Rotation Matrix To Euler Angles](http://www.learnopencv.com/rotation-matrix-to-euler-angles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles) |\n|[Speeding up Dlibâ€™s Facial Landmark Detector](http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/) | |\n|[Warp one triangle to another using OpenCV ( C++ / Python )](http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/WarpTriangle) |\n|[Average Face : OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/average-face-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceAverage) |\n|[Face Swap using OpenCV ( C++ / Python )](http://www.learnopencv.com/face-swap-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceSwap) |\n|[Face Morph Using OpenCV â€” C++ / Python](http://www.learnopencv.com/face-morph-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceMorph) |\n|[Deep Learning Example using NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/) | |\n|[NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/nvidia-digits-3-on-ec2/) | |\n|[Homography Examples using OpenCV ( Python / C ++ )](http://www.learnopencv.com/homography-examples-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Homography) |\n|[Filling holes in an image using OpenCV ( Python / C++ )](http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Holes) |\n|[How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?](http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FPS) |\n|[Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python)](http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Delaunay) |\n|[OpenCV (C++ vs Python) vs MATLAB for Computer Vision](http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/) | |\n|[Facial Landmark Detection](http://www.learnopencv.com/facial-landmark-detection/) | |\n|[Why does OpenCV use BGR color format ?](http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/) | |\n|[Computer Vision for Predicting Facial Attractiveness](http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness) |\n|[applyColorMap for pseudocoloring in OpenCV ( C++ / Python )](http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colormap) |\n|[Image Alignment (ECC) in OpenCV ( C++ / Python )](http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment) |\n|[How to find OpenCV version in Python and C++ ?](http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/) | |\n|[Baidu banned from ILSVRC 2015](http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/) | |\n|[OpenCV Transparent API](http://www.learnopencv.com/opencv-transparent-api/) | |\n|[How Computer Vision Solved the Greatest Soccer Mystery of All Time](http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/) | |\n|[Embedded Vision Summit 2015](http://www.learnopencv.com/embedded-vision-summit-2015/) | |\n|[Read an Image in OpenCV ( Python, C++ )](http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/imread) |\n|[Non-Photorealistic Rendering using OpenCV ( Python, C++ )](http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering) |\n|[Seamless Cloning using OpenCV ( Python , C++ )](http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning) |\n|[OpenCV Threshold ( Python , C++ )](http://www.learnopencv.com/opencv-threshold-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Threshold) |\n|[Blob Detection Using OpenCV ( Python, C++ )](http://www.learnopencv.com/blob-detection-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BlobDetector) |\n|[Turn your OpenCV Code into a Web API in under 10 minutes â€” Part 1](http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/) | |\n|[How to compile OpenCV sample Code ?](http://www.learnopencv.com/how-to-compile-opencv-sample-Code/) | |\n|[Install OpenCV 3 on Yosemite ( OSX 10.10.x )](http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/) | |\n', '{"language":"Jupyter Notebook","stars":22546,"forks":11734,"watchers":22546,"open_issues":276,"topics":["ai","computer-vision","computervision","deep-learning","deep-neural-networks","deeplearning","machine-learning","opencv","opencv-cpp","opencv-library","opencv-python","opencv-tutorial","opencv3"],"default_branch":"master","size_kb":3810062,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:kromydas:learnopencv","source_url":"https://github.com/kromydas/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:kromydas:learnopencv","source_url":"https://github.com/kromydas/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"}]', NULL, NULL, 'pending', 70, '536767570cb59fdfc243e97e8cd75081', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-spmallick-learnopencv from https://github.com/spmallick.png
Image converted to WebP: data/images/github-spmallick-learnopencv.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-MLEveryday-100-Days-Of-ML-Code', 'github--mleveryday--100-days-of-ml-code', '100-Days-Of-ML-Code', 'MLEveryday', 'è‹±æ–‡åŽŸç‰ˆè¯·ç§»æ­¥Avik-Jainã€‚æ•°æ®åœ¨è¿™é‡Œã€‚ ç¿»è¯‘å‰è¯·å…ˆé˜…è¯»è§„èŒƒã€‚å¸¸è§é—®é¢˜è§£ç­”è§FAQã€‚ - æœ‰ç›‘ç£å­¦ä¹  - æ•°æ®é¢„å¤„ç† - ç®€å•çº¿æ€§å›žå½’ - å¤šå…ƒçº¿æ€§å›žå½’ - é€»è¾‘å›žå½’ - kè¿‘é‚»æ³•(k-NN) - æ”¯æŒå‘é‡æœº(SVM) - å†³ç­–æ ‘ - éšæœºæ£®æž— - æ— ç›‘ç£å­¦ä¹  - K-å‡å€¼èšç±» - å±‚æ¬¡èšç±» æ•°æ®é¢„å¤„ç†å®žçŽ° <p align="center"> <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg"> </p> ç®€å•çº¿æ€§å›žå½’å®žçŽ° <p align="center"> <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg"> </p> å¤šå…ƒçº¿æ€§å›žå½’å®žçŽ° <p align="center"> <img src="https://github.com/MachineLearni...', '["100-days-of-ml-code","chinese-simplified","deep-learning","infographics","jupyter-notebook","keras","machine-learning","python","supervised-learning","tensorflow","tutorial","unsupervised-learning","jupyter notebook"]', 'other', 22120, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/MLEveryday/100-Days-Of-ML-Code","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æœºå™¨å­¦ä¹ 100å¤©\n\nè‹±æ–‡åŽŸç‰ˆè¯·ç§»æ­¥[Avik-Jain](https://github.com/Avik-Jain/100-Days-Of-ML-Code)ã€‚æ•°æ®åœ¨[è¿™é‡Œ](https://github.com/MachineLearning100/100-Days-Of-ML-Code/tree/master/datasets)ã€‚\n\nç¿»è¯‘å‰è¯·å…ˆé˜…è¯»[è§„èŒƒ](Translation%20specification.MD)ã€‚å¸¸è§é—®é¢˜è§£ç­”è§[FAQ](FAQ.MD)ã€‚\n\n# ç›®å½•\n- æœ‰ç›‘ç£å­¦ä¹ \n  - [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†--ç¬¬1å¤©)\n  - [ç®€å•çº¿æ€§å›žå½’](#ç®€å•çº¿æ€§å›žå½’--ç¬¬2å¤©)\n  - [å¤šå…ƒçº¿æ€§å›žå½’](#å¤šå…ƒçº¿æ€§å›žå½’--ç¬¬3å¤©)\n  - [é€»è¾‘å›žå½’](#é€»è¾‘å›žå½’--ç¬¬4å¤©)\n  - [kè¿‘é‚»æ³•(k-NN)](#kè¿‘é‚»æ³•k-nn--ç¬¬7å¤©)\n  - [æ”¯æŒå‘é‡æœº(SVM)](#æ”¯æŒå‘é‡æœºsvm--ç¬¬12å¤©)\n  - [å†³ç­–æ ‘](#å†³ç­–æ ‘--ç¬¬23å¤©)\n  - [éšæœºæ£®æž—](#éšæœºæ£®æž—--ç¬¬33å¤©)\n- æ— ç›‘ç£å­¦ä¹ \n  - [K-å‡å€¼èšç±»](#k-å‡å€¼èšç±»--ç¬¬43å¤©)\n  - [å±‚æ¬¡èšç±»](#å±‚æ¬¡èšç±»--ç¬¬54å¤©)\n\n## æ•°æ®é¢„å¤„ç† | ç¬¬1å¤©\n[æ•°æ®é¢„å¤„ç†å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data_Preprocessing.md)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg">\n</p>\n\n## ç®€å•çº¿æ€§å›žå½’ | ç¬¬2å¤©\n[ç®€å•çº¿æ€§å›žå½’å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%202_Simple_Linear_Regression.md)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg">\n</p>\n\n## å¤šå…ƒçº¿æ€§å›žå½’ | ç¬¬3å¤©\n[å¤šå…ƒçº¿æ€§å›žå½’å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%203_Multiple_Linear_Regression.md)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.png">\n</p>\n\n## é€»è¾‘å›žå½’ | ç¬¬4å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%204.jpg">\n</p>\n\n## é€»è¾‘å›žå½’ | ç¬¬5å¤©\nä»Šå¤©æˆ‘æ·±å…¥ç ”ç©¶äº†é€»è¾‘å›žå½’åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒèƒŒåŽçš„æ•°å­¦æ˜¯ä»€ä¹ˆã€‚å­¦ä¹ äº†å¦‚ä½•è®¡ç®—ä»£ä»·å‡½æ•°ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥å°†ä»£ä»·å‡½æ•°é™ä½Žåˆ°æœ€å°ã€‚<br>\nç”±äºŽæ—¶é—´å…³ç³»ï¼Œæˆ‘å°†éš”å¤©å‘å¸ƒä¿¡æ¯å›¾ã€‚å¦‚æžœæœ‰äººåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸæœ‰ä¸€å®šç»éªŒï¼Œå¹¶æ„¿æ„å¸®æˆ‘ç¼–å†™ä»£ç æ–‡æ¡£ï¼Œä¹Ÿäº†è§£githubçš„Markdownè¯­æ³•ï¼Œè¯·åœ¨é¢†è‹±è”ç³»æˆ‘ã€‚\n\n## é€»è¾‘å›žå½’ | ç¬¬6å¤©\n[é€»è¾‘å›žå½’å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%206_Logistic_Regression.md)\n\n## Kè¿‘é‚»æ³•(k-NN) | ç¬¬7å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%207.jpg">\n</p>\n\n## é€»è¾‘å›žå½’èƒŒåŽçš„æ•°å­¦ | ç¬¬8å¤©\nä¸ºäº†ä½¿æˆ‘å¯¹é€»è¾‘å›žå½’çš„è§è§£æ›´åŠ æ¸…æ™°ï¼Œæˆ‘åœ¨ç½‘ä¸Šæœç´¢äº†ä¸€äº›èµ„æºæˆ–æ–‡ç« ï¼Œç„¶åŽæˆ‘å°±å‘çŽ°äº†Saishruthi Swaminathançš„<a href = "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">è¿™ç¯‡æ–‡ç« </a><br>\n\nå®ƒç»™å‡ºäº†é€»è¾‘å›žå½’çš„è¯¦ç»†æè¿°ã€‚è¯·åŠ¡å¿…çœ‹ä¸€çœ‹ã€‚\n\n## æ”¯æŒå‘é‡æœº(SVM) | ç¬¬9å¤©\nç›´è§‚äº†è§£SVMæ˜¯ä»€ä¹ˆä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒæ¥è§£å†³åˆ†ç±»é—®é¢˜ã€‚\n\n## æ”¯æŒå‘é‡æœºå’ŒKè¿‘é‚»æ³• | ç¬¬10å¤©\näº†è§£æ›´å¤šå…³äºŽSVMå¦‚ä½•å·¥ä½œå’Œå®žçŽ°knnç®—æ³•çš„çŸ¥è¯†ã€‚\n\n## Kè¿‘é‚»æ³•(k-NN) | ç¬¬11å¤©\n[Kè¿‘é‚»æ³•(k-NN)å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2011_K-NN.md)\n\n## æ”¯æŒå‘é‡æœº(SVM) | ç¬¬12å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2012.jpg">\n</p>\n\n## æ”¯æŒå‘é‡æœº(SVM) | ç¬¬13å¤©\n[SVMå®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.md)\n\n## æ”¯æŒå‘é‡æœº(SVM)çš„å®žçŽ° | ç¬¬14å¤©\nä»Šå¤©æˆ‘åœ¨çº¿æ€§ç›¸å…³æ•°æ®ä¸Šå®žçŽ°äº†SVMã€‚ä½¿ç”¨Scikit-Learnåº“ã€‚åœ¨scikit-learnä¸­æˆ‘ä»¬æœ‰SVCåˆ†ç±»å™¨ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚å°†åœ¨ä¸‹ä¸€æ¬¡å®žçŽ°æ—¶ä½¿ç”¨kernel-trickã€‚Pythonä»£ç è§[æ­¤å¤„](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.py),Jupyter notebookè§[æ­¤å¤„](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.ipynb)ã€‚\n\n## æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨(Naive Bayes Classifier)å’Œé»‘ç›’æœºå™¨å­¦ä¹ (Black Box Machine Learning) | ç¬¬15å¤©\nå­¦ä¹ ä¸åŒç±»åž‹çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åŒæ—¶å¼€å§‹<a href="https://bloomberg.github.io/foml/#home">Bloomberg</a>çš„è¯¾ç¨‹ã€‚è¯¾ç¨‹åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ˜¯é»‘ç›’æœºå™¨å­¦ä¹ ã€‚å®ƒç»™å‡ºäº†é¢„æµ‹å‡½æ•°ï¼Œç‰¹å¾æå–ï¼Œå­¦ä¹ ç®—æ³•ï¼Œæ€§èƒ½è¯„ä¼°ï¼Œäº¤å‰éªŒè¯ï¼Œæ ·æœ¬åå·®ï¼Œéžå¹³ç¨³æ€§ï¼Œè¿‡åº¦æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„æ•´ä½“è§‚ç‚¹ã€‚\n\n## é€šè¿‡å†…æ ¸æŠ€å·§å®žçŽ°æ”¯æŒå‘é‡æœº | ç¬¬16å¤©\nä½¿ç”¨Scikit-Learnåº“å®žçŽ°äº†SVMç®—æ³•ä»¥åŠå†…æ ¸å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æˆ‘ä»¬çš„æ•°æ®ç‚¹æ˜ å°„åˆ°æ›´é«˜ç»´åº¦ä»¥æ‰¾åˆ°æœ€ä½³è¶…å¹³é¢ã€‚\n\n## åœ¨Courseraå¼€å§‹æ·±åº¦å­¦ä¹ çš„ä¸“ä¸šè¯¾ç¨‹ | ç¬¬17å¤©\nåœ¨1å¤©å†…å®Œæˆç¬¬1å‘¨å’Œç¬¬2å‘¨å†…å®¹ä»¥åŠå­¦ä¹ è¯¾ç¨‹ä¸­çš„é€»è¾‘å›žå½’ç¥žç»ç½‘ç»œã€‚\n\n## ç»§ç»­Courseraä¸Šçš„æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹ | ç¬¬18å¤©\nå®Œæˆè¯¾ç¨‹1ã€‚ç”¨Pythonè‡ªå·±å®žçŽ°ä¸€ä¸ªç¥žç»ç½‘ç»œã€‚\n\n## å­¦ä¹ é—®é¢˜å’ŒYaser Abu-Mostafaæ•™æŽˆ | ç¬¬19å¤©\nå¼€å§‹Yaser Abu-Mostafaæ•™æŽˆçš„Caltechæœºå™¨å­¦ä¹ è¯¾ç¨‹-CS156ä¸­çš„è¯¾ç¨‹1ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯å¯¹å³å°†åˆ°æ¥çš„è¯¾ç¨‹çš„ä¸€ç§ä»‹ç»ã€‚ä»–ä¹Ÿä»‹ç»äº†æ„ŸçŸ¥ç®—æ³•ã€‚\n\n## æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹2 | ç¬¬20å¤©\nå®Œæˆæ”¹è¿›æ·±åº¦ç¥žç»ç½‘ç»œç¬¬1å‘¨å†…å®¹ï¼šå‚æ•°è°ƒæ•´ï¼Œæ­£åˆ™åŒ–å’Œä¼˜åŒ–ã€‚\n\n## ç½‘é¡µæœç½— | ç¬¬21å¤©\nè§‚çœ‹äº†ä¸€äº›å…³äºŽå¦‚ä½•ä½¿ç”¨Beautiful Soupè¿›è¡Œç½‘ç»œçˆ¬è™«çš„æ•™ç¨‹ï¼Œä»¥ä¾¿æ”¶é›†ç”¨äºŽæž„å»ºæ¨¡åž‹çš„æ•°æ®ã€‚\n\n## å­¦ä¹ è¿˜å¯è¡Œå—? | ç¬¬22å¤©\nå®ŒæˆYaser Abu-Mostafaæ•™æŽˆçš„Caltechæœºå™¨å­¦ä¹ è¯¾ç¨‹-CS156ä¸­çš„è¯¾ç¨‹2ã€‚å­¦ä¹ Hoeffdingä¸ç­‰å¼ã€‚\n\n## å†³ç­–æ ‘ | ç¬¬23å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2023%20-%20Chinese.jpg">\n</p>\n\n## ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„ä»‹ç» | ç¬¬24å¤©\nBloomberg MLè¯¾ç¨‹çš„ç¬¬3è¯¾ä»‹ç»äº†ä¸€äº›æ ¸å¿ƒæ¦‚å¿µï¼Œå¦‚è¾“å…¥ç©ºé—´ï¼ŒåŠ¨ä½œç©ºé—´ï¼Œç»“æžœç©ºé—´ï¼Œé¢„æµ‹å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°å’Œå‡è®¾ç©ºé—´ã€‚\n\n## å†³ç­–æ ‘ | ç¬¬25å¤©\n[å†³ç­–æ ‘å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2025_Decision_Tree.md)\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬26å¤©\nå‘çŽ°YouTubeä¸€ä¸ªç¥žå¥‡çš„é¢‘é“[3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)ï¼Œå®ƒæœ‰ä¸€ä¸ªæ’­æ”¾åˆ—è¡¨ã€Šçº¿æ€§ä»£æ•°çš„æœ¬è´¨ã€‹ã€‚çœ‹å®Œäº†4ä¸ªè§†é¢‘ï¼ŒåŒ…æ‹¬äº†å‘é‡ï¼Œçº¿æ€§ç»„åˆï¼Œè·¨åº¦ï¼ŒåŸºå‘é‡ï¼Œçº¿æ€§å˜æ¢å’ŒçŸ©é˜µä¹˜æ³•ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬27å¤©\nç»§ç»­è§‚çœ‹äº†4ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬ä¸‰ç»´å˜æ¢ã€è¡Œåˆ—å¼ã€é€†çŸ©é˜µã€åˆ—ç©ºé—´ã€é›¶ç©ºé—´å’Œéžæ–¹çŸ©é˜µã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬28å¤©\nç»§ç»­è§‚çœ‹äº†3ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬ç‚¹ç§¯å’Œå‰ç§¯ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬29å¤©\nè§‚çœ‹äº†å‰©ä½™çš„è§†é¢‘12åˆ°14ï¼Œå†…å®¹åŒ…æ‹¬ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼Œä»¥åŠæŠ½è±¡å‘é‡ç©ºé—´ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## å¾®ç§¯åˆ†çš„æœ¬è´¨ | ç¬¬30å¤©\nå®Œæˆä¸Šä¸€æ’­æ”¾åˆ—è¡¨åŽï¼ŒYouTubeæŽ¨èäº†æ–°å†…å®¹ã€Šå¾®ç§¯åˆ†çš„æœ¬è´¨ã€‹ï¼Œä»Šå¤©çœ‹å®Œäº†å…¶ä¸­çš„3ä¸ªè§†é¢‘ï¼ŒåŒ…æ‹¬å¯¼æ•°ã€é“¾å¼æ³•åˆ™ã€ä¹˜ç§¯æ³•åˆ™å’ŒæŒ‡æ•°å¯¼æ•°ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=13407)ã€‚\n\n## å¾®ç§¯åˆ†çš„æœ¬è´¨ | ç¬¬31å¤©\nè§‚çœ‹äº†2ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬éšåˆ†åŒ–ä¸Žæžé™ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=13407)ã€‚\n\n## å¾®ç§¯åˆ†çš„æœ¬è´¨ | ç¬¬32å¤©\nè§‚çœ‹äº†å‰©ä½™çš„4ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬ç§¯åˆ†ä¸Žé«˜é˜¶å¯¼æ•°ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=13407)ã€‚\n\n## éšæœºæ£®æž— | ç¬¬33å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2033.png">\n</p>\n\n## éšæœºæ£®æž— | ç¬¬34å¤©\n[éšæœºæ£®æž—å®žçŽ°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2034_Random_Forests.md)\n\n## ä»€ä¹ˆæ˜¯ç¥žç»ç½‘ç»œï¼Ÿ | æ·±åº¦å­¦ä¹ ï¼Œç¬¬1ç«  | ç¬¬ 35å¤©\nYoutubeé¢‘é“3Blue1Brownä¸­æœ‰ç²¾å½©çš„è§†é¢‘ä»‹ç»ç¥žç»ç½‘ç»œã€‚è¿™ä¸ªè§†é¢‘æä¾›äº†å¾ˆå¥½çš„è§£é‡Šï¼Œå¹¶ä½¿ç”¨æ‰‹å†™æ•°å­—æ•°æ®é›†æ¼”ç¤ºåŸºæœ¬æ¦‚å¿µã€‚\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## æ¢¯åº¦ä¸‹é™æ³•ï¼Œç¥žç»ç½‘ç»œå¦‚ä½•å­¦ä¹  | æ·±åº¦å­¦ä¹ ï¼Œç¬¬2ç«  | ç¬¬36å¤©\nYoutubeé¢‘é“3Blue1Brownå…³äºŽç¥žç»ç½‘ç»œçš„ç¬¬2éƒ¨åˆ†ï¼Œè¿™ä¸ªè§†é¢‘ç”¨æœ‰è¶£çš„æ–¹å¼è§£é‡Šäº†æ¢¯åº¦ä¸‹é™æ³•ã€‚æŽ¨èå¿…é¡»è§‚çœ‹169.\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## åå‘ä¼ æ’­æ³•ç©¶ç«Ÿåšä»€ä¹ˆï¼Ÿ | æ·±åº¦å­¦ä¹ ï¼Œç¬¬3ç«  | ç¬¬37å¤©\nYoutubeé¢‘é“3Blue1Brownå…³äºŽç¥žç»ç½‘ç»œçš„ç¬¬3éƒ¨åˆ†ï¼Œè¿™ä¸ªè§†é¢‘ä¸»è¦ä»‹ç»äº†åå¯¼æ•°å’Œåå‘ä¼ æ’­æ³•ã€‚\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## åå‘ä¼ æ’­æ³•æ¼”ç®— | æ·±åº¦å­¦ä¹ ï¼Œç¬¬4ç«  | ç¬¬38å¤©\nYoutubeé¢‘é“3Blue1Brownå…³äºŽç¥žç»ç½‘ç»œçš„ç¬¬3éƒ¨åˆ†ï¼Œè¿™ä¸ªè§†é¢‘ä¸»è¦ä»‹ç»äº†åå¯¼æ•°å’Œåå‘ä¼ æ’­æ³•ã€‚\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## ç¬¬1éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬39å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2039.ipynb)ã€‚\n\n## ç¬¬2éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬40å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2040.ipynb)ã€‚\n\n## ç¬¬3éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬41å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2041.ipynb)ã€‚\n\n## ç¬¬4éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬42å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2042.ipynb)ã€‚\n\n## K-å‡å€¼èšç±» | ç¬¬43å¤©\nè½¬åˆ°æ— ç›‘ç£å­¦ä¹ ï¼Œå¹¶ç ”ç©¶äº†èšç±»ã€‚å¯åœ¨[ä½œè€…ç½‘ç«™](http://www.avikjain.me/)æŸ¥è¯¢ã€‚å‘çŽ°ä¸€ä¸ªå¥‡å¦™çš„[åŠ¨ç”»](http://shabal.in/visuals/kmeans/6.html)æœ‰åŠ©äºŽç†è§£K-å‡å€¼èšç±»ã€‚\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2043.jpg">\n</p>\n\n## K-å‡å€¼èšç±» | ç¬¬44å¤©\nå®žçŽ°ï¼ˆå¾…æ·»åŠ ä»£ç ï¼‰\n\n## æ·±å…¥ç ”ç©¶ | NUMPY | ç¬¬45å¤©\nå¾—åˆ°JK VanderPlaså†™çš„ä¹¦ã€ŠPythonæ•°æ®ç§‘å­¦æ‰‹å†Œï¼ˆPython Data Science HandBookï¼‰ã€‹ï¼ŒJupyter notebooksåœ¨[è¿™é‡Œ](https://github.com/jakevdp/PythonDataScienceHandbook)ã€‚\n<br>**[é«˜æ¸…ä¸­æ–‡ç‰ˆpdf](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other%20Docs/Python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C.zip)**\n<br>ç¬¬2ç« ï¼šNumPyä»‹ç»ï¼ŒåŒ…æ‹¬æ•°æ®ç±»åž‹ã€æ•°ç»„å’Œæ•°ç»„è®¡ç®—ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[2 NumPyå…¥é—¨](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb)\n<br>[2.1 ç†è§£Pythonä¸­çš„æ•°æ®ç±»åž‹](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb)\n<br>[2.2 NumPyæ•°ç»„åŸºç¡€](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n<br>[2.3 NumPyæ•°ç»„çš„è®¡ç®—ï¼šé€šç”¨å‡½æ•°](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.03-Computation-on-arrays-ufuncs.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | NUMPY | ç¬¬46å¤©\nç¬¬2ç« ï¼š èšåˆ, æ¯”è¾ƒè¿ç®—ç¬¦å’Œå¹¿æ’­ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[2.4 èšåˆï¼šæœ€å°å€¼ã€æœ€å¤§å€¼å’Œå…¶ä»–å€¼](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.04-Computation-on-arrays-aggregates.ipynb)\n<br>[2.5 æ•°ç»„çš„è®¡ç®—ï¼šå¹¿æ’­](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.05-Computation-on-arrays-broadcasting.ipynb)\n<br>[2.6 æ¯”è¾ƒã€æŽ©ç å’Œå¸ƒå°”è¿ç®—](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.06-Boolean-Arrays-and-Masks.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | NUMPY | ç¬¬47å¤©\nç¬¬2ç« ï¼š èŠ±å“¨çš„ç´¢å¼•ï¼Œæ•°ç»„æŽ’åºï¼Œç»“æž„åŒ–æ•°æ®ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[2.7 èŠ±å“¨çš„ç´¢å¼•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.07-Fancy-Indexing.ipynb)\n<br>[2.8 æ•°ç»„çš„æŽ’åº](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)\n<br>[2.9 ç»“æž„åŒ–æ•°æ®ï¼šNumPyçš„ç»“æž„åŒ–æ•°ç»„](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.09-<br>Structured-Data-NumPy.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | PANDAS | ç¬¬48å¤©\nç¬¬3ç« ï¼šPandasæ•°æ®å¤„ç†\n<br>åŒ…å«Pandaså¯¹è±¡ï¼Œæ•°æ®å–å€¼ä¸Žé€‰æ‹©ï¼Œæ•°å€¼è¿ç®—æ–¹æ³•ï¼Œå¤„ç†ç¼ºå¤±å€¼ï¼Œå±‚çº§ç´¢å¼•ï¼Œåˆå¹¶æ•°æ®é›†ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[3 Pandasæ•°æ®å¤„ç†](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)\n<br>[3.1 Pandaså¯¹è±¡ç®€ä»‹](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb)\n<br>[3.2 æ•°æ®å–å€¼ä¸Žé€‰æ‹©](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02-Data-Indexing-and-Selection.ipynb)\n<br>[3.3 Pandasæ•°å€¼è¿ç®—æ–¹æ³•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.03-Operations-in-Pandas.ipynb)\n<br>[3.4 å¤„ç†ç¼ºå¤±å€¼](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb)\n<br>[3.5 å±‚çº§ç´¢å¼•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb)\n<br>[3.6 åˆå¹¶æ•°æ®é›†ï¼šConCatå’ŒAppendæ–¹æ³•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | PANDAS | ç¬¬49å¤©\nç¬¬3ç« ï¼šå®Œæˆå‰©ä½™å†…å®¹-åˆå¹¶ä¸Žè¿žæŽ¥ï¼Œç´¯è®¡ä¸Žåˆ†ç»„ï¼Œæ•°æ®é€è§†è¡¨ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[3.7 åˆå¹¶æ•°æ®é›†ï¼šåˆå¹¶ä¸Žè¿žæŽ¥](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb)\n<br>[3.8 ç´¯è®¡ä¸Žåˆ†ç»„](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb)\n<br>[3.9 æ•°æ®é€è§†è¡¨](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | PANDAS | ç¬¬50å¤©\nç¬¬3ç« ï¼šå‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œï¼Œå¤„ç†æ—¶é—´åºåˆ—ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[3.10 å‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œ](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb)\n<br>[3.11 å¤„ç†æ—¶é—´åºåˆ—](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb)\n<br>[3.12 é«˜æ€§èƒ½Pandasï¼ševal()ä¸Žquery()](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | MATPLOTLIB | ç¬¬51å¤©\nç¬¬4ç« ï¼šMatplotlibæ•°æ®å¯è§†åŒ–\n<br>å­¦ä¹ ç®€æ˜“çº¿å½¢å›¾, ç®€æ˜“æ•£ç‚¹å›¾ï¼Œå¯†åº¦å›¾ä¸Žç­‰é«˜çº¿å›¾.\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[4 Matplotlibæ•°æ®å¯è§†åŒ–](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n<br>[4.1 ç®€æ˜“çº¿å½¢å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb)\n<br>[4.2 ç®€æ˜“æ•£ç‚¹å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb)\n<br>[4.3 å¯è§†åŒ–å¼‚å¸¸å¤„ç†](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.03-Errorbars.ipynb)\n<br>[4.4 å¯†åº¦å›¾ä¸Žç­‰é«˜çº¿å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | MATPLOTLIB | ç¬¬52å¤©\nç¬¬4ç« ï¼šMatplotlibæ•°æ®å¯è§†åŒ–\n<br>å­¦ä¹ ç›´æ–¹å›¾ï¼Œé…ç½®å›¾ä¾‹ï¼Œé…ç½®é¢œè‰²æ¡ï¼Œå¤šå­å›¾ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š \n<br>[4.5 ç›´æ–¹å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.05-Histograms-and-Binnings.ipynb)\n<br>[4.6 é…ç½®å›¾ä¾‹](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.06-Customizing-Legends.ipynb)\n<br>[4.7 é…ç½®é¢œè‰²æ¡](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.07-Customizing-Colorbars.ipynb)\n<br>[4.8 å¤šå­å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.08-Multiple-Subplots.ipynb)\n<br>[4.9 æ–‡å­—ä¸Žæ³¨é‡Š](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | MATPLOTLIB | ç¬¬53å¤©\nç¬¬4ç« ï¼šMatplotlibæ•°æ®å¯è§†åŒ–\n<br>å­¦ä¹ ä¸‰ç»´ç»˜å›¾ã€‚\n<br>[4.12 ç”»ä¸‰ç»´å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb)\n\n## å±‚æ¬¡èšç±» | ç¬¬54å¤©\n[åŠ¨ç”»æ¼”ç¤º](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other%20Docs/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB.gif)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2054.jpg">\n</p>\n', '{"language":"Jupyter Notebook","stars":22120,"forks":5550,"watchers":22120,"open_issues":16,"topics":["100-days-of-ml-code","chinese-simplified","deep-learning","infographics","jupyter-notebook","keras","machine-learning","python","supervised-learning","tensorflow","tutorial","unsupervised-learning"],"default_branch":"master","size_kb":44918,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"}]', NULL, 'MIT', 'approved', 80, '3884fd349558dc46f776a543750e6718', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-MLEveryday-100-Days-Of-ML-Code from https://github.com/MLEveryday.png
Image converted to WebP: data/images/github-MLEveryday-100-Days-Of-ML-Code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-patchy631-ai-engineering-hub', 'github--patchy631--ai-engineering-hub', 'ai-engineering-hub', 'patchy631', '<p align="center"> <a href="https://trendshift.io/repositories/12800"> <img src="assets/TRENDING-BADGE.png" alt="Trending Badge" style="width: 250px; height: 55px;" width="250" height="55"/> </a> </p> <p align="center"> <img src="assets/ai-eng-hub.gif" alt="AI Engineering Hub Banner"> </p> --- Welcome to the **AI Engineering Hub** - your comprehensive resource for learning and building with AI! AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding ...', '["agents","ai","llms","machine-learning","mcp","rag","jupyter notebook"]', 'other', 22082, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/patchy631/ai-engineering-hub","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://trendshift.io/repositories/12800">\n    <img src="assets/TRENDING-BADGE.png" alt="Trending Badge" style="width: 250px; height: 55px;" width="250" height="55"/>\n  </a>\n</p>\n\n<p align="center">\n  <img src="assets/ai-eng-hub.gif" alt="AI Engineering Hub Banner">\n</p>\n\n---\n\n# AI Engineering Hub ðŸš€\n\nWelcome to the **AI Engineering Hub** - your comprehensive resource for learning and building with AI!\n\n## ðŸŒŸ Why This Repo?\n\nAI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:\n- **93+ Production-Ready Projects** across all skill levels\n- In-depth tutorials on **LLMs, RAG, Agents, and more**\n- Real-world **AI agent** applications\n- Examples to implement, adapt, and scale in your projects\n\nWhether you''re a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.\n\n---\n\n## ðŸ“‹ Table of Contents\n\n- [Getting Started](#-getting-started)\n- [Newsletter](#-stay-updated-with-our-newsletter)\n- [Projects by Difficulty](#-projects-by-difficulty)\n  - [Beginner Projects (22)](#-beginner-projects)\n  - [Intermediate Projects (48)](#-intermediate-projects)\n  - [Advanced Projects (23)](#-advanced-projects)\n- [Contributing](#-contribute-to-the-ai-engineering-hub)\n- [License](#-license)\n\n---\n\n## ðŸŽ¯ Getting Started\n\nNew to AI Engineering? Start here:\n\n1. **Complete Beginners**: Check out the [AI Engineering Roadmap](./ai-engineering-roadmap) for a comprehensive learning path\n2. **Learn the Basics**: Start with [Beginner Projects](#-beginner-projects) like OCR apps and simple RAG implementations\n3. **Build Your Skills**: Move to [Intermediate Projects](#-intermediate-projects) with agents and complex workflows\n4. **Master Advanced Concepts**: Tackle [Advanced Projects](#-advanced-projects) including fine-tuning and production systems\n\n---\n\n## ðŸ“¬ Stay Updated with Our Newsletter!\n\n**Get a FREE Data Science eBook** ðŸ“– with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. [Subscribe now!](https://join.dailydoseofds.com)\n\n[![Daily Dose of Data Science Newsletter](https://github.com/patchy631/ai-engineering/blob/main/resources/join_ddods.png)](https://join.dailydoseofds.com)\n\n---\n\n## ðŸŽ“ Projects by Difficulty\n\n### ðŸŸ¢ Beginner Projects\n\nPerfect for getting started with AI engineering. These projects focus on single components and straightforward implementations.\n\n#### OCR & Vision\n- [**LaTeX OCR with Llama**](./LaTeX-OCR-with-Llama) - Convert LaTeX equation images to code using Llama 3.2 vision\n- [**Llama OCR**](./llama-ocr) - 100% local OCR app with Llama 3.2 and Streamlit\n- [**Gemma-3 OCR**](./gemma3-ocr) - Local OCR with structured text extraction using Gemma-3\n- [**Qwen 2.5 OCR**](./qwen-2.5VL-ocr) - Text extraction using Qwen 2.5 VL model\n\n#### Chat Interfaces & UI\n- [**Local ChatGPT with DeepSeek**](./local-chatgpt%20with%20DeepSeek) - Mini-ChatGPT with DeepSeek-R1 and Chainlit\n- [**Local ChatGPT with Llama**](./local-chatgpt) - ChatGPT clone using Llama 3.2 vision\n- [**Local ChatGPT with Gemma 3**](./local-chatgpt%20with%20Gemma%203) - Local chat interface with Gemma 3\n- [**DeepSeek Thinking UI**](./deepseek-thinking-ui) - ChatGPT with visible reasoning using DeepSeek-R1\n- [**Qwen3 Thinking UI**](./qwen3-thinking-ui) - Thinking UI with Qwen3:4B and Streamlit\n- [**GPT-OSS Thinking UI**](./gpt-oss-thinking-ui) - GPT-OSS with reasoning visualization\n- [**Streaming AI Chatbot**](./streaming-ai-chatbot) - Real-time AI streaming with Motia framework\n\n#### Basic RAG\n- [**Simple RAG Workflow**](./simple-rag-workflow) - Basic RAG with LlamaIndex and Ollama\n- [**Document Chat RAG**](./document-chat-rag) - Chat with documents using Llama 3.3\n- [**Fastest RAG Stack**](./fastest-rag-stack) - Fast RAG with SambaNova, LlamaIndex, and Qdrant\n- [**GitHub RAG**](./github-rag) - Chat with GitHub repos locally\n- [**ModernBERT RAG**](./modernbert-rag) - RAG with ModernBert embeddings\n- [**Llama 4 RAG**](./llama-4-rag) - RAG powered by Meta''s Llama 4\n\n#### Multimodal & Media\n- [**Image Generation with Janus-Pro**](./imagegen-janus-pro) - Local image generation with DeepSeek Janus-pro 7B\n- [**Video RAG with Gemini**](./video-rag-gemini) - Chat with videos using Gemini AI\n\n#### Other Tools\n- [**Website to API with FireCrawl**](./Website-to-API-with-FireCrawl) - Convert websites to APIs\n- [**AI News Generator**](./ai_news_generator) - News generation with CrewAI and Cohere\n- [**Siamese Network**](./siamese-network) - Digit similarity detection on MNIST\n\n---\n\n### ðŸŸ¡ Intermediate Projects\n\nMulti-component systems, agentic workflows, and advanced features for experienced practitioners.\n\n#### AI Agents & Workflows\n- [**YouTube Trend Analysis**](./Youtube-trend-analysis) - Analyze YouTube trends with CrewAI and BrightData\n- [**AutoGen Stock Analyst**](./autogen-stock-analyst) - Advanced analyst with Microsoft AutoGen\n- [**Agentic RAG**](./agentic_rag) - RAG with document search and web fallback\n- [**Agentic RAG with DeepSeek**](./agentic_rag_deepseek) - Enterprise agentic RAG with GroundX\n- [**Book Writer Flow**](./book-writer-flow) - Automated book writing with CrewAI\n- [**Content Planner Flow**](./content_planner_flow) - Content workflow with CrewAI Flow\n- [**Brand Monitoring**](./brand-monitoring) - Automated brand monitoring system\n- [**Hotel Booking Crew**](./hotel-booking-crew) - Multi-agent hotel booking with DeepSeek-R1\n- [**Deploy Agentic RAG**](./deploy-agentic-rag) - Private Agentic RAG API with LitServe\n- [**Zep Memory Assistant**](./zep-memory-assistant) - AI Agent with human-like memory\n- [**Agent with MCP Memory**](./agent-with-mcp-memory) - Agents with Graphiti memory and Opik\n- [**ACP Code**](./acp-code) - Agent Communication Protocol demo\n- [**Motia Content Creation**](./motia-content-creation) - Social media automation workflow\n\n#### Voice & Audio\n- [**Real-time Voice Bot**](./real-time-voicebot) - Conversational travel guide with AssemblyAI\n- [**RAG Voice Agent**](./rag-voice-agent) - Real-time RAG Voice Agent with Cartesia\n- [**Chat with Audios**](./chat-with-audios) - RAG over audio files\n- [**Audio Analysis Toolkit**](./audio-analysis-toolkit) - Audio analysis with AssemblyAI\n- [**Multilingual Meeting Notes**](./multilingual-meeting-notes-generator) - Auto meeting notes with language detection\n\n#### Advanced RAG\n- [**RAG with Dockling**](./rag-with-dockling) - RAG over Excel with IBM''s Docling\n- [**Trustworthy RAG**](./trustworthy-rag) - RAG over complex docs with TLM\n- [**Fastest RAG with Milvus and Groq**](./fastest-rag-milvus-groq) - Sub-15ms retrieval latency\n- [**Chat with Code**](./chat-with-code) - Chat with code using Qwen3-Coder\n- [**RAG SQL Router**](./rag-sql-router) - Agent with RAG and SQL routing\n\n#### Multimodal\n- [**DeepSeek Multimodal RAG**](./deepseek-multimodal-RAG) - MultiModal RAG with DeepSeek-Janus-Pro\n- [**ColiVara Website RAG**](./Colivara-deepseek-website-RAG) - MultiModal RAG for websites\n- [**Multimodal RAG with AssemblyAI**](./multimodal-rag-assemblyai) - Audio + vector database + CrewAI\n\n#### MCP (Model Context Protocol)\n- [**Cursor Linkup MCP**](./cursor_linkup_mcp) - Custom MCP with deep web search\n- [**EyeLevel MCP RAG**](./eyelevel-mcp-rag) - MCP for RAG over complex docs\n- [**LlamaIndex MCP**](./llamaindex-mcp) - Local MCP client with LlamaIndex\n- [**MCP Agentic RAG**](./mcp-agentic-rag) - MCP-powered Agentic RAG for Cursor\n- [**MCP Agentic RAG Firecrawl**](./mcp-agentic-rag-firecrawl) - Agentic RAG with Firecrawl\n- [**MCP Video RAG**](./mcp-video-rag) - Video RAG using Ragie via MCP\n- [**MCP Voice Agent**](./mcp-voice-agent) - Voice agent with Firecrawl and Supabase\n- [**SDV MCP**](./sdv-mcp) - Synthetic Data Vault orchestration\n- [**KitOps MCP**](./kitops-mcp) - ML model management with KitOps\n- [**Stagehand Ã— MCP-Use**](./stagehand%20x%20mcp-use) - Web automation with Stagehand MCP\n\n#### Model Comparison & Evaluation\n- [**Evaluation and Observability**](./eval-and-observability) - E2E RAG evaluation with CometML Opik\n- [**Llama 4 vs DeepSeek-R1**](./llama-4_vs_deepseek-r1) - Compare models using RAG\n- [**Qwen3 vs DeepSeek-R1**](./qwen3_vs_deepseek-r1) - Model comparison with Opik\n- [**O3 vs Claude Code**](./o3-vs-claude-code) - Compare Claude 3.7 and o3\n- [**Sonnet4 vs O4**](./sonnet4-vs-o4) - Code generation comparison\n- [**Sonnet4 vs Qwen3-Coder**](./sonnet4-vs-qwen3-coder) - Coder model comparison\n- [**Code Model Comparison**](./code-model-comparison) - Frontier model code comparison\n- [**GPT-OSS vs Qwen3**](./gpt-oss-vs-qwen3) - Reasoning capabilities comparison\n\n---\n\n### ðŸ”´ Advanced Projects\n\nComplex systems, fine-tuning, production deployments, and cutting-edge implementations.\n\n#### Fine-tuning & Model Development\n- [**DeepSeek Fine-tuning**](./DeepSeek-finetuning) - Fine-tune DeepSeek with Unsloth and Ollama\n- [**Build Reasoning Model**](./Build-reasoning-model) - Build DeepSeek-R1-like reasoning models\n- [**Attention Is All You Need Implementation**](./attention-is-all-you-need-impl) - Transformer architecture from scratch\n\n#### Advanced Agent Systems\n- [**NVIDIA Demo**](./nvidia-demo) - Documentation writer with CrewAI Flows and NVIDIA NIM\n- [**Documentation Writer Flow**](./documentation-writer-flow) - Agentic documentation workflow\n- [**Multi-Agent Deep Researcher**](./Multi-Agent-deep-researcher-mcp-windows-linux) - MCP-powered deep researcher\n- [**Multiplatform Deep Researcher**](./multiplatform_deep_researcher) - Multi-platform research with BrightData\n- [**Web Browsing Agent**](./web-browsing-agent) - Browser automation with CrewAI and Stagehand\n- [**Paralegal Agent Crew**](./paralegal-agent-crew) - Intelligent paralegal with RAG\n- [**FireCrawl Agent**](./firecrawl-agent) - Corrective RAG with web search fallback\n- [**Context Engineering Workflow**](./context-engineering-workflow) - Research assistant with TensorLake and Zep\n- [**Parlant Conversational Agent**](./parlant-conversational-agent) - Compliance-driven conversational agent\n- [**Stock Portfolio Analysis Agent**](./stock-portfolio-analysis-agent) - Portfolio analysis with React frontend\n- [**Guidelines vs Traditional Prompt**](./guidelines-vs-traditional-prompt) - Structured guidelines comparison\n\n#### Advanced MCP & Infrastructure\n- [**MindsDB MCP**](./mindsdb-mcp) - Unified MCP for all data sources\n- [**Financial Analyst DeepSeek**](./financial-analyst-deepseek) - MCP financial analysis workflow\n- [**Graphiti MCP**](./graphiti-mcp) - Persistent memory with Zep''s Graphiti\n- [**Pixeltable MCP**](./pixeltable-mcp) - Unified multimodal data orchestration\n- [**Ultimate AI Assistant**](./ultimate-ai-assitant-using-mcp) - Multi-MCP server interface\n\n#### Production Systems\n- [**GroundX Document Pipeline**](./groundX-doc-pipeline) - World-class document processing\n- [**NotebookLM Clone**](./notebook-lm-clone) - Full NotebookLM with RAG, citations, and podcasts\n\n#### Learning Resources\n- [**AI Engineering Roadmap**](./ai-engineering-roadmap) - Complete guide from Python to production AI\n\n---\n\n## ðŸ“¢ Contribute to the AI Engineering Hub!\n\nWe welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Here''s how to get involved:\n\n1. **Fork** the repository\n2. Create a new branch for your contribution\n3. Submit a **Pull Request** and describe the improvements\n\nCheck out our [contributing guidelines](CONTRIBUTING.md) for more details.\n\n---\n\n## ðŸ“œ License\n\nThis repository is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n## ðŸ’¬ Connect\n\nFor discussions, suggestions, and more, feel free to [create an issue](https://github.com/patchy631/ai-engineering/issues) or reach out directly!\n\n**Happy Coding!** ðŸŽ‰\n', '{"language":"Jupyter Notebook","stars":22082,"forks":3612,"watchers":22082,"open_issues":114,"topics":["agents","ai","llms","machine-learning","mcp","rag"],"default_branch":"main","size_kb":267351,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:patchy631:ai-engineering","source_url":"https://github.com/patchy631/ai-engineering"},{"type":"has_code","target_id":"github:patchy631:ai-engineering","source_url":"https://github.com/patchy631/ai-engineering"}]', NULL, 'MIT', 'approved', 80, 'bb7a98ead00ed58e268a6573ad03b51a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-patchy631-ai-engineering-hub from https://github.com/patchy631.png
Image converted to WebP: data/images/github-patchy631-ai-engineering-hub.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-jina-ai-serve', 'github--jina-ai--serve', 'serve', 'jina-ai', '<a href="https://pypi.org/project/jina/"><img alt="PyPI" src="https://img.shields.io/pypi/v/jina?label=Release&style=flat-square"></a> <a href="https://discord.jina.ai"><img src="https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square"></a> <a href="https://pypistats.org/packages/jina"><img alt="PyPI - Downloads from official pypistats" src="https://img.shields.io/pypi/dm/jina?style=flat-square"></a> <a href="https://github.com/jina-ai/jina/actions/wo...', '["cloud-native","cncf","deep-learning","docker","fastapi","framework","generative-ai","grpc","jaeger","kubernetes","llmops","machine-learning","microservice","mlops","multimodal","neural-search","opentelemetry","orchestration","pipeline","prometheus","python"]', 'other', 21798, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/jina-ai/serve","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Jina-Serve\n<a href="https://pypi.org/project/jina/"><img alt="PyPI" src="https://img.shields.io/pypi/v/jina?label=Release&style=flat-square"></a>\n<a href="https://discord.jina.ai"><img src="https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square"></a>\n<a href="https://pypistats.org/packages/jina"><img alt="PyPI - Downloads from official pypistats" src="https://img.shields.io/pypi/dm/jina?style=flat-square"></a>\n<a href="https://github.com/jina-ai/jina/actions/workflows/cd.yml"><img alt="Github CD status" src="https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg"></a>\n\nJina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets. Scale your services from local development to production while focusing on your core logic.\n\n## Key Features\n\n- Native support for all major ML frameworks and data types\n- High-performance service design with scaling, streaming, and dynamic batching\n- LLM serving with streaming output\n- Built-in Docker integration and Executor Hub\n- One-click deployment to Jina AI Cloud\n- Enterprise-ready with Kubernetes and Docker Compose support\n\n<details>\n<summary><strong>Comparison with FastAPI</strong></summary>\n\nKey advantages over FastAPI:\n\n- DocArray-based data handling with native gRPC support\n- Built-in containerization and service orchestration\n- Seamless scaling of microservices\n- One-command cloud deployment\n</details>\n\n## Install \n\n```bash\npip install jina\n```\n\nSee guides for [Apple Silicon](https://jina.ai/serve/get-started/install/apple-silicon-m1-m2/) and [Windows](https://jina.ai/serve/get-started/install/windows/).\n\n## Core Concepts\n\nThree main layers:\n- **Data**: BaseDoc and DocList for input/output\n- **Serving**: Executors process Documents, Gateway connects services\n- **Orchestration**: Deployments serve Executors, Flows create pipelines\n\n## Build AI Services\n\nLet''s create a gRPC-based AI service using StableLM:\n\n```python\nfrom jina import Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom transformers import pipeline\n\n\nclass Prompt(BaseDoc):\n    text: str\n\n\nclass Generation(BaseDoc):\n    prompt: str\n    text: str\n\n\nclass StableLM(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.generator = pipeline(\n            ''text-generation'', model=''stabilityai/stablelm-base-alpha-3b''\n        )\n\n    @requests\n    def generate(self, docs: DocList[Prompt], **kwargs) -> DocList[Generation]:\n        generations = DocList[Generation]()\n        prompts = docs.text\n        llm_outputs = self.generator(prompts)\n        for prompt, output in zip(prompts, llm_outputs):\n            generations.append(Generation(prompt=prompt, text=output))\n        return generations\n```\n\nDeploy with Python or YAML:\n\n```python\nfrom jina import Deployment\nfrom executor import StableLM\n\ndep = Deployment(uses=StableLM, timeout_ready=-1, port=12345)\n\nwith dep:\n    dep.block()\n```\n\n```yaml\njtype: Deployment\nwith:\n uses: StableLM\n py_modules:\n   - executor.py\n timeout_ready: -1\n port: 12345\n```\n\nUse the client:\n\n```python\nfrom jina import Client\nfrom docarray import DocList\nfrom executor import Prompt, Generation\n\nprompt = Prompt(text=''suggest an interesting image generation prompt'')\nclient = Client(port=12345)\nresponse = client.post(''/'', inputs=[prompt], return_type=DocList[Generation])\n```\n\n## Build Pipelines\n\nChain services into a Flow:\n\n```python\nfrom jina import Flow\n\nflow = Flow(port=12345).add(uses=StableLM).add(uses=TextToImage)\n\nwith flow:\n    flow.block()\n```\n\n## Scaling and Deployment\n\n### Local Scaling\n\nBoost throughput with built-in features:\n- Replicas for parallel processing\n- Shards for data partitioning\n- Dynamic batching for efficient model inference\n\nExample scaling a Stable Diffusion deployment:\n\n```yaml\njtype: Deployment\nwith:\n uses: TextToImage\n timeout_ready: -1\n py_modules:\n   - text_to_image.py\n env:\n  CUDA_VISIBLE_DEVICES: RR\n replicas: 2\n uses_dynamic_batching:\n   /default:\n     preferred_batch_size: 10\n     timeout: 200\n```\n\n### Cloud Deployment\n\n#### Containerize Services\n\n1. Structure your Executor:\n```\nTextToImage/\nâ”œâ”€â”€ executor.py\nâ”œâ”€â”€ config.yml\nâ”œâ”€â”€ requirements.txt\n```\n\n2. Configure:\n```yaml\n# config.yml\njtype: TextToImage\npy_modules:\n - executor.py\nmetas:\n name: TextToImage\n description: Text to Image generation Executor\n```\n\n3. Push to Hub:\n```bash\njina hub push TextToImage\n```\n\n#### Deploy to Kubernetes\n```bash\njina export kubernetes flow.yml ./my-k8s\nkubectl apply -R -f my-k8s\n```\n\n#### Use Docker Compose\n```bash\njina export docker-compose flow.yml docker-compose.yml\ndocker-compose up\n```\n\n#### JCloud Deployment\n\nDeploy with a single command:\n```bash\njina cloud deploy jcloud-flow.yml\n```\n\n## LLM Streaming\n\nEnable token-by-token streaming for responsive LLM applications:\n\n1. Define schemas:\n```python\nfrom docarray import BaseDoc\n\n\nclass PromptDocument(BaseDoc):\n    prompt: str\n    max_tokens: int\n\n\nclass ModelOutputDocument(BaseDoc):\n    token_id: int\n    generated_text: str\n```\n\n2. Initialize service:\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n\nclass TokenStreamingExecutor(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.model = GPT2LMHeadModel.from_pretrained(''gpt2'')\n```\n\n3. Implement streaming:\n```python\n@requests(on=''/stream'')\nasync def task(self, doc: PromptDocument, **kwargs) -> ModelOutputDocument:\n    input = tokenizer(doc.prompt, return_tensors=''pt'')\n    input_len = input[''input_ids''].shape[1]\n    for _ in range(doc.max_tokens):\n        output = self.model.generate(**input, max_new_tokens=1)\n        if output[0][-1] == tokenizer.eos_token_id:\n            break\n        yield ModelOutputDocument(\n            token_id=output[0][-1],\n            generated_text=tokenizer.decode(\n                output[0][input_len:], skip_special_tokens=True\n            ),\n        )\n        input = {\n            ''input_ids'': output,\n            ''attention_mask'': torch.ones(1, len(output[0])),\n        }\n```\n\n4. Serve and use:\n```python\n# Server\nwith Deployment(uses=TokenStreamingExecutor, port=12345, protocol=''grpc'') as dep:\n    dep.block()\n\n\n# Client\nasync def main():\n    client = Client(port=12345, protocol=''grpc'', asyncio=True)\n    async for doc in client.stream_doc(\n        on=''/stream'',\n        inputs=PromptDocument(prompt=''what is the capital of France ?'', max_tokens=10),\n        return_type=ModelOutputDocument,\n    ):\n        print(doc.generated_text)\n```\n\n## Support\n\nJina-serve is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE).\n', '{"language":"Python","stars":21798,"forks":2237,"watchers":21798,"open_issues":15,"topics":["cloud-native","cncf","deep-learning","docker","fastapi","framework","generative-ai","grpc","jaeger","kubernetes","llmops","machine-learning","microservice","mlops","multimodal","neural-search","opentelemetry","orchestration","pipeline","prometheus"],"default_branch":"master","size_kb":1681072,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:jina-ai:jina","source_url":"https://github.com/jina-ai/jina"},{"type":"has_code","target_id":"github:jina-ai:jina","source_url":"https://github.com/jina-ai/jina"}]', NULL, 'Apache-2.0', 'approved', 65, '4fbc92e72eddf31562ee31f22f403f2f', NULL, NULL, CURRENT_TIMESTAMP);
