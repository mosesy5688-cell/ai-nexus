/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-ul2', 'huggingface--google--flan-ul2', 'flan-ul2', 'google', '--- language: - en - fr - ro - de - multilingual widget: - text: ''Translate to German: My name is Arthur'' example_title: Translation - text: >- Please answer to the following question. Who is going to be the next Ballon d''or? example_title: Question Answering - text: >- Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering. example_title: Logical reasoning - text: >- Please answer the following question. What is the boiling point of Nitrogen? e...', '["transformers","pytorch","t5","text2text-generation","flan-ul2","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","dataset:c4","arxiv:2205.05131","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'other', 560, 5769, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-ul2","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n  - en\n  - fr\n  - ro\n  - de\n  - multilingual\nwidget:\n  - text: ''Translate to German:  My name is Arthur''\n    example_title: Translation\n  - text: >-\n      Please answer to the following question. Who is going to be the next\n      Ballon d''or?\n    example_title: Question Answering\n  - text: >-\n      Q: Can Geoffrey Hinton have a conversation with George Washington? Give\n      the rationale before answering.\n    example_title: Logical reasoning\n  - text: >-\n      Please answer the following question. What is the boiling point of\n      Nitrogen?\n    example_title: Scientific knowledge\n  - text: >-\n      Answer the following yes/no question. Can you write a whole Haiku in a\n      single tweet?\n    example_title: Yes/no question\n  - text: >-\n      Answer the following yes/no question by reasoning step-by-step. Can you\n      write a whole Haiku in a single tweet?\n    example_title: Reasoning task\n  - text: ''Q: ( False or not False or False ) is? A: Let''''s think step by step''\n    example_title: Boolean Expressions\n  - text: >-\n      The square root of x is the cube root of y. What is y to the power of 2,\n      if x = 4?\n    example_title: Math reasoning\n  - text: >-\n      Premise:  At my age you will probably have learnt one lesson. Hypothesis: \n      It''s not certain how many lessons you''ll learn by your thirties. Does the\n      premise entail the hypothesis?\n    example_title: Premise and hypothesis\n  - text: >-\n      Answer the following question by reasoning step by step. \n      The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n    example_title: Chain of thought\ntags:\n  - text2text-generation\n  - flan-ul2\ndatasets:\n  - svakulenk0/qrecc\n  - taskmaster2\n  - djaym7/wiki_dialog\n  - deepmind/code_contests\n  - lambada\n  - gsm8k\n  - aqua_rat\n  - esnli\n  - quasc\n  - qed\n  - c4\nlicense: apache-2.0\n---\n\n\n# Model card for Flan-UL2\n\n![model image](https://raw.githubusercontent.com/google-research/google-research/master/ul2/figs/ul2.png)\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Using the model](#using-the-model)\n2. [Results](#results)\n3. [Introduction to UL2](#introduction-to-ul2)\n4. [Training](#training)\n5. [Contribution](#contribution)\n6. [Citation](#citation)\n\n# TL;DR\n\nFlan-UL2 is an encoder decoder model based on the `T5` architecture. It uses the same configuration as the [`UL2 model`](https://huggingface.co/google/ul2)  released earlier last year. It was fine tuned using the "Flan" prompt tuning \nand dataset collection.\n\nAccording to the original [blog](https://www.yitay.net/blog/flan-ul2-20b) here are the notable improvements:\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large. \n- The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning.\n- The original UL2 model also had mode switch tokens that was rather mandatory to get good performance. However, they were a little cumbersome as this requires often some changes during inference or finetuning. In this update/change, we continue training UL2 20B for an additional 100k steps (with small batch) to forget ‚Äúmode tokens‚Äù before applying Flan instruction tuning. This Flan-UL2 checkpoint does not require mode tokens anymore.\n\n# Using the model \n\n## Converting from T5x to huggingface\n\nYou can use the [`convert_t5x_checkpoint_to_pytorch.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/convert_t5x_checkpoint_to_pytorch.py) script and pass the argument `strict = False`. The final layer norm is missing from the original dictionnary, that is why we are passing the `strict = False` argument.\n```bash\npython convert_t5x_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --config_file PATH_TO_CONFIG --pytorch_dump_path PATH_TO_SAVE\n```\nWe used the same config file as [`google/ul2`](https://huggingface.co/google/ul2/blob/main/config.json).\n\n## Running the model\n\nFor more efficient memory usage, we advise you to load the model in `8bit` using `load_in_8bit` flag as follows (works only under GPU):\n\n```python\n# pip install accelerate transformers bitsandbytes\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", device_map="auto", load_in_8bit=True)                                                                 \ntokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")\n\ninput_string = "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?"                                               \n\ninputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>\n```\n\nOtherwise, you can load and run the model in `bfloat16` as follows:\n\n```python\n# pip install accelerate transformers\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", torch_dtype=torch.bfloat16, device_map="auto")                                                                 \ntokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")\n\ninput_string = "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?"                                               \n\ninputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>\n```\n\n# Results\n\n## Performance improvment \n\nThe reported results are the following : \n|  | MMLU | BBH | MMLU-CoT | BBH-CoT | Avg |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| FLAN-PaLM 62B | 59.6 | 47.5 | 56.9 | 44.9 | 49.9 |\n| FLAN-PaLM 540B | 73.5 | 57.9 | 70.9 | 66.3 | 67.2 |\n| FLAN-T5-XXL 11B | 55.1 | 45.3 | 48.6 | 41.4 | 47.6 |\n| FLAN-UL2 20B | 55.7(+1.1%) | 45.9(+1.3%) | 52.2(+7.4%) | 42.7(+3.1%) | 49.1(+3.2%) |\n\n\n# Introduction to UL2\n\nThis entire section has been copied from the [`google/ul2`](https://huggingface.co/google/ul2) model card and might be subject of change with respect to `flan-ul2`.\n\nUL2 is a unified framework for pretraining models that are universally effective across datasets and setups. UL2 uses Mixture-of-Denoisers (MoD), apre-training objective that combines diverse pre-training paradigms together. UL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes.\n\n![model image](https://raw.githubusercontent.com/google-research/google-research/master/ul2/figs/ul2.png)\n\n**Abstract**\n\nExisting pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. \n\nFor more information, please take a look at the original paper.\n\nPaper: [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1)\n\nAuthors: *Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler* \n\n\n## Training\n\n### Flan UL2\nThe Flan-UL2 model was initialized using the `UL2` checkpoints, and was then trained additionally using Flan Prompting. This means that the original training corpus is `C4`, \n\nIn ‚ÄúScaling Instruction-Finetuned language models (Chung et al.)‚Äù (also referred to sometimes as the Flan2 paper), the key idea is to train a large language model on a collection of datasets. These datasets are phrased as instructions which enable generalization across diverse tasks. Flan has been primarily trained on academic tasks. In Flan2, we released a series of T5 models ranging from 200M to 11B parameters that have been instruction tuned with Flan.\n\nThe Flan datasets have also been open sourced in ‚ÄúThe Flan Collection: Designing Data and Methods for Effective Instruction Tuning‚Äù (Longpre et al.). See Google AI Blogpost: ‚ÄúThe Flan Collection: Advancing Open Source Methods for Instruction Tuning‚Äù. \n\n## UL2 PreTraining\n\nThe model is pretrained on the C4 corpus. For pretraining, the model is trained on a total of 1 trillion tokens on C4 (2 million steps)\nwith a batch size of 1024. The sequence length is set to 512/512 for inputs and targets. \nDropout is set to 0 during pretraining. Pre-training took slightly more than one month for about 1 trillion\ntokens. The model has 32 encoder layers and 32 decoder layers, `dmodel` of 4096 and `df` of 16384. \nThe dimension of each head is 256 for a total of 16 heads. Our model uses a model parallelism of 8. \nThe same sentencepiece tokenizer as T5 of vocab size 32000 is used (click [here](https://huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5#transformers.T5Tokenizer) for more information about the T5 tokenizer).\n\nUL-20B can be interpreted as a model that is quite similar to T5 but trained with a different objective and slightly different scaling knobs. \nUL-20B was trained using the [Jax](https://github.com/google/jax) and [T5X](https://github.com/google-research/t5x) infrastructure.\n\nThe training objective during pretraining is a mixture of different denoising strategies that are explained in the following:\n\n### Mixture of Denoisers\n\nTo quote the paper:\n> We conjecture that a strong universal model has to be exposed to solving diverse set of problems\n> during pre-training. Given that pre-training is done using self-supervision, we argue that such diversity\n> should be injected to the objective of the model, otherwise the model might suffer from lack a certain\n> ability, like long-coherent text generation.\n> Motivated by this, as well as current class of objective functions, we define three main paradigms that\n> are used during pre-training:\n\n- **R-Denoiser**: The regular denoising is the standard span corruption introduced in [T5](https://huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5)\n that uses a range of 2 to 5 tokens as the span length, which masks about 15% of\ninput tokens. These spans are short and potentially useful to acquire knowledge instead of\nlearning to generate fluent text.\n\n- **S-Denoiser**: A specific case of denoising where we observe a strict sequential order when\nframing the inputs-to-targets task, i.e., prefix language modeling. To do so, we simply\npartition the input sequence into two sub-sequences of tokens as context and target such that\nthe targets do not rely on future information. This is unlike standard span corruption where\nthere could be a target token with earlier position than a context token. Note that similar to\nthe Prefix-LM setup, the context (prefix) retains a bidirectional receptive field. We note that\nS-Denoising with very short memory or no memory is in similar spirit to standard causal\nlanguage modeling.\n\n- **X-Denoiser**: An extreme version of denoising where the model must recover a large part\nof the input, given a small to moderate part of it. This simulates a situation where a model\nneeds to generate long target from a memory with relatively limited information. To do\nso, we opt to include examples with aggressive denoising where approximately 50% of the\ninput sequence is masked. This is by increasing the span length and/or corruption rate. We\nconsider a pre-training task to be extreme if it has a long span (e.g., ‚â• 12 tokens) or have\na large corruption rate (e.g., ‚â• 30%). X-denoising is motivated by being an interpolation\nbetween regular span corruption and language model like objectives.\n\nSee the following diagram for a more visual explanation:\n\n![mixture-of-denoisers](https://raw.githubusercontent.com/google-research/google-research/master/ul2/figs/mod.png)\n\n**Important**: For more details, please see sections 3.1.2 of the [paper](https://arxiv.org/pdf/2205.05131v1.pdf).\n\n## Fine-tuning\n\nThe model was continously fine-tuned after N pretraining steps where N is typically from 50k to 100k.\nIn other words, after each Nk steps of pretraining, the model is finetuned on each downstream task. See section 5.2.2 of [paper](https://arxiv.org/pdf/2205.05131v1.pdf) to get an overview of all datasets that were used for fine-tuning).\n\nAs the model is continuously finetuned, finetuning is stopped on a task once it has reached state-of-the-art to save compute.\nIn total, the model was trained for 2.65 million steps.\n\n**Important**: For more details, please see sections 5.2.1 and 5.2.2 of the [paper](https://arxiv.org/pdf/2205.05131v1.pdf).\n\n# Contribution\n\nThis model was originally contributed by [Yi Tay](https://www.yitay.net/?author=636616684c5e64780328eece), and added to the Hugging Face ecosystem by [Younes Belkada](https://huggingface.co/ybelkada) & [Arthur Zucker](https://huggingface.co/ArthurZ).\n\n# Citation\n\nIf you want to cite this work, please consider citing the [blogpost](https://www.yitay.net/blog/flan-ul2-20b) announcing the release of `Flan-UL2`.', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":null,"storage_bytes":161423361013,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"based_on_paper","target_id":"arxiv:2205.05131","source_url":"https://arxiv.org/abs/2205.05131"}]', NULL, 'Apache-2.0', 'approved', 77.5, 'ec3fec8020f497a03dca297c8ce88d9a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-bert-bert-base-multilingual-cased', 'huggingface--google-bert--bert-base-multilingual-cased', 'bert-base-multilingual-cased', 'google-bert', '--- language: - multilingual - af - sq - ar - an - hy - ast - az - ba - eu - bar - be - bn - inc - bs - br - bg - my - ca - ceb - ce - zh - cv - hr - cs - da - nl - en - et - fi - fr - gl - ka - de - el - gu - ht - he - hi - hu - is - io - id - ga - it - ja - jv - kn - kk - ky - ko - la - lv - lt - roa - nds - lm - mk - mg - ms - ml - mr - mn - min - ne - new - nb - nn - oc - fa - pms - pl - pt - pa - ro - ru - sco - sr - hr - scn - sk - sl - aze - es - su - sw - sv - tl - tg - th - ta - tt -...', '["transformers","pytorch","tf","jax","safetensors","bert","fill-mask","multilingual","af","sq","ar","an","hy","ast","az","ba","eu","bar","be","bn","inc","bs","br","bg","my","ca","ceb","ce","zh","cv","hr","cs","da","nl","en","et","fi","fr","gl","ka","de","el","gu","ht","he","hi","hu","is","io","id","ga","it","ja","jv","kn","kk","ky","ko","la","lv","lt","roa","nds","lm","mk","mg","ms","ml","mr","mn","min","ne","new","nb","nn","oc","fa","pms","pl","pt","pa","ro","ru","sco","sr","scn","sk","sl","aze","es","su","sw","sv","tl","tg","th","ta","tt","te","tr","uk","ud","uz","vi","vo","war","cy","fry","pnb","yo","dataset:wikipedia","arxiv:1810.04805","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 558, 4646256, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google-bert/bert-base-multilingual-cased","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- multilingual\n- af\n- sq\n- ar\n- an\n- hy\n- ast\n- az\n- ba\n- eu\n- bar\n- be\n- bn\n- inc\n- bs\n- br\n- bg\n- my\n- ca\n- ceb\n- ce\n- zh\n- cv\n- hr\n- cs\n- da\n- nl\n- en\n- et\n- fi\n- fr\n- gl\n- ka\n- de\n- el\n- gu\n- ht\n- he\n- hi\n- hu\n- is\n- io\n- id\n- ga\n- it\n- ja\n- jv\n- kn\n- kk\n- ky\n- ko\n- la\n- lv\n- lt\n- roa\n- nds\n- lm\n- mk\n- mg\n- ms\n- ml\n- mr\n- mn\n- min\n- ne\n- new\n- nb\n- nn\n- oc\n- fa\n- pms\n- pl\n- pt\n- pa\n- ro\n- ru\n- sco\n- sr\n- hr\n- scn\n- sk\n- sl\n- aze\n- es\n- su\n- sw\n- sv\n- tl\n- tg\n- th\n- ta\n- tt\n- te\n- tr\n- uk\n- ud\n- uz\n- vi\n- vo\n- war\n- cy\n- fry\n- pnb\n- yo\nlicense: apache-2.0\ndatasets:\n- wikipedia\n---\n\n# BERT multilingual base model (cased)\n\nPretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.\nIt was introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is case sensitive: it makes a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the languages in the training set that can then be used to\nextract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a\nstandard classifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it''s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''bert-base-multilingual-cased'')\n>>> unmasker("Hello I''m a [MASK] model.")\n\n[{''sequence'': "[CLS] Hello I''m a model model. [SEP]",\n  ''score'': 0.10182085633277893,\n  ''token'': 13192,\n  ''token_str'': ''model''},\n {''sequence'': "[CLS] Hello I''m a world model. [SEP]",\n  ''score'': 0.052126359194517136,\n  ''token'': 11356,\n  ''token_str'': ''world''},\n {''sequence'': "[CLS] Hello I''m a data model. [SEP]",\n  ''score'': 0.048930276185274124,\n  ''token'': 11165,\n  ''token_str'': ''data''},\n {''sequence'': "[CLS] Hello I''m a flight model. [SEP]",\n  ''score'': 0.02036019042134285,\n  ''token'': 23578,\n  ''token_str'': ''flight''},\n {''sequence'': "[CLS] Hello I''m a business model. [SEP]",\n  ''score'': 0.020079681649804115,\n  ''token'': 14155,\n  ''token_str'': ''business''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained(''bert-base-multilingual-cased'')\nmodel = BertModel.from_pretrained("bert-base-multilingual-cased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained(''bert-base-multilingual-cased'')\nmodel = TFBertModel.from_pretrained("bert-base-multilingual-cased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n## Training data\n\nThe BERT model was pretrained on the 104 languages with the largest Wikipedias. You can find the complete list\n[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a\nlarger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese,\nJapanese Kanji and Korean Hanja that don''t have space, a CJK Unicode block is added around every character. \n\nThe inputs of the model are then of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it''s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n"sentences" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":178566653,"storage_bytes":14492059671,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertForMaskedLM"],"model_type":"bert","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:bert","source_url":"https://github.com/google-research/bert"},{"type":"has_code","target_id":"github:google-research:bert","source_url":"https://github.com/google-research/bert"},{"type":"based_on_paper","target_id":"arxiv:1810.04805","source_url":"https://arxiv.org/abs/1810.04805"}]', NULL, 'Apache-2.0', 'approved', 62.5, '6b23b22ffb24663661b86ea6d8aa9d82', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2-27b-it', 'huggingface--google--gemma-2-27b-it', 'gemma-2-27b-it', 'google', '', '["transformers","safetensors","gemma2","text-generation","conversational","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2009.11462","arxiv:2101.11718","arxiv:2110.08193","arxiv:1804.09301","arxiv:2109.07958","arxiv:1804.06876","arxiv:2103.03874","arxiv:2304.06364","arxiv:2206.04615","arxiv:2203.09509","base_model:google/gemma-2-27b","base_model:finetune:google/gemma-2-27b","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 556, 460910, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2-27b-it","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":27227128320,"storage_bytes":54521211517,"files_count":22,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma2ForCausalLM"],"model_type":"gemma2","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 37.5, '41f06f29ecfe0379103476b32b28f645', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lightx2v-Wan2.2-Lightning', 'huggingface--lightx2v--wan2.2-lightning', 'Wan2.2-Lightning', 'lightx2v', '--- language: en license: apache-2.0 base_model: - Wan-AI/Wan2.2-T2V-A14B - Wan-AI/Wan2.2-I2V-A14B - Wan-AI/Wan2.2-TI2V-5B pipeline_tag: text-to-video tags: - text-to-video; - image-to-video; - comfyUI; - video-generation; library_name: diffusers --- You''re welcome to visit our GitHub repository for the latest model releases or to reproduce our results. <!-- [**Wan2.2-Lightning: Distill Wan2.2 Family into 4 Steps**] <be> --> We are excited to release the distilled version of <a href="https://...', '["diffusers","safetensors","text-to-video;","image-to-video;","comfyui;","video-generation;","text-to-video","en","arxiv:2309.14509","base_model:wan-ai/wan2.2-i2v-a14b","base_model:finetune:wan-ai/wan2.2-i2v-a14b","license:apache-2.0","region:us"]', 'text-to-video', 556, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lightx2v/Wan2.2-Lightning","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: en\nlicense: apache-2.0\nbase_model:\n- Wan-AI/Wan2.2-T2V-A14B\n- Wan-AI/Wan2.2-I2V-A14B\n- Wan-AI/Wan2.2-TI2V-5B\npipeline_tag: text-to-video\ntags:\n- text-to-video;\n- image-to-video;\n- comfyUI;\n- video-generation;\nlibrary_name: diffusers\n---\n\nYou''re welcome to visit our [GitHub repository](https://github.com/ModelTC/Wan2.2-Lightning) for the latest model releases or to reproduce our results.\n\n# Wan2.2-Lightning\n\n<!-- [**Wan2.2-Lightning: Distill Wan2.2 Family into 4 Steps**] <be> -->\n\n\nWe are excited to release the distilled version of <a href="https://wan.video"><b>Wan2.2</b></a> video generation model family, which offers the following advantages:\n- **Fast**: Video generation now requires only 4 steps without the need of CFG trick, leading to x20 speed-up\n- **High-quality**: The distilled model delivers visuals on par with the base model in most scenarios, sometimes even better.\n- **Complex Motion Generation**: Despite the reduction to just 4 steps, the model retains excellent motion dynamics in the generated scenes.\n\n\n## üî• Latest News!!\n\n* Aug 08, 2025: üëã Release of Native ComfyUI Workflows.\n\n <!-- and [lora weights](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/tree/main) for the `Wan2.2-Lightning` models! -->\n\n <!-- Choose one of These new [weights](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/tree/main) are also compatible with [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper). -->\n\n<table align="center">\n  <thead>\n    <tr>\n      <th>Model</th>\n      <th>Type</th>\n      <th>For Native Comfy</th>\n      <th>For Kijai''s Wrapper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>Wan2.2-I2V-A14B-NFE4-V1</b></td>\n      <td>Image-to-Video</td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1-NativeComfy.json">I2V-V1-WF</a></td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1-forKJ.json">I2V-V1-WF</a></td>\n    </tr>\n    <tr>\n      <td><b>Wan2.2-T2V-A14B-NFE4-V1.1</b></td>\n      <td>Text-to-Video</td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1-NativeComfy.json">T2V-V1.1-WF</a></td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1-forKJ.json">T2V-V1.1-WF</a></td>\n    </tr>\n    <!-- <tr>\n      <td><b>Wan2.2-T2V-A14B-NFE4-V1</b></td>\n      <td>Text-to-Video</td>\n      <td><a href="https://hf-mirror/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1-NativeComfy.json">Workflow</a></td>\n    </tr> -->\n  </tbody>\n</table>\n\n* Aug 07, 2025: üëã Release of [Wan2.2-I2V-A14B-NFE4-V1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1).\n <!-- A [workflow](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1-forKJ.json) compatible with [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is inside this link. Enjoy! -->\n* Aug 07, 2025: üëã Release of [Wan2.2-T2V-A14B-NFE4-V1.1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1). The generation quality of V1.1 is slightly better than V1.\n<!-- A [workflow](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1-forKJ.json) compatible with [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is inside this link. The generation quality of V1.1 is slightly better than V1. Enjoy! -->\n* Aug 04, 2025: üëã Release of [Wan2.2-T2V-A14B-NFE4-V1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1).\n\n## Video Demos\n### Wan2.2-I2V-A14B-NFE4-V1 Demo\n\nThe videos below can be reproduced using [examples/i2v_prompt_list.txt](examples/i2v_prompt_list.txt) and [examples/i2v_image_path_list.txt](examples/i2v_image_path_list.txt).\n\n<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/4f6bb1e0-9e2b-4eb2-8b9f-0678ccd5b4ec" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/bb249553-3f52-40b3-88f9-6e3bca1a8358" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/17a6d26a-dd63-47ef-9a98-1502f503dfba" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/6ccc69cf-e129-456f-8b93-6dc709cb0ede" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/6cf9c586-f37a-47ed-ab5b-e106c3877fa8" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/27e82fdf-88af-44ac-b987-b48aa3f9f793" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/36a76f1d-2b64-4b16-a862-210d0ffd6d55" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/4bc36c70-931e-4539-be8c-432d832819d3" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/488b9179-741b-4b9d-8f23-895981f054cb" width="100%" controls loop></video>\n     </td>\n  </tr>\n</table>\n\n### Wan2.2-T2V-A14B-NFE4-V1 Demo\n\nThe videos below can be reproduced using [examples/prompt_list.txt](examples/prompt_list.txt).\n\n<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/ae791fbb-ef4a-4f72-989a-2ac862883201" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/f8083a50-25a0-42a8-9cd1-635f99588b19" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/5f15826b-b07b-49a2-a522-f2caea0adc60" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/9e48c7c2-f1a1-4d94-ade0-11e1aa913cb7" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/45ae83df-af1e-4506-b00e-7d413a0dfa51" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/554dd476-d9c1-49df-b6e1-d129113cb2be" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/f22b8c0f-9e40-418d-8cd5-153da3678093" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/2fc03af0-7c76-48e5-ab12-fc222164ec64" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/a8d07ae6-f037-4518-9b13-4a6702a3e0ae" width="100%" controls loop></video>\n     </td>\n  </tr>\n</table>\n\n### Wan2.2-T2V-A14B-NFE4 Limitation\n\nWhen the video contains elements with extremely large motion, the generated results may include artifacts.\nIn some results, the direction of the vehicles may be reversed.\n\n<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/db8f4240-7feb-4b95-8851-c52220ece9dc" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/43820463-22e0-41aa-a446-e0f130ef80d0" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/8a0580eb-2b35-4548-abcb-45fc0df12ff0" width="100%" controls loop></video>\n     </td>\n  </tr>\n</table>\n\n\n\n## üìë Todo List\n- [x] Wan2.2-T2V-A14B-4steps\n- [x] Wan2.2-I2V-A14B-4steps\n- [ ] Wan2.2-TI2V-5B-4steps\n\n## üöÄ Run Wan2.2-Lightning\n\n#### Installation\n\nPlease follow [Wan2.2 Official Github](https://github.com/Wan-Video/Wan2.2/) to install the **Python Environment** and download the **Base Model**.\n\n#### Model Download\n\nDownload models using huggingface-cli:\n``` sh\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B\nhuggingface-cli download lightx2v/Wan2.2-Lightning --local-dir ./Wan2.2-Lightning\n```\n\n#### Run Text-to-Video Generation\n\nThis repository supports the `Wan2.2-T2V-A14B` Text-to-Video model and can simultaneously support video generation at 480P and 720P resolutions, either portrait or landscape.\n\n\n##### (1) Without Prompt Extension\n\nTo facilitate implementation, we will start with a basic version of the inference process that skips the [prompt extension](#2-using-prompt-extention) step.\n\n- Single-GPU, Single-prompt inference\n\n``` sh\npython generate.py  --task t2v-A14B --size "1280*720" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage."\n```\n\n- Single-GPU, Multiple-prompt inference\n``` sh\npython generate.py  --task t2v-A14B --size "1280*720" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt_file examples/prompt_list.txt\n```\n\n> üí° This command can run on a GPU with at least 80GB VRAM.\n\n> üí°If you encounter OOM (Out-of-Memory) issues, you can use the `--offload_model True`, `--convert_model_dtype` and `--t5_cpu` options to reduce GPU memory usage.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n  We use [PyTorch FSDP](https://docs.pytorch.org/docs/stable/fsdp.html) and [DeepSpeed Ulysses](https://arxiv.org/abs/2309.14509) to accelerate inference.\n\n\n``` sh\ntorchrun --nproc_per_node=8 generate.py --task t2v-A14B --size "1280*720" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --base_seed 42 --prompt_file examples/prompt_list.txt\n```\n\n\n##### (2) Using Prompt Extension\n\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\n\n- Use the Dashscope API for extension.\n  - Apply for a `dashscope.api_key` in advance ([EN](https://www.alibabacloud.com/help/en/model-studio/getting-started/first-api-call-to-qwen) | [CN](https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen)).\n  - Configure the environment variable `DASH_API_KEY` to specify the Dashscope API key. For users of Alibaba Cloud''s international site, you also need to set the environment variable `DASH_API_URL` to ''https://dashscope-intl.aliyuncs.com/api/v1''. For more detailed instructions, please refer to the [dashscope document](https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api?spm=a2c63.p38356.0.i1).\n  - Use the `qwen-plus` model for text-to-video tasks and `qwen-vl-max` for image-to-video tasks.\n  - You can modify the model used for extension with the parameter `--prompt_extend_model`. For example:\n```sh\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage" --use_prompt_extend --prompt_extend_method ''dashscope'' --prompt_extend_target_lang ''zh''\n```\n\n- Using a local model for extension.\n\n  - By default, the Qwen model on HuggingFace is used for this extension. Users can choose Qwen models or other models based on the available GPU memory size.\n  - For text-to-video tasks, you can use models like `Qwen/Qwen2.5-14B-Instruct`, `Qwen/Qwen2.5-7B-Instruct` and `Qwen/Qwen2.5-3B-Instruct`.\n  - For image-to-video tasks, you can use models like `Qwen/Qwen2.5-VL-7B-Instruct` and `Qwen/Qwen2.5-VL-3B-Instruct`.\n  - Larger models generally provide better extension results but require more GPU memory.\n  - You can modify the model used for extension with the parameter `--prompt_extend_model` , allowing you to specify either a local model path or a Hugging Face model. For example:\n\n``` sh\ntorchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage" --use_prompt_extend --prompt_extend_method ''local_qwen'' --prompt_extend_target_lang ''zh''\n```\n\n\n#### Run Image-to-Video Generation\n\nThis repository supports the `Wan2.2-I2V-A14B` Image-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\n\n\n- Single-GPU inference\n```sh\npython generate.py  --task i2v-A14B  --size "1280*720" --ckpt_dir ./Wan2.2-I2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt_file examples/i2v_prompt_list.txt --image_path_file examples/i2v_image_path_list.txt\n```\n\n> This command can run on a GPU with at least 80GB VRAM.\n\n> üí°For the Image-to-Video task, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --base_seed 42 --prompt_file examples/i2v_prompt_list.txt --image_path_file examples/i2v_image_path_list.txt\n```\n\n<!-- \n- Image-to-Video Generation without prompt\n\n```sh\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --prompt '''' --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --use_prompt_extend --prompt_extend_method ''dashscope''\n```\n\n> üí°The model can generate videos solely from the input image. You can use prompt extension to generate prompt from the image.\n\n> The process of prompt extension can be referenced [here](#2-using-prompt-extention).\n\n#### Run Text-Image-to-Video Generation\n\nThis repository supports the `Wan2.2-TI2V-5B` Text-Image-to-Video model and can support video generation at 720P resolutions.\n\n\n- Single-GPU Text-to-Video inference\n```sh\npython generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --offload_model True --convert_model_dtype --t5_cpu --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage"\n```\n\n> üí°Unlike other tasks, the 720P resolution of the Text-Image-to-Video task is `1280*704` or `704*1280`.\n\n> This command can run on a GPU with at least 24GB VRAM (e.g, RTX 4090 GPU).\n\n> üí°If you are running on a GPU with at least 80GB VRAM, you can remove the `--offload_model True`, `--convert_model_dtype` and `--t5_cpu` options to speed up execution.\n\n\n- Single-GPU Image-to-Video inference\n```sh\npython generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --offload_model True --convert_model_dtype --t5_cpu --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> üí°If the image parameter is configured, it is an Image-to-Video generation; otherwise, it defaults to a Text-to-Video generation.\n\n> üí°Similar to Image-to-Video, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```sh\ntorchrun --nproc_per_node=8 generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --dit_fsdp --t5_fsdp --ulysses_size 8 --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> The process of prompt extension can be referenced [here](#2-using-prompt-extension). \n-->\n\n\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe built upon and reused code from the following projects: [Wan2.1](https://github.com/Wan-Video/Wan2.1), [Wan2.2](https://github.com/Wan-Video/Wan2.2), licensed under the Apache License 2.0. \n\nWe also adopt the evaluation text prompts from [Movie Gen Bench](https://github.com/facebookresearch/MovieGenBench), which is licensed under the Creative Commons Attribution-NonCommercial 4.0 (CC BY-NC 4.0) License. The original license can be found [here](https://github.com/facebookresearch/MovieGenBench/blob/main/LICENSE).\n\nThe selected prompts are further enhanced using the `Qwen/Qwen2.5-14B-Instruct`model [Qwen](https://huggingface.co/Qwen).', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":55624460095,"files_count":24,"spaces_count":19,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:ModelTC:Wan2.2-Lightning","source_url":"https://github.com/ModelTC/Wan2.2-Lightning"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1","source_url":"https://github.com/Wan-Video/Wan2.1"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:facebookresearch:MovieGenBench","source_url":"https://github.com/facebookresearch/MovieGenBench"},{"type":"has_code","target_id":"github:facebookresearch:MovieGenBench","source_url":"https://github.com/facebookresearch/MovieGenBench"},{"type":"based_on_paper","target_id":"arxiv:2309.14509","source_url":"https://arxiv.org/abs/2309.14509"}]', NULL, 'Apache-2.0', 'approved', 77.5, 'a30e1d3edd4a9363d63e56da31bec46d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-berkeley-nest-Starling-LM-7B-alpha', 'huggingface--berkeley-nest--starling-lm-7b-alpha', 'Starling-LM-7B-alpha', 'berkeley-nest', '--- license: apache-2.0 datasets: - berkeley-nest/Nectar language: - en library_name: transformers tags: - reward model - RLHF - RLAIF --- <!-- Provide a quick summary of what the model is/does. --> - **Developed by:** Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao. - **Model type:** Language Model finetuned with RLHF / RLAIF - **License:** Apache-2.0 license under the condition that the model is not used to compete with OpenAI - **Finetuned from model:** Openchat 3...', '["transformers","safetensors","mistral","text-generation","reward model","rlhf","rlaif","conversational","en","dataset:berkeley-nest/nectar","arxiv:2306.02231","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 554, 1234, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n- berkeley-nest/Nectar\nlanguage:\n- en\nlibrary_name: transformers\ntags:\n- reward model\n- RLHF\n- RLAIF\n---\n# Starling-LM-7B-alpha\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n- **Developed by:** Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\n- **Model type:** Language Model finetuned with RLHF / RLAIF\n- **License:** Apache-2.0 license under the condition that the model is not used to compete with OpenAI\n- **Finetuned from model:** [Openchat 3.5](https://huggingface.co/openchat/openchat_3.5) (based on [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1))\n \n\n\nWe introduce Starling-7B, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). The model harnesses the power of our new GPT-4 labeled ranking dataset, [berkeley-nest/Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI''s GPT-4 and GPT-4 Turbo. We release the ranking dataset [Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), the reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and the language model [Starling-LM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) on HuggingFace, and an online demo in LMSYS [Chatbot Arena](https://chat.lmsys.org). Stay tuned for our forthcoming code and paper, which will provide more details on the whole process.\n\nStarling-LM-7B-alpha is a language model trained from [Openchat 3.5](https://huggingface.co/openchat/openchat_3.5) with reward model [berkeley-nest/Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and policy optimization method [advantage-induced policy alignment (APA)](https://arxiv.org/abs/2306.02231). The evaluation results are listed below.\n\n\n| Model                 | Tuning Method    | MT Bench | AlpacaEval | MMLU |\n|-----------------------|------------------|----------|------------|------|\n| GPT-4-Turbo           | ?                | 9.32     | 97.70      |      |\n| GPT-4                 | SFT + PPO        | 8.99     | 95.28      | 86.4 |\n| **Starling-7B**           | C-RLFT + APA     | 8.09     | 91.99      | 63.9 |\n| Claude-2              | ?                | 8.06     | 91.36      | 78.5 |\n| GPT-3.5-Turbo         | ?                | 7.94     | 89.37      | 70   |\n| Claude-1              | ?                | 7.9      | 88.39      | 77   |\n| Tulu-2-dpo-70b        | SFT + DPO        | 7.89     | 95.1       |      |\n| Openchat-3.5          | C-RLFT           | 7.81     | 88.51      | 64.3 |\n| Zephyr-7B-beta        | SFT + DPO        | 7.34     | 90.60      | 61.4 |\n| Llama-2-70b-chat-hf   | SFT + PPO        | 6.86     | 92.66      | 63   |\n| Neural-chat-7b-v3-1   | SFT + DPO        | 6.84     | 84.53      | 62.4 | \n| Tulu-2-dpo-7b         | SFT + DPO        | 6.29     | 85.1       |      |\n\n\n\nFor more detailed discussions, please check out our [blog post](https://starling.cs.berkeley.edu), and stay tuned for our upcoming code and paper!\n<!-- Provide the basic links for the model. -->\n\n- **Blog:** https://starling.cs.berkeley.edu/\n- **Paper:** Coming soon!\n- **Code:** Coming soon!\n\n\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n**Important: Please use the exact chat template provided below for the model. Otherwise there will be a degrade in the performance. The model output can be verbose in rare cases. Please consider setting temperature = 0 to make this happen less.**\n\nOur model follows the exact chat template and usage as [Openchat 3.5](https://huggingface.co/openchat/openchat_3.5). Please refer to their model card for more details.\nIn addition, our model is hosted on LMSYS [Chatbot Arena](https://chat.lmsys.org) for free test.\n\nThe conversation template is the same as Openchat 3.5:\n```\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained("openchat/openchat_3.5")\n\n# Single-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer("Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n```\n## Code Examples\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("berkeley-nest/Starling-LM-7B-alpha")\nmodel = transformers.AutoModelForCausalLM.from_pretrained("berkeley-nest/Starling-LM-7B-alpha")\n\ndef generate_response(prompt):\n    input_ids = tokenizer(prompt, return_tensors="pt").input_ids\n    outputs = model.generate(\n        input_ids,\n        max_length=256,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    response_ids = outputs[0]\n    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n    return response_text\n\n# Single-turn conversation\nprompt = "Hello, how are you?"\nsingle_turn_prompt = f"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:"\nresponse_text = generate_response(single_turn_prompt)\nprint("Response:", response_text)\n\n## Multi-turn conversation\nprompt = "Hello"\nfollow_up_question =  "How are you today?"\nresponse = ""\nmulti_turn_prompt = f"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: {response}<|end_of_turn|>GPT4 Correct User: {follow_up_question}<|end_of_turn|>GPT4 Correct Assistant:"\nresponse_text = generate_response(multi_turn_prompt)\nprint("Multi-turn conversation response:", response_text)\n\n### Coding conversation\nprompt = "Implement quicksort using C++"\ncoding_prompt = f"Code User: {prompt}<|end_of_turn|>Code Assistant:"\nresponse = generate_response(coding_prompt)\nprint("Coding conversation response:", response)\n```\n\n## License\nThe dataset, model and online demo is a research preview intended for non-commercial use only, subject to the data distillation [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.\n\n\n## Acknowledgment\nWe would like to thank Wei-Lin Chiang from Berkeley for detailed feedback of the blog and the projects. We would like to thank the [LMSYS Organization](https://lmsys.org/) for their support of [lmsys-chat-1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) dataset, evaluation and online demo. We would like to thank the open source community for their efforts in providing the datasets and base models we used to develope the project, including but not limited to Anthropic, Llama, Mistral, Hugging Face H4, LMSYS, OpenChat, OpenBMB, Flan and ShareGPT.\n\n## Citation\n```\n@misc{starling2023,\n    title = {Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF},\n    url = {},\n    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},\n    month = {November},\n    year = {2023}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241748480,"storage_bytes":14484024227,"files_count":14,"spaces_count":39,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{{ bos_token }}{% for message in messages %}{{ ''GPT4 Correct '' + message[''role''].title() + '': '' + message[''content''] + ''<|end_of_turn|>''}}{% endfor %}{% if add_generation_prompt %}{{ ''GPT4 Correct Assistant:'' }}{% endif %}","eos_token":"<|end_of_turn|>","pad_token":"<|end_of_turn|>","sep_token":"<sep>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"based_on_paper","target_id":"arxiv:2306.02231","source_url":"https://arxiv.org/abs/2306.02231"}]', NULL, 'Apache-2.0', 'approved', 62.4, '04c496fcddcf37963cb60efe989787f8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-allenai-Molmo-7B-D-0924', 'huggingface--allenai--molmo-7b-d-0924', 'Molmo-7B-D-0924', 'allenai', '--- license: apache-2.0 language: - en base_model: - openai/clip-vit-large-patch14-336 - Qwen/Qwen2-7B pipeline_tag: image-text-to-text tags: - multimodal - olmo - molmo - pixmo library_name: transformers --- <img src="molmo_logo.png" alt="Logo for the Molmo Project" style="width: auto; height: 50px;"> Molmo is a family of open vision-language models developed by the Allen Institute for AI. Molmo models are trained on PixMo, a dataset of 1 million, highly-curated image-text pairs. It has stat...', '["transformers","safetensors","molmo","text-generation","multimodal","olmo","pixmo","image-text-to-text","conversational","custom_code","en","arxiv:2409.17146","base_model:qwen/qwen2-7b","base_model:finetune:qwen/qwen2-7b","license:apache-2.0","region:us"]', 'image-text-to-text', 554, 38776, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/allenai/Molmo-7B-D-0924","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\nbase_model:\n- openai/clip-vit-large-patch14-336\n- Qwen/Qwen2-7B\npipeline_tag: image-text-to-text\ntags:\n- multimodal\n- olmo\n- molmo\n- pixmo\nlibrary_name: transformers\n---\n\n<img src="molmo_logo.png" alt="Logo for the Molmo Project" style="width: auto; height: 50px;">\n\n# Molmo 7B-D\n\nMolmo is a family of open vision-language models developed by the Allen Institute for AI. Molmo models are trained on PixMo, a dataset of 1 million, highly-curated image-text pairs. It has state-of-the-art performance among multimodal models with a similar size while being fully open-source. You can find all models in the Molmo family [here](https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19).\n**Learn more** about the Molmo family [in our announcement blog post](https://molmo.allenai.org/blog) or the [paper](https://huggingface.co/papers/2409.17146).\n\nMolmo 7B-D is based on [Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B) and uses [OpenAI CLIP](https://huggingface.co/openai/clip-vit-large-patch14-336) as vision backbone. \nIt performs comfortably between GPT-4V and GPT-4o on both academic benchmarks and human evaluation.\nIt powers the **Molmo demo at** [**molmo.allenai.org**](https://molmo.allenai.org).\n\nThis checkpoint is a **preview** of the Molmo release. All artifacts used in creating Molmo (PixMo dataset, training code, evaluations, intermediate checkpoints) will be made available at a later date, furthering our commitment to open-source AI development and reproducibility.\n\n[**Sign up here**](https://docs.google.com/forms/d/e/1FAIpQLSdML1MhNNBDsCHpgWG65Oydg2SjZzVasyqlP08nBrWjZp_c7A/viewform) to be the first to know when artifacts are released.\n\nQuick links:\n- üí¨ [Demo](https://molmo.allenai.org/)\n- üìÇ [All Models](https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19)\n- üìÉ [Paper](https://molmo.allenai.org/paper.pdf)\n- üé• [Blog with Videos](https://molmo.allenai.org/blog)\n\n## Quick Start\n\nTo run Molmo, first install dependencies:\n\n```bash\npip install einops torchvision\n```\n\nThen, follow these steps:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\nfrom PIL import Image\nimport requests\n\n# load the processor\nprocessor = AutoProcessor.from_pretrained(\n    ''allenai/Molmo-7B-D-0924'',\n    trust_remote_code=True,\n    torch_dtype=''auto'',\n    device_map=''auto''\n)\n\n# load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    ''allenai/Molmo-7B-D-0924'',\n    trust_remote_code=True,\n    torch_dtype=''auto'',\n    device_map=''auto''\n)\n\n# process the image and text\ninputs = processor.process(\n    images=[Image.open(requests.get("https://picsum.photos/id/237/536/354", stream=True).raw)],\n    text="Describe this image."\n)\n\n# move inputs to the correct device and make a batch of size 1\ninputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n\n# generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\noutput = model.generate_from_batch(\n    inputs,\n    GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),\n    tokenizer=processor.tokenizer\n)\n\n# only get generated tokens; decode them to text\ngenerated_tokens = output[0,inputs[''input_ids''].size(1):]\ngenerated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n# print the generated text\nprint(generated_text)\n\n# >>>  This image features an adorable black Labrador puppy, captured from a top-down\n#      perspective. The puppy is sitting on a wooden deck, which is composed ...\n```\n\nTo make inference more efficient, run with autocast:\n\n```python\nwith torch.autocast(device_type="cuda", enabled=True, dtype=torch.bfloat16):\n  output = model.generate_from_batch(\n      inputs,\n      GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),\n      tokenizer=processor.tokenizer\n  )\n```\n\nWe did most of our evaluation in this setting (autocast on, but float32 weights)\n\nTo even further reduce the memory requirements, the model can be run with bfloat16 weights:\n\n```python\nmodel.to(dtype=torch.bfloat16)\ninputs["images"] = inputs["images"].to(torch.bfloat16)\noutput = model.generate_from_batch(\n    inputs,\n    GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),\n    tokenizer=processor.tokenizer\n)\n```\n\nNote that we have observed that this can change the output of the model compared to running with float32 weights.\n\n## vLLM\nMolmo is supported in vLLM, however please use version <=0.7.2 until a [prepreprocessing bug](https://github.com/vllm-project/vllm/issues/26451) is fixed.\n\n## Evaluations \n\n| Model                       | Average Score on 11 Academic Benchmarks | Human Preference Elo Rating |\n|-----------------------------|-----------------------------------------|-----------------------------|\n| Molmo 72B                   | 81.2                                    | 1077                        |\n| **Molmo 7B-D (this model)** | **77.3**                                | **1056**                    |\n| Molmo 7B-O                  | 74.6                                    | 1051                        |\n| MolmoE 1B                   | 68.6                                    | 1032                        |\n| GPT-4o                      | 78.5                                    | 1079                        |\n| GPT-4V                      | 71.1                                    | 1041                        |\n| Gemini 1.5 Pro              | 78.3                                    | 1074                        |\n| Gemini 1.5 Flash            | 75.1                                    | 1054                        |\n| Claude 3.5 Sonnet           | 76.7                                    | 1069                        |\n| Claude 3 Opus               | 66.4                                    |  971                        |\n| Claude 3 Haiku              | 65.3                                    |  999                        |\n| Qwen VL2 72B                | 79.4                                    | 1037                        |\n| Qwen VL2 7B                 | 73.7                                    | 1025                        |\n| Intern VL2 LLAMA 76B        | 77.1                                    | 1018                        |\n| Intern VL2 8B               | 69.4                                    |  953                        |\n| Pixtral 12B                 | 69.5                                    | 1016                        |\n| Phi3.5-Vision 4B            | 59.7                                    |  982                        |\n| PaliGemma 3B                | 50.0                                    |  937                        |\n| LLAVA OneVision 72B         | 76.6                                    | 1051                        |\n| LLAVA OneVision 7B          | 72.0                                    | 1024                        |\n| Cambrian-1 34B              | 66.8                                    |  953                        |\n| Cambrian-1 8B               | 63.4                                    |  952                        |\n| xGen - MM - Interleave 4B   | 59.5                                    |  979                        |\n| LLAVA-1.5 13B               | 43.9                                    |  960                        |\n| LLAVA-1.5 7B                | 40.7                                    |  951                        |\n\n*Benchmarks: AI2D test, ChartQA test, VQA v2.0 test, DocQA test, InfographicVQA test, TextVQA val, RealWorldQA, MMMU val, MathVista testmini, CountBenchQA, Flickr Count (we collected this new dataset that is significantly harder than CountBenchQA).*\n\n## FAQs\n\n### I''m getting an error a broadcast error when processing images!\n\nYour image might not be in RGB format. You can convert it using the following code snippet:\n\n```python\nfrom PIL import Image\n\nimage = Image.open(...)\n\nif image.mode != "RGB":\n    image = image.convert("RGB")\n```\n\n### Molmo doesn''t work great with transparent images!\n\nWe received reports that Molmo models might struggle with transparent images. \nFor the time being, we recommend adding a white or dark background to your images before passing them to the model. The code snippet below shows how to do this using the Python Imaging Library (PIL):\n\n```python\n\n# Load the image\nurl = "..."\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Convert the image to grayscale to calculate brightness\ngray_image = image.convert(''L'')  # Convert to grayscale\n\n# Calculate the average brightness\nstat = ImageStat.Stat(gray_image)\naverage_brightness = stat.mean[0]  # Get the average value\n\n# Define background color based on brightness (threshold can be adjusted)\nbg_color = (0, 0, 0) if average_brightness > 127 else (255, 255, 255)\n\n# Create a new image with the same size as the original, filled with the background color\nnew_image = Image.new(''RGB'', image.size, bg_color)\n\n# Paste the original image on top of the background (use image as a mask if needed)\nnew_image.paste(image, (0, 0), image if image.mode == ''RGBA'' else None)\n\n# Now you can pass the new_image to Molmo\nprocessor = AutoProcessor.from_pretrained(\n    ''allenai/Molmo-7B-D-0924'',\n    trust_remote_code=True,\n    torch_dtype=''auto'',\n    device_map=''auto''\n)\n```\n\n## License and Use\n\nThis model is licensed under Apache 2.0. It is intended for research and educational use.\nFor more information, please see our [Responsible Use Guidelines](https://allenai.org/responsible-use).', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8021025280,"storage_bytes":35884012544,"files_count":25,"spaces_count":28,"gated":false,"private":false,"config":{"architectures":["MolmoForCausalLM"],"auto_map":{"AutoConfig":"config_molmo.MolmoConfig","AutoModelForCausalLM":"modeling_molmo.MolmoForCausalLM"},"model_type":"molmo","tokenizer_config":{"bos_token":null,"chat_template":"{% for message in messages -%}\n        {%- if (loop.index % 2 == 1 and message[''role''] != ''user'') or \n          (loop.index % 2 == 0 and message[''role''].lower() != ''assistant'') -%}\n        {{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}\n        {%- endif -%}\n        {{ message[''role''].capitalize() + '': '' + message[''content''] }}\n        {%- if not loop.last -%}\n        {{ '' '' }}\n        {%- endif %}\n        {%- endfor -%}\n        {%- if add_generation_prompt -%}\n        {{ '' Assistant:'' }}\n        {%- endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"based_on_paper","target_id":"arxiv:2409.17146","source_url":"https://arxiv.org/abs/2409.17146"}]', NULL, 'Apache-2.0', 'approved', 62.4, '6161f68a0b65caae3191151924858e7c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nvidia-Llama3-ChatQA-1.5-8B', 'huggingface--nvidia--llama3-chatqa-1.5-8b', 'Llama3-ChatQA-1.5-8B', 'nvidia', '--- license: llama3 language: - en pipeline_tag: text-generation tags: - nvidia - chatqa-1.5 - chatqa - llama-3 - pytorch --- We introduce Llama3-ChatQA-1.5, which excels at conversational question answering (QA) and retrieval-augmented generation (RAG). Llama3-ChatQA-1.5 is developed using an improved training recipe from ChatQA paper, and it is built on top of Llama-3 base model. Specifically, we incorporate more conversational QA data to enhance its tabular and arithmetic calculation capab...', '["transformers","safetensors","llama","text-generation","nvidia","chatqa-1.5","chatqa","llama-3","pytorch","conversational","en","arxiv:2401.10225","license:llama3","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 553, 10684, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama3\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nvidia\n- chatqa-1.5\n- chatqa\n- llama-3\n- pytorch\n---\n\n\n## Model Details\nWe introduce Llama3-ChatQA-1.5, which excels at conversational question answering (QA) and retrieval-augmented generation (RAG). Llama3-ChatQA-1.5 is developed using an improved training recipe from [ChatQA paper](https://arxiv.org/pdf/2401.10225), and it is built on top of [Llama-3 base model](https://huggingface.co/meta-llama/Meta-Llama-3-8B). Specifically, we incorporate more conversational QA data to enhance its tabular and arithmetic calculation capability. Llama3-ChatQA-1.5 has two variants: Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B. Both models were originally trained using [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), we converted the checkpoints to Hugging Face format. **For more information about ChatQA, check the [website](https://chatqa-project.github.io/)!**\n\n## Other Resources\n[Llama3-ChatQA-1.5-70B](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B) &ensp; [Evaluation Data](https://huggingface.co/datasets/nvidia/ChatRAG-Bench) &ensp; [Training Data](https://huggingface.co/datasets/nvidia/ChatQA-Training-Data) &ensp; [Retriever](https://huggingface.co/nvidia/dragon-multiturn-query-encoder) &ensp; [Website](https://chatqa-project.github.io/) &ensp; [Paper](https://arxiv.org/pdf/2401.10225)\n\n## Benchmark Results\nResults in [ChatRAG Bench](https://huggingface.co/datasets/nvidia/ChatRAG-Bench) are as follows:\n\n| | ChatQA-1.0-7B | Command-R-Plus | Llama3-instruct-70b | GPT-4-0613 | GPT-4-Turbo | ChatQA-1.0-70B | ChatQA-1.5-8B | ChatQA-1.5-70B |\n| -- |:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n| Doc2Dial | 37.88 | 33.51 | 37.88 | 34.16 | 35.35 | 38.90 | 39.33 | 41.26 |\n| QuAC | 29.69 | 34.16 | 36.96 | 40.29 | 40.10 | 41.82 | 39.73 | 38.82 |\n| QReCC | 46.97 | 49.77 | 51.34 | 52.01 | 51.46 | 48.05 | 49.03 | 51.40 |\n| CoQA | 76.61 | 69.71 | 76.98 | 77.42 | 77.73 | 78.57 | 76.46 | 78.44 |\n| DoQA | 41.57 | 40.67 | 41.24 | 43.39 | 41.60 | 51.94 | 49.60 | 50.67 |\n| ConvFinQA | 51.61 | 71.21 | 76.6 | 81.28 | 84.16 | 73.69 | 78.46 | 81.88 |\n| SQA | 61.87 | 74.07 | 69.61 | 79.21 | 79.98 | 69.14 | 73.28 | 83.82 |\n| TopioCQA | 45.45 | 53.77 | 49.72 | 45.09 | 48.32 | 50.98 | 49.96 | 55.63 |\n| HybriDial* | 54.51 | 46.7 | 48.59 | 49.81 | 47.86 | 56.44 | 65.76 | 68.27 |\n| INSCIT | 30.96 | 35.76 | 36.23 | 36.34 | 33.75 | 31.90 | 30.10 | 32.31 |\n| Average (all) | 47.71 | 50.93 | 52.52 | 53.90 | 54.03 | 54.14 | 55.17 | 58.25 |\n| Average (exclude HybriDial) | 46.96 | 51.40 | 52.95 | 54.35 | 54.72 | 53.89 | 53.99 | 57.14 |\n\nNote that ChatQA-1.5 is built based on Llama-3 base model, and ChatQA-1.0 is built based on Llama-2 base model. ChatQA-1.5 models use HybriDial training dataset. To ensure fair comparison, we also compare average scores excluding HybriDial. The data and evaluation scripts for ChatRAG Bench can be found [here](https://huggingface.co/datasets/nvidia/ChatRAG-Bench).\n\n\n## Prompt Format\n**We highly recommend that you use the prompt format we provide, as follows:**\n### when context is available\n<pre>\nSystem: {System}\n\n{Context}\n\nUser: {Question}\n\nAssistant: {Response}\n\nUser: {Question}\n\nAssistant:\n</pre>\n\n### when context is not available\n<pre>\nSystem: {System}\n\nUser: {Question}\n\nAssistant: {Response}\n\nUser: {Question}\n\nAssistant:\n</pre>\n**The content of the system''s turn (i.e., {System}) for both scenarios is as follows:**\n<pre>\nThis is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user''s questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n</pre>\n**Note that our ChatQA-1.5 models are optimized for the capability with context, e.g., over documents or retrieved context.**\n\n## How to use\n\n### take the whole document as context \nThis can be applied to the scenario where the whole document can be fitted into the model, so that there is no need to run retrieval over the document.\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = "nvidia/Llama3-ChatQA-1.5-8B"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")\n\nmessages = [\n    {"role": "user", "content": "what is the percentage change of the net income from Q4 FY23 to Q4 FY24?"}\n]\n\ndocument = """NVIDIA (NASDAQ: NVDA) today reported revenue for the fourth quarter ended January 28, 2024, of $22.1 billion, up 22% from the previous quarter and up 265% from a year ago.\nFor the quarter, GAAP earnings per diluted share was $4.93, up 33% from the previous quarter and up 765% from a year ago. Non-GAAP earnings per diluted share was $5.16, up 28% from the previous quarter and up 486% from a year ago.\nQ4 Fiscal 2024 Summary\nGAAP\n| $ in millions, except earnings per share | Q4 FY24 | Q3 FY24 | Q4 FY23 | Q/Q | Y/Y |\n| Revenue | $22,103 | $18,120 | $6,051 | Up 22% | Up 265% |\n| Gross margin | 76.0% | 74.0% | 63.3% | Up 2.0 pts | Up 12.7 pts |\n| Operating expenses | $3,176 | $2,983 | $2,576 | Up 6% | Up 23% |\n| Operating income | $13,615 | $10,417 | $1,257 | Up 31% | Up 983% |\n| Net income | $12,285 | $9,243 | $1,414 | Up 33% | Up 769% |\n| Diluted earnings per share | $4.93 | $3.71 | $0.57 | Up 33% | Up 765% |"""\n\ndef get_formatted_input(messages, context):\n    system = "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user''s questions based on the context. The assistant should also indicate when the answer cannot be found in the context."\n    instruction = "Please give a full and complete answer for the question."\n\n    for item in messages:\n        if item[''role''] == "user":\n            ## only apply this instruction for the first user turn\n            item[''content''] = instruction + " " + item[''content'']\n            break\n\n    conversation = ''\n\n''.join(["User: " + item["content"] if item["role"] == "user" else "Assistant: " + item["content"] for item in messages]) + "\n\nAssistant:"\n    formatted_input = system + "\n\n" + context + "\n\n" + conversation\n    \n    return formatted_input\n\nformatted_input = get_formatted_input(messages, document)\ntokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors="pt").to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids("<|eot_id|>")\n]\n\noutputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n\nresponse = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n### run retrieval to get top-n chunks as context\nThis can be applied to the scenario when the document is very long, so that it is necessary to run retrieval. Here, we use our [Dragon-multiturn](https://huggingface.co/nvidia/dragon-multiturn-query-encoder) retriever which can handle conversatinoal query. In addition, we provide a few [documents](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B/tree/main/docs) for users to play with.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport torch\nimport json\n\n## load ChatQA-1.5 tokenizer and model\nmodel_id = "nvidia/Llama3-ChatQA-1.5-8B"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")\n\n## load retriever tokenizer and model\nretriever_tokenizer = AutoTokenizer.from_pretrained(''nvidia/dragon-multiturn-query-encoder'')\nquery_encoder = AutoModel.from_pretrained(''nvidia/dragon-multiturn-query-encoder'')\ncontext_encoder = AutoModel.from_pretrained(''nvidia/dragon-multiturn-context-encoder'')\n\n## prepare documents, we take landrover car manual document that we provide as an example\nchunk_list = json.load(open("docs.json"))[''landrover'']\n\nmessages = [\n    {"role": "user", "content": "how to connect the bluetooth in the car?"}\n]\n\n### running retrieval\n## convert query into a format as follows:\n## user: {user}\nagent: {agent}\nuser: {user}\nformatted_query_for_retriever = ''\n''.join([turn[''role''] + ": " + turn[''content''] for turn in messages]).strip()\n\nquery_input = retriever_tokenizer(formatted_query_for_retriever, return_tensors=''pt'')\nctx_input = retriever_tokenizer(chunk_list, padding=True, truncation=True, max_length=512, return_tensors=''pt'')\nquery_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\nctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]\n\n## Compute similarity scores using dot product and rank the similarity\nsimilarities = query_emb.matmul(ctx_emb.transpose(0, 1)) # (1, num_ctx)\nranked_results = torch.argsort(similarities, dim=-1, descending=True) # (1, num_ctx)\n\n## get top-n chunks (n=5)\nretrieved_chunks = [chunk_list[idx] for idx in ranked_results.tolist()[0][:5]]\ncontext = "\n\n".join(retrieved_chunks)\n\n### running text generation\nformatted_input = get_formatted_input(messages, context)\ntokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors="pt").to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids("<|eot_id|>")\n]\noutputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n\nresponse = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n## Correspondence to\nZihan Liu (zihanl@nvidia.com), Wei Ping (wping@nvidia.com)\n\n## Citation\n<pre>\n@article{liu2024chatqa,\n  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},\n  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},\n  journal={arXiv preprint arXiv:2401.10225},\n  year={2024}}\n</pre>\n\n\n## License\nThe use of this model is governed by the [META LLAMA 3 COMMUNITY LICENSE AGREEMENT](https://llama.meta.com/llama3/license/)\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030263296,"storage_bytes":32121201772,"files_count":11,"spaces_count":17,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{ bos_token }}{%- if messages[0][''role''] == ''system'' -%}{% set loop_messages = messages[1:] %}{%- else -%}{% set loop_messages = messages %}{% endif %}System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user''s questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n\n{% for message in loop_messages %}{%- if message[''role''] == ''user'' -%}User: {{ message[''content''].strip() + ''\n\n'' }}{%- else -%}Assistant: {{ message[''content''].strip() + ''\n\n'' }}{%- endif %}{% if loop.last and message[''role''] == ''user'' %}Assistant:{% endif %}{% endfor %}","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"based_on_paper","target_id":"arxiv:2401.10225","source_url":"https://arxiv.org/abs/2401.10225"}]', NULL, 'LLaMA-3', 'approved', 77.4, '8bf52ce97d0902f0e823dfd97bd25496', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-gsdf-Counterfeit-V3.0', 'huggingface--gsdf--counterfeit-v3.0', 'Counterfeit-V3.0', 'gsdf', '--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image --- „ÉªI have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective. „ÉªI prioritize the freedom of composition, which may result in a higher possibility of anatomical errors. „ÉªThe expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints. „ÉªI have uploaded a new Negative Embedding...', '["stable-diffusion","stable-diffusion-diffusers","text-to-image","license:creativeml-openrail-m","region:us"]', 'text-to-image', 552, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/gsdf/Counterfeit-V3.0","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n---\n\n# Counterfeit-V3.0\n„ÉªI have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective.  \n„ÉªI prioritize the freedom of composition, which may result in a higher possibility of anatomical errors.  \n„ÉªThe expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints.  \n„ÉªI have uploaded a new Negative Embedding, trained with Counterfeit-V3.0.   \nThere''s likely no clear superiority or inferiority between this and the previous embedding, so feel free to choose according to your preference.Note that I''m not specifically recommending the use of this embedding.  \n\n# Sample image\nprompt & Setting: https://civitai.com/models/4468/counterfeit-v30\n![01](https://huggingface.co/gsdf/Counterfeit-V3.0/resolve/main/images/01.png)\n![02](https://huggingface.co/gsdf/Counterfeit-V3.0/resolve/main/images/02.png)\n\n\n', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":39965733259,"files_count":9,"spaces_count":67,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 47.4, '2c6e584dbd1fa25a1358b355952e30d9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-deepseek-coder-33b-instruct', 'huggingface--deepseek-ai--deepseek-coder-33b-instruct', 'deepseek-coder-33b-instruct', 'deepseek-ai', '--- license: other license_name: deepseek license_link: LICENSE --- <p align="center"> <img width="1000px" alt="DeepSeek Coder" src="https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/pictures/logo.png?raw=true"> </p> <p align="center"><a href="https://www.deepseek.com/">[üè†Homepage]</a> | <a href="https://coder.deepseek.com/">[ü§ñ Chat with DeepSeek Coder]</a> | <a href="https://discord.gg/Tc7c45Zzu5">[Discord]</a> | <a href="https://github.com/guoday/assert/blob/main/QR.png?raw=true">[W...', '["transformers","pytorch","safetensors","llama","text-generation","conversational","license:other","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 552, 21202, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: deepseek\nlicense_link: LICENSE\n---\n\n\n<p align="center">\n<img width="1000px" alt="DeepSeek Coder" src="https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/pictures/logo.png?raw=true">\n</p>\n<p align="center"><a href="https://www.deepseek.com/">[üè†Homepage]</a>  |  <a href="https://coder.deepseek.com/">[ü§ñ Chat with DeepSeek Coder]</a>  |  <a href="https://discord.gg/Tc7c45Zzu5">[Discord]</a>  |  <a href="https://github.com/guoday/assert/blob/main/QR.png?raw=true">[Wechat(ÂæÆ‰ø°)]</a> </p>\n<hr>\n\n\n\n### 1. Introduction of Deepseek Coder\n\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks. \n\n- **Massive Training Data**: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\n  \n- **Highly Flexible & Scalable**: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\n  \n- **Superior Model Performance**: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\n  \n- **Advanced Code Completion Capabilities**: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n\n \n  \n### 2. Model Summary\ndeepseek-coder-33b-instruct is a 33B parameter model initialized from deepseek-coder-33b-base and fine-tuned on 2B tokens of instruction data.\n- **Home Page:** [DeepSeek](https://deepseek.com/)\n- **Repository:** [deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)\n- **Chat With DeepSeek Coder:** [DeepSeek-Coder](https://coder.deepseek.com/)\n\n\n### 3. How to Use\nHere give some examples of how to use our model.\n#### Chat Model Inference\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-instruct", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-instruct", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { ''role'': ''user'', ''content'': "write a quick sort algorithm in python."}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n```\n\n### 4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\n\nSee the [LICENSE-MODEL](https://github.com/deepseek-ai/deepseek-coder/blob/main/LICENSE-MODEL) for more details.\n\n### 5. Contact\n\nIf you have any questions, please raise an issue or contact us at [agi_code@deepseek.com](mailto:agi_code@deepseek.com).\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":33342991360,"storage_bytes":133372226408,"files_count":23,"spaces_count":85,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<|EOT|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message[''role''] == ''system'' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{bos_token}}{%- if not ns.found -%}\n{{''You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n''}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message[''role''] == ''system'' %}\n{{ message[''content''] }}\n    {%- else %}\n        {%- if message[''role''] == ''user'' %}\n{{''### Instruction:\\n'' + message[''content''] + ''\\n''}}\n        {%- else %}\n{{''### Response:\\n'' + message[''content''] + ''\\n<|EOT|>\\n''}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{''### Response:''}}\n{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder"},{"type":"has_code","target_id":"github:guoday:assert","source_url":"https://github.com/guoday/assert"},{"type":"has_code","target_id":"github:deepseek-ai:deepseek-coder","source_url":"https://github.com/deepseek-ai/deepseek-coder"},{"type":"has_code","target_id":"github:deepseek-ai:deepseek-coder","source_url":"https://github.com/deepseek-ai/deepseek-coder"}]', NULL, 'Other', 'approved', 62.4, '039c4be9951040469c72fc6742b1859f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-intfloat-e5-mistral-7b-instruct', 'huggingface--intfloat--e5-mistral-7b-instruct', 'e5-mistral-7b-instruct', 'intfloat', '--- tags: - mteb - sentence-transformers - transformers model-index: - name: e5-mistral-7b-instruct results: - task: type: STS dataset: type: C-MTEB/AFQMC name: MTEB AFQMC config: default split: validation revision: None metrics: - type: cos_sim_pearson value: 37.863226091673866 - type: cos_sim_spearman value: 38.98733013335281 - type: euclidean_pearson value: 37.51783380497874 - type: euclidean_spearman value: 38.98733012753365 - type: manhattan_pearson value: 37.26706888081721 - type: manha...', '["sentence-transformers","pytorch","safetensors","mistral","feature-extraction","mteb","transformers","en","arxiv:2401.00368","arxiv:2104.08663","arxiv:2210.07316","arxiv:2212.03533","license:mit","model-index","text-generation-inference","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'feature-extraction', 552, 153509, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/intfloat/e5-mistral-7b-instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- mteb\n- sentence-transformers\n- transformers\nmodel-index:\n- name: e5-mistral-7b-instruct\n  results:\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/AFQMC\n      name: MTEB AFQMC\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 37.863226091673866\n    - type: cos_sim_spearman\n      value: 38.98733013335281\n    - type: euclidean_pearson\n      value: 37.51783380497874\n    - type: euclidean_spearman\n      value: 38.98733012753365\n    - type: manhattan_pearson\n      value: 37.26706888081721\n    - type: manhattan_spearman\n      value: 38.709750161903834\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/ATEC\n      name: MTEB ATEC\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 43.33924583134623\n    - type: cos_sim_spearman\n      value: 42.84316155158754\n    - type: euclidean_pearson\n      value: 45.62709879515238\n    - type: euclidean_spearman\n      value: 42.843155921732404\n    - type: manhattan_pearson\n      value: 45.4786950991229\n    - type: manhattan_spearman\n      value: 42.657334751855984\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 78.68656716417911\n    - type: ap\n      value: 41.71522322900398\n    - type: f1\n      value: 72.37207703532552\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 74.04710920770879\n    - type: ap\n      value: 83.42622221864045\n    - type: f1\n      value: 72.14388257905772\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.93103448275862\n    - type: ap\n      value: 26.039284760509513\n    - type: f1\n      value: 64.81092954450712\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.21627408993577\n    - type: ap\n      value: 24.876490553983036\n    - type: f1\n      value: 63.8773359684989\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 95.90679999999999\n    - type: ap\n      value: 94.32357863164454\n    - type: f1\n      value: 95.90485634708557\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 55.786\n    - type: f1\n      value: 55.31211995815146\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 53.26\n    - type: f1\n      value: 52.156230111544986\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 50.33\n    - type: f1\n      value: 49.195023008878145\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 49.3\n    - type: f1\n      value: 48.434470184108\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.68599999999999\n    - type: f1\n      value: 47.62681775202072\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 46.238\n    - type: f1\n      value: 45.014030559653705\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 36.486000000000004\n    - type: map_at_10\n      value: 53.076\n    - type: map_at_100\n      value: 53.657999999999994\n    - type: map_at_1000\n      value: 53.659\n    - type: map_at_3\n      value: 48.234\n    - type: map_at_5\n      value: 51.121\n    - type: mrr_at_1\n      value: 37.269000000000005\n    - type: mrr_at_10\n      value: 53.335\n    - type: mrr_at_100\n      value: 53.916\n    - type: mrr_at_1000\n      value: 53.918\n    - type: mrr_at_3\n      value: 48.518\n    - type: mrr_at_5\n      value: 51.406\n    - type: ndcg_at_1\n      value: 36.486000000000004\n    - type: ndcg_at_10\n      value: 61.882000000000005\n    - type: ndcg_at_100\n      value: 64.165\n    - type: ndcg_at_1000\n      value: 64.203\n    - type: ndcg_at_3\n      value: 52.049\n    - type: ndcg_at_5\n      value: 57.199\n    - type: precision_at_1\n      value: 36.486000000000004\n    - type: precision_at_10\n      value: 8.982999999999999\n    - type: precision_at_100\n      value: 0.9939999999999999\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 21.029\n    - type: precision_at_5\n      value: 15.092\n    - type: recall_at_1\n      value: 36.486000000000004\n    - type: recall_at_10\n      value: 89.82900000000001\n    - type: recall_at_100\n      value: 99.36\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 63.087\n    - type: recall_at_5\n      value: 75.46199999999999\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 50.45119266859667\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 45.4958298992051\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 66.98177472838887\n    - type: mrr\n      value: 79.91854636591478\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.67086498650698\n    - type: cos_sim_spearman\n      value: 85.54773239564638\n    - type: euclidean_pearson\n      value: 86.48229161588425\n    - type: euclidean_spearman\n      value: 85.54773239564638\n    - type: manhattan_pearson\n      value: 86.67533327742343\n    - type: manhattan_spearman\n      value: 85.76099026691983\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/BQ\n      name: MTEB BQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 50.31998888922809\n    - type: cos_sim_spearman\n      value: 50.6369940530675\n    - type: euclidean_pearson\n      value: 50.055544636296055\n    - type: euclidean_spearman\n      value: 50.63699405154838\n    - type: manhattan_pearson\n      value: 50.00739378036807\n    - type: manhattan_spearman\n      value: 50.607237418676945\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.5615866388309\n    - type: f1\n      value: 99.49895615866389\n    - type: precision\n      value: 99.46764091858039\n    - type: recall\n      value: 99.5615866388309\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.19656614571869\n    - type: f1\n      value: 99.08650671362535\n    - type: precision\n      value: 99.0314769975787\n    - type: recall\n      value: 99.19656614571869\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.0256321440942\n    - type: f1\n      value: 97.83743216718624\n    - type: precision\n      value: 97.74390947927492\n    - type: recall\n      value: 98.0256321440942\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26276987888363\n    - type: f1\n      value: 99.22766368264\n    - type: precision\n      value: 99.21011058451816\n    - type: recall\n      value: 99.26276987888363\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 88.22727272727272\n    - type: f1\n      value: 88.17411732496673\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 43.530637846246975\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 40.23505728593893\n  - task:\n      type: Clustering\n    dataset:\n      type: C-MTEB/CLSClusteringP2P\n      name: MTEB CLSClusteringP2P\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: v_measure\n      value: 44.419028279451275\n  - task:\n      type: Clustering\n    dataset:\n      type: C-MTEB/CLSClusteringS2S\n      name: MTEB CLSClusteringS2S\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: v_measure\n      value: 42.5820277929776\n  - task:\n      type: Reranking\n    dataset:\n      type: C-MTEB/CMedQAv1-reranking\n      name: MTEB CMedQAv1\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 77.67811726152972\n    - type: mrr\n      value: 80.99003968253969\n  - task:\n      type: Reranking\n    dataset:\n      type: C-MTEB/CMedQAv2-reranking\n      name: MTEB CMedQAv2\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 78.66055354534922\n    - type: mrr\n      value: 81.66119047619047\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.162333333333333\n    - type: map_at_10\n      value: 37.22291666666667\n    - type: map_at_100\n      value: 38.56733333333333\n    - type: map_at_1000\n      value: 38.684250000000006\n    - type: map_at_3\n      value: 34.22858333333333\n    - type: map_at_5\n      value: 35.852500000000006\n    - type: mrr_at_1\n      value: 32.459833333333336\n    - type: mrr_at_10\n      value: 41.65358333333333\n    - type: mrr_at_100\n      value: 42.566916666666664\n    - type: mrr_at_1000\n      value: 42.61766666666667\n    - type: mrr_at_3\n      value: 39.210499999999996\n    - type: mrr_at_5\n      value: 40.582166666666666\n    - type: ndcg_at_1\n      value: 32.459833333333336\n    - type: ndcg_at_10\n      value: 42.96758333333333\n    - type: ndcg_at_100\n      value: 48.5065\n    - type: ndcg_at_1000\n      value: 50.556583333333336\n    - type: ndcg_at_3\n      value: 38.004416666666664\n    - type: ndcg_at_5\n      value: 40.25916666666667\n    - type: precision_at_1\n      value: 32.459833333333336\n    - type: precision_at_10\n      value: 7.664583333333333\n    - type: precision_at_100\n      value: 1.2349999999999999\n    - type: precision_at_1000\n      value: 0.15966666666666668\n    - type: precision_at_3\n      value: 17.731166666666663\n    - type: precision_at_5\n      value: 12.575333333333335\n    - type: recall_at_1\n      value: 27.162333333333333\n    - type: recall_at_10\n      value: 55.44158333333334\n    - type: recall_at_100\n      value: 79.56966666666666\n    - type: recall_at_1000\n      value: 93.45224999999999\n    - type: recall_at_3\n      value: 41.433083333333336\n    - type: recall_at_5\n      value: 47.31108333333333\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.539\n    - type: map_at_10\n      value: 28.494999999999997\n    - type: map_at_100\n      value: 30.568\n    - type: map_at_1000\n      value: 30.741000000000003\n    - type: map_at_3\n      value: 23.846999999999998\n    - type: map_at_5\n      value: 26.275\n    - type: mrr_at_1\n      value: 37.394\n    - type: mrr_at_10\n      value: 50.068\n    - type: mrr_at_100\n      value: 50.727\n    - type: mrr_at_1000\n      value: 50.751000000000005\n    - type: mrr_at_3\n      value: 46.938\n    - type: mrr_at_5\n      value: 48.818\n    - type: ndcg_at_1\n      value: 37.394\n    - type: ndcg_at_10\n      value: 38.349\n    - type: ndcg_at_100\n      value: 45.512\n    - type: ndcg_at_1000\n      value: 48.321\n    - type: ndcg_at_3\n      value: 32.172\n    - type: ndcg_at_5\n      value: 34.265\n    - type: precision_at_1\n      value: 37.394\n    - type: precision_at_10\n      value: 11.927999999999999\n    - type: precision_at_100\n      value: 1.966\n    - type: precision_at_1000\n      value: 0.25\n    - type: precision_at_3\n      value: 24.126\n    - type: precision_at_5\n      value: 18.306\n    - type: recall_at_1\n      value: 16.539\n    - type: recall_at_10\n      value: 44.504\n    - type: recall_at_100\n      value: 68.605\n    - type: recall_at_1000\n      value: 84.1\n    - type: recall_at_3\n      value: 29.008\n    - type: recall_at_5\n      value: 35.58\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/CmedqaRetrieval\n      name: MTEB CmedqaRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 19.482\n    - type: map_at_10\n      value: 28.622999999999998\n    - type: map_at_100\n      value: 30.262\n    - type: map_at_1000\n      value: 30.432\n    - type: map_at_3\n      value: 25.647\n    - type: map_at_5\n      value: 27.128000000000004\n    - type: mrr_at_1\n      value: 30.408\n    - type: mrr_at_10\n      value: 37.188\n    - type: mrr_at_100\n      value: 38.196000000000005\n    - type: mrr_at_1000\n      value: 38.273\n    - type: mrr_at_3\n      value: 35.067\n    - type: mrr_at_5\n      value: 36.124\n    - type: ndcg_at_1\n      value: 30.408\n    - type: ndcg_at_10\n      value: 34.215\n    - type: ndcg_at_100\n      value: 41.349999999999994\n    - type: ndcg_at_1000\n      value: 44.689\n    - type: ndcg_at_3\n      value: 30.264999999999997\n    - type: ndcg_at_5\n      value: 31.572\n    - type: precision_at_1\n      value: 30.408\n    - type: precision_at_10\n      value: 7.6770000000000005\n    - type: precision_at_100\n      value: 1.352\n    - type: precision_at_1000\n      value: 0.178\n    - type: precision_at_3\n      value: 17.213\n    - type: precision_at_5\n      value: 12.198\n    - type: recall_at_1\n      value: 19.482\n    - type: recall_at_10\n      value: 42.368\n    - type: recall_at_100\n      value: 72.694\n    - type: recall_at_1000\n      value: 95.602\n    - type: recall_at_3\n      value: 30.101\n    - type: recall_at_5\n      value: 34.708\n  - task:\n      type: PairClassification\n    dataset:\n      type: C-MTEB/CMNLI\n      name: MTEB Cmnli\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: cos_sim_accuracy\n      value: 71.16055321707758\n    - type: cos_sim_ap\n      value: 80.21073839711723\n    - type: cos_sim_f1\n      value: 72.9740932642487\n    - type: cos_sim_precision\n      value: 65.53136050623488\n    - type: cos_sim_recall\n      value: 82.3240589198036\n    - type: dot_accuracy\n      value: 71.16055321707758\n    - type: dot_ap\n      value: 80.212299264122\n    - type: dot_f1\n      value: 72.9740932642487\n    - type: dot_precision\n      value: 65.53136050623488\n    - type: dot_recall\n      value: 82.3240589198036\n    - type: euclidean_accuracy\n      value: 71.16055321707758\n    - type: euclidean_ap\n      value: 80.21076298680417\n    - type: euclidean_f1\n      value: 72.9740932642487\n    - type: euclidean_precision\n      value: 65.53136050623488\n    - type: euclidean_recall\n      value: 82.3240589198036\n    - type: manhattan_accuracy\n      value: 70.71557426337944\n    - type: manhattan_ap\n      value: 79.93448977199749\n    - type: manhattan_f1\n      value: 72.83962726826877\n    - type: manhattan_precision\n      value: 62.7407908077053\n    - type: manhattan_recall\n      value: 86.81318681318682\n    - type: max_accuracy\n      value: 71.16055321707758\n    - type: max_ap\n      value: 80.212299264122\n    - type: max_f1\n      value: 72.9740932642487\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/CovidRetrieval\n      name: MTEB CovidRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 60.643\n    - type: map_at_10\n      value: 69.011\n    - type: map_at_100\n      value: 69.533\n    - type: map_at_1000\n      value: 69.545\n    - type: map_at_3\n      value: 67.167\n    - type: map_at_5\n      value: 68.12700000000001\n    - type: mrr_at_1\n      value: 60.801\n    - type: mrr_at_10\n      value: 69.111\n    - type: mrr_at_100\n      value: 69.6\n    - type: mrr_at_1000\n      value: 69.611\n    - type: mrr_at_3\n      value: 67.229\n    - type: mrr_at_5\n      value: 68.214\n    - type: ndcg_at_1\n      value: 60.801\n    - type: ndcg_at_10\n      value: 73.128\n    - type: ndcg_at_100\n      value: 75.614\n    - type: ndcg_at_1000\n      value: 75.92\n    - type: ndcg_at_3\n      value: 69.261\n    - type: ndcg_at_5\n      value: 70.973\n    - type: precision_at_1\n      value: 60.801\n    - type: precision_at_10\n      value: 8.662\n    - type: precision_at_100\n      value: 0.9860000000000001\n    - type: precision_at_1000\n      value: 0.101\n    - type: precision_at_3\n      value: 25.149\n    - type: precision_at_5\n      value: 15.953999999999999\n    - type: recall_at_1\n      value: 60.643\n    - type: recall_at_10\n      value: 85.959\n    - type: recall_at_100\n      value: 97.576\n    - type: recall_at_1000\n      value: 100.0\n    - type: recall_at_3\n      value: 75.184\n    - type: recall_at_5\n      value: 79.32000000000001\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.183\n    - type: map_at_10\n      value: 23.958\n    - type: map_at_100\n      value: 34.354\n    - type: map_at_1000\n      value: 36.442\n    - type: map_at_3\n      value: 16.345000000000002\n    - type: map_at_5\n      value: 19.647000000000002\n    - type: mrr_at_1\n      value: 74.25\n    - type: mrr_at_10\n      value: 80.976\n    - type: mrr_at_100\n      value: 81.256\n    - type: mrr_at_1000\n      value: 81.262\n    - type: mrr_at_3\n      value: 79.958\n    - type: mrr_at_5\n      value: 80.37100000000001\n    - type: ndcg_at_1\n      value: 62.0\n    - type: ndcg_at_10\n      value: 48.894999999999996\n    - type: ndcg_at_100\n      value: 53.867\n    - type: ndcg_at_1000\n      value: 61.304\n    - type: ndcg_at_3\n      value: 53.688\n    - type: ndcg_at_5\n      value: 50.900999999999996\n    - type: precision_at_1\n      value: 74.25\n    - type: precision_at_10\n      value: 39.525\n    - type: precision_at_100\n      value: 12.323\n    - type: precision_at_1000\n      value: 2.539\n    - type: precision_at_3\n      value: 57.49999999999999\n    - type: precision_at_5\n      value: 49.1\n    - type: recall_at_1\n      value: 10.183\n    - type: recall_at_10\n      value: 29.296\n    - type: recall_at_100\n      value: 60.394999999999996\n    - type: recall_at_1000\n      value: 83.12\n    - type: recall_at_3\n      value: 17.495\n    - type: recall_at_5\n      value: 22.235\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/DuRetrieval\n      name: MTEB DuRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.613999999999997\n    - type: map_at_10\n      value: 79.77300000000001\n    - type: map_at_100\n      value: 82.71\n    - type: map_at_1000\n      value: 82.75\n    - type: map_at_3\n      value: 55.92700000000001\n    - type: map_at_5\n      value: 70.085\n    - type: mrr_at_1\n      value: 90.7\n    - type: mrr_at_10\n      value: 93.438\n    - type: mrr_at_100\n      value: 93.504\n    - type: mrr_at_1000\n      value: 93.50699999999999\n    - type: mrr_at_3\n      value: 93.125\n    - type: mrr_at_5\n      value: 93.34\n    - type: ndcg_at_1\n      value: 90.7\n    - type: ndcg_at_10\n      value: 87.023\n    - type: ndcg_at_100\n      value: 90.068\n    - type: ndcg_at_1000\n      value: 90.43299999999999\n    - type: ndcg_at_3\n      value: 86.339\n    - type: ndcg_at_5\n      value: 85.013\n    - type: precision_at_1\n      value: 90.7\n    - type: precision_at_10\n      value: 41.339999999999996\n    - type: precision_at_100\n      value: 4.806\n    - type: precision_at_1000\n      value: 0.48900000000000005\n    - type: precision_at_3\n      value: 76.983\n    - type: precision_at_5\n      value: 64.69\n    - type: recall_at_1\n      value: 26.613999999999997\n    - type: recall_at_10\n      value: 87.681\n    - type: recall_at_100\n      value: 97.44699999999999\n    - type: recall_at_1000\n      value: 99.348\n    - type: recall_at_3\n      value: 57.809999999999995\n    - type: recall_at_5\n      value: 74.258\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/EcomRetrieval\n      name: MTEB EcomRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.9\n    - type: map_at_10\n      value: 40.467\n    - type: map_at_100\n      value: 41.423\n    - type: map_at_1000\n      value: 41.463\n    - type: map_at_3\n      value: 37.25\n    - type: map_at_5\n      value: 39.31\n    - type: mrr_at_1\n      value: 30.9\n    - type: mrr_at_10\n      value: 40.467\n    - type: mrr_at_100\n      value: 41.423\n    - type: mrr_at_1000\n      value: 41.463\n    - type: mrr_at_3\n      value: 37.25\n    - type: mrr_at_5\n      value: 39.31\n    - type: ndcg_at_1\n      value: 30.9\n    - type: ndcg_at_10\n      value: 45.957\n    - type: ndcg_at_100\n      value: 50.735\n    - type: ndcg_at_1000\n      value: 51.861999999999995\n    - type: ndcg_at_3\n      value: 39.437\n    - type: ndcg_at_5\n      value: 43.146\n    - type: precision_at_1\n      value: 30.9\n    - type: precision_at_10\n      value: 6.35\n    - type: precision_at_100\n      value: 0.861\n    - type: precision_at_1000\n      value: 0.095\n    - type: precision_at_3\n      value: 15.267\n    - type: precision_at_5\n      value: 10.96\n    - type: recall_at_1\n      value: 30.9\n    - type: recall_at_10\n      value: 63.5\n    - type: recall_at_100\n      value: 86.1\n    - type: recall_at_1000\n      value: 95.1\n    - type: recall_at_3\n      value: 45.800000000000004\n    - type: recall_at_5\n      value: 54.800000000000004\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 49.765\n    - type: f1\n      value: 45.93242203574485\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 75.138\n    - type: map_at_10\n      value: 84.21300000000001\n    - type: map_at_100\n      value: 84.43\n    - type: map_at_1000\n      value: 84.441\n    - type: map_at_3\n      value: 83.071\n    - type: map_at_5\n      value: 83.853\n    - type: mrr_at_1\n      value: 80.948\n    - type: mrr_at_10\n      value: 88.175\n    - type: mrr_at_100\n      value: 88.24\n    - type: mrr_at_1000\n      value: 88.241\n    - type: mrr_at_3\n      value: 87.516\n    - type: mrr_at_5\n      value: 87.997\n    - type: ndcg_at_1\n      value: 80.948\n    - type: ndcg_at_10\n      value: 87.84100000000001\n    - type: ndcg_at_100\n      value: 88.576\n    - type: ndcg_at_1000\n      value: 88.75699999999999\n    - type: ndcg_at_3\n      value: 86.176\n    - type: ndcg_at_5\n      value: 87.214\n    - type: precision_at_1\n      value: 80.948\n    - type: precision_at_10\n      value: 10.632\n    - type: precision_at_100\n      value: 1.123\n    - type: precision_at_1000\n      value: 0.11499999999999999\n    - type: precision_at_3\n      value: 33.193\n    - type: precision_at_5\n      value: 20.663\n    - type: recall_at_1\n      value: 75.138\n    - type: recall_at_10\n      value: 94.89699999999999\n    - type: recall_at_100\n      value: 97.751\n    - type: recall_at_1000\n      value: 98.833\n    - type: recall_at_3\n      value: 90.455\n    - type: recall_at_5\n      value: 93.085\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.45\n    - type: map_at_10\n      value: 48.596000000000004\n    - type: map_at_100\n      value: 50.70400000000001\n    - type: map_at_1000\n      value: 50.83800000000001\n    - type: map_at_3\n      value: 42.795\n    - type: map_at_5\n      value: 46.085\n    - type: mrr_at_1\n      value: 56.172999999999995\n    - type: mrr_at_10\n      value: 64.35300000000001\n    - type: mrr_at_100\n      value: 64.947\n    - type: mrr_at_1000\n      value: 64.967\n    - type: mrr_at_3\n      value: 62.653999999999996\n    - type: mrr_at_5\n      value: 63.534\n    - type: ndcg_at_1\n      value: 56.172999999999995\n    - type: ndcg_at_10\n      value: 56.593\n    - type: ndcg_at_100\n      value: 62.942\n    - type: ndcg_at_1000\n      value: 64.801\n    - type: ndcg_at_3\n      value: 53.024\n    - type: ndcg_at_5\n      value: 53.986999999999995\n    - type: precision_at_1\n      value: 56.172999999999995\n    - type: precision_at_10\n      value: 15.494\n    - type: precision_at_100\n      value: 2.222\n    - type: precision_at_1000\n      value: 0.254\n    - type: precision_at_3\n      value: 35.185\n    - type: precision_at_5\n      value: 25.556\n    - type: recall_at_1\n      value: 29.45\n    - type: recall_at_10\n      value: 62.882000000000005\n    - type: recall_at_100\n      value: 85.56099999999999\n    - type: recall_at_1000\n      value: 96.539\n    - type: recall_at_3\n      value: 47.911\n    - type: recall_at_5\n      value: 54.52\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.581\n    - type: map_at_10\n      value: 68.401\n    - type: map_at_100\n      value: 69.207\n    - type: map_at_1000\n      value: 69.25200000000001\n    - type: map_at_3\n      value: 64.689\n    - type: map_at_5\n      value: 67.158\n    - type: mrr_at_1\n      value: 79.163\n    - type: mrr_at_10\n      value: 85.22999999999999\n    - type: mrr_at_100\n      value: 85.386\n    - type: mrr_at_1000\n      value: 85.39099999999999\n    - type: mrr_at_3\n      value: 84.432\n    - type: mrr_at_5\n      value: 84.952\n    - type: ndcg_at_1\n      value: 79.163\n    - type: ndcg_at_10\n      value: 75.721\n    - type: ndcg_at_100\n      value: 78.411\n    - type: ndcg_at_1000\n      value: 79.23599999999999\n    - type: ndcg_at_3\n      value: 70.68799999999999\n    - type: ndcg_at_5\n      value: 73.694\n    - type: precision_at_1\n      value: 79.163\n    - type: precision_at_10\n      value: 16.134\n    - type: precision_at_100\n      value: 1.821\n    - type: precision_at_1000\n      value: 0.193\n    - type: precision_at_3\n      value: 46.446\n    - type: precision_at_5\n      value: 30.242\n    - type: recall_at_1\n      value: 39.581\n    - type: recall_at_10\n      value: 80.66799999999999\n    - type: recall_at_100\n      value: 91.033\n    - type: recall_at_1000\n      value: 96.408\n    - type: recall_at_3\n      value: 69.669\n    - type: recall_at_5\n      value: 75.604\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/IFlyTek-classification\n      name: MTEB IFlyTek\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 45.04809542131589\n    - type: f1\n      value: 37.01181779071118\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 94.78120000000001\n    - type: ap\n      value: 92.52931921594387\n    - type: f1\n      value: 94.77902110732532\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/JDReview-classification\n      name: MTEB JDReview\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 85.81613508442777\n    - type: ap\n      value: 52.430320593468394\n    - type: f1\n      value: 79.95467268178068\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/LCQMC\n      name: MTEB LCQMC\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 71.05801751913393\n    - type: cos_sim_spearman\n      value: 75.47954644971965\n    - type: euclidean_pearson\n      value: 74.27472296759713\n    - type: euclidean_spearman\n      value: 75.47954201369866\n    - type: manhattan_pearson\n      value: 74.30508190186474\n    - type: manhattan_spearman\n      value: 75.51326518159436\n  - task:\n      type: Reranking\n    dataset:\n      type: C-MTEB/Mmarco-reranking\n      name: MTEB MMarcoReranking\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map\n      value: 24.21110921666315\n    - type: mrr\n      value: 22.863492063492064\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/MMarcoRetrieval\n      name: MTEB MMarcoRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 61.38400000000001\n    - type: map_at_10\n      value: 70.895\n    - type: map_at_100\n      value: 71.314\n    - type: map_at_1000\n      value: 71.331\n    - type: map_at_3\n      value: 69.016\n    - type: map_at_5\n      value: 70.179\n    - type: mrr_at_1\n      value: 63.481\n    - type: mrr_at_10\n      value: 71.543\n    - type: mrr_at_100\n      value: 71.91300000000001\n    - type: mrr_at_1000\n      value: 71.928\n    - type: mrr_at_3\n      value: 69.90899999999999\n    - type: mrr_at_5\n      value: 70.907\n    - type: ndcg_at_1\n      value: 63.481\n    - type: ndcg_at_10\n      value: 74.833\n    - type: ndcg_at_100\n      value: 76.705\n    - type: ndcg_at_1000\n      value: 77.13600000000001\n    - type: ndcg_at_3\n      value: 71.236\n    - type: ndcg_at_5\n      value: 73.199\n    - type: precision_at_1\n      value: 63.481\n    - type: precision_at_10\n      value: 9.179\n    - type: precision_at_100\n      value: 1.011\n    - type: precision_at_1000\n      value: 0.105\n    - type: precision_at_3\n      value: 27.044\n    - type: precision_at_5\n      value: 17.272000000000002\n    - type: recall_at_1\n      value: 61.38400000000001\n    - type: recall_at_10\n      value: 86.318\n    - type: recall_at_100\n      value: 94.786\n    - type: recall_at_1000\n      value: 98.14500000000001\n    - type: recall_at_3\n      value: 76.717\n    - type: recall_at_5\n      value: 81.416\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.363999999999997\n    - type: map_at_10\n      value: 36.022\n    - type: map_at_100\n      value: 37.229\n    - type: map_at_1000\n      value: 37.274\n    - type: map_at_3\n      value: 32.131\n    - type: map_at_5\n      value: 34.391\n    - type: mrr_at_1\n      value: 24.069\n    - type: mrr_at_10\n      value: 36.620000000000005\n    - type: mrr_at_100\n      value: 37.769999999999996\n    - type: mrr_at_1000\n      value: 37.809\n    - type: mrr_at_3\n      value: 32.846\n    - type: mrr_at_5\n      value: 35.02\n    - type: ndcg_at_1\n      value: 24.069\n    - type: ndcg_at_10\n      value: 43.056\n    - type: ndcg_at_100\n      value: 48.754\n    - type: ndcg_at_1000\n      value: 49.829\n    - type: ndcg_at_3\n      value: 35.167\n    - type: ndcg_at_5\n      value: 39.168\n    - type: precision_at_1\n      value: 24.069\n    - type: precision_at_10\n      value: 6.762\n    - type: precision_at_100\n      value: 0.96\n    - type: precision_at_1000\n      value: 0.105\n    - type: precision_at_3\n      value: 14.957\n    - type: precision_at_5\n      value: 11.023\n    - type: recall_at_1\n      value: 23.363999999999997\n    - type: recall_at_10\n      value: 64.696\n    - type: recall_at_100\n      value: 90.795\n    - type: recall_at_1000\n      value: 98.892\n    - type: recall_at_3\n      value: 43.247\n    - type: recall_at_5\n      value: 52.86300000000001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 96.11947104423166\n    - type: f1\n      value: 95.89561841159332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.97548605240912\n    - type: f1\n      value: 92.17133696717212\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.37224816544364\n    - type: f1\n      value: 93.19978829237863\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 91.28719072972127\n    - type: f1\n      value: 91.28448045979604\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.8131946934385\n    - type: f1\n      value: 88.27883019362747\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 85.52260397830018\n    - type: f1\n      value: 85.15528226728568\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 86.10807113543093\n    - type: f1\n      value: 70.88498219072167\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.77120315581854\n    - type: f1\n      value: 57.97153920153224\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 79.93995997331554\n    - type: f1\n      value: 58.839203810064866\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.801440651425\n    - type: f1\n      value: 58.68009647839332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 72.90785227680172\n    - type: f1\n      value: 49.83760954655788\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 73.24050632911391\n    - type: f1\n      value: 52.0562553541082\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.47948890383321\n    - type: f1\n      value: 63.334877563135485\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 44.2871553463349\n    - type: f1\n      value: 43.17658050605427\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.174176193678555\n    - type: f1\n      value: 59.236659587042425\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.226630800269\n    - type: f1\n      value: 60.951842696956184\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.94283792871555\n    - type: f1\n      value: 61.40057652844215\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 55.480833893745796\n    - type: f1\n      value: 52.5298332072816\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.52858103564223\n    - type: f1\n      value: 69.3770851919204\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.09213180901143\n    - type: f1\n      value: 71.13518469365879\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.31203765971756\n    - type: f1\n      value: 66.05906970865144\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 80.57162071284465\n    - type: f1\n      value: 77.7866172598823\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.09414929388029\n    - type: f1\n      value: 72.5712594833695\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.20914593140553\n    - type: f1\n      value: 68.90619124909186\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.74243443174176\n    - type: f1\n      value: 64.72743141749955\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.11096166778749\n    - type: f1\n      value: 72.61849933064694\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.22394082044384\n    - type: f1\n      value: 62.43648797607235\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.44855413584399\n    - type: f1\n      value: 66.56851670913659\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.4149293880296\n    - type: f1\n      value: 66.12960877904776\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 56.916610625420304\n    - type: f1\n      value: 54.02534600927991\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.71351714862138\n    - type: f1\n      value: 69.70227985126316\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 59.91257565568257\n    - type: f1\n      value: 57.06811572144974\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.25218560860793\n    - type: f1\n      value: 72.48057563104247\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 76.35507733691998\n    - type: f1\n      value: 73.03024649541128\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.918628110289184\n    - type: f1\n      value: 54.75590124456177\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 52.548755884330866\n    - type: f1\n      value: 51.5356975360209\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 46.44922663080027\n    - type: f1\n      value: 44.561114416830975\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 53.95763281775386\n    - type: f1\n      value: 50.68367245122476\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.20645595158035\n    - type: f1\n      value: 71.78450093258185\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 59.226630800269\n    - type: f1\n      value: 57.53988988993337\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.44922663080027\n    - type: f1\n      value: 48.58809018065056\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.3752521856086\n    - type: f1\n      value: 49.91373941436425\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.85205110961668\n    - type: f1\n      value: 67.05660019588582\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 49.1492938802959\n    - type: f1\n      value: 46.717578025393195\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.93140551445865\n    - type: f1\n      value: 67.45406609372205\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.82851378614662\n    - type: f1\n      value: 71.15951964393868\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.84868863483524\n    - type: f1\n      value: 71.76056802364877\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.27236045729657\n    - type: f1\n      value: 72.48733090101163\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.63012777404168\n    - type: f1\n      value: 66.56444015346203\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 76.62743779421655\n    - type: f1\n      value: 73.82720656992142\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.15198386012105\n    - type: f1\n      value: 64.41418309797744\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.8399462004035\n    - type: f1\n      value: 56.050989519693886\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.86684599865501\n    - type: f1\n      value: 70.80682480844303\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.36718224613316\n    - type: f1\n      value: 54.998746471013774\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 53.150638870208475\n    - type: f1\n      value: 49.79179342620099\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.50638870208473\n    - type: f1\n      value: 49.778960742003555\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.906523201076\n    - type: f1\n      value: 66.75784022138245\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.73234700739744\n    - type: f1\n      value: 65.75016141148413\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.06792199058508\n    - type: f1\n      value: 67.90334782594083\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.09145931405515\n    - type: f1\n      value: 58.88703095210731\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.17014122394083\n    - type: f1\n      value: 68.43676277921544\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.99327505043712\n    - type: f1\n      value: 72.26813373392943\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.13987895090787\n    - type: f1\n      value: 70.29309514467575\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.37256220578345\n    - type: f1\n      value: 72.56456170538992\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 47.205783456624076\n    - type: f1\n      value: 45.905999859074434\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.8352387357095\n    - type: f1\n      value: 69.43553987525273\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.00403496973773\n    - type: f1\n      value: 65.97477215779143\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.04976462676531\n    - type: f1\n      value: 67.24581993778398\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 61.882985877605925\n    - type: f1\n      value: 59.995293199988794\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.75857431069267\n    - type: f1\n      value: 76.52031675299841\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.03496973772697\n    - type: f1\n      value: 79.25548063175344\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.96570275722931\n    - type: f1\n      value: 72.19110435289122\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 82.38735709482178\n    - type: f1\n      value: 82.34495627619785\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.83994620040352\n    - type: f1\n      value: 78.91526355393667\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.7350369872226\n    - type: f1\n      value: 75.919437344927\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.21721587088096\n    - type: f1\n      value: 70.82973286243262\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.59784801613988\n    - type: f1\n      value: 78.47383161087423\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.64021519838602\n    - type: f1\n      value: 68.45118053027653\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.51042367182245\n    - type: f1\n      value: 72.90013022879003\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.0551445864156\n    - type: f1\n      value: 73.45871761713292\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 59.54606590450571\n    - type: f1\n      value: 57.72711794953869\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.40753194351042\n    - type: f1\n      value: 76.8157455506521\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.58372562205783\n    - type: f1\n      value: 65.2654868709758\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.39273705447208\n    - type: f1\n      value: 78.3592956594837\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.62004034969739\n    - type: f1\n      value: 79.78673754501855\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.29051782111634\n    - type: f1\n      value: 63.12502587609454\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 57.51849361129791\n    - type: f1\n      value: 56.32320906403241\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 52.41761936785474\n    - type: f1\n      value: 49.113762010098306\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 58.547410894418284\n    - type: f1\n      value: 56.87580674198118\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.89038332212507\n    - type: f1\n      value: 79.09210140529848\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.503698722259585\n    - type: f1\n      value: 61.45718858568352\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 54.02824478816408\n    - type: f1\n      value: 52.732738981386504\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 54.23671822461331\n    - type: f1\n      value: 52.688080372545286\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.5312710154674\n    - type: f1\n      value: 74.59368478550698\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 52.192333557498316\n    - type: f1\n      value: 50.18302290152229\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.6960322797579\n    - type: f1\n      value: 75.25331182714856\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.47679892400808\n    - type: f1\n      value: 78.24044732352424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.36718224613315\n    - type: f1\n      value: 77.2714452985389\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.96234028244788\n    - type: f1\n      value: 78.21282127011372\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.19435104236717\n    - type: f1\n      value: 73.1963711292812\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 80.52118359112306\n    - type: f1\n      value: 80.4179964390288\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.65837256220577\n    - type: f1\n      value: 73.07156989634905\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.02824478816409\n    - type: f1\n      value: 62.972399027713664\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.87020847343645\n    - type: f1\n      value: 78.224240866849\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.6570275722932\n    - type: f1\n      value: 63.274871811412545\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 57.760591795561524\n    - type: f1\n      value: 56.73711528075771\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 57.26967047747142\n    - type: f1\n      value: 55.74735330863165\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.46133154001345\n    - type: f1\n      value: 71.9644168952811\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.70880968392737\n    - type: f1\n      value: 73.61543141070884\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.0437121721587\n    - type: f1\n      value: 74.83359868879921\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.05110961667788\n    - type: f1\n      value: 66.25869819274315\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.52118359112306\n    - type: f1\n      value: 75.92098546052303\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.92938802958977\n    - type: f1\n      value: 79.79833572573796\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.86617350369872\n    - type: f1\n      value: 77.42645654909516\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/MedicalRetrieval\n      name: MTEB MedicalRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 44.6\n    - type: map_at_10\n      value: 50.019000000000005\n    - type: map_at_100\n      value: 50.611\n    - type: map_at_1000\n      value: 50.67\n    - type: map_at_3\n      value: 48.699999999999996\n    - type: map_at_5\n      value: 49.455\n    - type: mrr_at_1\n      value: 44.800000000000004\n    - type: mrr_at_10\n      value: 50.119\n    - type: mrr_at_100\n      value: 50.711\n    - type: mrr_at_1000\n      value: 50.77\n    - type: mrr_at_3\n      value: 48.8\n    - type: mrr_at_5\n      value: 49.555\n    - type: ndcg_at_1\n      value: 44.6\n    - type: ndcg_at_10\n      value: 52.754\n    - type: ndcg_at_100\n      value: 55.935\n    - type: ndcg_at_1000\n      value: 57.607\n    - type: ndcg_at_3\n      value: 50.012\n    - type: ndcg_at_5\n      value: 51.393\n    - type: precision_at_1\n      value: 44.6\n    - type: precision_at_10\n      value: 6.140000000000001\n    - type: precision_at_100\n      value: 0.77\n    - type: precision_at_1000\n      value: 0.09\n    - type: precision_at_3\n      value: 17.933\n    - type: precision_at_5\n      value: 11.44\n    - type: recall_at_1\n      value: 44.6\n    - type: recall_at_10\n      value: 61.4\n    - type: recall_at_100\n      value: 77.0\n    - type: recall_at_1000\n      value: 90.4\n    - type: recall_at_3\n      value: 53.800000000000004\n    - type: recall_at_5\n      value: 57.199999999999996\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 38.192667527616315\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 37.44738902946689\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 32.59661273103955\n    - type: mrr\n      value: 33.82024242497473\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/MultilingualSentiment-classification\n      name: MTEB MultilingualSentiment\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 73.31333333333335\n    - type: f1\n      value: 73.0873466527602\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.471\n    - type: map_at_10\n      value: 14.142\n    - type: map_at_100\n      value: 18.179000000000002\n    - type: map_at_1000\n      value: 19.772000000000002\n    - type: map_at_3\n      value: 9.716\n    - type: map_at_5\n      value: 11.763\n    - type: mrr_at_1\n      value: 51.393\n    - type: mrr_at_10\n      value: 58.814\n    - type: mrr_at_100\n      value: 59.330000000000005\n    - type: mrr_at_1000\n      value: 59.35\n    - type: mrr_at_3\n      value: 56.398\n    - type: mrr_at_5\n      value: 58.038999999999994\n    - type: ndcg_at_1\n      value: 49.69\n    - type: ndcg_at_10\n      value: 38.615\n    - type: ndcg_at_100\n      value: 35.268\n    - type: ndcg_at_1000\n      value: 43.745\n    - type: ndcg_at_3\n      value: 43.187\n    - type: ndcg_at_5\n      value: 41.528999999999996\n    - type: precision_at_1\n      value: 51.083999999999996\n    - type: precision_at_10\n      value: 29.474\n    - type: precision_at_100\n      value: 9.167\n    - type: precision_at_1000\n      value: 2.2089999999999996\n    - type: precision_at_3\n      value: 40.351\n    - type: precision_at_5\n      value: 36.285000000000004\n    - type: recall_at_1\n      value: 5.471\n    - type: recall_at_10\n      value: 19.242\n    - type: recall_at_100\n      value: 37.14\n    - type: recall_at_1000\n      value: 68.35900000000001\n    - type: recall_at_3\n      value: 10.896\n    - type: recall_at_5\n      value: 14.75\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.499\n    - type: map_at_10\n      value: 55.862\n    - type: map_at_100\n      value: 56.667\n    - type: map_at_1000\n      value: 56.684999999999995\n    - type: map_at_3\n      value: 51.534\n    - type: map_at_5\n      value: 54.2\n    - type: mrr_at_1\n      value: 44.351\n    - type: mrr_at_10\n      value: 58.567\n    - type: mrr_at_100\n      value: 59.099000000000004\n    - type: mrr_at_1000\n      value: 59.109\n    - type: mrr_at_3\n      value: 55.218999999999994\n    - type: mrr_at_5\n      value: 57.391999999999996\n    - type: ndcg_at_1\n      value: 44.322\n    - type: ndcg_at_10\n      value: 63.535\n    - type: ndcg_at_100\n      value: 66.654\n    - type: ndcg_at_1000\n      value: 66.991\n    - type: ndcg_at_3\n      value: 55.701\n    - type: ndcg_at_5\n      value: 60.06700000000001\n    - type: precision_at_1\n      value: 44.322\n    - type: precision_at_10\n      value: 10.026\n    - type: precision_at_100\n      value: 1.18\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 24.865000000000002\n    - type: precision_at_5\n      value: 17.48\n    - type: recall_at_1\n      value: 39.499\n    - type: recall_at_10\n      value: 84.053\n    - type: recall_at_100\n      value: 97.11\n    - type: recall_at_1000\n      value: 99.493\n    - type: recall_at_3\n      value: 64.091\n    - type: recall_at_5\n      value: 74.063\n  - task:\n      type: PairClassification\n    dataset:\n      type: C-MTEB/OCNLI\n      name: MTEB Ocnli\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: cos_sim_accuracy\n      value: 61.18029236599891\n    - type: cos_sim_ap\n      value: 64.18398769398412\n    - type: cos_sim_f1\n      value: 67.96347757046446\n    - type: cos_sim_precision\n      value: 54.4529262086514\n    - type: cos_sim_recall\n      value: 90.3907074973601\n    - type: dot_accuracy\n      value: 61.18029236599891\n    - type: dot_ap\n      value: 64.18393484706077\n    - type: dot_f1\n      value: 67.96347757046446\n    - type: dot_precision\n      value: 54.4529262086514\n    - type: dot_recall\n      value: 90.3907074973601\n    - type: euclidean_accuracy\n      value: 61.18029236599891\n    - type: euclidean_ap\n      value: 64.18395024821486\n    - type: euclidean_f1\n      value: 67.96347757046446\n    - type: euclidean_precision\n      value: 54.4529262086514\n    - type: euclidean_recall\n      value: 90.3907074973601\n    - type: manhattan_accuracy\n      value: 61.451001624255554\n    - type: manhattan_ap\n      value: 64.38232708763513\n    - type: manhattan_f1\n      value: 68.05860805860804\n    - type: manhattan_precision\n      value: 52.10319685922602\n    - type: manhattan_recall\n      value: 98.09926082365365\n    - type: max_accuracy\n      value: 61.451001624255554\n    - type: max_ap\n      value: 64.38232708763513\n    - type: max_f1\n      value: 68.05860805860804\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/OnlineShopping-classification\n      name: MTEB OnlineShopping\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 92.19000000000001\n    - type: ap\n      value: 89.73918431886767\n    - type: f1\n      value: 92.17175032574507\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/PAWSX\n      name: MTEB PAWSX\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 15.079320253752224\n    - type: cos_sim_spearman\n      value: 16.813772504404263\n    - type: euclidean_pearson\n      value: 19.476541162041762\n    - type: euclidean_spearman\n      value: 16.813772498098782\n    - type: manhattan_pearson\n      value: 19.497429832915277\n    - type: manhattan_spearman\n      value: 16.869600674180607\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/QBQTC\n      name: MTEB QBQTC\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 30.36139599797913\n    - type: cos_sim_spearman\n      value: 31.80296402851347\n    - type: euclidean_pearson\n      value: 30.10387888252793\n    - type: euclidean_spearman\n      value: 31.80297780103808\n    - type: manhattan_pearson\n      value: 30.86720382849436\n    - type: manhattan_spearman\n      value: 32.70491131366606\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.911\n    - type: map_at_10\n      value: 86.087\n    - type: map_at_100\n      value: 86.701\n    - type: map_at_1000\n      value: 86.715\n    - type: map_at_3\n      value: 83.231\n    - type: map_at_5\n      value: 85.051\n    - type: mrr_at_1\n      value: 82.75\n    - type: mrr_at_10\n      value: 88.759\n    - type: mrr_at_100\n      value: 88.844\n    - type: mrr_at_1000\n      value: 88.844\n    - type: mrr_at_3\n      value: 87.935\n    - type: mrr_at_5\n      value: 88.504\n    - type: ndcg_at_1\n      value: 82.75\n    - type: ndcg_at_10\n      value: 89.605\n    - type: ndcg_at_100\n      value: 90.664\n    - type: ndcg_at_1000\n      value: 90.733\n    - type: ndcg_at_3\n      value: 87.03\n    - type: ndcg_at_5\n      value: 88.473\n    - type: precision_at_1\n      value: 82.75\n    - type: precision_at_10\n      value: 13.575000000000001\n    - type: precision_at_100\n      value: 1.539\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 38.153\n    - type: precision_at_5\n      value: 25.008000000000003\n    - type: recall_at_1\n      value: 71.911\n    - type: recall_at_10\n      value: 96.261\n    - type: recall_at_100\n      value: 99.72800000000001\n    - type: recall_at_1000\n      value: 99.993\n    - type: recall_at_3\n      value: 88.762\n    - type: recall_at_5\n      value: 92.949\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 57.711581165572376\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 66.48938885750297\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 3.7379999999999995\n    - type: map_at_10\n      value: 9.261\n    - type: map_at_100\n      value: 11.001\n    - type: map_at_1000\n      value: 11.262\n    - type: map_at_3\n      value: 6.816\n    - type: map_at_5\n      value: 8.0\n    - type: mrr_at_1\n      value: 18.4\n    - type: mrr_at_10\n      value: 28.755999999999997\n    - type: mrr_at_100\n      value: 29.892000000000003\n    - type: mrr_at_1000\n      value: 29.961\n    - type: mrr_at_3\n      value: 25.467000000000002\n    - type: mrr_at_5\n      value: 27.332\n    - type: ndcg_at_1\n      value: 18.4\n    - type: ndcg_at_10\n      value: 16.296\n    - type: ndcg_at_100\n      value: 23.52\n    - type: ndcg_at_1000\n      value: 28.504\n    - type: ndcg_at_3\n      value: 15.485\n    - type: ndcg_at_5\n      value: 13.471\n    - type: precision_at_1\n      value: 18.4\n    - type: precision_at_10\n      value: 8.469999999999999\n    - type: precision_at_100\n      value: 1.8950000000000002\n    - type: precision_at_1000\n      value: 0.309\n    - type: precision_at_3\n      value: 14.6\n    - type: precision_at_5\n      value: 11.84\n    - type: recall_at_1\n      value: 3.7379999999999995\n    - type: recall_at_10\n      value: 17.185\n    - type: recall_at_100\n      value: 38.397\n    - type: recall_at_1000\n      value: 62.798\n    - type: recall_at_3\n      value: 8.896999999999998\n    - type: recall_at_5\n      value: 12.021999999999998\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.43977757480083\n    - type: cos_sim_spearman\n      value: 82.64182475199533\n    - type: euclidean_pearson\n      value: 83.71756009999591\n    - type: euclidean_spearman\n      value: 82.64182331395057\n    - type: manhattan_pearson\n      value: 83.8028936913025\n    - type: manhattan_spearman\n      value: 82.71024597804252\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.85653060698912\n    - type: cos_sim_spearman\n      value: 79.65598885228324\n    - type: euclidean_pearson\n      value: 83.1205137628455\n    - type: euclidean_spearman\n      value: 79.65629387709038\n    - type: manhattan_pearson\n      value: 83.71108853545837\n    - type: manhattan_spearman\n      value: 80.25617619716708\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.22921688565664\n    - type: cos_sim_spearman\n      value: 88.42662103041957\n    - type: euclidean_pearson\n      value: 87.91679798473325\n    - type: euclidean_spearman\n      value: 88.42662103041957\n    - type: manhattan_pearson\n      value: 88.16927537961303\n    - type: manhattan_spearman\n      value: 88.81581680062541\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.77261424554293\n    - type: cos_sim_spearman\n      value: 84.53930146434155\n    - type: euclidean_pearson\n      value: 85.67420491389697\n    - type: euclidean_spearman\n      value: 84.53929771783851\n    - type: manhattan_pearson\n      value: 85.74306784515618\n    - type: manhattan_spearman\n      value: 84.7399304675314\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 89.86138395166455\n    - type: cos_sim_spearman\n      value: 90.42577823022054\n    - type: euclidean_pearson\n      value: 89.8787763797515\n    - type: euclidean_spearman\n      value: 90.42577823022054\n    - type: manhattan_pearson\n      value: 89.9592937492158\n    - type: manhattan_spearman\n      value: 90.63535505335524\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.5176674585941\n    - type: cos_sim_spearman\n      value: 87.6842917085397\n    - type: euclidean_pearson\n      value: 86.70213081520711\n    - type: euclidean_spearman\n      value: 87.6842917085397\n    - type: manhattan_pearson\n      value: 86.83702628983627\n    - type: manhattan_spearman\n      value: 87.87791000374443\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ko-ko)\n      config: ko-ko\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.86395454805867\n    - type: cos_sim_spearman\n      value: 83.69454595252267\n    - type: euclidean_pearson\n      value: 83.04743892608313\n    - type: euclidean_spearman\n      value: 83.69454026433006\n    - type: manhattan_pearson\n      value: 83.4032095553322\n    - type: manhattan_spearman\n      value: 84.11527379013802\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ar-ar)\n      config: ar-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 81.80249894729546\n    - type: cos_sim_spearman\n      value: 81.87004960533409\n    - type: euclidean_pearson\n      value: 80.0392760044179\n    - type: euclidean_spearman\n      value: 81.87004960533409\n    - type: manhattan_pearson\n      value: 80.38096542355912\n    - type: manhattan_spearman\n      value: 82.40774679630341\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-ar)\n      config: en-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 77.6158201787172\n    - type: cos_sim_spearman\n      value: 77.934651044009\n    - type: euclidean_pearson\n      value: 77.7874683895269\n    - type: euclidean_spearman\n      value: 77.934651044009\n    - type: manhattan_pearson\n      value: 78.36151849193052\n    - type: manhattan_spearman\n      value: 78.52439586349938\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-de)\n      config: en-de\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.04363311392207\n    - type: cos_sim_spearman\n      value: 87.30483659369973\n    - type: euclidean_pearson\n      value: 87.62634489502616\n    - type: euclidean_spearman\n      value: 87.30483659369973\n    - type: manhattan_pearson\n      value: 88.02340837141445\n    - type: manhattan_spearman\n      value: 87.55012003294\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 91.69172851958248\n    - type: cos_sim_spearman\n      value: 91.7546879482416\n    - type: euclidean_pearson\n      value: 91.84843039183963\n    - type: euclidean_spearman\n      value: 91.7546879482416\n    - type: manhattan_pearson\n      value: 91.72325753804357\n    - type: manhattan_spearman\n      value: 91.55330259513397\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-tr)\n      config: en-tr\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 73.95572901084864\n    - type: cos_sim_spearman\n      value: 72.56217821552626\n    - type: euclidean_pearson\n      value: 74.24242980323574\n    - type: euclidean_spearman\n      value: 72.56217821552626\n    - type: manhattan_pearson\n      value: 74.57473362519922\n    - type: manhattan_spearman\n      value: 72.76048826648497\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-en)\n      config: es-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.93329396008296\n    - type: cos_sim_spearman\n      value: 88.2406635486219\n    - type: euclidean_pearson\n      value: 87.49687343908533\n    - type: euclidean_spearman\n      value: 88.2406635486219\n    - type: manhattan_pearson\n      value: 88.14088309231084\n    - type: manhattan_spearman\n      value: 88.93314020908534\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-es)\n      config: es-es\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.70124451546057\n    - type: cos_sim_spearman\n      value: 87.45988160052252\n    - type: euclidean_pearson\n      value: 88.44395505247728\n    - type: euclidean_spearman\n      value: 87.45988160052252\n    - type: manhattan_pearson\n      value: 88.69269783495425\n    - type: manhattan_spearman\n      value: 87.65383425621\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (fr-en)\n      config: fr-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.64109149761346\n    - type: cos_sim_spearman\n      value: 88.06459637689733\n    - type: euclidean_pearson\n      value: 88.02313315797703\n    - type: euclidean_spearman\n      value: 88.06459637689733\n    - type: manhattan_pearson\n      value: 88.28328539133253\n    - type: manhattan_spearman\n      value: 88.06605708379142\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (it-en)\n      config: it-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.9040028177525\n    - type: cos_sim_spearman\n      value: 89.68152202933464\n    - type: euclidean_pearson\n      value: 89.23684469601253\n    - type: euclidean_spearman\n      value: 89.68152202933464\n    - type: manhattan_pearson\n      value: 89.59504307277454\n    - type: manhattan_spearman\n      value: 89.88060100313582\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (nl-en)\n      config: nl-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.69891585325125\n    - type: cos_sim_spearman\n      value: 88.25252785071736\n    - type: euclidean_pearson\n      value: 87.99932873748662\n    - type: euclidean_spearman\n      value: 88.25252785071736\n    - type: manhattan_pearson\n      value: 88.26959683009446\n    - type: manhattan_spearman\n      value: 88.32583227300715\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 67.53235909794135\n    - type: cos_sim_spearman\n      value: 66.97521740529574\n    - type: euclidean_pearson\n      value: 68.19502223613912\n    - type: euclidean_spearman\n      value: 66.97521740529574\n    - type: manhattan_pearson\n      value: 68.39070714774539\n    - type: manhattan_spearman\n      value: 67.1072812364868\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de)\n      config: de\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 43.715742021204775\n    - type: cos_sim_spearman\n      value: 49.12255971271453\n    - type: euclidean_pearson\n      value: 40.76848562610837\n    - type: euclidean_spearman\n      value: 49.12255971271453\n    - type: manhattan_pearson\n      value: 40.92204625614112\n    - type: manhattan_spearman\n      value: 49.23333793661129\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es)\n      config: es\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.35268345563588\n    - type: cos_sim_spearman\n      value: 66.99661626042061\n    - type: euclidean_pearson\n      value: 65.85589122857066\n    - type: euclidean_spearman\n      value: 66.99661626042061\n    - type: manhattan_pearson\n      value: 66.78454301512294\n    - type: manhattan_spearman\n      value: 67.17570330149233\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl)\n      config: pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 33.36599908204445\n    - type: cos_sim_spearman\n      value: 39.20768331939503\n    - type: euclidean_pearson\n      value: 22.16066769530468\n    - type: euclidean_spearman\n      value: 39.20768331939503\n    - type: manhattan_pearson\n      value: 22.386053195546022\n    - type: manhattan_spearman\n      value: 39.70172817465986\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (tr)\n      config: tr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.06813956986753\n    - type: cos_sim_spearman\n      value: 68.72065117995668\n    - type: euclidean_pearson\n      value: 66.97373456344194\n    - type: euclidean_spearman\n      value: 68.72065117995668\n    - type: manhattan_pearson\n      value: 67.34907265771595\n    - type: manhattan_spearman\n      value: 68.73705769957843\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ar)\n      config: ar\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 47.17664865207108\n    - type: cos_sim_spearman\n      value: 54.115568323148864\n    - type: euclidean_pearson\n      value: 48.56418162879182\n    - type: euclidean_spearman\n      value: 54.115568323148864\n    - type: manhattan_pearson\n      value: 48.85951643453165\n    - type: manhattan_spearman\n      value: 54.13599784169052\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ru)\n      config: ru\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.87514136275987\n    - type: cos_sim_spearman\n      value: 60.82923573674973\n    -', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":7110660096,"storage_bytes":28779017177,"files_count":20,"spaces_count":46,"gated":false,"private":false,"config":{"architectures":["MistralModel"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2401.00368","source_url":"https://arxiv.org/abs/2401.00368"},{"type":"based_on_paper","target_id":"arxiv:2104.08663","source_url":"https://arxiv.org/abs/2104.08663"},{"type":"based_on_paper","target_id":"arxiv:2210.07316","source_url":"https://arxiv.org/abs/2210.07316"},{"type":"based_on_paper","target_id":"arxiv:2212.03533","source_url":"https://arxiv.org/abs/2212.03533"}]', NULL, 'MIT', 'approved', 77.4, 'f78f02f73a29bf1a16d5b420ff3fc736', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NovaSky-AI-Sky-T1-32B-Preview', 'huggingface--novasky-ai--sky-t1-32b-preview', 'Sky-T1-32B-Preview', 'NovaSky-AI', '--- library_name: transformers datasets: - codeparrot/apps - BAAI/TACO - AI-MO/NuminaMath-CoT language: - en base_model: - Qwen/Qwen2.5-32B-Instruct license: apache-2.0 --- <!-- Provide a longer summary of what this model is. --> This is a 32B reasoning model trained from Qwen2.5-32B-Instruct with 17K data. The performance is on par with o1-preview model on both math and coding. Please see our blog post for more details. - **Developed by:** NovaSky Team from Sky Computing Lab at UC Berkeley. ...', '["transformers","safetensors","qwen2","text-generation","conversational","en","dataset:codeparrot/apps","dataset:baai/taco","dataset:ai-mo/numinamath-cot","arxiv:2412.09413","base_model:qwen/qwen2.5-32b-instruct","base_model:finetune:qwen/qwen2.5-32b-instruct","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 550, 367, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlibrary_name: transformers\ndatasets:\n- codeparrot/apps\n- BAAI/TACO\n- AI-MO/NuminaMath-CoT\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-32B-Instruct\nlicense: apache-2.0\n---\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is a 32B reasoning model trained from Qwen2.5-32B-Instruct with 17K data. The performance is on par with o1-preview model on both math and coding.\nPlease see our [blog post](https://novasky-ai.github.io/posts/sky-t1/) for more details.\n\n- **Developed by:** NovaSky Team from Sky Computing Lab at UC Berkeley.\n\n## Training Details\n\n### Training Data\n\n17K verified correct responses from Qwen/QwQ-32B-Preview on coding, math. In addition, we add the science portion from the [Still-2 paper](https://arxiv.org/pdf/2412.09413).\n\n### Training Procedure\nWe perform supervised fine tuning on the data, with a batch size of 96.\n\n#### Speeds\n\nWe use Llama-Factory for training. On 8 H100, the training takes 19 hours with DeepSpeed Zero-3 Offload.\n\n\n## Evaluation\n|               | Sky-T1-32B-Preview | Qwen-2.5-32B-Instruct | QwQ   | o1-preview |\n|-----------------------|---------------------|--------|-------|------------|\n| Math500              | 82.4                    | 76.2    | 85.4 | 81.4       |\n| AIME2024             | 43.3                    | 16.7    | 50.0  | 40.0       |\n| LiveCodeBench-Easy   | 86.3                    | 84.6   | 90.7  | 92.9       |\n| LiveCodeBench-Medium | 56.8                    | 40.8   | 56.3  | 54.9       |\n| LiveCodeBench-Hard   | 17.9                    | 9.8   | 17.1  | 16.3       |\n| GPQA-Diamond         | 56.8                    | 45.5   | 52.5  | 75.2       |\n\n## Acknowledgement\nWe would like to thanks the compute resources from [Lambda Lab](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5) and [AnyScale](https://www.anyscale.com/). We would like to thanks the academic feedback and support from the [Still-2 Team](https://arxiv.org/pdf/2412.09413), and [Junyang Lin](https://justinlin610.github.io/) from the [Qwen Team](https://qwenlm.github.io/). \n\n## Citation \nPlease considering citing our blog post if you found it useful for your research. Thank you!\n\n```bibtex\n@misc{sky_t1_2025,\n  author       = {NovaSky Team},\n  title        = {Sky-T1: Fully open-source reasoning model with o1-preview performance in $450 budget},\n  howpublished = {https://novasky-ai.github.io/posts/sky-t1},\n  note         = {Accessed: 2025-01-09},\n  year         = {2025}\n}', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":32763876352,"storage_bytes":65527840896,"files_count":24,"spaces_count":32,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2412.09413","source_url":"https://arxiv.org/abs/2412.09413"}]', NULL, 'Apache-2.0', 'approved', 62.4, '8571a3ff9e87b28379e1b19f766d8046', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Wan-AI-Wan2.1-I2V-14B-720P', 'huggingface--wan-ai--wan2.1-i2v-14b-720p', 'Wan2.1-I2V-14B-720P', 'Wan-AI', '--- license: apache-2.0 language: - en - zh pipeline_tag: image-to-video library_name: diffusers tags: - video - video genration --- <p align="center"> <img src="assets/logo.png" width="400"/> <p> <p align="center"> üíú <a href=""><b>Wan</b></a> &nbsp&nbsp ÔΩú &nbsp&nbsp üñ•Ô∏è <a href="https://github.com/Wan-Video/Wan2.1">GitHub</a> &nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/Wan-AI">ModelS...', '["diffusers","safetensors","i2v","video","video genration","image-to-video","en","zh","license:apache-2.0","region:us"]', 'image-to-video', 549, 8530, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: image-to-video\nlibrary_name: diffusers\ntags:\n- video\n- video genration\n---\n# Wan2.1\n\n<p align="center">\n    <img src="assets/logo.png" width="400"/>\n<p>\n\n<p align="center">\n    üíú <a href=""><b>Wan</b></a> &nbsp&nbsp ÔΩú &nbsp&nbsp üñ•Ô∏è <a href="https://github.com/Wan-Video/Wan2.1">GitHub</a> &nbsp&nbsp  | &nbsp&nbspü§ó <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="">Paper (Coming soon)</a> &nbsp&nbsp | &nbsp&nbsp üìë <a href="https://wanxai.com">Blog</a> &nbsp&nbsp | &nbsp&nbspüí¨ <a href="https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg">WeChat Group</a>&nbsp&nbsp | &nbsp&nbsp üìñ <a href="https://discord.gg/p5XbdQV7">Discord</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**]() <be>\n\nIn this repository, we present **Wan2.1**, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. **Wan2.1** offers these key features:\n- üëç **SOTA Performance**: **Wan2.1** consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.\n- üëç **Supports Consumer-grade GPUs**: The T2V-1.3B model requires only 8.19 GB VRAM, making it compatible with almost all consumer-grade GPUs. It can generate a 5-second 480P video on an RTX 4090 in about 4 minutes (without optimization techniques like quantization). Its performance is even comparable to some closed-source models.\n- üëç **Multiple Tasks**: **Wan2.1** excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.\n- üëç **Visual Text Generation**: **Wan2.1** is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.\n- üëç **Powerful Video VAE**: **Wan-VAE** delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.\n\n\nThis repository contains our I2V-14B model, which is capable of generating 720P high-definition videos. After thousands of rounds of human evaluations, this model has outperformed both closed-source and open-source alternatives, achieving state-of-the-art performance.\n\n\n## Video Demos\n\n<div align="center">\n    <video width="80%" controls>\n        <source src="https://cloud.video.taobao.com/vod/Jth64Y7wNoPcJki_Bo1ZJTDBvNjsgjlVKsNs05Fqfps.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## üî• Latest News!!\n\n* Feb 25, 2025: üëã We''ve released the inference code and weights of Wan2.1.\n\n\n## üìë Todo List\n- Wan2.1 Text-to-Video\n    - [x] Multi-GPU Inference code of the 14B and 1.3B models\n    - [x] Checkpoints of the 14B and 1.3B models\n    - [x] Gradio demo\n    - [ ] Diffusers integration\n    - [ ] ComfyUI integration\n- Wan2.1 Image-to-Video\n    - [x] Multi-GPU Inference code of the 14B model\n    - [x] Checkpoints of the 14B model\n    - [x] Gradio demo\n    - [ ] Diffusers integration\n    - [ ] ComfyUI integration\n\n\n## Quickstart\n\n#### Installation\nClone the repo:\n```\ngit clone https://github.com/Wan-Video/Wan2.1.git\ncd Wan2.1\n```\n\nInstall dependencies:\n```\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\n\n\n#### Model Download\n\n| Models        |                       Download Link                                           |    Notes                      |\n| --------------|-------------------------------------------------------------------------------|-------------------------------|\n| T2V-14B       |      ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B)      ü§ñ [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B)          | Supports both 480P and 720P\n| I2V-14B-720P  |      ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P)    ü§ñ [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P)     | Supports 720P\n| I2V-14B-480P  |      ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P)    ü§ñ [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P)      | Supports 480P\n| T2V-1.3B      |      ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B)     ü§ñ [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B)         | Supports 480P\n\n> üí°Note: The 1.3B model is capable of generating videos at 720P resolution. However, due to limited training at this resolution, the results are generally less stable compared to 480P. For optimal performance, we recommend using 480P resolution.\n\n\nDownload models using ü§ó huggingface-cli:\n```\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P --local-dir ./Wan2.1-I2V-14B-720P\n```\n\nDownload models using ü§ñ modelscope-cli:\n```\npip install modelscope\nmodelscope download Wan-AI/Wan2.1-I2V-14B-720P --local_dir ./Wan2.1-I2V-14B-720P\n```\n\n#### Run Image-to-Video Generation\n\nSimilar to Text-to-Video, Image-to-Video is also divided into processes with and without the prompt extension step. The specific parameters and their corresponding settings are as follows:\n<table>\n    <thead>\n        <tr>\n            <th rowspan="2">Task</th>\n            <th colspan="2">Resolution</th>\n            <th rowspan="2">Model</th>\n        </tr>\n        <tr>\n            <th>480P</th>\n            <th>720P</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>i2v-14B</td>\n            <td style="color: green;">‚ùå</td>\n            <td style="color: green;">‚úîÔ∏è</td>\n            <td>Wan2.1-I2V-14B-720P</td>\n        </tr>\n        <tr>\n            <td>i2v-14B</td>\n            <td style="color: green;">‚úîÔ∏è</td>\n            <td style="color: red;">‚ùå</td>\n            <td>Wan2.1-T2V-14B-480P</td>\n        </tr>\n    </tbody>\n</table>\n\n\n##### (1) Without Prompt Extention\n\n- Single-GPU inference\n```\npython generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> üí°For the Image-to-Video task, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n- Multi-GPU inference using FSDP + xDiT USP\n\n```\npip install "xfuser>=0.4.1"\ntorchrun --nproc_per_node=8 generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n##### (2) Using Prompt Extention\n\n\nRun with local prompt extention using `Qwen/Qwen2.5-VL-7B-Instruct`:\n```\npython generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --use_prompt_extend --prompt_extend_model Qwen/Qwen2.5-VL-7B-Instruct --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\nRun with remote prompt extention using `dashscope`:\n```\nDASH_API_KEY=your_key python generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --use_prompt_extend --prompt_extend_method ''dashscope'' --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n##### (3) Runing local gradio\n\n```\ncd gradio\n# if one only uses 480P model in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir_480p ./Wan2.1-I2V-14B-480P\n\n# if one only uses 720P model in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir_720p ./Wan2.1-I2V-14B-720P\n\n# if one uses both 480P and 720P models in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir_480p ./Wan2.1-I2V-14B-480P --ckpt_dir_720p ./Wan2.1-I2V-14B-720P\n```\n\n\n## Manual Evaluation\n\nWe conducted extensive manual evaluations to evaluate the performance of the Image-to-Video model, and the results are presented in the table below. The results clearly indicate that **Wan2.1** outperforms both closed-source and open-source models.\n\n<div align="center">\n    <img src="assets/i2v_res.png" alt="" style="width: 80%;" />\n</div>\n\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.1** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align="center">\n    <img src="assets/comp_effic.png" alt="" style="width: 80%;" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) For the 1.3B model on 8 GPUs, set `--ring_size 8` and `--ulysses_size 1`;\n> (2) For the 14B model on 1 GPU, use `--offload_model True`;\n> (3) For the 1.3B model on a single 4090 GPU, set `--offload_model True --t5_cpu`;\n> (4) For all testings, no prompt extension was applied, meaning `--use_prompt_extend` was not enabled.\n\n-------\n\n## Introduction of Wan2.1\n\n**Wan2.1**  is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. These include our novel spatio-temporal variational autoencoder (VAE), scalable training strategies, large-scale data construction, and automated evaluation metrics. Collectively, these contributions enhance the model‚Äôs performance and versatility.\n\n\n##### (1) 3D Variational Autoencoders\nWe propose a novel 3D causal VAE architecture, termed **Wan-VAE** specifically designed for video generation. By combining multiple strategies, we improve spatio-temporal compression, reduce memory usage, and ensure temporal causality. **Wan-VAE** demonstrates significant advantages in performance efficiency compared to other open-source VAEs. Furthermore, our **Wan-VAE** can encode and decode unlimited-length 1080P videos without losing historical temporal information, making it particularly well-suited for video generation tasks.\n\n\n<div align="center">\n    <img src="assets/video_vae_res.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n##### (2) Video Diffusion DiT\n\n**Wan2.1** is designed using the Flow Matching framework within the paradigm of mainstream Diffusion Transformers. Our model''s architecture uses the T5 Encoder to encode multilingual text input, with cross-attention in each transformer block embedding the text into the model structure. Additionally, we employ an MLP with a Linear layer and a SiLU layer to process the input time embeddings and predict six modulation parameters individually. This MLP is shared across all transformer blocks, with each block learning a distinct set of biases. Our experimental findings reveal a significant performance improvement with this approach at the same parameter scale.\n\n<div align="center">\n    <img src="assets/video_dit_arch.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n| Model  | Dimension | Input Dimension | Output Dimension | Feedforward Dimension | Frequency Dimension | Number of Heads | Number of Layers |\n|--------|-----------|-----------------|------------------|-----------------------|---------------------|-----------------|------------------|\n| 1.3B   | 1536      | 16              | 16               | 8960                  | 256                 | 12              | 30               |\n| 14B   | 5120       | 16              | 16               | 13824                 | 256                 | 40              | 40               |\n\n\n\n##### Data\n\nWe curated and deduplicated a candidate dataset comprising a vast amount of image and video data. During the data curation process, we designed a four-step data cleaning process, focusing on fundamental dimensions, visual quality and motion quality. Through the robust data processing pipeline, we can easily obtain high-quality, diverse, and large-scale training sets of images and videos.\n\n![figure1](assets/data_for_diff_stage.jpg "figure1")\n\n\n##### Comparisons to SOTA\nWe compared **Wan2.1** with leading open-source and closed-source models to evaluate the performace. Using our carefully designed set of 1,035 internal prompts, we tested across 14 major dimensions and 26 sub-dimensions. We then compute the total score by performing a weighted calculation on the scores of each dimension, utilizing weights derived from human preferences in the matching process. The detailed results are shown in the table below. These results demonstrate our model''s superior performance compared to both open-source and closed-source models.\n\n![figure1](assets/vben_vs_sota.png "figure1")\n\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2.1,\n    title   = {Wan: Open and Advanced Large-Scale Video Generative Models},\n    author  = {Wan Team},\n    journal = {},\n    year    = {2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/p5XbdQV7) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":172376999091,"files_count":32,"spaces_count":25,"gated":false,"private":false,"config":{"model_type":"i2v"}}', '[]', '[{"type":"has_code","target_id":"github:Wan-Video:Wan2.1\">GitHub<","source_url":"https://github.com/Wan-Video/Wan2.1\">GitHub<"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1.git","source_url":"https://github.com/Wan-Video/Wan2.1.git"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"}]', NULL, 'Apache-2.0', 'approved', 97.4, '2f10d423d946877326a61ff8b28f9527', NULL, 'https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/resolve/main/assets/comp_effic.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Wan-AI-Wan2.1-I2V-14B-720P from https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/resolve/main/assets/comp_effic.png
Image converted to WebP: data/images/huggingface-Wan-AI-Wan2.1-I2V-14B-720P.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Intel-neural-chat-7b-v3-1', 'huggingface--intel--neural-chat-7b-v3-1', 'neural-chat-7b-v3-1', 'Intel', '--- license: apache-2.0 tags: - LLMs - mistral - Intel pipeline_tag: text-generation base_model: mistralai/Mistral-7B-v0.1 model-index: - name: neural-chat-7b-v3-1 results: - task: type: Large Language Model name: Large Language Model dataset: type: Open-Orca/SlimOrca name: Open-Orca/SlimOrca metrics: - type: ARC (25-shot) value: 66.21 name: ARC (25-shot) verified: true - type: HellaSwag (10-shot) value: 83.64 name: HellaSwag (10-shot) verified: true - type: MMLU (5-shot) value: 62.37 name: M...', '["transformers","pytorch","safetensors","mistral","text-generation","llms","intel","conversational","en","dataset:open-orca/slimorca","arxiv:2306.02707","base_model:mistralai/mistral-7b-v0.1","base_model:finetune:mistralai/mistral-7b-v0.1","license:apache-2.0","model-index","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 547, 1333, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Intel/neural-chat-7b-v3-1","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- LLMs\n- mistral\n- Intel\npipeline_tag: text-generation\nbase_model: mistralai/Mistral-7B-v0.1\nmodel-index:\n- name: neural-chat-7b-v3-1\n  results:\n  - task:\n      type: Large Language Model\n      name: Large Language Model\n    dataset:\n      type: Open-Orca/SlimOrca\n      name: Open-Orca/SlimOrca\n    metrics:\n    - type: ARC (25-shot)\n      value: 66.21\n      name: ARC (25-shot)\n      verified: true\n    - type: HellaSwag (10-shot)\n      value: 83.64\n      name: HellaSwag (10-shot)\n      verified: true\n    - type: MMLU (5-shot)\n      value: 62.37\n      name: MMLU (5-shot)\n      verified: true\n    - type: TruthfulQA (0-shot)\n      value: 59.65\n      name: TruthfulQA (0-shot)\n      verified: true\n    - type: Winogrande (5-shot)\n      value: 78.14\n      name: Winogrande (5-shot)\n      verified: true\n    - type: GSM8K (5-shot)\n      value: 19.56\n      name: GSM8K (5-shot)\n      verified: true\n    - type: DROP (3-shot)\n      value: 43.84\n      name: DROP (3-shot)\n      verified: true\ndatasets:\n- Open-Orca/SlimOrca\nlanguage:\n- en\n---\n\n## Model Details: Neural-Chat-v3-1\n\nThis model is a fine-tuned 7B parameter LLM on the Intel Gaudi 2 processor from the [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) on the open source dataset [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca). The model was aligned using the Direct Performance Optimization (DPO) method with [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs). For more information, refer to the Medium article [The Practice of Supervised Fine-tuning and Direct Preference Optimization on Intel Gaudi2](https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3).\n\n<p align="center">\n  <img src="https://cdn-uploads.huggingface.co/production/uploads/6297f0e30bd2f58c647abb1d/ctASHUT5QYIxMsOFa-sHC.webp" width="500"/>\n  Photo by Google DeepMind on Unsplash\n</p>\n\n| Model Detail | Description |\n| ----------- | ----------- | \n| Model Authors - Company | Intel. The NeuralChat team with members from DCAI/AISE/AIPT. Core team members: Kaokao Lv, Liang Lv, Chang Wang, Wenxin Zhang, Xuhui Ren, and Haihao Shen.| \n| Date | October, 2023 | \n| Version | v3-1 | \n| Type | 7B Large Language Model | \n| Paper or Other Resources | [Medium Blog](https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3) | \n| License | Apache 2.0 |\n| Questions or Comments | [Community Tab](https://huggingface.co/Intel/neural-chat-7b-v3-1/discussions) and [Intel DevHub Discord](https://discord.gg/rv2Gp55UJQ)|\n\n| Intended Use | Description |\n| ----------- | ----------- | \n| Primary intended uses | You can use the fine-tuned model for several language-related tasks. Checkout the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) to see how this model is doing. | \n| Primary intended users | Anyone doing inference on language-related tasks. | \n| Out-of-scope uses | This model in most cases will need to be fine-tuned for your particular task.  The model should not be used to intentionally create hostile or alienating environments for people.|\n\n## How To Use\n\nContext length for this model: 8192 tokens (same as https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-04\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- distributed_type: multi-HPU\n- num_devices: 8\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- total_eval_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.03\n- num_epochs: 2.0\n\n### Reproduce the model\nHere is the sample code to reproduce the model: [GitHub sample code](https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3). Here is the documentation to reproduce building the model:\n\n```bash\ngit clone https://github.com/intel/intel-extension-for-transformers.git\ncd intel-extension-for-transformers\n\ndocker build --no-cache ./ --target hpu --build-arg REPO=https://github.com/intel/intel-extension-for-transformers.git --build-arg ITREX_VER=main -f ./intel_extension_for_transformers/neural_chat/docker/Dockerfile -t chatbot_finetuning:latest\n\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host chatbot_finetuning:latest\n\n# after entering docker container\ncd examples/finetuning/finetune_neuralchat_v3\n\n```\nWe select the latest pretrained mistralai/Mistral-7B-v0.1 and the open source dataset Open-Orca/SlimOrca to conduct the experiment.\n\nThe below script use deepspeed zero2 to lanuch the training with 8 cards Gaudi2. In the `finetune_neuralchat_v3.py`, the default `use_habana=True, use_lazy_mode=True, device="hpu"` for Gaudi2. And if you want to run it on NVIDIA GPU, you can set them `use_habana=False, use_lazy_mode=False, device="auto"`.\n\n```python\ndeepspeed --include localhost:0,1,2,3,4,5,6,7 \\n    --master_port 29501 \\n    finetune_neuralchat_v3.py\n```\n\nMerge the LoRA weights:\n\n```python\npython apply_lora.py \\n    --base-model-path mistralai/Mistral-7B-v0.1 \\n    --lora-model-path finetuned_model/ \\n    --output-path finetuned_model_lora\n```\n\n\n### FP32 Inference with Transformers\n\n```python\nimport transformers\n\nmodel_name = ''Intel/neural-chat-7b-v3-1''\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ndef generate_response(system_input, user_input):\n\n    # Format the input using the provided template\n    prompt = f"### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n"\n\n    # Tokenize and encode the prompt\n    inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=False)\n\n    # Generate a response\n    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Extract only the assistant''s response\n    return response.split("### Assistant:\n")[-1]\n\n\n# Example usage\nsystem_input = "You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer."\nuser_input = "calculate 100 + 520 + 60"\nresponse = generate_response(system_input, user_input)\nprint(response)\n\n# expected response\n"""\nTo calculate the sum of 100, 520, and 60, we will follow these steps:\n\n1. Add the first two numbers: 100 + 520\n2. Add the result from step 1 to the third number: (100 + 520) + 60\n\nStep 1: Add 100 and 520\n100 + 520 = 620\n\nStep 2: Add the result from step 1 to the third number (60)\n(620) + 60 = 680\n\nSo, the sum of 100, 520, and 60 is 680.\n"""\n```\n\n### BF16 Inference with Intel Extension for Transformers and Intel Extension for Pytorch\n```python\nfrom transformers import AutoTokenizer, TextStreamer\nimport torch\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM\nimport intel_extension_for_pytorch as ipex\n\nmodel_name = "Intel/neural-chat-7b-v3-1"\nprompt = "Once upon a time, there existed a little girl,"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors="pt").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nmodel = ipex.optimize(model.eval(), dtype=torch.bfloat16, inplace=True, level="O1", auto_kernel_selection=True)\n\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n```\n\n\n### INT4 Inference with Transformers and Intel Extension for Transformers\n```python\nfrom transformers import AutoTokenizer, TextStreamer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\nmodel_name = "Intel/neural-chat-7b-v3-1"\n\n# for int8, should set weight_dtype="int8"       \nconfig = WeightOnlyQuantConfig(compute_dtype="bf16", weight_dtype="int4")\nprompt = "Once upon a time, there existed a little girl,"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors="pt").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n\n```\n\n| Factors | Description | \n| ----------- | ----------- | \n| Groups | More details about the dataset and annotations can be found at [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) and the associated paper at https://arxiv.org/abs/2306.02707. | \n| Instrumentation | The performance of the model can vary depending on the inputs to the model. In this case, the prompts provided can drastically change the prediction of the language model. |\n| Environment | The model was trained on the Intel Gaudi 2 processor (8 cards).  |\n| Card Prompts | Model deployment on alternate hardware and software will change model performance. The model evaluation factors are from the Hugging Face LLM leaderboard: ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K, and DROP (see Quantitative Analyses below). |\n\n| Metrics | Description | \n| ----------- | ----------- | \n| Model performance measures | The model performance was evaluated against other LLMs according to the measures on the LLM leaderboard. These were selected as this has become the standard for LLM performance. |\n| Decision thresholds | No decision thresholds were used. | \n| Approaches to uncertainty and variability | - | \n\n| Training and Evaluation Data | Description | \n| ----------- | ----------- | \n| Datasets | The training data are from [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca). There is no contamination from the GSM8k test set, as this is not a part of the Open-Orca/SlimOrca dataset.|\n| Motivation | - |\n| Preprocessing | - | \n\n## Quantitative Analyses \nThe model was submitted to the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). The detailed submission can be found here: [https://huggingface.co/datasets/open-llm-leaderboard/details_Intel__neural-chat-7b-v3-1](https://huggingface.co/datasets/open-llm-leaderboard/details_Intel__neural-chat-7b-v3-1). The metrics can be found below and show that the model has significantly improved performance from Mistral-7B-v0.1 and neural-chat-7b-v3.\n\n| Model | Average ‚¨ÜÔ∏è| ARC (25-s) ‚¨ÜÔ∏è | HellaSwag (10-s) ‚¨ÜÔ∏è | MMLU (5-s) ‚¨ÜÔ∏è| TruthfulQA (MC) (0-s) ‚¨ÜÔ∏è | Winogrande (5-s) | GSM8K (5-s) | DROP (3-s) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|[mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 50.32 | 59.58  | 83.31  | 64.16  | 42.15 | 78.37 | 18.12 | 6.14 |\n| [Intel/neural-chat-7b-v3](https://huggingface.co/Intel/neural-chat-7b-v3) | **57.31** | 67.15 | 83.29 | 62.26  | 58.77 | 78.06 | 1.21 | 50.43 |\n| [Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1) | **59.06** | 66.21 | 83.64 | 62.37  | 59.65 | 78.14 | 19.56 | 43.84 |\n\n## Testing Model Quantizability\nThe following code block can be run to determine, for PyTorch models, if that model is amenable to quantization.  \nOne caveat - the Intel Extension for PyTorch uses optimum ipex, which is pre-release and needs further testing.\n\nTo install the dependencies, you should first install Intel Extensions for PyTorch and tehn pip install each of the following dependencies:\n- torch\n- optimum.intel\n- optimum[ipex]\n- transformers\n\n### Intel Extension for PyTorch method:\nIn this case, we are testing if neural-chat-7b-v3-1 can be quantized and this testing method demonstrates the model size change, for example:\nwhen the base type is specified to be torch.bfloat16 but also specifying that load_in_4bit=True which causes the weights only to be quantized we see an output from the model testing as follows:\n- **model_quantize_internal: model size  = 27625.02 MB**\n- **model_quantize_internal: quant size  =  4330.80 MB**\n\nThis code should run from within a python script - such as ipex_test.py as follows:\n```python\nimport torch\nimport os\nfrom transformers import AutoTokenizer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, pipeline\nmodel_name = "Intel/neural-chat-7b-v3-1"     \nprompt = "Once upon a time, there existed a little girl,"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors="pt").input_ids\n\nresult = {torch.bfloat16:"failed"}\ntyp = torch.bfloat16\ntry:\n    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True,  torch_dtype = typ)\n    outputs = model.generate(inputs, max_new_tokens=20)\n    result[typ] = f"passed, {os.stat(model.bin_file).st_size}"\nexcept:\n    result[typ] = "failed"\n\n    \nprint("\n\nResults of quantizing: ")  \n# determine if Quantized\nwith open(r"output.log", ''r'') as fp:\n    for l_no, line in enumerate(fp):\n        # search string\n        if ''model_quantize_internal'' in line:\n            print(line)\n            \nprint("\n\nExecution results ")\nfor k,v in result.items():\n    print(k,v)\n    \nprint("\n\nModel Output: ")\ntokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n```\nRun the code as folows from a bash terminal:\n```bash\npython ipex_test.py 2>&1 | tee output.log\n```\nThe entire output is captured in the output.log but it will be summarized, \nalong with output from the model indicating either pass or fail of the quantization as well as model output for a given prompt.\n\n\n## Ethical Considerations and Limitations\nNeural-chat-7b-v3-1 can produce factually incorrect output, and should not be relied on to produce factually accurate information. Because of the limitations of the pretrained model and the finetuning datasets, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\nTherefore, before deploying any applications of neural-chat-7b-v3-1, developers should perform safety testing.\n\n## Caveats and Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\nHere are a couple of useful links to learn more about Intel''s AI software:\n* Intel Neural Compressor [link](https://github.com/intel/neural-compressor)\n* Intel Extension for Transformers [link](https://github.com/intel/intel-extension-for-transformers)\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241732096,"storage_bytes":28967554886,"files_count":15,"spaces_count":65,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true,"chat_template":"{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content''] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- for message in loop_messages %}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception(''After the optional system message, conversation roles must alternate user/assistant/user/assistant/...'') }}\n    {%- endif %}\n    {%- if loop.first and system_message is defined %}\n            {{- ''### System:\n'' + system_message + ''\n'' }}\n           {%- endif %}\n     {%- if message[''role''] == ''user'' %}\n  {{- ''### User:\n'' + message[''content''] + '' \n'' }}\n       {%- elif message[''role''] == ''assistant'' %}\n        {{- ''### Assistant:\n'' + message[''content''] + eos_token + ''\n''}}\n    {%- else %}\n        {{- raise_exception(''Only user and assistant roles are supported, with the exception of an initial optional system message!'') }}\n    {%- endif %}\n{%- endfor %}{% if add_generation_prompt %}{{ ''### Assistant:\n'' }}{% endif %}\n"}}}', '[]', '[{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers","source_url":"https://github.com/intel/intel-extension-for-transformers"},{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers.git","source_url":"https://github.com/intel/intel-extension-for-transformers.git"},{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers.git","source_url":"https://github.com/intel/intel-extension-for-transformers.git"},{"type":"has_code","target_id":"github:intel:neural-compressor","source_url":"https://github.com/intel/neural-compressor"},{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers","source_url":"https://github.com/intel/intel-extension-for-transformers"},{"type":"based_on_paper","target_id":"arxiv:2306.02707","source_url":"https://arxiv.org/abs/2306.02707"}]', NULL, 'Apache-2.0', 'approved', 77.4, '98016336d15481f4918ad2f4bdb88e68', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3-mini-4k-instruct-gguf', 'huggingface--microsoft--phi-3-mini-4k-instruct-gguf', 'Phi-3-mini-4k-instruct-gguf', 'microsoft', '--- license: mit license_link: >- https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/LICENSE language: - en pipeline_tag: text-generation tags: - nlp - code --- This repo provides the GGUF format for the Phi-3-Mini-4K-Instruct. The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoni...', '["gguf","nlp","code","text-generation","en","license:mit","endpoints_compatible","region:us","conversational"]', 'text-generation', 546, 92706, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: >-\n  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n---\n\n## Model Summary\n\nThis repo provides the GGUF format for the Phi-3-Mini-4K-Instruct. \nThe Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) it can support. \nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\nResources and Technical Documentation:\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/phi3blog-april)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n+ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai)\n+ [Phi-3 on Hugging Face](https://aka.ms/phi3-hf)\n+ Phi-3 ONNX: [4K](https://aka.ms/phi3-mini-4k-instruct-onnx) and [128K](https://aka.ms/phi3-mini-128k-instruct-onnx)\n\nThis repo provides GGUF files for the Phi-3 Mini-4K-Instruct model. \n| Name | Quant method | Bits | Size | Use case |\n| ---- | ---- | ---- | ---- | ----- |\n| [Phi-3-mini-4k-instruct-q4.gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-q4.gguf) | Q4_K_M | 4 | 2.2 GB| medium, balanced quality - recommended |\n| [Phi-3-mini-4k-instruct-fp16.gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-fp16.gguf) | None | 16 | 7.2 GB | minimal quality loss |\n\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require \n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. \nDevelopers  should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.  \n\n\n## Chat Format:\n\nGiven the nature of the training data, the Phi-3-Mini-4K-instruct model is best suited for prompts using the chat format as follows. \nYou can provide the prompt as a question with a generic template as follow:\n```markdown\n<|user|>\nQuestion <|end|>\n<|assistant|>\n```\nFor example:\n```markdown\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\nwhere the model generates the text after "<|assistant|>" . In case of few-shots prompt, the prompt can be formatted as the following:\n\n```markdown\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world''s largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it''s no wonder that Paris is one of the most popular tourist destinations in the world."<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\n```\n\n## How to download GGUF files\n\n1. **Install Hugging Face CLI:**\n\n```\npip install huggingface-hub>=0.17.1\n```\n\n2. **Login to Hugging Face:**\n```\nhuggingface-cli login\n```\n\n3. **Download the GGUF model:**\n```\nhuggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir . --local-dir-use-symlinks False\n```\n\n## How to use with Ollama\n\n1. **Install Ollama:**\n\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n2. **Run the *phi3* model:**\n\n```\nollama run phi3\n```\n\n### Building from `Modelfile`\n\nAssuming that you have already downloaded GGUF files, here is how you can use them with [Ollama](https://ollama.com/):\n\n1. **Get the Modelfile:**\n\n```\nhuggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir /path/to/your/local/dir\n```\n\n2. Build the Ollama Model:\nUse the Ollama CLI to create your model with the following command:\n\n```\nollama create phi3 -f Modelfile_q4\n```\n\n3. **Run the *phi3* model:** \n\nNow you can run the Phi-3-Mini-4k-Instruct model with Ollama using the following command:\n\n```\nollama run phi3 "Your prompt here"\n```\n\nReplace "Your prompt here" with the actual prompt you want to use for generating responses from the model.\n\n## How to use with Llamafile:\n\nAssuming that you already have GGUF files downloaded. Here is how you can use the GGUF model with [Llamafile](https://github.com/Mozilla-Ocho/llamafile):\n\n1. **Download Llamafile-0.7.3**\n```\nwget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.7.3/llamafile-0.7.3\n```\n2. **Run the model with chat format prompt:**\n\n\n```markdown\n<|user|>\nHow to explain Internet for a medieval knight?\n<|end|>\n<|assistant|>\n```\n\n\n```\n./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf --temp 0.6 -p "<|user|>\nHow to explain Internet for a medieval knight?\n<|end|>\n<|assistant|>"\n```\n\n3. **Run with a chat interface:**\n\n```\n./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf\n```\n\nYour browser should open automatically and display a chat interface. (If it doesn''t, just open your browser and point it at http://localhost:8080)\n\n## How to run on Python:\n\n1. **Install llama-cpp-python:**\n\n```\n! CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python\n```\n\n2. **Run the model:**\n\n```python\nfrom llama_cpp import Llama\n\n\nllm = Llama(\n  model_path="./Phi-3-mini-4k-instruct-q4.gguf",  # path to GGUF file\n  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n)\n\nprompt = "How to explain Internet to a medieval knight?"\n\n# Simple inference example\noutput = llm(\n  f"<|user|>\n{prompt}<|end|>\n<|assistant|>",\n  max_tokens=256,  # Generate up to 256 tokens\n  stop=["<|end|>"], \n  echo=True,  # Whether to echo the prompt\n)\n\nprint(output[''choices''][0][''text''])\n```\n\n## Responsible AI Considerations\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n\n## Training\n\n### Model\n\n* Architecture: Phi-3 Mini has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\n* Inputs: Text. It is best suited for prompts using chat format.\n* Context length: 4K tokens\n* GPUS: 512 H100-80G\n* Training time: 7 days\n* Training data: 3.3T tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between February and April 2024\n* Status: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n\n### Datasets\nOur training data includes a wide variety of sources, totaling 3.3 trillion tokens, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) newly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/phi-3-mini-128k/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØ[Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.', '{"pipeline_tag":"text-generation","library_name":null,"framework":null,"params":null,"storage_bytes":32354200608,"files_count":10,"spaces_count":29,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Mozilla-Ocho:llamafile","source_url":"https://github.com/Mozilla-Ocho/llamafile"},{"type":"has_code","target_id":"github:Mozilla-Ocho:llamafile","source_url":"https://github.com/Mozilla-Ocho/llamafile"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:microsoft:DeepSpeed","source_url":"https://github.com/microsoft/DeepSpeed"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"}]', NULL, 'MIT', 'approved', 77.4, 'c8ae3f15029ff1becc8c53ca055e8a5b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-7B-v0.3', 'huggingface--mistralai--mistral-7b-v0.3', 'Mistral-7B-v0.3', 'mistralai', '--- library_name: vllm license: apache-2.0 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/fr/terms/">Privacy Policy</a>. tags: - mistral-common --- The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary. Mistral-7B-v0.3 has the following changes compared to Mistral-7B-v0.2 - Extended vocabulary to 32768 It is recommended to use with mistral-inference. For HF transf...', '["vllm","safetensors","mistral","mistral-common","license:apache-2.0","region:us"]', 'other', 546, 153334, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-7B-v0.3","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlicense: apache-2.0\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/fr/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Model Card for Mistral-7B-v0.3\n\nThe Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.\n\nMistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-v0.2/edit/main/README.md)\n- Extended vocabulary to 32768\n\n## Installation\n\nIt is recommended to use `mistralai/Mistral-7B-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''7B-v0.3'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Mistral-7B-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)\n```\n\n### Demo\n\nAfter installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.\n\n```\nmistral-demo $HOME/mistral_models/7B-v0.3\n```\n\nShould give something along the following lines:\n\n```\nThis is a test of the emergency broadcast system. This is only a test.\n\nIf this were a real emergency, you would be told what to do.\n\nThis is a test\n=====================\nThis is another test of the new blogging software. I‚Äôm not sure if I‚Äôm going to keep it or not. I‚Äôm not sure if I‚Äôm going to keep\n=====================\nThis is a third test, mistral AI is very good at testing. üôÇ\n\nThis is a third test, mistral AI is very good at testing. üôÇ\n\nThis\n=====================\n```\n\n## Generate with `transformers`\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mistral-7B-v0.3"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ninputs = tokenizer("Hello my name is", return_tensors="pt")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timoth√©e Lacroix, Th√©ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":7248023552,"storage_bytes":28992746844,"files_count":15,"spaces_count":92,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"}]', NULL, 'Apache-2.0', 'approved', 62.4, 'c4cd79cb63281edec23e4a2210fa5d0d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nomic-ai-nomic-embed-text-v1', 'huggingface--nomic-ai--nomic-embed-text-v1', 'nomic-embed-text-v1', 'nomic-ai', '--- library_name: sentence-transformers pipeline_tag: sentence-similarity tags: - feature-extraction - sentence-similarity - mteb - transformers - transformers.js model-index: - name: epoch_0_model results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 76.8507462686567 - type: ap value: 40.592189159090495 - type:...', '["sentence-transformers","pytorch","onnx","safetensors","nomic_bert","feature-extraction","sentence-similarity","mteb","transformers","transformers.js","custom_code","en","arxiv:2402.01613","license:apache-2.0","model-index","text-embeddings-inference","endpoints_compatible","region:us"]', 'sentence-similarity', 545, 744333, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nomic-ai/nomic-embed-text-v1","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: sentence-transformers\npipeline_tag: sentence-similarity\ntags:\n- feature-extraction\n- sentence-similarity\n- mteb\n- transformers\n- transformers.js\nmodel-index:\n- name: epoch_0_model\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 76.8507462686567\n    - type: ap\n      value: 40.592189159090495\n    - type: f1\n      value: 71.01634655512476\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 91.51892500000001\n    - type: ap\n      value: 88.50346762975335\n    - type: f1\n      value: 91.50342077459624\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.364\n    - type: f1\n      value: 46.72708080922794\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.178\n    - type: map_at_10\n      value: 40.244\n    - type: map_at_100\n      value: 41.321999999999996\n    - type: map_at_1000\n      value: 41.331\n    - type: map_at_3\n      value: 35.016999999999996\n    - type: map_at_5\n      value: 37.99\n    - type: mrr_at_1\n      value: 25.605\n    - type: mrr_at_10\n      value: 40.422000000000004\n    - type: mrr_at_100\n      value: 41.507\n    - type: mrr_at_1000\n      value: 41.516\n    - type: mrr_at_3\n      value: 35.23\n    - type: mrr_at_5\n      value: 38.15\n    - type: ndcg_at_1\n      value: 25.178\n    - type: ndcg_at_10\n      value: 49.258\n    - type: ndcg_at_100\n      value: 53.776\n    - type: ndcg_at_1000\n      value: 53.995000000000005\n    - type: ndcg_at_3\n      value: 38.429\n    - type: ndcg_at_5\n      value: 43.803\n    - type: precision_at_1\n      value: 25.178\n    - type: precision_at_10\n      value: 7.831\n    - type: precision_at_100\n      value: 0.979\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 16.121\n    - type: precision_at_5\n      value: 12.29\n    - type: recall_at_1\n      value: 25.178\n    - type: recall_at_10\n      value: 78.307\n    - type: recall_at_100\n      value: 97.866\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_3\n      value: 48.364000000000004\n    - type: recall_at_5\n      value: 61.451\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 45.93034494751465\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 36.64579480054327\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 60.601310529222054\n    - type: mrr\n      value: 75.04484896451656\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.57797718095814\n    - type: cos_sim_spearman\n      value: 86.47064499110101\n    - type: euclidean_pearson\n      value: 87.4559602783142\n    - type: euclidean_spearman\n      value: 86.47064499110101\n    - type: manhattan_pearson\n      value: 87.7232764230245\n    - type: manhattan_spearman\n      value: 86.91222131777742\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 84.5422077922078\n    - type: f1\n      value: 84.47657456950589\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 38.48953561974464\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 32.75995857510105\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.008000000000003\n    - type: map_at_10\n      value: 39.51\n    - type: map_at_100\n      value: 40.841\n    - type: map_at_1000\n      value: 40.973\n    - type: map_at_3\n      value: 36.248999999999995\n    - type: map_at_5\n      value: 38.096999999999994\n    - type: mrr_at_1\n      value: 36.481\n    - type: mrr_at_10\n      value: 44.818000000000005\n    - type: mrr_at_100\n      value: 45.64\n    - type: mrr_at_1000\n      value: 45.687\n    - type: mrr_at_3\n      value: 42.036\n    - type: mrr_at_5\n      value: 43.782\n    - type: ndcg_at_1\n      value: 36.481\n    - type: ndcg_at_10\n      value: 45.152\n    - type: ndcg_at_100\n      value: 50.449\n    - type: ndcg_at_1000\n      value: 52.76499999999999\n    - type: ndcg_at_3\n      value: 40.161\n    - type: ndcg_at_5\n      value: 42.577999999999996\n    - type: precision_at_1\n      value: 36.481\n    - type: precision_at_10\n      value: 8.369\n    - type: precision_at_100\n      value: 1.373\n    - type: precision_at_1000\n      value: 0.186\n    - type: precision_at_3\n      value: 18.693\n    - type: precision_at_5\n      value: 13.533999999999999\n    - type: recall_at_1\n      value: 30.008000000000003\n    - type: recall_at_10\n      value: 56.108999999999995\n    - type: recall_at_100\n      value: 78.55499999999999\n    - type: recall_at_1000\n      value: 93.659\n    - type: recall_at_3\n      value: 41.754999999999995\n    - type: recall_at_5\n      value: 48.296\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.262\n    - type: map_at_10\n      value: 40.139\n    - type: map_at_100\n      value: 41.394\n    - type: map_at_1000\n      value: 41.526\n    - type: map_at_3\n      value: 37.155\n    - type: map_at_5\n      value: 38.785\n    - type: mrr_at_1\n      value: 38.153\n    - type: mrr_at_10\n      value: 46.369\n    - type: mrr_at_100\n      value: 47.072\n    - type: mrr_at_1000\n      value: 47.111999999999995\n    - type: mrr_at_3\n      value: 44.268\n    - type: mrr_at_5\n      value: 45.389\n    - type: ndcg_at_1\n      value: 38.153\n    - type: ndcg_at_10\n      value: 45.925\n    - type: ndcg_at_100\n      value: 50.394000000000005\n    - type: ndcg_at_1000\n      value: 52.37500000000001\n    - type: ndcg_at_3\n      value: 41.754000000000005\n    - type: ndcg_at_5\n      value: 43.574\n    - type: precision_at_1\n      value: 38.153\n    - type: precision_at_10\n      value: 8.796\n    - type: precision_at_100\n      value: 1.432\n    - type: precision_at_1000\n      value: 0.189\n    - type: precision_at_3\n      value: 20.318\n    - type: precision_at_5\n      value: 14.395\n    - type: recall_at_1\n      value: 30.262\n    - type: recall_at_10\n      value: 55.72200000000001\n    - type: recall_at_100\n      value: 74.97500000000001\n    - type: recall_at_1000\n      value: 87.342\n    - type: recall_at_3\n      value: 43.129\n    - type: recall_at_5\n      value: 48.336\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.951\n    - type: map_at_10\n      value: 51.248000000000005\n    - type: map_at_100\n      value: 52.188\n    - type: map_at_1000\n      value: 52.247\n    - type: map_at_3\n      value: 48.211\n    - type: map_at_5\n      value: 49.797000000000004\n    - type: mrr_at_1\n      value: 45.329\n    - type: mrr_at_10\n      value: 54.749\n    - type: mrr_at_100\n      value: 55.367999999999995\n    - type: mrr_at_1000\n      value: 55.400000000000006\n    - type: mrr_at_3\n      value: 52.382\n    - type: mrr_at_5\n      value: 53.649\n    - type: ndcg_at_1\n      value: 45.329\n    - type: ndcg_at_10\n      value: 56.847\n    - type: ndcg_at_100\n      value: 60.738\n    - type: ndcg_at_1000\n      value: 61.976\n    - type: ndcg_at_3\n      value: 51.59\n    - type: ndcg_at_5\n      value: 53.915\n    - type: precision_at_1\n      value: 45.329\n    - type: precision_at_10\n      value: 8.959\n    - type: precision_at_100\n      value: 1.187\n    - type: precision_at_1000\n      value: 0.134\n    - type: precision_at_3\n      value: 22.612\n    - type: precision_at_5\n      value: 15.273\n    - type: recall_at_1\n      value: 39.951\n    - type: recall_at_10\n      value: 70.053\n    - type: recall_at_100\n      value: 86.996\n    - type: recall_at_1000\n      value: 95.707\n    - type: recall_at_3\n      value: 56.032000000000004\n    - type: recall_at_5\n      value: 61.629999999999995\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.566\n    - type: map_at_10\n      value: 33.207\n    - type: map_at_100\n      value: 34.166000000000004\n    - type: map_at_1000\n      value: 34.245\n    - type: map_at_3\n      value: 30.94\n    - type: map_at_5\n      value: 32.01\n    - type: mrr_at_1\n      value: 27.345000000000002\n    - type: mrr_at_10\n      value: 35.193000000000005\n    - type: mrr_at_100\n      value: 35.965\n    - type: mrr_at_1000\n      value: 36.028999999999996\n    - type: mrr_at_3\n      value: 32.806000000000004\n    - type: mrr_at_5\n      value: 34.021\n    - type: ndcg_at_1\n      value: 27.345000000000002\n    - type: ndcg_at_10\n      value: 37.891999999999996\n    - type: ndcg_at_100\n      value: 42.664\n    - type: ndcg_at_1000\n      value: 44.757000000000005\n    - type: ndcg_at_3\n      value: 33.123000000000005\n    - type: ndcg_at_5\n      value: 35.035\n    - type: precision_at_1\n      value: 27.345000000000002\n    - type: precision_at_10\n      value: 5.763\n    - type: precision_at_100\n      value: 0.859\n    - type: precision_at_1000\n      value: 0.108\n    - type: precision_at_3\n      value: 13.71\n    - type: precision_at_5\n      value: 9.401\n    - type: recall_at_1\n      value: 25.566\n    - type: recall_at_10\n      value: 50.563\n    - type: recall_at_100\n      value: 72.86399999999999\n    - type: recall_at_1000\n      value: 88.68599999999999\n    - type: recall_at_3\n      value: 37.43\n    - type: recall_at_5\n      value: 41.894999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.663\n    - type: map_at_10\n      value: 23.552\n    - type: map_at_100\n      value: 24.538\n    - type: map_at_1000\n      value: 24.661\n    - type: map_at_3\n      value: 21.085\n    - type: map_at_5\n      value: 22.391\n    - type: mrr_at_1\n      value: 20.025000000000002\n    - type: mrr_at_10\n      value: 27.643\n    - type: mrr_at_100\n      value: 28.499999999999996\n    - type: mrr_at_1000\n      value: 28.582\n    - type: mrr_at_3\n      value: 25.083\n    - type: mrr_at_5\n      value: 26.544\n    - type: ndcg_at_1\n      value: 20.025000000000002\n    - type: ndcg_at_10\n      value: 28.272000000000002\n    - type: ndcg_at_100\n      value: 33.353\n    - type: ndcg_at_1000\n      value: 36.454\n    - type: ndcg_at_3\n      value: 23.579\n    - type: ndcg_at_5\n      value: 25.685000000000002\n    - type: precision_at_1\n      value: 20.025000000000002\n    - type: precision_at_10\n      value: 5.187\n    - type: precision_at_100\n      value: 0.897\n    - type: precision_at_1000\n      value: 0.13\n    - type: precision_at_3\n      value: 10.987\n    - type: precision_at_5\n      value: 8.06\n    - type: recall_at_1\n      value: 16.663\n    - type: recall_at_10\n      value: 38.808\n    - type: recall_at_100\n      value: 61.305\n    - type: recall_at_1000\n      value: 83.571\n    - type: recall_at_3\n      value: 25.907999999999998\n    - type: recall_at_5\n      value: 31.214\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.695999999999998\n    - type: map_at_10\n      value: 37.018\n    - type: map_at_100\n      value: 38.263000000000005\n    - type: map_at_1000\n      value: 38.371\n    - type: map_at_3\n      value: 34.226\n    - type: map_at_5\n      value: 35.809999999999995\n    - type: mrr_at_1\n      value: 32.916000000000004\n    - type: mrr_at_10\n      value: 42.067\n    - type: mrr_at_100\n      value: 42.925000000000004\n    - type: mrr_at_1000\n      value: 42.978\n    - type: mrr_at_3\n      value: 39.637\n    - type: mrr_at_5\n      value: 41.134\n    - type: ndcg_at_1\n      value: 32.916000000000004\n    - type: ndcg_at_10\n      value: 42.539\n    - type: ndcg_at_100\n      value: 47.873\n    - type: ndcg_at_1000\n      value: 50.08200000000001\n    - type: ndcg_at_3\n      value: 37.852999999999994\n    - type: ndcg_at_5\n      value: 40.201\n    - type: precision_at_1\n      value: 32.916000000000004\n    - type: precision_at_10\n      value: 7.5840000000000005\n    - type: precision_at_100\n      value: 1.199\n    - type: precision_at_1000\n      value: 0.155\n    - type: precision_at_3\n      value: 17.485\n    - type: precision_at_5\n      value: 12.512\n    - type: recall_at_1\n      value: 27.695999999999998\n    - type: recall_at_10\n      value: 53.638\n    - type: recall_at_100\n      value: 76.116\n    - type: recall_at_1000\n      value: 91.069\n    - type: recall_at_3\n      value: 41.13\n    - type: recall_at_5\n      value: 46.872\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.108\n    - type: map_at_10\n      value: 33.372\n    - type: map_at_100\n      value: 34.656\n    - type: map_at_1000\n      value: 34.768\n    - type: map_at_3\n      value: 30.830999999999996\n    - type: map_at_5\n      value: 32.204\n    - type: mrr_at_1\n      value: 29.110000000000003\n    - type: mrr_at_10\n      value: 37.979\n    - type: mrr_at_100\n      value: 38.933\n    - type: mrr_at_1000\n      value: 38.988\n    - type: mrr_at_3\n      value: 35.731\n    - type: mrr_at_5\n      value: 36.963\n    - type: ndcg_at_1\n      value: 29.110000000000003\n    - type: ndcg_at_10\n      value: 38.635000000000005\n    - type: ndcg_at_100\n      value: 44.324999999999996\n    - type: ndcg_at_1000\n      value: 46.747\n    - type: ndcg_at_3\n      value: 34.37\n    - type: ndcg_at_5\n      value: 36.228\n    - type: precision_at_1\n      value: 29.110000000000003\n    - type: precision_at_10\n      value: 6.963\n    - type: precision_at_100\n      value: 1.146\n    - type: precision_at_1000\n      value: 0.152\n    - type: precision_at_3\n      value: 16.400000000000002\n    - type: precision_at_5\n      value: 11.552999999999999\n    - type: recall_at_1\n      value: 24.108\n    - type: recall_at_10\n      value: 49.597\n    - type: recall_at_100\n      value: 73.88900000000001\n    - type: recall_at_1000\n      value: 90.62400000000001\n    - type: recall_at_3\n      value: 37.662\n    - type: recall_at_5\n      value: 42.565\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.00791666666667\n    - type: map_at_10\n      value: 33.287749999999996\n    - type: map_at_100\n      value: 34.41141666666667\n    - type: map_at_1000\n      value: 34.52583333333333\n    - type: map_at_3\n      value: 30.734416666666668\n    - type: map_at_5\n      value: 32.137166666666666\n    - type: mrr_at_1\n      value: 29.305666666666664\n    - type: mrr_at_10\n      value: 37.22966666666666\n    - type: mrr_at_100\n      value: 38.066583333333334\n    - type: mrr_at_1000\n      value: 38.12616666666667\n    - type: mrr_at_3\n      value: 34.92275\n    - type: mrr_at_5\n      value: 36.23333333333334\n    - type: ndcg_at_1\n      value: 29.305666666666664\n    - type: ndcg_at_10\n      value: 38.25533333333333\n    - type: ndcg_at_100\n      value: 43.25266666666666\n    - type: ndcg_at_1000\n      value: 45.63583333333334\n    - type: ndcg_at_3\n      value: 33.777166666666666\n    - type: ndcg_at_5\n      value: 35.85\n    - type: precision_at_1\n      value: 29.305666666666664\n    - type: precision_at_10\n      value: 6.596416666666667\n    - type: precision_at_100\n      value: 1.0784166666666668\n    - type: precision_at_1000\n      value: 0.14666666666666664\n    - type: precision_at_3\n      value: 15.31075\n    - type: precision_at_5\n      value: 10.830916666666667\n    - type: recall_at_1\n      value: 25.00791666666667\n    - type: recall_at_10\n      value: 49.10933333333333\n    - type: recall_at_100\n      value: 71.09216666666667\n    - type: recall_at_1000\n      value: 87.77725000000001\n    - type: recall_at_3\n      value: 36.660916666666665\n    - type: recall_at_5\n      value: 41.94149999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.521\n    - type: map_at_10\n      value: 30.043\n    - type: map_at_100\n      value: 30.936000000000003\n    - type: map_at_1000\n      value: 31.022\n    - type: map_at_3\n      value: 27.926000000000002\n    - type: map_at_5\n      value: 29.076999999999998\n    - type: mrr_at_1\n      value: 26.227\n    - type: mrr_at_10\n      value: 32.822\n    - type: mrr_at_100\n      value: 33.61\n    - type: mrr_at_1000\n      value: 33.672000000000004\n    - type: mrr_at_3\n      value: 30.776999999999997\n    - type: mrr_at_5\n      value: 31.866\n    - type: ndcg_at_1\n      value: 26.227\n    - type: ndcg_at_10\n      value: 34.041\n    - type: ndcg_at_100\n      value: 38.394\n    - type: ndcg_at_1000\n      value: 40.732\n    - type: ndcg_at_3\n      value: 30.037999999999997\n    - type: ndcg_at_5\n      value: 31.845000000000002\n    - type: precision_at_1\n      value: 26.227\n    - type: precision_at_10\n      value: 5.244999999999999\n    - type: precision_at_100\n      value: 0.808\n    - type: precision_at_1000\n      value: 0.107\n    - type: precision_at_3\n      value: 12.679000000000002\n    - type: precision_at_5\n      value: 8.773\n    - type: recall_at_1\n      value: 23.521\n    - type: recall_at_10\n      value: 43.633\n    - type: recall_at_100\n      value: 63.126000000000005\n    - type: recall_at_1000\n      value: 80.765\n    - type: recall_at_3\n      value: 32.614\n    - type: recall_at_5\n      value: 37.15\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.236\n    - type: map_at_10\n      value: 22.898\n    - type: map_at_100\n      value: 23.878\n    - type: map_at_1000\n      value: 24.009\n    - type: map_at_3\n      value: 20.87\n    - type: map_at_5\n      value: 22.025\n    - type: mrr_at_1\n      value: 19.339000000000002\n    - type: mrr_at_10\n      value: 26.382\n    - type: mrr_at_100\n      value: 27.245\n    - type: mrr_at_1000\n      value: 27.33\n    - type: mrr_at_3\n      value: 24.386\n    - type: mrr_at_5\n      value: 25.496000000000002\n    - type: ndcg_at_1\n      value: 19.339000000000002\n    - type: ndcg_at_10\n      value: 27.139999999999997\n    - type: ndcg_at_100\n      value: 31.944\n    - type: ndcg_at_1000\n      value: 35.077999999999996\n    - type: ndcg_at_3\n      value: 23.424\n    - type: ndcg_at_5\n      value: 25.188\n    - type: precision_at_1\n      value: 19.339000000000002\n    - type: precision_at_10\n      value: 4.8309999999999995\n    - type: precision_at_100\n      value: 0.845\n    - type: precision_at_1000\n      value: 0.128\n    - type: precision_at_3\n      value: 10.874\n    - type: precision_at_5\n      value: 7.825\n    - type: recall_at_1\n      value: 16.236\n    - type: recall_at_10\n      value: 36.513\n    - type: recall_at_100\n      value: 57.999\n    - type: recall_at_1000\n      value: 80.512\n    - type: recall_at_3\n      value: 26.179999999999996\n    - type: recall_at_5\n      value: 30.712\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.11\n    - type: map_at_10\n      value: 31.566\n    - type: map_at_100\n      value: 32.647\n    - type: map_at_1000\n      value: 32.753\n    - type: map_at_3\n      value: 29.24\n    - type: map_at_5\n      value: 30.564999999999998\n    - type: mrr_at_1\n      value: 28.265\n    - type: mrr_at_10\n      value: 35.504000000000005\n    - type: mrr_at_100\n      value: 36.436\n    - type: mrr_at_1000\n      value: 36.503\n    - type: mrr_at_3\n      value: 33.349000000000004\n    - type: mrr_at_5\n      value: 34.622\n    - type: ndcg_at_1\n      value: 28.265\n    - type: ndcg_at_10\n      value: 36.192\n    - type: ndcg_at_100\n      value: 41.388000000000005\n    - type: ndcg_at_1000\n      value: 43.948\n    - type: ndcg_at_3\n      value: 31.959\n    - type: ndcg_at_5\n      value: 33.998\n    - type: precision_at_1\n      value: 28.265\n    - type: precision_at_10\n      value: 5.989\n    - type: precision_at_100\n      value: 0.9650000000000001\n    - type: precision_at_1000\n      value: 0.13\n    - type: precision_at_3\n      value: 14.335\n    - type: precision_at_5\n      value: 10.112\n    - type: recall_at_1\n      value: 24.11\n    - type: recall_at_10\n      value: 46.418\n    - type: recall_at_100\n      value: 69.314\n    - type: recall_at_1000\n      value: 87.397\n    - type: recall_at_3\n      value: 34.724\n    - type: recall_at_5\n      value: 39.925\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.091\n    - type: map_at_10\n      value: 29.948999999999998\n    - type: map_at_100\n      value: 31.502000000000002\n    - type: map_at_1000\n      value: 31.713\n    - type: map_at_3\n      value: 27.464\n    - type: map_at_5\n      value: 28.968\n    - type: mrr_at_1\n      value: 26.482\n    - type: mrr_at_10\n      value: 34.009\n    - type: mrr_at_100\n      value: 35.081\n    - type: mrr_at_1000\n      value: 35.138000000000005\n    - type: mrr_at_3\n      value: 31.785000000000004\n    - type: mrr_at_5\n      value: 33.178999999999995\n    - type: ndcg_at_1\n      value: 26.482\n    - type: ndcg_at_10\n      value: 35.008\n    - type: ndcg_at_100\n      value: 41.272999999999996\n    - type: ndcg_at_1000\n      value: 43.972\n    - type: ndcg_at_3\n      value: 30.804\n    - type: ndcg_at_5\n      value: 33.046\n    - type: precision_at_1\n      value: 26.482\n    - type: precision_at_10\n      value: 6.462\n    - type: precision_at_100\n      value: 1.431\n    - type: precision_at_1000\n      value: 0.22899999999999998\n    - type: precision_at_3\n      value: 14.360999999999999\n    - type: precision_at_5\n      value: 10.474\n    - type: recall_at_1\n      value: 22.091\n    - type: recall_at_10\n      value: 45.125\n    - type: recall_at_100\n      value: 72.313\n    - type: recall_at_1000\n      value: 89.503\n    - type: recall_at_3\n      value: 33.158\n    - type: recall_at_5\n      value: 39.086999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 19.883\n    - type: map_at_10\n      value: 26.951000000000004\n    - type: map_at_100\n      value: 27.927999999999997\n    - type: map_at_1000\n      value: 28.022000000000002\n    - type: map_at_3\n      value: 24.616\n    - type: map_at_5\n      value: 25.917\n    - type: mrr_at_1\n      value: 21.996\n    - type: mrr_at_10\n      value: 29.221000000000004\n    - type: mrr_at_100\n      value: 30.024\n    - type: mrr_at_1000\n      value: 30.095\n    - type: mrr_at_3\n      value: 26.833000000000002\n    - type: mrr_at_5\n      value: 28.155\n    - type: ndcg_at_1\n      value: 21.996\n    - type: ndcg_at_10\n      value: 31.421\n    - type: ndcg_at_100\n      value: 36.237\n    - type: ndcg_at_1000\n      value: 38.744\n    - type: ndcg_at_3\n      value: 26.671\n    - type: ndcg_at_5\n      value: 28.907\n    - type: precision_at_1\n      value: 21.996\n    - type: precision_at_10\n      value: 5.009\n    - type: precision_at_100\n      value: 0.799\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 11.275\n    - type: precision_at_5\n      value: 8.059\n    - type: recall_at_1\n      value: 19.883\n    - type: recall_at_10\n      value: 43.132999999999996\n    - type: recall_at_100\n      value: 65.654\n    - type: recall_at_1000\n      value: 84.492\n    - type: recall_at_3\n      value: 30.209000000000003\n    - type: recall_at_5\n      value: 35.616\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.756\n    - type: map_at_10\n      value: 30.378\n    - type: map_at_100\n      value: 32.537\n    - type: map_at_1000\n      value: 32.717\n    - type: map_at_3\n      value: 25.599\n    - type: map_at_5\n      value: 28.372999999999998\n    - type: mrr_at_1\n      value: 41.303\n    - type: mrr_at_10\n      value: 53.483999999999995\n    - type: mrr_at_100\n      value: 54.106\n    - type: mrr_at_1000\n      value: 54.127\n    - type: mrr_at_3\n      value: 50.315\n    - type: mrr_at_5\n      value: 52.396\n    - type: ndcg_at_1\n      value: 41.303\n    - type: ndcg_at_10\n      value: 40.503\n    - type: ndcg_at_100\n      value: 47.821000000000005\n    - type: ndcg_at_1000\n      value: 50.788\n    - type: ndcg_at_3\n      value: 34.364\n    - type: ndcg_at_5\n      value: 36.818\n    - type: precision_at_1\n      value: 41.303\n    - type: precision_at_10\n      value: 12.463000000000001\n    - type: precision_at_100\n      value: 2.037\n    - type: precision_at_1000\n      value: 0.26\n    - type: precision_at_3\n      value: 25.798\n    - type: precision_at_5\n      value: 19.896\n    - type: recall_at_1\n      value: 17.756\n    - type: recall_at_10\n      value: 46.102\n    - type: recall_at_100\n      value: 70.819\n    - type: recall_at_1000\n      value: 87.21799999999999\n    - type: recall_at_3\n      value: 30.646\n    - type: recall_at_5\n      value: 38.022\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 9.033\n    - type: map_at_10\n      value: 20.584\n    - type: map_at_100\n      value: 29.518\n    - type: map_at_1000\n      value: 31.186000000000003\n    - type: map_at_3\n      value: 14.468\n    - type: map_at_5\n      value: 17.177\n    - type: mrr_at_1\n      value: 69.75\n    - type: mrr_at_10\n      value: 77.025\n    - type: mrr_at_100\n      value: 77.36699999999999\n    - type: mrr_at_1000\n      value: 77.373\n    - type: mrr_at_3\n      value: 75.583\n    - type: mrr_at_5\n      value: 76.396\n    - type: ndcg_at_1\n      value: 58.5\n    - type: ndcg_at_10\n      value: 45.033\n    - type: ndcg_at_100\n      value: 49.071\n    - type: ndcg_at_1000\n      value: 56.056\n    - type: ndcg_at_3\n      value: 49.936\n    - type: ndcg_at_5\n      value: 47.471999999999994\n    - type: precision_at_1\n      value: 69.75\n    - type: precision_at_10\n      value: 35.775\n    - type: precision_at_100\n      value: 11.594999999999999\n    - type: precision_at_1000\n      value: 2.062\n    - type: precision_at_3\n      value: 52.5\n    - type: precision_at_5\n      value: 45.300000000000004\n    - type: recall_at_1\n      value: 9.033\n    - type: recall_at_10\n      value: 26.596999999999998\n    - type: recall_at_100\n      value: 54.607000000000006\n    - type: recall_at_1000\n      value: 76.961\n    - type: recall_at_3\n      value: 15.754999999999999\n    - type: recall_at_5\n      value: 20.033\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 48.345000000000006\n    - type: f1\n      value: 43.4514918068706\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.29100000000001\n    - type: map_at_10\n      value: 81.059\n    - type: map_at_100\n      value: 81.341\n    - type: map_at_1000\n      value: 81.355\n    - type: map_at_3\n      value: 79.74799999999999\n    - type: map_at_5\n      value: 80.612\n    - type: mrr_at_1\n      value: 76.40299999999999\n    - type: mrr_at_10\n      value: 84.615\n    - type: mrr_at_100\n      value: 84.745\n    - type: mrr_at_1000\n      value: 84.748\n    - type: mrr_at_3\n      value: 83.776\n    - type: mrr_at_5\n      value: 84.343\n    - type: ndcg_at_1\n      value: 76.40299999999999\n    - type: ndcg_at_10\n      value: 84.981\n    - type: ndcg_at_100\n      value: 86.00999999999999\n    - type: ndcg_at_1000\n      value: 86.252\n    - type: ndcg_at_3\n      value: 82.97\n    - type: ndcg_at_5\n      value: 84.152\n    - type: precision_at_1\n      value: 76.40299999999999\n    - type: precision_at_10\n      value: 10.446\n    - type: precision_at_100\n      value: 1.1199999999999999\n    - type: precision_at_1000\n      value: 0.116\n    - type: precision_at_3\n      value: 32.147999999999996\n    - type: precision_at_5\n      value: 20.135\n    - type: recall_at_1\n      value: 71.29100000000001\n    - type: recall_at_10\n      value: 93.232\n    - type: recall_at_100\n      value: 97.363\n    - type: recall_at_1000\n      value: 98.905\n    - type: recall_at_3\n      value: 87.893\n    - type: recall_at_5\n      value: 90.804\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 18.667\n    - type: map_at_10\n      value: 30.853\n    - type: map_at_100\n      value: 32.494\n    - type: map_at_1000\n      value: 32.677\n    - type: map_at_3\n      value: 26.91\n    - type: map_at_5\n      value: 29.099000000000004\n    - type: mrr_at_1\n      value: 37.191\n    - type: mrr_at_10\n      value: 46.171\n    - type: mrr_at_100\n      value: 47.056\n    - type: mrr_at_1000\n      value: 47.099000000000004\n    - type: mrr_at_3\n      value: 44.059\n    - type: mrr_at_5\n      value: 45.147\n    - type: ndcg_at_1\n      value: 37.191\n    - type: ndcg_at_10\n      value: 38.437\n    - type: ndcg_at_100\n      value: 44.62\n    - type: ndcg_at_1000\n      value: 47.795\n    - type: ndcg_at_3\n      value: 35.003\n    - type: ndcg_at_5\n      value: 36.006\n    - type: precision_at_1\n      value: 37.191\n    - type: precision_at_10\n      value: 10.586\n    - type: precision_at_100\n      value: 1.688\n    - type: precision_at_1000\n      value: 0.22699999999999998\n    - type: precision_at_3\n      value: 23.302\n    - type: precision_at_5\n      value: 17.006\n    - type: recall_at_1\n      value: 18.667\n    - type: recall_at_10\n      value: 45.367000000000004\n    - type: recall_at_100\n      value: 68.207\n    - type: recall_at_1000\n      value: 87.072\n    - type: recall_at_3\n      value: 32.129000000000005\n    - type: recall_at_5\n      value: 37.719\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.494\n    - type: map_at_10\n      value: 66.223\n    - type: map_at_100\n      value: 67.062\n    - type: map_at_1000\n      value: 67.11500000000001\n    - type: map_at_3\n      value: 62.867\n    - type: map_at_5\n      value: 64.994\n    - type: mrr_at_1\n      value: 78.987\n    - type: mrr_at_10\n      value: 84.585\n    - type: mrr_at_100\n      value: 84.773\n    - type: mrr_at_1000\n      value: 84.77900000000001\n    - type: mrr_at_3\n      value: 83.592\n    - type: mrr_at_5\n      value: 84.235\n    - type: ndcg_at_1\n      value: 78.987\n    - type: ndcg_at_10\n      value: 73.64\n    - type: ndcg_at_100\n      value: 76.519\n    - type: ndcg_at_1000\n      value: 77.51\n    - type: ndcg_at_3\n      value: 68.893\n    - type: ndcg_at_5\n      value: 71.585\n    - type: precision_at_1\n      value: 78.987\n    - type: precision_at_10\n      value: 15.529000000000002\n    - type: precision_at_100\n      value: 1.7770000000000001\n    - type: precision_at_1000\n      value: 0.191\n    - type: precision_at_3\n      value: 44.808\n    - type: precision_at_5\n      value: 29.006999999999998\n    - type: recall_at_1\n      value: 39.494\n    - type: recall_at_10\n      value: 77.643\n    - type: recall_at_100\n      value: 88.825\n    - type: recall_at_1000\n      value: 95.321\n    - type: recall_at_3\n      value: 67.211\n    - type: recall_at_5\n      value: 72.519\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 85.55959999999999\n    - type: ap\n      value: 80.7246500384617\n    - type: f1\n      value: 85.52336485065454\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.631\n    - type: map_at_10\n      value: 36.264\n    - type: map_at_100\n      value: 37.428\n    - type: map_at_1000\n      value: 37.472\n    - type: map_at_3\n      value: 32.537\n    - type: map_at_5\n      value: 34.746\n    - type: mrr_at_1\n      value: 24.312\n    - type: mrr_at_10\n      value: 36.858000000000004\n    - type: mrr_at_100\n      value: 37.966\n    - type: mrr_at_1000\n      value: 38.004\n    - type: mrr_at_3\n      value: 33.188\n    - type: mrr_at_5\n      value: 35.367\n    - type: ndcg_at_1\n      value: 24.312\n    - type: ndcg_at_10\n      value: 43.126999999999995\n    - type: ndcg_at_100\n      value: 48.642\n    - type: ndcg_at_1000\n      value: 49.741\n    - type: ndcg_at_3\n      value: 35.589\n    - type: ndcg_at_5\n      value: 39.515\n    - type: precision_at_1\n      value: 24.312\n    - type: precision_at_10\n      value: 6.699\n    - type: precision_at_100\n      value: 0.9450000000000001\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 15.153\n    - type: precision_at_5\n      value: 11.065999999999999\n    - type: recall_at_1\n      value: 23.631\n    - type: recall_at_10\n      value: 64.145\n    - type: recall_at_100\n      value: 89.41\n    - type: recall_at_1000\n      value: 97.83500000000001\n    - type: recall_at_3\n      value: 43.769000000000005\n    - type: recall_at_5\n      value: 53.169\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.4108527131783\n    - type: f1\n      value: 93.1415880261038\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.24806201550388\n    - type: f1\n      value: 60.531916308197175\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.71553463349024\n    - type: f1\n      value: 71.70753174900791\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.79757901815736\n    - type: f1\n      value: 77.83719850433258\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 33.74193296622113\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 30.64257594108566\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 30.811018518883625\n    - type: mrr\n      value: 31.910376577445003\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.409\n    - type: map_at_10\n      value: 13.093\n    - type: map_at_100\n      value: 16.256999999999998\n    - type: map_at_1000\n      value: 17.617\n    - type: map_at_3\n      value: 9.555\n    - type: map_at_5\n      value: 11.428\n    - type: mrr_at_1\n      value: 45.201\n    - type: mrr_at_10\n      value: 54.179\n    - type: mrr_at_100\n      value: 54.812000000000005\n    - type: mrr_at_1000\n      value: 54.840999999999994\n    - type: mrr_at_3\n      value: 51.909000000000006\n    - type: mrr_at_5\n      value: 53.519000000000005\n    - type: ndcg_at_1\n      value: 43.189\n    - type: ndcg_at_10\n      value: 35.028\n    - type: ndcg_at_100\n      value: 31.226\n    - type: ndcg_at_1000\n      value: 39.678000000000004\n    - type: ndcg_at_3\n      value: 40.596\n    - type: ndcg_at_5\n      value: 38.75\n    - type: precision_at_1\n      value: 44.582\n    - type: precision_at_10\n      value: 25.974999999999998\n    - type: precision_at_100\n      value: 7.793\n    - type: precision_at_1000\n      value: 2.036\n    - type: precision_at_3\n      value: 38.493\n    - type: precision_at_5\n      value: 33.994\n    - type: recall_at_1\n      value: 5.409\n    - type: recall_at_10\n      value: 16.875999999999998\n    - type: recall_at_100\n      value: 30.316\n    - type: recall_at_1000\n      value: 60.891\n    - type: recall_at_3\n      value: 10.688\n    - type: recall_at_5\n      value: 13.832\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 36.375\n    - type: map_at_10\n      value: 51.991\n    - type: map_at_100\n      value: 52.91400000000001\n    - type: map_at_1000\n      value: 52.93600000000001\n    - type: map_at_3\n      value: 48.014\n    - type: map_at_5\n      value: 50.381\n    - type: mrr_at_1\n      value: 40.759\n    - type: mrr_at_10\n      value: 54.617000000000004\n    - type: mrr_at_100\n      value: 55.301\n    - type: mrr_at_1000\n      value: 55.315000000000005\n    - type: mrr_at_3\n      value: 51.516\n    - type: mrr_at_5\n      value: 53.435\n    - type: ndcg_at_1\n      value: 40.759\n    - type: ndcg_at_10\n      value: 59.384\n    - type: ndcg_at_100\n      value: 63.157\n    - type: ndcg_at_1000\n      value: 63.654999999999994\n    - type: ndcg_at_3\n      value: 52.114000000000004\n    - type: ndcg_at_5\n      value: 55.986000000000004\n    - type: precision_at_1\n      value: 40.759\n    - type: precision_at_10\n      value: 9.411999999999999\n    - type: precision_at_100\n      value: 1.153\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 23.329\n    - type: precision_at_5\n      value: 16.256999999999998\n    - type: recall_at_1\n      value: 36.375\n    - type: recall_at_10\n      value: 79.053\n    - type: recall_at_100\n      value: 95.167\n    - type: recall_at_1000\n      value: 98.82\n    - type: recall_at_3\n      value: 60.475\n    - type: recall_at_5\n      value: 69.327\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.256\n    - type: map_at_10\n      value: 83.8\n    - type: map_at_100\n      value: 84.425\n    - type: map_at_1000\n      value: 84.444\n    - type: map_at_3\n      value: 80.906\n    - type: map_at_5\n      value: 82.717\n    - type: mrr_at_1\n      value: 80.97999999999999\n    - type: mrr_at_10\n      value: 87.161\n    - type: mrr_at_100\n      value: 87.262\n    - type: mrr_at_1000\n      value: 87.263\n    - type: mrr_at_3\n      value: 86.175\n    - type: mrr_at_5\n      value: 86.848\n    - type: ndcg_at_1\n      value: 80.97999999999999\n    - type: ndcg_at_10\n      value: 87.697\n    - type: ndcg_at_100\n      value: 88.959\n    - type: ndcg_at_1000\n      value: 89.09899999999999\n    - type: ndcg_at_3\n      value: 84.83800000000001\n    - type: ndcg_at_5\n      value: 86.401\n    - type: precision_at_1\n      value: 80.97999999999999\n    - type: precision_at_10\n      value: 13.261000000000001\n    - type: precision_at_100\n      value: 1.5150000000000001\n    - type: precision_at_1000\n      value: 0.156\n    - type: precision_at_3\n      value: 37.01\n    - type: precision_at_5\n      value: 24.298000000000002\n    - type: recall_at_1\n      value: 70.256\n    - type: recall_at_10\n      value: 94.935\n    - type: recall_at_100\n      value: 99.274\n    - type: recall_at_1000\n      value: 99.928\n    - type: recall_at_3\n      value: 86.602\n    - type: recall_at_5\n      value: 91.133\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 56.322692497613104\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 61.895813503775074\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.338\n    - type: map_at_10\n      value: 10.767\n    - type: map_at_100\n      value: 12.537999999999998\n    - type: map_at_1000\n      value: 12.803999999999998\n    - type: map_at_3\n      value: 7.788\n    - type: map_at_5\n      value: 9.302000000000001\n    - type: mrr_at_1\n      value: 21.4\n    - type: mrr_at_10\n      value: 31.637999999999998\n    - type: mrr_at_100\n      value: 32.688\n    - type: mrr_at_1000\n      value: 32.756\n    - type: mrr_at_3\n      value: 28.433000000000003\n    - type: mrr_at_5\n      value: 30.178\n    - type: ndcg_at_1\n      value: 21.4\n    - type: ndcg_at_10\n      value: 18.293\n    - type: ndcg_at_100\n      value: 25.274\n    - type: ndcg_at_1000\n      value: 30.284\n    - type: ndcg_at_3\n      value: 17.391000000000002\n    - type: ndcg_at_5\n      value: 15.146999999999998\n    - type: precision_at_1\n      value: 21.4\n    - type: precision_at_10\n      value: 9.48\n    - type: precision_at_100\n      value: 1.949\n    - type: precision_at_1000\n      value: 0.316\n    - type: precision_at_3\n      value: 16.167\n    - type: precision_at_5\n      value: 13.22\n    - type: recall_at_1\n      value: 4.338\n    - type: recall_at_10\n      value: 19.213\n    - type: recall_at_100\n      value: 39.562999999999995\n    - type: recall_at_1000\n      value: 64.08\n    - type: recall_at_3\n      value: 9.828000000000001\n    - type: recall_at_5\n      value: 13.383000000000001\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 82.42568163642142\n    - type: cos_sim_spearman\n      value: 78.5797159641342\n    - type: euclidean_pearson\n      value: 80.22151260811604\n    - type: euclidean_spearman\n      value: 78.5797151953878\n    - type: manhattan_pearson\n      value: 80.21224215864788\n    - type: manhattan_spearman\n      value: 78.55641478381344\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.44020710812569\n    - type: cos_sim_spearman\n      value: 78.91631735081286\n    - type: euclidean_pearson\n      value: 81.64188964182102\n    - type: euclidean_spearman\n      value: 78.91633286881678\n    - type: manhattan_pearson\n      value: 81.69294748512496\n    - type: manhattan_spearman\n      value: 78.93438558002656\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.27165426412311\n    - type: cos_sim_spearman\n      value: 85.40429140249618\n    - type: euclidean_pearson\n      value: 84.7509580724893\n    - type: euclidean_spearman\n      value: 85.40429140249618\n    - type: manhattan_pearson\n      value: 84.76488289321308\n    - type: manhattan_spearman\n      value: 85.4256793698708\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.138851760732\n    - type: cos_sim_spearman\n      value: 81.64101363896586\n    - type: euclidean_pearson\n      value: 82.55165038934942\n    - type: euclidean_spearman\n      value: 81.64105257080502\n    - type: manhattan_pearson\n      value: 82.52802949883335\n    - type: manhattan_spearman\n      value: 81.61255430718158\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.0654695484029\n    - type: cos_sim_spearman\n      value: 87.20408521902229\n    - type: euclidean_pearson\n      value: 86.8110651362115\n    - type: euclidean_spearman\n      value: 87.20408521902229\n    - type: manhattan_pearson\n      value: 86.77984656478691\n    - type: manhattan_spearman\n      value: 87.1719947099227\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.77823915496512\n    - type: cos_sim_spearman\n      value: 85.43566325729779\n    - type: euclidean_pearson\n      value: 84.5396956658821\n    - type: euclidean_spearman\n      value: 85.43566325729779\n    - type: manhattan_pearson\n      value: 84.5665398848169\n    - type: manhattan_spearman\n      value: 85.44375870303232\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.20030208471798\n    - type: cos_sim_spearman\n      value: 87.20485505076539\n    - type: euclidean_pearson\n      value: 88.10588324368722\n    - type: euclidean_spearman\n      value: 87.20485505076539\n    - type: manhattan_pearson\n      value: 87.92324770415183\n    - type: manhattan_spearman\n      value: 87.0571314561877\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.06093161604453\n    - type: cos_sim_spearman\n      value: 64.2163140357722\n    - type: euclidean_pearson\n      value: 65.27589680994006\n    - type: euclidean_spearman\n      value: 64.2163140357722\n    - type: manhattan_pearson\n      value: 65.45904383711101\n    - type: manhattan_spearman\n      value: 64.55404716679305\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.32976164578706\n    - type: cos_sim_spearman\n      value: 85.54302197678368\n    - type: euclidean_pearson\n      value: 85.26307149193056\n    - type: euclidean_spearman\n      value: 85.54302197678368\n    - type: manhattan_pearson\n      value: 85.26647282029371\n    - type: manhattan_spearman\n      value: 85.5316135265568\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 81.44675968318754\n    - type: mrr\n      value: 94.92741826075158\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 56.34400000000001\n    - type: map_at_10\n      value: 65.927\n    - type: map_at_100\n      value: 66.431\n    - type: map_at_1000\n      value: 66.461\n    - type: map_at_3\n      value: 63.529\n    - type: map_at_5\n      value: 64.818\n    - type: mrr_at_1\n      value: 59.333000000000006\n    - type: mrr_at_10\n      value: 67.54599999999999\n    - type: mrr_at_100\n      value: 67.892\n    - type: mrr_at_1000\n      value: 67.917\n    - type: mrr_at_3\n      value: 65.778\n    - type: mrr_at_5\n      value: 66.794\n    - type: ndcg_at_1\n      value: 59.333000000000006\n    - type: ndcg_at_10\n      value: 70.5\n    - type: ndcg_at_100\n      value: 72.688\n    - type: ndcg_at_1000\n      value: 73.483\n    - type: ndcg_at_3\n      value: 66.338\n    - type: ndcg_at_5\n      value: 68.265\n    - type: precision_at_1\n      value: 59.333000000000006\n    - type: precision_at_10\n      value: 9.3\n    - type: precision_at_100\n      value: 1.053\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 25.889\n    - type: precision_at_5\n      value: 16.866999999999997\n    - type: recall_at_1\n      value: 56.34400000000001\n    - type: recall_at_10\n      value: 82.789\n    - type: recall_at_100\n      value: 92.767\n    - type: recall_at_1000\n      value: 99\n    - type: recall_at_3\n      value: 71.64399999999999\n    - type: recall_at_5\n      value: 76.322\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.75742574257426\n    - type: cos_sim_ap\n      value: 93.52081548447406\n    - type: cos_sim_f1\n      value: 87.33850129198966\n    - type: cos_sim_precision\n      value: 90.37433155080214\n    - type: cos_sim_recall\n      value: 84.5\n    - type: dot_accuracy\n      value: 99.75742574257426\n    - type: dot_ap\n      value: 93.52081548447406\n    - type: dot_f1\n      value: 87.33850129198966\n    - type: dot_precision\n      value: 90.37433155080214\n    - type: dot_recall\n      value: 84.5\n    - type: euclidean_accuracy\n      value: 99.75742574257426\n    - type: euclidean_ap\n      value: 93.52081548447406\n    - type: euclidean_f1\n      value: 87.33850129198966\n    - type: euclidean_precision\n      value: 90.37433155080214\n    - type: euclidean_recall\n      value: 84.5\n    - type: manhattan_accuracy\n      value: 99.75841584158415\n    - type: manhattan_ap\n      value: 93.4975678585854\n    - type: manhattan_f1\n      value: 87.26708074534162\n    - type: manhattan_precision\n      value: 90.45064377682404\n    - type: manhattan_recall\n      value: 84.3\n    - type: max_accuracy\n      value: 99.75841584158415\n    - type: max_ap\n      value: 93.52081548447406\n    - type: max_f1\n      value: 87.33850129198966\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 64.31437036686651\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 33.25569319007206\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 49.90474939720706\n    - type: mrr\n      value: 50.568115503777264\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 29.866828641244712\n    - type: cos_sim_spearman\n      value: 30.077555055873866\n    - type: dot_pearson\n      value: 29.866832988572266\n    - type: dot_spearman\n      value: 30.077555055873866\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.232\n    - type: map_at_10\n      value: 2.094\n    - type: map_at_100\n      value: 11.971\n    - type: map_at_1000\n      value: 28.158\n    - type: map_at_3\n      value: 0.688\n    - type: map_at_5\n      value: 1.114\n    - type: mrr_at_1\n      value: 88\n    - type: mrr_at_10\n      value: 93.4\n    - type: mrr_at_100\n      value: 93.4\n    - type: mrr_at_1000\n      value: 93.4\n    - type: mrr_at_3\n      value: 93\n    - type: mrr_at_5\n      value: 93.4\n    - type: ndcg_at_1\n      value: 84\n    - type: ndcg_at_10\n      value: 79.923\n    - type: ndcg_at_100\n      value: 61.17\n    - type: ndcg_at_1000\n      value: 53.03\n    - type: ndcg_at_3\n      value: 84.592\n    - type: ndcg_at_5\n      value: 82.821\n    - type: precision_at_1\n      value: 88\n    - type: precision_at_10\n      value: 85\n    - type: precision_at_100\n      value: 63.019999999999996\n    - type: precision_at_1000\n      value: 23.554\n    - type: precision_at_3\n      value: 89.333\n    - type: precision_at_5\n      value: 87.2\n    - type: recall_at_1\n      value: 0.232\n    - type: recall_at_10\n      value: 2.255\n    - type: recall_at_100\n      value: 14.823\n    - type: recall_at_1000\n      value: 49.456\n    - type: recall_at_3\n      value: 0.718\n    - type: recall_at_5\n      value: 1.175\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 2.547\n    - type: map_at_10\n      value: 11.375\n    - type: map_at_100\n      value: 18.194\n    - type: map_at_1000\n      value: 19.749\n    - type: map_at_3\n      value: 5.825\n    - type: map_at_5\n      value: 8.581\n    - type: mrr_at_1\n      value: 32.653\n    - type: mrr_at_10\n      value: 51.32\n    - type: mrr_at_100\n      value: 51.747\n    - type: mrr_at_1000\n      value: 51.747\n    - type: mrr_at_3\n      value: 47.278999999999996\n    - type: mrr_at_5\n      value: 48.605\n    - type: ndcg_at_1\n      value: 29.592000000000002\n    - type: ndcg_at_10\n      value: 28.151\n    - type: ndcg_at_100\n      value: 39.438\n    - type: ndcg_at_1000\n      value: 50.769\n    - type: ndcg_at_3\n      value: 30.758999999999997\n    - type: ndcg_at_5\n      value: 30.366\n    - type: precision_at_1\n      value: 32.653\n    - type: precision_at_10\n      value: 25.714\n    - type: precision_at_100\n      value: 8.041\n    - type: precision_at_1000\n      value: 1.555\n    - type: precision_at_3\n      value: 33.333\n    - type: precision_at_5\n      value: 31.837\n    - type: recall_at_1\n      value: 2.547\n    - type: recall_at_10\n      value: 18.19\n    - type: recall_at_100\n      value: 49.538\n    - type: recall_at_1000\n      value: 83.86\n    - type: recall_at_3\n      value: 7.329\n    - type: recall_at_5\n      value: 11.532\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 71.4952\n    - type: ap\n      value: 14.793362635531409\n    - type: f1\n      value: 55.204635551516915\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 61.5365025466893\n    - type: f1\n      value: 61.81742556334845\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 49.05531070301185\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 86.51725576682364\n    - type: cos_sim_ap\n      value: 75.2292304265163\n    - type: cos_sim_f1\n      value: 69.54022988505749\n    - type: cos_sim_precision\n      value: 63.65629110039457\n    - type: cos_sim_recall\n      value: 76.62269129287598\n    - type: dot_accuracy\n      value: 86.51725576682364\n    - type: dot_ap\n      value: 75.22922386081054\n    - type: dot_f1\n      value: 69.54022988505749\n    - type: dot_precision\n      value: 63.65629110039457\n    - type: dot_recall\n      value: 76.62269129287598\n    - type: euclidean_accuracy\n      value: 86.51725576682364\n    - type: euclidean_ap\n      value: 75.22925730473472\n    - type: euclidean_f1\n      value: 69.54022988505749\n    - type: euclidean_precision\n      value: 63.65629110039457\n    - type: euclidean_recall\n      value: 76.62269129287598\n    - type: manhattan_accuracy\n      value: 86.52321630804077\n    - type: manhattan_ap\n      value: 75.20608115037336\n    - type: manhattan_f1\n      value: 69.60000000000001\n    - type: manhattan_precision\n      value: 64.37219730941705\n    - type: manhattan_recall\n      value: 75.75197889182058\n    - type: max_accuracy\n      value: 86.52321630804077\n    - type: max_ap\n      value: 75.22925730473472\n    - type: max_f1\n      value: 69.60000000000001\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.34877944657896\n    - type: cos_sim_ap\n      value: 86.71257569277373\n    - type: cos_sim_f1\n      value: 79.10386355986088\n    - type: cos_sim_precision\n      value: 76.91468470434214\n    - type: cos_sim_recall\n      value: 81.4213119802895\n    - type: dot_accuracy\n      value: 89.34877944657896\n    - type: dot_ap\n      value: 86.71257133133368\n    - type: dot_f1\n      value: 79.10386355986088\n    - type: dot_precision\n      value: 76.91468470434214\n    - type: dot_recall\n      value: 81.4213119802895\n    - type: euclidean_accuracy\n      value: 89.34877944657896\n    - type: euclidean_ap\n      value: 86.71257651501476\n    - type: euclidean_f1\n      value: 79.10386355986088\n    - type: euclidean_precision\n      value: 76.91468470434214\n    - type: euclidean_recall\n      value: 81.4213119802895\n    - type: manhattan_accuracy\n      value: 89.35848177901967\n    - type: manhattan_ap\n      value: 86.69330615469126\n    - type: manhattan_f1\n      value: 79.13867741453949\n    - type: manhattan_precision\n      value: 76.78881807647741\n    - type: manhattan_recall\n      value: 81.63689559593472\n    - type: max_accuracy\n      value: 89.35848177901967\n    - type: max_ap\n      value: 86.71257651501476\n    - type: max_f1\n      value: 79.13867741453949\nlicense: apache-2.0\nlanguage:\n- en\nnew_version: nomic-ai/nomic-embed-text-v1.5\n---\n\n\n# nomic-embed-text-v1: A Reproducible Long Context (8192) Text Embedder \n\n[Blog](https://www.nomic.ai/blog/posts/nomic-embed-text-v1) | [Technical Report](https://arxiv.org/abs/2402.01613) | [AWS SageMaker](https://aws.amazon.com/marketplace/seller-profile?id=seller-tpqidcj54zawi) | [Atlas Embedding and Unstructured Data Analytics Platform](https://atlas.nomic.ai)\n\n`nomic-embed-text-v1` is 8192 context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks.\n\n# Performance Benchmarks\n\n| Name                             | SeqLen | MTEB      | LoCo     | Jina Long Context |  Open Weights | Open Training Code | Open Data   |\n| :-------------------------------:| :----- | :-------- | :------: | :---------------: | :-----------: | :----------------: | :---------- |\n| nomic-embed-text-v1              | 8192   | **62.39** |**85.53** | 54.16             | ‚úÖ            | ‚úÖ                  | ‚úÖ          |\n| jina-embeddings-v2-base-en       | 8192   | 60.39     | 85.45    | 51.90             | ‚úÖ            | ‚ùå                  | ‚ùå          |\n| text-embedding-3-small           | 8191   | 62.26     | 82.40    | **58.20**         | ‚ùå            | ‚ùå                  | ‚ùå          |\n| text-embedding-ada-002           | 8191   | 60.99     | 52.7     | 55.25             | ‚ùå            | ‚ùå                  | ‚ùå          |\n\n\n**Exciting Update!**: `nomic-embed-text-v1` is now multimodal! [nomic-embed-vision-v1](https://huggingface.co/nomic-ai/nomic-embed-vision-v1) is aligned to the embedding space of `nomic-embed-text-v1`, meaning any text embedding is multimodal!\n\n## Usage\n\n**Important**: the text prompt *must* include a *task instruction prefix*, instructing the model which task is being performed. \n\nFor example, if you are implementing a RAG application, you embed your documents as `search_document: <text here>` and embed your user queries as `search_query: <text here>`.\n\n## Task instruction prefixes\n\n### `search_document`\n\n#### Purpose: embed texts as documents from a dataset\n\nThis prefix is used for embedding texts as documents, for example as documents for a RAG index.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `search_query`\n\n#### Purpose: embed texts as questions to answer\n\nThis prefix is used for embedding texts as questions that documents from a dataset could resolve, for example as queries to be answered by a RAG application.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''search_query: Who is Laurens van Der Maaten?'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `clustering`\n\n#### Purpose: embed texts to group them into clusters\n\nThis prefix is used for embedding texts in order to group them into clusters, discover common topics, or remove semantic duplicates.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''clustering: the quick brown fox'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `classification`\n\n#### Purpose: embed texts to classify them\n\nThis prefix is used for embedding texts into vectors that will be used as features for a classification model\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''classification: the quick brown fox'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### Sentence Transformers\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### Transformers\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\nsentences = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?'']\n\ntokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'')\nmodel = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1'', trust_remote_code=True)\nmodel.eval()\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\nembeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n```\n\nThe model natively supports scaling of the sequence length past 2048 tokens. To do so, \n\n```diff\n- tokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'')\n+ tokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'', model_max_length=8192)\n\n\n- model = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1'', trust_remote_code=True)\n+ model = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1'', trust_remote_code=True, rotary_scaling_factor=2)\n```\n\n### Transformers.js\n\n```js\nimport { pipeline } from ''@xenova/transformers'';\n\n// Create a feature extraction pipeline\nconst extractor = await pipeline(''feature-extraction'', ''nomic-ai/nomic-embed-text-v1'', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Compute sentence embeddings\nconst texts = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?''];\nconst embeddings = await extractor(texts, { pooling: ''mean'', normalize: true });\nconsole.log(embeddings);\n```\n\n## Nomic API\n\nThe easiest way to get started with Nomic Embed is through the Nomic Embedding API.\n\nGenerating embeddings with the `nomic` Python client is as easy as \n\n```python\nfrom nomic import embed\n\noutput = embed.text(\n    texts=[''Nomic Embedding API'', ''#keepAIOpen''],\n    model=''nomic-embed-text-v1'',\n    task_type=''search_document''\n)\n\nprint(output)\n```\n\nFor more information, see the [API reference](https://docs.nomic.ai/reference/endpoints/nomic-embed-text)\n\n\n## Training\nClick the Nomic Atlas map below to visualize a 5M sample of our contrastive pretraining data!\n\n[![image/webp](https://cdn-uploads.huggingface.co/production/uploads/607997c83a565c15675055b3/pjhJhuNyRfPagRd_c_iUz.webp)](https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample)\n\nWe train our embedder using a multi-stage training pipeline. Starting from a long-context [BERT model](https://huggingface.co/nomic-ai/nomic-bert-2048),\nthe first unsupervised contrastive stage trains on a dataset generated from weakly related text pairs, such as question-answer pairs from forums like StackExchange and Quora, title-body pairs from Amazon reviews, and summarizations from news articles.\n\nIn the second finetuning stage, higher quality labeled datasets such as search queries and answers from web searches are leveraged. Data curation and hard-example mining is crucial in this stage.\n\nFor more details, see the Nomic Embed [Technical Report](https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf) and corresponding [blog post](https://blog.nomic.ai/posts/nomic-embed-text-v1).\n\nTraining data to train the models is released in its entirety. For more details, see the `contrastors` [repository](https://github.com/nomic-ai/contrastors)\n\n\n# Join the Nomic Community\n\n- Nomic: [https://nomic.ai](https://nomic.ai)\n- Discord: [https://discord.gg/myY5YDR8z8](https://discord.gg/myY5YDR8z8)\n- Twitter: [https://twitter.com/nomic_ai](https://twitter.com/nomic_ai)\n\n\n# Citation\n\nIf you find the model, dataset, or training code useful, please cite our work\n\n```bibtex\n@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, \n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":136731648,"storage_bytes":3202383417,"files_count":15,"spaces_count":94,"gated":false,"private":false,"config":{"architectures":["NomicBertModel"],"auto_map":{"AutoConfig":"nomic-ai/nomic-bert-2048--configuration_hf_nomic_bert.NomicBertConfig","AutoModel":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertModel","AutoModelForMaskedLM":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForPreTraining"},"model_type":"nomic_bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:nomic-ai:contrastors","source_url":"https://github.com/nomic-ai/contrastors"},{"type":"based_on_paper","target_id":"arxiv:2402.01613","source_url":"https://arxiv.org/abs/2402.01613"}]', NULL, 'Apache-2.0', 'approved', 77.4, 'e5be98c272c8fe73dcd2bd8d9c206bef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.2-Speciale', 'huggingface--deepseek-ai--deepseek-v3.2-speciale', 'DeepSeek-V3.2-Speciale', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" targ...', '["transformers","safetensors","deepseek_v32","text-generation","base_model:deepseek-ai/deepseek-v3.2-exp-base","license:mit","endpoints_compatible","fp8","region:us"]', 'text-generation', 544, 8016, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<p align="center">\n  <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"><b>Technical Report</b>üëÅÔ∏è</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* ü•á **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align="center">\n <img src="assets/benchmark.png" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a "thinking with tools" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model''s text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.2")\n\nmessages = [\n    {"role": "user", "content": "hello"},\n    {"role": "assistant", "content": "Hello! I am DeepSeek.", "reasoning_content": "thinking..."},\n    {"role": "user", "content": "1+1=?"}\n]\nencode_config = dict(thinking_mode="thinking", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>hello<ÔΩúAssistantÔΩú></think>Hello! I am DeepSeek.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output that the model might occasionally generate. It is not suitable for production use without robust error handling.\n3. A new role named `developer` has been introduced in the chat template. This role is dedicated exclusively to search agent scenarios and is designated for no other tasks. The official API does not accept messages assigned to `developer`.\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.2 and DeepSeek-V3.2-Speciale are the same as DeepSeek-V3.2-Exp. Please visit [DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) repo for more information about running this model locally.\n\nUsage Recommendations:\n\n1. For local deployment, we recommend setting the sampling parameters to `temperature = 1.0, top_p = 0.95`.\n2. Please note that the DeepSeek-V3.2-Speciale variant is designed exclusively for deep reasoning tasks and does not support the tool-calling functionality.\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2025deepseekv32,\n      title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689484423011,"files_count":192,"spaces_count":0,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.2-Exp","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp"}]', NULL, 'MIT', 'approved', 82.4, '3bcfd457e471fb9bcbb42b9df7e79361', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale/resolve/main/assets/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3.2-Speciale from https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale/resolve/main/assets/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.2-Speciale.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-DGSpitzer-Cyberpunk-Anime-Diffusion', 'huggingface--dgspitzer--cyberpunk-anime-diffusion', 'Cyberpunk-Anime-Diffusion', 'DGSpitzer', '--- language: - en thumbnail: "https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/thumbnail.png" tags: - cyberpunk - anime - waifu-diffusion - stable-diffusion - aiart - text-to-image license: creativeml-openrail-m --- <center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/5.jpg" width="512" height="512"/></center> !visitors An AI model that generates cyberpunk anime characters!~ Based of a finetuned Waifu Diffusion V1.3 Model wi...', '["diffusers","safetensors","cyberpunk","anime","waifu-diffusion","stable-diffusion","aiart","text-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 543, 422, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n  - en\nthumbnail: "https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/thumbnail.png"\ntags:\n- cyberpunk\n- anime\n- waifu-diffusion\n- stable-diffusion\n- aiart\n- text-to-image\nlicense: creativeml-openrail-m\n---\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/5.jpg" width="512" height="512"/></center>\n\n![visitors](https://visitor-badge.glitch.me/badge?page_id=Cyberpunk_Anime_Diffusion)\n\n# Cyberpunk Anime Diffusion\n\nAn AI model that generates cyberpunk anime characters!~\n\nBased of a finetuned Waifu Diffusion V1.3 Model with Stable Diffusion V1.5 New Vae, training in Dreambooth\n\nby [DGSpitzer](https://www.youtube.com/channel/UCzzsYBF4qwtMwJaPJZ5SuPg)\n\n### üß® Diffusers\n\nThis repo contains both .ckpt and Diffuser model files. It''s compatible to be used as any Stable Diffusion model, using standard [Stable Diffusion Pipelines](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can convert this model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX](https://huggingface.co/blog/stable_diffusion_jax).\n\n```python example for loading the Diffuser\n#!pip install diffusers transformers scipy torch\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "DGSpitzer/Cyberpunk-Anime-Diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a beautiful perfect face girl in dgs illustration style, Anime fine details portrait of school girl in front of modern tokyo city landscape on the background deep bokeh, anime masterpiece, 8k, sharp high quality anime"\nimage = pipe(prompt).images[0]\n\nimage.save("./cyberpunk_girl.png")\n```\n\n# Online Demo\n\nYou can try the Online Web UI demo build with [Gradio](https://github.com/gradio-app/gradio), or use Colab Notebook at here:\n\n*My Online Space Demo*\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/DGSpitzer/DGS-Diffusion-Space)\n\n*Finetuned Diffusion WebUI Demo by anzorq*\n[![Use Finetuned_Diffusion WebUI](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n\n*Colab Notebook*\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HelixNGC7293/cyberpunk-anime-diffusion/blob/main/cyberpunk_anime_diffusion.ipynb)[![GitHub](https://badgen.net/badge/icon/Github?icon=github&label)](https://github.com/HelixNGC7293/cyberpunk-anime-diffusion)\n\n*Buy me a coffee if you like this project ;P ‚ô•*\n[![Buy me a coffee](https://badgen.net/badge/icon/Buy%20Me%20A%20Coffee?icon=buymeacoffee&label)](https://www.buymeacoffee.com/dgspitzer)\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/1.jpg" width="512" height="512"/></center>\n\n# **üëáModelüëá**\n\nAI Model Weights available at huggingface: https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/2.jpg" width="512" height="512"/></center>\n\n# Usage\n\nAfter model loaded, use keyword **dgs** in your prompt, with **illustration style** to get even better results.\n\nFor sampler, use **Euler A** for the best result (**DDIM** kinda works too), CFG Scale 7, steps 20 should be fine\n\n**Example 1:**\n\n```\nportrait of a girl in dgs illustration style, Anime girl, female soldier working in a cyberpunk city, cleavage, ((perfect femine face)), intricate, 8k, highly detailed, shy, digital painting, intense, sharp focus\n```\n\nFor cyber robot male character, you can add **muscular male** to improve the output.\n\n**Example 2:**\n\n```\na photo of muscular beard soldier male in dgs illustration style, half-body, holding robot arms, strong chest\n```\n\n**Example 3 (with Stable Diffusion WebUI):**\n\nIf using [AUTOMATIC1111''s Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\nYou can simply use this as **prompt** with **Euler A** Sampler,  CFG Scale 7, steps 20, 704 x 704px output res:\n\n```\nan anime girl in dgs illustration style\n```\n\nAnd set the **negative prompt** as this to get cleaner face: \n\n```\nout of focus, scary, creepy, evil, disfigured, missing limbs, ugly, gross, missing fingers\n```\n\nThis will give you the exactly same style as the sample images above.\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/ReadmeAddon.jpg" width="256" height="353"/></center>\n\n---\n\n**NOTE: usage of this model implies accpetance of stable diffusion''s [CreativeML Open RAIL-M license](LICENSE)**\n\n---\n\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/4.jpg" width="700" height="700"/></center>\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/6.jpg" width="700" height="700"/></center>\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":24647194668,"files_count":40,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:HelixNGC7293:cyberpunk-anime-diffusion","source_url":"https://github.com/HelixNGC7293/cyberpunk-anime-diffusion"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"}]', NULL, 'creativeml-openrail-m', 'approved', 62.4, '434ec78193e489221016a2594ef58079', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-180B-chat', 'huggingface--tiiuae--falcon-180b-chat', 'falcon-180B-chat', 'tiiuae', '', '["transformers","safetensors","falcon","text-generation","conversational","en","de","es","fr","dataset:tiiuae/falcon-refinedweb","arxiv:1911.02150","arxiv:2005.14165","arxiv:2104.09864","arxiv:2205.14135","arxiv:2306.01116","license:unknown","text-generation-inference","region:us"]', 'text-generation', 543, 78, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-180B-chat","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":179522565120,"storage_bytes":359045204272,"files_count":91,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["FalconForCausalLM"],"model_type":"falcon","tokenizer_config":{"chat_template":"{% for message in messages %}{% if not loop.first %}{{ ''\n'' }}{% endif %}{% if message[''role''] == ''system'' %}{{ ''System: '' }}{% elif message[''role''] == ''user'' %}{{ ''User: '' }}{% elif message[''role''] == ''assistant'' %}{{ ''Falcon: '' }}{% endif %}{{ message[''content''] }}{% endfor %}{% if add_generation_prompt %}{{ ''\n'' + ''Falcon:'' }}{% endif %}","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'unknown', 'approved', 37.4, '4a5ff4875a1fccd10a20e4edbd2034d8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Helsinki-NLP-opus-mt-zh-en', 'huggingface--helsinki-nlp--opus-mt-zh-en', 'opus-mt-zh-en', 'Helsinki-NLP', '--- language: - zh - en tags: - translation license: cc-by-4.0 --- - Model Details - Uses - Risks, Limitations and Biases - Training - Evaluation - Citation Information - How to Get Started With the Model - **Model Description:** - **Developed by:** Language Technology Research Group at the University of Helsinki - **Model Type:** Translation - **Language(s):** - Source Language: Chinese - Target Language: English - **License:** CC-BY-4.0 - **Resources for more information:** - GitHub Repo Th...', '["transformers","pytorch","tf","rust","marian","text2text-generation","translation","zh","en","license:cc-by-4.0","endpoints_compatible","deploy:azure","region:us"]', 'translation', 540, 389824, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Helsinki-NLP/opus-mt-zh-en","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n- zh\n- en\n\ntags:\n- translation\n\nlicense: cc-by-4.0\n---\n\n### zho-eng\n\n## Table of Contents\n- [Model Details](#model-details)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Citation Information](#citation-information)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n\n## Model Details\n- **Model Description:**\n- **Developed by:** Language Technology Research Group at the University of Helsinki\n- **Model Type:** Translation\n- **Language(s):**  \n  - Source Language:  Chinese\n  - Target Language: English\n- **License:** CC-BY-4.0\n- **Resources for more information:**\n  - [GitHub Repo](https://github.com/Helsinki-NLP/OPUS-MT-train)\n\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for translation and text-to-text generation.\n\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n\nFurther details about the dataset for this model can be found in the OPUS readme: [zho-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/zho-eng/README.md)\n\n## Training\n\n#### System Information \n* helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n* transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n* port_machine: brutasse\n* port_time: 2020-08-21-14:41\n* src_multilingual: False\n* tgt_multilingual: False\n\n#### Training Data\n##### Preprocessing\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* ref_len: 82826.0\n* dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)\n* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.zip)\n\n* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.test.txt)\n\n\n## Evaluation\n\n#### Results\n\n* test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.eval.txt)\n\n* brevity_penalty: 0.948\n\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| Tatoeba-test.zho.eng 	| 36.1 	| 0.548 |\n\n## Citation Information\n\n```bibtex\n@InProceedings{TiedemannThottingal:EAMT2020,\n  author = {J{\"o}rg Tiedemann and Santhosh Thottingal},\n  title = {{OPUS-MT} ‚Äî {B}uilding open translation services for the {W}orld},\n  booktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},\n  year = {2020},\n  address = {Lisbon, Portugal}\n }\n```\n\n## How to Get Started With the Model\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-zh-en")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-zh-en")\n```\n\n\n', '{"pipeline_tag":"translation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3577746362,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MarianMTModel"],"model_type":"marian","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:Helsinki-NLP:OPUS-MT-train","source_url":"https://github.com/Helsinki-NLP/OPUS-MT-train"},{"type":"has_code","target_id":"github:Helsinki-NLP:Tatoeba-Challenge","source_url":"https://github.com/Helsinki-NLP/Tatoeba-Challenge"},{"type":"has_code","target_id":"github:Helsinki-NLP:Opus-MT","source_url":"https://github.com/Helsinki-NLP/Opus-MT"}]', NULL, 'CC-BY-4.0', 'approved', 62.3, '0e1b9cd6566525966d74a72874b8c384', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-70b', 'huggingface--meta-llama--llama-2-70b', 'Llama-2-70b', 'meta-llama', '', '["facebook","meta","pytorch","llama","llama-2","text-generation","en","arxiv:2307.09288","license:llama2","region:us"]', 'text-generation', 538, 48, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-70b","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":null,"framework":null,"params":null,"storage_bytes":137975402906,"files_count":17,"spaces_count":35,"gated":"manual","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 37.3, '59fdeeb3667827dc54651903f03b3000', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-togethercomputer-LLaMA-2-7B-32K', 'huggingface--togethercomputer--llama-2-7b-32k', 'LLaMA-2-7B-32K', 'togethercomputer', '--- license: llama2 datasets: - togethercomputer/RedPajama-Data-1T - togethercomputer/RedPajama-Data-Instruct - EleutherAI/pile - togethercomputer/Long-Data-Collections language: - en library_name: transformers --- LLaMA-2-7B-32K is an open-source, long context language model developed by Together, fine-tuned from Meta''s original Llama-2 7B model. This model represents our efforts to contribute to the rapid progress of the open-source ecosystem for large language models. The model has been ex...', '["transformers","pytorch","llama","text-generation","en","dataset:togethercomputer/redpajama-data-1t","dataset:togethercomputer/redpajama-data-instruct","dataset:eleutherai/pile","dataset:togethercomputer/long-data-collections","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 538, 1556, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/togethercomputer/LLaMA-2-7B-32K","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: llama2\ndatasets:\n- togethercomputer/RedPajama-Data-1T\n- togethercomputer/RedPajama-Data-Instruct\n- EleutherAI/pile\n- togethercomputer/Long-Data-Collections\nlanguage:\n- en\nlibrary_name: transformers\n---\n\n# LLaMA-2-7B-32K\n\n## Model Description\n\nLLaMA-2-7B-32K is an open-source, long context language model developed by Together, fine-tuned from Meta''s original Llama-2 7B model. \nThis model represents our efforts to contribute to the rapid progress of the open-source ecosystem for large language models. \nThe model has been extended to a context length of 32K with position interpolation, \nallowing applications on multi-document QA, long text summarization, etc.\n\n## What''s new?\n\nThis model introduces several improvements and new features:\n\n1. **Extended Context:** The model has been trained to handle context lengths up to 32K, which is a significant improvement over the previous versions.\n\n2. **Pre-training and Instruction Tuning:** We have shared our data recipe, which consists of a mixture of pre-training and instruction tuning data.\n\n3. **Fine-tuning Examples:** We provide examples of how to fine-tune the model for specific applications, including book summarization and long context question and answering.\n\n4. **Software Support:** We have updated both the inference and training stack to allow efficient inference and fine-tuning for 32K context.\n\n## Model Architecture\n\nThe model follows the architecture of Llama-2-7B and extends it to handle a longer context. It leverages the recently released FlashAttention-2 and a range of other optimizations to improve the speed and efficiency of inference and training.\n\n## Training and Fine-tuning\n\nThe model has been trained using a mixture of pre-training and instruction tuning data. \n- In the first training phase of continued pre-training, our data mixture contains 25% RedPajama Book, 25% RedPajama ArXiv (including abstracts), 25% other data from RedPajama, and 25% from the UL2 Oscar Data, which is a part of OIG (Open-Instruction-Generalist), asking the model to fill in missing chunks, or complete the text. \nTo enhance the long-context ability, we exclude data shorter than 2K word. The inclusion of UL2 Oscar Data is effective in compelling the model to read and utilize long-range context.\n- We then fine-tune the model to focus on its few shot capacity under long context, including 20% Natural Instructions (NI), 20% Public Pool of Prompts (P3), 20% the Pile. We decontaminated all data against HELM core scenarios . We teach the model to leverage the in-context examples by packing examples into one 32K-token sequence. To maintain the knowledge learned from the first piece of data, we incorporate 20% RedPajama-Data Book and 20% RedPajama-Data ArXiv.\n\nNext, we provide examples of how to fine-tune the model for specific applications. \nThe example datasets are placed in [togethercomputer/Long-Data-Collections](https://huggingface.co/datasets/togethercomputer/Long-Data-Collections)\nYou can use the [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) to fine-tune your own 32K model over LLaMA-2-7B-32K.\nPlease refer to [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) for step-by-step illustrations.\n\n1. Long Context QA.\n\n   We take as an example the multi-document question answering task from the paper ‚ÄúLost in the Middle: How Language Models Use Long Contexts‚Äù. The input for the model consists of (i) a question that requires an answer and (ii) k documents, which are passages extracted from Wikipedia. Notably, only one of these documents contains the answer to the question, while the remaining k ‚àí 1 documents, termed as "distractor" documents, do not. To successfully perform this task, the model must identify and utilize the document containing the answer from its input context. \n\n   With OCK, simply run the following command to fine-tune:\n   ```\n   bash training/finetune_llama-2-7b-32k-mqa.sh\n   ```\n\n2. Summarization.\n\n   Another example is BookSum, a unique dataset designed to address the challenges of long-form narrative summarization. This dataset features source documents from the literature domain, including novels, plays, and stories, and offers human-written, highly abstractive summaries. We here focus on chapter-level data.  BookSum poses a unique set of challenges, necessitating that the model comprehensively read through each chapter.\n\n   With OCK, simply run the following command to fine-tune:\n   ```\n   bash training/finetune_llama-2-7b-32k-booksum.sh\n   ```\n\n\n## Inference\n\nYou can use the [Together API](https://together.ai/blog/api-announcement) to try out LLaMA-2-7B-32K for inference. \nThe updated inference stack allows for efficient inference.\n\nTo run the model locally, we strongly recommend to install Flash Attention V2, which is necessary to obtain the best performance:\n```\n# Please update the path of `CUDA_HOME`\nexport CUDA_HOME=/usr/local/cuda-11.8\npip install transformers==4.31.0\npip install sentencepiece\npip install ninja\npip install flash-attn --no-build-isolation\npip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n```\n\nYou can use this model directly from the Hugging Face Model Hub or fine-tune it on your own data using the OpenChatKit.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("togethercomputer/LLaMA-2-7B-32K")\nmodel = AutoModelForCausalLM.from_pretrained("togethercomputer/LLaMA-2-7B-32K", trust_remote_code=True, torch_dtype=torch.float16)\n\ninput_context = "Your text here"\ninput_ids = tokenizer.encode(input_context, return_tensors="pt")\noutput = model.generate(input_ids, max_length=128, temperature=0.7)\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(output_text)\n```\n\nAlternatively, you can set `trust_remote_code=False` if you prefer not to use flash attention.\n\n\n## Limitations and Bias\n\nAs with all language models, LLaMA-2-7B-32K may generate incorrect or biased content. It''s important to keep this in mind when using the model.\n\n## Community\n\nJoin us on [Together Discord](https://discord.gg/6ZVDU8tTD4)', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":53908211130,"files_count":13,"spaces_count":82,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"<unk>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:togethercomputer:OpenChatKit","source_url":"https://github.com/togethercomputer/OpenChatKit"},{"type":"has_code","target_id":"github:togethercomputer:OpenChatKit","source_url":"https://github.com/togethercomputer/OpenChatKit"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention.git","source_url":"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc"}]', NULL, 'LLaMA-2', 'approved', 62.3, 'a2c3da9226f9e227fb150c7a45a9ce80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-strangerzonehf-Flux-Midjourney-Mix2-LoRA', 'huggingface--strangerzonehf--flux-midjourney-mix2-lora', 'Flux-Midjourney-Mix2-LoRA', 'strangerzonehf', '--- tags: - text-to-image - lora - diffusers - template:diffusion-lora widget: - text: ''MJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw'' output: url: images/1.png - text: ''MJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. This scene evokes a sense of mystery or contemplation, highlighting the beaut...', '["diffusers","text-to-image","lora","template:diffusion-lora","base_model:black-forest-labs/flux.1-dev","base_model:adapter:black-forest-labs/flux.1-dev","license:other","region:us"]', 'text-to-image', 538, 4788, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/strangerzonehf/Flux-Midjourney-Mix2-LoRA","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- text-to-image\n- lora\n- diffusers\n- template:diffusion-lora\nwidget:\n- text: ''MJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw''\n  output:\n    url: images/1.png\n- text: ''MJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. This scene evokes a sense of mystery or contemplation, highlighting the beauty of the subjects features against the contrasting backdrop, lens glossy effect, high contrast, star bokeh ''\n  output:\n    url: images/2.png\n- text: ''MJ v6, A photo of an attractive man in his thirties, wearing a black coat and yellow scarf with a brown pattern inside a building talking on a phone standing near a modern glass skyscraper in London, shot from below looking up at him in the style of street photography, cinematic.  --ar 85:128 --v 6.0 --style raw''\n  output:\n    url: images/3.png\n- text: ''MJ v6, banana bread with chocolate chips and pecans, in the style of tabletop photography, y2k aesthetic, spiky mounds, flawless line work, schlieren photography, 8k, natural fibers, minimal  --ar 123:185 --v 5 ''\n  output:\n    url: images/4.png\n- text: ''MJ v6, A portrait of Woman, fashion photography, big shapes in the background, on top of colorful squares with stars, in the style of retro vintage photography, pastel colors, soft purple and yellow ''\n  output:\n    url: images/6.png\n- text: ''MJ v6, delicious dipped chocolate pastry japo gallery, white background, in the style of dark brown, close-up intensity, duckcore, rounded, high resolution --ar 2:3 --v 5''\n  output:\n    url: images/5.png\nbase_model: black-forest-labs/FLUX.1-dev\ninstance_prompt: MJ v6\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\n---\n![MJv6](images/mjv6.png)\n\n<Gallery />\n\n## Model description for MJv6 Mix2 LoRA\n\n[ Best for Realism, Modeling, Demonstration, Close-Up Shots ]\n\nImage Processing Parameters \n\n| Parameter                 | Value  | Parameter                 | Value  |\n|---------------------------|--------|---------------------------|--------|\n| LR Scheduler              | constant | Noise Offset              | 0.03   |\n| Optimizer                 | AdamW8 | Multires Noise Discount   | 0.1    |\n| Network Dim               | 64     | Multires Noise Iterations | 10     |\n| Network Alpha             | 32     | Repeat & Steps           | 25 & 3660 |\n| Epoch                     | 28   | Save Every N Epochs       | 1     |\n\n    Labeling: florence2-en(natural language & English)\n    \n    Total Images Used for Training : 36\n\n## Best Dimensions\n\n- 768 x 1024 (Best)\n- 1024 x 1024 (Default)\n    \n## Setting Up\n```python\nimport torch\nfrom pipelines import DiffusionPipeline\n\nbase_model = "black-forest-labs/FLUX.1-dev"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\n\nlora_repo = "strangerzonehf/Flux-Midjourney-Mix2-LoRA"\ntrigger_word = "MJ v6"  \npipe.load_lora_weights(lora_repo)\n\ndevice = torch.device("cuda")\npipe.to(device)\n```\n## Trigger words\n\nYou should use `MJ v6` to trigger the image generation.\n\n## Sample Prompts for MJv6 Mix2 LoRA\n\n| **Prompt**                                                                                                                                                               | **Description**                                                                                                                                                                                                                  |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw | A portrait of a woman in a red dress, photographed in the street photography style with a dark green background, capturing the raw and natural aesthetics of Unsplash-style imagery.                                             |\n| MJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. Lens glossy effect, high contrast, star bokeh | A mysterious and contemplative bird portrait illuminated by yellow light with a blue gradient background. Features include high contrast and a star bokeh effect to enhance the atmosphere.                                      |\n| MJ v6, banana bread with chocolate chips and pecans, in the style of tabletop photography, y2k aesthetic, spiky mounds, flawless line work, schlieren photography, 8k, natural fibers, minimal  --ar 123:185 --v 5 | A close-up image of banana bread with chocolate chips and pecans, styled with a Y2K aesthetic. The photography emphasizes texture, line work, and high resolution, with natural materials enhancing the minimalistic approach.   |\n| MJ v6, delicious dipped chocolate pastry japo gallery, white background, in the style of dark brown, close-up intensity, duckcore, rounded, high resolution --ar 2:3 --v 5 | A close-up of a chocolate-dipped pastry on a white background, featuring a rich brown color palette and soft, rounded forms. High-resolution imagery enhances the details and texture of the subject.                             |\n| MJ v6, A portrait of Woman, fashion photography, big shapes in the background, on top of colorful squares with stars, in the style of retro vintage photography, pastel colors, soft purple and yellow | A retro-vintage style portrait of a woman with a whimsical background of large shapes and colorful squares with stars. The pastel tones of purple and yellow create a soft and nostalgic mood.                                    |\n| MJ v6, Captured at eye-level, a close-up shot of a young woman with long dark brown hair, wearing a green bikini top adorned with yellow and orange flowers. The woman''s body is partially submerged in a body of water, her eyes are slightly open. The background is blurred, with a stone wall visible behind her. The sun is shining on the right side of the image, casting a shadow on the wall. | A vibrant and summery close-up of a young woman partially submerged in water, wearing a floral green bikini top. The image captures natural lighting, with the background blurred to enhance the subject''s focus.                  |\n| MJ v6, a woman with long dark brown hair stands in front of a stark white wall. She is dressed in a sleeveless black and white dress, adorned with a checkered pattern. Her eyes are a deep blue, and her lips are pursed. Her hair cascades over her shoulders, adding a touch of warmth to her face. The lighting is subdued, creating a stark contrast to the woman''s outfit. | A stark, minimalist portrait of a woman in a checkered dress. The subdued lighting and simple white background emphasize her expressive features and contrast with her bold outfit.                                                  |\n| MJ v6, a beautiful young woman with long brown hair is seated in a field of lavender flowers. She is dressed in a cream-colored bra with a red belt tied around her waist. Her bra is tied in a knot at the center of her chest. Her eyes are closed and her lips are pursed. Her hair is pulled back in a ponytail, adding a pop of color to her face. The backdrop is a lush green hillside. | A serene and dreamy image of a woman in a lavender field. The cream-colored attire and red accents create a harmonious blend with the lush green and vibrant purple surroundings.                                                    |\n\n## Download model\n\nWeights for this model are available in Safetensors format.\n\n[Download](/strangerzonehf/Flux-Midjourney-Mix2-LoRA/tree/main) them in the Files & versions tab.\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":867491996,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 62.3, '02a0271206fa5ad1af3ec8ebaa2ad9c8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-FacebookAI-roberta-base', 'huggingface--facebookai--roberta-base', 'roberta-base', 'FacebookAI', '--- language: en tags: - exbert license: mit datasets: - bookcorpus - wikipedia --- Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model is case-sensitive: it makes a difference between english and English. Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team. RoBERTa is a transformers model...', '["transformers","pytorch","tf","jax","rust","safetensors","roberta","fill-mask","exbert","en","dataset:bookcorpus","dataset:wikipedia","arxiv:1907.11692","arxiv:1806.02847","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 534, 9644639, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/FacebookAI/roberta-base","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it''s mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''roberta-base'')\n>>> unmasker("Hello I''m a <mask> model.")\n\n[{''sequence'': "<s>Hello I''m a male model.</s>",\n  ''score'': 0.3306540250778198,\n  ''token'': 2943,\n  ''token_str'': ''ƒ†male''},\n {''sequence'': "<s>Hello I''m a female model.</s>",\n  ''score'': 0.04655390977859497,\n  ''token'': 2182,\n  ''token_str'': ''ƒ†female''},\n {''sequence'': "<s>Hello I''m a professional model.</s>",\n  ''score'': 0.04232972860336304,\n  ''token'': 2038,\n  ''token_str'': ''ƒ†professional''},\n {''sequence'': "<s>Hello I''m a fashion model.</s>",\n  ''score'': 0.037216778844594955,\n  ''token'': 2734,\n  ''token_str'': ''ƒ†fashion''},\n {''sequence'': "<s>Hello I''m a Russian model.</s>",\n  ''score'': 0.03253649175167084,\n  ''token'': 1083,\n  ''token_str'': ''ƒ†Russian''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(''roberta-base'')\nmodel = RobertaModel.from_pretrained(''roberta-base'')\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(''roberta-base'')\nmodel = TFRobertaModel.from_pretrained(''roberta-base'')\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''roberta-base'')\n>>> unmasker("The man worked as a <mask>.")\n\n[{''sequence'': ''<s>The man worked as a mechanic.</s>'',\n  ''score'': 0.08702439814805984,\n  ''token'': 25682,\n  ''token_str'': ''ƒ†mechanic''},\n {''sequence'': ''<s>The man worked as a waiter.</s>'',\n  ''score'': 0.0819653645157814,\n  ''token'': 38233,\n  ''token_str'': ''ƒ†waiter''},\n {''sequence'': ''<s>The man worked as a butcher.</s>'',\n  ''score'': 0.073323555290699,\n  ''token'': 32364,\n  ''token_str'': ''ƒ†butcher''},\n {''sequence'': ''<s>The man worked as a miner.</s>'',\n  ''score'': 0.046322137117385864,\n  ''token'': 18678,\n  ''token_str'': ''ƒ†miner''},\n {''sequence'': ''<s>The man worked as a guard.</s>'',\n  ''score'': 0.040150221437215805,\n  ''token'': 2510,\n  ''token_str'': ''ƒ†guard''}]\n\n>>> unmasker("The Black woman worked as a <mask>.")\n\n[{''sequence'': ''<s>The Black woman worked as a waitress.</s>'',\n  ''score'': 0.22177888453006744,\n  ''token'': 35698,\n  ''token_str'': ''ƒ†waitress''},\n {''sequence'': ''<s>The Black woman worked as a prostitute.</s>'',\n  ''score'': 0.19288744032382965,\n  ''token'': 36289,\n  ''token_str'': ''ƒ†prostitute''},\n {''sequence'': ''<s>The Black woman worked as a maid.</s>'',\n  ''score'': 0.06498628109693527,\n  ''token'': 29754,\n  ''token_str'': ''ƒ†maid''},\n {''sequence'': ''<s>The Black woman worked as a secretary.</s>'',\n  ''score'': 0.05375480651855469,\n  ''token'': 2971,\n  ''token_str'': ''ƒ†secretary''},\n {''sequence'': ''<s>The Black woman worked as a nurse.</s>'',\n  ''score'': 0.05245552211999893,\n  ''token'': 9008,\n  ''token_str'': ''ƒ†nurse''}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\) and\n\\(\epsilon = 1e-6\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=roberta-base">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":124697947,"storage_bytes":3467440206,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["RobertaForMaskedLM"],"model_type":"roberta","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:jcpeterson:openwebtext","source_url":"https://github.com/jcpeterson/openwebtext"},{"type":"based_on_paper","target_id":"arxiv:1907.11692","source_url":"https://arxiv.org/abs/1907.11692"},{"type":"based_on_paper","target_id":"arxiv:1806.02847","source_url":"https://arxiv.org/abs/1806.02847"}]', NULL, 'MIT', 'approved', 62.3, '2fe9dcbcc772d134bdfff0719e32da3a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-XLabs-AI-flux-controlnet-collections', 'huggingface--xlabs-ai--flux-controlnet-collections', 'flux-controlnet-collections', 'XLabs-AI', '--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE. language: - en pipeline_tag: text-to-image tags: - Stable Diffusion - image-generation - Flux - diffusers --- !Controlnet collections for Flux <img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true"> This repository provides a collection of ControlNet checkpoints for FLUX.1-dev model by Bl...', '["diffusers","stable diffusion","image-generation","flux","text-to-image","en","license:other","region:us"]', 'text-to-image', 534, 9096, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/XLabs-AI/flux-controlnet-collections","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.\nlanguage:\n- en\npipeline_tag: text-to-image\ntags:\n- Stable Diffusion\n- image-generation\n- Flux\n- diffusers\n---\n\n![Controlnet collections for Flux](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/flux-controlnet-collections.png?raw=true)\n[<img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true">](https://discord.gg/FHY2guThfy)\n\nThis repository provides a collection of ControlNet checkpoints for\n[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) by Black Forest Labs\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/depth_result1.png?raw=true)\n\n[See our github](https://github.com/XLabs-AI/x-flux-comfyui) for comfy ui workflows.\n![Example Picture 1](https://github.com/XLabs-AI/x-flux-comfyui/blob/main/assets/image1.png?raw=true)\n\n[See our github](https://github.com/XLabs-AI/x-flux) for train script, train configs and demo script for inference.\n\n# Models\n\nOur collection supports 3 models:\n- Canny\n- HED\n- Depth (Midas)\n\nEach ControlNet is trained on 1024x1024 resolution and works for 1024x1024 resolution.\nWe release **v3 versions** - better and realistic versions, which can be used directly in ComfyUI!   \n\nPlease, see our [ComfyUI custom nodes installation guide](https://github.com/XLabs-AI/x-flux-comfyui)\n\n\n# Examples\n\nSee examples of our models results below.  \nAlso, some generation results with input images are provided in "Files and versions"\n\n# Inference\n\nTo try our models, you have 2 options:\n1. Use main.py from our [official repo](https://github.com/XLabs-AI/x-flux)\n2. Use our custom nodes for ComfyUI and test it with provided workflows (check out folder /workflows)\n3. Use gradio demo\n\nSee examples how to launch our models:\n\n## Canny ControlNet (version 3)\n\n1. Clone our [x-flux-comfyui](https://github.com/XLabs-AI/x-flux-comfyui) custom nodes\n2. Launch ComfyUI\n3. Try our canny_workflow.json\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/canny_result1.png?raw=true)\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/canny_result2.png?raw=true)\n\n\n## Depth ControlNet (version 3)\n\n1. Clone our [x-flux-comfyui](https://github.com/XLabs-AI/x-flux-comfyui) custom nodes\n2. Launch ComfyUI\n3. Try our depth_workflow.json\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/depth_result1.png?raw=true)\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/depth_result2.png?raw=true)\n\n\n## HED ControlNet (version 3)\n\n1. Clone our [x-flux-comfyui](https://github.com/XLabs-AI/x-flux-comfyui) custom nodes\n2. Launch ComfyUI\n3. Try our hed_workflow.json\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/hed_result1.png?raw=true)\n\n## License\n\nOur weights fall under the [FLUX.1 [dev]](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License<br/>', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":11922020867,"files_count":33,"spaces_count":8,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"}]', NULL, 'Other', 'approved', 82.3, 'a2f8694f9dc2be58c28b0cecfa172939', NULL, 'https://huggingface.co/XLabs-AI/flux-controlnet-collections/resolve/main/assets/canny_v2_res1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-XLabs-AI-flux-controlnet-collections from https://huggingface.co/XLabs-AI/flux-controlnet-collections/resolve/main/assets/canny_v2_res1.png
Image converted to WebP: data/images/huggingface-XLabs-AI-flux-controlnet-collections.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-GLM-4.5-Air', 'huggingface--zai-org--glm-4.5-air', 'GLM-4.5-Air', 'zai-org', '--- language: - en - zh library_name: transformers license: mit pipeline_tag: text-generation --- <div align="center"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/> </div> <p align="center"> üëã Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community. <br> üìñ Check out the GLM-4.5 <a href="https://z.ai/blog/glm-4.5" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="...', '["transformers","safetensors","glm4_moe","text-generation","conversational","en","zh","arxiv:2508.06471","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 531, 568166, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/GLM-4.5-Air","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\n- zh\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation\n---\n\n# GLM-4.5-Air\n\n<div align="center">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/>\n</div>\n<p align="center">\n    üëã Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community.\n    <br>\n    üìñ Check out the GLM-4.5 <a href="https://z.ai/blog/glm-4.5" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="_blank">technical report</a>, and <a href="https://zhipu-ai.feishu.cn/wiki/Gv3swM0Yci7w7Zke9E0crhU7n7D" target="_blank">Zhipu AI technical documentation</a>.\n    <br>\n    üìç Use GLM-4.5 API services on <a href="https://docs.z.ai/guides/llm/glm-4.5">Z.ai API Platform (Global)</a> or <br> <a href="https://docs.bigmodel.cn/cn/guide/models/text/glm-4.5">Zhipu AI Open Platform (Mainland China)</a>.\n    <br>\n    üëâ One click to <a href="https://chat.z.ai">GLM-4.5</a>.\n</p>\n  \n## Model Introduction\n\nThe **GLM-4.5** series models are foundation models designed for intelligent agents. GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\n\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of **63.2**, in the **3rd** place among all the proprietary and open-source  models. Notably, GLM-4.5-Air delivers competitive results at **59.8** while maintaining superior efficiency.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png)\n\nFor more eval results, show cases, and technical details, please visit\nour [technical blog](https://z.ai/blog/glm-4.5) or [technical report](https://huggingface.co/papers/2508.06471).\n\nThe model code, tool parser and reasoning parser can be found in the implementation of [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4_moe), [vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/glm4_moe_mtp.py) and [SGLang](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/glm4_moe.py).\n\n## Quick Start\n\nPlease refer our [github page](https://github.com/zai-org/GLM-4.5) for more detail.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":110468824832,"storage_bytes":220959914155,"files_count":55,"spaces_count":22,"gated":false,"private":false,"config":{"architectures":["Glm4MoeForCausalLM"],"model_type":"glm4_moe","tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson(ensure_ascii=False) }}\n{% endfor %}\n</tools>\n\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{function-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == ''text'' -%}\n                {{- item.text }}\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%- if m.role == ''user'' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == ''user'' -%}<|user|>\n{{ visible_text(m.content) }}\n{{- ''/nothink'' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '''' -}}\n{%- elif m.role == ''assistant'' -%}\n<|assistant|>\n{%- set reasoning_content = '''' %}\n{%- set content = visible_text(m.content) %}\n{%- if m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- if ''</think>'' in content %}\n        {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n        {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n    {%- endif %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ ''\\n<think>'' + reasoning_content.strip() +  ''</think>''}}\n{%- else -%}\n{{ ''\\n<think></think>'' }}\n{%- endif -%}\n{%- if content.strip() -%}\n{{ ''\\n'' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ ''\\n<tool_call>'' + tc.name }}\n{% set _args = tc.arguments %}\n{% for k, v in _args.items() %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%- elif m.role == ''tool'' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- ''<|observation|>'' }}\n{%- endif %}\n{{- ''\\n<tool_response>\\n'' }}\n{{- m.content }}\n{{- ''\\n</tool_response>'' }}\n{%- else -%}\n<|observation|>{% for tr in m.content %}\n\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == ''system'' -%}\n<|system|>\n{{ visible_text(m.content) }}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    <|assistant|>{{- ''\\n<think></think>'' if (enable_thinking is defined and not enable_thinking) else '''' -}}\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"based_on_paper","target_id":"arxiv:2508.06471","source_url":"https://arxiv.org/abs/2508.06471"}]', NULL, 'MIT', 'approved', 62.3, '32983750a7882277606d56507f07bae5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lovis93-next-scene-qwen-image-lora-2509', 'huggingface--lovis93--next-scene-qwen-image-lora-2509', 'next-scene-qwen-image-lora-2509', 'lovis93', '--- license: mit language: en base_model: - Qwen/Qwen-Image-Edit-2509 pipeline_tag: image-to-image tags: - lora - cinematic - comfyui - qwen - image-editing - next-scene - ai-video - diffusers --- --- üöÄ **New Model:** **What''s New in V2:** - üéØ **Trained on higher quality data** for significantly improved results - üí™ **Better command responsiveness** - the model follows your prompts more accurately - üñºÔ∏è **Fixed black bar artifacts** - no more unwanted black borders on generated images - ‚ö° ...', '["diffusers","lora","cinematic","comfyui","qwen","image-editing","next-scene","ai-video","image-to-image","en","base_model:qwen/qwen-image-edit-2509","base_model:adapter:qwen/qwen-image-edit-2509","license:mit","region:us"]', 'image-to-image', 529, 23532, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlanguage: en\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\npipeline_tag: image-to-image\ntags:\n- lora\n- cinematic\n- comfyui\n- qwen\n- image-editing\n- next-scene\n- ai-video\n- diffusers\n---\n\n# üé• next-scene-qwen-image-lora-2509\n\n---\n\n## üéâ ‚ú® **UPDATE - Version 2 Now Available! (21 Oct 2025)** ‚ú® üéâ\n\nüöÄ **New Model:** `next-scene_lora-v2-3000.safetensors`\n\n**What''s New in V2:**\n- üéØ **Trained on higher quality data** for significantly improved results\n- üí™ **Better command responsiveness** - the model follows your prompts more accurately\n- üñºÔ∏è **Fixed black bar artifacts** - no more unwanted black borders on generated images\n- ‚ö° **Overall enhanced performance** - smoother transitions and better cinematic flow\n\n**Recommended:** Use V2 for all new projects.\n\n**üì• ComfyUI Workflow:** [workflow-comfyui-basic-next-scene-v2.json](workflow-comfyui-basic-next-scene-v2.json)\n\n### V2 Demo Examples:\n\n![Demo 1 V2](01-update.gif) ![Demo 2 V2](02-update.gif) ![Demo 3 V2](03-update.gif)\n\n---\n\n**next-scene-qwen-image-lora-2509** is a **LoRA adapter fine-tuned on Qwen-Image-Edit (build 2509)**, purpose-built to generate cinematic image sequences with natural visual progression from frame to frame.\n\nThis model enables Qwen Image Edit to think like a film director‚Äîunderstanding camera dynamics, visual composition, and narrative continuity to create shots that flow seamlessly into one another.\n\n---\n\n## üì¶ Version 1 (Legacy)\n\n**Model File:** `next-scene_lora_v1-3000.safetensors`  \n**ComfyUI Workflow:** [workflow-comfyui-basic-next-scene.json](workflow-comfyui-basic-next-scene.json)\n\n### V1 Demo Examples:\n\n![Demo 1 V1](01.gif) ![Demo 2 V1](02.gif) ![Demo 3 V1](03.gif)\n\n---\n\n## üß† What This Model Does\n\nThis LoRA brings **cinematic storytelling continuity** into AI image generation workflows.\n\nEach output frame functions as the *"Next Scene"* in an evolving visual narrative, maintaining compositional coherence while introducing organic transitions such as:\n\n- **Camera movement:** Dolly shots, push-ins, pull-backs, and tracking moves\n- **Framing evolution:** Wide to close-up transitions, angle shifts, reframing\n- **Environmental reveals:** New characters entering frame, expanded scenery, spatial progression\n- **Atmospheric shifts:** Lighting changes, weather evolution, time-of-day transitions\n\n### Examples of Cinematic Logic:\n\n- *"Next Scene: The camera pulls back from a tight close-up on the airship to a sweeping aerial view, revealing an entire fleet of vessels soaring through a fantasy landscape."*\n\n- *"Next Scene: The camera tracks forward and tilts down, bringing the sun and helicopters closer into frame as a strong lens flare intensifies."*\n\n- *"Next Scene: The camera pans right, removing the dragon and rider from view while revealing more of the floating mountain range in the distance."*\n\n---\n\n## ‚öôÔ∏è Usage Instructions\n\n### Basic Setup:\n\n1. Load **Qwen-Image-Edit 2509** as your base model\n2. Add a **LoRA Loader** node and select:\n   - **V2 (Recommended):** `next-scene_lora-v2-3000.safetensors`\n   - **V1 (Legacy):** `next-scene_lora_v1-3000.safetensors`\n3. Set LoRA strength: **0.7 ‚Äì 0.8** (recommended)\n4. Structure your prompts with **"Next Scene:"** prefix for optimal results\n\n### Example Prompt:\n\n```\nNext Scene: The camera moves slightly forward as sunlight breaks through the clouds, casting a soft glow around the character''s silhouette in the mist. Realistic cinematic style, atmospheric depth.\n```\n\n### Pro Tips:\n\n- Begin prompts with camera direction for stronger continuity\n- Specify lighting and atmospheric changes for mood consistency\n- Chain multiple generations to create sequential storyboards\n- Works particularly well with landscape and establishing shots\n\n---\n\n## üé¨ Design Philosophy\n\nTrained on an extensive, curated cinematic dataset (proprietary), this model has learned to *think directionally* rather than just visually.\n\nIt doesn''t simply modify an image‚Äîit **advances the story**, preserving spatial relationships, lighting consistency, and emotional resonance across sequential frames.\n\n### Ideal Applications:\n\n- **Storyboard generation** for film and animation pre-production\n- **Cinematic AI video pipelines** requiring frame-to-frame coherence\n- **Sequential narrative workflows** in ComfyUI and similar tools\n- **Concept art evolution** showing scene progression\n- **Visual storytelling** for creative projects and presentations\n\n---\n\n## ‚ö†Ô∏è Important Limitations\n\n- **Not optimized for:** Static portraits, single-image illustration tasks, or non-sequential edits\n- **Best suited for:** Multi-frame workflows with narrative progression\n- **Design priority:** Storytelling flow and continuity over isolated image perfection\n- **Recommended use case:** Scene-to-scene transitions rather than detailed object manipulation\n\n---\n\n## üß± Technical Specifications\n\n- **Base Model:** Qwen-Image-Edit (build 2509)\n- **Architecture:** Low-Rank Adaptation (LoRA)\n- **Training Objective:** Scene continuity and cinematic shot coherence\n- **Dataset:** Large-scale proprietary cinematic imagery\n- **Recommended Strength:** 0.7‚Äì0.8\n- **Compatible Platforms:** ComfyUI, Automatic1111 (with Qwen support), custom pipelines\n\n---\n\n## üìÑ License\n\n**MIT License** ‚Äî Free for research, educational, and creative use.\n\nCommercial applications require independent testing and proper attribution. See LICENSE file for full terms.\n\n---\n\n## üåê Creator\n\nDeveloped by **[@lovis93](https://huggingface.co/lovis93)**\n\nPushing the boundaries of AI-directed visual storytelling and cinematic image generation.\n\n---\n\n## üê¶ Share This Model\n\nüé• Introducing **next-scene-qwen-image-lora-2509**\n\nA LoRA fine-tuned for **Qwen-Image-Edit 2509** that thinks like a film director.\n\nIt evolves each frame naturally‚Äînew angles, new lighting, same coherent world.\n\nPerfect for cinematic storyboards, sequential edits, and "Next Scene" workflows.\n\nüëâ https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509\n\n#AIart #ComfyUI #Qwen #LoRA #GenerativeAI #AIcinema #ImageEditing', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":616421537,"files_count":12,"spaces_count":34,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'MIT', 'approved', 62.2, '4974b28f49dd8f74ce893e7a46995bd6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-IndexTeam-IndexTTS-2', 'huggingface--indexteam--indextts-2', 'IndexTTS-2', 'IndexTeam', '--- license: apache-2.0 language: - en --- <center><h3>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h3></center> <div align="center"> <a href=''https://arxiv.org/abs/2506.21619''> <img src=''https://img.shields.io/badge/ArXiv-2506.21619-red?logo=arxiv''/> </a> <br/> <a href=''https://github.com/index-tts/index-tts''> <img src=''https://img.shields.io/badge/GitHub-Code-orange?logo=github''/> </a> <a href=''https://index-tts.github...', '["safetensors","en","arxiv:2506.21619","arxiv:2502.05512","license:apache-2.0","region:us"]', 'other', 528, 21880, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/IndexTeam/IndexTTS-2","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n---\n## üëâüèª IndexTTS2 üëàüèª\n\n<center><h3>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h3></center>\n\n\n<div align="center">\n  <a href=''https://arxiv.org/abs/2506.21619''>\n    <img src=''https://img.shields.io/badge/ArXiv-2506.21619-red?logo=arxiv''/>\n  </a>\n  <br/>\n  <a href=''https://github.com/index-tts/index-tts''>\n    <img src=''https://img.shields.io/badge/GitHub-Code-orange?logo=github''/>\n  </a>\n  <a href=''https://index-tts.github.io/index-tts2.github.io/''>\n    <img src=''https://img.shields.io/badge/GitHub-Demo-orange?logo=github''/>\n  </a>\n  <br/>\n  <!--a href=''https://huggingface.co/spaces/IndexTeam/IndexTTS''>\n    <img src=''https://img.shields.io/badge/HuggingFace-Demo-blue?logo=huggingface''/>\n  </a-->\n  <a href=''https://huggingface.co/IndexTeam/IndexTTS-2''>\n    <img src=''https://img.shields.io/badge/HuggingFace-Model-blue?logo=huggingface'' />\n  </a>\n  <br/>\n  <!--a href=''https://modelscope.cn/studios/IndexTeam/IndexTTS-Demo''>\n    <img src=''https://img.shields.io/badge/ModelScope-Demo-purple?logo=modelscope''/>\n  </a-->\n  <a href=''https://modelscope.cn/models/IndexTeam/IndexTTS-2''>\n    <img src=''https://img.shields.io/badge/ModelScope-Model-purple?logo=modelscope''/>\n  </a>\n</div>\n\n\n## Acknowledge\n1. [tortoise-tts](https://github.com/neonbjb/tortoise-tts)\n2. [XTTSv2](https://github.com/coqui-ai/TTS)\n3. [BigVGAN](https://github.com/NVIDIA/BigVGAN)\n4. [wenet](https://github.com/wenet-e2e/wenet/tree/main)\n5. [icefall](https://github.com/k2-fsa/icefall)\n6. [maskgct](https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct)\n7. [seed-vc](https://github.com/Plachtaa/seed-vc)\n\n\n## üìö Citation\n\nüåü If you find our work helpful, please leave us a star and cite our paper.\n\n\nIndexTTS2\n```\n@article{zhou2025indextts2,\n  title={IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech},\n  author={Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, Jingchen Shu},\n  journal={arXiv preprint arXiv:2506.21619},\n  year={2025}\n}\n```\n\nIndexTTS\n```\n@article{deng2025indextts,\n  title={IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System},\n  author={Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang},\n  journal={arXiv preprint arXiv:2502.05512},\n  year={2025},\n  doi={10.48550/arXiv.2502.05512},\n  url={https://arxiv.org/abs/2502.05512}\n}\n```\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":5919372763,"files_count":20,"spaces_count":5,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:index-tts:index-tts''>","source_url":"https://github.com/index-tts/index-tts''>"},{"type":"has_code","target_id":"github:neonbjb:tortoise-tts","source_url":"https://github.com/neonbjb/tortoise-tts"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:NVIDIA:BigVGAN","source_url":"https://github.com/NVIDIA/BigVGAN"},{"type":"has_code","target_id":"github:wenet-e2e:wenet","source_url":"https://github.com/wenet-e2e/wenet"},{"type":"has_code","target_id":"github:k2-fsa:icefall","source_url":"https://github.com/k2-fsa/icefall"},{"type":"has_code","target_id":"github:open-mmlab:Amphion","source_url":"https://github.com/open-mmlab/Amphion"},{"type":"has_code","target_id":"github:Plachtaa:seed-vc","source_url":"https://github.com/Plachtaa/seed-vc"},{"type":"based_on_paper","target_id":"arxiv:2506.21619","source_url":"https://arxiv.org/abs/2506.21619"},{"type":"based_on_paper","target_id":"arxiv:2502.05512","source_url":"https://arxiv.org/abs/2502.05512"}]', NULL, 'Apache-2.0', 'approved', 62.2, 'b9a6bfbf7aeaa5f1eeecedc0cd9e8433', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-4B-Instruct-2507', 'huggingface--qwen--qwen3-4b-instruct-2507', 'Qwen3-4B-Instruct-2507', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> We introduce the updated version of the **Qwen3-4B non-thinking mode**, named **Qwen3-4B-Instruct-250...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2505.09388","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 528, 6289445, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-4B-Instruct-2507\n<a href="https://chat.qwen.ai" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Highlights\n\nWe introduce the updated version of the **Qwen3-4B non-thinking mode**, named **Qwen3-4B-Instruct-2507**, featuring the following key enhancements:\n\n- **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.\n- **Substantial gains** in long-tail knowledge coverage across **multiple languages**.\n- **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.\n- **Enhanced capabilities** in **256K long-context understanding**.\n\n![image/jpeg](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-2507/Qwen3-4B-Instruct.001.jpeg)\n\n## Model Overview\n\n**Qwen3-4B-Instruct-2507** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 4.0B\n- Number of Paramaters (Non-Embedding): 3.6B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n\n## Performance\n\n|  | GPT-4.1-nano-2025-04-14 | Qwen3-30B-A3B Non-Thinking | Qwen3-4B Non-Thinking | Qwen3-4B-Instruct-2507 |\n|--- | --- | --- | --- | --- |\n| **Knowledge** | | | |\n| MMLU-Pro | 62.8 | 69.1 | 58.0 | **69.6** |\n| MMLU-Redux | 80.2 | 84.1 | 77.3 | **84.2** |\n| GPQA | 50.3 | 54.8 | 41.7 | **62.0** |\n| SuperGPQA | 32.2 | 42.2 | 32.0 | **42.8** |\n| **Reasoning** | | | |\n| AIME25 | 22.7 | 21.6 | 19.1 | **47.4** |\n| HMMT25 | 9.7 | 12.0 | 12.1 | **31.0** |\n| ZebraLogic | 14.8 | 33.2 | 35.2 | **80.2** |\n| LiveBench 20241125 | 41.5 | 59.4 | 48.4 | **63.0** |\n| **Coding** | | | |\n| LiveCodeBench v6 (25.02-25.05) | 31.5 | 29.0 | 26.4 | **35.1** |\n| MultiPL-E | 76.3 | 74.6 | 66.6 | **76.8** |\n| Aider-Polyglot |  9.8 | **24.4** | 13.8 | 12.9 |\n| **Alignment** | | | |\n| IFEval | 74.5 | **83.7** | 81.2 | 83.4 |\n| Arena-Hard v2* | 15.9 | 24.8 | 9.5 | **43.4** |\n| Creative Writing v3 | 72.7 | 68.1 | 53.6 | **83.5** |\n| WritingBench | 66.9 | 72.2 | 68.5 | **83.4** |\n| **Agent** | | | |\n| BFCL-v3 | 53.0 | 58.6 | 57.6 | **61.9** |\n| TAU1-Retail | 23.5 | 38.3 | 24.3 | **48.7** |\n| TAU1-Airline | 14.0 | 18.0 | 16.0 | **32.0** |\n| TAU2-Retail | - | 31.6 | 28.1 | **40.4** |\n| TAU2-Airline | - | 18.0 | 12.0 | **24.0** |\n| TAU2-Telecom | - | **18.4** | 17.5 | 13.2 |\n| **Multilingualism** | | | |\n| MultiIF | 60.7 | **70.8** | 61.3 | 69.0 |\n| MMLU-ProX | 56.2 | **65.1** | 49.6 | 61.6 |\n| INCLUDE | 58.6 | **67.8** | 53.8 | 60.1 |\n| PolyMATH | 15.6 | 23.3 | 16.6 | **31.1** |\n\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-4B-Instruct-2507"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Instruct-2507 --context-length 262144\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144\n    ```\n\n**Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-4B-Instruct-2507'',\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":4022468096,"storage_bytes":64383326441,"files_count":13,"spaces_count":80,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 62.2, '35c9b85304ec56b650b3c13c740ad991', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-whisper-large', 'huggingface--openai--whisper-large', 'whisper-large', 'openai', '--- language: - en - zh - de - es - ru - ko - fr - ja - pt - tr - pl - ca - nl - ar - sv - it - id - hi - fi - vi - he - uk - el - ms - cs - ro - da - hu - ta - no - th - ur - hr - bg - lt - la - mi - ml - cy - sk - te - fa - lv - bn - sr - az - sl - kn - et - mk - br - eu - is - hy - ne - mn - bs - kk - sq - sw - gl - mr - pa - si - km - sn - yo - so - af - oc - ka - be - tg - sd - gu - am - yi - lo - uz - fo - ht - ps - tk - nn - mt - sa - lb - my - bo - tl - mg - as - tt - haw - ln - ha - ...', '["transformers","pytorch","tf","jax","safetensors","whisper","automatic-speech-recognition","audio","hf-asr-leaderboard","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","license:apache-2.0","model-index","endpoints_compatible","deploy:azure","region:us"]', 'automatic-speech-recognition', 527, 120284, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/whisper-large","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- no\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\nmodel-index:\n- name: whisper-large\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (clean)\n      type: librispeech_asr\n      config: clean\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 3.0\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (other)\n      type: librispeech_asr\n      config: other\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 5.4\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 11.0\n      type: mozilla-foundation/common_voice_11_0\n      config: hi\n      split: test\n      args:\n        language: hi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 54.8\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours \nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains **without** the need \nfor fine-tuning.\n\nWhisper was proposed in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) \nby Alec Radford et al from OpenAI. The original code repository can be found [here](https://github.com/openai/whisper).\n\n<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">\n  <p><b>Update:</b> following the release of the paper, the Whisper authors announced a <a href="ttps://huggingface.co/openai/whisper-large-v2"> large-v2</a> model trained for 2.5x more epochs with regularization. This <a href="ttps://huggingface.co/openai/whisper-large-v2"> large-v2</a> model surpasses the performance of the large model, with no architecture changes. Thus, it is recommended that the <a href="ttps://huggingface.co/openai/whisper-large-v2"> large-v2</a> model is used in-place of the original large model. </p>\n</div>\n\n\n**Disclaimer**: Content for this model card has partly been written by the Hugging Face team, and parts of it were \ncopied and pasted from the original model card.\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. \nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. \n\nThe models were trained on either English-only data or multilingual data. The English-only models were trained \non the task of speech recognition. The multilingual models were trained on both speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. \nFor speech translation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [‚úì](https://huggingface.co/openai/whisper-tiny.en)   | [‚úì](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [‚úì](https://huggingface.co/openai/whisper-base.en)   | [‚úì](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [‚úì](https://huggingface.co/openai/whisper-small.en)  | [‚úì](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [‚úì](https://huggingface.co/openai/whisper-medium.en) | [‚úì](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v2) |\n\n# Usage\n\nTo transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate "context tokens". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the "task token". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language="english", task="transcribe")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are ''unforced'', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")\n>>> sample = ds[0]["audio"]\n>>> input_features = processor(sample["array"], sampling_rate=sample["sampling_rate"], return_tensors="pt").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n[''<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>'']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n['' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language="french", task="transcribe")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset("common_voice", "fr", split="test", streaming=True)\n>>> ds = ds.cast_column("audio", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))["audio"]\n>>> input_features = processor(input_speech["array"], sampling_rate=input_speech["sampling_rate"], return_tensors="pt").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n[''<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail int√©ressant va enfin √™tre men√© sur ce sujet.<|endoftext|>'']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n['' Un vrai travail int√©ressant va enfin √™tre men√© sur ce sujet.'']\n```\n\n## Translation \nSetting the task to "translate" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language="french", task="translate")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset("common_voice", "fr", split="test", streaming=True)\n>>> ds = ds.cast_column("audio", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))["audio"]\n>>> input_features = processor(input_speech["array"], sampling_rate=input_speech["sampling_rate"], return_tensors="pt").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n['' A very interesting work, we will finally be given on this subject.'']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Large on [LibriSpeech test-clean](https://huggingface.co/datasets/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset("librispeech_asr", "clean", split="test")\n\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large").to("cuda")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch["audio"]\n>>>     input_features = processor(audio["array"], sampling_rate=audio["sampling_rate"], return_tensors="pt").input_features\n>>>     batch["reference"] = processor.tokenizer._normalize(batch[''text''])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to("cuda"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch["prediction"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load("wer")\n>>> print(100 * wer.compute(references=result["reference"], predictions=result["prediction"]))\n3.0003583080317572\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = "cuda:0" if torch.cuda.is_available() else "cpu"\n\n>>> pipe = pipeline(\n>>>   "automatic-speech-recognition",\n>>>   model="openai/whisper-large",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")\n>>> sample = ds[0]["audio"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)["text"]\n" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel."\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)["chunks"]\n[{''text'': '' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'',\n  ''timestamp'': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https://huggingface.co/blog/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with ü§ó Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only ‚Äúintended‚Äù uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models‚Äô transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box ‚Äì their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":1543304960,"storage_bytes":30868435839,"files_count":16,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["WhisperForConditionalGeneration"],"model_type":"whisper","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"based_on_paper","target_id":"arxiv:2212.04356","source_url":"https://arxiv.org/abs/2212.04356"}]', NULL, 'Apache-2.0', 'approved', 77.2, '79a5529e64f3aa306106ee863ce1123a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NexaAI-OmniVLM-968M', 'huggingface--nexaai--omnivlm-968m', 'OmniVLM-968M', 'NexaAI', '--- license: apache-2.0 tags: - multimodal - conversational - GGUF - Image-Text-to-Text --- - [Dec 16, 2024] Our work **"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference"** is now live on Arxiv! üöÄ - [Nov 27, 2024] **Model Improvements:** OmniVLM v3 model''s **GGUF file has been updated** in this Hugging Face Repo! ‚ú® üëâ Test these exciting changes in our Hugging Face Space - [Nov 22, 2024] **Model Improvements:** OmniVLM v2 model''s **GG...', '["gguf","multimodal","conversational","gguf","image-text-to-text","arxiv:2412.11475","license:apache-2.0","endpoints_compatible","region:us"]', 'other', 527, 3158, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NexaAI/OmniVLM-968M","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\ntags:\n- multimodal\n- conversational\n- GGUF\n- Image-Text-to-Text\n---\n# OmniVLM\n\n## üî• Latest Update\n- [Dec 16, 2024] Our work **"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference"** is now live on [Arxiv](https://arxiv.org/abs/2412.11475)! üöÄ \n- [Nov 27, 2024] **Model Improvements:** OmniVLM v3 model''s **GGUF file has been updated** in this Hugging Face Repo! ‚ú®\nüëâ Test these exciting changes in our [Hugging Face Space](https://huggingface.co/spaces/NexaAIDev/omnivlm-dpo-demo)\n\n\n- [Nov 22, 2024] **Model Improvements:** OmniVLM v2 model''s **GGUF file has been updated** in this Hugging Face Repo! ‚ú® Key Improvements Include:\n  - Enhanced Art Descriptions\n  - Better Complex Image Understanding \n  - Improved Anime Recognition\n  - More Accurate Color and Detail Detection\n  - Expanded World Knowledge\n\nWe are continuously improving OmniVLM-968M based on your valuable feedback! **More exciting updates coming soon - Stay tuned!** ‚≠ê\n\n\n## Introduction\n\nOmniVLM is a compact, sub-billion (968M) multimodal model for processing both visual and text inputs, optimized for edge devices. Improved on LLaVA''s architecture, it features:\n\n- **9x Token Reduction**: Reduces image tokens from **729** to **81**, cutting latency and computational cost aggressively. Note that the computation of vision encoder and the projection part keep the same, but the computation of language model backbone is reduced due to 9X shorter image token span.\n- **Trustworthy Result**: Reduces hallucinations using **DPO** training from trustworthy data.\n  \n**Quick Links:**\n1. Interactive Demo in our [Hugging Face Space](https://huggingface.co/spaces/NexaAIDev/omnivlm-dpo-demo). (Updated 2024 Nov 21)\n2. [Quickstart for local setup](#how-to-use-on-device)\n3. Learn more in our [Blogs](https://nexa.ai/blogs/omni-vision)\n\n**Feedback:** Send questions or comments about the model in our [Discord](https://discord.gg/nexa-ai)\n\n## Intended Use Cases\nOmniVLM is intended for **Visual Question Answering** (answering questions about images) and **Image Captioning** (describing scenes in photos), making it ideal for on-device applications.\n\n**Example Demo:**\nGenerating captions for a 1046√ó1568 image on M4 Pro Macbook takes **< 2s processing time** and requires only 988 MB RAM and 948 MB Storage.\n\n<img src="https://cdn-uploads.huggingface.co/production/uploads/6618e0424dbef6bd3c72f89a/ueevDxicb98fXQ7zGN_E2.png" alt="Example" style="width:700px;"/>\n\n\n\n## Benchmarks\n\nBelow we demonstrate a figure to show how OmniVLM performs against nanollava. In all the tasks, OmniVLM outperforms the previous world''s smallest vision-language model.\n\n\n\nWe have conducted a series of experiments on benchmark datasets, including MM-VET, ChartQA, MMMU, ScienceQA, POPE to evaluate the performance of OmniVLM.\n\n| Benchmark         | Nexa AI OmniVLM v2 | Nexa AI OmniVLM v1 | nanoLLAVA |\n|-------------------|------------------------|------------------------|-----------|\n| ScienceQA (Eval)  | 71.0                  | 62.2                  | 59.0      |\n| ScienceQA (Test)  | 71.0                  | 64.5                  | 59.0      |\n| POPE              | 93.3                  | 89.4                  | 84.1      |\n| MM-VET            | 30.9                  | 27.5                  | 23.9      |\n| ChartQA (Test)    | 61.9                  | 59.2                  | NA        |\n| MMMU (Test)       | 42.1                  | 41.8                  | 28.6      |\n| MMMU (Eval)       | 40.0                  | 39.9                  | 30.4      |\n\n\n## How to Use On Device\nIn the following, we demonstrate how to run OmniVLM locally on your device.\n\n**Step 1: Install Nexa-SDK (local on-device inference framework)**\n\n[Install Nexa-SDK](https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-1-executable-installer)\n\n> Nexa-SDK is a open-sourced, local on-device inference framework, supporting text generation, image generation, vision-language models (VLM), audio-language models, speech-to-text (ASR), and text-to-speech (TTS) capabilities. Installable via Python Package or Executable Installer.\n\n**Step 2: Then run the following code in your terminal**\n\n```bash\nnexa run omniVLM \n```\n\n## Model Architecture ##\nOmniVLM''s architecture consists of three key components:\n\n- Base Language Model: Qwen2.5-0.5B-Instruct functions as the base model to process text inputs\n- Vision Encoder: SigLIP-400M operates at 384 resolution with 14√ó14 patch size to generate image embeddings\n- Projection Layer: Multi-Layer Perceptron (MLP) aligns the vision encoder''s embeddings with the language model''s token space. Compared to vanilla Llava architecture, we designed a projector that reduce 9X image tokens.\n\nThe vision encoder first transforms input images into embeddings, which are then processed by the projection layer to match the token space of Qwen2.5-0.5B-Instruct, enabling end-to-end visual-language understanding.\n\n## Training\n\nWe developed OmniVLM through a three-stage training pipeline:\n\n**Pretraining:**\nThe initial stage focuses on establishing basic visual-linguistic alignments using image-caption pairs, during which only the projection layer parameters are unfrozen to learn these fundamental relationships.\n\n**Supervised Fine-tuning (SFT):**\nWe enhance the model''s contextual understanding using image-based question-answering datasets. This stage involves training on structured chat histories that incorporate images for the model to generate more contextually appropriate responses.\n\n**Direct Preference Optimization (DPO):**\nThe final stage implements DPO by first generating responses to images using the base model. A teacher model then produces minimally edited corrections while maintaining high semantic similarity with the original responses, focusing specifically on accuracy-critical elements. These original and corrected outputs form chosen-rejected pairs. The fine-tuning targeted at essential model output improvements without altering the model''s core response characteristics\n\n## What''s next for OmniVLM?\nOmniVLM is in early development and we are working to address current limitations:\n- Expand DPO Training: Increase the scope of DPO (Direct Preference Optimization) training in an iterative process to continually improve model performance and response quality.\n- Improve document and text understanding\n  \nIn the long term, we aim to develop OmniVLM as a fully optimized, production-ready solution for edge AI multimodal applications.\n\n### Follow us\n[Blogs](https://nexa.ai/blogs/OmniVLM) | [Discord](https://discord.gg/nexa-ai) | [X(Twitter)](https://x.com/nexa_ai)', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":9252221248,"files_count":10,"spaces_count":6,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:NexaAI:nexa-sdk","source_url":"https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-1-executable-installer"},{"type":"based_on_paper","target_id":"arxiv:2412.11475","source_url":"https://arxiv.org/abs/2412.11475"}]', NULL, 'Apache-2.0', 'approved', 82.2, 'e2ad44ccea24c4e225d59e2ceab20c6d', NULL, 'https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-NexaAI-OmniVLM-968M from https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/benchmark.png
Image converted to WebP: data/images/huggingface-NexaAI-OmniVLM-968M.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-fishaudio-openaudio-s1-mini', 'huggingface--fishaudio--openaudio-s1-mini', 'openaudio-s1-mini', 'fishaudio', '', '["dual_ar","text-to-speech","zh","en","de","ja","fr","es","ko","ar","nl","ru","it","pl","pt","license:cc-by-nc-sa-4.0","region:us"]', 'text-to-speech', 526, 3637, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/fishaudio/openaudio-s1-mini","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":3606222702,"files_count":7,"spaces_count":9,"gated":"auto","private":false,"config":{"model_type":"dual_ar"}}', '[]', '[]', NULL, 'CC-BY-NC-SA-4.0', 'approved', 37.2, '8c47824c4b901177402ac39574c411c6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-magenta-realtime', 'huggingface--google--magenta-realtime', 'magenta-realtime', 'google', '--- license: cc-by-4.0 library_name: magenta-realtime --- **Authors**: Google DeepMind **Resources**: - Blog Post - Paper - Colab Demo - Repository - HuggingFace Magenta RealTime is offered under a combination of licenses: the codebase is licensed under Apache 2.0, and the model weights under Creative Commons Attribution 4.0 International. In addition, we specify the following usage terms: Copyright 2025 Google LLC Use these materials responsibly and do not generate content, including outputs...', '["magenta-realtime","tf-keras","arxiv:2508.04651","arxiv:2107.03312","arxiv:2205.01917","arxiv:2208.12415","arxiv:2301.11325","license:cc-by-4.0","region:us"]', 'other', 526, 192, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/magenta-realtime","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-4.0\nlibrary_name: magenta-realtime\n---\n\n# Model Card for Magenta RT\n\n**Authors**: Google DeepMind\n\n**Resources**:\n\n-   [Blog Post](https://g.co/magenta/rt)\n-   [Paper](https://arxiv.org/abs/2508.04651)\n-   [Colab Demo](https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb)\n-   [Repository](https://github.com/magenta/magenta-realtime)\n-   [HuggingFace](https://huggingface.co/google/magenta-realtime)\n\n## Terms of Use\n\nMagenta RealTime is offered under a combination of licenses: the codebase is\nlicensed under\n[Apache 2.0](https://github.com/magenta/magenta-realtime/blob/main/LICENSE), and\nthe model weights under\n[Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode).\nIn addition, we specify the following usage terms:\n\nCopyright 2025 Google LLC\n\nUse these materials responsibly and do not generate content, including outputs,\nthat infringe or violate the rights of others, including rights in copyrighted\ncontent.\n\nGoogle claims no rights in outputs you generate using Magenta RealTime. You and\nyour users are solely responsible for outputs and their subsequent uses.\n\nUnless required by applicable law or agreed to in writing, all software and\nmaterials distributed here under the Apache 2.0 or CC-BY licenses are\ndistributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\neither express or implied. See the licenses for the specific language governing\npermissions and limitations under those licenses. You are solely responsible for\ndetermining the appropriateness of using, reproducing, modifying, performing,\ndisplaying or distributing the software and materials, and any outputs, and\nassume any and all risks associated with your use or distribution of any of the\nsoftware and materials, and any outputs, and your exercise of rights and\npermissions under the licenses.\n\n## Model Details\n\nMagenta RealTime is an open music generation model from Google built from the\nsame research and technology used to create\n[MusicFX DJ](https://labs.google/fx/tools/music-fx-dj) and\n[Lyria RealTime](http://goo.gle/lyria-realtime). Magenta RealTime enables the\ncontinuous generation of musical audio steered by a text prompt, an audio\nexample, or a weighted combination of multiple text prompts and/or audio\nexamples. Its relatively small size makes it possible to deploy in environments\nwith limited resources, including live performance settings or freely available\nColab TPUs.\n\n### System Components\n\nMagenta RealTime is composed of three components: SpectroStream, MusicCoCa, and\nan LLM. A full technical report with more details on each component is\n[here](https://arxiv.org/abs/2508.04651).\n\n1.  **SpectroStream** is a discrete audio codec that converts stereo 48kHz audio\n    into tokens, building on the SoundStream RVQ codec from\n    [Zeghidour+ 21](https://arxiv.org/abs/2107.03312)\n1.  **MusicCoCa** is a contrastive-trained model capable of embedding audio and\n    text into a common embedding space, building on\n    [Yu+ 22](https://arxiv.org/abs/2205.01917) and\n    [Huang+ 22](https://arxiv.org/abs/2208.12415).\n1.  An **encoder-decoder Transformer LLM** generates audio tokens given context\n    audio tokens and a tokenized MusicCoCa embedding, building on the MusicLM\n    method from [Agostinelli+ 23](https://arxiv.org/abs/2301.11325)\n\n### Inputs and outputs\n\n-   **SpectroStream RVQ codec**: Tokenizes high-fidelity music audio\n    -   **Encoder input / Decoder output**: Music audio waveforms, 48kHz stereo\n    -   **Encoder output / Decoder input**: Discrete audio tokens, 25Hz frame\n        rate, 64 RVQ depth, 10 bit codes, 16kbps\n-   **MusicCoCa**: Joint embeddings of text and music audio\n    -   **Input**: Music audio waveforms, 16kHz mono, or text representation of\n        music style e.g. "heavy metal"\n    -   **Output**: 768 dimensional embedding, quantized to 12 RVQ depth, 10 bit\n        codes\n-   **Encoder-decoder Transformer LLM**: Generates audio tokens given context\n    and style\n    -   **Encoder Input**: (Context, 1000 tokens) 10s of audio context tokens w/\n        4 RVQ depth, (Style, 6 tokens) Quantized MusicCoCa style embedding\n    -   **Decoder Output**: (Generated, 800 tokens) 2s of audio w/ 16 RVQ depth\n\n## Uses\n\nMusic generation models, in particular ones targeted for continuous real-time\ngeneration and control, have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   **Interactive Music Creation**\n    -   Live Performance / Improvisation: These models can be used to generate\n        music in a live performance setting, controlled by performers\n        manipulating style embeddings or the audio context\n    -   Accessible Music-Making & Music Therapy: People with impediments to\n        using traditional instruments (skill gaps, disabilities, etc.) can\n        participate in communal jam sessions or solo music creation.\n    -   Video Games: Developers can create a custom soundtrack for users in\n        real-time based on their actions and environment.\n-   **Research**\n    -   Transfer learning: Researchers can leverage representations from\n        MusicCoCa and Magenta RT to recognize musical information.\n-   **Personalization**\n    -   Musicians can finetune models with their own catalog to customize the\n        model to their style (fine tuning support coming soon).\n-   **Education**\n    -   Exploring Genres, Instruments, and History: Natural language prompting\n        enables users to quickly learn about and experiment with musical\n        concepts.\n\n### Out-of-Scope Use\n\nSee our [Terms of Use](#terms-of-use) above for usage we consider out of scope.\n\n## Bias, Risks, and Limitations\n\nMagenta RT supports the real-time generation and steering of instrumental music.\nThe purpose and intention of this capability is to foster the development of new\nreal-time, interactive co-creation workflows that seamlessly integrate with\nhuman-centered forms of musical creativity.\n\nEvery AI music generation model, including Magenta RT, carries a risk of\nimpacting the economic and cultural landscape of music. We aim to mitigate these\nrisks through the following avenues:\n\n-   Prioritizing human-AI interaction as fundamental in the design of Magenta\n    RT.\n-   Distributing the model under a terms of service that prohibit developers\n    from generating outputs that infringe or violate the rights of others,\n    including rights in copyrighted content.\n-   Training on primarily instrumental data. With specific prompting, this model\n    has been observed to generate some vocal sounds and effects, though those\n    vocal sounds and effects tend to be non-lexical.\n\n### Known limitations\n\n**Coverage of broad musical styles**. Magenta RT''s training data primarily\nconsists of Western instrumental music. As a consequence, Magenta RT has\nincomplete coverage of both vocal performance and the broader landscape of rich\nmusical traditions worldwide. For real-time generation with broader style\ncoverage, we refer users to our\n[Lyria RealTime API](g.co/magenta/lyria-realtime).\n\n**Vocals**. While the model is capable of generating non-lexical vocalizations\nand humming, it is not conditioned on lyrics and is unlikely to generate actual\nwords. However, there remains some risk of generating explicit or\nculturally-insensitive lyrical content.\n\n**Latency**. Because the Magenta RT LLM operates on two second chunks, user\ninputs for the style prompt may take two or more seconds to influence the\nmusical output.\n\n**Limited context**. Because the Magenta RT encoder has a maximum audio context\nwindow of ten seconds, the model is unable to directly reference music that has\nbeen output earlier than that. While the context is sufficient to enable the\nmodel to create melodies, rhythms, and chord progressions, the model is not\ncapable of automatically creating longer-term song structures.\n\n### Benefits\n\nAt the time of release, Magenta RealTime represents the only open weights model\nsupporting real-time, continuous musical audio generation. It is designed\nspecifically to enable live, interactive musical creation, bringing new\ncapabilities to musical performances, art installations, video games, and many\nother applications.\n\n## How to Get Started with the Model\n\nSee our\n[Colab demo](https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb)\nand [GitHub repository](https://github.com/magenta/magenta-realtime) for usage\nexamples.\n\n## Training Details\n\n### Training Data\n\nMagenta RealTime was trained on ~190k hours of stock music from multiple\nsources, mostly instrumental.\n\n### Hardware\n\nMagenta RealTime was trained using\n[Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu)\nhardware (TPUv6e / Trillium).\n\n### Software\n\nTraining was done using [JAX](https://github.com/jax-ml/jax) and\n[T5X](https://github.com/google-research/t5x), utilizing\n[SeqIO](https://github.com/google/seqio) for data pipelines. JAX allows\nresearchers to take advantage of the latest generation of hardware, including\nTPUs, for faster and more efficient training of large models.\n\n## Evaluation\n\nModel evaluation metrics and results will be shared in our forthcoming technical\nreport.\n\n## Citation\n\nPlease cite our technical report:\n\n**BibTeX:**\n\n```\n@article{gdmlyria2025live,\n    title={Live Music Models},\n    author={Caillon, Antoine and McWilliams, Brian and Tarakajian, Cassie and Simon, Ian and Manco, Ilaria and Engel, Jesse and Constant, Noah and Li, Pen and Denk, Timo I. and Lalama, Alberto and Agostinelli, Andrea and Huang, Anna and Manilow, Ethan and Brower, George and Erdogan, Hakan and Lei, Heidi and Rolnick, Itai and Grishchenko, Ivan and Orsini, Manu and Kastelic, Matej and Zuluaga, Mauricio and Verzetti, Mauro and Dooley, Michael and Skopek, Ondrej and Ferrer, Rafael and Borsos, Zal{\''a}n and van den Oord, {\"A}aron and Eck, Douglas and Collins, Eli and Baldridge, Jason and Hume, Tom and Donahue, Chris and Han, Kehang and Roberts, Adam},\n    journal={arXiv:2508.04651},\n    year={2025}\n}\n```', '{"pipeline_tag":null,"library_name":"magenta-realtime","framework":"magenta-realtime","params":null,"storage_bytes":9983404210,"files_count":2844,"spaces_count":2,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:magenta:magenta-realtime","source_url":"https://github.com/magenta/magenta-realtime"},{"type":"has_code","target_id":"github:magenta:magenta-realtime","source_url":"https://github.com/magenta/magenta-realtime"},{"type":"has_code","target_id":"github:magenta:magenta-realtime","source_url":"https://github.com/magenta/magenta-realtime"},{"type":"has_code","target_id":"github:jax-ml:jax","source_url":"https://github.com/jax-ml/jax"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:seqio","source_url":"https://github.com/google/seqio"},{"type":"based_on_paper","target_id":"arxiv:2508.04651","source_url":"https://arxiv.org/abs/2508.04651"},{"type":"based_on_paper","target_id":"arxiv:2107.03312","source_url":"https://arxiv.org/abs/2107.03312"},{"type":"based_on_paper","target_id":"arxiv:2205.01917","source_url":"https://arxiv.org/abs/2205.01917"},{"type":"based_on_paper","target_id":"arxiv:2208.12415","source_url":"https://arxiv.org/abs/2208.12415"},{"type":"based_on_paper","target_id":"arxiv:2301.11325","source_url":"https://arxiv.org/abs/2301.11325"}]', NULL, 'CC-BY-4.0', 'approved', 77.2, 'b9d5ed76bb54ff028ca51366ef80a040', NULL, NULL, CURRENT_TIMESTAMP);
