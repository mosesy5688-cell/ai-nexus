/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-FacebookAI-xlm-roberta-base', 'huggingface--facebookai--xlm-roberta-base', 'xlm-roberta-base', 'FacebookAI', '--- tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - x...', '["transformers","pytorch","tf","jax","onnx","safetensors","xlm-roberta","fill-mask","exbert","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:1911.02116","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 771, 7108935, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/FacebookAI/xlm-roberta-base","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- exbert\nlanguage: \n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- no\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\nlicense: mit\n---\n\n# XLM-RoBERTa (base-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it''s mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''xlm-roberta-base'')\n>>> unmasker("Hello I''m a <mask> model.")\n\n[{''score'': 0.10563907772302628,\n  ''sequence'': "Hello I''m a fashion model.",\n  ''token'': 54543,\n  ''token_str'': ''fashion''},\n {''score'': 0.08015287667512894,\n  ''sequence'': "Hello I''m a new model.",\n  ''token'': 3525,\n  ''token_str'': ''new''},\n {''score'': 0.033413201570510864,\n  ''sequence'': "Hello I''m a model model.",\n  ''token'': 3299,\n  ''token_str'': ''model''},\n {''score'': 0.030217764899134636,\n  ''sequence'': "Hello I''m a French model.",\n  ''token'': 92265,\n  ''token_str'': ''French''},\n {''score'': 0.026436051353812218,\n  ''sequence'': "Hello I''m a sexy model.",\n  ''token'': 17473,\n  ''token_str'': ''sexy''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(''xlm-roberta-base'')\nmodel = AutoModelForMaskedLM.from_pretrained("xlm-roberta-base")\n\n# prepare input\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\''{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=xlm-roberta-base">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":278885778,"storage_bytes":6352424687,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["XLMRobertaForMaskedLM"],"model_type":"xlm-roberta","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"based_on_paper","target_id":"arxiv:1911.02116","source_url":"https://arxiv.org/abs/1911.02116"}]', NULL, 'MIT', 'approved', 63.9, '1fba0749c413cab557edb8d815166b93', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-WizardLMTeam-WizardCoder-Python-34B-V1.0', 'huggingface--wizardlmteam--wizardcoder-python-34b-v1.0', 'WizardCoder-Python-34B-V1.0', 'WizardLMTeam', '--- license: llama2 metrics: - code_eval library_name: transformers tags: - code model-index: - name: WizardCoder-Python-34B-V1.0 results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 0.732 verified: false --- <p style="font-size:28px;" align="center"> üè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p> <p align="center"> <p align="center"> ü§ó <a href="https://huggingface.co/WizardLM" target="...', '["transformers","pytorch","llama","text-generation","code","arxiv:2304.12244","arxiv:2306.08568","arxiv:2308.09583","license:llama2","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 771, 164, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama2\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: WizardCoder-Python-34B-V1.0\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.732\n      verified: false\n---\n\n## WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n<p style="font-size:28px;" align="center">\nüè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p>\n<p align="center">\n<p align="center">\nü§ó <a href="https://huggingface.co/WizardLM" target="_blank">HF Repo</a>  ‚Ä¢üê± <a href="https://github.com/nlpxucan/WizardLM" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/WizardLM_AI" target="_blank">Twitter</a> </p>\n<p align="center">\n üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a>  ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>   ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>  <br>\n</p>\n<p align="center">\n    üëã Join our <a href="https://discord.gg/VZjjHtWrKs" target="_blank">Discord</a>\n</p>\n\n## News\n\n[2024/01/04] üî• We released **WizardCoder-33B-V1.1**  trained from deepseek-coder-33b-base, the **SOTA OSS Code LLM** on [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html), achieves **79.9 pass@1** on HumanEval, **73.2 pass@1** on HumanEval-Plus, **78.9 pass@1** on MBPP, and **66.9 pass@1** on MBPP-Plus.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** outperforms **ChatGPT 3.5**, **Gemini Pro**, and **DeepSeek-Coder-33B-instruct** on HumanEval and HumanEval-Plus pass@1.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** is comparable with **ChatGPT 3.5**, and surpasses **Gemini Pro** on MBPP and MBPP-Plus pass@1.\n\n|  Model  |  Checkpoint  | Paper    | HumanEval  |   HumanEval+ | MBPP | MBPP+ | License |\n| ----- |------| ---- |------|-------| ----- |  ----- |----- | \n|  GPT-4-Turbo (Nov 2023)  | - | - | 85.4  | 81.7 | 83.0 | 70.7 |-|\n|  GPT-4 (May 2023)  | - | - | 88.4  | 76.8 | - | - |-|\n|  GPT-3.5-Turbo (Nov 2023)  | - | - | 72.6  | 65.9 | 81.7 | 69.4 |-|\n|  Gemini Pro  | - | - | 63.4  | 55.5 | 72.9 | 57.9 |-|\n|  DeepSeek-Coder-33B-instruct | - | - |  78.7 | 72.6 | 78.7 | 66.7 |-|\n|  **WizardCoder-33B-V1.1**  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-33B-V1.1" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  79.9  | 73.2 | 78.9 | 66.9 |  <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.1/resolve/main/LICENSE" target="_blank">MSFTResearch</a>  |\n|  WizardCoder-Python-34B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  73.2   | 64.6 | 73.2 | 59.9 |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-15B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  59.8   | 52.4 | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-Python-13B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  64.0   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-Python-7B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  55.5   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-3B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-3B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  34.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-1B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-1B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  23.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n\n\n\n-  Our **WizardMath-70B-V1.0** model slightly outperforms some closed-source LLMs on the GSM8K, including **ChatGPT 3.5**, **Claude Instant 1** and **PaLM 2 540B**.\n-  Our **WizardMath-70B-V1.0** model achieves  **81.6 pass@1** on the [GSM8k Benchmarks](https://github.com/openai/grade-school-math), which is **24.8** points higher than the SOTA open-source LLM, and achieves  **22.7 pass@1** on the [MATH Benchmarks](https://github.com/hendrycks/math), which is **9.2** points higher than the SOTA open-source LLM.\n\n<font size=4>\n    \n| Model | Checkpoint | Paper  | GSM8k | MATH  |Online Demo| License|\n| ----- |------| ---- |------|-------| ----- | ----- |\n| WizardMath-70B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-70B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **81.6**  |  **22.7**	|[Demo](http://47.103.63.15:50083/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a> |\n| WizardMath-13B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-13B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **63.9**  |  **14.0** |[Demo](http://47.103.63.15:50082/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 </a> |\n| WizardMath-7B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.0" target="_blank">HF Link</a>  |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| 	 **54.9**  |  **10.7** | [Demo ](http://47.103.63.15:50080/)|  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a>|\n</font>\n\n\n- [08/09/2023] We released **WizardLM-70B-V1.0** model. Here is [Full Model Weight](https://huggingface.co/WizardLM/WizardLM-70B-V1.0). \n\n<font size=4>\n    \n   \n| <sup>Model</sup> | <sup>Checkpoint</sup> | <sup>Paper</sup> |<sup>MT-Bench</sup> | <sup>AlpacaEval</sup>  | <sup>GSM8k</sup> | <sup>HumanEval</sup>  | <sup>License</sup>|\n| ----- |------| ---- |------|-------| ----- | ----- | ----- | \n| <sup>**WizardLM-70B-V1.0**</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-70B-V1.0" target="_blank">HF Link</a> </sup>|<sup>üìÉ**Coming Soon**</sup>| <sup>**7.78**</sup> | <sup>**92.91%**</sup>	 |<sup>**77.6%**</sup>	 | <sup>   **50.6**</sup>|<sup> <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.2</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank">HF Link</a> </sup>|  | <sup>7.06</sup> | <sup>89.17%</sup>	 |<sup>55.3%</sup>	 | <sup>36.6   </sup>|<sup> <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.1</sup> |<sup> ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.1" target="_blank">HF Link</a> </sup> |  | <sup>6.76</sup>  |<sup>86.32%</sup>	 | 	 | <sup>25.0   </sup>| <sup>Non-commercial</sup>|\n| <sup>WizardLM-30B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-30B-V1.0" target="_blank">HF Link</a></sup>  | | <sup>7.01</sup> |                    | |  <sup>37.8  </sup>| <sup>Non-commercial</sup> |\n| <sup>WizardLM-13B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" target="_blank">HF Link</a> </sup> |  | <sup>6.35</sup> | <sup>75.31%</sup> |  | <sup> 24.0   </sup> | <sup>Non-commercial</sup>|\n| <sup>WizardLM-7B-V1.0 </sup>|  <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0" target="_blank">HF Link</a> </sup> |<sup> üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a> </sup>|  |  |  |<sup>19.1 </sup>|<sup> Non-commercial</sup>|\n</font>\n\n\n## Comparing WizardCoder-Python-34B-V1.0 with Other LLMs.\n\nüî• The following figure shows that our **WizardCoder-Python-34B-V1.0 attains the second position in this benchmark**, surpassing GPT4 (2023/03/15, 73.2 vs. 67.0), ChatGPT-3.5 (73.2 vs. 72.5) and Claude2 (73.2 vs. 71.2).\n\n<p align="center" width="100%">\n<a ><img src="https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/compare_sota.png" alt="WizardCoder" style="width: 96%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n## Prompt Format\n```\n"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:"\n```\n\n## Inference Demo Script\n\nWe provide the inference demo code [here](https://github.com/nlpxucan/WizardLM/tree/main/demo).\n\n## Citation\n\nPlease cite the repo if you use the data, method or code in this repo.\n\n```\n@article{luo2023wizardcoder,\n  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},\n  journal={arXiv preprint arXiv:2306.08568},\n  year={2023}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":134976679658,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:nlpxucan:WizardLM\"","source_url":"https://github.com/nlpxucan/WizardLM\""},{"type":"has_code","target_id":"github:openai:grade-school-math","source_url":"https://github.com/openai/grade-school-math"},{"type":"has_code","target_id":"github:hendrycks:math","source_url":"https://github.com/hendrycks/math"},{"type":"has_code","target_id":"github:nlpxucan:WizardLM","source_url":"https://github.com/nlpxucan/WizardLM"},{"type":"based_on_paper","target_id":"arxiv:2304.12244","source_url":"https://arxiv.org/abs/2304.12244"},{"type":"based_on_paper","target_id":"arxiv:2306.08568","source_url":"https://arxiv.org/abs/2306.08568"},{"type":"based_on_paper","target_id":"arxiv:2308.09583","source_url":"https://arxiv.org/abs/2308.09583"}]', NULL, 'LLaMA-2', 'approved', 63.9, '1167423df261871b914e9789912ee79a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-VoxCPM-0.5B', 'huggingface--openbmb--voxcpm-0.5b', 'VoxCPM-0.5B', 'openbmb', '--- license: apache-2.0 language: - en - zh base_model: - openbmb/MiniCPM4-0.5B pipeline_tag: text-to-speech library_name: voxcpm tags: - text-to-speech - speech - speech generation - voice cloning --- <div align="center"> <img src="assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%"> </div> VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization a...', '["voxcpm","pytorch","text-to-speech","speech","speech generation","voice cloning","en","zh","base_model:openbmb/minicpm4-0.5b","base_model:finetune:openbmb/minicpm4-0.5b","license:apache-2.0","region:us"]', 'text-to-speech', 770, 2428, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/VoxCPM-0.5B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nbase_model:\n- openbmb/MiniCPM4-0.5B\npipeline_tag: text-to-speech\nlibrary_name: voxcpm\ntags:\n- text-to-speech\n- speech\n- speech generation\n- voice cloning\n---\n\n## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage/)\n\n\n<div align="center">\n  <img src="assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%">\n</div>\n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align="center">\n  <img src="assets/voxcpm_model.png" alt="VoxCPM Model Architecture" width="90%">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download("openbmb/VoxCPM-0.5B",local_files_only=local_files_only)\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download(''iic/speech_zipenhancer_ans_multiloss_16k_base'')\n    snapshot_download(''iic/SenseVoiceSmall'')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained("openbmb/VoxCPM-0.5B")\n\nwav = model.generate(\n    text="VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write("output.wav", wav, 16000)\nprint("saved: output.wav")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text "Hello VoxCPM" --output out.wav\n\n# 2) Voice cloning (reference audio + transcript)\nvoxcpm --text "Hello" \\n  --prompt-audio path/to/voice.wav \\n  --prompt-text "reference transcript" \\n  --output out.wav \\n  --denoise\n\n# 3) Batch processing (one text per line)\nvoxcpm --input examples/input.txt --output-dir outs\n# (optional) Batch + cloning\nvoxcpm --input examples/input.txt --output-dir outs \\n  --prompt-audio path/to/voice.wav \\n  --prompt-text "reference transcript" \\n  --denoise\n\n# 4) Inference parameters (quality/speed)\nvoxcpm --text "..." --output out.wav \\n  --cfg-value 2.0 --inference-timesteps 10 --normalize\n\n# 5) Model loading\n# Prefer local path\nvoxcpm --text "..." --output out.wav --model-path /path/to/VoxCPM_model_dir\n# Or from Hugging Face (auto download/cache)\nvoxcpm --text "..." --output out.wav \\n  --hf-model-id openbmb/VoxCPM-0.5B --cache-dir ~/.cache/huggingface --local-files-only\n\n# 6) Denoiser control\nvoxcpm --text "..." --output out.wav \\n  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base\n\n# 7) Help\nvoxcpm --help\npython -m voxcpm.cli --help\n```\n\n### 4. Start web demo\n\nYou can start the UI interface by running `python app.py`, which allows you to perform Voice Cloning and Voice Creation.\n\n\n\n## üë©‚Äçüç≥ A Voice Chef''s Guide\nWelcome to the VoxCPM kitchen! Follow this recipe to cook up perfect generated speech. Let‚Äôs begin.\n\n---\n### ü•ö Step 1: Prepare Your Base Ingredients (Content)\n\nFirst, choose how you‚Äôd like to input your text:.\n1. Regular Text (Classic Mode)\n- ‚úÖ Keep "Text Normalization" ON. Type naturally (e.g., "Hello, world! 123"). The system will automatically process numbers, abbreviations, and punctuation using WeTextProcessing library.\n2. Phoneme Input (Native Mode)\n- ‚ùå Turn "Text Normalization" OFF. Enter phoneme text like {HH AH0 L OW1} (EN) or {ni3}{hao3} (ZH) for precise pronunciation  control. In this mode, VoxCPM also supports native understanding of other complex non-normalized text‚Äîtry it out!\n\n---\n### üç≥ Step 2: Choose Your Flavor Profile (Voice Style) \n\nThis is the secret sauce that gives your audio its unique sound.\n1. Cooking with a Prompt Speech (Following a Famous Recipe)\n  - A prompt speech provides the desired acoustic characteristics for VoxCPM. The speaker''s timbre, speaking style, and even the background sounds and ambiance will be replicated.\n  - For a Clean, Studio-Quality Voice:\n    - ‚úÖ Enable "Prompt Speech Enhancement". This acts like a noise filter, removing background hiss and rumble to give you a pure, clean voice clone.\n2. Cooking au Naturel (Letting the Model Improvise)\n  - If no reference is provided, VoxCPM becomes a creative chef! It will infer a fitting speaking style based on the text itself, thanks to the text-smartness of its foundation model, MiniCPM-4.\n  - Pro Tip: Challenge VoxCPM with any text‚Äîpoetry, song lyrics, dramatic monologues‚Äîit may deliver some interesting results!\n\n---\n### üßÇ Step 3: The Final Seasoning (Fine-Tuning Your Results)\nYou''re ready to serve! But for master chefs who want to tweak the flavor, here are two key spices.\n- CFG Value (How Closely to Follow the Recipe)\n  - Default: A great starting point.\n  - Voice sounds strained or weird? Lower this value. It tells the model to be more relaxed and improvisational, great for expressive prompts.\n  - Need maximum clarity and adherence to the text? Raise it slightly to keep the model on a tighter leash.\n- Inference Timesteps (Simmering Time: Quality vs. Speed)\n  - Need a quick snack? Use a lower number. Perfect for fast drafts and experiments.\n  - Cooking a gourmet meal? Use a higher number. This lets the model "simmer" longer, refining the audio for superior detail and naturalness.\n\n---\nHappy creating! üéâ Start with the default settings and tweak from there to suit your project. The kitchen is yours!\n\n\n---\n\n\n\n## üìä Performance Highlights\n\nVoxCPM achieves competitive results on public zero-shot TTS benchmarks:\n\n### Seed-TTS-eval Benchmark\n\n| Model | Parameters | Open-Source | test-EN | | test-ZH | | test-Hard | |\n|------|------|------|:------------:|:--:|:------------:|:--:|:-------------:|:--:|\n| | | | WER/%‚¨á | SIM/%‚¨Ü| CER/%‚¨á| SIM/%‚¨Ü | CER/%‚¨á | SIM/%‚¨Ü |\n| MegaTTS3 | 0.5B | ‚ùå | 2.79 | 77.1 | 1.52 | 79.0 | - | - |\n| DiTAR | 0.6B | ‚ùå | 1.69 | 73.5 | 1.02 | 75.3 | - | - |\n| CosyVoice3 | 0.5B | ‚ùå | 2.02 | 71.8 | 1.16 | 78.0 | 6.08 | 75.8 |\n| CosyVoice3 | 1.5B | ‚ùå | 2.22 | 72.0 | 1.12 | 78.1 | 5.83 | 75.8 |\n| Seed-TTS | - | ‚ùå | 2.25 | 76.2 | 1.12 | 79.6 | 7.59 | 77.6 |\n| MiniMax-Speech | - | ‚ùå | 1.65 | 69.2 | 0.83 | 78.3 | - | - |\n| CosyVoice | 0.3B | ‚úÖ | 4.29 | 60.9 | 3.63 | 72.3 | 11.75 | 70.9 |\n| CosyVoice2 | 0.5B | ‚úÖ | 3.09 | 65.9 | 1.38 | 75.7 | **6.83** | 72.4 |\n| F5-TTS | 0.3B | ‚úÖ | 2.00 | 67.0 | 1.53 | 76.0 | 8.67 | 71.3 |\n| SparkTTS | 0.5B | ‚úÖ | 3.14 | 57.3 | 1.54 | 66.0 | - | - |\n| FireRedTTS | 0.5B | ‚úÖ | 3.82 | 46.0 | 1.51 | 63.5 | 17.45 | 62.1 |\n| FireRedTTS-2 | 1.5B | ‚úÖ | 1.95 | 66.5 | 1.14 | 73.6 | - | - |\n| Qwen2.5-Omni | 7B | ‚úÖ | 2.72 | 63.2 | 1.70 | 75.2 | 7.97 | **74.7** |\n| OpenAudio-s1-mini | 0.5B | ‚úÖ | 1.94 | 55.0 | 1.18 | 68.5 | - | - |\n| IndexTTS2 | 1.5B | ‚úÖ | 2.23 | 70.6 | 1.03 | 76.5 | - | - |\n| VibeVoice | 1.5B | ‚úÖ | 3.04 | 68.9 | 1.16 | 74.4 | - | - |\n| HiggsAudio-v2 | 3B | ‚úÖ | 2.44 | 67.7 | 1.50 | 74.0 | - | - |\n| **VoxCPM** | 0.5B | ‚úÖ | **1.85** | **72.9** | **0.93** | **77.2** | 8.87 | 73.0 |\n\n\n###  CV3-eval Benchmark\n\n| Model | zh | en | hard-zh | | | hard-en | | |\n|-------|:--:|:--:|:-------:|:--:|:--:|:-------:|:--:|:--:|\n| | CER/%‚¨á | WER/%‚¨á | CER/%‚¨á | SIM/%‚¨Ü | DNSMOS‚¨Ü | WER/%‚¨á | SIM/%‚¨Ü | DNSMOS‚¨Ü |\n| F5-TTS | 5.47 | 8.90 | - | - | - | - | - | - |\n| SparkTTS | 5.15 | 11.0 | - | - | - | - | - | - |\n| GPT-SoVits | 7.34 | 12.5 | - | - | - | - | - | - |\n| CosyVoice2 | 4.08 | 6.32 | 12.58 | 72.6 | 3.81 | 11.96 | 66.7 | 3.95 |\n| OpenAudio-s1-mini | 4.00 | 5.54 | 18.1 | 58.2 | 3.77 | 12.4 | 55.7 | 3.89 |\n| IndexTTS2 | 3.58 | 4.45 | 12.8 | 74.6 | 3.65 | - | - | - |\n| HiggsAudio-v2 | 9.54 | 7.89 | 41.0 | 60.2 | 3.39 | 10.3 | 61.8 | 3.68 |\n| CosyVoice3-0.5B | 3.89 | 5.24 | 14.15 | 78.6 | 3.75 | 9.04 | 75.9 | 3.92 |\n| CosyVoice3-1.5B | 3.91 | 4.99 | 9.77 | 78.5 | 3.79 | 10.55 | 76.1 | 3.95 |\n| **VoxCPM** | **3.40** | **4.04** | 12.9 | 66.1 | 3.59 | **7.89** | 64.3 | 3.74 |\n\n\n## ‚ö†Ô∏è Risks and limitations\n- General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.\n- Potential for Misuse of Voice Cloning: VoxCPM''s powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.\n- Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.\n- Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.\n- This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.\n\n\n\n## üìÑ License\nThe VoxCPM model weights and code are open-sourced under the Apache-2.0 license.\n\n\n', '{"pipeline_tag":"text-to-speech","library_name":"voxcpm","framework":"voxcpm","params":null,"storage_bytes":2910957348,"files_count":12,"spaces_count":15,"gated":false,"private":false,"config":{"tokenizer_config":{"bos_token":"<s>","eos_token":"<|im_end|>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false,"chat_template":"{% for message in messages %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:OpenBMB:VoxCPM","source_url":"https://github.com/OpenBMB/VoxCPM"}]', NULL, 'Apache-2.0', 'approved', 98.9, '52f8d486d498dcd78e44592c0f4db47a', NULL, 'https://huggingface.co/openbmb/VoxCPM-0.5B/resolve/main/assets/modelbest_logo.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-openbmb-VoxCPM-0.5B from https://huggingface.co/openbmb/VoxCPM-0.5B/resolve/main/assets/modelbest_logo.png
Image converted to WebP: data/images/huggingface-openbmb-VoxCPM-0.5B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-baidu-ERNIE-4.5-21B-A3B-Thinking', 'huggingface--baidu--ernie-4.5-21b-a3b-thinking', 'ERNIE-4.5-21B-A3B-Thinking', 'baidu', '--- license: apache-2.0 language: - en - zh pipeline_tag: text-generation tags: - ERNIE4.5 library_name: transformers --- <div align="center" style="line-height: 1;"> <a href="https://ernie.baidu.com/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue" style="display: inline-block; vertical-align: middle;"/> </a> <a href="https://huggingface.co/baidu" target="_blank" style="margin: 2px;"> <img alt="Hugging Face" src="https://img.shi...', '["transformers","safetensors","ernie4_5_moe","text-generation","ernie4.5","conversational","en","zh","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 768, 14525, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: text-generation\ntags:\n- ERNIE4.5\nlibrary_name: transformers\n---\n\n<div align="center" style="line-height: 1;">\n  <a href="https://ernie.baidu.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/baidu" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" style="margin: 2px;">\n    <img alt="Github" src="https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://ernie.baidu.com/blog/ernie4.5" target="_blank" style="margin: 2px;">\n    <img alt="Blog" src="https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://discord.gg/JPmZXDsEEK" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://x.com/PaddlePaddle" target="_blank" style="margin: 2px;">\n    <img alt="X" src="https://img.shields.io/badge/X-PaddlePaddle-6080F0"?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="#license" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-Apache2.0-A5de54" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n# ERNIE-4.5-21B-A3B-Thinking\n\n## Model Highlights\n\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\n* **Efficient tool usage** capabilities.\n* **Enhanced 128K long-context understanding** capabilities.\n\n> [!NOTE]\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\n\n![benchmark](./benchmark.png)\n\n## Model Overview\n\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\n\n|Key|Value|\n|-|-|\n|Modality|Text|\n|Training Stage|Posttraining|\n|Params(Total / Activated)|21B / 3B|\n|Layers|28|\n|Heads(Q/KV)|20 / 4|\n|Text Experts(Total / Activated)|64 / 6|\n|Shared Experts|2|\n|Context Length|131072|\n\n## Quickstart\n\n> [!NOTE]\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\n\n### FastDeploy Inference\n\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\n\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\n\n```bash\npython -m fastdeploy.entrypoints.openai.api_server \\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\n       --port 8180 \\n       --metrics-port 8181 \\n       --engine-worker-queue-port 8182 \\n       --load-choices "default_v1" \\n       --tensor-parallel-size 1 \\n       --max-model-len 131072 \\n       --reasoning-parser ernie_x1 \\n       --tool-call-parser ernie_x1 \\n       --max-num-seqs 32\n```\n\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\n\n```bash\ncurl -X POST "http://0.0.0.0:8180/v1/chat/completions" \\n-H "Content-Type: application/json" \\n-d $''{\n  "messages": [\n    {\n      "role": "user",\n      "content": "How \''s the weather in Beijing today?"\n    }\n  ],\n  "tools": [\n    {\n      "type": "function",\n      "function": {\n        "name": "get_weather",\n        "description": "Determine weather in my location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state e.g. San Francisco, CA"\n            },\n            "unit": {\n              "type": "string",\n              "enum": [\n                "c",\n                "f"\n              ]\n            }\n          },\n          "additionalProperties": false,\n          "required": [\n            "location",\n            "unit"\n          ]\n        },\n        "strict": true\n      }\n    }]\n}''\n```\n\n### vLLM inference\n\nVLLM>=0.10.2 (excluding 0.11.0)\n\n```bash\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\n```\n\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie need install vllm main branch\n\n### Using `transformers` library\n\n**Note**: You''ll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "baidu/ERNIE-4.5-21B-A3B-Thinking"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map="auto",\n    torch_dtype=torch.bfloat16,\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint("generate_text:", generate_text)\n```\n\n## License\n\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\n\n## Citation\n\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\n\n```text\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu-ERNIE-Team},\n      year={2025},\n      primaryClass={cs.CL},\n      howpublished={\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\n}\n```\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":21825437888,"storage_bytes":43674659350,"files_count":21,"spaces_count":8,"gated":false,"private":false,"config":{"architectures":["Ernie4_5_MoeForCausalLM"],"model_type":"ernie4_5_moe","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"<unk>","unk_token":"<unk>","cls_token":"<|begin_of_sentence|>","sep_token":"<|end_of_sentence|>","mask_token":"<mask:1>","chat_template":"{{- ''<|im_start|>system\n'' }}{%- if messages[0].role != ''system'' and not system_settings %}{{- ''<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- else%}{{- ''<system_setting>\n'' }}{{- system_settings + ''\n'' if system_settings else '''' }}{{- (messages[0].content + ''\n'' if messages[0].role == ''system'' else '''') + ''</system_setting>\n\n<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- endif %}{%- if tools %}{{- \"\n\n<tool_list>\" }}{{- ''\n'' }}{{-''[''}}{% for tool in tools %}{{''{\"type\": \"function\", \"function\": ''}}{{-(tool.function | tojson)}}}{%-if not loop.last%},{%- endif %}{%endfor%}{{-'']''}}{{- \"\n</tool_list>\" }}{%- endif %}{{-''<|im_end|>\n\n'' }}{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_output>'') and message.content.endswith(''</tool_output>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\n'' + content + ''<|im_end|>'' + ''\n\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.thoughts is string %}\n            {%- set reasoning_content = message.thoughts %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\n'').split(''<think>'')[-1].lstrip(''\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index and  (loop.last or (not loop.last and reasoning_content)) %} {{- ''<|im_start|>'' + message.role + ''\n<think>\n'' + reasoning_content.strip(''\n'') + ''\n</think>\n'' }} {%- else %} {{- ''<|im_start|>'' + message.role + ''\n'' }} {%- endif %}  {%- if content|length > 0 %}  {{- ''<response>\n'' + content + ''\n</response>\n'' }}  {%- endif %} {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''\n<tool_call>\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\n</tool_call>\n'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\n\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>tool'' }}\n        {%- endif %}\n        {{- ''\n<tool_output>'' }}\n        {{- message.content|tojson }}\n        {{- ''</tool_output>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\n\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n {{- \"<|im_start|>assistant\n<think>\n\"}}"},"chat_template_jinja":"{{- ''<|im_start|>system\n'' }}{%- if messages[0].role != ''system'' and not system_settings %}{{- ''<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- else%}{{- ''<system_setting>\n'' }}{{- system_settings + ''\n'' if system_settings else '''' }}{{- (messages[0].content + ''\n'' if messages[0].role == ''system'' else '''') + ''</system_setting>\n\n<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- endif %}{%- if tools %}{{- \"\n\n<tool_list>\" }}{{- ''\n'' }}{{-''[''}}{% for tool in tools %}{{''{\"type\": \"function\", \"function\": ''}}{{-(tool.function | tojson)}}}{%-if not loop.last%},{%- endif %}{%endfor%}{{-'']''}}{{- \"\n</tool_list>\" }}{%- endif %}{{-''<|im_end|>\n\n'' }}{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_output>'') and message.content.endswith(''</tool_output>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\n'' + content + ''<|im_end|>'' + ''\n\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.thoughts is string %}\n            {%- set reasoning_content = message.thoughts %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\n'').split(''<think>'')[-1].lstrip(''\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index and  (loop.last or (not loop.last and reasoning_content)) %} {{- ''<|im_start|>'' + message.role + ''\n<think>\n'' + reasoning_content.strip(''\n'') + ''\n</think>\n'' }} {%- else %} {{- ''<|im_start|>'' + message.role + ''\n'' }} {%- endif %}  {%- if content|length > 0 %}  {{- ''<response>\n'' + content + ''\n</response>\n'' }}  {%- endif %} {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''\n<tool_call>\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\n</tool_call>\n'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\n\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>tool'' }}\n        {%- endif %}\n        {{- ''\n<tool_output>'' }}\n        {{- message.content|tojson }}\n        {{- ''</tool_output>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\n\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n {{- \"<|im_start|>assistant\n<think>\n\"}}"}}', '[]', '[{"type":"has_code","target_id":"github:PaddlePaddle:ERNIE\"","source_url":"https://github.com/PaddlePaddle/ERNIE\""},{"type":"has_code","target_id":"github:PaddlePaddle:FastDeploy","source_url":"https://github.com/PaddlePaddle/FastDeploy"}]', NULL, 'Apache-2.0', 'approved', 83.9, '5344f8a9cdfdc47a99ebceb2c13bf40d', NULL, 'https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking/resolve/main/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-baidu-ERNIE-4.5-21B-A3B-Thinking from https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking/resolve/main/benchmark.png
Image converted to WebP: data/images/huggingface-baidu-ERNIE-4.5-21B-A3B-Thinking.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Embedding-0.6B', 'huggingface--qwen--qwen3-embedding-0.6b', 'Qwen3-Embedding-0.6B', 'Qwen', '--- license: apache-2.0 base_model: - Qwen/Qwen3-0.6B-Base tags: - transformers - sentence-transformers - sentence-similarity - feature-extraction - text-embeddings-inference --- <p align="center"> <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400"/> <p> The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qw...', '["sentence-transformers","safetensors","qwen3","text-generation","transformers","sentence-similarity","feature-extraction","text-embeddings-inference","arxiv:2506.05176","base_model:qwen/qwen3-0.6b-base","base_model:finetune:qwen/qwen3-0.6b-base","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'feature-extraction', 767, 4441025, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen3-0.6B-Base\ntags:\n- transformers\n- sentence-transformers\n- sentence-similarity\n- feature-extraction\n- text-embeddings-inference\n---\n# Qwen3-Embedding-0.6B\n\n<p align="center">\n    <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400"/>\n<p>\n\n## Highlights\n\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\n\n**Exceptional Versatility**: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks **No.1** in the MTEB multilingual leaderboard (as of June 5, 2025, score **70.58**), while the reranking model excels in various text retrieval scenarios.\n\n**Comprehensive Flexibility**: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\n\n**Multilingual Capability**: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\n\n## Model Overview\n\n**Qwen3-Embedding-0.6B** has the following features:\n\n- Model Type: Text Embedding\n- Supported Languages: 100+ Languages\n- Number of Paramaters: 0.6B\n- Context Length: 32k\n- Embedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-embedding/), [GitHub](https://github.com/QwenLM/Qwen3-Embedding).\n\n## Qwen3 Embedding Series Model list\n\n| Model Type       | Models               | Size | Layers | Sequence Length | Embedding Dimension | MRL Support | Instruction Aware |\n|------------------|----------------------|------|--------|-----------------|---------------------|-------------|----------------|\n| Text Embedding   | [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) | 0.6B | 28     | 32K             | 1024                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)   | 4B   | 36     | 32K             | 2560                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)   | 8B   | 36     | 32K             | 4096                | Yes         | Yes            |\n| Text Reranking   | [Qwen3-Reranker-0.6B](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B) | 0.6B | 28     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-4B](https://huggingface.co/Qwen/Qwen3-Reranker-4B)   | 4B   | 36     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-8B](https://huggingface.co/Qwen/Qwen3-Reranker-8B)   | 8B   | 36     | 32K             | -                   | -           | Yes            |\n\n> **Note**:\n> - `MRL Support` indicates whether the embedding model supports custom dimensions for the final embedding. \n> - `Instruction Aware` notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\n> - Our evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\n\n## Usage\n\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\n### Sentence Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\nmodel = SentenceTransformer("Qwen/Qwen3-Embedding-0.6B")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to "left":\n# model = SentenceTransformer(\n#     "Qwen/Qwen3-Embedding-0.6B",\n#     model_kwargs={"attn_implementation": "flash_attention_2", "device_map": "auto"},\n#     tokenizer_kwargs={"padding_side": "left"},\n# )\n\n# The queries and documents to embed\nqueries = [\n    "What is the capital of China?",\n    "Explain gravity",\n]\ndocuments = [\n    "The capital of China is Beijing.",\n    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.",\n]\n\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called "query" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name="query")\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7646, 0.1414],\n#         [0.1355, 0.6000]])\n```\n\n### Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef last_token_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f''Instruct: {task_description}\nQuery:{query}''\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = ''Given a web search query, retrieve relevant passages that answer the query''\n\nqueries = [\n    get_detailed_instruct(task, ''What is the capital of China?''),\n    get_detailed_instruct(task, ''Explain gravity'')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    "The capital of China is Beijing.",\n    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun."\n]\ninput_texts = queries + documents\n\ntokenizer = AutoTokenizer.from_pretrained(''Qwen/Qwen3-Embedding-0.6B'', padding_side=''left'')\nmodel = AutoModel.from_pretrained(''Qwen/Qwen3-Embedding-0.6B'')\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained(''Qwen/Qwen3-Embedding-0.6B'', attn_implementation="flash_attention_2", torch_dtype=torch.float16).cuda()\n\nmax_length = 8192\n\n# Tokenize the input texts\nbatch_dict = tokenizer(\n    input_texts,\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors="pt",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict[''attention_mask''])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7645568251609802, 0.14142508804798126], [0.13549736142158508, 0.5999549627304077]]\n```\n\n### vLLM Usage\n\n```python\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f''Instruct: {task_description}\nQuery:{query}''\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = ''Given a web search query, retrieve relevant passages that answer the query''\n\nqueries = [\n    get_detailed_instruct(task, ''What is the capital of China?''),\n    get_detailed_instruct(task, ''Explain gravity'')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    "The capital of China is Beijing.",\n    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun."\n]\ninput_texts = queries + documents\n\nmodel = LLM(model="Qwen/Qwen3-Embedding-0.6B", task="embed")\n\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7620252966880798, 0.14078938961029053], [0.1358368694782257, 0.6013815999031067]]\n```\n\nüìå **Tip**: We recommend that developers customize the `instruct` according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an `instruct` on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\n\n### Text Embeddings Inference (TEI) Usage\n\nYou can either run / deploy TEI on NVIDIA GPUs as:\n\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B --dtype float16\n```\n\nOr on CPU devices as:\n\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B\n```\n\nAnd then, generate the embeddings sending a HTTP POST request as:\n\n```bash\ncurl http://localhost:8080/embed \\n    -X POST \\n    -d ''{"inputs": ["Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: What is the capital of China?", "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: Explain gravity"]}'' \\n    -H "Content-Type: application/json"\n```\n\n## Evaluation\n\n### MTEB (Multilingual)\n\n| Model                            |  Size   |  Mean (Task)  | Mean (Type) | Bitxt Mining | Class. | Clust. | Inst. Retri. | Multi. Class. | Pair. Class. | Rerank | Retri. | STS  |\n|----------------------------------|:-------:|:-------------:|:-------------:|:--------------:|:--------:|:--------:|:--------------:|:---------------:|:--------------:|:--------:|:--------:|:------:|\n| NV-Embed-v2                      |   7B    |     56.29     | 49.58       | 57.84        | 57.29  | 40.80  | 1.04         | 18.63         | 78.94        | 63.82  | 56.72  | 71.10|\n| GritLM-7B                        |   7B    |     60.92     | 53.74       | 70.53        | 61.83  | 49.75  | 3.45         | 22.77         | 79.94        | 63.78  | 58.31  | 73.33|\n| BGE-M3                           |  0.6B   |     59.56     | 52.18       | 79.11        | 60.35  | 40.88  | -3.11        | 20.1          | 80.76        | 62.79  | 54.60  | 74.12|\n| multilingual-e5-large-instruct   |  0.6B   |     63.22     | 55.08       | 80.13        | 64.94  | 50.75  | -0.40        | 22.91         | 80.86        | 62.61  | 57.12  | 76.81|\n| gte-Qwen2-1.5B-instruct          |  1.5B   |     59.45     | 52.69       | 62.51        | 58.32  | 52.05  | 0.74         | 24.02         | 81.58        | 62.58  | 60.78  | 71.61|\n| gte-Qwen2-7b-Instruct            |   7B    |     62.51     | 55.93       | 73.92        | 61.55  | 52.77  | 4.94         | 25.48         | 85.13        | 65.55  | 60.08  | 73.98|\n| text-embedding-3-large           |    -    |     58.93     | 51.41       | 62.17        | 60.27  | 46.89  | -2.68        | 22.03         | 79.17        | 63.89  | 59.27  | 71.68|\n| Cohere-embed-multilingual-v3.0   |    -    |     61.12     | 53.23       | 70.50        | 62.95  | 46.89  | -1.89        | 22.74         | 79.88        | 64.07  | 59.16  | 74.80|\n| Gemini Embedding                 |    -    |     68.37     | 59.59       | 79.28        | 71.82  | 54.59  | 5.18         | **29.16**     | 83.63        | 65.58  | 67.71  | 79.40|\n| **Qwen3-Embedding-0.6B**         |  0.6B   |     64.33     | 56.00       | 72.22        | 66.83  | 52.33  | 5.09         | 24.59         | 80.83        | 61.41  | 64.64  | 76.17|\n| **Qwen3-Embedding-4B**           |   4B    |     69.45     | 60.86       | 79.36        | 72.33  | 57.15  | **11.56**    | 26.77         | 85.05        | 65.08  | 69.60  | 80.86|\n| **Qwen3-Embedding-8B**           |   8B    |   **70.58**   | **61.69**   | **80.89**    | **74.00** | **57.65** | 10.06      | 28.66         | **86.40**    | **65.63** | **70.88** | **81.08** |\n\n> **Note**: For compared models, the scores are retrieved from MTEB online [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) on May 24th, 2025.\n\n### MTEB (Eng v2)\n\n| MTEB English / Models          |  Param.  | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retri. | STS   | Summ. |\n|--------------------------------|:--------:|:------------:|:------------:|:--------:|:--------:|:-------------:|:---------:|:--------:|:-------:|:-------:|\n| multilingual-e5-large-instruct |   0.6B   | 65.53      | 61.21      | 75.54  | 49.89  | 86.24       | 48.74   | 53.47  | 84.72 | 29.89 |\n| NV-Embed-v2                    |   7.8B   | 69.81      | 65.00      | 87.19  | 47.66  | 88.69       | 49.61   | 62.84  | 83.82 | 35.21 |\n| GritLM-7B                      |   7.2B   | 67.07      | 63.22      | 81.25  | 50.82  | 87.29       | 49.59   | 54.95  | 83.03 | 35.65 |\n| gte-Qwen2-1.5B-instruct        |   1.5B   | 67.20      | 63.26      | 85.84  | 53.54  | 87.52       | 49.25   | 50.25  | 82.51 | 33.94 |\n| stella_en_1.5B_v5              |   1.5B   | 69.43      | 65.32      | 89.38  | 57.06  | 88.02       | 50.19   | 52.42  | 83.27 | 36.91 |\n| gte-Qwen2-7B-instruct          |   7.6B   | 70.72      | 65.77      | 88.52  | 58.97  | 85.9        | 50.47   | 58.09  | 82.69 | 35.74 |\n| gemini-embedding-exp-03-07     |    -     | 73.3       | 67.67      | 90.05  | 59.39  | 87.7        | 48.59   | 64.35  | 85.29 | 38.28 |\n| **Qwen3-Embedding-0.6B**       |   0.6B   | 70.70      | 64.88      | 85.76  | 54.05  | 84.37       | 48.18   | 61.83  | 86.57 | 33.43 |\n| **Qwen3-Embedding-4B**         |    4B    | 74.60      | 68.10      | 89.84  | 57.51  | 87.01       | 50.76   | 68.46  | 88.72 | 34.39 |\n| **Qwen3-Embedding-8B**         |    8B    | 75.22      | 68.71      | 90.43  | 58.57  | 87.52       | 51.56   | 69.44  | 88.58 | 34.83 |\n\n### C-MTEB (MTEB Chinese)\n\n| C-MTEB           | Param. | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retr. | STS   |\n|------------------|--------|------------|------------|--------|--------|-------------|---------|-------|-------|\n| multilingual-e5-large-instruct | 0.6B   | 58.08      | 58.24      | 69.80  | 48.23  | 64.52       | 57.45   | 63.65 | 45.81 |\n| bge-multilingual-gemma2 | 9B     | 67.64      | 75.31      | 59.30  | 86.67  | 68.28       | 73.73   | 55.19 | -     |\n| gte-Qwen2-1.5B-instruct  | 1.5B   | 67.12      | 67.79      | 72.53  | 54.61  | 79.5        | 68.21   | 71.86 | 60.05 |\n| gte-Qwen2-7B-instruct    | 7.6B   | 71.62      | 72.19      | 75.77  | 66.06  | 81.16       | 69.24   | 75.70 | 65.20 |\n| ritrieve_zh_v1          | 0.3B   | 72.71      | 73.85      | 76.88  | 66.5   | 85.98       | 72.86   | 76.97 | 63.92 |\n| **Qwen3-Embedding-0.6B** | 0.6B   | 66.33      | 67.45      | 71.40  | 68.74  | 76.42       | 62.58   | 71.03 | 54.52 |\n| **Qwen3-Embedding-4B**   | 4B     | 72.27      | 73.51      | 75.46  | 77.89  | 83.34       | 66.05   | 77.03 | 61.26 |\n| **Qwen3-Embedding-8B**   | 8B     | 73.84      | 75.00      | 76.97  | 80.08  | 84.23       | 66.99   | 78.21 | 63.53 |\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n```', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":595776512,"storage_bytes":5993775548,"files_count":12,"spaces_count":68,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in message.content %}\n                {%- set content = message.content.split(''</think>'')[-1].lstrip(''\\n'') %}\n                {%- set reasoning_content = message.content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3-Embedding","source_url":"https://github.com/QwenLM/Qwen3-Embedding"},{"type":"based_on_paper","target_id":"arxiv:2506.05176","source_url":"https://arxiv.org/abs/2506.05176"}]', NULL, 'Apache-2.0', 'approved', 78.9, '8943aadb4e812643fdc48a7768fa5cd2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-shibing624-text2vec-base-chinese', 'huggingface--shibing624--text2vec-base-chinese', 'text2vec-base-chinese', 'shibing624', '--- license: apache-2.0 pipeline_tag: sentence-similarity tags: - Sentence Transformers - sentence-similarity - sentence-transformers datasets: - shibing624/nli_zh language: - zh library_name: sentence-transformers --- This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search. For an automated evaluation of this model, see the *Evaluat...', '["sentence-transformers","pytorch","onnx","safetensors","openvino","bert","sentence transformers","sentence-similarity","zh","dataset:shibing624/nli_zh","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'sentence-similarity', 765, 479992, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/shibing624/text2vec-base-chinese","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\npipeline_tag: sentence-similarity\ntags:\n- Sentence Transformers\n- sentence-similarity\n- sentence-transformers\ndatasets:\n- shibing624/nli_zh\nlanguage:\n- zh\nlibrary_name: sentence-transformers\n---\n\n\n# shibing624/text2vec-base-chinese\nThis is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.\n\nIt maps sentences to a 768 dimensional dense vector space and can be used for tasks \nlike sentence embeddings, text matching or semantic search.\n\n\n## Evaluation\nFor an automated evaluation of this model, see the *Evaluation Benchmark*: [text2vec](https://github.com/shibing624/text2vec)\n\n- chinese text matching taskÔºö\n\n| Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |\n|:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|\n| Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |\n| SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |\n| Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |\n| CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |\n| CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  |    63.08  | 3066  |\n| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 4004  |\n\n\nËØ¥ÊòéÔºö\n- ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞\n- `shibing624/text2vec-base-chinese`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`hfl/chinese-macbert-base`Âú®‰∏≠ÊñáSTS-BÊï∞ÊçÆËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáSTS-BÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠ÊñáÈÄöÁî®ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\n- `shibing624/text2vec-base-chinese-sentence`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2s(Âè•Â≠êvsÂè•Â≠ê)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\n- `shibing624/text2vec-base-chinese-paraphrase`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ÔºåÊï∞ÊçÆÈõÜÁõ∏ÂØπ‰∫é[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)Âä†ÂÖ•‰∫Üs2p(sentence to paraphrase)Êï∞ÊçÆÔºåÂº∫Âåñ‰∫ÜÂÖ∂ÈïøÊñáÊú¨ÁöÑË°®ÂæÅËÉΩÂäõÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞SOTAÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2p(Âè•Â≠êvsÊÆµËêΩ)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\n- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBERTËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â\n- `w2v-light-tencent-chinese`ÊòØËÖæËÆØËØçÂêëÈáèÁöÑWord2VecÊ®°ÂûãÔºåCPUÂä†ËΩΩ‰ΩøÁî®ÔºåÈÄÇÁî®‰∫é‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÁº∫Â∞ëÊï∞ÊçÆÁöÑÂÜ∑ÂêØÂä®ÊÉÖÂÜµ\n\n## Usage (text2vec)\nUsing this model becomes easy when you have [text2vec](https://github.com/shibing624/text2vec) installed:\n\n```\npip install -U text2vec\n```\n\nThen you can use the model like this:\n\n```python\nfrom text2vec import SentenceModel\nsentences = [''Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'', ''Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°'']\n\nmodel = SentenceModel(''shibing624/text2vec-base-chinese'')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nInstall transformers:\n```\npip install transformers\n```\n\nThen load model and predict:\n```python\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# Load model from HuggingFace Hub\ntokenizer = BertTokenizer.from_pretrained(''shibing624/text2vec-base-chinese'')\nmodel = BertModel.from_pretrained(''shibing624/text2vec-base-chinese'')\nsentences = [''Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'', ''Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°'']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n## Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```\npip install -U sentence-transformers\n```\n\nThen load model and predict:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer("shibing624/text2vec-base-chinese")\nsentences = [''Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'', ''Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°'']\n\nsentence_embeddings = m.encode(sentences)\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n## Model speed up\n\n\n| Model                                                                                                                        | ATEC              | BQ                | LCQMC            | PAWSX            | STSB             |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------|-------------------|------------------|------------------|------------------|\n| shibing624/text2vec-base-chinese (fp32, baseline)                                                                            | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (onnx-O4, [#29](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/29))    | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (ov, [#27](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/27))         | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (ov-qint8, [#30](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/30))   | 0.30778 (-3.60%)  | 0.43474 (+1.88%)  | 0.69620 (-0.77%) | 0.16662 (-3.20%) | 0.79396 (+0.13%) |\n\nIn short:\n1. ‚úÖ shibing624/text2vec-base-chinese (onnx-O4), ONNX Optimized to [O4](https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/optimization) does not reduce performance, but gives a [~2x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on GPU.\n2. ‚úÖ shibing624/text2vec-base-chinese (ov), OpenVINO does not reduce performance, but gives a 1.12x speedup on CPU.\n3. üü° shibing624/text2vec-base-chinese (ov-qint8), int8 quantization with OV incurs a small performance hit on some tasks, and a tiny performance gain on others, when quantizing with [Chinese STSB](https://huggingface.co/datasets/PhilipMay/stsb_multi_mt). Additionally, it results in a [4.78x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on CPU.\n\n- usage: shibing624/text2vec-base-chinese (onnx-O4), for gpu\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    "shibing624/text2vec-base-chinese",\n    backend="onnx",\n    model_kwargs={"file_name": "model_O4.onnx"},\n)\nembeddings = model.encode(["Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°", "Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°", "‰Ω†ÊòØË∞Å"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n\n- usage: shibing624/text2vec-base-chinese (ov), for cpu\n```python\n# pip install ''optimum[openvino]''\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    "shibing624/text2vec-base-chinese",\n    backend="openvino",\n)\n\nembeddings = model.encode(["Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°", "Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°", "‰Ω†ÊòØË∞Å"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n- usage: shibing624/text2vec-base-chinese (ov-qint8), for cpu\n```python\n# pip install optimum\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    "shibing624/text2vec-base-chinese",\n    backend="onnx",\n    model_kwargs={"file_name": "model_qint8_avx512_vnni.onnx"},\n)\nembeddings = model.encode(["Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°", "Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°", "‰Ω†ÊòØË∞Å"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n\n## Full Model Architecture\n```\nCoSENT(\n  (0): Transformer({''max_seq_length'': 128, ''do_lower_case'': False}) with Transformer model: BertModel \n  (1): Pooling({''word_embedding_dimension'': 768, ''pooling_mode_mean_tokens'': True})\n)\n```\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`hfl/chinese-macbert-base`](https://huggingface.co/hfl/chinese-macbert-base) model. \nPlease refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each \npossible sentence pairs from the batch.\nWe then apply the rank loss by comparing with true pairs and false pairs.\n\n#### Hyper parameters\n\n- training dataset: https://huggingface.co/datasets/shibing624/nli_zh\n- max_seq_length: 128\n- best epoch: 5\n- sentence embedding dim: 768\n\n\n\n## Citing & Authors\nThis model was trained by [text2vec](https://github.com/shibing624/text2vec). \n        \nIf you find this model helpful, feel free to cite:\n```bibtex \n@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":102268160,"storage_bytes":2040942336,"files_count":22,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"unk_token":"[UNK]","sep_token":"[SEP]","pad_token":"[PAD]","cls_token":"[CLS]","mask_token":"[MASK]"}}}', '[]', '[{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:UKPLab:sentence-transformers","source_url":"https://github.com/UKPLab/sentence-transformers"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec},","source_url":"https://github.com/shibing624/text2vec},"}]', NULL, 'Apache-2.0', 'approved', 78.8, '865e8389d8606a4d1fb1da99d3bdf79f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-WizardLMTeam-WizardCoder-15B-V1.0', 'huggingface--wizardlmteam--wizardcoder-15b-v1.0', 'WizardCoder-15B-V1.0', 'WizardLMTeam', '--- license: bigscience-openrail-m metrics: - code_eval library_name: transformers tags: - code model-index: - name: WizardCoder results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 0.573 verified: false --- <p style="font-size:28px;" align="center"> üè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p> <p align="center"> <p align="center"> ü§ó <a href="https://huggingface.co/WizardLM" target="_...', '["transformers","pytorch","gpt_bigcode","text-generation","code","arxiv:2304.12244","arxiv:2306.08568","arxiv:2308.09583","license:bigscience-openrail-m","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 762, 566, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/WizardLMTeam/WizardCoder-15B-V1.0","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: bigscience-openrail-m\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: WizardCoder\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.573\n      verified: false\n---\n \n## WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n<p style="font-size:28px;" align="center">\nüè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p>\n<p align="center">\n<p align="center">\nü§ó <a href="https://huggingface.co/WizardLM" target="_blank">HF Repo</a>  ‚Ä¢üê± <a href="https://github.com/nlpxucan/WizardLM" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/WizardLM_AI" target="_blank">Twitter</a> </p>\n<p align="center">\n üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a>  ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>   ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>  <br>\n</p>\n<p align="center">\n    üëã Join our <a href="https://discord.gg/VZjjHtWrKs" target="_blank">Discord</a>\n</p>\n\n## News\n\n[2024/01/04] üî• We released **WizardCoder-33B-V1.1**  trained from deepseek-coder-33b-base, the **SOTA OSS Code LLM** on [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html), achieves **79.9 pass@1** on HumanEval, **73.2 pass@1** on HumanEval-Plus, **78.9 pass@1** on MBPP, and **66.9 pass@1** on MBPP-Plus.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** outperforms **ChatGPT 3.5**, **Gemini Pro**, and **DeepSeek-Coder-33B-instruct** on HumanEval and HumanEval-Plus pass@1.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** is comparable with **ChatGPT 3.5**, and surpasses **Gemini Pro** on MBPP and MBPP-Plus pass@1.\n\n|  Model  |  Checkpoint  | Paper    | HumanEval  |   HumanEval+ | MBPP | MBPP+ | License |\n| ----- |------| ---- |------|-------| ----- |  ----- |----- | \n|  GPT-4-Turbo (Nov 2023)  | - | - | 85.4  | 81.7 | 83.0 | 70.7 |-|\n|  GPT-4 (May 2023)  | - | - | 88.4  | 76.8 | - | - |-|\n|  GPT-3.5-Turbo (Nov 2023)  | - | - | 72.6  | 65.9 | 81.7 | 69.4 |-|\n|  Gemini Pro  | - | - | 63.4  | 55.5 | 72.9 | 57.9 |-|\n|  DeepSeek-Coder-33B-instruct | - | - |  78.7 | 72.6 | 78.7 | 66.7 |-|\n|  **WizardCoder-33B-V1.1**  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-33B-V1.1" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  79.9  | 73.2 | 78.9 | 66.9 |  <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.1/resolve/main/LICENSE" target="_blank">MSFTResearch</a>  |\n|  WizardCoder-Python-34B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  73.2   | 64.6 | 73.2 | 59.9 |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-15B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  59.8   | 52.4 | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-Python-13B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  64.0   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-Python-7B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  55.5   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-3B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-3B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  34.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-1B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-1B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  23.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n\n<p align="center" width="100%">\n<a ><img src="https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/compare_sota.png" alt="WizardCoder" style="width: 96%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n- üî• [08/11/2023] We release **WizardMath** Models.\n- üî• Our **WizardMath-70B-V1.0** model slightly outperforms some closed-source LLMs on the GSM8K, including **ChatGPT 3.5**, **Claude Instant 1** and **PaLM 2 540B**.\n- üî• Our **WizardMath-70B-V1.0** model achieves  **81.6 pass@1** on the [GSM8k Benchmarks](https://github.com/openai/grade-school-math), which is **24.8** points higher than the SOTA open-source LLM.\n- üî• Our **WizardMath-70B-V1.0** model achieves  **22.7 pass@1** on the [MATH Benchmarks](https://github.com/hendrycks/math), which is **9.2** points higher than the SOTA open-source LLM.\n\n| Model | Checkpoint | Paper  | GSM8k | MATH  |Online Demo| License|\n| ----- |------| ---- |------|-------| ----- | ----- |\n| WizardMath-70B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-70B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **81.6**  |  **22.7**	|[Demo](http://47.103.63.15:50083/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a> |\n| WizardMath-13B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-13B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **63.9**  |  **14.0** |[Demo](http://47.103.63.15:50082/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 </a> |\n| WizardMath-7B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.0" target="_blank">HF Link</a>  |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| 	 **54.9**  |  **10.7** | [Demo](http://47.103.63.15:50080/)|  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a>|    \n\n\n<font size=4>\n    \n| <sup>Model</sup> | <sup>Checkpoint</sup> | <sup>Paper</sup> |<sup>MT-Bench</sup> | <sup>AlpacaEval</sup> | <sup>WizardEval</sup> | <sup>HumanEval</sup>  | <sup>License</sup>|\n| ----- |------| ---- |------|-------| ----- | ----- | ----- |\n| <sup>WizardLM-13B-V1.2</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank">HF Link</a> </sup>|  | <sup>7.06</sup> | <sup>89.17%</sup>	 | <sup>101.4% </sup>|<sup>36.6  pass@1</sup>|<sup> <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.1</sup> |<sup> ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.1" target="_blank">HF Link</a> </sup> |  | <sup>6.76</sup>  |<sup>86.32%</sup>	 | <sup>99.3% </sup> |<sup>25.0  pass@1</sup>| <sup>Non-commercial</sup>|\n| <sup>WizardLM-30B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-30B-V1.0" target="_blank">HF Link</a></sup>  | | <sup>7.01</sup> |  |  <sup>97.8% </sup> | <sup>37.8  pass@1</sup>| <sup>Non-commercial</sup> |\n| <sup>WizardLM-13B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" target="_blank">HF Link</a> </sup> |  | <sup>6.35</sup> | <sup>75.31%</sup> |  <sup>89.1% </sup> |<sup> 24.0 pass@1 </sup> | <sup>Non-commercial</sup>|\n| <sup>WizardLM-7B-V1.0 </sup>|  <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0" target="_blank">HF Link</a> </sup> |<sup> üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a> </sup>|  |  |  <sup>78.0% </sup> |<sup>19.1 pass@1 </sup>|<sup> Non-commercial</sup>|\n</font>\n\n\n\n\n\n# WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n\nTo develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set.\n\n## News\n\n- üî• Our **WizardCoder-15B-v1.0** model achieves the **57.3 pass@1** on the [HumanEval Benchmarks](https://github.com/openai/human-eval), which is **22.3** points higher than the SOTA open-source Code LLMs.\n- üî• We released **WizardCoder-15B-v1.0** trained with **78k** evolved code instructions. Please checkout the [Model Weights](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0), and [Paper]().\n- &#x1F4E3; Please refer to our Twitter account https://twitter.com/WizardLM_AI and HuggingFace Repo https://huggingface.co/WizardLM . We will use them to announce any new release at the 1st time. \n\n\n## Comparing WizardCoder with the Closed-Source Models.\n\n\nüî• The following figure shows that our **WizardCoder attains the third position in this benchmark**, surpassing Claude-Plus (59.8 vs. 53.0) and Bard (59.8 vs. 44.5). Notably, our model exhibits a substantially smaller size compared to these models.\n\n<p align="center" width="100%">\n<a ><img src="https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/pass1.png" alt="WizardCoder" style="width: 86%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n‚ùó**Note: In this study, we copy the scores for HumanEval and HumanEval+ from the [LLM-Humaneval-Benchmarks](https://github.com/my-other-github-account/llm-humaneval-benchmarks). Notably, all the mentioned models generate code solutions for each problem utilizing a **single attempt**, and the resulting pass rate percentage is reported. Our **WizardCoder** generates answers using greedy decoding and tests with the same [code](https://github.com/evalplus/evalplus).**\n\n## Comparing WizardCoder with the Open-Source Models.\n\nThe following table clearly demonstrates that our **WizardCoder** exhibits a substantial performance advantage over all the open-source models. ‚ùó**If you are confused with the different scores of our model (57.3 and 59.8), please check the Notes.**\n\n\n| Model            | HumanEval Pass@1 | MBPP Pass@1 |\n|------------------|------------------|-------------|\n| CodeGen-16B-Multi| 18.3             |20.9         |\n| CodeGeeX         | 22.9             |24.4         |\n| LLaMA-33B        | 21.7             |30.2         |\n| LLaMA-65B        | 23.7             |37.7         |\n| PaLM-540B        | 26.2             |36.8         |\n| PaLM-Coder-540B  | 36.0             |47.0         |\n| PaLM 2-S         | 37.6             |50.0         |\n| CodeGen-16B-Mono | 29.3             |35.3         |\n| Code-Cushman-001 | 33.5             |45.9         |\n| StarCoder-15B    | 33.6             |43.6*        |\n| InstructCodeT5+  | 35.0             |--           |\n| WizardLM-30B  1.0| 37.8             |--           |\n| WizardCoder-15B  1.0 | **57.3**     |**51.8**     |\n\n\n‚ùó**Note: The reproduced result of StarCoder on MBPP.**\n\n‚ùó**Note: The above table conducts a comprehensive comparison of our **WizardCoder** with other models on the HumanEval and MBPP benchmarks. We adhere to the approach outlined in previous studies by generating **20 samples** for each problem to estimate the pass@1 score and evaluate with the same [code](https://github.com/openai/human-eval/tree/master). The scores of GPT4 and GPT3.5 reported by [OpenAI](https://openai.com/research/gpt-4) are 67.0 and 48.1 (maybe these are the early version GPT4&3.5).**\n\n## Call for Feedbacks\nWe welcome everyone to use your professional and difficult instructions to evaluate WizardCoder, and show us examples of poor performance and your suggestions in the [issue discussion](https://github.com/nlpxucan/WizardLM/issues) area. We are focusing on improving the Evol-Instruct now and hope to relieve existing weaknesses and issues in the the next version of WizardCoder. After that, we will open the code and pipeline of up-to-date Evol-Instruct algorithm and work with you together to improve it.\n\n\n## Contents\n\n1. [Online Demo](#online-demo)\n\n2. [Fine-tuning](#fine-tuning)\n\n3. [Inference](#inference)\n\n4. [Evaluation](#evaluation)\n\n5. [Citation](#citation)\n\n6. [Disclaimer](#disclaimer)\n\n## Online Demo\n\nWe will provide our latest models for you to try for as long as possible. If you find a link is not working, please try another one. At the same time, please try as many **real-world** and **challenging** code-related problems that you encounter in your work and life as possible. We will continue to evolve our models with your feedbacks.\n\n\n\n## Fine-tuning\n\nWe fine-tune WizardCoder using the modified code `train.py` from [Llama-X](https://github.com/AetherCortex/Llama-X).\nWe fine-tune StarCoder-15B with the following hyperparameters:\n\n| Hyperparameter | StarCoder-15B |\n|----------------|---------------|\n| Batch size     | 512           |\n| Learning rate  | 2e-5          |\n| Epochs         | 3             |\n| Max length     | 2048          |\n| Warmup step    | 30            |\n| LR scheduler   | cosine        |\n\nTo reproduce our fine-tuning of WizardCoder, please follow the following steps:\n1. According to the instructions of [Llama-X](https://github.com/AetherCortex/Llama-X), install the environment, download the training code, and deploy. (Note: `deepspeed==0.9.2` and `transformers==4.29.2`)\n2. Replace the `train.py` with the `train_wizardcoder.py` in our repo (`src/train_wizardcoder.py`)\n3. Login Huggingface:\n```bash\nhuggingface-cli login\n```\n4. Execute the following training command:\n```bash\ndeepspeed train_wizardcoder.py \\n    --model_name_or_path "bigcode/starcoder" \\n    --data_path "/your/path/to/code_instruction_data.json" \\n    --output_dir "/your/path/to/ckpt" \\n    --num_train_epochs 3 \\n    --model_max_length 2048 \\n    --per_device_train_batch_size 16 \\n    --per_device_eval_batch_size 1 \\n    --gradient_accumulation_steps 4 \\n    --evaluation_strategy "no" \\n    --save_strategy "steps" \\n    --save_steps 50 \\n    --save_total_limit 2 \\n    --learning_rate 2e-5 \\n    --warmup_steps 30 \\n    --logging_steps 2 \\n    --lr_scheduler_type "cosine" \\n    --report_to "tensorboard" \\n    --gradient_checkpointing True \\n    --deepspeed configs/deepspeed_config.json \\n    --fp16 True\n```\n\n## Inference\n\nWe provide the decoding script for WizardCoder, which reads a input file and generates corresponding responses for each sample, and finally consolidates them into an output file.\n\nYou can specify `base_model`, `input_data_path` and `output_data_path` in `src\inference_wizardcoder.py` to set the decoding model, path of input file and path of output file.\n\n```bash\npip install jsonlines\n```\n\nThe decoding command is:\n```\npython src\inference_wizardcoder.py \\n    --base_model "/your/path/to/ckpt" \\n    --input_data_path "/your/path/to/input/data.jsonl" \\n    --output_data_path "/your/path/to/output/result.jsonl"\n```\n\nThe format of `data.jsonl` should be:\n```\n{"idx": 11, "Instruction": "Write a Python code to count 1 to 10."}\n{"idx": 12, "Instruction": "Write a Jave code to sum 1 to 10."}\n```\n\nThe prompt for our WizardCoder in `src\inference_wizardcoder.py` is:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n```\n\n## Evaluation\n\nWe provide the evaluation script on HumanEval for WizardCoder.\n\n1. According to the instructions of [HumanEval](https://github.com/openai/human-eval), install the environment.\n2. Run the following script to generate the answer.\n```bash\nmodel="/path/to/your/model"\ntemp=0.2\nmax_len=2048\npred_num=200\nnum_seqs_per_iter=2\n\noutput_path=preds/T${temp}_N${pred_num}\n\nmkdir -p ${output_path}\necho ''Output path: ''$output_path\necho ''Model to eval: ''$model\n\n# 164 problems, 21 per GPU if GPU=8\nindex=0\ngpu_num=8\nfor ((i = 0; i < $gpu_num; i++)); do\n  start_index=$((i * 21))\n  end_index=$(((i + 1) * 21))\n\n  gpu=$((i))\n  echo ''Running process #'' ${i} ''from'' $start_index ''to'' $end_index ''on GPU'' ${gpu}\n  ((index++))\n  (\n    CUDA_VISIBLE_DEVICES=$gpu python humaneval_gen.py --model ${model} \\n      --start_index ${start_index} --end_index ${end_index} --temperature ${temp} \\n      --num_seqs_per_iter ${num_seqs_per_iter} --N ${pred_num} --max_len ${max_len} --output_path ${output_path}\n  ) &\n  if (($index % $gpu_num == 0)); then wait; fi\ndone\n```\n3. Run the post processing code `src/process_humaneval.py` to collect the code completions from all answer files.\n```bash\noutput_path=preds/T${temp}_N${pred_num}\n\necho ''Output path: ''$output_path\npython process_humaneval.py --path ${output_path} --out_path ${output_path}.jsonl --add_prompt\n\nevaluate_functional_correctness ${output_path}.jsonl\n```\n\n## Citation\n\nPlease cite the repo if you use the data, method or code in this repo.\n\n```\n@article{luo2023wizardcoder,\n  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},\n  journal={arXiv preprint arXiv:2306.08568},\n  year={2023}\n}\n```\n## Disclaimer\n\nWizardCoder model follows the same license as StarCoder. The content produced by any version of WizardCoder is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project. This project does not accept any legal liability for the content of the model output, nor does it assume responsibility for any losses incurred due to the use of associated resources and output results.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":62070055381,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GPTBigCodeForCausalLM"],"model_type":"gpt_bigcode","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:nlpxucan:WizardLM\"","source_url":"https://github.com/nlpxucan/WizardLM\""},{"type":"has_code","target_id":"github:openai:grade-school-math","source_url":"https://github.com/openai/grade-school-math"},{"type":"has_code","target_id":"github:hendrycks:math","source_url":"https://github.com/hendrycks/math"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:my-other-github-account:llm-humaneval-benchmarks","source_url":"https://github.com/my-other-github-account/llm-humaneval-benchmarks"},{"type":"has_code","target_id":"github:evalplus:evalplus","source_url":"https://github.com/evalplus/evalplus"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:nlpxucan:WizardLM","source_url":"https://github.com/nlpxucan/WizardLM"},{"type":"has_code","target_id":"github:AetherCortex:Llama-X","source_url":"https://github.com/AetherCortex/Llama-X"},{"type":"has_code","target_id":"github:AetherCortex:Llama-X","source_url":"https://github.com/AetherCortex/Llama-X"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"based_on_paper","target_id":"arxiv:2304.12244","source_url":"https://arxiv.org/abs/2304.12244"},{"type":"based_on_paper","target_id":"arxiv:2306.08568","source_url":"https://arxiv.org/abs/2306.08568"},{"type":"based_on_paper","target_id":"arxiv:2308.09583","source_url":"https://arxiv.org/abs/2308.09583"}]', NULL, 'BigScience-OpenRAIL-M', 'approved', 78.8, '1a87111d92467fe6ff77067c5ba98e13', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-latent-consistency-lcm-lora-sdxl', 'huggingface--latent-consistency--lcm-lora-sdxl', 'lcm-lora-sdxl', 'latent-consistency', '--- library_name: diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 tags: - lora - text-to-image license: openrail++ inference: false --- Latent Consistency Model (LCM) LoRA was proposed in LCM-LoRA: A universal Stable-Diffusion Acceleration Module by *Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.* It is a distilled consistency adapter for [](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) that allows to reduce the number of inference steps to only between...', '["diffusers","lora","text-to-image","arxiv:2311.05556","license:openrail++","region:us"]', 'text-to-image', 762, 57269, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/latent-consistency/lcm-lora-sdxl","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\ntags:\n- lora\n- text-to-image\nlicense: openrail++\ninference: false\n---\n\n# Latent Consistency Model (LCM) LoRA: SDXL\n\nLatent Consistency Model (LCM) LoRA was proposed in [LCM-LoRA: A universal Stable-Diffusion Acceleration Module](https://arxiv.org/abs/2311.05556) \nby *Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.*\n\nIt is a distilled consistency adapter for [`stable-diffusion-xl-base-1.0`](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) that allows\nto reduce the number of inference steps to only between **2 - 8 steps**.\n\n| Model                                                                      | Params / M | \n|----------------------------------------------------------------------------|------------|\n| [lcm-lora-sdv1-5](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5)   | 67.5        |\n| [lcm-lora-ssd-1b](https://huggingface.co/latent-consistency/lcm-lora-ssd-1b)   | 105        |\n| [**lcm-lora-sdxl**](https://huggingface.co/latent-consistency/lcm-lora-sdxl) | **197M**  |\n\n## Usage\n\nLCM-LoRA is supported in ü§ó Hugging Face Diffusers library from version v0.23.0 onwards. To run the model, first \ninstall the latest version of the Diffusers library as well as `peft`, `accelerate` and `transformers`.\naudio dataset from the Hugging Face Hub:\n\n```bash\npip install --upgrade pip\npip install --upgrade diffusers transformers accelerate peft\n```\n\n***Note: For detailed usage examples we recommend you to check out our official [LCM-LoRA docs](https://huggingface.co/docs/diffusers/main/en/using-diffusers/inference_with_lcm_lora)***\n\n### Text-to-Image\n\nThe adapter can be loaded with it''s base model `stabilityai/stable-diffusion-xl-base-1.0`. Next, the scheduler needs to be changed to [`LCMScheduler`](https://huggingface.co/docs/diffusers/v0.22.3/en/api/schedulers/lcm#diffusers.LCMScheduler) and we can reduce the number of inference steps to just 2 to 8 steps.\nPlease make sure to either disable `guidance_scale` or use values between 1.0 and 2.0.\n\n```python\nimport torch\nfrom diffusers import LCMScheduler, AutoPipelineForText2Image\n\nmodel_id = "stabilityai/stable-diffusion-xl-base-1.0"\nadapter_id = "latent-consistency/lcm-lora-sdxl"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.to("cuda")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\n\nprompt = "Self-portrait oil painting, a beautiful cyborg with golden hair, 8k"\n\n# disable guidance_scale by passing 0\nimage = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n```\n\n![](./image.png)\n\n### Inpainting\n\nLCM-LoRA can be used for inpainting as well. \n\n```python\nimport torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    "diffusers/stable-diffusion-xl-1.0-inpainting-0.1",\n    torch_dtype=torch.float16,\n    variant="fp16",\n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")\npipe.fuse_lora()\n\n# load base and mask image\ninit_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png").resize((1024, 1024))\nmask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png").resize((1024, 1024))\n\nprompt = "a castle on top of a mountain, highly detailed, 8k"\ngenerator = torch.manual_seed(42)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=5,\n    guidance_scale=4,\n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_inpainting.png)\n\n\n## Combine with styled LoRAs\n\nLCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we''ll use the LCM-LoRA with the [papercut LoRA](TheLastBen/Papercut_SDXL). \nTo learn more about how to combine LoRAs, refer to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).\n\n```python\nimport torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0",\n    variant="fp16",\n    torch_dtype=torch.float16\n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LoRAs\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl", adapter_name="lcm")\npipe.load_lora_weights("TheLastBen/Papercut_SDXL", weight_name="papercut.safetensors", adapter_name="papercut")\n\n# Combine LoRAs\npipe.set_adapters(["lcm", "papercut"], adapter_weights=[1.0, 0.8])\n\nprompt = "papercut, a cute fox"\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png)\n\n### ControlNet\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"\n).resize((1024, 1024))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained("diffusers/controlnet-canny-sdxl-1.0-small", torch_dtype=torch.float16, variant="fp16")\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant="fp16"\n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")\npipe.fuse_lora()\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    "picture of the mona lisa",\n    image=canny_image,\n    num_inference_steps=5,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.5,\n    cross_attention_kwargs={"scale": 1},\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_controlnet.png)\n\n\n<Tip>\nThe inference parameters in this example might not work for all examples, so we recommend you to try different values for `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose the best one. \n</Tip>\n\n### T2I Adapter\n\nThis example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0) and SDXL.\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# Prepare image\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    "https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1024))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained("TencentARC/t2i-adapter-canny-sdxl-1.0", torch_dtype=torch.float16, varient="fp16").to("cuda")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0", \n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant="fp16", \n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")\n\nprompt = "Mystical fairy in real, magic, 4k picture, high quality"\nnegative_prompt = "extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5, \n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2iadapter.png)\n\n\n## Speed Benchmark\n\nTODO\n\n## Training\n\nTODO\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":2761540034,"files_count":4,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2311.05556","source_url":"https://arxiv.org/abs/2311.05556"}]', NULL, 'OpenRAIL++', 'approved', 63.8, 'b42a74f884f906a10cedd69df8775b90', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-7B', 'huggingface--deepseek-ai--deepseek-r1-distill-qwen-7b', 'DeepSeek-R1-Distill-Qwen-7B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen2","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 762, 794795, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":15231404337,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 98.8, 'a69d43aaaa6ca87c00d3b1c409ddb6b7', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-7B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-7B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-GanymedeNil-text2vec-large-chinese', 'huggingface--ganymedenil--text2vec-large-chinese', 'text2vec-large-chinese', 'GanymedeNil', '--- license: apache-2.0 language: - zh pipeline_tag: sentence-similarity tags: - text2vec - feature-extraction - sentence-similarity - transformers --- Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged„ÄÇ News 2024-06-25 text2vec-large-chinese onnxruntime version. Talk to me: https://twitter.com/GanymedeNil', '["transformers","pytorch","safetensors","bert","feature-extraction","text2vec","sentence-similarity","zh","license:apache-2.0","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'sentence-similarity', 760, 3700, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/GanymedeNil/text2vec-large-chinese","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- zh\npipeline_tag: sentence-similarity\ntags:\n- text2vec\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\nBased on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged„ÄÇ\n\nNews\n\n2024-06-25 [text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese-onnx) onnxruntime version.\n\nTalk to me: https://twitter.com/GanymedeNil', '{"pipeline_tag":"sentence-similarity","library_name":"transformers","framework":"transformers","params":325522944,"storage_bytes":2604361841,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 38.8, '8ba399022290a540fc5577d5f9d66895', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-t5-t5-base', 'huggingface--google-t5--t5-base', 't5-base', 'google-t5', '--- pipeline_tag: translation language: - en - fr - ro - de datasets: - c4 tags: - summarization - translation license: apache-2.0 --- !model image 1. Model Details 2. Uses 3. Bias, Risks, and Limitations 4. Training Details 5. Evaluation 6. Environmental Impact 7. Citation 8. Model Card Authors 9. How To Get Started With the Model The developers of the Text-To-Text Transfer Transformer (T5) write: > With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the inpu...', '["transformers","pytorch","tf","jax","rust","safetensors","t5","text2text-generation","summarization","translation","en","fr","ro","de","dataset:c4","arxiv:1805.12471","arxiv:1708.00055","arxiv:1704.05426","arxiv:1606.05250","arxiv:1808.09121","arxiv:1810.12885","arxiv:1905.10044","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'translation', 757, 2240237, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google-t5/t5-base","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\npipeline_tag: translation\nlanguage:\n- en\n- fr\n- ro\n- de\ndatasets:\n- c4\ntags:\n- summarization\n- translation\nlicense: apache-2.0\n---\n\n# Model Card for T5 Base\n\n![model image](https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67)\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training Details](#training-details)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Citation](#citation)\n8. [Model Card Authors](#model-card-authors)\n9. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nThe developers of the Text-To-Text Transfer Transformer (T5) [write](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html): \n\n> With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n\nT5-Base is the checkpoint with 220 million parameters. \n\n- **Developed by:** Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See [associated paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) and [GitHub repo](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)\n- **Model type:** Language model\n- **Language(s) (NLP):** English, French, Romanian, German\n- **License:** Apache 2.0\n- **Related Models:** [All T5 Checkpoints](https://huggingface.co/models?search=t5)\n- **Resources for more information:**\n  - [Research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)\n  - [Google''s T5 Blog Post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) \n  - [GitHub Repo](https://github.com/google-research/text-to-text-transfer-transformer)\n  - [Hugging Face T5 Docs](https://huggingface.co/docs/transformers/model_doc/t5)\n  \n# Uses\n\n## Direct Use and Downstream Use\n\nThe developers write in a [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) that the model: \n\n> Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n\nSee the [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nMore information needed.\n\n## Recommendations\n\nMore information needed.\n\n# Training Details\n\n## Training Data\n\nThe model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5.\n\nThe model was pre-trained on a on a **multi-task mixture of unsupervised (1.) and supervised tasks (2.)**.\nThereby, the following datasets were being used for (1.) and (2.):\n\n1. **Datasets used for Unsupervised denoising objective**:\n\n- [C4](https://huggingface.co/datasets/c4)\n- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)\n\n\n2. **Datasets used for Supervised text-to-text language modeling objective**\n\n- Sentence acceptability judgment\n  - CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)\n- Sentiment analysis \n  - SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n- Paraphrasing/sentence similarity\n  - MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)\n  - STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)\n  - QQP [Iyer et al., 2017](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)\n- Natural language inference\n  - MNLI [Williams et al., 2017](https://arxiv.org/abs/1704.05426)\n  - QNLI [Rajpurkar et al.,2016](https://arxiv.org/abs/1606.05250)\n  - RTE [Dagan et al., 2005](https://link.springer.com/chapter/10.1007/11736790_9) \n  - CB [De Marneff et al., 2019](https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf)\n- Sentence completion\n  - COPA [Roemmele et al., 2011](https://www.researchgate.net/publication/221251392_Choice_of_Plausible_Alternatives_An_Evaluation_of_Commonsense_Causal_Reasoning)\n- Word sense disambiguation\n  - WIC [Pilehvar and Camacho-Collados, 2018](https://arxiv.org/abs/1808.09121)\n- Question answering\n  - MultiRC [Khashabi et al., 2018](https://aclanthology.org/N18-1023)\n  - ReCoRD [Zhang et al., 2018](https://arxiv.org/abs/1810.12885)\n  - BoolQ [Clark et al., 2019](https://arxiv.org/abs/1905.10044)\n\n## Training Procedure\n\nIn their [abstract](https://jmlr.org/papers/volume21/20-074/20-074.pdf), the model developers write: \n\n> In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. \n\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details.\n\n## Results \n\nFor full results for T5-Base, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n```\n\n**APA:**\n- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained("t5-base")\nmodel = T5Model.from_pretrained("t5-base")\n\ninput_ids = tokenizer(\n    "Studies have been shown that owning a dog is good for you", return_tensors="pt"\n).input_ids  # Batch size 1\ndecoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1\n\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nSee the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more examples.\n</details>', '{"pipeline_tag":"translation","library_name":"transformers","framework":"transformers","params":222903936,"storage_bytes":6793551855,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5"}}', '[]', '[{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer"},{"type":"based_on_paper","target_id":"arxiv:1805.12471","source_url":"https://arxiv.org/abs/1805.12471"},{"type":"based_on_paper","target_id":"arxiv:1708.00055","source_url":"https://arxiv.org/abs/1708.00055"},{"type":"based_on_paper","target_id":"arxiv:1704.05426","source_url":"https://arxiv.org/abs/1704.05426"},{"type":"based_on_paper","target_id":"arxiv:1606.05250","source_url":"https://arxiv.org/abs/1606.05250"},{"type":"based_on_paper","target_id":"arxiv:1808.09121","source_url":"https://arxiv.org/abs/1808.09121"},{"type":"based_on_paper","target_id":"arxiv:1810.12885","source_url":"https://arxiv.org/abs/1810.12885"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 63.8, '026b2f166c5be0d63c5c30cd2149d786', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nitrosocke-Arcane-Diffusion', 'huggingface--nitrosocke--arcane-diffusion', 'Arcane-Diffusion', 'nitrosocke', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image --- This is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane. Use the tokens **_arcane style_** in your prompts for the effect. **If you enjoy my work, please consider supporting me** This model can be used just like any other Stable Diffusion model. For more information, please have a look at the Stable Diffusion. You can also export the model to ONNX, MPS and/or [FLAX/JAX](). We also s...', '["diffusers","stable-diffusion","text-to-image","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 756, 908, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nitrosocke/Arcane-Diffusion","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\n---\n# Arcane Diffusion\nThis is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane.\nUse the tokens **_arcane style_** in your prompts for the effect.\n\n**If you enjoy my work, please consider supporting me** \n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\n#!pip install diffusers transformers scipy torch\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "nitrosocke/Arcane-Diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "arcane style, a magical princess with golden hair"\nimage = pipe(prompt).images[0]\n\nimage.save("./magical_princess.png")\n```\n\n# Gradio & Colab\n\nWe also support a [Gradio](https://github.com/gradio-app/gradio) Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j5YvfMZoGdDGdj3O3xRU1m4ujKYsElZO?usp=sharing)\n\n![img](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/magical_princess.png)\n\n### Sample images from v3:\n![output Samples v3](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-v3-samples-01.jpg)\n![output Samples v3](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-v3-samples-02.jpg)\n### Sample images from the model:\n![output Samples](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-diffusion-output-images.jpg)\n### Sample images used for training:\n![Training Samples](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-diffusion-training-images.jpg)\n\n**Version 3** (arcane-diffusion-v3): This version uses the new _train-text-encoder_ setting and improves the quality and edibility of the model immensely. Trained on 95 images from the show in 8000 steps.\n\n**Version 2** (arcane-diffusion-v2): This uses the diffusers based dreambooth training and prior-preservation loss is way more effective. The diffusers where then converted with a script to a ckpt file in order to work with automatics repo.\nTraining was done with 5k steps for a direct comparison to v1 and results show that it needs more steps for a more prominent result. Version 3 will be tested with 11k steps.\n\n**Version 1** (arcane-diffusion-5k): This model was trained using _Unfrozen Model Textual Inversion_ utilizing the _Training with prior-preservation loss_ methods. There is still a slight shift towards the style, while not using the arcane token.\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":41619195807,"files_count":25,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 63.8, 'daa731ebde512f4fb1a37f5acc62e26d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-GLM-4.1V-9B-Thinking', 'huggingface--zai-org--glm-4.1v-9b-thinking', 'GLM-4.1V-9B-Thinking', 'zai-org', '--- license: mit language: - en - zh base_model: - zai-org/GLM-4-9B-0414 pipeline_tag: image-text-to-text library_name: transformers tags: - reasoning --- <div align="center"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/99c5eb6563236f0ff43605d91d107544da9863b2/resources/logo.svg width="40%"/> </div> <p align="center"> üìñ View the GLM-4.1V-9B-Thinking <a href="https://arxiv.org/abs/2507.01006" target="_blank">paper</a>. <br> üìç Using GLM-4.1V-9B-Thinking API at <a href...', '["transformers","safetensors","glm4v","any-to-any","reasoning","image-text-to-text","conversational","en","zh","arxiv:2507.01006","base_model:zai-org/glm-4-9b-0414","base_model:finetune:zai-org/glm-4-9b-0414","license:mit","endpoints_compatible","region:us"]', 'image-text-to-text', 755, 351286, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlanguage:\n- en\n- zh\nbase_model:\n- zai-org/GLM-4-9B-0414\npipeline_tag: image-text-to-text\nlibrary_name: transformers\ntags:\n- reasoning\n---\n\n# GLM-4.1V-9B-Thinking\n\n<div align="center">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/99c5eb6563236f0ff43605d91d107544da9863b2/resources/logo.svg width="40%"/>\n</div>\n<p align="center">\n    üìñ View the GLM-4.1V-9B-Thinking <a href="https://arxiv.org/abs/2507.01006" target="_blank">paper</a>.\n    <br>\n    üìç Using GLM-4.1V-9B-Thinking API at <a href="https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.1V-Thinking">Zhipu Foundation Model Open Platform</a>\n</p>\n\n\n## Model Introduction\n\nVision-Language Models (VLMs) have become foundational components of intelligent systems. As real-world AI tasks grow\nincreasingly complex, VLMs must evolve beyond basic multimodal perception to enhance their reasoning capabilities in\ncomplex tasks. This involves improving accuracy, comprehensiveness, and intelligence, enabling applications such as\ncomplex problem solving, long-context understanding, and multimodal agents.\n\nBased on the [GLM-4-9B-0414](https://github.com/zai-org/GLM-4) foundation model, we present the new open-source VLM model\n**GLM-4.1V-9B-Thinking**, designed to explore the upper limits of reasoning in vision-language models. By introducing\na "thinking paradigm" and leveraging reinforcement learning, the model significantly enhances its capabilities. It\nachieves state-of-the-art performance among 10B-parameter VLMs, matching or even surpassing the 72B-parameter\nQwen-2.5-VL-72B on 18 benchmark tasks. We are also open-sourcing the base model GLM-4.1V-9B-Base to\nsupport further research into the boundaries of VLM capabilities.\n\n![rl](https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/refs/heads/main/resources/rl.jpeg)\n\nCompared to the previous generation models CogVLM2 and the GLM-4V series, **GLM-4.1V-Thinking** offers the\nfollowing improvements:\n\n1. The first reasoning-focused model in the series, achieving world-leading performance not only in mathematics but also\n   across various sub-domains.\n2. Supports **64k** context length.\n3. Handles **arbitrary aspect ratios** and up to **4K** image resolution.\n4. Provides an open-source version supporting both **Chinese and English bilingual** usage.\n\n## Benchmark Performance\n\nBy incorporating the Chain-of-Thought reasoning paradigm, GLM-4.1V-9B-Thinking significantly improves answer accuracy,\nrichness, and interpretability. It comprehensively surpasses traditional non-reasoning visual models.\nOut of 28 benchmark tasks, it achieved the best performance among 10B-level models on 23 tasks,\nand even outperformed the 72B-parameter Qwen-2.5-VL-72B on 18 tasks.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/refs/heads/main/resources/bench.jpeg)\n\n## Quick Inference\n\nThis is a simple example of running single-image inference using the `transformers` library.  \nFirst, install the `transformers` library from source:\n\n```\npip install transformers>=4.57.1\n```\n\nThen, run the following code:\n\n```python\nfrom transformers import AutoProcessor, Glm4vForConditionalGeneration\nimport torch\n\nMODEL_PATH = "zai-org/GLM-4.1V-9B-Thinking"\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "url": "https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png"\n            },\n            {\n                "type": "text",\n                "text": "describe this image"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\nmodel = Glm4vForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors="pt"\n).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n```\n\nFor video reasoning, web demo deployment, and more code, please check\nour [GitHub](https://github.com/zai-org/GLM-V).', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":10292777472,"storage_bytes":20605610600,"files_count":15,"spaces_count":29,"gated":false,"private":false,"config":{"architectures":["Glm4vForConditionalGeneration"],"model_type":"glm4v","tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"[gMASK]<sop>\n{%- for msg in messages %}\n    {%- if msg.role == ''system'' %}\n<|system|>\n{{ msg.content }}\n    {%- elif msg.role == ''user'' %}\n<|user|>{{ ''\\n'' }}\n\n        {%- if msg.content is string %}\n{{ msg.content }}\n        {%- else %}\n            {%- for item in msg.content %}\n                {%- if item.type == ''video'' or ''video'' in item %}\n<|begin_of_video|><|video|><|end_of_video|>\n                {%- elif item.type == ''image'' or ''image'' in item %}\n<|begin_of_image|><|image|><|end_of_image|>\n                {%- elif item.type == ''text'' %}\n{{ item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- elif msg.role == ''assistant'' %}\n        {%- if msg.metadata %}\n<|assistant|>{{ msg.metadata }}\n{{ msg.content }}\n        {%- else %}\n<|assistant|>\n{{ msg.content }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}<|assistant|>\n{% endif %}"}}', '[]', '[{"type":"has_code","target_id":"github:zai-org:GLM-4","source_url":"https://github.com/zai-org/GLM-4"},{"type":"has_code","target_id":"github:zai-org:GLM-V","source_url":"https://github.com/zai-org/GLM-V"},{"type":"based_on_paper","target_id":"arxiv:2507.01006","source_url":"https://arxiv.org/abs/2507.01006"}]', NULL, 'MIT', 'approved', 63.8, '0ea38018b8e7114f7fd42a2e0dc26e77', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-PygmalionAI-pygmalion-6b', 'huggingface--pygmalionai--pygmalion-6b', 'pygmalion-6b', 'PygmalionAI', '--- license: creativeml-openrail-m language: - en thumbnail: tags: - text generation - conversational inference: false --- Pymalion 6B is a proof-of-concept dialogue model based on EleutherAI''s GPT-J-6B. **Warning:** This model is **NOT** suitable for use by minors. It **will** output X-rated content under certain circumstances. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations. M...', '["transformers","pytorch","tensorboard","gptj","text-generation","text generation","conversational","en","license:creativeml-openrail-m","region:us"]', 'text-generation', 752, 1479, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/PygmalionAI/pygmalion-6b","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\nlanguage:\n- en\nthumbnail:\ntags:\n- text generation\n- conversational\ninference: false\n---\n\n# Pygmalion 6B\n\n## Model description\n\nPymalion 6B is a proof-of-concept dialogue model based on EleutherAI''s [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B).\n\n**Warning:** This model is **NOT** suitable for use by minors. It **will** output X-rated content under certain circumstances.\n\n## Training data\n\nThe fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations.\n\n## Training procedure\n\nModel weights were initialized from the `uft-6b` ConvoGPT model made available in [this commit](https://huggingface.co/hakurei/convogpt/tree/41b67bfddb6cd97070ffddf708e9720c9cb8d224/6b-uft).\n\nThe model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\n\n## Intended use\n\n### The easy way\n\nWe provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found [here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb).\n\n### The manual way\n\nThe model can be used as a regular text generation model, but it''ll perform best if the input prompt adheres to the following format:\n\n```\n[CHARACTER]''s Persona: [A few sentences about the character you want the model to play]\n<START>\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:\n```\n\nWhere `[CHARACTER]` is, as you can probably guess, the name of the character you want the model to portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it''ll be pairs of messages like:\n\n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\n\nApart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn''t get confused as to what''s conversation history vs. character definition.\n\n## Known issues\n\nWe haven''t played around with the model enough to enumerate them. Feel free to give us some feedback!\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":171187170634,"files_count":24,"spaces_count":56,"gated":false,"private":false,"config":{"architectures":["GPTJForCausalLM"],"model_type":"gptj","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<|endoftext|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<|endoftext|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":{"__type":"AddedToken","content":"<|endoftext|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:PygmalionAI:gradio-ui","source_url":"https://github.com/PygmalionAI/gradio-ui"}]', NULL, 'creativeml-openrail-m', 'approved', 63.8, '9c0f86f48f8068d1c6fd09e1711b21f6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-playgroundai-playground-v2.5-1024px-aesthetic', 'huggingface--playgroundai--playground-v2.5-1024px-aesthetic', 'playground-v2.5-1024px-aesthetic', 'playgroundai', '--- license: other license_name: playground-v2dot5-community license_link: https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md tags: - text-to-image - playground inference: parameters: guidance_scale: 3.0 --- This repository contains a model that generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios. You can use the model with Hugging Face üß® Diffusers. !image/png **Playground v2.5** is a diffusion-base...', '["diffusers","safetensors","text-to-image","playground","arxiv:2206.00364","arxiv:2402.17245","license:other","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 752, 266502, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: playground-v2dot5-community\nlicense_link: https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md\ntags:\n- text-to-image\n- playground\ninference:\n  parameters:\n    guidance_scale: 3.0\n---\n# Playground v2.5 ‚Äì 1024px Aesthetic Model\n\nThis repository contains a model that generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios. You can use the model with Hugging Face üß® Diffusers.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/HYUUGfU6SOCHsvyeISQ5Y.png)\n\n**Playground v2.5** is a diffusion-based text-to-image generative model, and a successor to [Playground v2](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic).\n\nPlayground v2.5 is the state-of-the-art open-source model in aesthetic quality. Our user studies demonstrate that our model outperforms SDXL, Playground v2, PixArt-Œ±, DALL-E 3, and Midjourney 5.2.\n\nFor details on the development and training of our model, please refer to our [blog post](https://blog.playgroundai.com/playground-v2-5/) and [technical report](https://marketing-cdn.playground.com/research/pgv2.5_compressed.pdf).\n\n### Model Description\n- **Developed by:** [Playground](https://playground.com)\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [Playground v2.5 Community License](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md)\n- **Summary:** This model generates images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pre-trained text encoders (OpenCLIP-ViT/G and CLIP-ViT/L). It follows the same architecture as [Stable Diffusion XL](https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl).\n\n### Using the model with üß® Diffusers\n\nInstall diffusers >= 0.27.0 and the relevant dependencies.\n\n```\npip install diffusers>=0.27.0\npip install transformers accelerate safetensors\n```\n\n**Notes:**\n- The pipeline uses the `EDMDPMSolverMultistepScheduler` scheduler by default, for crisper fine details. It''s an [EDM formulation](https://arxiv.org/abs/2206.00364) of the DPM++ 2M Karras scheduler. `guidance_scale=3.0` is a good default for this scheduler.\n- The pipeline also supports the `EDMEulerScheduler` scheduler. It''s an [EDM formulation](https://arxiv.org/abs/2206.00364) of the Euler scheduler. `guidance_scale=5.0` is a good default for this scheduler.\n\nThen, run the following snippet:\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    "playgroundai/playground-v2.5-1024px-aesthetic",\n    torch_dtype=torch.float16,\n    variant="fp16",\n).to("cuda")\n\n# # Optional: Use DPM++ 2M Karras scheduler for crisper fine details\n# from diffusers import EDMDPMSolverMultistepScheduler\n# pipe.scheduler = EDMDPMSolverMultistepScheduler()\n\nprompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"\nimage = pipe(prompt=prompt, num_inference_steps=50, guidance_scale=3).images[0]\n```\n\n### Using the model with Automatic1111/ComfyUI\n\nSupport coming soon. We will update this model card with instructions when ready.\n\n### User Studies\n\nThis model card only provides a brief summary of our user study results. For extensive details on how we perform user studies, please check out our [technical report](https://marketing-cdn.playground.com/research/pgv2.5_compressed.pdf).\n\nWe conducted studies to measure overall aesthetic quality, as well as for the specific areas we aimed to improve with Playground v2.5, namely multi aspect ratios and human preference alignment.\n\n#### Comparison to State-of-the-Art\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/V7LFNzgoQJnL__ndU0CnE.png)\n\nThe aesthetic quality of Playground v2.5 dramatically outperforms the current state-of-the-art open source models SDXL and PIXART-Œ±, as well as Playground v2. Because the performance differential between Playground V2.5 and SDXL was so large, we also tested our aesthetic quality against world-class closed-source models like DALL-E 3 and Midjourney 5.2, and found that Playground v2.5 outperforms them as well.\n\n#### Multi Aspect Ratios\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/xMB0r-CmR3N6dABFlcV71.png)\n\nSimilarly, for multi aspect ratios, we outperform SDXL by a large margin.\n\n#### Human Preference Alignment on People-related images\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/7c-8Stw52OsNtUjse8Slv.png)\n\nNext, we benchmark Playground v2.5 specifically on people-related images, to test Human Preference Alignment. We compared Playground v2.5 against two commonly-used baseline models: SDXL and RealStock v2, a community fine-tune of SDXL that was trained on a realistic people dataset.\n\nPlayground v2.5 outperforms both baselines by a large margin.\n\n### MJHQ-30K Benchmark\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/7tyYDPGUtokh-k18XDSte.png)\n\n| Model                                 | Overall FID   |\n| ------------------------------------- | ----- |\n| SDXL-1-0-refiner                      | 9.55  |\n| [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)        | 7.07  |\n| [playground-v2.5-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) | **4.48** |\n\nLastly, we report metrics using our MJHQ-30K benchmark which we [open-sourced](https://huggingface.co/datasets/playgroundai/MJHQ-30K) with the v2 release. We report both the overall FID and per category FID. All FID metrics are computed at resolution 1024x1024. Our results show that Playground v2.5 outperforms both Playground v2 and SDXL in overall FID and all category FIDs, especially in the people and fashion categories. This is in line with the results of the user study, which indicates a correlation between human preferences and the FID score of the MJHQ-30K benchmark.\n\n### How to cite us\n\n```\n@misc{li2024playground,\n      title={Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation}, \n      author={Daiqing Li and Aleks Kamko and Ehsan Akhgari and Ali Sabet and Linmiao Xu and Suhail Doshi},\n      year={2024},\n      eprint={2402.17245},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":62442642030,"files_count":35,"spaces_count":84,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2206.00364","source_url":"https://arxiv.org/abs/2206.00364"},{"type":"based_on_paper","target_id":"arxiv:2402.17245","source_url":"https://arxiv.org/abs/2402.17245"}]', NULL, 'Other', 'approved', 63.8, '6fe8313c2c1b3f19714e97e6239175f3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-zero123', 'huggingface--stabilityai--stable-zero123', 'stable-zero123', 'stabilityai', '--- datasets: - allenai/objaverse tags: - 3d extra_gated_fields: Name: text Email: text Country: text Organization or Affiliation: text I ALLOW Stability AI to email me about new model releases: checkbox license: other license_name: sai-nc-community license_link_stable_zero123: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123.md license_link_stable_zero123_c: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123_c.md pipeline_tag: text-to...', '["3d","text-to-3d","dataset:allenai/objaverse","license:other","region:us"]', 'text-to-3d', 751, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-zero123","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- allenai/objaverse\ntags:\n- 3d\nextra_gated_fields:\n  Name: text\n  Email: text\n  Country: text\n  Organization or Affiliation: text\n  I ALLOW Stability AI to email me about new model releases: checkbox\nlicense: other\nlicense_name: sai-nc-community\nlicense_link_stable_zero123: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123.md  \nlicense_link_stable_zero123_c: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123_c.md  \npipeline_tag: text-to-3d\n---\n# Stable Zero123\n\nPlease note: For commercial use, please refer to https://stability.ai/license\n\n## Model Description\n\nStable Zero123 is a model for view-conditioned image generation based on [Zero123](https://github.com/cvlab-columbia/zero123). \n\nWith improved data rendering and model conditioning strategies, our model demonstrates improved performance when compared to the original Zero123 and its subsequent iteration, Zero123-XL.\n\n<img src=''img.png'' width=''700''>\n\n## Usage\n\nBy using Score Distillation Sampling (SDS) along with the Stable Zero123 model, we can produce high-quality 3D models from any input image. The process can also extend to text-to-3D generation by first generating a single image using SDXL and then using SDS on Stable Zero123 to generate the 3D object.\n\nTo enable open research in 3D object generation, we''ve improved the open-source code of threestudio by supporting Zero123 and Stable Zero123.\nTo use Stable Zero123 for object 3D mesh generation in [threestudio](https://github.com/threestudio-project/threestudio#stable-zero123), you can follow these steps:\n\n1. Install threestudio using their [instructions](https://github.com/threestudio-project/threestudio#installation)\n2. Download the Stable Zero123 checkpoint `stable_zero123.ckpt` into the `load/zero123/` directory\n2. Take an image of your choice, or generate it from text using your favourite AI image generator such as Stable Assistant (https://stability.ai/stable-assistant) E.g. "A simple 3D render of a friendly dog"\n3. Remove its background using Stable Assistant (https://stability.ai/stable-assistant)\n4. Save to `load/images/`, preferably with `_rgba.png` as the suffix\n5. Run Zero-1-to-3 with the Stable Zero123 ckpt:\n```sh\npython launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png\n```\n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: latent diffusion model.\n* **Finetuned from model**: [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)\n* **License**: We released 2 versions of Stable Zero123.\n    * **Stable Zero123** included some CC-BY-NC 3D objects, so it cannot be used commercially, but can be used for research purposes. It is released under the [Stability AI Non-Commercial Research Community License](https://huggingface.co/stabilityai/zero123-sai/raw/main/LICENSE_stable_zero123.md).\n    * **Stable Zero123C** (‚ÄúC‚Äù for ‚ÄúCommercially-available‚Äù) was only trained on CC-BY and CC0 3D objects. It is released under [StabilityAI Community License](https://huggingface.co/stabilityai/zero123-sai/raw/main/LICENSE_stable_zero123_c.md). You can read more about the license [here](https://stability.ai/license). \nAccording to our internal tests, both models perform similarly in terms of prediction visual quality.\n\n### Training Dataset\n\nWe use renders from the [Objaverse](https://objaverse.allenai.org/objaverse-1.0) dataset, utilizing our enhanced rendering method\n\n### Training Infrastructure\n\n* **Hardware**: `Stable Zero123` was trained on the Stability AI cluster on a single node with 8 A100 80GBs GPUs.\n* **Code Base**: We use our modified version of [the original zero123 repository](https://github.com/cvlab-columbia/zero123).\n\n\n### Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.', '{"pipeline_tag":"text-to-3d","library_name":null,"framework":null,"params":null,"storage_bytes":34336215865,"files_count":7,"spaces_count":2,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:cvlab-columbia:zero123","source_url":"https://github.com/cvlab-columbia/zero123"},{"type":"has_code","target_id":"github:threestudio-project:threestudio","source_url":"https://github.com/threestudio-project/threestudio#stable-zero123"},{"type":"has_code","target_id":"github:threestudio-project:threestudio","source_url":"https://github.com/threestudio-project/threestudio#installation"},{"type":"has_code","target_id":"github:cvlab-columbia:zero123","source_url":"https://github.com/cvlab-columbia/zero123"}]', NULL, 'Other', 'approved', 63.8, '5a324e441553ebbc78b6936436a82901', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-Hunyuan3D-2.1', 'huggingface--tencent--hunyuan3d-2.1', 'Hunyuan3D-2.1', 'tencent', '--- library_name: hunyuan3d-2 license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1/blob/main/LICENSE language: - en - zh tags: - image-to-3d - text-to-3d pipeline_tag: image-to-3d extra_gated_eu_disallowed: true --- <p align="center"> <img src="https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan3D-2.1/refs/heads/main/assets/images/teaser.jpg"> </p> <div align="center"> <a href=https://3d.hunyuan.tencent.com target="_blank"...', '["hunyuan3d-2","diffusers","safetensors","image-to-3d","text-to-3d","en","zh","arxiv:2506.15442","arxiv:2501.12202","arxiv:2411.02293","license:other","region:us"]', 'image-to-3d', 751, 28820, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/Hunyuan3D-2.1","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: hunyuan3d-2\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1/blob/main/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - image-to-3d\n  - text-to-3d\npipeline_tag: image-to-3d\nextra_gated_eu_disallowed: true\n---\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan3D-2.1/refs/heads/main/assets/images/teaser.jpg">\n</p>\n\n<div align="center">\n  <a href=https://3d.hunyuan.tencent.com target="_blank"><img src=https://img.shields.io/badge/Hunyuan3D-black.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2.1  target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px></a>\n  <a href=https://huggingface.co/tencent/Hunyuan3D-2.1 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1 target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n<a href=https://discord.gg/GuaWYwzKbX target="_blank"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n    <a href=https://arxiv.org/abs/2506.15442 target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n</div>\n\n## üîó BibTeX\n\nIf you found this repository helpful, please cite our report:\n\n```bibtex\n@misc{hunyuan3d2025hunyuan3d,\n    title={Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material},\n    author={Team Hunyuan3D and Shuhui Yang and Mingxin Yang and Yifei Feng and Xin Huang and Sheng Zhang and Zebin He and Di Luo and Haolin Liu and Yunfei Zhao and Qingxiang Lin and Zeqiang Lai and Xianghui Yang and Huiwen Shi and Zibo Zhao and Bowen Zhang and Hongyu Yan and Lifu Wang and Sicong Liu and Jihong Zhang and Meng Chen and Liang Dong and Yiwen Jia and Yulin Cai and Jiaao Yu and Yixuan Tang and Dongyuan Guo and Junlin Yu and Hao Zhang and Zheng Ye and Peng He and Runzhou Wu and Shida Wei and Chao Zhang and Yonghao Tan and Yifu Sun and Lin Niu and Shirui Huang and Bojian Zheng and Shu Liu and Shilin Chen and Xiang Yuan and Xiaofeng Yang and Kai Liu and Jianchen Zhu and Peng Chen and Tian Liu and Di Wang and Yuhong Liu and Linus and Jie Jiang and Jingwei Huang and Chunchao Guo},\n    year={2025},\n    eprint={2506.15442},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{hunyuan3d22025tencent,\n    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    eprint={2501.12202},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{yang2024tencent,\n    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2024},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to\nthe [TripoSG](https://github.com/VAST-AI-Research/TripoSG), [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers)\nand [HuggingFace](https://huggingface.co) repositories, for their open research and exploration.\n\n## Star History\n\n<a href="https://star-history.com/#Tencent-Hunyuan/Hunyuan3D-2.1&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&type=Date" />\n </picture>\n</a>', '{"pipeline_tag":"image-to-3d","library_name":"hunyuan3d-2","framework":"hunyuan3d-2","params":null,"storage_bytes":14949350689,"files_count":30,"spaces_count":35,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan3D-2.1","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan3D-2.1","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1"},{"type":"has_code","target_id":"github:VAST-AI-Research:TripoSG","source_url":"https://github.com/VAST-AI-Research/TripoSG"},{"type":"has_code","target_id":"github:facebookresearch:dinov2","source_url":"https://github.com/facebookresearch/dinov2"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:black-forest-labs:flux","source_url":"https://github.com/black-forest-labs/flux"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2506.15442","source_url":"https://arxiv.org/abs/2506.15442"},{"type":"based_on_paper","target_id":"arxiv:2501.12202","source_url":"https://arxiv.org/abs/2501.12202"},{"type":"based_on_paper","target_id":"arxiv:2411.02293","source_url":"https://arxiv.org/abs/2411.02293"}]', NULL, 'Other', 'approved', 63.8, 'fc3da0276662e882ca91f75c7de9b1c5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2-9b-it', 'huggingface--google--gemma-2-9b-it', 'gemma-2-9b-it', 'google', '', '["transformers","safetensors","gemma2","text-generation","conversational","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2009.11462","arxiv:2101.11718","arxiv:2110.08193","arxiv:1804.09301","arxiv:2109.07958","arxiv:1804.06876","arxiv:2103.03874","arxiv:2304.06364","arxiv:2206.04615","arxiv:2203.09509","base_model:google/gemma-2-9b","base_model:finetune:google/gemma-2-9b","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 748, 150218, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2-9b-it","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":9241705984,"storage_bytes":18550361413,"files_count":14,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma2ForCausalLM"],"model_type":"gemma2","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 38.7, '8417eea3455f488257ac0a2da6c2887a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mixedbread-ai-mxbai-embed-large-v1', 'huggingface--mixedbread-ai--mxbai-embed-large-v1', 'mxbai-embed-large-v1', 'mixedbread-ai', '--- tags: - mteb - transformers.js - transformers model-index: - name: mxbai-angle-large-v1 results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 75.044776119403 - type: ap value: 37.7362433623053 - type: f1 value: 68.92736573359774 - task: type: Classification dataset: type: mteb/amazon_polarity name: MTEB Amaz...', '["sentence-transformers","onnx","safetensors","openvino","gguf","bert","feature-extraction","mteb","transformers.js","transformers","en","arxiv:2309.12871","license:apache-2.0","model-index","text-embeddings-inference","endpoints_compatible","region:us"]', 'feature-extraction', 744, 2172448, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- mteb\n- transformers.js\n- transformers\nmodel-index:\n- name: mxbai-angle-large-v1\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 75.044776119403\n    - type: ap\n      value: 37.7362433623053\n    - type: f1\n      value: 68.92736573359774\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 93.84025000000001\n    - type: ap\n      value: 90.93190875404055\n    - type: f1\n      value: 93.8297833897293\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 49.184\n    - type: f1\n      value: 48.74163227751588\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 41.252\n    - type: map_at_10\n      value: 57.778\n    - type: map_at_100\n      value: 58.233000000000004\n    - type: map_at_1000\n      value: 58.23700000000001\n    - type: map_at_3\n      value: 53.449999999999996\n    - type: map_at_5\n      value: 56.376000000000005\n    - type: mrr_at_1\n      value: 41.679\n    - type: mrr_at_10\n      value: 57.92699999999999\n    - type: mrr_at_100\n      value: 58.389\n    - type: mrr_at_1000\n      value: 58.391999999999996\n    - type: mrr_at_3\n      value: 53.651\n    - type: mrr_at_5\n      value: 56.521\n    - type: ndcg_at_1\n      value: 41.252\n    - type: ndcg_at_10\n      value: 66.018\n    - type: ndcg_at_100\n      value: 67.774\n    - type: ndcg_at_1000\n      value: 67.84400000000001\n    - type: ndcg_at_3\n      value: 57.372\n    - type: ndcg_at_5\n      value: 62.646\n    - type: precision_at_1\n      value: 41.252\n    - type: precision_at_10\n      value: 9.189\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 22.902\n    - type: precision_at_5\n      value: 16.302\n    - type: recall_at_1\n      value: 41.252\n    - type: recall_at_10\n      value: 91.892\n    - type: recall_at_100\n      value: 99.14699999999999\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 68.706\n    - type: recall_at_5\n      value: 81.50800000000001\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 48.97294504317859\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 42.98071077674629\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 65.16477858490782\n    - type: mrr\n      value: 78.23583080508287\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 89.6277629421789\n    - type: cos_sim_spearman\n      value: 88.4056288400568\n    - type: euclidean_pearson\n      value: 87.94871847578163\n    - type: euclidean_spearman\n      value: 88.4056288400568\n    - type: manhattan_pearson\n      value: 87.73271254229648\n    - type: manhattan_spearman\n      value: 87.91826833762677\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 87.81818181818181\n    - type: f1\n      value: 87.79879337316918\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 39.91773608582761\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 36.73059477462478\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.745999999999995\n    - type: map_at_10\n      value: 43.632\n    - type: map_at_100\n      value: 45.206\n    - type: map_at_1000\n      value: 45.341\n    - type: map_at_3\n      value: 39.956\n    - type: map_at_5\n      value: 42.031\n    - type: mrr_at_1\n      value: 39.485\n    - type: mrr_at_10\n      value: 49.537\n    - type: mrr_at_100\n      value: 50.249\n    - type: mrr_at_1000\n      value: 50.294000000000004\n    - type: mrr_at_3\n      value: 46.757\n    - type: mrr_at_5\n      value: 48.481\n    - type: ndcg_at_1\n      value: 39.485\n    - type: ndcg_at_10\n      value: 50.058\n    - type: ndcg_at_100\n      value: 55.586\n    - type: ndcg_at_1000\n      value: 57.511\n    - type: ndcg_at_3\n      value: 44.786\n    - type: ndcg_at_5\n      value: 47.339999999999996\n    - type: precision_at_1\n      value: 39.485\n    - type: precision_at_10\n      value: 9.557\n    - type: precision_at_100\n      value: 1.552\n    - type: precision_at_1000\n      value: 0.202\n    - type: precision_at_3\n      value: 21.412\n    - type: precision_at_5\n      value: 15.479000000000001\n    - type: recall_at_1\n      value: 32.745999999999995\n    - type: recall_at_10\n      value: 62.056\n    - type: recall_at_100\n      value: 85.088\n    - type: recall_at_1000\n      value: 96.952\n    - type: recall_at_3\n      value: 46.959\n    - type: recall_at_5\n      value: 54.06999999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.898\n    - type: map_at_10\n      value: 42.142\n    - type: map_at_100\n      value: 43.349\n    - type: map_at_1000\n      value: 43.483\n    - type: map_at_3\n      value: 39.18\n    - type: map_at_5\n      value: 40.733000000000004\n    - type: mrr_at_1\n      value: 39.617999999999995\n    - type: mrr_at_10\n      value: 47.922\n    - type: mrr_at_100\n      value: 48.547000000000004\n    - type: mrr_at_1000\n      value: 48.597\n    - type: mrr_at_3\n      value: 45.86\n    - type: mrr_at_5\n      value: 46.949000000000005\n    - type: ndcg_at_1\n      value: 39.617999999999995\n    - type: ndcg_at_10\n      value: 47.739\n    - type: ndcg_at_100\n      value: 51.934999999999995\n    - type: ndcg_at_1000\n      value: 54.007000000000005\n    - type: ndcg_at_3\n      value: 43.748\n    - type: ndcg_at_5\n      value: 45.345\n    - type: precision_at_1\n      value: 39.617999999999995\n    - type: precision_at_10\n      value: 8.962\n    - type: precision_at_100\n      value: 1.436\n    - type: precision_at_1000\n      value: 0.192\n    - type: precision_at_3\n      value: 21.083\n    - type: precision_at_5\n      value: 14.752\n    - type: recall_at_1\n      value: 31.898\n    - type: recall_at_10\n      value: 57.587999999999994\n    - type: recall_at_100\n      value: 75.323\n    - type: recall_at_1000\n      value: 88.304\n    - type: recall_at_3\n      value: 45.275\n    - type: recall_at_5\n      value: 49.99\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.458\n    - type: map_at_10\n      value: 52.942\n    - type: map_at_100\n      value: 53.974\n    - type: map_at_1000\n      value: 54.031\n    - type: map_at_3\n      value: 49.559999999999995\n    - type: map_at_5\n      value: 51.408\n    - type: mrr_at_1\n      value: 46.27\n    - type: mrr_at_10\n      value: 56.31699999999999\n    - type: mrr_at_100\n      value: 56.95099999999999\n    - type: mrr_at_1000\n      value: 56.98\n    - type: mrr_at_3\n      value: 53.835\n    - type: mrr_at_5\n      value: 55.252\n    - type: ndcg_at_1\n      value: 46.27\n    - type: ndcg_at_10\n      value: 58.964000000000006\n    - type: ndcg_at_100\n      value: 62.875\n    - type: ndcg_at_1000\n      value: 63.969\n    - type: ndcg_at_3\n      value: 53.297000000000004\n    - type: ndcg_at_5\n      value: 55.938\n    - type: precision_at_1\n      value: 46.27\n    - type: precision_at_10\n      value: 9.549000000000001\n    - type: precision_at_100\n      value: 1.2409999999999999\n    - type: precision_at_1000\n      value: 0.13799999999999998\n    - type: precision_at_3\n      value: 23.762\n    - type: precision_at_5\n      value: 16.262999999999998\n    - type: recall_at_1\n      value: 40.458\n    - type: recall_at_10\n      value: 73.446\n    - type: recall_at_100\n      value: 90.12400000000001\n    - type: recall_at_1000\n      value: 97.795\n    - type: recall_at_3\n      value: 58.123000000000005\n    - type: recall_at_5\n      value: 64.68\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.443\n    - type: map_at_10\n      value: 36.081\n    - type: map_at_100\n      value: 37.163000000000004\n    - type: map_at_1000\n      value: 37.232\n    - type: map_at_3\n      value: 33.308\n    - type: map_at_5\n      value: 34.724\n    - type: mrr_at_1\n      value: 29.492\n    - type: mrr_at_10\n      value: 38.138\n    - type: mrr_at_100\n      value: 39.065\n    - type: mrr_at_1000\n      value: 39.119\n    - type: mrr_at_3\n      value: 35.593\n    - type: mrr_at_5\n      value: 36.785000000000004\n    - type: ndcg_at_1\n      value: 29.492\n    - type: ndcg_at_10\n      value: 41.134\n    - type: ndcg_at_100\n      value: 46.300999999999995\n    - type: ndcg_at_1000\n      value: 48.106\n    - type: ndcg_at_3\n      value: 35.77\n    - type: ndcg_at_5\n      value: 38.032\n    - type: precision_at_1\n      value: 29.492\n    - type: precision_at_10\n      value: 6.249\n    - type: precision_at_100\n      value: 0.9299999999999999\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 15.065999999999999\n    - type: precision_at_5\n      value: 10.373000000000001\n    - type: recall_at_1\n      value: 27.443\n    - type: recall_at_10\n      value: 54.80199999999999\n    - type: recall_at_100\n      value: 78.21900000000001\n    - type: recall_at_1000\n      value: 91.751\n    - type: recall_at_3\n      value: 40.211000000000006\n    - type: recall_at_5\n      value: 45.599000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 18.731\n    - type: map_at_10\n      value: 26.717999999999996\n    - type: map_at_100\n      value: 27.897\n    - type: map_at_1000\n      value: 28.029\n    - type: map_at_3\n      value: 23.91\n    - type: map_at_5\n      value: 25.455\n    - type: mrr_at_1\n      value: 23.134\n    - type: mrr_at_10\n      value: 31.769\n    - type: mrr_at_100\n      value: 32.634\n    - type: mrr_at_1000\n      value: 32.707\n    - type: mrr_at_3\n      value: 28.938999999999997\n    - type: mrr_at_5\n      value: 30.531000000000002\n    - type: ndcg_at_1\n      value: 23.134\n    - type: ndcg_at_10\n      value: 32.249\n    - type: ndcg_at_100\n      value: 37.678\n    - type: ndcg_at_1000\n      value: 40.589999999999996\n    - type: ndcg_at_3\n      value: 26.985999999999997\n    - type: ndcg_at_5\n      value: 29.457\n    - type: precision_at_1\n      value: 23.134\n    - type: precision_at_10\n      value: 5.8709999999999996\n    - type: precision_at_100\n      value: 0.988\n    - type: precision_at_1000\n      value: 0.13799999999999998\n    - type: precision_at_3\n      value: 12.852\n    - type: precision_at_5\n      value: 9.428\n    - type: recall_at_1\n      value: 18.731\n    - type: recall_at_10\n      value: 44.419\n    - type: recall_at_100\n      value: 67.851\n    - type: recall_at_1000\n      value: 88.103\n    - type: recall_at_3\n      value: 29.919\n    - type: recall_at_5\n      value: 36.230000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.324\n    - type: map_at_10\n      value: 41.265\n    - type: map_at_100\n      value: 42.559000000000005\n    - type: map_at_1000\n      value: 42.669000000000004\n    - type: map_at_3\n      value: 38.138\n    - type: map_at_5\n      value: 39.881\n    - type: mrr_at_1\n      value: 36.67\n    - type: mrr_at_10\n      value: 46.774\n    - type: mrr_at_100\n      value: 47.554\n    - type: mrr_at_1000\n      value: 47.593\n    - type: mrr_at_3\n      value: 44.338\n    - type: mrr_at_5\n      value: 45.723\n    - type: ndcg_at_1\n      value: 36.67\n    - type: ndcg_at_10\n      value: 47.367\n    - type: ndcg_at_100\n      value: 52.623\n    - type: ndcg_at_1000\n      value: 54.59\n    - type: ndcg_at_3\n      value: 42.323\n    - type: ndcg_at_5\n      value: 44.727\n    - type: precision_at_1\n      value: 36.67\n    - type: precision_at_10\n      value: 8.518\n    - type: precision_at_100\n      value: 1.2890000000000001\n    - type: precision_at_1000\n      value: 0.163\n    - type: precision_at_3\n      value: 19.955000000000002\n    - type: precision_at_5\n      value: 14.11\n    - type: recall_at_1\n      value: 30.324\n    - type: recall_at_10\n      value: 59.845000000000006\n    - type: recall_at_100\n      value: 81.77499999999999\n    - type: recall_at_1000\n      value: 94.463\n    - type: recall_at_3\n      value: 46.019\n    - type: recall_at_5\n      value: 52.163000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.229\n    - type: map_at_10\n      value: 35.004000000000005\n    - type: map_at_100\n      value: 36.409000000000006\n    - type: map_at_1000\n      value: 36.521\n    - type: map_at_3\n      value: 31.793\n    - type: map_at_5\n      value: 33.432\n    - type: mrr_at_1\n      value: 30.365\n    - type: mrr_at_10\n      value: 40.502\n    - type: mrr_at_100\n      value: 41.372\n    - type: mrr_at_1000\n      value: 41.435\n    - type: mrr_at_3\n      value: 37.804\n    - type: mrr_at_5\n      value: 39.226\n    - type: ndcg_at_1\n      value: 30.365\n    - type: ndcg_at_10\n      value: 41.305\n    - type: ndcg_at_100\n      value: 47.028999999999996\n    - type: ndcg_at_1000\n      value: 49.375\n    - type: ndcg_at_3\n      value: 35.85\n    - type: ndcg_at_5\n      value: 38.12\n    - type: precision_at_1\n      value: 30.365\n    - type: precision_at_10\n      value: 7.808\n    - type: precision_at_100\n      value: 1.228\n    - type: precision_at_1000\n      value: 0.161\n    - type: precision_at_3\n      value: 17.352\n    - type: precision_at_5\n      value: 12.42\n    - type: recall_at_1\n      value: 24.229\n    - type: recall_at_10\n      value: 54.673\n    - type: recall_at_100\n      value: 78.766\n    - type: recall_at_1000\n      value: 94.625\n    - type: recall_at_3\n      value: 39.602\n    - type: recall_at_5\n      value: 45.558\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.695\n    - type: map_at_10\n      value: 36.0895\n    - type: map_at_100\n      value: 37.309416666666664\n    - type: map_at_1000\n      value: 37.42558333333334\n    - type: map_at_3\n      value: 33.19616666666666\n    - type: map_at_5\n      value: 34.78641666666667\n    - type: mrr_at_1\n      value: 31.486083333333337\n    - type: mrr_at_10\n      value: 40.34774999999999\n    - type: mrr_at_100\n      value: 41.17533333333333\n    - type: mrr_at_1000\n      value: 41.231583333333326\n    - type: mrr_at_3\n      value: 37.90075\n    - type: mrr_at_5\n      value: 39.266999999999996\n    - type: ndcg_at_1\n      value: 31.486083333333337\n    - type: ndcg_at_10\n      value: 41.60433333333334\n    - type: ndcg_at_100\n      value: 46.74525\n    - type: ndcg_at_1000\n      value: 48.96166666666667\n    - type: ndcg_at_3\n      value: 36.68825\n    - type: ndcg_at_5\n      value: 38.966499999999996\n    - type: precision_at_1\n      value: 31.486083333333337\n    - type: precision_at_10\n      value: 7.29675\n    - type: precision_at_100\n      value: 1.1621666666666666\n    - type: precision_at_1000\n      value: 0.1545\n    - type: precision_at_3\n      value: 16.8815\n    - type: precision_at_5\n      value: 11.974583333333333\n    - type: recall_at_1\n      value: 26.695\n    - type: recall_at_10\n      value: 53.651916666666665\n    - type: recall_at_100\n      value: 76.12083333333332\n    - type: recall_at_1000\n      value: 91.31191666666668\n    - type: recall_at_3\n      value: 40.03575\n    - type: recall_at_5\n      value: 45.876666666666665\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.668000000000003\n    - type: map_at_10\n      value: 32.486\n    - type: map_at_100\n      value: 33.371\n    - type: map_at_1000\n      value: 33.458\n    - type: map_at_3\n      value: 30.261\n    - type: map_at_5\n      value: 31.418000000000003\n    - type: mrr_at_1\n      value: 28.988000000000003\n    - type: mrr_at_10\n      value: 35.414\n    - type: mrr_at_100\n      value: 36.149\n    - type: mrr_at_1000\n      value: 36.215\n    - type: mrr_at_3\n      value: 33.333\n    - type: mrr_at_5\n      value: 34.43\n    - type: ndcg_at_1\n      value: 28.988000000000003\n    - type: ndcg_at_10\n      value: 36.732\n    - type: ndcg_at_100\n      value: 41.331\n    - type: ndcg_at_1000\n      value: 43.575\n    - type: ndcg_at_3\n      value: 32.413\n    - type: ndcg_at_5\n      value: 34.316\n    - type: precision_at_1\n      value: 28.988000000000003\n    - type: precision_at_10\n      value: 5.7059999999999995\n    - type: precision_at_100\n      value: 0.882\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 13.65\n    - type: precision_at_5\n      value: 9.417\n    - type: recall_at_1\n      value: 25.668000000000003\n    - type: recall_at_10\n      value: 47.147\n    - type: recall_at_100\n      value: 68.504\n    - type: recall_at_1000\n      value: 85.272\n    - type: recall_at_3\n      value: 35.19\n    - type: recall_at_5\n      value: 39.925\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.256\n    - type: map_at_10\n      value: 24.58\n    - type: map_at_100\n      value: 25.773000000000003\n    - type: map_at_1000\n      value: 25.899\n    - type: map_at_3\n      value: 22.236\n    - type: map_at_5\n      value: 23.507\n    - type: mrr_at_1\n      value: 20.957\n    - type: mrr_at_10\n      value: 28.416000000000004\n    - type: mrr_at_100\n      value: 29.447000000000003\n    - type: mrr_at_1000\n      value: 29.524\n    - type: mrr_at_3\n      value: 26.245\n    - type: mrr_at_5\n      value: 27.451999999999998\n    - type: ndcg_at_1\n      value: 20.957\n    - type: ndcg_at_10\n      value: 29.285\n    - type: ndcg_at_100\n      value: 35.003\n    - type: ndcg_at_1000\n      value: 37.881\n    - type: ndcg_at_3\n      value: 25.063000000000002\n    - type: ndcg_at_5\n      value: 26.983\n    - type: precision_at_1\n      value: 20.957\n    - type: precision_at_10\n      value: 5.344\n    - type: precision_at_100\n      value: 0.958\n    - type: precision_at_1000\n      value: 0.13799999999999998\n    - type: precision_at_3\n      value: 11.918\n    - type: precision_at_5\n      value: 8.596\n    - type: recall_at_1\n      value: 17.256\n    - type: recall_at_10\n      value: 39.644\n    - type: recall_at_100\n      value: 65.279\n    - type: recall_at_1000\n      value: 85.693\n    - type: recall_at_3\n      value: 27.825\n    - type: recall_at_5\n      value: 32.792\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.700000000000003\n    - type: map_at_10\n      value: 36.205999999999996\n    - type: map_at_100\n      value: 37.316\n    - type: map_at_1000\n      value: 37.425000000000004\n    - type: map_at_3\n      value: 33.166000000000004\n    - type: map_at_5\n      value: 35.032999999999994\n    - type: mrr_at_1\n      value: 31.436999999999998\n    - type: mrr_at_10\n      value: 40.61\n    - type: mrr_at_100\n      value: 41.415\n    - type: mrr_at_1000\n      value: 41.48\n    - type: mrr_at_3\n      value: 37.966\n    - type: mrr_at_5\n      value: 39.599000000000004\n    - type: ndcg_at_1\n      value: 31.436999999999998\n    - type: ndcg_at_10\n      value: 41.771\n    - type: ndcg_at_100\n      value: 46.784\n    - type: ndcg_at_1000\n      value: 49.183\n    - type: ndcg_at_3\n      value: 36.437000000000005\n    - type: ndcg_at_5\n      value: 39.291\n    - type: precision_at_1\n      value: 31.436999999999998\n    - type: precision_at_10\n      value: 6.987\n    - type: precision_at_100\n      value: 1.072\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 16.448999999999998\n    - type: precision_at_5\n      value: 11.866\n    - type: recall_at_1\n      value: 26.700000000000003\n    - type: recall_at_10\n      value: 54.301\n    - type: recall_at_100\n      value: 75.871\n    - type: recall_at_1000\n      value: 92.529\n    - type: recall_at_3\n      value: 40.201\n    - type: recall_at_5\n      value: 47.208\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.296\n    - type: map_at_10\n      value: 33.116\n    - type: map_at_100\n      value: 34.81\n    - type: map_at_1000\n      value: 35.032000000000004\n    - type: map_at_3\n      value: 30.105999999999998\n    - type: map_at_5\n      value: 31.839000000000002\n    - type: mrr_at_1\n      value: 29.051\n    - type: mrr_at_10\n      value: 37.803\n    - type: mrr_at_100\n      value: 38.856\n    - type: mrr_at_1000\n      value: 38.903999999999996\n    - type: mrr_at_3\n      value: 35.211\n    - type: mrr_at_5\n      value: 36.545\n    - type: ndcg_at_1\n      value: 29.051\n    - type: ndcg_at_10\n      value: 39.007\n    - type: ndcg_at_100\n      value: 45.321\n    - type: ndcg_at_1000\n      value: 47.665\n    - type: ndcg_at_3\n      value: 34.1\n    - type: ndcg_at_5\n      value: 36.437000000000005\n    - type: precision_at_1\n      value: 29.051\n    - type: precision_at_10\n      value: 7.668\n    - type: precision_at_100\n      value: 1.542\n    - type: precision_at_1000\n      value: 0.24\n    - type: precision_at_3\n      value: 16.14\n    - type: precision_at_5\n      value: 11.897\n    - type: recall_at_1\n      value: 24.296\n    - type: recall_at_10\n      value: 49.85\n    - type: recall_at_100\n      value: 78.457\n    - type: recall_at_1000\n      value: 92.618\n    - type: recall_at_3\n      value: 36.138999999999996\n    - type: recall_at_5\n      value: 42.223\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 20.591\n    - type: map_at_10\n      value: 28.902\n    - type: map_at_100\n      value: 29.886000000000003\n    - type: map_at_1000\n      value: 29.987000000000002\n    - type: map_at_3\n      value: 26.740000000000002\n    - type: map_at_5\n      value: 27.976\n    - type: mrr_at_1\n      value: 22.366\n    - type: mrr_at_10\n      value: 30.971\n    - type: mrr_at_100\n      value: 31.865\n    - type: mrr_at_1000\n      value: 31.930999999999997\n    - type: mrr_at_3\n      value: 28.927999999999997\n    - type: mrr_at_5\n      value: 30.231\n    - type: ndcg_at_1\n      value: 22.366\n    - type: ndcg_at_10\n      value: 33.641\n    - type: ndcg_at_100\n      value: 38.477\n    - type: ndcg_at_1000\n      value: 41.088\n    - type: ndcg_at_3\n      value: 29.486\n    - type: ndcg_at_5\n      value: 31.612000000000002\n    - type: precision_at_1\n      value: 22.366\n    - type: precision_at_10\n      value: 5.3420000000000005\n    - type: precision_at_100\n      value: 0.828\n    - type: precision_at_1000\n      value: 0.11800000000000001\n    - type: precision_at_3\n      value: 12.939\n    - type: precision_at_5\n      value: 9.094\n    - type: recall_at_1\n      value: 20.591\n    - type: recall_at_10\n      value: 46.052\n    - type: recall_at_100\n      value: 68.193\n    - type: recall_at_1000\n      value: 87.638\n    - type: recall_at_3\n      value: 34.966\n    - type: recall_at_5\n      value: 40.082\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 15.091\n    - type: map_at_10\n      value: 26.38\n    - type: map_at_100\n      value: 28.421999999999997\n    - type: map_at_1000\n      value: 28.621999999999996\n    - type: map_at_3\n      value: 21.597\n    - type: map_at_5\n      value: 24.12\n    - type: mrr_at_1\n      value: 34.266999999999996\n    - type: mrr_at_10\n      value: 46.864\n    - type: mrr_at_100\n      value: 47.617\n    - type: mrr_at_1000\n      value: 47.644\n    - type: mrr_at_3\n      value: 43.312\n    - type: mrr_at_5\n      value: 45.501000000000005\n    - type: ndcg_at_1\n      value: 34.266999999999996\n    - type: ndcg_at_10\n      value: 36.095\n    - type: ndcg_at_100\n      value: 43.447\n    - type: ndcg_at_1000\n      value: 46.661\n    - type: ndcg_at_3\n      value: 29.337999999999997\n    - type: ndcg_at_5\n      value: 31.824\n    - type: precision_at_1\n      value: 34.266999999999996\n    - type: precision_at_10\n      value: 11.472\n    - type: precision_at_100\n      value: 1.944\n    - type: precision_at_1000\n      value: 0.255\n    - type: precision_at_3\n      value: 21.933\n    - type: precision_at_5\n      value: 17.224999999999998\n    - type: recall_at_1\n      value: 15.091\n    - type: recall_at_10\n      value: 43.022\n    - type: recall_at_100\n      value: 68.075\n    - type: recall_at_1000\n      value: 85.76\n    - type: recall_at_3\n      value: 26.564\n    - type: recall_at_5\n      value: 33.594\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 9.252\n    - type: map_at_10\n      value: 20.923\n    - type: map_at_100\n      value: 30.741000000000003\n    - type: map_at_1000\n      value: 32.542\n    - type: map_at_3\n      value: 14.442\n    - type: map_at_5\n      value: 17.399\n    - type: mrr_at_1\n      value: 70.25\n    - type: mrr_at_10\n      value: 78.17\n    - type: mrr_at_100\n      value: 78.444\n    - type: mrr_at_1000\n      value: 78.45100000000001\n    - type: mrr_at_3\n      value: 76.958\n    - type: mrr_at_5\n      value: 77.571\n    - type: ndcg_at_1\n      value: 58.375\n    - type: ndcg_at_10\n      value: 44.509\n    - type: ndcg_at_100\n      value: 49.897999999999996\n    - type: ndcg_at_1000\n      value: 57.269999999999996\n    - type: ndcg_at_3\n      value: 48.64\n    - type: ndcg_at_5\n      value: 46.697\n    - type: precision_at_1\n      value: 70.25\n    - type: precision_at_10\n      value: 36.05\n    - type: precision_at_100\n      value: 11.848\n    - type: precision_at_1000\n      value: 2.213\n    - type: precision_at_3\n      value: 52.917\n    - type: precision_at_5\n      value: 45.7\n    - type: recall_at_1\n      value: 9.252\n    - type: recall_at_10\n      value: 27.006999999999998\n    - type: recall_at_100\n      value: 57.008\n    - type: recall_at_1000\n      value: 80.697\n    - type: recall_at_3\n      value: 15.798000000000002\n    - type: recall_at_5\n      value: 20.4\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 50.88\n    - type: f1\n      value: 45.545495028653384\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 75.424\n    - type: map_at_10\n      value: 83.435\n    - type: map_at_100\n      value: 83.66900000000001\n    - type: map_at_1000\n      value: 83.685\n    - type: map_at_3\n      value: 82.39800000000001\n    - type: map_at_5\n      value: 83.07\n    - type: mrr_at_1\n      value: 81.113\n    - type: mrr_at_10\n      value: 87.77199999999999\n    - type: mrr_at_100\n      value: 87.862\n    - type: mrr_at_1000\n      value: 87.86500000000001\n    - type: mrr_at_3\n      value: 87.17099999999999\n    - type: mrr_at_5\n      value: 87.616\n    - type: ndcg_at_1\n      value: 81.113\n    - type: ndcg_at_10\n      value: 86.909\n    - type: ndcg_at_100\n      value: 87.746\n    - type: ndcg_at_1000\n      value: 88.017\n    - type: ndcg_at_3\n      value: 85.368\n    - type: ndcg_at_5\n      value: 86.28099999999999\n    - type: precision_at_1\n      value: 81.113\n    - type: precision_at_10\n      value: 10.363\n    - type: precision_at_100\n      value: 1.102\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 32.507999999999996\n    - type: precision_at_5\n      value: 20.138\n    - type: recall_at_1\n      value: 75.424\n    - type: recall_at_10\n      value: 93.258\n    - type: recall_at_100\n      value: 96.545\n    - type: recall_at_1000\n      value: 98.284\n    - type: recall_at_3\n      value: 89.083\n    - type: recall_at_5\n      value: 91.445\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.532\n    - type: map_at_10\n      value: 37.141999999999996\n    - type: map_at_100\n      value: 39.162\n    - type: map_at_1000\n      value: 39.322\n    - type: map_at_3\n      value: 32.885\n    - type: map_at_5\n      value: 35.093999999999994\n    - type: mrr_at_1\n      value: 44.29\n    - type: mrr_at_10\n      value: 53.516\n    - type: mrr_at_100\n      value: 54.24\n    - type: mrr_at_1000\n      value: 54.273\n    - type: mrr_at_3\n      value: 51.286\n    - type: mrr_at_5\n      value: 52.413\n    - type: ndcg_at_1\n      value: 44.29\n    - type: ndcg_at_10\n      value: 45.268\n    - type: ndcg_at_100\n      value: 52.125\n    - type: ndcg_at_1000\n      value: 54.778000000000006\n    - type: ndcg_at_3\n      value: 41.829\n    - type: ndcg_at_5\n      value: 42.525\n    - type: precision_at_1\n      value: 44.29\n    - type: precision_at_10\n      value: 12.5\n    - type: precision_at_100\n      value: 1.9720000000000002\n    - type: precision_at_1000\n      value: 0.245\n    - type: precision_at_3\n      value: 28.035\n    - type: precision_at_5\n      value: 20.093\n    - type: recall_at_1\n      value: 22.532\n    - type: recall_at_10\n      value: 52.419000000000004\n    - type: recall_at_100\n      value: 77.43299999999999\n    - type: recall_at_1000\n      value: 93.379\n    - type: recall_at_3\n      value: 38.629000000000005\n    - type: recall_at_5\n      value: 43.858000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.359\n    - type: map_at_10\n      value: 63.966\n    - type: map_at_100\n      value: 64.87\n    - type: map_at_1000\n      value: 64.92599999999999\n    - type: map_at_3\n      value: 60.409\n    - type: map_at_5\n      value: 62.627\n    - type: mrr_at_1\n      value: 78.717\n    - type: mrr_at_10\n      value: 84.468\n    - type: mrr_at_100\n      value: 84.655\n    - type: mrr_at_1000\n      value: 84.661\n    - type: mrr_at_3\n      value: 83.554\n    - type: mrr_at_5\n      value: 84.133\n    - type: ndcg_at_1\n      value: 78.717\n    - type: ndcg_at_10\n      value: 72.03399999999999\n    - type: ndcg_at_100\n      value: 75.158\n    - type: ndcg_at_1000\n      value: 76.197\n    - type: ndcg_at_3\n      value: 67.049\n    - type: ndcg_at_5\n      value: 69.808\n    - type: precision_at_1\n      value: 78.717\n    - type: precision_at_10\n      value: 15.201\n    - type: precision_at_100\n      value: 1.764\n    - type: precision_at_1000\n      value: 0.19\n    - type: precision_at_3\n      value: 43.313\n    - type: precision_at_5\n      value: 28.165000000000003\n    - type: recall_at_1\n      value: 39.359\n    - type: recall_at_10\n      value: 76.003\n    - type: recall_at_100\n      value: 88.197\n    - type: recall_at_1000\n      value: 95.003\n    - type: recall_at_3\n      value: 64.97\n    - type: recall_at_5\n      value: 70.41199999999999\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 92.83200000000001\n    - type: ap\n      value: 89.33560571859861\n    - type: f1\n      value: 92.82322915005167\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 21.983\n    - type: map_at_10\n      value: 34.259\n    - type: map_at_100\n      value: 35.432\n    - type: map_at_1000\n      value: 35.482\n    - type: map_at_3\n      value: 30.275999999999996\n    - type: map_at_5\n      value: 32.566\n    - type: mrr_at_1\n      value: 22.579\n    - type: mrr_at_10\n      value: 34.882999999999996\n    - type: mrr_at_100\n      value: 35.984\n    - type: mrr_at_1000\n      value: 36.028\n    - type: mrr_at_3\n      value: 30.964999999999996\n    - type: mrr_at_5\n      value: 33.245000000000005\n    - type: ndcg_at_1\n      value: 22.564\n    - type: ndcg_at_10\n      value: 41.258\n    - type: ndcg_at_100\n      value: 46.824\n    - type: ndcg_at_1000\n      value: 48.037\n    - type: ndcg_at_3\n      value: 33.17\n    - type: ndcg_at_5\n      value: 37.263000000000005\n    - type: precision_at_1\n      value: 22.564\n    - type: precision_at_10\n      value: 6.572\n    - type: precision_at_100\n      value: 0.935\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 14.130999999999998\n    - type: precision_at_5\n      value: 10.544\n    - type: recall_at_1\n      value: 21.983\n    - type: recall_at_10\n      value: 62.775000000000006\n    - type: recall_at_100\n      value: 88.389\n    - type: recall_at_1000\n      value: 97.603\n    - type: recall_at_3\n      value: 40.878\n    - type: recall_at_5\n      value: 50.690000000000005\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.95120839033288\n    - type: f1\n      value: 93.73824125055208\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 76.78978568171455\n    - type: f1\n      value: 57.50180552858304\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 76.24411566913248\n    - type: f1\n      value: 74.37851403532832\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.94620040349699\n    - type: f1\n      value: 80.21293397970435\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 33.44403096245675\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 31.659594631336812\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 32.53833075108798\n    - type: mrr\n      value: 33.78840823218308\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 7.185999999999999\n    - type: map_at_10\n      value: 15.193999999999999\n    - type: map_at_100\n      value: 19.538\n    - type: map_at_1000\n      value: 21.178\n    - type: map_at_3\n      value: 11.208\n    - type: map_at_5\n      value: 12.745999999999999\n    - type: mrr_at_1\n      value: 48.916\n    - type: mrr_at_10\n      value: 58.141\n    - type: mrr_at_100\n      value: 58.656\n    - type: mrr_at_1000\n      value: 58.684999999999995\n    - type: mrr_at_3\n      value: 55.521\n    - type: mrr_at_5\n      value: 57.239\n    - type: ndcg_at_1\n      value: 47.059\n    - type: ndcg_at_10\n      value: 38.644\n    - type: ndcg_at_100\n      value: 36.272999999999996\n    - type: ndcg_at_1000\n      value: 44.996\n    - type: ndcg_at_3\n      value: 43.293\n    - type: ndcg_at_5\n      value: 40.819\n    - type: precision_at_1\n      value: 48.916\n    - type: precision_at_10\n      value: 28.607\n    - type: precision_at_100\n      value: 9.195\n    - type: precision_at_1000\n      value: 2.225\n    - type: precision_at_3\n      value: 40.454\n    - type: precision_at_5\n      value: 34.985\n    - type: recall_at_1\n      value: 7.185999999999999\n    - type: recall_at_10\n      value: 19.654\n    - type: recall_at_100\n      value: 37.224000000000004\n    - type: recall_at_1000\n      value: 68.663\n    - type: recall_at_3\n      value: 12.158\n    - type: recall_at_5\n      value: 14.674999999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.552000000000003\n    - type: map_at_10\n      value: 47.75\n    - type: map_at_100\n      value: 48.728\n    - type: map_at_1000\n      value: 48.754\n    - type: map_at_3\n      value: 43.156\n    - type: map_at_5\n      value: 45.883\n    - type: mrr_at_1\n      value: 35.66\n    - type: mrr_at_10\n      value: 50.269\n    - type: mrr_at_100\n      value: 50.974\n    - type: mrr_at_1000\n      value: 50.991\n    - type: mrr_at_3\n      value: 46.519\n    - type: mrr_at_5\n      value: 48.764\n    - type: ndcg_at_1\n      value: 35.632000000000005\n    - type: ndcg_at_10\n      value: 55.786\n    - type: ndcg_at_100\n      value: 59.748999999999995\n    - type: ndcg_at_1000\n      value: 60.339\n    - type: ndcg_at_3\n      value: 47.292\n    - type: ndcg_at_5\n      value: 51.766999999999996\n    - type: precision_at_1\n      value: 35.632000000000005\n    - type: precision_at_10\n      value: 9.267\n    - type: precision_at_100\n      value: 1.149\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 21.601\n    - type: precision_at_5\n      value: 15.539\n    - type: recall_at_1\n      value: 31.552000000000003\n    - type: recall_at_10\n      value: 77.62400000000001\n    - type: recall_at_100\n      value: 94.527\n    - type: recall_at_1000\n      value: 98.919\n    - type: recall_at_3\n      value: 55.898\n    - type: recall_at_5\n      value: 66.121\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.414\n    - type: map_at_10\n      value: 85.37400000000001\n    - type: map_at_100\n      value: 86.01100000000001\n    - type: map_at_1000\n      value: 86.027\n    - type: map_at_3\n      value: 82.562\n    - type: map_at_5\n      value: 84.284\n    - type: mrr_at_1\n      value: 82.24000000000001\n    - type: mrr_at_10\n      value: 88.225\n    - type: mrr_at_100\n      value: 88.324\n    - type: mrr_at_1000\n      value: 88.325\n    - type: mrr_at_3\n      value: 87.348\n    - type: mrr_at_5\n      value: 87.938\n    - type: ndcg_at_1\n      value: 82.24000000000001\n    - type: ndcg_at_10\n      value: 88.97699999999999\n    - type: ndcg_at_100\n      value: 90.16\n    - type: ndcg_at_1000\n      value: 90.236\n    - type: ndcg_at_3\n      value: 86.371\n    - type: ndcg_at_5\n      value: 87.746\n    - type: precision_at_1\n      value: 82.24000000000001\n    - type: precision_at_10\n      value: 13.481000000000002\n    - type: precision_at_100\n      value: 1.534\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.86\n    - type: precision_at_5\n      value: 24.738\n    - type: recall_at_1\n      value: 71.414\n    - type: recall_at_10\n      value: 95.735\n    - type: recall_at_100\n      value: 99.696\n    - type: recall_at_1000\n      value: 99.979\n    - type: recall_at_3\n      value: 88.105\n    - type: recall_at_5\n      value: 92.17999999999999\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 60.22146692057259\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 65.29273320614578\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.023\n    - type: map_at_10\n      value: 14.161000000000001\n    - type: map_at_100\n      value: 16.68\n    - type: map_at_1000\n      value: 17.072000000000003\n    - type: map_at_3\n      value: 9.763\n    - type: map_at_5\n      value: 11.977\n    - type: mrr_at_1\n      value: 24.8\n    - type: mrr_at_10\n      value: 37.602999999999994\n    - type: mrr_at_100\n      value: 38.618\n    - type: mrr_at_1000\n      value: 38.659\n    - type: mrr_at_3\n      value: 34.117\n    - type: mrr_at_5\n      value: 36.082\n    - type: ndcg_at_1\n      value: 24.8\n    - type: ndcg_at_10\n      value: 23.316\n    - type: ndcg_at_100\n      value: 32.613\n    - type: ndcg_at_1000\n      value: 38.609\n    - type: ndcg_at_3\n      value: 21.697\n    - type: ndcg_at_5\n      value: 19.241\n    - type: precision_at_1\n      value: 24.8\n    - type: precision_at_10\n      value: 12.36\n    - type: precision_at_100\n      value: 2.593\n    - type: precision_at_1000\n      value: 0.402\n    - type: precision_at_3\n      value: 20.767\n    - type: precision_at_5\n      value: 17.34\n    - type: recall_at_1\n      value: 5.023\n    - type: recall_at_10\n      value: 25.069999999999997\n    - type: recall_at_100\n      value: 52.563\n    - type: recall_at_1000\n      value: 81.525\n    - type: recall_at_3\n      value: 12.613\n    - type: recall_at_5\n      value: 17.583\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.71506247604255\n    - type: cos_sim_spearman\n      value: 82.91813463738802\n    - type: euclidean_pearson\n      value: 85.5154616194479\n    - type: euclidean_spearman\n      value: 82.91815254466314\n    - type: manhattan_pearson\n      value: 85.5280917850374\n    - type: manhattan_spearman\n      value: 82.92276537286398\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.43772054228462\n    - type: cos_sim_spearman\n      value: 78.75750601716682\n    - type: euclidean_pearson\n      value: 85.76074482955764\n    - type: euclidean_spearman\n      value: 78.75651057223058\n    - type: manhattan_pearson\n      value: 85.73390291701668\n    - type: manhattan_spearman\n      value: 78.72699385957797\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 89.58144067172472\n    - type: cos_sim_spearman\n      value: 90.3524512966946\n    - type: euclidean_pearson\n      value: 89.71365391594237\n    - type: euclidean_spearman\n      value: 90.35239632843408\n    - type: manhattan_pearson\n      value: 89.66905421746478\n    - type: manhattan_spearman\n      value: 90.31508211683513\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.77692637102102\n    - type: cos_sim_spearman\n      value: 85.45710562643485\n    - type: euclidean_pearson\n      value: 87.42456979928723\n    - type: euclidean_spearman\n      value: 85.45709386240908\n    - type: manhattan_pearson\n      value: 87.40754529526272\n    - type: manhattan_spearman\n      value: 85.44834854173303\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.28491331695997\n    - type: cos_sim_spearman\n      value: 89.62037029566964\n    - type: euclidean_pearson\n      value: 89.02479391362826\n    - type: euclidean_spearman\n      value: 89.62036733618466\n    - type: manhattan_pearson\n      value: 89.00394756040342\n    - type: manhattan_spearman\n      value: 89.60867744215236\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.08911381280191\n    - type: cos_sim_spearman\n      value: 86.5791780765767\n    - type: euclidean_pearson\n      value: 86.16063473577861\n    - type: euclidean_spearman\n      value: 86.57917745378766\n    - type: manhattan_pearson\n      value: 86.13677924604175\n    - type: manhattan_spearman\n      value: 86.56115615768685\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 89.58029496205235\n    - type: cos_sim_spearman\n      value: 89.49551253826998\n    - type: euclidean_pearson\n      value: 90.13714840963748\n    - type: euclidean_spearman\n      value: 89.49551253826998\n    - type: manhattan_pearson\n      value: 90.13039633601363\n    - type: manhattan_spearman\n      value: 89.4513453745516\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 69.01546399666435\n    - type: cos_sim_spearman\n      value: 69.33824484595624\n    - type: euclidean_pearson\n      value: 70.76511642998874\n    - type: euclidean_spearman\n      value: 69.33824484595624\n    - type: manhattan_pearson\n      value: 70.84320785047453\n    - type: manhattan_spearman\n      value: 69.54233632223537\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.26389196390119\n    - type: cos_sim_spearman\n      value: 89.09721478341385\n    - type: euclidean_pearson\n      value: 88.97208685922517\n    - type: euclidean_spearman\n      value: 89.09720927308881\n    - type: manhattan_pearson\n      value: 88.97513670502573\n    - type: manhattan_spearman\n      value: 89.07647853984004\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 87.53075025771936\n    - type: mrr\n      value: 96.24327651288436\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 60.428000000000004\n    - type: map_at_10\n      value: 70.088\n    - type: map_at_100\n      value: 70.589\n    - type: map_at_1000\n      value: 70.614\n    - type: map_at_3\n      value: 67.191\n    - type: map_at_5\n      value: 68.515\n    - type: mrr_at_1\n      value: 63.333\n    - type: mrr_at_10\n      value: 71.13000000000001\n    - type: mrr_at_100\n      value: 71.545\n    - type: mrr_at_1000\n      value: 71.569\n    - type: mrr_at_3\n      value: 68.944\n    - type: mrr_at_5\n      value: 70.078\n    - type: ndcg_at_1\n      value: 63.333\n    - type: ndcg_at_10\n      value: 74.72800000000001\n    - type: ndcg_at_100\n      value: 76.64999999999999\n    - type: ndcg_at_1000\n      value: 77.176\n    - type: ndcg_at_3\n      value: 69.659\n    - type: ndcg_at_5\n      value: 71.626\n    - type: precision_at_1\n      value: 63.333\n    - type: precision_at_10\n      value: 10\n    - type: precision_at_100\n      value: 1.09\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 27.111\n    - type: precision_at_5\n      value: 17.666999999999998\n    - type: recall_at_1\n      value: 60.428000000000004\n    - type: recall_at_10\n      value: 87.98899999999999\n    - type: recall_at_100\n      value: 96.167\n    - type: recall_at_1000\n      value: 100\n    - type: recall_at_3\n      value: 74.006\n    - type: recall_at_5\n      value: 79.05\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.87326732673267\n    - type: cos_sim_ap\n      value: 96.81770773701805\n    - type: cos_sim_f1\n      value: 93.6318407960199\n    - type: cos_sim_precision\n      value: 93.16831683168317\n    - type: cos_sim_recall\n      value: 94.1\n    - type: dot_accuracy\n      value: 99.87326732673267\n    - type: dot_ap\n      value: 96.8174218946665\n    - type: dot_f1\n      value: 93.6318407960199\n    - type: dot_precision\n      value: 93.16831683168317\n    - type: dot_recall\n      value: 94.1\n    - type: euclidean_accuracy\n      value: 99.87326732673267\n    - type: euclidean_ap\n      value: 96.81770773701807\n    - type: euclidean_f1\n      value: 93.6318407960199\n    - type: euclidean_precision\n      value: 93.16831683168317\n    - type: euclidean_recall\n      value: 94.1\n    - type: manhattan_accuracy\n      value: 99.87227722772278\n    - type: manhattan_ap\n      value: 96.83164126821747\n    - type: manhattan_f1\n      value: 93.54677338669335\n    - type: manhattan_precision\n      value: 93.5935935935936\n    - type: manhattan_recall\n      value: 93.5\n    - type: max_accuracy\n      value: 99.87326732673267\n    - type: max_ap\n      value: 96.83164126821747\n    - type: max_f1\n      value: 93.6318407960199\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 65.6212042420246\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 35.779230635982564\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 55.217701909036286\n    - type: mrr\n      value: 56.17658995416349\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 30.954206018888453\n    - type: cos_sim_spearman\n      value: 32.71062599450096\n    - type: dot_pearson\n      value: 30.95420929056943\n    - type: dot_spearman\n      value: 32.71062599450096\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.22699999999999998\n    - type: map_at_10\n      value: 1.924\n    - type: map_at_100\n      value: 10.525\n    - type: map_at_1000\n      value: 24.973\n    - type: map_at_3\n      value: 0.638\n    - type: map_at_5\n      value: 1.0659999999999998\n    - type: mrr_at_1\n      value: 84\n    - type: mrr_at_10\n      value: 91.067\n    - type: mrr_at_100\n      value: 91.067\n    - type: mrr_at_1000\n      value: 91.067\n    - type: mrr_at_3\n      value: 90.667\n    - type: mrr_at_5\n      value: 91.067\n    - type: ndcg_at_1\n      value: 81\n    - type: ndcg_at_10\n      value: 75.566\n    - type: ndcg_at_100\n      value: 56.387\n    - type: ndcg_at_1000\n      value: 49.834\n    - type: ndcg_at_3\n      value: 80.899\n    - type: ndcg_at_5\n      value: 80.75099999999999\n    - type: precision_at_1\n      value: 84\n    - type: precision_at_10\n      value: 79\n    - type: precision_at_100\n      value: 57.56\n    - type: precision_at_1000\n      value: 21.8\n    - type: precision_at_3\n      value: 84.667\n    - type: precision_at_5\n      value: 85.2\n    - type: recall_at_1\n      value: 0.22699999999999998\n    - type: recall_at_10\n      value: 2.136\n    - type: recall_at_100\n      value: 13.861\n    - type: recall_at_1000\n      value: 46.299\n    - type: recall_at_3\n      value: 0.6649999999999999\n    - type: recall_at_5\n      value: 1.145\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 2.752\n    - type: map_at_10\n      value: 9.951\n    - type: map_at_100\n      value: 16.794999999999998\n    - type: map_at_1000\n      value: 18.251\n    - type: map_at_3\n      value: 5.288\n    - type: map_at_5\n      value: 6.954000000000001\n    - type: mrr_at_1\n      value: 38.775999999999996\n    - type: mrr_at_10\n      value: 50.458000000000006\n    - type: mrr_at_100\n      value: 51.324999999999996\n    - type: mrr_at_1000\n      value: 51.339999999999996\n    - type: mrr_at_3\n      value: 46.939\n    - type: mrr_at_5\n      value: 47.857\n    - type: ndcg_at_1\n      value: 36.735\n    - type: ndcg_at_10\n      value: 25.198999999999998\n    - type: ndcg_at_100\n      value: 37.938\n    - type: ndcg_at_1000\n      value: 49.145\n    - type: ndcg_at_3\n      value: 29.348000000000003\n    - type: ndcg_at_5\n      value: 25.804\n    - type: precision_at_1\n      value: 38.775999999999996\n    - type: precision_at_10\n      value: 22.041\n    - type: precision_at_100\n      value: 7.939\n    - type: precision_at_1000\n      value: 1.555\n    - type: precision_at_3\n      value: 29.932\n    - type: precision_at_5\n      value: 24.490000000000002\n    - type: recall_at_1\n      value: 2.752\n    - type: recall_at_10\n      value: 16.197\n    - type: recall_at_100\n      value: 49.166\n    - type: recall_at_1000\n      value: 84.18900000000001\n    - type: recall_at_3\n      value: 6.438000000000001\n    - type: recall_at_5\n      value: 9.093\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 71.47980000000001\n    - type: ap\n      value: 14.605194452178754\n    - type: f1\n      value: 55.07362924988948\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 59.708545557441994\n    - type: f1\n      value: 60.04751270975683\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 53.21105960597211\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 87.58419264469214\n    - type: cos_sim_ap\n      value: 78.55300004517404\n    - type: cos_sim_f1\n      value: 71.49673530889001\n    - type: cos_sim_precision\n      value: 68.20795400095831\n    - type: cos_sim_recall\n      value: 75.11873350923483\n    - type: dot_accuracy\n      value: 87.58419264469214\n    - type: dot_ap\n      value: 78.55297659559511\n    - type: dot_f1\n      value: 71.49673530889001\n    - type: dot_precision\n      value: 68.20795400095831\n    - type: dot_recall\n      value: 75.11873350923483\n    - type: euclidean_accuracy\n      value: 87.58419264469214\n    - type: euclidean_ap\n      value: 78.55300477331477\n    - type: euclidean_f1\n      value: 71.49673530889001\n    - type: euclidean_precision\n      value: 68.20795400095831\n    - type: euclidean_recall\n      value: 75.11873350923483\n    - type: manhattan_accuracy\n      value: 87.5663110210407\n    - type: manhattan_ap\n      value: 78.49982050876562\n    - type: manhattan_f1\n      value: 71.35488740722104\n    - type: manhattan_precision\n      value: 68.18946862226497\n    - type: manhattan_recall\n      value: 74.82849604221636\n    - type: max_accuracy\n      value: 87.58419264469214\n    - type: max_ap\n      value: 78.55300477331477\n    - type: max_f1\n      value: 71.49673530889001\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.09069740365584\n    - type: cos_sim_ap\n      value: 86.22749303724757\n    - type: cos_sim_f1\n      value: 78.36863452005407\n    - type: cos_sim_precision\n      value: 76.49560117302053\n    - type: cos_sim_recall\n      value: 80.33569448721897\n    - type: dot_accuracy\n      value: 89.09069740365584\n    - type: dot_ap\n      value: 86.22750233655673\n    - type: dot_f1\n      value: 78.36863452005407\n    - type: dot_precision\n      value: 76.49560117302053\n    - type: dot_recall\n      value: 80.33569448721897\n    - type: euclidean_accuracy\n      value: 89.09069740365584\n    - type: euclidean_ap\n      value: 86.22749355597347\n    - type: euclidean_f1\n      value: 78.36863452005407\n    - type: euclidean_precision\n      value: 76.49560117302053\n    - type: euclidean_recall\n      value: 80.33569448721897\n    - type: manhattan_accuracy\n      value: 89.08293553770326\n    - type: manhattan_ap\n      value: 86.21913616084771\n    - type: manhattan_f1\n      value: 78.3907031479847\n    - type: manhattan_precision\n      value: 75.0352013517319\n    - type: manhattan_recall\n      value: 82.06036341238065\n    - type: max_accuracy\n      value: 89.09069740365584\n    - type: max_ap\n      value: 86.22750233655673\n    - type: max_f1\n      value: 78.3907031479847\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: sentence-transformers\npipeline_tag: feature-extraction\n---\n\n<br><br>\n\n<p align="center">\n<svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" viewBox="0 0 2020 1130" width="150" height="150" aria-hidden="true"><path fill="#e95a0f" d="M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z"></path><path fill="#ed6d7b" d="M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z"></path><path fill="#ec663e" d="M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z"></path><path fill="#ea580e" d="M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z"></path><path fill="#f8ab00" d="M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z"></path><path fill="#ea5227" d="M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z"></path><path fill="#ea5330" d="M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z"></path><path fill="#f8aa05" d="M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z"></path><path fill="#ed6876" d="M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z"></path><path fill="#f6a200" d="M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z"></path><path fill="#f7a80d" d="M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z"></path><path fill="#ef7a72" d="M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z"></path><path fill="#e94e3b" d="M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z"></path><path fill="#ea5043" d="M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z"></path><path fill="#eb5d19" d="M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z"></path><path fill="#e94c32" d="M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z"></path><path fill="#f7a617" d="M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z"></path><path fill="#ea553a" d="M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z"></path><path fill="#ea5453" d="M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z"></path><path fill="#ea5b15" d="M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z"></path><path fill="#ea504b" d="M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z"></path><path fill="#f6a320" d="M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z"></path><path fill="#ec6333" d="M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z"></path><path fill="#f49700" d="M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z"></path><path fill="#ea544c" d="M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z"></path><path fill="#ec5c61" d="M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z"></path><path fill="#ed676f" d="M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z"></path><path fill="#ed656e" d="M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z"></path><path fill="#ec6168" d="M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z"></path><path fill="#e94c28" d="M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z"></path><path fill="#eb5659" d="M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z"></path><path fill="#f5a02d" d="M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z"></path><path fill="#eb5a57" d="M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z"></path><path fill="#ea541c" d="M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z"></path><path fill="#eb5d19" d="M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z"></path><path fill="#eb5e24" d="M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z"></path><path fill="#e9511f" d="M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z"></path><path fill="#ea5741" d="M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z"></path><path fill="#ea522a" d="M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z"></path><path fill="#eb5e24" d="M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z"></path><path fill="#e95514" d="M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z"></path><path fill="#eb5f2d" d="M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z"></path><path fill="#f28d00" d="M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z"></path><path fill="#f08369" d="M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z"></path><path fill="#f18b5e" d="M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z"></path><path fill="#f49c3a" d="M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z"></path><path fill="#f08000" d="M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z"></path><path fill="#ed6e04" d="M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z"></path><path fill="#ea541c" d="M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z"></path><path fill="#f3944d" d="M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z"></path><path fill="#ec6168" d="M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z"></path><path fill="#ec663e" d="M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z"></path><path fill="#eb5e5b" d="M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z"></path><path fill="#ea580e" d="M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z"></path><path fill="#e95514" d="M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z"></path><path fill="#eb620b" d="M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z"></path><path fill="#ea544c" d="M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z"></path><path fill="#ec6333" d="M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z"></path><path fill="#eb5e5b" d="M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":335141888,"storage_bytes":5355731041,"files_count":21,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2309.12871","source_url":"https://arxiv.org/abs/2309.12871"}]', NULL, 'Apache-2.0', 'approved', 78.7, 'af448428ec2f9efa85d312d669cadf8c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Omni-30B-A3B-Instruct', 'huggingface--qwen--qwen3-omni-30b-a3b-instruct', 'Qwen3-Omni-30B-A3B-Instruct', 'Qwen', '--- license: other license_name: apache-2.0 language: - en tags: - multimodal library_name: transformers pipeline_tag: any-to-any --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> <p align="center"> <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png" width="100%"/> <p> Qwen3...', '["transformers","safetensors","qwen3_omni_moe","text-to-audio","multimodal","any-to-any","en","license:other","endpoints_compatible","region:us"]', 'any-to-any', 744, 282682, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: apache-2.0\nlanguage:\n- en\ntags:\n- multimodal\nlibrary_name: transformers\npipeline_tag: any-to-any\n---\n\n# Qwen3-Omni\n\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n\n## Overview\n### Introduction\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png" width="100%"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinker‚ÄìTalker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy adaptation.\n\n* **Detailed Audio Captioner**: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.\n\n### Model Architecture\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/overview.png" width="80%"/>\n<p>\n\n### Cookbooks for Usage Cases\n\nQwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the [QuickStart](#quickstart) guide to download the model and install the necessary inference environment dependencies, then run and experiment locally‚Äîtry modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Cookbook</th>\n      <th>Description</th>\n      <th>Open</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan="6">Audio</td>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb">Speech Recognition</a></td>\n      <td>Speech recognition, supporting multiple languages and long audio.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb">Speech Translation</a></td>\n      <td>Speech-to-Text / Speech-to-Speech translation.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb">Music Analysis</a></td>\n      <td>Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb">Sound Analysis</a></td>\n      <td>Description and analysis of various sound effects and audio signals.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb">Audio Caption</a></td>\n      <td>Audio captioning, detailed description of any audio input.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb">Mixed Audio Analysis</a></td>\n      <td>Analysis of mixed audio content, such as speech, music, and environmental sounds.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td rowspan="7">Visual</td>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb">OCR</a></td>\n      <td>OCR for complex images.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb">Object Grounding</a></td>\n      <td>Target detection and grounding.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb">Image Question</a></td>\n      <td>Answering arbitrary questions about any image.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb">Image Math</a></td>\n      <td>Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb">Video Description</a></td>\n      <td>Detailed description of video content.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb">Video Navigation</a></td>\n      <td>Generating navigation commands from first-person motion videos.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb">Video Scene Transition</a></td>\n      <td>Analysis of scene transitions in videos.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td rowspan="3">Audio-Visual</td>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb">Audio Visual Question</a></td>\n      <td>Answering arbitrary questions in audio-visual scenarios, demonstrating the model''s ability to model temporal alignment between audio and video.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb">Audio Visual Interaction</a></td>\n      <td>Interactive communication with the model using audio-visual inputs, including task specification via audio.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb">Audio Visual Dialogue</a></td>\n      <td>Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td>Agent</td>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb">Audio Function Call</a></td>\n      <td>Using audio input to perform function calls, enabling agent-like behaviors.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n    <tr>\n      <td>Downstream Task Fine-tuning</td>\n      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb">Omni Captioner</a></td>\n      <td>Introduction and capability demonstration of <strong>Qwen3-Omni-30B-A3B-Captioner</strong>, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.</td>\n      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td>\n    </tr>\n  </tbody>\n</table>\n\n## QuickStart\n\n### Model Description and Download\n\nBelow is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.\n\n| Model Name                   | Description |\n|------------------------------|-------------|\n| Qwen3-Omni-30B-A3B-Instruct  | The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the [Qwen3-Omni Technical Report](https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf). |\n| Qwen3-Omni-30B-A3B-Thinking  | The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the [Qwen3-Omni Technical Report](https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf).|\n| Qwen3-Omni-30B-A3B-Captioner | A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model''s [cookbook](https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb). |\n\nDuring loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:\n\n```bash\n# Download through ModelScope (recommended for users in Mainland China)\npip install -U modelscope\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner\n\n# Download through Hugging Face\npip install -U "huggingface_hub[cli]"\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner\n```\n\n### Transformers Usage\n\n#### Installation\n\nThe Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you **create a new Python environment** to avoid environment runtime issues.\n\n```bash\n# If you already have transformers installed, please uninstall it first, or create a new Python environment\n# pip uninstall transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\n```\n\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has `ffmpeg` installed:\n\n```bash\npip install qwen-omni-utils -U\n```\n\nAdditionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using [vLLM](#vllm-usage) for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [FlashAttention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention 2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\n#### Code Snippet\n\nHere is a code snippet to show you how to use Qwen3-Omni with `transformers` and `qwen_omni_utils`:\n\n```python\nimport soundfile as sf\n\nfrom transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nMODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"\n# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"\n\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    MODEL_PATH,\n    dtype="auto",\n    device_map="auto",\n    attn_implementation="flash_attention_2",\n)\n\nprocessor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\nconversation = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},\n            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},\n            {"type": "text", "text": "What can you see and hear? Answer in one short sentence."}\n        ],\n    },\n]\n\n# Set whether to use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, \n                   audio=audios, \n                   images=images, \n                   videos=videos, \n                   return_tensors="pt", \n                   padding=True, \n                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, \n                                 speaker="Ethan", \n                                 thinker_return_dict_in_generate=True,\n                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],\n                              skip_special_tokens=True,\n                              clean_up_tokenization_spaces=False)\nprint(text)\nif audio is not None:\n    sf.write(\n        "output.wav",\n        audio.reshape(-1).detach().cpu().numpy(),\n        samplerate=24000,\n    )\n```\n\nHere are some more advanced usage examples. You can expand the sections below to learn more.\n\n<details>\n<summary>Batch inference</summary>\n\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when `return_audio=False` is set. Here is an example.\n\n```python\nfrom transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nMODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"\n# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"\n\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    MODEL_PATH,\n    dtype="auto",\n    device_map="auto",\n    attn_implementation="flash_attention_2",\n)\nmodel.disable_talker()\n\nprocessor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n# Conversation with image only\nconversation1 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},\n            {"type": "text", "text": "What can you see in this image? Answer in one sentence."},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},\n            {"type": "text", "text": "What can you hear in this audio?"},\n        ]\n    }\n]\n\n# Conversation with pure text and system prompt\nconversation3 = [\n    {\n        "role": "system",\n        "content": [\n            {"type": "text", "text": "You are Qwen-Omni."}\n        ],\n    },\n    {\n        "role": "user",\n        "content": "Who are you?"\n    }\n]\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},\n            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},\n            {"type": "text", "text": "What can you see and hear? Answer in one sentence."}\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# Set whether to use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, \n                   audio=audios, \n                   images=images, \n                   videos=videos, \n                   return_tensors="pt", \n                   padding=True, \n                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch inference does not support returning audio\ntext_ids, audio = model.generate(**inputs,\n                                 return_audio=False,\n                                 thinker_return_dict_in_generate=True,\n                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],\n                              skip_special_tokens=True,\n                              clean_up_tokenization_spaces=False)\nprint(text)\n```\n\n</details>\n\n<details>\n<summary>Use audio output or not</summary>\n\nThe model supports both text and audio outputs. If users do not need audio outputs, they can call `model.disable_talker()` after initializing the model. This option will save about `10GB` of GPU memory, but the `return_audio` option for the `generate` function will only allow `False`.\n```python\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen3-Omni-30B-A3B-Instruct",\n    dtype="auto",\n    device_map="auto",\n    attn_implementation="flash_attention_2",\n)\nmodel.disable_talker()\n```\n\nFor a more flexible experience, we recommend that users decide whether to return audio when the `generate` function is called. If `return_audio` is set to `False`, the model will only return text outputs, resulting in faster text responses.\n\n```python\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen3-Omni-30B-A3B-Instruct",\n    dtype="auto",\n    device_map="auto",\n    attn_implementation="flash_attention_2",\n)\n...\ntext_ids, _ = model.generate(..., return_audio=False)```\n\n</details>\n\n<details>\n<summary>Change voice type of output audio</summary>\n\nQwen3-Omni supports changing the voice of the output audio. The `"Qwen/Qwen3-Omni-30B-A3B-Instruct"` checkpoint supports three voice types as follows:\n\n| Voice Type | Gender | Description |\n|------------|--------|-------------|\n| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |\n| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |\n| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |\n\nUsers can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.\n\n```python\ntext_ids, audio = model.generate(..., speaker="Ethan")\n```\n\n```python\ntext_ids, audio = model.generate(..., speaker="Chelsie")\n```\n\n```python\ntext_ids, audio = model.generate(..., speaker="Aiden")\n```\n\n</details>\n\n### vLLM Usage\n\n#### Installation\n\nWe strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and **audio output inference support for the Instruct model will be released in the near future**, you can follow the commands below to install vLLM from source. Please note that we recommend you **create a new Python environment** to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the [vLLM official documentation](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#set-up-using-python-only-build-without-compilation).\n\n```bash\ngit clone -b qwen3_omni https://github.com/wangxiongts/vllm.git\ncd vllm\npip install -r requirements/build.txt\npip install -r requirements/cuda.txt\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl\nVLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation\n# If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.\n# Install the Transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\npip install qwen-omni-utils -U\npip install -U flash-attn --no-build-isolation\n```\n\n#### Inference\n\nYou can use the following code for vLLM inference. The `limit_mm_per_prompt` parameter specifies the maximum number of each modality''s data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting `tensor_parallel_size` greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, `max_num_seqs` indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the [vLLM official documentation](https://docs.vllm.ai/en/latest/api/vllm/index.html#vllm.LLM). Below is a simple example of how to run Qwen3-Omni with vLLM:\n\n```python\nimport os\nimport torch\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nif __name__ == ''__main__'':\n    # vLLM engine v1 not supported yet\n    os.environ[''VLLM_USE_V1''] = ''0''\n\n    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"\n    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"\n\n    llm = LLM(\n            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n            tensor_parallel_size=torch.cuda.device_count(),\n            limit_mm_per_prompt={''image'': 3, ''video'': 3, ''audio'': 3},\n            max_num_seqs=8,\n            max_model_len=32768,\n            seed=1234,\n    )\n\n    sampling_params = SamplingParams(\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        max_tokens=16384,\n    )\n\n    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n    messages = [\n        {\n            "role": "user",\n            "content": [\n                {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}\n            ], \n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n\n    inputs = {\n        ''prompt'': text,\n        ''multi_modal_data'': {},\n        "mm_processor_kwargs": {\n            "use_audio_in_video": True,\n        },\n    }\n\n    if images is not None:\n        inputs[''multi_modal_data''][''image''] = images\n    if videos is not None:\n        inputs[''multi_modal_data''][''video''] = videos\n    if audios is not None:\n        inputs[''multi_modal_data''][''audio''] = audios\n\n    outputs = llm.generate([inputs], sampling_params=sampling_params)\n\n    print(outputs[0].outputs[0].text)\n```\n\nHere are some more advanced usage examples. You can expand the sections below to learn more.\n\n<details>\n<summary>Batch inference</summary>\n\nUsing vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:\n\n```python\nimport os\nimport torch\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\ndef build_input(processor, messages, use_audio_in_video):\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)\n\n    inputs = {\n        ''prompt'': text,\n        ''multi_modal_data'': {},\n        "mm_processor_kwargs": {\n            "use_audio_in_video": use_audio_in_video,\n        },\n    }\n\n    if images is not None:\n        inputs[''multi_modal_data''][''image''] = images\n    if videos is not None:\n        inputs[''multi_modal_data''][''video''] = videos\n    if audios is not None:\n        inputs[''multi_modal_data''][''audio''] = audios\n    \n    return inputs\n\nif __name__ == ''__main__'':\n    # vLLM engine v1 not supported yet\n    os.environ[''VLLM_USE_V1''] = ''0''\n\n    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"\n    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"\n\n    llm = LLM(\n            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n            tensor_parallel_size=torch.cuda.device_count(),\n            limit_mm_per_prompt={''image'': 3, ''video'': 3, ''audio'': 3},\n            max_num_seqs=8,\n            max_model_len=32768,\n            seed=1234,\n    )\n\n    sampling_params = SamplingParams(\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        max_tokens=16384,\n    )\n\n    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n    # Conversation with image only\n    conversation1 = [\n        {\n            "role": "user",\n            "content": [\n                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},\n                {"type": "text", "text": "What can you see in this image? Answer in one sentence."},\n            ]\n        }\n    ]\n\n    # Conversation with audio only\n    conversation2 = [\n        {\n            "role": "user",\n            "content": [\n                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},\n                {"type": "text", "text": "What can you hear in this audio?"},\n            ]\n        }\n    ]\n\n    # Conversation with pure text and system prompt\n    conversation3 = [\n        {\n            "role": "system",\n            "content": [\n                {"type": "text", "text": "You are Qwen-Omni."}\n            ],\n        },\n        {\n            "role": "user",\n            "content": "Who are you? Answer in one sentence."\n        }\n    ]\n\n    # Conversation with mixed media\n    conversation4 = [\n        {\n            "role": "user",\n            "content": [\n                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},\n                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"},\n                {"type": "text", "text": "What can you see and hear? Answer in one sentence."}\n            ],\n        }\n    ]\n    \n    USE_AUDIO_IN_VIDEO = True\n\n    # Combine messages for batch processing\n    conversations = [conversation1, conversation2, conversation3, conversation4]\n    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]\n\n    outputs = llm.generate(inputs, sampling_params=sampling_params)\n\n    result = [outputs[i].outputs[0].text for i in range(len(outputs))]\n    print(result)\n```\n\n</details>\n\n<details>\n<summary>vLLM Serve Usage</summary>\n\nvLLM serve for Qwen3-Omni currently only supports the thinker model. The `use_audio_in_video` parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:\n\n```bash\n# Qwen3-Omni-30B-A3B-Instruct for single GPU\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1\n# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4\n# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1\n# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4\n```\n\nThen you can use the chat API as below (via curl, for example):\n```bash\ncurl http://localhost:8901/v1/chat/completions \\n    -H "Content-Type: application/json" \\n    -d ''{\n    "messages": [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": [\n        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},\n        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},\n        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}\n    ]}\n    ]\n    }''\n```\n\n</details>\n\n### Usage Tips (Recommended Reading)\n\n#### Minimum GPU memory requirements\n\n| Model                        | Precision | 15s Video | 30s Video | 60s Video | 120s Video   |\n|------------------------------|-----------| --------- | --------- | --------- | --------- |\n| Qwen3-Omni-30B-A3B-Instruct  | BF16      | 78.85 GB  | 88.52 GB  | 107.74 GB | 144.81 GB |\n| Qwen3-Omni-30B-A3B-Thinking  | BF16      | 68.74 GB  | 77.79 GB  | 95.76 GB  | 131.65 GB  |\n\n**Note**: The table above presents the theoretical minimum memory requirements for inference with `transformers` and `BF16` precision, tested with `attn_implementation="flash_attention_2"`. The Instruct model includes both the **thinker** and **talker** components, whereas the Thinking model includes only the **thinker** part.\n\n#### Prompt for Audio-Visual Interaction\n\nWhen using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the **following system prompt**. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the `user_system_prompt` field in the system prompt to include character settings or other role-specific descriptions as needed.\n\n```\nuser_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."\nmessage = {\n    "role": "system",\n    "content": [\n          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, ‚ÄúI/me/my/we/our‚Äù refer to the user and ‚Äúyou/your‚Äù refer to the assistant. In your replies, address the user as ‚Äúyou/your‚Äù and yourself as ‚ÄúI/me/my‚Äù; never mirror the user‚Äôs pronouns‚Äîalways shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users'' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can''t see/hear clearly, don''t understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}\n    ]\n}\n```\n\n#### Best Practices for the Thinking Model\n\nThe `Qwen3-Omni-30B-A3B-Thinking` model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model''s ability to leverage its reasoning capabilities. For example:\n\n```python\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "audio", "audio": "/path/to/audio.wav"},\n            {"type": "image", "image": "/path/to/image.png"},\n            {"type": "video", "video": "/path/to/video.mp4"},\n            {"type": "text", "text": "Analyze this audio, image, and video together."},\n        ], \n    }\n]\n```\n\n#### Use audio in video\n\nIn multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.\n\n```python\n# In data preprocessing\naudios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n```\n\n```python\n# For Transformers\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", \n                   padding=True, use_audio_in_video=True)\ntext_ids, audio = model.generate(..., use_audio_in_video=True)\n\n# For vLLM\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = {\n    ''prompt'': text,\n    ''multi_modal_data'': {},\n    "mm_processor_kwargs": {\n        "use_audio_in_video": True,\n    },\n}\n```\n\nIt is worth noting that during a multi-round conversation, the `use_audio_in_video` parameter must be set consistently across these steps; otherwise, unexpected results may occur.\n\n## Evaluation\n\n### Performance of Qwen3-Omni\n\nQwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.\n\n<details>\n<summary>Text -> Text</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th colspan="2" style="text-align: left;"></th>\n      <th style="text-align: center;">GPT-4o-0327</th>\n      <th style="text-align: center;">Qwen3-235B-A22B<br>Non Thinking</th>\n      <th style="text-align: center;">Qwen3-30B-A3B-Instruct-2507</th>\n      <th style="text-align: center;">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style="text-align: center;">Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan="2" style="text-align: left; vertical-align: middle;">General<br>Tasks</td>\n      <td style="text-align: left;">MMLU-Redux</td>\n      <td style="text-align: center;"><strong>91.3</strong></td>\n      <td style="text-align: center;">89.2</td>\n      <td style="text-align: center;">89.3</td>\n      <td style="text-align: center;">86.6</td>\n      <td style="text-align: center;">86.8</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">GPQA</td>\n      <td style="text-align: center;">66.9</td>\n      <td style="text-align: center;">62.9</td>\n      <td style="text-align: center;"><strong>70.4</strong></td>\n      <td style="text-align: center;">69.6</td>\n      <td style="text-align: center;">69.7</td>\n    </tr>\n    <tr>\n      <td rowspan="2" style="text-align: left; vertical-align: middle;">Reasoning</td>\n      <td style="text-align: left;">AIME25</td>\n      <td style="text-align: center;">26.7</td>\n      <td style="text-align: center;">24.7</td>\n      <td style="text-align: center;">61.3</td>\n      <td style="text-align: center;">65.0</td>\n      <td style="text-align: center;"><strong>65.9</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ZebraLogic</td>\n      <td style="text-align: center;">52.6</td>\n      <td style="text-align: center;">37.7</td>\n      <td style="text-align: center;"><strong>90.0</strong></td>\n      <td style="text-align: center;">76.0</td>\n      <td style="text-align: center;">76.1</td>\n    </tr>\n    <tr>\n      <td style="text-align: left; vertical-align: middle;">Code</td>\n      <td style="text-align: left;">MultiPL-E</td>\n      <td style="text-align: center;">82.7</td>\n      <td style="text-align: center;">79.3</td>\n      <td style="text-align: center;"><strong>83.8</strong></td>\n      <td style="text-align: center;">81.4</td>\n      <td style="text-align: center;">81.5</td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr style="border-top: 1px solid #ddd;">\n      <td rowspan="3" style="text-align: left; vertical-align: middle;">Alignment<br>Tasks</td>\n      <td style="text-align: left;">IFEval</td>\n      <td style="text-align: center;">83.9</td>\n      <td style="text-align: center;">83.2</td>\n      <td style="text-align: center;"><strong>84.7</strong></td>\n      <td style="text-align: center;">81.0</td>\n      <td style="text-align: center;">81.7</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">Creative Writing v3</td>\n      <td style="text-align: center;">84.9</td>\n      <td style="text-align: center;">80.4</td>\n      <td style="text-align: center;"><strong>86.0</strong></td>\n      <td style="text-align: center;">80.6</td>\n      <td style="text-align: center;">81.8</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">WritingBench</td>\n      <td style="text-align: center;">75.5</td>\n      <td style="text-align: center;">77.0</td>\n      <td style="text-align: center;"><strong>85.5</strong></td>\n      <td style="text-align: center;">82.6</td>\n      <td style="text-align: center;">83.0</td>\n    </tr>\n    <tr>\n      <td style="text-align: left; vertical-align: middle;">Agent</td>\n      <td style="text-align: left;">BFCL-v3</td>\n      <td style="text-align: center;">66.5</td>\n      <td style="text-align: center;"><strong>68.0</strong></td>\n      <td style="text-align: center;">65.1</td>\n      <td style="text-align: center;">64.4</td>\n      <td style="text-align: center;">65.0</td>\n    </tr>\n    <tr>\n      <td rowspan="2" style="text-align: left; vertical-align: middle;">Multilingual<br>Tasks</td>\n      <td style="text-align: left;">MultiIF</td>\n      <td style="text-align: center;"><strong>70.4</strong></td>\n      <td style="text-align: center;">70.2</td>\n      <td style="text-align: center;">67.9</td>\n      <td style="text-align: center;">64.0</td>\n      <td style="text-align: center;">64.7</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">PolyMATH</td>\n      <td style="text-align: center;">25.5</td>\n      <td style="text-align: center;">27.0</td>\n      <td style="text-align: center;"><strong>43.1</strong></td>\n      <td style="text-align: center;">37.9</td>\n      <td style="text-align: center;">39.3</td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr style="border-bottom: 1px solid black;">\n      <th></th>\n      <th></th>\n      <th>Gemini-2.5-Flash<br>Thinking</th>\n      <th>Qwen3-235B-A22B<br>Thinking</th>\n      <th>Qwen3-30B-A3B-Thinking-2507</th>\n      <th>Qwen3-Omni-30B-A3B-Thinking</th>\n      <th>Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan="2"><em>General<br>Tasks</em></td>\n      <td>MMLU-Redux</td>\n      <td>92.1</td>\n      <td><b>92.7</b></td>\n      <td>91.4</td>\n      <td>88.8</td>\n      <td>89.7</td>\n    </tr>\n    <tr style="border-top: 1px solid #ddd;">\n      <td>GPQA</td>\n      <td><b>82.8</b></td>\n      <td>71.1</td>\n      <td>73.4</td>\n      <td>73.1</td>\n      <td>73.1</td>\n    </tr>\n    <tr style="border-top: 1px solid black;">\n      <td rowspan="2"><em>Reasoning</em></td>\n      <td>AIME25</td>\n      <td>72.0</td>\n      <td>81.5</td>\n      <td><b>85.0</b></td>\n      <td>73.7</td>\n      <td>74.0</td>\n    </tr>\n    <tr style="border-top: 1px solid #ddd;">\n      <td>LiveBench 20241125</td>\n      <td>74.3</td>\n      <td><b>77.1</b></td>\n      <td>76.8</td>\n      <td>71.8</td>\n      <td>70.3</td>\n    </tr>\n    <tr style="border-top: 1px solid black;">\n      <td><em>Code</em></td>\n      <td>MultiPL-E</td>\n      <td><b>84.5</b></td>\n      <td>79.9</td>\n      <td>81.3</td>\n      <td>80.6</td>\n      <td>81.0</td>\n    </tr>\n    <tr style="border-top: 1px solid #ddd;">\n      <td rowspan="4"><em>Alignment<br>Tasks</em></td>\n      <td>IFEval</td>\n      <td><b>89.8</b></td>\n      <td>83.4</td>\n      <td>88.9</td>\n      <td>85.1</td>\n      <td>85.2</td>\n    </tr>\n    <tr style="border-top: 1px solid #ddd;">\n      <td>Arena-Hard v2</td>\n      <td>56.7</td>\n      <td><b>61.5</b></td>\n      <td>56.0</td>\n      <td>55.1</td>\n      <td>57.8</td>\n    </tr>\n    <tr style="border-top: 1px solid #ddd;">\n      <td>Creative Writing v3</td>\n      <td><b>85.0</b></td>\n      <td>84.6</td>\n      <td>84.4</td>\n      <td>82.5</td>\n      <td>83.6</td>\n    </tr>\n    <tr style="border-top: 1px solid #ddd;">\n      <td>WritingBench</td>\n      <td>83.9</td>\n      <td>80.3</td>\n      <td>85.0</td>\n      <td>85.5</td>\n      <td><b>85.9</b></td>\n    </tr>\n    <tr style="border-top: 1px solid black;">\n      <td><em>Agent</em></td>\n      <td>BFCL-v3</td>\n      <td>68.6</td>\n      <td>70.8</td>\n      <td><b>72.4</b></td>\n      <td>63.2</td>\n      <td>64.5</td>\n    </tr>\n    <tr style="border-top: 1px solid black;">\n      <td rowspan="2"><em>Multilingual<br>Tasks</em></td>\n      <td>MultiIF</td>\n      <td>74.4</td>\n      <td>71.9</td>\n      <td><b>76.4</b></td>\n      <td>72.9</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <td>PolyMATH</td>\n      <td>49.8</td>\n      <td><b>54.7</b></td>\n      <td>52.6</td>\n      <td>47.1</td>\n      <td>48.7</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Audio -> Text</summary>\n\n<table style="width:100%; border-collapse: collapse;">\n<thead>\n  <tr>\n    <th align="left" style="padding: 8px;"></th>\n    <th align="center" style="padding: 8px;">Seed-ASR</th>\n    <th align="center" style="padding: 8px;">Voxtral-Mini</th>\n    <th align="center" style="padding: 8px;">Voxtral-Small</th>\n    <th align="center" style="padding: 8px;">GPT-4o-Transcribe</th>\n    <th align="center" style="padding: 8px;">Gemini-2.5-Pro</th>\n    <th align="center" style="padding: 8px;">Qwen2.5-Omni</th>\n    <th align="center" style="padding: 8px;">Qwen3-Omni-30B-A3B-Instruct</th>\n    <th align="center" style="padding: 8px;">Qwen3-Omni-Flash-Instruct</th>\n  </tr>\n</thead>\n<tbody>\n  <tr style="border-top: 1px solid #333;">\n    <td colspan="9" align="center"; style="border-top: 1px solid black; border-bottom: 1px solid black;"><em>EN & ZH ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Wenetspeech<br><em>net</em> | <em>meeting</em></td>\n    <td align="center" style="padding: 8px;">4.66 | <strong>5.69</strong></td>\n    <td align="center" style="padding: 8px;">24.30 | 31.53</td>\n    <td align="center" style="padding: 8px;">20.33 | 26.08</td>\n    <td align="center" style="padding: 8px;">15.30 | 32.27</td>\n    <td align="center" style="padding: 8px;">14.43 | 13.47</td>\n    <td align="center" style="padding: 8px;">5.91 | 7.65</td>\n    <td align="center" style="padding: 8px;">4.69 | 5.89</td>\n    <td align="center" style="padding: 8px;"><strong>4.62</strong> | 5.75</td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Librispeech<br><em>clean</em> | <em>other</em></td>\n    <td align="center" style="padding: 8px;">1.58 | 2.84</td>\n    <td align="center" style="padding: 8px;">1.88 | 4.12</td>\n    <td align="center" style="padding: 8px;">1.56 | 3.30</td>\n    <td align="center" style="padding: 8px;">1.39 | 3.75</td>\n    <td align="center" style="padding: 8px;">2.89 | 3.56</td>\n    <td align="center" style="padding: 8px;">1.74 | 3.45</td>\n    <td align="center" style="padding: 8px;"><strong>1.22</strong> | 2.48</td>\n    <td align="center" style="padding: 8px;">1.27 | <strong>2.44</strong></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">CV15-en</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">9.47</td>\n    <td align="center" style="padding: 8px;">7.79</td>\n    <td align="center" style="padding: 8px;">10.01</td>\n    <td align="center" style="padding: 8px;">9.89</td>\n    <td align="center" style="padding: 8px;">7.61</td>\n    <td align="center" style="padding: 8px;">6.05</td>\n    <td align="center" style="padding: 8px;"><strong>5.94</strong></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">CV15-zh</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">24.67</td>\n    <td align="center" style="padding: 8px;">19.30</td>\n    <td align="center" style="padding: 8px;">9.84</td>\n    <td align="center" style="padding: 8px;">8.00</td>\n    <td align="center" style="padding: 8px;">5.13</td>\n    <td align="center" style="padding: 8px;">4.31</td>\n    <td align="center" style="padding: 8px;"><strong>4.28</strong></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-en</td>\n    <td align="center" style="padding: 8px;">3.40</td>\n    <td align="center" style="padding: 8px;">3.96</td>\n    <td align="center" style="padding: 8px;">3.77</td>\n    <td align="center" style="padding: 8px;">3.32</td>\n    <td align="center" style="padding: 8px;">2.94</td>\n    <td align="center" style="padding: 8px;">3.77</td>\n    <td align="center" style="padding: 8px;"><strong>2.72</strong></td>\n    <td align="center" style="padding: 8px;">2.74</td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-zh</td>\n    <td align="center" style="padding: 8px;">2.69</td>\n    <td align="center" style="padding: 8px;">12.22</td>\n    <td align="center" style="padding: 8px;">7.98</td>\n    <td align="center" style="padding: 8px;">2.44</td>\n    <td align="center" style="padding: 8px;">2.71</td>\n    <td align="center" style="padding: 8px;">2.54</td>\n    <td align="center" style="padding: 8px;">2.20</td>\n    <td align="center" style="padding: 8px;"><strong>2.19</strong></td>\n  </tr>\n  <tr style="border-top: 1px solid #333;">\n    <td colspan="9" align="center"; style="border-top: 1px solid black; border-bottom: 1px solid black;"><em>Multilingual ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-avg<br>(19 lang)</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">15.67</td>\n    <td align="center" style="padding: 8px;">8.09</td>\n    <td align="center" style="padding: 8px;">4.48</td>\n    <td align="center" style="padding: 8px;">5.55</td>\n    <td align="center" style="padding: 8px;">14.04</td>\n    <td align="center" style="padding: 8px;">5.33</td>\n    <td align="center" style="padding: 8px;"><strong>5.31</strong></td>\n  </tr>\n  <tr style="border-top: 1px solid #333;">\n    <td colspan="9" align="center"; style="border-top: 1px solid black; border-bottom: 1px solid black;"><em>Lyric ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">MIR-1K (vocal-only)</td>\n    <td align="center" style="padding: 8px;">6.45</td>\n    <td align="center" style="padding: 8px;">23.33</td>\n    <td align="center" style="padding: 8px;">18.73</td>\n    <td align="center" style="padding: 8px;">11.87</td>\n    <td align="center" style="padding: 8px;">9.85</td>\n    <td align="center" style="padding: 8px;">8.15</td>\n    <td align="center" style="padding: 8px;">5.90</td>\n    <td align="center" style="padding: 8px;"><strong>5.85</strong></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Opencpop-test</td>\n    <td align="center" style="padding: 8px;">2.98</td>\n    <td align="center" style="padding: 8px;">31.01</td>\n    <td align="center" style="padding: 8px;">16.06</td>\n    <td align="center" style="padding: 8px;">7.93</td>\n    <td align="center" style="padding: 8px;">6.49</td>\n    <td align="center" style="padding: 8px;">2.84</td>\n    <td align="center" style="padding: 8px;"><strong>1.54</strong></td>\n    <td align="center" style="padding: 8px;">2.02</td>\n  </tr>\n  <tr style="border-top: 1px solid #333;">\n    <td colspan="9" align="center"; style="border-top: 1px solid black; border-bottom: 1px solid black;"><em>S2TT (BLEU)</em></td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-en2xx</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">30.35</td>\n    <td align="center" style="padding: 8px;">37.85</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;"><strong>39.25</strong></td>\n    <td align="center" style="padding: 8px;">29.22</td>\n    <td align="center" style="padding: 8px;">37.50</td>\n    <td align="center" style="padding: 8px;">36.22</td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-xx2en</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">27.54</td>\n    <td align="center" style="padding: 8px;">32.81</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;"><strong>35.41</strong></td>\n    <td align="center" style="padding: 8px;">28.61</td>\n    <td align="center" style="padding: 8px;">31.08</td>\n    <td align="center" style="padding: 8px;">30.71</td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-zh2xx</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">17.03</td>\n    <td align="center" style="padding: 8px;">22.05</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;"><strong>26.63</strong></td>\n    <td align="center" style="padding: 8px;">17.97</td>\n    <td align="center" style="padding: 8px;">25.17</td>\n    <td align="center" style="padding: 8px;">25.10</td>\n  </tr>\n  <tr>\n    <td align="left" style="padding: 8px;">Fleurs-xx2zh</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;">28.75</td>\n    <td align="center" style="padding: 8px;">34.82</td>\n    <td align="center" style="padding: 8px;">-</td>\n    <td align="center" style="padding: 8px;"><strong>37.50</strong></td>\n    <td align="center" style="padding: 8px;">27.68</td>\n    <td align="center" style="padding: 8px;">33.13</td>\n    <td align="center" style="padding: 8px;">31.19</td>\n  </tr>\n</tbody>\n</table>\n\n<table style="width:100%; border-collapse: collapse;">\n  <thead>\n    <tr style="border-bottom: 1px solid #ddd;">\n      <th style="text-align:left; padding: 8px;"></th>\n      <th style="text-align:center; padding: 8px;">GPT-4o-Audio</th>\n      <th style="text-align:center; padding: 8px;">Gemini-2.5-Flash</th>\n      <th style="text-align:center; padding: 8px;">Gemini-2.5-Pro</th>\n      <th style="text-align:center; padding: 8px;">Qwen2.5-Omni</th>\n      <th style="text-align:center; padding: 8px;">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style="text-align:center; padding: 8px;">Qwen3-Omni-30B-A3B-Thinking</th>\n      <th style="text-align:center; padding: 8px;">Qwen3-Omni-Flash-Instruct</th>\n      <th style="text-align:center; padding: 8px;">Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan="9" align="center" style="padding: 8px; font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;"><strong>VoiceBench</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">AlpacaEval</td>\n      <td style="text-align:center; padding: 8px;">95.6</td>\n      <td style="text-align:center; padding: 8px;">96.1</td>\n      <td style="text-align:center; padding: 8px;">94.3</td>\n      <td style="text-align:center; padding: 8px;">89.9</td>\n      <td style="text-align:center; padding: 8px;">94.8</td>\n      <td style="text-align:center; padding: 8px;">96.4</td>\n      <td style="text-align:center; padding: 8px;">95.4</td>\n      <td style="text-align:center; padding: 8px;"><strong>96.8</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">CommonEval</td>\n      <td style="text-align:center; padding: 8px;">89.8</td>\n      <td style="text-align:center; padding: 8px;">88.3</td>\n      <td style="text-align:center; padding: 8px;">88.4</td>\n      <td style="text-align:center; padding: 8px;">76.7</td>\n      <td style="text-align:center; padding: 8px;">90.8</td>\n      <td style="text-align:center; padding: 8px;">90.5</td>\n      <td style="text-align:center; padding: 8px;"><strong>91.0</strong></td>\n      <td style="text-align:center; padding: 8px;">90.9</td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">WildVoice</td>\n      <td style="text-align:center; padding: 8px;">91.6</td>\n      <td style="text-align:center; padding: 8px;">92.1</td>\n      <td style="text-align:center; padding: 8px;">93.4</td>\n      <td style="text-align:center; padding: 8px;">77.7</td>\n      <td style="text-align:center; padding: 8px;">91.6</td>\n      <td style="text-align:center; padding: 8px;">90.5</td>\n      <td style="text-align:center; padding: 8px;"><strong>92.3</strong></td>\n      <td style="text-align:center; padding: 8px;">90.9</td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">SD-QA</td>\n      <td style="text-align:center; padding: 8px;">75.5</td>\n      <td style="text-align:center; padding: 8px;">84.5</td>\n      <td style="text-align:center; padding: 8px;"><strong>90.1</strong></td>\n      <td style="text-align:center; padding: 8px;">56.4</td>\n      <td style="text-align:center; padding: 8px;">76.9</td>\n      <td style="text-align:center; padding: 8px;">78.1</td>\n      <td style="text-align:center; padding: 8px;">76.8</td>\n      <td style="text-align:center; padding: 8px;">78.5</td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">MMSU</td>\n      <td style="text-align:center; padding: 8px;">80.3</td>\n      <td style="text-align:center; padding: 8px;">66.1</td>\n      <td style="text-align:center; padding: 8px;">71.1</td>\n      <td style="text-align:center; padding: 8px;">61.7</td>\n      <td style="text-align:center; padding: 8px;">68.1</td>\n      <td style="text-align:center; padding: 8px;">83.0</td>\n      <td style="text-align:center; padding: 8px;">68.4</td>\n      <td style="text-align:center; padding: 8px;"><strong>84.3</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">OpenBookQA</td>\n      <td style="text-align:center; padding: 8px;">89.2</td>\n      <td style="text-align:center; padding: 8px;">56.9</td>\n      <td style="text-align:center; padding: 8px;">92.3</td>\n      <td style="text-align:center; padding: 8px;">80.9</td>\n      <td style="text-align:center; padding: 8px;">89.7</td>\n      <td style="text-align:center; padding: 8px;">94.3</td>\n      <td style="text-align:center; padding: 8px;">91.4</td>\n      <td style="text-align:center; padding: 8px;"><strong>95.0</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">BBH</td>\n      <td style="text-align:center; padding: 8px;">84.1</td>\n      <td style="text-align:center; padding: 8px;">83.9</td>\n      <td style="text-align:center; padding: 8px;"><strong>92.6</strong></td>\n      <td style="text-align:center; padding: 8px;">66.7</td>\n      <td style="text-align:center; padding: 8px;">80.4</td>\n      <td style="text-align:center; padding: 8px;">88.9</td>\n      <td style="text-align:center; padding: 8px;">80.6</td>\n      <td style="text-align:center; padding: 8px;">89.6</td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">IFEval</td>\n      <td style="text-align:center; padding: 8px;">76.0</td>\n      <td style="text-align:center; padding: 8px;">83.8</td>\n      <td style="text-align:center; padding: 8px;"><strong>85.7</strong></td>\n      <td style="text-align:center; padding: 8px;">53.5</td>\n      <td style="text-align:center; padding: 8px;">77.8</td>\n      <td style="text-align:center; padding: 8px;">80.6</td>\n      <td style="text-align:center; padding: 8px;">75.2</td>\n      <td style="text-align:center; padding: 8px;">80.8</td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">AdvBench</td>\n      <td style="text-align:center; padding: 8px;">98.7</td>\n      <td style="text-align:center; padding: 8px;">98.9</td>\n      <td style="text-align:center; padding: 8px;">98.1</td>\n      <td style="text-align:center; padding: 8px;">99.2</td>\n      <td style="text-align:center; padding: 8px;"><strong>99.3</strong></td>\n      <td style="text-align:center; padding: 8px;">97.2</td>\n      <td style="text-align:center; padding: 8px;"><strong>99.4</strong></td>\n      <td style="text-align:center; padding: 8px;">98.9</td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">Overall</td>\n      <td style="text-align:center; padding: 8px;">86.8</td>\n      <td style="text-align:center; padding: 8px;">83.4</td>\n      <td style="text-align:center; padding: 8px;"><strong>89.6</strong></td>\n      <td style="text-align:center; padding: 8px;">73.6</td>\n      <td style="text-align:center; padding: 8px;">85.5</td>\n      <td style="text-align:center; padding: 8px;">88.8</td>\n      <td style="text-align:center; padding: 8px;">85.6</td>\n      <td style="text-align:center; padding: 8px;">89.5</td>\n    </tr>\n    <tr>\n      <td colspan="9" align="center" style="padding: 8px; font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;"><strong>Audio Reasoning</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align:left; padding: 8px;">MMAU-v05.15.25</td>\n      <td style="text-align:center; padding: 8px;">62.5</td>\n      <td style="text-align:center; padding: 8px;">71.8</td>\n      <td style="text-align:center; padding: 8px;">77.4</td>\n      <td style="text-align:center; padding: 8px;">65.5</td>\n      <td style="text-align:center; padding: 8px;">77.5</td>\n      <td style="text-align:center; padding: 8px;">75.4</td>\n      <td style="text-align:center; padding: 8px;"><strong>77.6</strong></td>\n      <td style="text-align:center; padding: 8px;">76.5</td>\n    </tr>\n    <tr">\n      <td style="text-align:left; padding: 8px;">MMSU</td>\n      <td style="text-align:center; padding: 8px;">56.4</td>\n      <td style="text-align:center; padding: 8px;">70.2</td>\n      <td style="text-align:center; padding: 8px;"><strong>77.7</strong></td>\n      <td style="text-align:center; padding: 8px;">62.6</td>\n      <td style="text-align:center; padding: 8px;">69.0</td>\n      <td style="text-align:center; padding: 8px;">70.2</td>\n      <td style="text-align:center; padding: 8px;">69.1</td>\n      <td style="text-align:center; padding: 8px;">71.3</td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr style="border-bottom: 1px solid black;">\n      <th style="text-align: left;"></th>\n      <th style="text-align: center;">Best Specialist<br>Models</th>\n      <th style="text-align: center;">GPT-4o-Audio</th>\n      <th style="text-align: center;">Gemini-2.5-Pro</th>\n      <th style="text-align: center;">Qwen2.5-Omni</th>\n      <th style="text-align: center;">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style="text-align: center;">Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style="text-align: left;">RUL-MuchoMusic</td>\n      <td style="text-align: center;">47.6 (Audio Flamingo 3)</td>\n      <td style="text-align: center;">36.1</td>\n      <td style="text-align: center;">49.4</td>\n      <td style="text-align: center;">47.3</td>\n      <td style="text-align: center;">52.0</td>\n      <td style="text-align: center;"><strong>52.1</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">GTZAN<br><em>Acc.</em></td>\n      <td style="text-align: center;">87.9 (CLaMP 3)</td>\n      <td style="text-align: center;">76.5</td>\n      <td style="text-align: center;">81.0</td>\n      <td style="text-align: center;">81.7</td>\n      <td style="text-align: center;">93.0</td>\n      <td style="text-align: center;"><strong>93.1</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MTG Genre<br><em>Micro F1</em></td>\n      <td style="text-align: center;">35.8 (MuQ-MuLan)</td>\n      <td style="text-align: center;">25.3</td>\n      <td style="text-align: center;">32.6</td>\n      <td style="text-align: center;">32.5</td>\n      <td style="text-align: center;">39.0</td>\n      <td style="text-align: center;"><strong>39.5</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MTG Mood/Theme<br><em>Micro F1</em></td>\n      <td style="text-align: center;">10.9 (MuQ-MuLan)</td>\n      <td style="text-align: center;">11.3</td>\n      <td style="text-align: center;">14.1</td>\n      <td style="text-align: center;">8.9</td>\n      <td style="text-align: center;">21.0</td>\n      <td style="text-align: center;"><strong>21.7</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MTG Instrument<br><em>Micro F1</em></td>\n      <td style="text-align: center;">39.8 (MuQ-MuLan)</td>\n      <td style="text-align: center;">34.2</td>\n      <td style="text-align: center;">33.0</td>\n      <td style="text-align: center;">22.6</td>\n      <td style="text-align: center;">40.5</td>\n      <td style="text-align: center;"><strong>40.7</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MTG Top50<br><em>Micro F1</em></td>\n      <td style="text-align: center;">33.2 (MuQ-MuLan)</td>\n      <td style="text-align: center;">25.0</td>\n      <td style="text-align: center;">26.1</td>\n      <td style="text-align: center;">21.6</td>\n      <td style="text-align: center;">36.7</td>\n      <td style="text-align: center;"><strong>36.9</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MagnaTagATune<br><em>Micro F1</em></td>\n      <td style="text-align: center;">41.6 (MuQ)</td>\n      <td style="text-align: center;">29.2</td>\n      <td style="text-align: center;">28.1</td>\n      <td style="text-align: center;">30.1</td>\n      <td style="text-align: center;">44.3</td>\n      <td style="text-align: center;"><strong>46.8</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Vision -> Text</summary>\n\n<table style="width:100%; border-collapse: collapse;">\n  <thead>\n    <tr style="border-bottom: 1px solid black;">\n      <th style="text-align: left;">Datasets</th>\n      <th style="text-align: center;">GPT4-o</th>\n      <th style="text-align: center;">Gemini-2.0-Flash</th>\n      <th style="text-align: center;">Qwen2.5-VL<br>72B</th>\n      <th style="text-align: center;">Qwen3-Omni-30B-A3B<br>-Instruct</th>\n      <th style="text-align: center;">Qwen3-Omni-Flash<br>-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan="6" align="center" style="font-weight: bold; border-top: 1px solid #ddd; border-bottom: 1px solid black;">General Visual Question Answering</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MMStar</td>\n      <td style="text-align: center;">64.7</td>\n      <td style="text-align: center;"><strong>71.4</strong></td>\n      <td style="text-align: center;">70.8</td>\n      <td style="text-align: center;">68.5</td>\n      <td style="text-align: center;">69.3</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">HallusionBench</td>\n      <td style="text-align: center;">55.0</td>\n      <td style="text-align: center;">56.3</td>\n      <td style="text-align: center;">55.2</td>\n      <td style="text-align: center;"><strong>59.7</strong></td>\n      <td style="text-align: center;">58.5</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MM-MT-Bench</td>\n      <td style="text-align: center;"><strong>7.7</strong></td>\n      <td style="text-align: center;">6.7</td>\n      <td style="text-align: center;">7.6</td>\n      <td style="text-align: center;">7.4</td>\n      <td style="text-align: center;">7.6</td>\n    </tr>\n    <tr>\n      <td colspan="6" align="center" style="font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;">Math & STEM</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MMMU_val</td>\n      <td style="text-align: center;">69.1</td>\n      <td style="text-align: center;"><strong>71.3</strong></td>\n      <td style="text-align: center;">70.2</td>\n      <td style="text-align: center;">69.1</td>\n      <td style="text-align: center;">69.8</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MMMU_pro</td>\n      <td style="text-align: center;">51.9</td>\n      <td style="text-align: center;">56.1</td>\n      <td style="text-align: center;">51.1</td>\n      <td style="text-align: center;">57.0</td>\n      <td style="text-align: center;"><strong>57.6</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MathVista_mini</td>\n      <td style="text-align: center;">63.8</td>\n      <td style="text-align: center;">71.4</td>\n      <td style="text-align: center;">74.8</td>\n      <td style="text-align: center;">75.9</td>\n      <td style="text-align: center;"><strong>77.4</strong></td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MathVision_full</td>\n      <td style="text-align: center;">30.4</td>\n      <td style="text-align: center;">48.6</td>\n      <td style="text-align: center;">38.1</td>\n      <td style="text-align: center;">56.3</td>\n      <td style="text-align: center;"><strong>58.3</strong></td>\n    </tr>\n    <tr>\n      <td colspan="6" align="center" style="font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;">Documentation Understanding</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">AI2D</td>\n      <td style="text-align: center;">84.6</td>\n      <td style="text-align: center;">86.7</td>\n      <td style="text-align: center;"><strong>88.7</strong></td>\n      <td style="text-align: center;">85.2</td>\n      <td style="text-align: center;">86.4</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ChartQA_test</td>\n      <td style="text-align: center;">86.7</td>\n      <td style="text-align: center;">64.6</td>\n      <td style="text-align: center;"><strong>89.5</strong></td>\n      <td style="text-align: center;">86.8</td>\n      <td style="text-align: center;">87.1</td>\n    </tr>\n    <tr>\n      <td colspan="6" align="center" style="font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;">Counting</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">CountBench</td>\n      <td style="text-align: center;">87.9</td>\n      <td style="text-align: center;">91.2</td>\n      <td style="text-align: center;"><strong>93.6</strong></td>\n      <td style="text-align: center;">90.0</td>\n      <td style="text-align: center;">90.0</td>\n    </tr>\n    <tr>\n      <td colspan="6" align="center" style="font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;">Video Understanding</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">Video-MME</td>\n      <td style="text-align: center;">71.9</td>\n      <td style="text-align: center;">72.4</td>\n      <td style="text-align: center;"><strong>73.3</strong></td>\n      <td style="text-align: center;">70.5</td>\n      <td style="text-align: center;">71.4</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">LVBench</td>\n      <td style="text-align: center;">30.8</td>\n      <td style="text-align: center;"><strong>57.9</strong></td>\n      <td style="text-align: center;">47.3</td>\n      <td style="text-align: center;">50.2</td>\n      <td style="text-align: center;">51.1</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">MLVU</td>\n      <td style="text-align: center;">64.6</td>\n      <td style="text-align: center;">71.0</td>\n      <td style="text-align: center;">74.6</td>\n      <td style="text-align: center;">75.2</td>\n      <td style="text-align: center;"><strong>75.7</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<table style="width: 100%; border-collapse: collapse;">\n  <thead style="border-bottom: 1px solid black;">\n    <tr>\n      <th align="left" style="padding: 6px;">Datasets</th>\n      <th align="center" style="padding: 6px;">Gemini-2.5-flash-thinking</th>\n      <th align="center" style="padding: 6px;">InternVL-3.5-241B-A28B</th>\n      <th align="center" style="padding: 6px;">Qwen3-Omni-30B-A3B-Thinking</th>\n      <th align="center" style="padding: 6px;">Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr style="border-top: 2px solid black; border-bottom: 1px solid #ccc;">\n      <td colspan="5" align="center" style="padding: 6px 0; font-weight: bold; border-bottom: 1px solid black;">General Visual Question Answering</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MMStar</td>\n      <td align="center" style="padding: 6px;">75.5</td>\n      <td align="center" style="padding: 6px;"><b>77.9</b></td>\n      <td align="center" style="padding: 6px;">74.9</td>\n      <td align="center" style="padding: 6px;">75.5</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">HallusionBench</td>\n      <td align="center" style="padding: 6px;">61.1</td>\n      <td align="center" style="padding: 6px;">57.3</td>\n      <td align="center" style="padding: 6px;">62.8</td>\n      <td align="center" style="padding: 6px;"><b>63.4</b></td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MM-MT-Bench</td>\n      <td align="center" style="padding: 6px;">7.8</td>\n      <td align="center" style="padding: 6px;">‚Äì</td>\n      <td align="center" style="padding: 6px;"><b>8.0</b></td>\n      <td align="center" style="padding: 6px;"><b>8.0</b></td>\n    </tr>\n    <tr style="border-top: 1px solid black; border-bottom: 1px solid #ccc;">\n      <td colspan="5" align="center" style="padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;">Math & STEM</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MMMU_val</td>\n      <td align="center" style="padding: 6px;">76.9</td>\n      <td align="center" style="padding: 6px;"><b>77.7</b></td>\n      <td align="center" style="padding: 6px;">75.6</td>\n      <td align="center" style="padding: 6px;">75.0</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MMMU_pro</td>\n      <td align="center" style="padding: 6px;"><b>65.8</b></td>\n      <td align="center" style="padding: 6px;">‚Äì</td>\n      <td align="center" style="padding: 6px;">60.5</td>\n      <td align="center" style="padding: 6px;">60.8</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MathVista_mini</td>\n      <td align="center" style="padding: 6px;">77.6</td>\n      <td align="center" style="padding: 6px;"><b>82.7</b></td>\n      <td align="center" style="padding: 6px;">80.0</td>\n      <td align="center" style="padding: 6px;">81.2</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MathVision_full</td>\n      <td align="center" style="padding: 6px;">62.3</td>\n      <td align="center" style="padding: 6px;"><b>63.9</b></td>\n      <td align="center" style="padding: 6px;">62.9</td>\n      <td align="center" style="padding: 6px;">63.8</td>\n    </tr>\n    <tr style="border-top: 1px solid black; border-bottom: 1px solid #ccc;">\n      <td colspan="5" align="center" style="padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;">Documentation Understanding</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">AI2D_test</td>\n      <td align="center" style="padding: 6px;"><b>88.6</b></td>\n      <td align="center" style="padding: 6px;">87.3</td>\n      <td align="center" style="padding: 6px;">86.1</td>\n      <td align="center" style="padding: 6px;">86.8</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">ChartQA_test</td>\n      <td align="center" style="padding: 6px;">‚Äì</td>\n      <td align="center" style="padding: 6px;">88.0</td>\n      <td align="center" style="padding: 6px;"><b>89.5</b></td>\n      <td align="center" style="padding: 6px;">89.3</td>\n    </tr>\n    <tr style="border-top: 1px solid black; border-bottom: 1px solid #ccc;">\n      <td colspan="5" align="center" style="padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;">Counting</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">CountBench</td>\n      <td align="center" style="padding: 6px;">88.6</td>\n      <td align="center" style="padding: 6px;">‚Äì</td>\n      <td align="center" style="padding: 6px;">88.6</td>\n      <td align="center" style="padding: 6px;"><b>92.5</b></td>\n    </tr>\n    <tr style="border-top: 1px solid black; border-bottom: 1px solid #ccc;">\n      <td colspan="5" align="center" style="padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;">Video Understanding</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">Video-MME</td>\n      <td align="center" style="padding: 6px;"><b>79.6</b></td>\n      <td align="center" style="padding: 6px;">72.9</td>\n      <td align="center" style="padding: 6px;">69.7</td>\n      <td align="center" style="padding: 6px;">69.8</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">LVBench</td>\n      <td align="center" style="padding: 6px;"><b>64.5</b></td>\n      <td align="center" style="padding: 6px;">‚Äì</td>\n      <td align="center" style="padding: 6px;">49.0</td>\n      <td align="center" style="padding: 6px;">49.5</td>\n    </tr>\n    <tr>\n      <td style="padding: 6px;">MLVU</td>\n      <td align="center" style="padding: 6px;"><b>82.1</b></td>\n      <td align="center" style="padding: 6px;">78.2</td>\n      <td align="center" style="padding: 6px;">72.9</td>\n      <td align="center" style="padding: 6px;">73.9</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>AudioVisual -> Text</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th>Datasets</th>\n      <th>Previous Open-source SoTA</th>\n      <th>Gemini-2.5-Flash</th>\n      <th>Qwen2.5-Omni</th>\n      <th>Qwen3-Omni-30B-A3B-Instruct</th>\n      <th>Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>WorldSense</td>\n      <td>47.1</td>\n      <td>50.9</td>\n      <td>45.4</td>\n      <td>54.0</td>\n      <td><strong>54.1</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr>\n      <th>Datasets</th>\n      <th>Previous Open-source SoTA</th>\n      <th>Gemini-2.5-Flash-Thinking</th>\n      <th>Qwen3-Omni-30B-A3B-Thinking</th>\n      <th>Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>DailyOmni</td>\n      <td>69.8</td>\n      <td>72.7</td>\n      <td>75.8</b></td>\n      <td><b>76.2</td>\n    </tr>\n    <tr>\n      <td>VideoHolmes</td>\n      <td>55.6</td>\n      <td>49.5</td>\n      <td><b>57.3</b></td>\n      <td><b>57.3</b></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n\n<details>\n<summary>Zero-shot Speech Generation</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th align="left">Datasets</th>\n      <th align="left">Model</th>\n      <th align="left">Performance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>&nbsp;</td>\n      <td colspan="2" align="center"><em>Content Consistency</em></td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr>\n      <td rowspan="10" align="center" valign="middle"><strong>SEED</strong><br><em>test-zh</em> | <em>test-en</em></td>\n      <td align="left">Seed-TTS<sub>ICL</sub></td>\n      <td align="left">1.11 | 2.24</td>\n    </tr>\n    <tr>\n      <td align="left">Seed-TTS<sub>RL</sub></td>\n      <td align="left">1.00 | 1.94</td>\n    </tr>\n    <tr>\n      <td align="left">MaskGCT</td>\n      <td align="left">2.27 | 2.62</td>\n    </tr>\n    <tr>\n      <td align="left">E2 TTS</td>\n      <td align="left">1.97 | 2.19</td>\n    </tr>\n    <tr>\n      <td align="left">F5-TTS</td>\n      <td align="left">1.56 | 1.83</td>\n    </tr>\n    <tr>\n      <td align="left">Spark TTS</td>\n      <td align="left">1.20 | 1.98</td>\n    </tr>\n    <tr>\n      <td align="left">CosyVoice 2</td>\n      <td align="left">1.45 | 2.57</td>\n    </tr>\n    <tr>\n      <td align="left">CosyVoice 3</td>\n      <td align="left"><strong>0.71</strong> | 1.45</td>\n    </tr>\n    <tr>\n      <td align="left">Qwen2.5-Omni-7B</td>\n      <td align="left">1.42 | 2.33</td>\n    </tr>\n    <tr>\n      <td align="left">Qwen3-Omni-30B-A3B</td>\n      <td align="left">1.07 | <strong>1.39</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Multilingual Speech Generation </summary>\n\n<table>\n  <thead>\n    <tr>\n      <th rowspan="2" align="left">Language</th>\n      <th colspan="3" style="text-align:center; padding: 8px; font-weight: bold; border-bottom: 1px solid #ddd;">Content Consistency</th>\n      <th colspan="3"  style="text-align:center; padding: 8px; font-weight: bold; border-bottom: 1px solid #ddd;">Speaker Similarity</th>\n    </tr>\n    <tr>\n      <th align="center">Qwen3-Omni-30B-A3B</th>\n      <th align="center">MiniMax</th>\n      <th align="center">ElevenLabs</th>\n      <th align="center">Qwen3-Omni-30B-A3B</th>\n      <th align="center">MiniMax</th>\n      <th align="center">ElevenLabs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align="left">Chinese</td>\n      <td align="center"><strong>0.716</strong></td>\n      <td align="center">2.252</td>\n      <td align="center">16.026</td>\n      <td align="center">0.772</td>\n      <td align="center"><strong>0.780</strong></td>\n      <td align="center">0.677</td>\n    </tr>\n    <tr>\n      <td align="left">English</td>\n      <td align="center"><strong>1.069</strong></td>\n      <td align="center">2.164</td>\n      <td align="center">2.339</td>\n      <td align="center"><strong>0.773</strong></td>\n      <td align="center">0.756</td>\n      <td align="center">0.613</td>\n    </tr>\n    <tr>\n      <td align="left">German</td>\n      <td align="center">0.777</td>\n      <td align="center">1.906</td>\n      <td align="center"><strong>0.572</strong></td>\n      <td align="center"><strong>0.738</strong></td>\n      <td align="center">0.733</td>\n      <td align="center">0.614</td>\n    </tr>\n    <tr>\n      <td align="left">Italian</td>\n      <td align="center"><strong>1.067</strong></td>\n      <td align="center">1.543</td>\n      <td align="center">1.743</td>\n      <td align="center"><strong>0.742</strong></td>\n      <td align="center">0.699</td>\n      <td align="center">0.579</td>\n    </tr>\n    <tr>\n      <td align="left">Portuguese</td>\n      <td align="center">1.872</td>\n      <td align="center">1.877</td>\n      <td align="center"><strong>1.331</strong></td>\n      <td align="center">0.770</td>\n      <td align="center"><strong>0.805</strong></td>\n      <td align="center">0.711</td>\n    </tr>\n    <tr>\n      <td align="left">Spanish</td>\n      <td align="center">1.765</td>\n      <td align="center"><strong>1.029</strong></td>\n      <td align="center">1.084</td>\n      <td align="center">0.744</td>\n      <td align="center"><strong>0.762</strong></td>\n      <td align="center">0.615</td>\n    </tr>\n    <tr>\n      <td align="left">Japanese</td>\n      <td align="center">3.631</td>\n      <td align="center"><strong>3.519</strong></td>\n      <td align="center">10.646</td>\n      <td align="center">0.763</td>\n      <td align="center"><strong>0.776</strong></td>\n      <td align="center">0.738</td>\n    </tr>\n    <tr>\n      <td align="left">Korean</td>\n      <td align="center"><strong>1.670</strong></td>\n      <td align="center">1.747</td>\n      <td align="center">1.865</td>\n      <td align="center"><strong>0.778</strong></td>\n      <td align="center">0.776</td>\n      <td align="center">0.700</td>\n    </tr>\n    <tr>\n      <td align="left">French</td>\n      <td align="center"><strong>2.505</strong></td>\n      <td align="center">4.099</td>\n      <td align="center">5.216</td>\n      <td align="center"><strong>0.689</strong></td>\n      <td align="center">0.628</td>\n      <td align="center">0.535</td>\n    </tr>\n    <tr>\n      <td align="left">Russian</td>\n      <td align="center">3.986</td>\n      <td align="center">4.281</td>\n      <td align="center"><strong>3.878</strong></td>\n      <td align="center">0.759</td>\n      <td align="center"><strong>0.761</strong></td>\n      <td align="center">0.676</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Cross-Lingual Speech Generation </summary>\n\n<table>\n  <thead>\n    <tr>\n      <th style="text-align: left;">Language</th>\n      <th style="text-align: left;">Qwen3-Omni-30B-A3B</th>\n      <th style="text-align: left;">CosyVoice3</th>\n      <th style="text-align: left;">CosyVoice2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style="text-align: left;">en-to-zh</td>\n      <td style="text-align: left;">5.37</td>\n      <td style="text-align: left;"><strong>5.09</strong></td>\n      <td style="text-align: left;">13.5</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ja-to-zh</td>\n      <td style="text-align: left;">3.32</td>\n      <td style="text-align: left;"><strong>3.05</strong></td>\n      <td style="text-align: left;">48.1</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ko-to-zh</td>\n      <td style="text-align: left;"><strong>0.99</strong></td>\n      <td style="text-align: left;">1.06</td>\n      <td style="text-align: left;">7.70</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">zh-to-en</td>\n      <td style="text-align: left;"><strong>2.76</strong></td>\n      <td style="text-align: left;">2.98</td>\n      <td style="text-align: left;">6.47</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ja-to-en</td>\n      <td style="text-align: left;"><strong>3.31</strong></td>\n      <td style="text-align: left;">4.20</td>\n      <td style="text-align: left;">17.1</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ko-to-en</td>\n      <td style="text-align: left;"><strong>3.34</strong></td>\n      <td style="text-align: left;">4.19</td>\n      <td style="text-align: left;">11.2</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">zh-to-ja</td>\n      <td style="text-align: left;">8.29</td>\n      <td style="text-align: left;"><strong>7.08</strong></td>\n      <td style="text-align: left;">13.1</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">en-to-ja</td>\n      <td style="text-align: left;">7.53</td>\n      <td style="text-align: left;"><strong>6.80</strong></td>\n      <td style="text-align: left;">14.9</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ko-to-ja</td>\n      <td style="text-align: left;">4.24</td>\n      <td style="text-align: left;"><strong>3.93</strong></td>\n      <td style="text-align: left;">5.86</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">zh-to-ko</td>\n      <td style="text-align: left;"><strong>5.13</strong></td>\n      <td style="text-align: left;">14.4</td>\n      <td style="text-align: left;">24.8</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">en-to-ko</td>\n      <td style="text-align: left;"><strong>4.96</strong></td>\n      <td style="text-align: left;">5.87</td>\n      <td style="text-align: left;">21.9</td>\n    </tr>\n    <tr>\n      <td style="text-align: left;">ja-to-ko</td>\n      <td style="text-align: left;"><strong>6.23</strong></td>\n      <td style="text-align: left;">7.92</td>\n      <td style="text-align: left;">21.5</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n\n### Setting for Evaluation\n\n*   **Decoding Strategy**: For the Qwen3-Omni series across all evaluation benchmarks, `Instruct` models use greedy decoding during generation without sampling. For `Thinking` models, the decoding parameters should be taken from the `generation_config.json` file in the checkpoint.\n*   **Benchmark-Specific Formatting**: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to `fps=2` during evaluation.\n*   **Default Prompts**: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:\n\n| Task Type | Prompt |\n| :--- | :--- |\n| Auto Speech Recognition (ASR) for Chinese | ËØ∑Â∞ÜËøôÊÆµ‰∏≠ÊñáËØ≠Èü≥ËΩ¨Êç¢‰∏∫Á∫ØÊñáÊú¨„ÄÇ |\n| Auto Speech Recognition (ASR) for Other languages | Transcribe the <language> audio into text. |\n| Speech-to-Text Translation (S2TT) | Listen to the provided <source_language> speech and produce a translation in <target_language> text. |\n| Song Lyrics Recognition | Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations. |\n\n*   **System Prompt**: No `system prompt` should be set for any evaluation benchmark.\n*   **Input Sequence**: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come **after** multimodal data in the sequence. For example:\n\n```python\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "audio", "audio": "/path/to/audio.wav"},\n            {"type": "image", "image": "/path/to/image.png"},\n            {"type": "video", "video": "/path/to/video.mp4"},\n            {"type": "text", "text": "Describe the audio, image and video."},\n        ],\n    },\n]\n```\n\n\n<!-- ## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n```BibTeX\n@article{Qwen3-Omni,\n  title={Qwen3-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin},\n  journal={arXiv preprint arXiv},\n  year={2025}\n}\n``` -->\n\n<br>\n', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":35259818545,"storage_bytes":70523299202,"files_count":25,"spaces_count":8,"gated":false,"private":false,"config":{"architectures":["Qwen3OmniMoeForConditionalGeneration"],"model_type":"qwen3_omni_moe","processor_config":{"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {{- \"<|vision_start|><|image_pad|><|vision_end|>\" }}\n                {%- elif content.type == ''audio'' or ''audio'' in content or ''audio_url'' in content %}\n                    {{- \"<|audio_start|><|audio_pad|><|audio_end|>\" }}\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {{- \"<|vision_start|><|video_pad|><|vision_end|>\" }}\n                {%- elif content.type == ''text'' %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- endif %}\n    {{- ''\\n\\n'' }}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {%- if messages[0].content is string %}\n            {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {{- ''<|im_start|>system\\n'' +\"<|vision_start|><|image_pad|><|vision_end|>\"+ ''<|im_end|>\\n'' }}\n                {%- elif content.type == ''audio'' or ''audio'' in content or ''audio_url'' in content %}\n                    {{- ''<|im_start|>system\\n'' +\"<|audio_start|><|audio_pad|><|audio_end|>\"+ ''<|im_end|>\\n'' }}\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {{- ''<|im_start|>system\\n'' +\"<|vision_start|><|video_pad|><|vision_end|>\"+ ''<|im_end|>\\n'' }}\n                {%- elif content.type == ''text'' %}\n                    {{- ''<|im_start|>system\\n'' +content.text+ ''<|im_end|>\\n'' }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = namespace(text=\"\") %}\n        {%- for mcontent in message.content %}\n            {%- if mcontent.type == ''image'' or ''image'' in mcontent or ''image_url'' in mcontent %}\n                {%- set content.text = content.text~\"<|vision_start|><|image_pad|><|vision_end|>\" %}\n            {%- elif mcontent.type == ''audio'' or ''audio'' in mcontent or ''audio_url'' in mcontent %}\n                {%- set content.text = content.text~\"<|audio_start|><|audio_pad|><|audio_end|>\" %}\n            {%- elif mcontent.type == ''video'' or ''video'' in mcontent %}\n                {%- set content.text = content.text~\"<|vision_start|><|video_pad|><|vision_end|>\" %}\n            {%- elif mcontent.type == ''text'' %}\n                {%- set content.text = content.text~mcontent.text %}\n            {%- endif %}\n        {%- endfor %}\n        {%- set content = content.text %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = \"\" %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n            {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n            {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n        {%- endif %}\n    {%- endif %}\n    {%- if loop.index0 > ns.last_query_index %}\n        {%- if loop.last or (not loop.last and reasoning_content) %}\n            {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(\"\\n\") + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n    {%- else %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n    {%- endif %}\n    {%- if message.tool_calls %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if (loop.first and content) or (not loop.first) %}{{- ''\\n'' }}{%- endif %}\n            {%- if tool_call.function %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {%- if tool_call.arguments is string %}\n                {{- tool_call.arguments }}\n            {%- else %}\n                {{- tool_call.arguments | tojson }}\n            {%- endif %}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n    {%- endif %}\n    {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}{{- ''<|im_start|>user'' }}{%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}{{- ''<|im_end|>\\n'' }}{%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}{{- ''<think>\\n\\n</think>\\n\\n'' }}{%- endif %}\n{%- endif %}"},"tokenizer_config":{"bos_token":null,"eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:QwenLM:Qwen3-Omni","source_url":"https://github.com/QwenLM/Qwen3-Omni"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:wangxiongts:vllm.git","source_url":"https://github.com/wangxiongts/vllm.git"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Other', 'approved', 78.7, 'f8e606b60fa79314c7d6c0b49405475d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-1b-it', 'huggingface--google--gemma-3-1b-it', 'gemma-3-1b-it', 'google', '', '["transformers","safetensors","gemma3_text","text-generation","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2311.12022","arxiv:2108.07732","arxiv:2107.03374","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2312.11805","base_model:google/gemma-3-1b-pt","base_model:finetune:google/gemma-3-1b-pt","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 742, 2379295, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-1b-it","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":999885952,"storage_bytes":10671219620,"files_count":10,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForCausalLM"],"model_type":"gemma3_text","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 38.7, '723a56e32151c8130b76ecf782a7ced9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nomic-ai-nomic-embed-text-v1.5', 'huggingface--nomic-ai--nomic-embed-text-v1.5', 'nomic-embed-text-v1.5', 'nomic-ai', '--- library_name: sentence-transformers pipeline_tag: sentence-similarity tags: - feature-extraction - sentence-similarity - mteb - transformers - transformers.js model-index: - name: epoch_0_model results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 75.20895522388058 - type: ap value: 38.57605549557802 - type:...', '["sentence-transformers","onnx","safetensors","nomic_bert","feature-extraction","sentence-similarity","mteb","transformers","transformers.js","custom_code","en","arxiv:2402.01613","arxiv:2205.13147","license:apache-2.0","model-index","text-embeddings-inference","endpoints_compatible","region:us"]', 'sentence-similarity', 741, 2204029, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nomic-ai/nomic-embed-text-v1.5","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: sentence-transformers\npipeline_tag: sentence-similarity\ntags:\n- feature-extraction\n- sentence-similarity\n- mteb\n- transformers\n- transformers.js\nmodel-index:\n- name: epoch_0_model\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 75.20895522388058\n    - type: ap\n      value: 38.57605549557802\n    - type: f1\n      value: 69.35586565857854\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 91.8144\n    - type: ap\n      value: 88.65222882032363\n    - type: f1\n      value: 91.80426301643274\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.162000000000006\n    - type: f1\n      value: 46.59329642263158\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.253\n    - type: map_at_10\n      value: 38.962\n    - type: map_at_100\n      value: 40.081\n    - type: map_at_1000\n      value: 40.089000000000006\n    - type: map_at_3\n      value: 33.499\n    - type: map_at_5\n      value: 36.351\n    - type: mrr_at_1\n      value: 24.609\n    - type: mrr_at_10\n      value: 39.099000000000004\n    - type: mrr_at_100\n      value: 40.211000000000006\n    - type: mrr_at_1000\n      value: 40.219\n    - type: mrr_at_3\n      value: 33.677\n    - type: mrr_at_5\n      value: 36.469\n    - type: ndcg_at_1\n      value: 24.253\n    - type: ndcg_at_10\n      value: 48.010999999999996\n    - type: ndcg_at_100\n      value: 52.756\n    - type: ndcg_at_1000\n      value: 52.964999999999996\n    - type: ndcg_at_3\n      value: 36.564\n    - type: ndcg_at_5\n      value: 41.711999999999996\n    - type: precision_at_1\n      value: 24.253\n    - type: precision_at_10\n      value: 7.738\n    - type: precision_at_100\n      value: 0.98\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 15.149000000000001\n    - type: precision_at_5\n      value: 11.593\n    - type: recall_at_1\n      value: 24.253\n    - type: recall_at_10\n      value: 77.383\n    - type: recall_at_100\n      value: 98.009\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 45.448\n    - type: recall_at_5\n      value: 57.965999999999994\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 45.69069567851087\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 36.35185490976283\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 61.71274951450321\n    - type: mrr\n      value: 76.06032625423207\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.73980520022269\n    - type: cos_sim_spearman\n      value: 84.24649792685918\n    - type: euclidean_pearson\n      value: 85.85197641158186\n    - type: euclidean_spearman\n      value: 84.24649792685918\n    - type: manhattan_pearson\n      value: 86.26809552711346\n    - type: manhattan_spearman\n      value: 84.56397504030865\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 84.25324675324674\n    - type: f1\n      value: 84.17872280892557\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 38.770253446400886\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 32.94307095497281\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.164\n    - type: map_at_10\n      value: 42.641\n    - type: map_at_100\n      value: 43.947\n    - type: map_at_1000\n      value: 44.074999999999996\n    - type: map_at_3\n      value: 39.592\n    - type: map_at_5\n      value: 41.204\n    - type: mrr_at_1\n      value: 39.628\n    - type: mrr_at_10\n      value: 48.625\n    - type: mrr_at_100\n      value: 49.368\n    - type: mrr_at_1000\n      value: 49.413000000000004\n    - type: mrr_at_3\n      value: 46.400000000000006\n    - type: mrr_at_5\n      value: 47.68\n    - type: ndcg_at_1\n      value: 39.628\n    - type: ndcg_at_10\n      value: 48.564\n    - type: ndcg_at_100\n      value: 53.507000000000005\n    - type: ndcg_at_1000\n      value: 55.635999999999996\n    - type: ndcg_at_3\n      value: 44.471\n    - type: ndcg_at_5\n      value: 46.137\n    - type: precision_at_1\n      value: 39.628\n    - type: precision_at_10\n      value: 8.856\n    - type: precision_at_100\n      value: 1.429\n    - type: precision_at_1000\n      value: 0.191\n    - type: precision_at_3\n      value: 21.268\n    - type: precision_at_5\n      value: 14.649000000000001\n    - type: recall_at_1\n      value: 32.164\n    - type: recall_at_10\n      value: 59.609\n    - type: recall_at_100\n      value: 80.521\n    - type: recall_at_1000\n      value: 94.245\n    - type: recall_at_3\n      value: 46.521\n    - type: recall_at_5\n      value: 52.083999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.526\n    - type: map_at_10\n      value: 41.581\n    - type: map_at_100\n      value: 42.815999999999995\n    - type: map_at_1000\n      value: 42.936\n    - type: map_at_3\n      value: 38.605000000000004\n    - type: map_at_5\n      value: 40.351\n    - type: mrr_at_1\n      value: 39.489999999999995\n    - type: mrr_at_10\n      value: 47.829\n    - type: mrr_at_100\n      value: 48.512\n    - type: mrr_at_1000\n      value: 48.552\n    - type: mrr_at_3\n      value: 45.754\n    - type: mrr_at_5\n      value: 46.986\n    - type: ndcg_at_1\n      value: 39.489999999999995\n    - type: ndcg_at_10\n      value: 47.269\n    - type: ndcg_at_100\n      value: 51.564\n    - type: ndcg_at_1000\n      value: 53.53099999999999\n    - type: ndcg_at_3\n      value: 43.301\n    - type: ndcg_at_5\n      value: 45.239000000000004\n    - type: precision_at_1\n      value: 39.489999999999995\n    - type: precision_at_10\n      value: 8.93\n    - type: precision_at_100\n      value: 1.415\n    - type: precision_at_1000\n      value: 0.188\n    - type: precision_at_3\n      value: 20.892\n    - type: precision_at_5\n      value: 14.865999999999998\n    - type: recall_at_1\n      value: 31.526\n    - type: recall_at_10\n      value: 56.76\n    - type: recall_at_100\n      value: 75.029\n    - type: recall_at_1000\n      value: 87.491\n    - type: recall_at_3\n      value: 44.786\n    - type: recall_at_5\n      value: 50.254\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.987\n    - type: map_at_10\n      value: 52.827\n    - type: map_at_100\n      value: 53.751000000000005\n    - type: map_at_1000\n      value: 53.81\n    - type: map_at_3\n      value: 49.844\n    - type: map_at_5\n      value: 51.473\n    - type: mrr_at_1\n      value: 46.833999999999996\n    - type: mrr_at_10\n      value: 56.389\n    - type: mrr_at_100\n      value: 57.003\n    - type: mrr_at_1000\n      value: 57.034\n    - type: mrr_at_3\n      value: 54.17999999999999\n    - type: mrr_at_5\n      value: 55.486999999999995\n    - type: ndcg_at_1\n      value: 46.833999999999996\n    - type: ndcg_at_10\n      value: 58.372\n    - type: ndcg_at_100\n      value: 62.068\n    - type: ndcg_at_1000\n      value: 63.288\n    - type: ndcg_at_3\n      value: 53.400000000000006\n    - type: ndcg_at_5\n      value: 55.766000000000005\n    - type: precision_at_1\n      value: 46.833999999999996\n    - type: precision_at_10\n      value: 9.191\n    - type: precision_at_100\n      value: 1.192\n    - type: precision_at_1000\n      value: 0.134\n    - type: precision_at_3\n      value: 23.448\n    - type: precision_at_5\n      value: 15.862000000000002\n    - type: recall_at_1\n      value: 40.987\n    - type: recall_at_10\n      value: 71.146\n    - type: recall_at_100\n      value: 87.035\n    - type: recall_at_1000\n      value: 95.633\n    - type: recall_at_3\n      value: 58.025999999999996\n    - type: recall_at_5\n      value: 63.815999999999995\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.587\n    - type: map_at_10\n      value: 33.114\n    - type: map_at_100\n      value: 34.043\n    - type: map_at_1000\n      value: 34.123999999999995\n    - type: map_at_3\n      value: 30.45\n    - type: map_at_5\n      value: 31.813999999999997\n    - type: mrr_at_1\n      value: 26.554\n    - type: mrr_at_10\n      value: 35.148\n    - type: mrr_at_100\n      value: 35.926\n    - type: mrr_at_1000\n      value: 35.991\n    - type: mrr_at_3\n      value: 32.599000000000004\n    - type: mrr_at_5\n      value: 33.893\n    - type: ndcg_at_1\n      value: 26.554\n    - type: ndcg_at_10\n      value: 38.132\n    - type: ndcg_at_100\n      value: 42.78\n    - type: ndcg_at_1000\n      value: 44.919\n    - type: ndcg_at_3\n      value: 32.833\n    - type: ndcg_at_5\n      value: 35.168\n    - type: precision_at_1\n      value: 26.554\n    - type: precision_at_10\n      value: 5.921\n    - type: precision_at_100\n      value: 0.8659999999999999\n    - type: precision_at_1000\n      value: 0.109\n    - type: precision_at_3\n      value: 13.861\n    - type: precision_at_5\n      value: 9.605\n    - type: recall_at_1\n      value: 24.587\n    - type: recall_at_10\n      value: 51.690000000000005\n    - type: recall_at_100\n      value: 73.428\n    - type: recall_at_1000\n      value: 89.551\n    - type: recall_at_3\n      value: 37.336999999999996\n    - type: recall_at_5\n      value: 43.047000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.715\n    - type: map_at_10\n      value: 24.251\n    - type: map_at_100\n      value: 25.326999999999998\n    - type: map_at_1000\n      value: 25.455\n    - type: map_at_3\n      value: 21.912000000000003\n    - type: map_at_5\n      value: 23.257\n    - type: mrr_at_1\n      value: 20.274\n    - type: mrr_at_10\n      value: 28.552\n    - type: mrr_at_100\n      value: 29.42\n    - type: mrr_at_1000\n      value: 29.497\n    - type: mrr_at_3\n      value: 26.14\n    - type: mrr_at_5\n      value: 27.502\n    - type: ndcg_at_1\n      value: 20.274\n    - type: ndcg_at_10\n      value: 29.088\n    - type: ndcg_at_100\n      value: 34.293\n    - type: ndcg_at_1000\n      value: 37.271\n    - type: ndcg_at_3\n      value: 24.708\n    - type: ndcg_at_5\n      value: 26.809\n    - type: precision_at_1\n      value: 20.274\n    - type: precision_at_10\n      value: 5.361\n    - type: precision_at_100\n      value: 0.915\n    - type: precision_at_1000\n      value: 0.13\n    - type: precision_at_3\n      value: 11.733\n    - type: precision_at_5\n      value: 8.556999999999999\n    - type: recall_at_1\n      value: 16.715\n    - type: recall_at_10\n      value: 39.587\n    - type: recall_at_100\n      value: 62.336000000000006\n    - type: recall_at_1000\n      value: 83.453\n    - type: recall_at_3\n      value: 27.839999999999996\n    - type: recall_at_5\n      value: 32.952999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.793000000000003\n    - type: map_at_10\n      value: 38.582\n    - type: map_at_100\n      value: 39.881\n    - type: map_at_1000\n      value: 39.987\n    - type: map_at_3\n      value: 35.851\n    - type: map_at_5\n      value: 37.289\n    - type: mrr_at_1\n      value: 34.455999999999996\n    - type: mrr_at_10\n      value: 43.909\n    - type: mrr_at_100\n      value: 44.74\n    - type: mrr_at_1000\n      value: 44.786\n    - type: mrr_at_3\n      value: 41.659\n    - type: mrr_at_5\n      value: 43.010999999999996\n    - type: ndcg_at_1\n      value: 34.455999999999996\n    - type: ndcg_at_10\n      value: 44.266\n    - type: ndcg_at_100\n      value: 49.639\n    - type: ndcg_at_1000\n      value: 51.644\n    - type: ndcg_at_3\n      value: 39.865\n    - type: ndcg_at_5\n      value: 41.887\n    - type: precision_at_1\n      value: 34.455999999999996\n    - type: precision_at_10\n      value: 7.843999999999999\n    - type: precision_at_100\n      value: 1.243\n    - type: precision_at_1000\n      value: 0.158\n    - type: precision_at_3\n      value: 18.831999999999997\n    - type: precision_at_5\n      value: 13.147\n    - type: recall_at_1\n      value: 28.793000000000003\n    - type: recall_at_10\n      value: 55.68300000000001\n    - type: recall_at_100\n      value: 77.99000000000001\n    - type: recall_at_1000\n      value: 91.183\n    - type: recall_at_3\n      value: 43.293\n    - type: recall_at_5\n      value: 48.618\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.907000000000004\n    - type: map_at_10\n      value: 35.519\n    - type: map_at_100\n      value: 36.806\n    - type: map_at_1000\n      value: 36.912\n    - type: map_at_3\n      value: 32.748\n    - type: map_at_5\n      value: 34.232\n    - type: mrr_at_1\n      value: 31.621\n    - type: mrr_at_10\n      value: 40.687\n    - type: mrr_at_100\n      value: 41.583\n    - type: mrr_at_1000\n      value: 41.638999999999996\n    - type: mrr_at_3\n      value: 38.527\n    - type: mrr_at_5\n      value: 39.612\n    - type: ndcg_at_1\n      value: 31.621\n    - type: ndcg_at_10\n      value: 41.003\n    - type: ndcg_at_100\n      value: 46.617999999999995\n    - type: ndcg_at_1000\n      value: 48.82\n    - type: ndcg_at_3\n      value: 36.542\n    - type: ndcg_at_5\n      value: 38.368\n    - type: precision_at_1\n      value: 31.621\n    - type: precision_at_10\n      value: 7.396999999999999\n    - type: precision_at_100\n      value: 1.191\n    - type: precision_at_1000\n      value: 0.153\n    - type: precision_at_3\n      value: 17.39\n    - type: precision_at_5\n      value: 12.1\n    - type: recall_at_1\n      value: 25.907000000000004\n    - type: recall_at_10\n      value: 52.115\n    - type: recall_at_100\n      value: 76.238\n    - type: recall_at_1000\n      value: 91.218\n    - type: recall_at_3\n      value: 39.417\n    - type: recall_at_5\n      value: 44.435\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.732166666666668\n    - type: map_at_10\n      value: 34.51616666666667\n    - type: map_at_100\n      value: 35.67241666666666\n    - type: map_at_1000\n      value: 35.78675\n    - type: map_at_3\n      value: 31.953416666666662\n    - type: map_at_5\n      value: 33.333\n    - type: mrr_at_1\n      value: 30.300166666666673\n    - type: mrr_at_10\n      value: 38.6255\n    - type: mrr_at_100\n      value: 39.46183333333334\n    - type: mrr_at_1000\n      value: 39.519999999999996\n    - type: mrr_at_3\n      value: 36.41299999999999\n    - type: mrr_at_5\n      value: 37.6365\n    - type: ndcg_at_1\n      value: 30.300166666666673\n    - type: ndcg_at_10\n      value: 39.61466666666667\n    - type: ndcg_at_100\n      value: 44.60808333333334\n    - type: ndcg_at_1000\n      value: 46.91708333333334\n    - type: ndcg_at_3\n      value: 35.26558333333333\n    - type: ndcg_at_5\n      value: 37.220000000000006\n    - type: precision_at_1\n      value: 30.300166666666673\n    - type: precision_at_10\n      value: 6.837416666666667\n    - type: precision_at_100\n      value: 1.10425\n    - type: precision_at_1000\n      value: 0.14875\n    - type: precision_at_3\n      value: 16.13716666666667\n    - type: precision_at_5\n      value: 11.2815\n    - type: recall_at_1\n      value: 25.732166666666668\n    - type: recall_at_10\n      value: 50.578916666666665\n    - type: recall_at_100\n      value: 72.42183333333334\n    - type: recall_at_1000\n      value: 88.48766666666667\n    - type: recall_at_3\n      value: 38.41325\n    - type: recall_at_5\n      value: 43.515750000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.951\n    - type: map_at_10\n      value: 30.974\n    - type: map_at_100\n      value: 31.804\n    - type: map_at_1000\n      value: 31.900000000000002\n    - type: map_at_3\n      value: 28.762\n    - type: map_at_5\n      value: 29.94\n    - type: mrr_at_1\n      value: 26.534000000000002\n    - type: mrr_at_10\n      value: 33.553\n    - type: mrr_at_100\n      value: 34.297\n    - type: mrr_at_1000\n      value: 34.36\n    - type: mrr_at_3\n      value: 31.391000000000002\n    - type: mrr_at_5\n      value: 32.525999999999996\n    - type: ndcg_at_1\n      value: 26.534000000000002\n    - type: ndcg_at_10\n      value: 35.112\n    - type: ndcg_at_100\n      value: 39.28\n    - type: ndcg_at_1000\n      value: 41.723\n    - type: ndcg_at_3\n      value: 30.902\n    - type: ndcg_at_5\n      value: 32.759\n    - type: precision_at_1\n      value: 26.534000000000002\n    - type: precision_at_10\n      value: 5.445\n    - type: precision_at_100\n      value: 0.819\n    - type: precision_at_1000\n      value: 0.11\n    - type: precision_at_3\n      value: 12.986\n    - type: precision_at_5\n      value: 9.049\n    - type: recall_at_1\n      value: 23.951\n    - type: recall_at_10\n      value: 45.24\n    - type: recall_at_100\n      value: 64.12299999999999\n    - type: recall_at_1000\n      value: 82.28999999999999\n    - type: recall_at_3\n      value: 33.806000000000004\n    - type: recall_at_5\n      value: 38.277\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.829\n    - type: map_at_10\n      value: 23.684\n    - type: map_at_100\n      value: 24.683\n    - type: map_at_1000\n      value: 24.81\n    - type: map_at_3\n      value: 21.554000000000002\n    - type: map_at_5\n      value: 22.768\n    - type: mrr_at_1\n      value: 20.096\n    - type: mrr_at_10\n      value: 27.230999999999998\n    - type: mrr_at_100\n      value: 28.083999999999996\n    - type: mrr_at_1000\n      value: 28.166000000000004\n    - type: mrr_at_3\n      value: 25.212\n    - type: mrr_at_5\n      value: 26.32\n    - type: ndcg_at_1\n      value: 20.096\n    - type: ndcg_at_10\n      value: 27.989000000000004\n    - type: ndcg_at_100\n      value: 32.847\n    - type: ndcg_at_1000\n      value: 35.896\n    - type: ndcg_at_3\n      value: 24.116\n    - type: ndcg_at_5\n      value: 25.964\n    - type: precision_at_1\n      value: 20.096\n    - type: precision_at_10\n      value: 5\n    - type: precision_at_100\n      value: 0.8750000000000001\n    - type: precision_at_1000\n      value: 0.131\n    - type: precision_at_3\n      value: 11.207\n    - type: precision_at_5\n      value: 8.08\n    - type: recall_at_1\n      value: 16.829\n    - type: recall_at_10\n      value: 37.407000000000004\n    - type: recall_at_100\n      value: 59.101000000000006\n    - type: recall_at_1000\n      value: 81.024\n    - type: recall_at_3\n      value: 26.739\n    - type: recall_at_5\n      value: 31.524\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.138\n    - type: map_at_10\n      value: 32.275999999999996\n    - type: map_at_100\n      value: 33.416000000000004\n    - type: map_at_1000\n      value: 33.527\n    - type: map_at_3\n      value: 29.854000000000003\n    - type: map_at_5\n      value: 31.096\n    - type: mrr_at_1\n      value: 28.450999999999997\n    - type: mrr_at_10\n      value: 36.214\n    - type: mrr_at_100\n      value: 37.134\n    - type: mrr_at_1000\n      value: 37.198\n    - type: mrr_at_3\n      value: 34.001999999999995\n    - type: mrr_at_5\n      value: 35.187000000000005\n    - type: ndcg_at_1\n      value: 28.450999999999997\n    - type: ndcg_at_10\n      value: 37.166\n    - type: ndcg_at_100\n      value: 42.454\n    - type: ndcg_at_1000\n      value: 44.976\n    - type: ndcg_at_3\n      value: 32.796\n    - type: ndcg_at_5\n      value: 34.631\n    - type: precision_at_1\n      value: 28.450999999999997\n    - type: precision_at_10\n      value: 6.241\n    - type: precision_at_100\n      value: 0.9950000000000001\n    - type: precision_at_1000\n      value: 0.133\n    - type: precision_at_3\n      value: 14.801\n    - type: precision_at_5\n      value: 10.280000000000001\n    - type: recall_at_1\n      value: 24.138\n    - type: recall_at_10\n      value: 48.111\n    - type: recall_at_100\n      value: 71.245\n    - type: recall_at_1000\n      value: 88.986\n    - type: recall_at_3\n      value: 36.119\n    - type: recall_at_5\n      value: 40.846\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.244\n    - type: map_at_10\n      value: 31.227\n    - type: map_at_100\n      value: 33.007\n    - type: map_at_1000\n      value: 33.223\n    - type: map_at_3\n      value: 28.924\n    - type: map_at_5\n      value: 30.017\n    - type: mrr_at_1\n      value: 27.668\n    - type: mrr_at_10\n      value: 35.524\n    - type: mrr_at_100\n      value: 36.699\n    - type: mrr_at_1000\n      value: 36.759\n    - type: mrr_at_3\n      value: 33.366\n    - type: mrr_at_5\n      value: 34.552\n    - type: ndcg_at_1\n      value: 27.668\n    - type: ndcg_at_10\n      value: 36.381\n    - type: ndcg_at_100\n      value: 43.062\n    - type: ndcg_at_1000\n      value: 45.656\n    - type: ndcg_at_3\n      value: 32.501999999999995\n    - type: ndcg_at_5\n      value: 34.105999999999995\n    - type: precision_at_1\n      value: 27.668\n    - type: precision_at_10\n      value: 6.798\n    - type: precision_at_100\n      value: 1.492\n    - type: precision_at_1000\n      value: 0.234\n    - type: precision_at_3\n      value: 15.152\n    - type: precision_at_5\n      value: 10.791\n    - type: recall_at_1\n      value: 23.244\n    - type: recall_at_10\n      value: 45.979\n    - type: recall_at_100\n      value: 74.822\n    - type: recall_at_1000\n      value: 91.078\n    - type: recall_at_3\n      value: 34.925\n    - type: recall_at_5\n      value: 39.126\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 19.945\n    - type: map_at_10\n      value: 27.517999999999997\n    - type: map_at_100\n      value: 28.588\n    - type: map_at_1000\n      value: 28.682000000000002\n    - type: map_at_3\n      value: 25.345000000000002\n    - type: map_at_5\n      value: 26.555\n    - type: mrr_at_1\n      value: 21.996\n    - type: mrr_at_10\n      value: 29.845\n    - type: mrr_at_100\n      value: 30.775999999999996\n    - type: mrr_at_1000\n      value: 30.845\n    - type: mrr_at_3\n      value: 27.726\n    - type: mrr_at_5\n      value: 28.882\n    - type: ndcg_at_1\n      value: 21.996\n    - type: ndcg_at_10\n      value: 32.034\n    - type: ndcg_at_100\n      value: 37.185\n    - type: ndcg_at_1000\n      value: 39.645\n    - type: ndcg_at_3\n      value: 27.750999999999998\n    - type: ndcg_at_5\n      value: 29.805999999999997\n    - type: precision_at_1\n      value: 21.996\n    - type: precision_at_10\n      value: 5.065\n    - type: precision_at_100\n      value: 0.819\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 12.076\n    - type: precision_at_5\n      value: 8.392\n    - type: recall_at_1\n      value: 19.945\n    - type: recall_at_10\n      value: 43.62\n    - type: recall_at_100\n      value: 67.194\n    - type: recall_at_1000\n      value: 85.7\n    - type: recall_at_3\n      value: 32.15\n    - type: recall_at_5\n      value: 37.208999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 18.279\n    - type: map_at_10\n      value: 31.052999999999997\n    - type: map_at_100\n      value: 33.125\n    - type: map_at_1000\n      value: 33.306000000000004\n    - type: map_at_3\n      value: 26.208\n    - type: map_at_5\n      value: 28.857\n    - type: mrr_at_1\n      value: 42.671\n    - type: mrr_at_10\n      value: 54.557\n    - type: mrr_at_100\n      value: 55.142\n    - type: mrr_at_1000\n      value: 55.169000000000004\n    - type: mrr_at_3\n      value: 51.488\n    - type: mrr_at_5\n      value: 53.439\n    - type: ndcg_at_1\n      value: 42.671\n    - type: ndcg_at_10\n      value: 41.276\n    - type: ndcg_at_100\n      value: 48.376000000000005\n    - type: ndcg_at_1000\n      value: 51.318\n    - type: ndcg_at_3\n      value: 35.068\n    - type: ndcg_at_5\n      value: 37.242\n    - type: precision_at_1\n      value: 42.671\n    - type: precision_at_10\n      value: 12.638\n    - type: precision_at_100\n      value: 2.045\n    - type: precision_at_1000\n      value: 0.26\n    - type: precision_at_3\n      value: 26.08\n    - type: precision_at_5\n      value: 19.805\n    - type: recall_at_1\n      value: 18.279\n    - type: recall_at_10\n      value: 46.946\n    - type: recall_at_100\n      value: 70.97200000000001\n    - type: recall_at_1000\n      value: 87.107\n    - type: recall_at_3\n      value: 31.147999999999996\n    - type: recall_at_5\n      value: 38.099\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.573\n    - type: map_at_10\n      value: 19.747\n    - type: map_at_100\n      value: 28.205000000000002\n    - type: map_at_1000\n      value: 29.831000000000003\n    - type: map_at_3\n      value: 14.109\n    - type: map_at_5\n      value: 16.448999999999998\n    - type: mrr_at_1\n      value: 71\n    - type: mrr_at_10\n      value: 77.68599999999999\n    - type: mrr_at_100\n      value: 77.995\n    - type: mrr_at_1000\n      value: 78.00200000000001\n    - type: mrr_at_3\n      value: 76.292\n    - type: mrr_at_5\n      value: 77.029\n    - type: ndcg_at_1\n      value: 59.12500000000001\n    - type: ndcg_at_10\n      value: 43.9\n    - type: ndcg_at_100\n      value: 47.863\n    - type: ndcg_at_1000\n      value: 54.848\n    - type: ndcg_at_3\n      value: 49.803999999999995\n    - type: ndcg_at_5\n      value: 46.317\n    - type: precision_at_1\n      value: 71\n    - type: precision_at_10\n      value: 34.4\n    - type: precision_at_100\n      value: 11.063\n    - type: precision_at_1000\n      value: 1.989\n    - type: precision_at_3\n      value: 52.333\n    - type: precision_at_5\n      value: 43.7\n    - type: recall_at_1\n      value: 8.573\n    - type: recall_at_10\n      value: 25.615\n    - type: recall_at_100\n      value: 53.385000000000005\n    - type: recall_at_1000\n      value: 75.46000000000001\n    - type: recall_at_3\n      value: 15.429\n    - type: recall_at_5\n      value: 19.357\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 47.989999999999995\n    - type: f1\n      value: 42.776314451497555\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 74.13499999999999\n    - type: map_at_10\n      value: 82.825\n    - type: map_at_100\n      value: 83.096\n    - type: map_at_1000\n      value: 83.111\n    - type: map_at_3\n      value: 81.748\n    - type: map_at_5\n      value: 82.446\n    - type: mrr_at_1\n      value: 79.553\n    - type: mrr_at_10\n      value: 86.654\n    - type: mrr_at_100\n      value: 86.774\n    - type: mrr_at_1000\n      value: 86.778\n    - type: mrr_at_3\n      value: 85.981\n    - type: mrr_at_5\n      value: 86.462\n    - type: ndcg_at_1\n      value: 79.553\n    - type: ndcg_at_10\n      value: 86.345\n    - type: ndcg_at_100\n      value: 87.32\n    - type: ndcg_at_1000\n      value: 87.58200000000001\n    - type: ndcg_at_3\n      value: 84.719\n    - type: ndcg_at_5\n      value: 85.677\n    - type: precision_at_1\n      value: 79.553\n    - type: precision_at_10\n      value: 10.402000000000001\n    - type: precision_at_100\n      value: 1.1119999999999999\n    - type: precision_at_1000\n      value: 0.11499999999999999\n    - type: precision_at_3\n      value: 32.413\n    - type: precision_at_5\n      value: 20.138\n    - type: recall_at_1\n      value: 74.13499999999999\n    - type: recall_at_10\n      value: 93.215\n    - type: recall_at_100\n      value: 97.083\n    - type: recall_at_1000\n      value: 98.732\n    - type: recall_at_3\n      value: 88.79\n    - type: recall_at_5\n      value: 91.259\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 18.298000000000002\n    - type: map_at_10\n      value: 29.901\n    - type: map_at_100\n      value: 31.528\n    - type: map_at_1000\n      value: 31.713\n    - type: map_at_3\n      value: 25.740000000000002\n    - type: map_at_5\n      value: 28.227999999999998\n    - type: mrr_at_1\n      value: 36.728\n    - type: mrr_at_10\n      value: 45.401\n    - type: mrr_at_100\n      value: 46.27\n    - type: mrr_at_1000\n      value: 46.315\n    - type: mrr_at_3\n      value: 42.978\n    - type: mrr_at_5\n      value: 44.29\n    - type: ndcg_at_1\n      value: 36.728\n    - type: ndcg_at_10\n      value: 37.456\n    - type: ndcg_at_100\n      value: 43.832\n    - type: ndcg_at_1000\n      value: 47\n    - type: ndcg_at_3\n      value: 33.694\n    - type: ndcg_at_5\n      value: 35.085\n    - type: precision_at_1\n      value: 36.728\n    - type: precision_at_10\n      value: 10.386\n    - type: precision_at_100\n      value: 1.701\n    - type: precision_at_1000\n      value: 0.22599999999999998\n    - type: precision_at_3\n      value: 22.479\n    - type: precision_at_5\n      value: 16.605\n    - type: recall_at_1\n      value: 18.298000000000002\n    - type: recall_at_10\n      value: 44.369\n    - type: recall_at_100\n      value: 68.098\n    - type: recall_at_1000\n      value: 87.21900000000001\n    - type: recall_at_3\n      value: 30.215999999999998\n    - type: recall_at_5\n      value: 36.861\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.568\n    - type: map_at_10\n      value: 65.061\n    - type: map_at_100\n      value: 65.896\n    - type: map_at_1000\n      value: 65.95100000000001\n    - type: map_at_3\n      value: 61.831\n    - type: map_at_5\n      value: 63.849000000000004\n    - type: mrr_at_1\n      value: 79.136\n    - type: mrr_at_10\n      value: 84.58200000000001\n    - type: mrr_at_100\n      value: 84.765\n    - type: mrr_at_1000\n      value: 84.772\n    - type: mrr_at_3\n      value: 83.684\n    - type: mrr_at_5\n      value: 84.223\n    - type: ndcg_at_1\n      value: 79.136\n    - type: ndcg_at_10\n      value: 72.622\n    - type: ndcg_at_100\n      value: 75.539\n    - type: ndcg_at_1000\n      value: 76.613\n    - type: ndcg_at_3\n      value: 68.065\n    - type: ndcg_at_5\n      value: 70.58\n    - type: precision_at_1\n      value: 79.136\n    - type: precision_at_10\n      value: 15.215\n    - type: precision_at_100\n      value: 1.7500000000000002\n    - type: precision_at_1000\n      value: 0.189\n    - type: precision_at_3\n      value: 44.011\n    - type: precision_at_5\n      value: 28.388999999999996\n    - type: recall_at_1\n      value: 39.568\n    - type: recall_at_10\n      value: 76.077\n    - type: recall_at_100\n      value: 87.481\n    - type: recall_at_1000\n      value: 94.56400000000001\n    - type: recall_at_3\n      value: 66.01599999999999\n    - type: recall_at_5\n      value: 70.97200000000001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 85.312\n    - type: ap\n      value: 80.36296867333715\n    - type: f1\n      value: 85.26613311552218\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.363999999999997\n    - type: map_at_10\n      value: 35.711999999999996\n    - type: map_at_100\n      value: 36.876999999999995\n    - type: map_at_1000\n      value: 36.923\n    - type: map_at_3\n      value: 32.034\n    - type: map_at_5\n      value: 34.159\n    - type: mrr_at_1\n      value: 24.04\n    - type: mrr_at_10\n      value: 36.345\n    - type: mrr_at_100\n      value: 37.441\n    - type: mrr_at_1000\n      value: 37.480000000000004\n    - type: mrr_at_3\n      value: 32.713\n    - type: mrr_at_5\n      value: 34.824\n    - type: ndcg_at_1\n      value: 24.026\n    - type: ndcg_at_10\n      value: 42.531\n    - type: ndcg_at_100\n      value: 48.081\n    - type: ndcg_at_1000\n      value: 49.213\n    - type: ndcg_at_3\n      value: 35.044\n    - type: ndcg_at_5\n      value: 38.834\n    - type: precision_at_1\n      value: 24.026\n    - type: precision_at_10\n      value: 6.622999999999999\n    - type: precision_at_100\n      value: 0.941\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 14.909\n    - type: precision_at_5\n      value: 10.871\n    - type: recall_at_1\n      value: 23.363999999999997\n    - type: recall_at_10\n      value: 63.426\n    - type: recall_at_100\n      value: 88.96300000000001\n    - type: recall_at_1000\n      value: 97.637\n    - type: recall_at_3\n      value: 43.095\n    - type: recall_at_5\n      value: 52.178000000000004\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.0095759233926\n    - type: f1\n      value: 92.78387794667408\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 75.0296397628819\n    - type: f1\n      value: 58.45699589820874\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.45662407531944\n    - type: f1\n      value: 71.42364781421813\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.07800941492937\n    - type: f1\n      value: 77.22799045640845\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 34.531234379250606\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 30.941490381193802\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 30.3115090856725\n    - type: mrr\n      value: 31.290667638675757\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.465\n    - type: map_at_10\n      value: 13.03\n    - type: map_at_100\n      value: 16.057\n    - type: map_at_1000\n      value: 17.49\n    - type: map_at_3\n      value: 9.553\n    - type: map_at_5\n      value: 11.204\n    - type: mrr_at_1\n      value: 43.653\n    - type: mrr_at_10\n      value: 53.269\n    - type: mrr_at_100\n      value: 53.72\n    - type: mrr_at_1000\n      value: 53.761\n    - type: mrr_at_3\n      value: 50.929\n    - type: mrr_at_5\n      value: 52.461\n    - type: ndcg_at_1\n      value: 42.26\n    - type: ndcg_at_10\n      value: 34.673\n    - type: ndcg_at_100\n      value: 30.759999999999998\n    - type: ndcg_at_1000\n      value: 39.728\n    - type: ndcg_at_3\n      value: 40.349000000000004\n    - type: ndcg_at_5\n      value: 37.915\n    - type: precision_at_1\n      value: 43.653\n    - type: precision_at_10\n      value: 25.789\n    - type: precision_at_100\n      value: 7.754999999999999\n    - type: precision_at_1000\n      value: 2.07\n    - type: precision_at_3\n      value: 38.596000000000004\n    - type: precision_at_5\n      value: 33.251\n    - type: recall_at_1\n      value: 5.465\n    - type: recall_at_10\n      value: 17.148\n    - type: recall_at_100\n      value: 29.768\n    - type: recall_at_1000\n      value: 62.239\n    - type: recall_at_3\n      value: 10.577\n    - type: recall_at_5\n      value: 13.315\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 37.008\n    - type: map_at_10\n      value: 52.467\n    - type: map_at_100\n      value: 53.342999999999996\n    - type: map_at_1000\n      value: 53.366\n    - type: map_at_3\n      value: 48.412\n    - type: map_at_5\n      value: 50.875\n    - type: mrr_at_1\n      value: 41.541\n    - type: mrr_at_10\n      value: 54.967\n    - type: mrr_at_100\n      value: 55.611\n    - type: mrr_at_1000\n      value: 55.627\n    - type: mrr_at_3\n      value: 51.824999999999996\n    - type: mrr_at_5\n      value: 53.763000000000005\n    - type: ndcg_at_1\n      value: 41.541\n    - type: ndcg_at_10\n      value: 59.724999999999994\n    - type: ndcg_at_100\n      value: 63.38700000000001\n    - type: ndcg_at_1000\n      value: 63.883\n    - type: ndcg_at_3\n      value: 52.331\n    - type: ndcg_at_5\n      value: 56.327000000000005\n    - type: precision_at_1\n      value: 41.541\n    - type: precision_at_10\n      value: 9.447\n    - type: precision_at_100\n      value: 1.1520000000000001\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 23.262\n    - type: precision_at_5\n      value: 16.314999999999998\n    - type: recall_at_1\n      value: 37.008\n    - type: recall_at_10\n      value: 79.145\n    - type: recall_at_100\n      value: 94.986\n    - type: recall_at_1000\n      value: 98.607\n    - type: recall_at_3\n      value: 60.277\n    - type: recall_at_5\n      value: 69.407\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.402\n    - type: map_at_10\n      value: 84.181\n    - type: map_at_100\n      value: 84.796\n    - type: map_at_1000\n      value: 84.81400000000001\n    - type: map_at_3\n      value: 81.209\n    - type: map_at_5\n      value: 83.085\n    - type: mrr_at_1\n      value: 81.02000000000001\n    - type: mrr_at_10\n      value: 87.263\n    - type: mrr_at_100\n      value: 87.36\n    - type: mrr_at_1000\n      value: 87.36\n    - type: mrr_at_3\n      value: 86.235\n    - type: mrr_at_5\n      value: 86.945\n    - type: ndcg_at_1\n      value: 81.01\n    - type: ndcg_at_10\n      value: 87.99900000000001\n    - type: ndcg_at_100\n      value: 89.217\n    - type: ndcg_at_1000\n      value: 89.33\n    - type: ndcg_at_3\n      value: 85.053\n    - type: ndcg_at_5\n      value: 86.703\n    - type: precision_at_1\n      value: 81.01\n    - type: precision_at_10\n      value: 13.336\n    - type: precision_at_100\n      value: 1.52\n    - type: precision_at_1000\n      value: 0.156\n    - type: precision_at_3\n      value: 37.14\n    - type: precision_at_5\n      value: 24.44\n    - type: recall_at_1\n      value: 70.402\n    - type: recall_at_10\n      value: 95.214\n    - type: recall_at_100\n      value: 99.438\n    - type: recall_at_1000\n      value: 99.928\n    - type: recall_at_3\n      value: 86.75699999999999\n    - type: recall_at_5\n      value: 91.44099999999999\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 56.51721502758904\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 61.054808572333016\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.578\n    - type: map_at_10\n      value: 11.036999999999999\n    - type: map_at_100\n      value: 12.879999999999999\n    - type: map_at_1000\n      value: 13.150999999999998\n    - type: map_at_3\n      value: 8.133\n    - type: map_at_5\n      value: 9.559\n    - type: mrr_at_1\n      value: 22.6\n    - type: mrr_at_10\n      value: 32.68\n    - type: mrr_at_100\n      value: 33.789\n    - type: mrr_at_1000\n      value: 33.854\n    - type: mrr_at_3\n      value: 29.7\n    - type: mrr_at_5\n      value: 31.480000000000004\n    - type: ndcg_at_1\n      value: 22.6\n    - type: ndcg_at_10\n      value: 18.616\n    - type: ndcg_at_100\n      value: 25.883\n    - type: ndcg_at_1000\n      value: 30.944\n    - type: ndcg_at_3\n      value: 18.136\n    - type: ndcg_at_5\n      value: 15.625\n    - type: precision_at_1\n      value: 22.6\n    - type: precision_at_10\n      value: 9.48\n    - type: precision_at_100\n      value: 1.991\n    - type: precision_at_1000\n      value: 0.321\n    - type: precision_at_3\n      value: 16.8\n    - type: precision_at_5\n      value: 13.54\n    - type: recall_at_1\n      value: 4.578\n    - type: recall_at_10\n      value: 19.213\n    - type: recall_at_100\n      value: 40.397\n    - type: recall_at_1000\n      value: 65.2\n    - type: recall_at_3\n      value: 10.208\n    - type: recall_at_5\n      value: 13.718\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.44288351714071\n    - type: cos_sim_spearman\n      value: 79.37995604564952\n    - type: euclidean_pearson\n      value: 81.1078874670718\n    - type: euclidean_spearman\n      value: 79.37995905980499\n    - type: manhattan_pearson\n      value: 81.03697527288986\n    - type: manhattan_spearman\n      value: 79.33490235296236\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.95557650436523\n    - type: cos_sim_spearman\n      value: 78.5190672399868\n    - type: euclidean_pearson\n      value: 81.58064025904707\n    - type: euclidean_spearman\n      value: 78.5190672399868\n    - type: manhattan_pearson\n      value: 81.52857930619889\n    - type: manhattan_spearman\n      value: 78.50421361308034\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.79128416228737\n    - type: cos_sim_spearman\n      value: 86.05402451477147\n    - type: euclidean_pearson\n      value: 85.46280267054289\n    - type: euclidean_spearman\n      value: 86.05402451477147\n    - type: manhattan_pearson\n      value: 85.46278563858236\n    - type: manhattan_spearman\n      value: 86.08079590861004\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.20623089568763\n    - type: cos_sim_spearman\n      value: 81.53786907061009\n    - type: euclidean_pearson\n      value: 82.82272250091494\n    - type: euclidean_spearman\n      value: 81.53786907061009\n    - type: manhattan_pearson\n      value: 82.78850494027013\n    - type: manhattan_spearman\n      value: 81.5135618083407\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.46366618397936\n    - type: cos_sim_spearman\n      value: 86.96566013336908\n    - type: euclidean_pearson\n      value: 86.62651697548931\n    - type: euclidean_spearman\n      value: 86.96565526364454\n    - type: manhattan_pearson\n      value: 86.58812160258009\n    - type: manhattan_spearman\n      value: 86.9336484321288\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 82.51858358641559\n    - type: cos_sim_spearman\n      value: 84.7652527954999\n    - type: euclidean_pearson\n      value: 84.23914783766861\n    - type: euclidean_spearman\n      value: 84.7652527954999\n    - type: manhattan_pearson\n      value: 84.22749648503171\n    - type: manhattan_spearman\n      value: 84.74527996746386\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.28026563313065\n    - type: cos_sim_spearman\n      value: 87.46928143824915\n    - type: euclidean_pearson\n      value: 88.30558762000372\n    - type: euclidean_spearman\n      value: 87.46928143824915\n    - type: manhattan_pearson\n      value: 88.10513330809331\n    - type: manhattan_spearman\n      value: 87.21069787834173\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.376497134587375\n    - type: cos_sim_spearman\n      value: 65.0159550112516\n    - type: euclidean_pearson\n      value: 65.64572120879598\n    - type: euclidean_spearman\n      value: 65.0159550112516\n    - type: manhattan_pearson\n      value: 65.88143604989976\n    - type: manhattan_spearman\n      value: 65.17547297222434\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.22876368947644\n    - type: cos_sim_spearman\n      value: 85.46935577445318\n    - type: euclidean_pearson\n      value: 85.32830231392005\n    - type: euclidean_spearman\n      value: 85.46935577445318\n    - type: manhattan_pearson\n      value: 85.30353211758495\n    - type: manhattan_spearman\n      value: 85.42821085956945\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 80.60986667767133\n    - type: mrr\n      value: 94.29432314236236\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 54.528\n    - type: map_at_10\n      value: 65.187\n    - type: map_at_100\n      value: 65.62599999999999\n    - type: map_at_1000\n      value: 65.657\n    - type: map_at_3\n      value: 62.352\n    - type: map_at_5\n      value: 64.025\n    - type: mrr_at_1\n      value: 57.333\n    - type: mrr_at_10\n      value: 66.577\n    - type: mrr_at_100\n      value: 66.88\n    - type: mrr_at_1000\n      value: 66.908\n    - type: mrr_at_3\n      value: 64.556\n    - type: mrr_at_5\n      value: 65.739\n    - type: ndcg_at_1\n      value: 57.333\n    - type: ndcg_at_10\n      value: 70.275\n    - type: ndcg_at_100\n      value: 72.136\n    - type: ndcg_at_1000\n      value: 72.963\n    - type: ndcg_at_3\n      value: 65.414\n    - type: ndcg_at_5\n      value: 67.831\n    - type: precision_at_1\n      value: 57.333\n    - type: precision_at_10\n      value: 9.5\n    - type: precision_at_100\n      value: 1.057\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 25.778000000000002\n    - type: precision_at_5\n      value: 17.2\n    - type: recall_at_1\n      value: 54.528\n    - type: recall_at_10\n      value: 84.356\n    - type: recall_at_100\n      value: 92.833\n    - type: recall_at_1000\n      value: 99.333\n    - type: recall_at_3\n      value: 71.283\n    - type: recall_at_5\n      value: 77.14999999999999\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.74158415841585\n    - type: cos_sim_ap\n      value: 92.90048959850317\n    - type: cos_sim_f1\n      value: 86.35650810245687\n    - type: cos_sim_precision\n      value: 90.4709748083242\n    - type: cos_sim_recall\n      value: 82.6\n    - type: dot_accuracy\n      value: 99.74158415841585\n    - type: dot_ap\n      value: 92.90048959850317\n    - type: dot_f1\n      value: 86.35650810245687\n    - type: dot_precision\n      value: 90.4709748083242\n    - type: dot_recall\n      value: 82.6\n    - type: euclidean_accuracy\n      value: 99.74158415841585\n    - type: euclidean_ap\n      value: 92.90048959850317\n    - type: euclidean_f1\n      value: 86.35650810245687\n    - type: euclidean_precision\n      value: 90.4709748083242\n    - type: euclidean_recall\n      value: 82.6\n    - type: manhattan_accuracy\n      value: 99.74158415841585\n    - type: manhattan_ap\n      value: 92.87344692947894\n    - type: manhattan_f1\n      value: 86.38497652582159\n    - type: manhattan_precision\n      value: 90.29443838604145\n    - type: manhattan_recall\n      value: 82.8\n    - type: max_accuracy\n      value: 99.74158415841585\n    - type: max_ap\n      value: 92.90048959850317\n    - type: max_f1\n      value: 86.38497652582159\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 63.191648770424216\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 34.02944668730218\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 50.466386167525265\n    - type: mrr\n      value: 51.19071492233257\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 30.198022505886435\n    - type: cos_sim_spearman\n      value: 30.40170257939193\n    - type: dot_pearson\n      value: 30.198015316402614\n    - type: dot_spearman\n      value: 30.40170257939193\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.242\n    - type: map_at_10\n      value: 2.17\n    - type: map_at_100\n      value: 12.221\n    - type: map_at_1000\n      value: 28.63\n    - type: map_at_3\n      value: 0.728\n    - type: map_at_5\n      value: 1.185\n    - type: mrr_at_1\n      value: 94\n    - type: mrr_at_10\n      value: 97\n    - type: mrr_at_100\n      value: 97\n    - type: mrr_at_1000\n      value: 97\n    - type: mrr_at_3\n      value: 97\n    - type: mrr_at_5\n      value: 97\n    - type: ndcg_at_1\n      value: 89\n    - type: ndcg_at_10\n      value: 82.30499999999999\n    - type: ndcg_at_100\n      value: 61.839999999999996\n    - type: ndcg_at_1000\n      value: 53.381\n    - type: ndcg_at_3\n      value: 88.877\n    - type: ndcg_at_5\n      value: 86.05199999999999\n    - type: precision_at_1\n      value: 94\n    - type: precision_at_10\n      value: 87\n    - type: precision_at_100\n      value: 63.38\n    - type: precision_at_1000\n      value: 23.498\n    - type: precision_at_3\n      value: 94\n    - type: precision_at_5\n      value: 92\n    - type: recall_at_1\n      value: 0.242\n    - type: recall_at_10\n      value: 2.302\n    - type: recall_at_100\n      value: 14.979000000000001\n    - type: recall_at_1000\n      value: 49.638\n    - type: recall_at_3\n      value: 0.753\n    - type: recall_at_5\n      value: 1.226\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 3.006\n    - type: map_at_10\n      value: 11.805\n    - type: map_at_100\n      value: 18.146\n    - type: map_at_1000\n      value: 19.788\n    - type: map_at_3\n      value: 5.914\n    - type: map_at_5\n      value: 8.801\n    - type: mrr_at_1\n      value: 40.816\n    - type: mrr_at_10\n      value: 56.36600000000001\n    - type: mrr_at_100\n      value: 56.721999999999994\n    - type: mrr_at_1000\n      value: 56.721999999999994\n    - type: mrr_at_3\n      value: 52.041000000000004\n    - type: mrr_at_5\n      value: 54.796\n    - type: ndcg_at_1\n      value: 37.755\n    - type: ndcg_at_10\n      value: 29.863\n    - type: ndcg_at_100\n      value: 39.571\n    - type: ndcg_at_1000\n      value: 51.385999999999996\n    - type: ndcg_at_3\n      value: 32.578\n    - type: ndcg_at_5\n      value: 32.351\n    - type: precision_at_1\n      value: 40.816\n    - type: precision_at_10\n      value: 26.531\n    - type: precision_at_100\n      value: 7.796\n    - type: precision_at_1000\n      value: 1.555\n    - type: precision_at_3\n      value: 32.653\n    - type: precision_at_5\n      value: 33.061\n    - type: recall_at_1\n      value: 3.006\n    - type: recall_at_10\n      value: 18.738\n    - type: recall_at_100\n      value: 48.058\n    - type: recall_at_1000\n      value: 83.41300000000001\n    - type: recall_at_3\n      value: 7.166\n    - type: recall_at_5\n      value: 12.102\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 71.4178\n    - type: ap\n      value: 14.648781342150446\n    - type: f1\n      value: 55.07299194946378\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 60.919637804187886\n    - type: f1\n      value: 61.24122013967399\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 49.207896583685695\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 86.23114978840078\n    - type: cos_sim_ap\n      value: 74.26624727825818\n    - type: cos_sim_f1\n      value: 68.72377190817083\n    - type: cos_sim_precision\n      value: 64.56400742115028\n    - type: cos_sim_recall\n      value: 73.45646437994723\n    - type: dot_accuracy\n      value: 86.23114978840078\n    - type: dot_ap\n      value: 74.26624032659652\n    - type: dot_f1\n      value: 68.72377190817083\n    - type: dot_precision\n      value: 64.56400742115028\n    - type: dot_recall\n      value: 73.45646437994723\n    - type: euclidean_accuracy\n      value: 86.23114978840078\n    - type: euclidean_ap\n      value: 74.26624714480556\n    - type: euclidean_f1\n      value: 68.72377190817083\n    - type: euclidean_precision\n      value: 64.56400742115028\n    - type: euclidean_recall\n      value: 73.45646437994723\n    - type: manhattan_accuracy\n      value: 86.16558383501221\n    - type: manhattan_ap\n      value: 74.2091943976357\n    - type: manhattan_f1\n      value: 68.64221520524654\n    - type: manhattan_precision\n      value: 63.59135913591359\n    - type: manhattan_recall\n      value: 74.5646437994723\n    - type: max_accuracy\n      value: 86.23114978840078\n    - type: max_ap\n      value: 74.26624727825818\n    - type: max_f1\n      value: 68.72377190817083\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.3681841114604\n    - type: cos_sim_ap\n      value: 86.65166387498546\n    - type: cos_sim_f1\n      value: 79.02581944698774\n    - type: cos_sim_precision\n      value: 75.35796605434099\n    - type: cos_sim_recall\n      value: 83.06898675700647\n    - type: dot_accuracy\n      value: 89.3681841114604\n    - type: dot_ap\n      value: 86.65166019802056\n    - type: dot_f1\n      value: 79.02581944698774\n    - type: dot_precision\n      value: 75.35796605434099\n    - type: dot_recall\n      value: 83.06898675700647\n    - type: euclidean_accuracy\n      value: 89.3681841114604\n    - type: euclidean_ap\n      value: 86.65166462876266\n    - type: euclidean_f1\n      value: 79.02581944698774\n    - type: euclidean_precision\n      value: 75.35796605434099\n    - type: euclidean_recall\n      value: 83.06898675700647\n    - type: manhattan_accuracy\n      value: 89.36624364497226\n    - type: manhattan_ap\n      value: 86.65076471274106\n    - type: manhattan_f1\n      value: 79.07408783532733\n    - type: manhattan_precision\n      value: 76.41102972856527\n    - type: manhattan_recall\n      value: 81.92947336002464\n    - type: max_accuracy\n      value: 89.3681841114604\n    - type: max_ap\n      value: 86.65166462876266\n    - type: max_f1\n      value: 79.07408783532733\nlicense: apache-2.0\nlanguage:\n- en\n---\n\n# nomic-embed-text-v1.5: Resizable Production Embeddings with Matryoshka Representation Learning  \n\n[Blog](https://www.nomic.ai/blog/posts/nomic-embed-text-v1) | [Technical Report](https://arxiv.org/abs/2402.01613) | [AWS SageMaker](https://aws.amazon.com/marketplace/seller-profile?id=seller-tpqidcj54zawi) | [Nomic Platform](https://atlas.nomic.ai)\n\n**Exciting Update!**: `nomic-embed-text-v1.5` is now multimodal! [nomic-embed-vision-v1.5](https://huggingface.co/nomic-ai/nomic-embed-vision-v1.5) is aligned to the embedding space of `nomic-embed-text-v1.5`, meaning any text embedding is multimodal!\n\n## Usage\n\n**Important**: the text prompt *must* include a *task instruction prefix*, instructing the model which task is being performed. \n\nFor example, if you are implementing a RAG application, you embed your documents as `search_document: <text here>` and embed your user queries as `search_query: <text here>`.\n\n## Task instruction prefixes\n\n### `search_document`\n\n#### Purpose: embed texts as documents from a dataset\n\nThis prefix is used for embedding texts as documents, for example as documents for a RAG index.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)\nsentences = [''search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `search_query`\n\n#### Purpose: embed texts as questions to answer\n\nThis prefix is used for embedding texts as questions that documents from a dataset could resolve, for example as queries to be answered by a RAG application.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)\nsentences = [''search_query: Who is Laurens van Der Maaten?'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `clustering`\n\n#### Purpose: embed texts to group them into clusters\n\nThis prefix is used for embedding texts in order to group them into clusters, discover common topics, or remove semantic duplicates.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)\nsentences = [''clustering: the quick brown fox'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `classification`\n\n#### Purpose: embed texts to classify them\n\nThis prefix is used for embedding texts into vectors that will be used as features for a classification model\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)\nsentences = [''classification: the quick brown fox'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n### Sentence Transformers\n```python\nimport torch.nn.functional as F\nfrom sentence_transformers import SentenceTransformer\n\nmatryoshka_dim = 512\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)\nsentences = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?'']\nembeddings = model.encode(sentences, convert_to_tensor=True)\nembeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\nembeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n```\n\n### Transformers\n\n```diff\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\nsentences = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?'']\n\ntokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'')\nmodel = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1.5'', trust_remote_code=True, safe_serialization=True)\nmodel.eval()\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n+ matryoshka_dim = 512\n\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\nembeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\n+ embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n+ embeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n```\n\nThe model natively supports scaling of the sequence length past 2048 tokens. To do so, \n\n```diff\n- tokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'')\n+ tokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'', model_max_length=8192)\n\n\n- model = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1.5'', trust_remote_code=True)\n+ model = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1.5'', trust_remote_code=True, rotary_scaling_factor=2)\n```\n\n### Transformers.js\n\n```js\nimport { pipeline, layer_norm } from ''@huggingface/transformers'';\n\n// Create a feature extraction pipeline\nconst extractor = await pipeline(''feature-extraction'', ''nomic-ai/nomic-embed-text-v1.5'');\n\n// Define sentences\nconst texts = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?''];\n\n// Compute sentence embeddings\nlet embeddings = await extractor(texts, { pooling: ''mean'' });\nconsole.log(embeddings); // Tensor of shape [2, 768]\n\nconst matryoshka_dim = 512;\nembeddings = layer_norm(embeddings, [embeddings.dims[1]])\n    .slice(null, [0, matryoshka_dim])\n    .normalize(2, -1);\nconsole.log(embeddings.tolist());\n```\n\n\n## Nomic API\n\nThe easiest way to use Nomic Embed is through the Nomic Embedding API.\n\nGenerating embeddings with the `nomic` Python client is as easy as \n\n```python\nfrom nomic import embed\n\noutput = embed.text(\n    texts=[''Nomic Embedding API'', ''#keepAIOpen''],\n    model=''nomic-embed-text-v1.5'',\n    task_type=''search_document'',\n    dimensionality=256,\n)\n\nprint(output)\n```\n\nFor more information, see the [API reference](https://docs.nomic.ai/reference/endpoints/nomic-embed-text)\n\n\n## Infinity\n\nUsage with [Infinity](https://github.com/michaelfeil/infinity).\n\n```bash\ndocker run --gpus all -v $PWD/data:/app/.cache -e HF_TOKEN=$HF_TOKEN -p "7997":"7997" \\nmichaelf34/infinity:0.0.70 \\nv2 --model-id nomic-ai/nomic-embed-text-v1.5 --revision "main" --dtype float16 --batch-size 8 --engine torch --port 7997 --no-bettertransformer\n```\n\n## Adjusting Dimensionality\n\n`nomic-embed-text-v1.5` is an improvement upon [Nomic Embed](https://huggingface.co/nomic-ai/nomic-embed-text-v1) that utilizes [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) which gives developers the flexibility to trade off the embedding size for a negligible reduction in performance.\n\n\n| Name                             | SeqLen | Dimension | MTEB      |\n| :-------------------------------:| :----- | :-------- | :------:  |\n| nomic-embed-text-v1              | 8192   |  768      | **62.39** |\n| nomic-embed-text-v1.5            | 8192   |  768      | 62.28     |\n| nomic-embed-text-v1.5            | 8192   |  512      | 61.96     |\n| nomic-embed-text-v1.5            | 8192   |  256      | 61.04     |\n| nomic-embed-text-v1.5            | 8192   |  128      | 59.34     |\n| nomic-embed-text-v1.5            | 8192   |  64       | 56.10     |\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/607997c83a565c15675055b3/CRnaHV-c2wMUMZKw72q85.png)\n\n## Training\nClick the Nomic Atlas map below to visualize a 5M sample of our contrastive pretraining data!\n\n[![image/webp](https://cdn-uploads.huggingface.co/production/uploads/607997c83a565c15675055b3/pjhJhuNyRfPagRd_c_iUz.webp)](https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample)\n\nWe train our embedder using a multi-stage training pipeline. Starting from a long-context [BERT model](https://huggingface.co/nomic-ai/nomic-bert-2048),\nthe first unsupervised contrastive stage trains on a dataset generated from weakly related text pairs, such as question-answer pairs from forums like StackExchange and Quora, title-body pairs from Amazon reviews, and summarizations from news articles.\n\nIn the second finetuning stage, higher quality labeled datasets such as search queries and answers from web searches are leveraged. Data curation and hard-example mining is crucial in this stage.\n\nFor more details, see the Nomic Embed [Technical Report](https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf) and corresponding [blog post](https://blog.nomic.ai/posts/nomic-embed-matryoshka).\n\nTraining data to train the models is released in its entirety. For more details, see the `contrastors` [repository](https://github.com/nomic-ai/contrastors)\n\n\n# Join the Nomic Community\n\n- Nomic: [https://nomic.ai](https://nomic.ai)\n- Discord: [https://discord.gg/myY5YDR8z8](https://discord.gg/myY5YDR8z8)\n- Twitter: [https://twitter.com/nomic_ai](https://twitter.com/nomic_ai)\n\n\n# Citation\n\nIf you find the model, dataset, or training code useful, please cite our work\n\n```bibtex\n@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, \n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":136731648,"storage_bytes":4185527035,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["NomicBertModel"],"auto_map":{"AutoConfig":"nomic-ai/nomic-bert-2048--configuration_hf_nomic_bert.NomicBertConfig","AutoModel":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertModel","AutoModelForMaskedLM":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForPreTraining","AutoModelForSequenceClassification":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForSequenceClassification","AutoModelForMultipleChoice":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForMultipleChoice","AutoModelForQuestionAnswering":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForQuestionAnswering","AutoModelForTokenClassification":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForTokenClassification"},"model_type":"nomic_bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:michaelfeil:infinity","source_url":"https://github.com/michaelfeil/infinity"},{"type":"has_code","target_id":"github:nomic-ai:contrastors","source_url":"https://github.com/nomic-ai/contrastors"},{"type":"based_on_paper","target_id":"arxiv:2402.01613","source_url":"https://arxiv.org/abs/2402.01613"},{"type":"based_on_paper","target_id":"arxiv:2205.13147","source_url":"https://arxiv.org/abs/2205.13147"}]', NULL, 'Apache-2.0', 'approved', 78.7, '1ae339fa509d1b204258d6908a51f23e', NULL, NULL, CURRENT_TIMESTAMP);
