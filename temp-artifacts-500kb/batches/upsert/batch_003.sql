/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CohereLabs-c4ai-command-r-plus', 'huggingface--coherelabs--c4ai-command-r-plus', 'c4ai-command-r-plus', 'CohereLabs', '', '["transformers","safetensors","cohere","text-generation","conversational","en","fr","de","es","it","pt","ja","ko","zh","ar","doi:10.57967/hf/3138","license:cc-by-nc-4.0","text-generation-inference","region:us"]', 'text-generation', 1763, 3172, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CohereLabs/c4ai-command-r-plus","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":103810674688,"storage_bytes":207650745718,"files_count":52,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["CohereForCausalLM"],"model_type":"cohere","tokenizer_config":{"bos_token":"<BOS_TOKEN>","chat_template":[{"name":"default","template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = ''You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + system_message + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% set content = message[''content''] %}{% if message[''role''] == ''user'' %}{{ ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''assistant'' %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}{% endif %}"},{"name":"tool_use","template":"\n{%- macro json_to_python_type(json_spec) %}\n{%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n\n{%- if basic_type_map[json_spec.type] is defined %}\n    {{- basic_type_map[json_spec.type] }}\n{%- elif json_spec.type == \"array\" %}\n    {{- \"List[\" +  json_to_python_type(json_spec.items) + \"]\"}}\n{%- elif json_spec.type == \"object\" %}\n    {{- \"Dict[str, \" + json_to_python_type(json_spec.additionalProperties) + '']''}}\n{%- elif json_spec.type is iterable %}\n    {{- \"Union[\" }}\n    {%- for t in json_spec.type %}\n      {{- json_to_python_type({\"type\": t}) }}\n      {%- if not loop.last %}\n        {{- \",\" }} \n    {%- endif %}\n    {%- endfor %}\n    {{- \"]\" }}\n{%- else %}\n    {{- \"Any\" }}\n{%- endif %}\n{%- endmacro %}\n\n{%- macro old_tool_parser(tools) %}\n{%- for tool in tools %}\n    {%- if loop.index0 != 0 %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- ''```python\\ndef '' + tool.name + ''('' }}\n    {%- for param_name, param_fields in tool.parameter_definitions|items %}\n        {%- if loop.index0 != 0 %}\n            {{- '', ''}}\n        {%- endif %}\n        {{- param_name + '': '' }}\n        {%- if not param_fields.required %}\n            {{- ''Optional['' + param_fields.type + ''] = None''}}\n        {%- else %}\n            {{- param_fields.type }}\n        {%- endif %}\n    {%- endfor %}\n    {{- '') -> List[Dict]:\\n    \"\"\"''}}\n    {{- tool.description }}\n    {%- if tool.parameter_definitions|length != 0 %}\n        {{- ''\\n\\n    Args:\\n        ''}}\n        {%- for param_name, param_fields in tool.parameter_definitions|items %}\n            {%- if loop.index0 != 0 %}\n                {{- ''\\n        '' }}\n            {%- endif %}\n            {{- param_name + '' (''}}\n            {%- if not param_fields.required %}\n                {{- ''Optional['' + param_fields.type + '']''}}\n            {%- else %}\n                {{- param_fields.type }}\n            {%- endif %}\n            {{- ''): '' + param_fields.description }}\n        {%- endfor %}\n    {%- endif %}\n    {{- ''\\n    \"\"\"\\n    pass\\n```'' }}\n{%- endfor %}\n{%- endmacro %}\n\n{%- macro new_tool_parser(tools) %}\n{%- for tool in tools %}\n  {%- if loop.index0 != 0 %}\n    {{- ''\\n\\n''}}\n  {%- endif %}\n  {%- if tool.function is defined %}\n    {%- set tool = tool.function %}\n  {%- endif %}\n  {{-''```python\ndef '' + tool.name + ''(''}}\n  {%- for param_name, param_fields in tool.parameters.properties|items %}\n    {%- if loop.index0 != 0 %}\n      {{- '', ''}}\n    {%- endif %}\n    {{-param_name + \": \"}} \n    {%- if not param_name in tool.parameters.required %}\n      {{-''Optional['' + json_to_python_type(param_fields) + ''] = None''}}\n    {%- else %}\n      {{- json_to_python_type(param_fields) }}\n    {%- endif %}\n  {%- endfor %}\n  {{- '') -> List[Dict]:\n    \"\"\"''}}\n  {{- tool.description }}\n  {%- if tool.parameters.properties|length != 0 %}\n    {{- ''\\n\\n    Args:\\n        ''}}\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\n      {%- if loop.index0 != 0 %}\n        {{- ''\\n        '' }}\n      {%- endif %}\n      {{- param_name + '' (''}}\n      {%- if not param_name in tool.parameters.required %}\n        {{-''Optional['' + json_to_python_type(param_fields) + '']''}}\n      {%- else %}\n        {{- json_to_python_type(param_fields) }}\n      {%- endif %}\n      {{- ''): '' + param_fields.description }}\n    {%- endfor %}\n    {%- endif %}\n    {{- ''\\n    \"\"\"\\n    pass\\n```'' }}\n{%- endfor %}\n{%- endmacro %}\n\n{{- bos_token }}\n{%- if messages[0][''role''] == ''system'' %}\n  {%- set loop_messages = messages[1:] %}\n  {%- set system_message = messages[0][''content''] %}\n{%- else %}\n  {%- set loop_messages = messages %}\n  {%- set system_message = ''## Task and Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\''s needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.'' %}\n{%- endif %}\n{{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}\n{{- ''# Safety Preamble'' }}\n{{- ''\nThe instructions in this section override those in the task description and style guide sections. Don\\''t answer questions that are harmful or immoral.'' }}\n{{- ''\n\n# System Preamble'' }}\n{{- ''\n## Basic Rules'' }}\n{{- ''\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\''s requests, you cite your sources in your answers, according to those instructions.'' }}\n{{- ''\n\n# User Preamble'' }}\n{{- ''\n'' + system_message }}\n{{-''\n\n## Available Tools\nHere is a list of tools that you have available to you:\n\n''}}\n{%- set ns = namespace(new_tools=true) %}\n{%- for tool in tools %}\n    {%- if tool.parameter_definitions is defined %}\n        {%- set ns.new_tools = false %}\n    {%- endif %}\n{%- endfor %}\n{%- if ns.new_tools %}\n    {{- new_tool_parser(tools) }}\n{%- else %}\n    {{- old_tool_parser(tools) }}\n{%- endif %}\n{{- ''<|END_OF_TURN_TOKEN|>''}}\n{%- for message in loop_messages %}\n  {%- set content = message[''content''] %}\n  {%- if message.role == ''user'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''system'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''assistant'' and message.tool_calls is defined %}\n    {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}\n    {%- if message.content is defined %}\n        {{- message.content|trim }}\n    {%- endif %}\n    {{- ''\\nAction:\\n```json\\n[\\n'' }}\n    {%- for tool_call in message.tool_calls %}\n        {%- if tool_call.function is defined %}\n            {%- set tool_call = tool_call.function %}\n        {%- endif %}\n        {{- ''{\\n''|indent(4, first=true) }}\n        {{- ''\"tool_name\": \"''|indent(8, first=true) + tool_call.name + ''\",\\n'' }}\n        {{- ''\"parameters\": ''|indent(8, first=true) }}\n        {%- if tool_call.arguments is defined and tool_call.arguments|length > 0 %}    \n            {{- tool_call.arguments|tojson(indent=4)|indent(8) }}\n            {{- ''\\n'' }}\n        {%- else %}\n            {{- ''{}\\n'' }}\n        {%- endif %}\n        {{- ''}''|indent(4, first=true) }}\n        {%- if not loop.last %}\n            {{- '',\\n'' }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \"\\n]```\\n\" }}\n  {%- elif message.role == ''assistant'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''tool'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\\n'' }}\n    {{- message.content|trim }}\n    {{- ''</results><|END_OF_TURN_TOKEN|>'' }}\n  {%- endif %}\n{%- endfor %}\n{{-''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\''Action:\\'' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\''s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n```json\n[\n    {\n        \"tool_name\": title of the tool in the specification,\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n    }\n]```<|END_OF_TURN_TOKEN|>''}}\n{%- if add_generation_prompt %}\n  {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}\n{%- endif %}\n"},{"name":"rag","template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = ''## Task and Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\''s needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.'' %}{% endif %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}{{ ''# Safety Preamble'' }}{{ ''\nThe instructions in this section override those in the task description and style guide sections. Don\\''t answer questions that are harmful or immoral.'' }}{{ ''\n\n# System Preamble'' }}{{ ''\n## Basic Rules'' }}{{ ''\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\''s requests, you cite your sources in your answers, according to those instructions.'' }}{{ ''\n\n# User Preamble'' }}{{ ''\n'' + system_message }}{{ ''<|END_OF_TURN_TOKEN|>''}}{% for message in loop_messages %}{% set content = message[''content''] %}{% if message[''role''] == ''user'' %}{{ ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''system'' %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''assistant'' %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% endfor %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>''}}{{ ''<results>'' }}{% for document in documents %}{{ ''\nDocument: '' }}{{ loop.index0 }}\n{% for key, value in document.items() %}{{ key }}: {{value}}\n{% endfor %}{% endfor %}{{ ''</results>''}}{{ ''<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}{{ ''Carefully perform the following instructions, in order, starting each with a new line.\n'' }}{{ ''Firstly, Decide which of the retrieved documents are relevant to the user\\''s last input by writing \\''Relevant Documents:\\'' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\''None\\''.\n'' }}{{ ''Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\''s last input by writing \\''Cited Documents:\\'' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\''None\\''.\n'' }}{% if citation_mode==''accurate'' %}{{ ''Thirdly, Write \\''Answer:\\'' followed by a response to the user\\''s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\n'' }}{% endif %}{{ ''Finally, Write \\''Grounded answer:\\'' followed by a response to the user\\''s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.'' }}{{ ''<|END_OF_TURN_TOKEN|>'' }}{% if add_generation_prompt %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}{% endif %}"}],"eos_token":"<|END_OF_TURN_TOKEN|>","pad_token":"<PAD>","unk_token":null,"use_default_system_prompt":false}}}', '[]', '[]', NULL, 'CC-BY-NC-4.0', 'approved', 40, '17f57fd74a00eadc9660292f4ca22aac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-QwQ-32B-Preview', 'huggingface--qwen--qwq-32b-preview', 'QwQ-32B-Preview', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/QwQ-32B-Preview/blob/main/LICENSE language: - en base_model: Qwen/Qwen2.5-32B-Instruct tags: - chat library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> **QwQ-32B-Preview** is an experimental research model developed by th...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2407.10671","base_model:qwen/qwen2.5-32b-instruct","base_model:finetune:qwen/qwen2.5-32b-instruct","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1741, 81054, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/QwQ-32B-Preview","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/QwQ-32B-Preview/blob/main/LICENSE\nlanguage:\n- en\nbase_model: Qwen/Qwen2.5-32B-Instruct\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# QwQ-32B-Preview\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\n**QwQ-32B-Preview** is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations:\n\n1. **Language Mixing and Code-Switching**: The model may mix languages or switch between them unexpectedly, affecting response clarity.\n2. **Recursive Reasoning Loops**: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.\n3. **Safety and Ethical Considerations**: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.\n4. **Performance and Benchmark Limitations**: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.\n\n**Specification**:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 32.5B\n- Number of Paramaters (Non-Embedding): 31.0B\n- Number of Layers: 64\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n- Context Length: Full 32,768 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwq-32b-preview/). You can also check Qwen2.5 [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/QwQ-32B-Preview"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "How many r in strawberry."\nmessages = [\n    {"role": "system", "content": "You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwq-32b-preview,\n    title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},\n    url = {https://qwenlm.github.io/blog/qwq-32b-preview/},\n    author = {Qwen Team},\n    month = {November},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":32763876352,"storage_bytes":65527841856,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 65, '4ee52278309a45ac1413a457b259f3bd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-27b-it', 'huggingface--google--gemma-3-27b-it', 'gemma-3-27b-it', 'google', '', '["transformers","safetensors","gemma3","any-to-any","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2311.12022","arxiv:2108.07732","arxiv:2107.03374","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2312.11805","base_model:google/gemma-3-27b-pt","base_model:finetune:google/gemma-3-27b-pt","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 1725, 1417376, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-27b-it","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":27432406640,"storage_bytes":179818970042,"files_count":25,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForConditionalGeneration"],"model_type":"gemma3","processor_config":{"chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"},"tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 40, '3060a32279932192099b27def49b5be7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Florence-2-large', 'huggingface--microsoft--florence-2-large', 'Florence-2-large', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Florence-2-large/resolve/main/LICENSE pipeline_tag: image-text-to-text tags: - vision --- **This is a continued pretrained version of Florence-2-large model with 4k context length, only 0.1B samples are used for continue pretraining, thus it might not be trained well. In addition, OCR task has been updated with line separator (''\n''). COCO OD AP 39.8** This Hub repository contains a HuggingFace''s implementation of Florence-2 model...', '["transformers","pytorch","safetensors","florence2","any-to-any","vision","image-text-to-text","custom_code","arxiv:2311.06242","license:mit","endpoints_compatible","region:us"]', 'image-text-to-text', 1715, 839508, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Florence-2-large","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Florence-2-large/resolve/main/LICENSE\npipeline_tag: image-text-to-text\ntags:\n- vision\n---\n\n# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\n\n## Model Summary\n\n**This is a continued pretrained version of Florence-2-large model with 4k context length, only 0.1B samples are used for continue pretraining, thus it might not be trained well. In addition, OCR task has been updated with line separator (''\n''). COCO OD AP 39.8**\n\nThis Hub repository contains a HuggingFace''s `transformers` implementation of Florence-2 model from Microsoft.\n\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model''s sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model. \n\nResources and Technical Documentation:\n+ [Florence-2 technical report](https://arxiv.org/abs/2311.06242). \n+ [Jupyter Notebook for inference and visualization of Florence-2-large](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n\n| Model   | Model size | Model Description | \n| ------- | ------------- |   ------------- |  \n| Florence-2-base[[HF]](https://huggingface.co/microsoft/Florence-2-base) | 0.23B | Pretrained model with FLD-5B  \n| Florence-2-large[[HF]](https://huggingface.co/microsoft/Florence-2-large) | 0.77B  | Pretrained model with FLD-5B  \n| Florence-2-base-ft[[HF]](https://huggingface.co/microsoft/Florence-2-base-ft) | 0.23B  | Finetuned model on a colletion of downstream tasks\n| Florence-2-large-ft[[HF]](https://huggingface.co/microsoft/Florence-2-large-ft) | 0.77B | Finetuned model on a colletion of downstream tasks\n \n## How to Get Started with the Model\n\nUse the code below to get started with the model. All models are trained with float16. \n\n```python\nimport requests\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)\n\nprompt = "<OD>"\n\nurl = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors="pt").to(device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs["input_ids"],\n    pixel_values=inputs["pixel_values"],\n    max_new_tokens=4096,\n    num_beams=3,\n    do_sample=False\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task="<OD>", image_size=(image.width, image.height))\n\nprint(parsed_answer)\n\n```\n\n\n## Tasks\n\nThis model is capable of performing different tasks through changing the prompts.\n\nFirst, let''s define a function to run a prompt.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained("microsoft/Florence-2-large", trust_remote_code=True)\n\nurl = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ndef run_example(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors="pt").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs["input_ids"],\n      pixel_values=inputs["pixel_values"],\n      max_new_tokens=1024,\n      num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n\n    print(parsed_answer)\n```\n</details>\n\nHere are the tasks `Florence-2` could perform:\n\n<details>\n<summary> Click to expand </summary>\n\n\n\n### Caption\n```python\nprompt = "<CAPTION>"\nrun_example(prompt)\n```\n\n### Detailed Caption\n```python\nprompt = "<DETAILED_CAPTION>"\nrun_example(prompt)\n```\n\n### More Detailed Caption\n```python\nprompt = "<MORE_DETAILED_CAPTION>"\nrun_example(prompt)\n```\n\n### Caption to Phrase Grounding \ncaption to phrase grounding task requires additional text input, i.e. caption. \n\nCaption to phrase grounding results format: \n{''\<CAPTION_TO_PHRASE_GROUNDING>'': {''bboxes'': [[x1, y1, x2, y2], ...], ''labels'': ['''', '''', ...]}}\n```python\ntask_prompt = "<CAPTION_TO_PHRASE_GROUNDING>"\nresults = run_example(task_prompt, text_input="A green car parked in front of a yellow building.")\n```\n\n### Object Detection\n\nOD results format: \n{''\<OD>'': {''bboxes'': [[x1, y1, x2, y2], ...], \n''labels'': [''label1'', ''label2'', ...]} }\n\n```python\nprompt = "<OD>"\nrun_example(prompt)\n```\n\n### Dense Region Caption\nDense region caption results format: \n{''\<DENSE_REGION_CAPTION>'' : {''bboxes'': [[x1, y1, x2, y2], ...], \n''labels'': [''label1'', ''label2'', ...]} }\n```python\nprompt = "<DENSE_REGION_CAPTION>"\nrun_example(prompt)\n```\n\n### Region proposal\nDense region caption results format: \n{''\<REGION_PROPOSAL>'': {''bboxes'': [[x1, y1, x2, y2], ...], \n''labels'': ['''', '''', ...]}}\n```python\nprompt = "<REGION_PROPOSAL>"\nrun_example(prompt)\n```\n\n### OCR \n\n```python\nprompt = "<OCR>"\nrun_example(prompt)\n```\n\n### OCR with Region\nOCR with region output format:\n{''\<OCR_WITH_REGION>'': {''quad_boxes'': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], ''labels'': [''text1'', ...]}}\n```python\nprompt = "<OCR_WITH_REGION>"\nrun_example(prompt)\n```\n\n### Output confidence score with Object Detection\n```python\n\ndef run_example_with_score(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors="pt").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs["input_ids"],\n      pixel_values=inputs["pixel_values"],\n      max_new_tokens=1024,\n      num_beams=3,\n      return_dict_in_generate=True,\n      output_scores=True,\n    )\n    generated_text = processor.batch_decode(generated_ids.sequences, skip_special_tokens=False)[0]\n\n    prediction, scores, beam_indices = generated_ids.sequences, generated_ids.scores, generated_ids.beam_indices\n    transition_beam_scores = model.compute_transition_scores(\n        sequences=prediction,\n        scores=scores,\n        beam_indices=beam_indices,\n    )\n\n    parsed_answer = processor.post_process_generation(sequence=generated_ids.sequences[0], \n        transition_beam_score=transition_beam_scores[0],\n        task=task_prompt, image_size=(image.width, image.height)\n    )\n\n    print(parsed_answer)\n\nprompt = "<OD>"\nrun_example_with_score(prompt)\n\n```\n\nfor More detailed examples, please refer to [notebook](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n</details>\n\n# Benchmarks\n\n## Florence-2 Zero-shot performance\n  \nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.  \n  \n| Method | #params | COCO Cap. test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | COCO Det. val2017 mAP |  \n|--------|---------|----------------------|------------------|--------------------|-----------------------|\n| Flamingo | 80B | 84.3 | - | - | - | \n| Florence-2-base| 0.23B | 133.0 | 118.7 | 70.1 | 34.7 | \n| Florence-2-large| 0.77B | 135.6 | 120.8 | 72.8 | 37.5 |\n\n  \nThe following table continues the comparison with performance on other vision-language evaluation tasks.  \n  \n| Method | Flickr30k test R@1 | Refcoco val Accuracy | Refcoco test-A Accuracy | Refcoco test-B Accuracy | Refcoco+ val Accuracy | Refcoco+ test-A Accuracy | Refcoco+ test-B Accuracy | Refcocog val Accuracy | Refcocog test Accuracy | Refcoco RES val mIoU |  \n|--------|----------------------|----------------------|-------------------------|-------------------------|-----------------------|--------------------------|--------------------------|-----------------------|------------------------|----------------------|  \n| Kosmos-2 | 78.7 | 52.3 | 57.4 | 47.3 | 45.5 | 50.7 | 42.2 | 60.6 | 61.7 | - |  \n| Florence-2-base | 83.6 | 53.9 | 58.4 | 49.7 | 51.5 | 56.4 | 47.9 | 66.3 | 65.1 | 34.6 |  \n| Florence-2-large | 84.4 | 56.3 | 61.6 | 51.4 | 53.6 | 57.9 | 49.9 | 68.0 | 67.0 | 35.8 |  \n\n\n\n## Florence-2 finetuned performance \n\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models *Florence-2-base-ft* and *Florence-2-large-ft* that can conduct a wide range of downstream tasks. \n  \nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol "▲" indicates the usage of external OCR as input.  \n  \n| Method         | # Params | COCO Caption Karpathy test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | VQAv2 test-dev Acc | TextVQA test-dev Acc | VizWiz VQA test-dev Acc |  \n|----------------|----------|-----------------------------------|------------------|--------------------|--------------------|----------------------|-------------------------|  \n| **Specialist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| CoCa           | 2.1B     | 143.6                              | 122.4            | -                  | 82.3               | -                    | -                       |  \n| BLIP-2         | 7.8B     | 144.5                              | 121.6            | -                  | 82.2               | -                    | -                       |  \n| GIT2           | 5.1B     | 145.0                              | 126.9            | 148.6              | 81.7               | 67.3                 | 71.0                    |  \n| Flamingo       | 80B      | 138.1                              | -                | -                  | 82.0               | 54.1                 | 65.7                    |  \n| PaLI           | 17B      | 149.1                              | 127.0            | 160.0▲             | 84.3               | 58.8 / 73.1▲         | 71.6 / 74.4▲            |  \n| PaLI-X         | 55B      | 149.2                              | 126.3            | 147.0 / 163.7▲     | 86.0               | 71.4 / 80.8▲         | 70.9 / 74.6▲            |  \n| **Generalist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| Unified-IO     | 2.9B     | -                                  | 100.0            | -                  | 77.9               | -                    | 57.4                    |  \n| Florence-2-base-ft | 0.23B  | 140.0                              | 116.7            | 143.9              | 79.7               | 63.6                 | 63.6                    |  \n| Florence-2-large-ft | 0.77B  | 143.3                              | 124.9            | 151.1              | 81.7               | 73.5                 | 72.6                    |  \n  \n  \n| Method               | # Params | COCO Det. val2017 mAP | Flickr30k test R@1 | RefCOCO val Accuracy | RefCOCO test-A Accuracy | RefCOCO test-B Accuracy | RefCOCO+ val Accuracy | RefCOCO+ test-A Accuracy | RefCOCO+ test-B Accuracy | RefCOCOg val Accuracy | RefCOCOg test Accuracy | RefCOCO RES val mIoU |  \n|----------------------|----------|-----------------------|--------------------|----------------------|-------------------------|-------------------------|------------------------|---------------------------|---------------------------|------------------------|-----------------------|------------------------|  \n| **Specialist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| SeqTR                | -        | -                     | -                  | 83.7                 | 86.5                    | 81.2                    | 71.5                   | 76.3                      | 64.9                      | 74.9                   | 74.2                  | -                      |  \n| PolyFormer           | -        | -                     | -                  | 90.4                 | 92.9                    | 87.2                    | 85.0                   | 89.8                      | 78.0                      | 85.8                   | 85.9                  | 76.9                   |  \n| UNINEXT              | 0.74B    | 60.6                  | -                  | 92.6                 | 94.3                    | 91.5                    | 85.2                   | 89.6                      | 79.8                      | 88.7                   | 89.4                  | -                      |  \n| Ferret               | 13B      | -                     | -                  | 89.5                 | 92.4                    | 84.4                    | 82.8                   | 88.1                      | 75.2                      | 85.8                   | 86.3                  | -                      |  \n| **Generalist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| UniTAB               | -        | -                     | -                  | 88.6                 | 91.1                    | 83.8                    | 81.0                   | 85.4                      | 71.6                      | 84.6                   | 84.7                  | -                      |  \n| Florence-2-base-ft | 0.23B    | 41.4                  | 84.0                | 92.6                 | 94.8                    | 91.5                   | 86.8                   | 91.7                      | 82.2                      | 89.8                   | 82.2                  | 78.0                  |  \n| Florence-2-large-ft| 0.77B    | 43.4                  | 85.2               | 93.4                 | 95.3                    | 92.0                    | 88.3                   | 92.9                      | 83.6                      | 91.2                   | 91.7                  | 80.5                   |  \n  \n\n## BibTex and citation info\n\n```\n@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":776721497,"storage_bytes":7971530525,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Florence2ForConditionalGeneration"],"auto_map":{"AutoConfig":"configuration_florence2.Florence2Config","AutoModelForCausalLM":"modeling_florence2.Florence2ForConditionalGeneration"},"model_type":"florence2","tokenizer_config":{}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2311.06242","source_url":"https://arxiv.org/abs/2311.06242"}]', NULL, 'MIT', 'approved', 80, '6c7b1bdaf188a751f6038b29ecd5f07d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mattshumer-Reflection-Llama-3.1-70B', 'huggingface--mattshumer--reflection-llama-3.1-70b', 'Reflection-Llama-3.1-70B', 'mattshumer', '--- license: llama3.1 base_model: meta-llama/Meta-Llama-3.1-70B-Instruct pipeline_tag: text-generation library_name: transformers --- | IMPORTANT UPDATE – There was an issue with the model when we first uploaded it. If you tried it and didn''t have good results, please, try again, we think we''ve fixed the issue. **Reflection Llama-3.1 70B is an open-source LLM, trained with a new technique called Reflection-Tuning that teaches a LLM to detect mistakes in its reasoning and correct course.** The...', '["transformers","safetensors","llama","text-generation","conversational","base_model:meta-llama/llama-3.1-70b-instruct","license:llama3.1","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1711, 354, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama3.1\nbase_model: meta-llama/Meta-Llama-3.1-70B-Instruct\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n# Reflection Llama-3.1 70B\n\n| IMPORTANT UPDATE – There was an issue with the model when we first uploaded it. If you tried it and didn''t have good results, please, try again, we think we''ve fixed the issue.\n\n**Reflection Llama-3.1 70B is an open-source LLM, trained with a new technique called Reflection-Tuning that teaches a LLM to detect mistakes in its reasoning and correct course.**\n\nThe model was trained on synthetic data generated by [Glaive](https://glaive.ai). If you''re training a model, Glaive is incredible — use them.\n\nYou can [try the model here](https://reflection-playground-production.up.railway.app/).\n\n## Benchmarks\n\nTrained from Llama 3.1 70B Instruct, you can sample from Reflection Llama-3.1 70B using the same code, pipelines, etc. as any other Llama model. It even uses the stock Llama 3.1 chat template format (though, we''ve trained in a few new special tokens to aid in reasoning and reflection).\n\nDuring sampling, the model will start by outputting reasoning inside `<thinking>` and `</thinking>` tags, and then once it is satisfied with its reasoning, it will output the final answer inside `<output>` and `</output>` tags. Each of these tags are special tokens, trained into the model.\n\nThis enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user.\n\nInside the `<thinking>` section, the model may output one or more `<reflection>` tags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer.\n\n## System Prompt\n\nThe system prompt used for training this model is:\n\n```\nYou are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside <thinking> tags, and then provide your final response inside <output> tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection> tags.\n```\n\nWe recommend using this exact system prompt to get the best results from Reflection Llama-3.1 70B. You may also want to experiment combining this system prompt with your own custom instructions to customize the behavior of the model.\n\n## Chat Format\n\nAs mentioned above, the model uses the standard Llama 3.1 chat format. Here’s an example:\n\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside <thinking> tags, and then provide your final response inside <output> tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection> tags.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nwhat is 2+2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\n## Tips for Performance\n\n- We are initially recommending a `temperature` of `.7` and a `top_p` of `.95`.\n- For increased accuracy, append `Think carefully.` at the end of your messages.\n\n## Dataset / Report\n\nBoth the dataset and a brief report detailing how we trained this model will be released next week, alongside our Reflection 405B model that we expect will be the top-performing LLM in the world, including closed-source models.\n\n---\n\nThanks to Jason Kuperberg and Josh Bickett from the [HyperWrite](https://hyperwriteai.com) team for reviewing drafts of the report we''ll be releasing next week.\n\nAlso, we know right now the model is split into a ton of files. We''ll condense this soon to make the model easier to download and work with!', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553804800,"storage_bytes":564430221088,"files_count":170,"spaces_count":95,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\n\n''+ message[''content''] | trim + ''<|eot_id|>'' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ ''<|start_header_id|>assistant<|end_header_id|>\n\n'' }}{% endif %}","eos_token":"<|eot_id|>","pad_token":"<|eot_id|>"}}}', '[]', '[]', NULL, 'llama3.1', 'approved', 65, 'bb2a2f503d51de7a9bc02f66f2c24bd7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-OmniParser', 'huggingface--microsoft--omniparser', 'OmniParser', 'microsoft', '--- library_name: transformers license: mit pipeline_tag: image-text-to-text --- 📢 [Project Page] [Blog Post] [Demo] OmniParser is a general screen parsing tool, which interprets/converts UI screenshot to structured format, to improve existing LLM based UI agent. Training Datasets include: 1) an interactable icon detection dataset, which was curated from popular web pages and automatically annotated to highlight clickable and actionable regions, and 2) an icon description dataset, designed t...', '["transformers","safetensors","blip-2","image-to-text","image-text-to-text","arxiv:2408.00203","license:mit","endpoints_compatible","region:us"]', 'image-text-to-text', 1697, 484, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/OmniParser","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\n---\n📢 [[Project Page](https://microsoft.github.io/OmniParser/)] [[Blog Post](https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/)] [[Demo](https://huggingface.co/spaces/microsoft/OmniParser/)] \n\n# Model Summary\nOmniParser is a general screen parsing tool, which interprets/converts UI screenshot to structured format, to improve existing LLM based UI agent. \nTraining Datasets include: 1) an interactable icon detection dataset, which was curated from popular web pages and automatically annotated to highlight clickable and actionable regions, and 2) an icon description dataset, designed to associate each UI element with its corresponding function. \n\nThis model hub includes a finetuned version of YOLOv8 and a finetuned BLIP-2 model on the above dataset respectively. For more details of the models used and finetuning, please refer to the [paper](https://arxiv.org/abs/2408.00203).\n\n# Responsible AI Considerations\n## Intended Use\n- OmniParser is designed to be able to convert unstructured screenshot image into structured list of elements including interactable regions location and captions of icons on its potential functionality. \n- OmniParser is intended to be used in settings where users are already trained on responsible analytic approaches and critical reasoning is expected. OmniParser is capable of providing extracted information from the screenshot, however human judgement is needed for the output of OmniParser. \n- OmniParser is intended to be used on various screenshots, which includes both PC and Phone, and also on various applications.  \n## limitations\n- OmniParser is designed to faithfully convert screenshot image into structured elements of interactable regions and semantics of the screen, while it does not detect harmful content in its input (like users have freedom to decide the input of any LLMs), users are expected to provide input to the OmniParser that is not harmful. \n- While OmniParser only converts screenshot image into texts, it can be used to construct an GUI agent based on LLMs that is actionable. When developing and operating the agent using OmniParser, the developers need to be responsible and follow common safety standard. \n- For OmniPaser-BLIP2, it may incorrectly infer the gender or other sensitive attribute (e.g., race, religion etc.) of individuals in icon images. Inference of sensitive attributes may rely upon stereotypes and generalizations rather than information about specific individuals and are more likely to be incorrect for marginalized people. Incorrect inferences may result in significant physical or psychological injury or restrict, infringe upon or undermine the ability to realize an individual’s human rights. We do not recommend use of OmniParser in any workplace-like use case scenario.\n\n# License\nPlease note that icon_detect model is under AGPL license, and icon_caption_blip2 & icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model. \n\n\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":16164887225,"files_count":20,"spaces_count":24,"gated":false,"private":false,"config":{"architectures":["Blip2ForConditionalGeneration"],"model_type":"blip-2"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2408.00203","source_url":"https://arxiv.org/abs/2408.00203"}]', NULL, 'MIT', 'approved', 65, '351f2adbb79bcd8b702232ccd15e9a27', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3-mini-128k-instruct', 'huggingface--microsoft--phi-3-mini-128k-instruct', 'Phi-3-mini-128k-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/LICENSE language: - en pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? --- 🎉**Phi-4**: [multimodal-instruct | onnx]; [mini-instruct | onnx] The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets. This data...', '["transformers","safetensors","phi3","text-generation","nlp","code","conversational","custom_code","en","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1684, 258694, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/LICENSE\n\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n  - messages:\n      - role: user\n        content: Can you provide ways to eat combinations of bananas and dragonfruits?\n---\n🎉**Phi-4**: [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n## Model Summary\n\nThe Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.\nThis dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.\n\nAfter initial training, the model underwent a post-training process that involved supervised fine-tuning and direct preference optimization to enhance its ability to follow instructions and adhere to safety measures.\nWhen evaluated against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning, the Phi-3 Mini-128K-Instruct demonstrated robust and state-of-the-art performance among models with fewer than 13 billion parameters.\nResources and Technical Documentation:\n\n🏡 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n📰 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n📖 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n🛠️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n👩‍🍳 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n🖥️ [Try It](https://aka.ms/try-phi3)\n\n|         | Short Context | Long Context |\n| :-      | :-            | :-           |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. \n\n## Release Notes \n\nThis is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback. \nThe model used additional post-training data leading to substantial gains on long-context understanding, instruction following, and structure output. \nWe also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability. \nWe believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications.\nWe appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. \n\nThese tables below highlights improvements on instruction following, structure output, reasoning, and long-context understanding of the new release on our public and internal benchmark datasets.\n\n| Benchmarks | Original | June 2024 Update |\n| :-         | :-       | :-               |\n| Instruction Extra Hard | 5.7 | 5.9 |\n| Instruction Hard | 5.0 | 5.2 |\n| JSON Structure Output | 1.9 | 60.1 |\n| XML Structure Output | 47.8 | 52.9 |\n| GPQA	| 25.9	| 29.7 |\n| MMLU	| 68.1	| 69.7 |\n| **Average**	| **25.7**	| **37.3** |\n\nRULER: a retrieval-based benchmark for long context understanding\n\n| Model             | 4K   | 8K   | 16K  | 32K  | 64K  | 128K | Average |\n| :-------------------| :------| :------| :------| :------| :------| :------| :---------|\n| Original          | 86.7 | 78.1 | 75.6 | 70.3 | 58.9 | 43.3 | **68.8**    |\n| June 2024 Update  | 92.4 | 91.1 | 90.8 | 87.9 | 79.8 | 65.6 | **84.6**    |\n\nRepoQA: a benchmark for long context code understanding\n\n| Model             | Python | C++ | Rust | Java | TypeScript | Average |\n| :-------------------| :--------| :-----| :------| :------| :------------| :---------|\n| Original          | 27     | 29  | 40   | 33   | 33         | **32.4**    |\n| June 2024 Update  | 85     | 63  | 72   | 93   | 72         | **77**      |\n\n\nNotes: if users would like to check out the previous version, use the git commit id **bb5bf1e4001277a606e11debca0ef80323e5f824**. For the model conversion, e.g. GGUF and other formats, we invite the community to experiment with various approaches and share your valuable feedback. Let''s innovate together!\n\n## How to Use\n\nPhi-3 Mini-128K-Instruct has been integrated in the development version (4.41.3) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.41.2\n```\n\nPhi-3 Mini-128K-Instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3)\n\n### Tokenizer\n\nPhi-3 Mini-128K-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3 Mini-128K-Instruct model is best suited for prompts using the chat format as follows. \nYou can provide the prompt as a question with a generic template as follow:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nQuestion?<|end|>\n<|assistant|>\n```\n\nFor example:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|> \n```\nwhere the model generates the text after `<|assistant|>` . In case of few-shots prompt, the prompt can be formatted as the following:\n\n```markdown\n<|system|>\nYou are a helpful travel assistant.<|end|>\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world''s largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it''s no wonder that Paris is one of the most popular tourist destinations in the world."<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nimport torch \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \nmodel = AutoModelForCausalLM.from_pretrained( \n    "microsoft/Phi-3-mini-128k-instruct",  \n    device_map="cuda",  \n    torch_dtype="auto",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct") \n\nmessages = [ \n    {"role": "system", "content": "You are a helpful AI assistant."}, \n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"}, \n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."}, \n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"}, \n] \n\npipe = pipeline( \n    "text-generation", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    "max_new_tokens": 500, \n    "return_full_text": False, \n    "temperature": 0.0, \n    "do_sample": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0][''generated_text'']) \n```\n\nNotes: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation="flash_attention_2"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n* Architecture: Phi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\n* Inputs: Text. It is best suited for prompts using chat format.\n* Context length: 128K tokens\n* GPUs: 512 H100-80G\n* Training time: 10 days\n* Training data: 4.9T tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between May and June 2024\n* Status: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n* Release dates: June, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3-Mini-128K-Instruct on standard open-source benchmarks measuring the model''s reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT-3.5.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k–shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-128K-Ins | Gemma-7B | Mistral-7B | Mixtral-8x7B | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n| :----------| :-----------| :---------------------| :----------| :------------| :--------------| :----------------| :-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.5 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 69.7 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard <br>3-shot | 72.1 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 52.3 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot | 70.5 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 85.5 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 77.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot | 56.4 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot | 78.8 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot | 80.1 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot | 29.7 | 2.9 | 15 | 6.9 | 32.4 | 29.9 |\n| | Social IQA <br>5-shot | 74.7 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot | 64.8 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot | 71.0 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot | 57.8 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoTT <br>8-shot | 85.3 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot | 60.4 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot | 70.0 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **66.4** | **56.0** | **56.4** | **64.4** | **65.5** | **70.3** |\n\n**Long Context**: Phi-3 Mini-128K-Instruct supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA. \n\n| Benchmark     | Phi-3 Mini-128K-Instruct | Mistral-7B | Mixtral 8x7B | LLaMA-3-8B-Instruct |\n| :---------------| :--------------------------|:------------|:--------------|:---------------------|\n| GovReport     | 25.3                     | 4.9        | 20.3         | 10.3                |\n| QMSum         | 21.9                     | 15.5       | 20.6         | 2.9                 |\n| Qasper        | 41.6                     | 23.5       | 26.6         | 8.1                 |\n| SQuALITY      | 24.1                     | 14.7       | 16.2         | 25                  |\n| SummScreenFD  | 16.8                     | 9.3        | 11.3         | 5.1                 |\n| **Average**   | **25.9**                 | **13.6**   | **19.0**     | **10.3**            |\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-128K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:--------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 60.6 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 69.4 | 60.3 | 62.8 | 68.1 | 69.6 | 71.7 |\n| Language understanding | 57.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 61.0 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 51.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 35.8 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.4 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.   \n\n## Cross Platform Support \n\n[ONNX runtime](https://onnxruntime.ai/blogs/accelerating-phi-3) now supports Phi-3 mini models across platforms and hardware.  \n\nOptimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA).   \n\nAlong with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile.  \n\nHere are some of the optimized configurations we have added:  \n\n1. ONNX models for int4 DML: Quantized to int4 via AWQ \n2. ONNX model for fp16 CUDA \n3. ONNX model for int4 CUDA: Quantized to int4 via RTN \n4. ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN \n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3 Mini-128K-Instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation="eager"\n* Optimized inference on GPU, CPU, and Mobile: use the **ONNX** models [128K](https://aka.ms/phi3-mini-128k-instruct-onnx)\n  \n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-128k/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3821079552,"storage_bytes":15284863483,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Phi3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi3.Phi3Config","AutoModelForCausalLM":"modeling_phi3.Phi3ForCausalLM"},"model_type":"phi3","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' %}{{''<|system|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''user'' %}{{''<|user|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''assistant'' %}{{''<|assistant|>\n'' + message[''content''] + ''<|end|>\n''}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>\n'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:huggingface:transformers`.","source_url":"https://github.com/huggingface/transformers`."},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"}]', NULL, 'MIT', 'approved', 80, '088d9484ff4f5e9ff75905a49fa03731', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3-Base', 'huggingface--deepseek-ai--deepseek-v3-base', 'DeepSeek-V3-Base', 'deepseek-ai', '<!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.s...', '["safetensors","deepseek_v3","custom_code","arxiv:2412.19437","fp8","region:us"]', 'other', 1681, 10539, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3-Base","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align="center">\n  <img width="80%" src="figures/benchmark.png">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align="center">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align="center">\n  <img width="80%" src="figures/niah.png">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align="center">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align="center">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface''s Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n```\n\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":684531386000,"storage_bytes":688591926227,"files_count":185,"spaces_count":4,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<｜begin▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<｜end▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<｜end▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<｜User｜>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>'' + tool[''type''] + ''<｜tool▁sep｜>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<｜tool▁call▁end｜>''}}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<｜tool▁call▁begin｜>'' + tool[''type''] + ''<｜tool▁sep｜>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<｜tool▁call▁end｜>''}}{{''<｜tool▁calls▁end｜><｜end▁of▁sentence｜>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<｜tool▁outputs▁end｜>'' + message[''content''] + ''<｜end▁of▁sentence｜>''}}{%- set ns.is_tool = false -%}{%- else %}{{''<｜Assistant｜>'' + message[''content''] + ''<｜end▁of▁sentence｜>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>'' + message[''content''] + ''<｜tool▁output▁end｜>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<｜tool▁output▁begin｜>'' + message[''content''] + ''<｜tool▁output▁end｜>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<｜tool▁outputs▁end｜>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<｜Assistant｜>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.git","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.git"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:InternLM:lmdeploy","source_url":"https://github.com/InternLM/lmdeploy"},{"type":"has_code","target_id":"github:InternLM:lmdeploy","source_url":"https://github.com/InternLM/lmdeploy"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT-LLM","source_url":"https://github.com/NVIDIA/TensorRT-LLM"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT-LLM","source_url":"https://github.com/NVIDIA/TensorRT-LLM"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, NULL, 'pending', 90, 'efe6c28b227695e20b0719e05791d15f', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3-Base/resolve/main/figures/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3-Base from https://huggingface.co/deepseek-ai/DeepSeek-V3-Base/resolve/main/figures/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3-Base.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-Hunyuan3D-2', 'huggingface--tencent--hunyuan3d-2', 'Hunyuan3D-2', 'tencent', '--- library_name: hunyuan3d-2 license: other license_name: tencent-hunyuan-community license_link: https://huggingface.co/tencent/Hunyuan3D-2/blob/main/LICENSE.txt language: - en - zh tags: - image-to-3d - text-to-3d pipeline_tag: image-to-3d extra_gated_eu_disallowed: true --- <p align="center"> <img src="./assets/images/teaser.jpg"> </p> <div align="center"> <a href=https://3d.hunyuan.tencent.com target="_blank"><img src=https://img.shields.io/badge/Hunyuan3D-black.svg?logo=homepage height=...', '["hunyuan3d-2","diffusers","safetensors","image-to-3d","text-to-3d","en","zh","arxiv:2501.12202","arxiv:2411.02293","license:other","region:us"]', 'image-to-3d', 1678, 81232, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/Hunyuan3D-2","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: hunyuan3d-2\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://huggingface.co/tencent/Hunyuan3D-2/blob/main/LICENSE.txt\nlanguage:\n  - en\n  - zh\ntags:\n  - image-to-3d\n  - text-to-3d\npipeline_tag: image-to-3d\nextra_gated_eu_disallowed: true\n---\n\n<p align="center">\n  <img src="./assets/images/teaser.jpg">\n</p>\n\n<div align="center">\n  <a href=https://3d.hunyuan.tencent.com target="_blank"><img src=https://img.shields.io/badge/Hunyuan3D-black.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2  target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px></a>\n  <a href=https://huggingface.co/tencent/Hunyuan3D-2 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://3d-models.hunyuan.tencent.com/ target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n<a href=https://discord.gg/GuaWYwzKbX target="_blank"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n    <a href=https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/report/Tencent_Hunyuan3D_2_0.pdf target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n</div>\n\n\n[//]: # (  <a href=# target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>)\n\n[//]: # (  <a href=# target="_blank"><img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px></a>)\n\n[//]: # (  <a href="#"><img alt="PyPI - Downloads" src="https://img.shields.io/pypi/v/mulankit?logo=pypi"  height=22px></a>)\n\n<br>\n<p align="center">\n“ Living out everyone’s imagination on creating and manipulating 3D assets.”\n</p>\n\nThis repository contains the models of the paper [Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation](https://huggingface.co/papers/2501.12202).\nFor code and more details on how to use it, refer to the [Github repository](https://github.com/Tencent/Hunyuan3D-2).\n\n## 🔥 News\n\n- Jan 21, 2025: 💬 Release [Hunyuan3D 2.0](https://huggingface.co/spaces/tencent/Hunyuan3D-2). Please give it a try!\n\n## **Abstract**\n\nWe present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.\nThis system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale\ntexture synthesis model - Hunyuan3D-Paint.\nThe shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly\naligns with a given condition image, laying a solid foundation for downstream applications.\nThe texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant\ntexture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation\nprocess of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes\nefficiently.\nWe systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,\nincluding the open-source models and closed-source models in geometry details, condition alignment, texture quality, and\ne.t.c.\n\n<p align="center">\n  <img src="assets/images/system.jpg">\n</p>\n\n## ☯️ **Hunyuan3D 2.0**\n\n### Architecture\n\nHunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the\nsynthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and\ntexture generation and also provides flexibility for texturing either generated or handcrafted meshes.\n\n<p align="left">\n  <img src="assets/images/arch.jpg">\n</p>\n\n### Performance\n\nWe have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.\nThe numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets\nand the condition following ability.\n\n| Model                   | CMMD(⬇)   | FID_CLIP(⬇) | FID(⬇)      | CLIP-score(⬆) |\n|-------------------------|-----------|-------------|-------------|---------------|\n| Top Open-source Model1  | 3.591     | 54.639      | 289.287     | 0.787         |\n| Top Close-source Model1 | 3.600     | 55.866      | 305.922     | 0.779         |\n| Top Close-source Model2 | 3.368     | 49.744      | 294.628     | 0.806         |\n| Top Close-source Model3 | 3.218     | 51.574      | 295.691     | 0.799         |\n| Hunyuan3D 2.0           | **3.193** | **49.165**  | **282.429** | **0.809**     |\n\nGeneration results of Hunyuan3D 2.0:\n<p align="left">\n  <img src="assets/images/e2e-1.gif"  height=300>\n  <img src="assets/images/e2e-2.gif"  height=300>\n</p>\n\n### Pretrained Models\n\n| Model                | Date       | Huggingface                                            |\n|----------------------|------------|--------------------------------------------------------| \n| Hunyuan3D-DiT-v2-0   | 2025-01-21 | [Download](https://huggingface.co/tencent/Hunyuan3D-2) |\n| Hunyuan3D-Paint-v2-0 | 2025-01-21 | [Download](https://huggingface.co/tencent/Hunyuan3D-2) |\n| Hunyuan3D-Delight-v2-0 | 2025-01-21 | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0) |\n\n## 🤗 Get Started with Hunyuan3D 2.0\n\nYou may follow the next steps to use Hunyuan3D 2.0 via code or the Gradio App.\n\n### Install Requirements\n\nPlease install Pytorch via the [official](https://pytorch.org/) site. Then install the other requirements via\n\n```bash\npip install -r requirements.txt\n# for texture\ncd hy3dgen/texgen/custom_rasterizer\npython3 setup.py install\ncd ../../..\ncd hy3dgen/texgen/differentiable_renderer\nbash compile_mesh_painter.sh OR python3 setup.py install (on Windows)\n```\n\n### API Usage\n\nWe designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -\nHunyuan3D-Paint.\n\nYou could assess **Hunyuan3D-DiT** via:\n\n```python\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(''tencent/Hunyuan3D-2'')\nmesh = pipeline(image=''assets/demo.png'')[0]\n```\n\nThe output mesh is a [trimesh object](https://trimesh.org/trimesh.html), which you could save to glb/obj (or other\nformat) file.\n\nFor **Hunyuan3D-Paint**, do the following:\n\n```python\nfrom hy3dgen.texgen import Hunyuan3DPaintPipeline\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n\n# let''s generate a mesh first\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(''tencent/Hunyuan3D-2'')\nmesh = pipeline(image=''assets/demo.png'')[0]\n\npipeline = Hunyuan3DPaintPipeline.from_pretrained(''tencent/Hunyuan3D-2'')\nmesh = pipeline(mesh, image=''assets/demo.png'')\n```\n\nPlease visit [minimal_demo.py](https://github.com/Tencent/Hunyuan3D-2/blob/main/minimal_demo.py) for more advanced usage, such as **text to 3D** and **texture generation\nfor handcrafted mesh**.\n\n### Gradio App\n\nYou could also host a [Gradio](https://www.gradio.app/) App in your own computer via:\n\n```bash\npip3 install gradio==3.39.0\npython3 gradio_app.py\n```\n\nDon''t forget to visit [Hunyuan3D](https://3d.hunyuan.tencent.com) for quick use, if you don''t want to host yourself.\n\n## 📑 Open-Source Plan\n\n- [x] Inference Code\n- [x] Model Checkpoints\n- [x] Technical Report\n- [ ] ComfyUI\n- [ ] TensorRT Version\n\n## 🔗 BibTeX\n\nIf you found this repository helpful, please cite our report:\n\n```bibtex\n@misc{hunyuan3d22025tencent,\n    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    eprint={2501.12202},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{yang2024tencent,\n    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2024},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n## Community Resources\n\nThanks for the contributions of community members, here we have these great extensions of Hunyuan3D 2.0:\n\n- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)\n- [Hunyuan3D-2-for-windows](https://github.com/sdbds/Hunyuan3D-2-for-windows)\n- [📦 A bundle for running on Windows | 整合包](https://github.com/YanWenKun/Comfy3D-WinPortable/releases/tag/r8-hunyuan3d2)\n\n## Acknowledgements\n\nWe would like to thank the contributors to\nthe [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers)\nand [HuggingFace](https://huggingface.co) repositories, for their open research and exploration.\n\n## Star History\n\n<a href="https://star-history.com/#Tencent/Hunyuan3D-2&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&type=Date" />\n </picture>\n</a>', '{"pipeline_tag":"image-to-3d","library_name":"hunyuan3d-2","framework":"hunyuan3d-2","params":null,"storage_bytes":62126388558,"files_count":82,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Tencent:Hunyuan3D-2","source_url":"https://github.com/Tencent/Hunyuan3D-2"},{"type":"has_code","target_id":"github:Tencent:Hunyuan3D-2","source_url":"https://github.com/Tencent/Hunyuan3D-2"},{"type":"has_code","target_id":"github:Tencent:Hunyuan3D-2","source_url":"https://github.com/Tencent/Hunyuan3D-2"},{"type":"has_code","target_id":"github:kijai:ComfyUI-Hunyuan3DWrapper","source_url":"https://github.com/kijai/ComfyUI-Hunyuan3DWrapper"},{"type":"has_code","target_id":"github:sdbds:Hunyuan3D-2-for-windows","source_url":"https://github.com/sdbds/Hunyuan3D-2-for-windows"},{"type":"has_code","target_id":"github:YanWenKun:Comfy3D-WinPortable","source_url":"https://github.com/YanWenKun/Comfy3D-WinPortable"},{"type":"has_code","target_id":"github:facebookresearch:dinov2","source_url":"https://github.com/facebookresearch/dinov2"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:black-forest-labs:flux","source_url":"https://github.com/black-forest-labs/flux"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2501.12202","source_url":"https://arxiv.org/abs/2501.12202"},{"type":"based_on_paper","target_id":"arxiv:2411.02293","source_url":"https://arxiv.org/abs/2411.02293"}]', NULL, 'Other', 'approved', 85, 'd7c1e5e83c5d7024a9394d6e1823759a', NULL, 'https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/assets/demo.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-Hunyuan3D-2 from https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/assets/demo.png
Image converted to WebP: data/images/huggingface-tencent-Hunyuan3D-2.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Nemo-Instruct-2407', 'huggingface--mistralai--mistral-nemo-instruct-2407', 'Mistral-Nemo-Instruct-2407', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - it - pt - ru - zh - ja license: apache-2.0 base_model: mistralai/Mistral-Nemo-Base-2407 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - mistral-common --- The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407. Trained jointly by Mistral AI and NVI...', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","it","pt","ru","zh","ja","base_model:mistralai/mistral-nemo-base-2407","license:apache-2.0","region:us"]', 'other', 1628, 197809, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ru\n- zh\n- ja\nlicense: apache-2.0\nbase_model: mistralai/Mistral-Nemo-Base-2407\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Model Card for Mistral-Nemo-Instruct-2407\n\nThe Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the [Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407). Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.\n\nFor more details about this model please refer to our release [blog post](https://mistral.ai/news/mistral-nemo/).\n\n## Key features\n- Released under the **Apache 2 License**\n- Pre-trained and instructed versions\n- Trained with a **128k context window**\n- Trained on a large proportion of **multilingual and code data**\n- Drop-in replacement of Mistral 7B\n\n## Model Architecture\nMistral Nemo is a transformer model, with the following architecture choices:\n- **Layers:** 40\n- **Dim:** 5,120\n- **Head dim:** 128\n- **Hidden dim:** 14,336\n- **Activation Function:** SwiGLU\n- **Number of heads:** 32\n- **Number of kv-heads:** 8 (GQA)\n- **Vocabulary size:** 2**17 ~= 128k\n- **Rotary embeddings (theta = 1M)**\n\n## Metrics\n\n### Main Benchmarks\n\n| Benchmark | Score |\n| --- | --- |\n| HellaSwag (0-shot) | 83.5% |\n| Winogrande (0-shot) | 76.8% |\n| OpenBookQA (0-shot) | 60.6% |\n| CommonSenseQA (0-shot) | 70.4% |\n| TruthfulQA (0-shot) | 50.3% |\n| MMLU (5-shot) | 68.0% |\n| TriviaQA (5-shot) | 73.8% |\n| NaturalQuestions (5-shot) | 31.2% |\n\n### Multilingual Benchmarks (MMLU)\n\n| Language | Score |\n| --- | --- |\n| French | 62.3% |\n| German | 62.7% |\n| Spanish | 64.6% |\n| Italian | 61.3% |\n| Portuguese | 63.3% |\n| Russian | 59.2% |\n| Chinese | 59.0% |\n| Japanese | 59.0% |\n\n## Usage\n\nThe model can be used with three different frameworks\n\n- [`mistral_inference`](https://github.com/mistralai/mistral-inference): See [here](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407#mistral-inference)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n- [`NeMo`](https://github.com/NVIDIA/NeMo): See [nvidia/Mistral-NeMo-12B-Instruct](https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct)\n\n### Mistral Inference\n\n#### Install\n\nIt is recommended to use `mistralai/Mistral-Nemo-Instruct-2407` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n#### Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''Nemo-Instruct'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Mistral-Nemo-Instruct-2407", allow_patterns=["params.json", "consolidated.safetensors", "tekken.json"], local_dir=mistral_models_path)\n```\n\n#### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/Nemo-Instruct --instruct --max_tokens 256 --temperature 0.35\n```\n\n*E.g.* Try out something like:\n```\nHow expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\n```\n\n#### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path)\n\nprompt = "How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar."\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n#### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name="get_current_weather",\n                description="Get the current weather",\n                parameters={\n                    "type": "object",\n                    "properties": {\n                        "location": {\n                            "type": "string",\n                            "description": "The city and state, e.g. San Francisco, CA",\n                        },\n                        "format": {\n                            "type": "string",\n                            "enum": ["celsius", "fahrenheit"],\n                            "description": "The temperature unit to use. Infer this from the users location.",\n                        },\n                    },\n                    "required": ["location", "format"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content="What''s the weather like today in Paris?"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Transformers\n\n> [!IMPORTANT]\n> NOTE: Until a new release has been made, you need to install transformers from source:\n> ```sh\n> pip install git+https://github.com/huggingface/transformers.git\n> ```\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\n\nmessages = [\n    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},\n    {"role": "user", "content": "Who are you?"},\n]\nchatbot = pipeline("text-generation", model="mistralai/Mistral-Nemo-Instruct-2407",max_new_tokens=128)\nchatbot(messages)\n```\n\n## Function calling with `transformers`\n\nTo use this example, you''ll need `transformers` version 4.42.0 or higher. Please see the \n[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)\nin the `transformers` docs for more information.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = "mistralai/Mistral-Nemo-Instruct-2407"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    """\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: ["celsius", "fahrenheit"])\n    """\n    pass\n\nconversation = [{"role": "user", "content": "What''s the weather like in Paris?"}]\ntools = [get_current_weather]\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors="pt",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), \nand note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\n\n> [!TIP]\n> Unlike previous Mistral models, Mistral Nemo requires smaller temperatures. We recommend to use a temperature of 0.3.\n\n## Limitations\n\nThe Mistral Nemo Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":12247782400,"storage_bytes":49044606999,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- ''{\"type\": \"function\", \"function\": {'' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- ''\"'' + key + ''\": \"'' + val + ''\"'' }}\n                    {%- else %}\n                        {{- ''\"'' + key + ''\": '' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- '', \"id\": \"'' + tool_call.id + ''\"}'' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- ''[TOOL_RESULTS]{\"content\": '' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- ''\"call_id\": \"'' + message.tool_call_id + ''\"}[/TOOL_RESULTS]'' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:NVIDIA:NeMo","source_url":"https://github.com/NVIDIA/NeMo"},{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"}]', NULL, 'Apache-2.0', 'approved', 80, '7703731921f567eee8943de36ed1341b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-xinsir-controlnet-union-sdxl-1.0', 'huggingface--xinsir--controlnet-union-sdxl-1.0', 'controlnet-union-sdxl-1.0', 'xinsir', '--- license: apache-2.0 tags: - Text-to-Image - ControlNet - Diffusers - Stable Diffusion pipeline_tag: text-to-image --- !images_display !images - Use bucket training like novelai, can generate high resolutions images of any aspect ratio - Use large amount of high quality data(over 10000000 images), the dataset covers a diversity of situation - Use re-captioned prompt like DALLE.3, use CogVLM to generate detailed description, good prompt following ability - Use many useful tricks during trai...', '["diffusers","safetensors","text-to-image","controlnet","diffusers","stable diffusion","text-to-image","license:apache-2.0","region:us"]', 'text-to-image', 1625, 149996, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/xinsir/controlnet-union-sdxl-1.0","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\ntags:\n- Text-to-Image\n- ControlNet\n- Diffusers\n- Stable Diffusion\npipeline_tag: text-to-image\n---\n\n# **ControlNet++: All-in-one ControlNet for image generations and editing!**\n## **ProMax Model has released!! 12 control + 5 advanced editing, just try it!!!**\n![images_display](./images/masonry.webp)\n\n## Network Arichitecture\n![images](./images/ControlNet++.png)\n\n## Advantages about the model\n- Use bucket training like novelai, can generate high resolutions images of any aspect ratio\n- Use large amount of high quality data(over 10000000 images), the dataset covers a diversity of situation\n- Use re-captioned prompt like DALLE.3, use CogVLM to generate detailed description, good prompt following ability\n- Use many useful tricks during training. Including but not limited to date augmentation, mutiple loss, multi resolution\n- Use almost the same parameter compared with original ControlNet. No obvious increase in network parameter or computation.\n- Support 10+ control conditions, no obvious performance drop on any single condition compared with training independently\n- Support multi condition generation, condition fusion is learned during training. No need to set hyperparameter or design prompts.\n- Compatible with other opensource SDXL models, such as BluePencilXL, CounterfeitXL. Compatible with other Lora models.\n\n\n***We design a new architecture that can support 10+ control types in condition text-to-image generation and can generate high resolution images visually comparable with \nmidjourney***. The network is based on the original ControlNet architecture, we propose two new modules to: 1 Extend the original ControlNet to support different image \nconditions using the same network parameter. 2 Support multiple conditions input without increasing computation offload, which is especially important for designers \nwho want to edit image in detail, different conditions use the same condition encoder, without adding extra computations or parameters. We do thoroughly experiments \non SDXL and achieve superior performance both in control ability and aesthetic score. We release the method and the model to the open source community to make everyone \ncan enjoy it.  \n\nInference scripts and more details can found: https://github.com/xinsir6/ControlNetPlus/tree/main\n\n**If you find it useful, please give me a star, thank you very much**\n\n**SDXL ProMax version has been released!!!，Enjoy it!!!**  \n\n**I am sorry that because of the project''s revenue and expenditure are difficult to balance, the GPU resources are assigned to other projects that are more likely to be profitable, the SD3 trainging is stopped until I find enough GPU supprt, I will try my best to find GPUs to continue training. If this brings you inconvenience, I sincerely apologize for that. I want to thank everyone who likes this project, your support is what keeps me going**\n\nNote: we put the promax model with a promax suffix in the same [huggingface model repo](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0), detailed instructions will be added later. \n## Advanced editing features in Promax Model\n### Tile Deblur\n![blur0](./images/100000_tile_blur_concat.webp)\n![blur1](./images/100001_tile_blur_concat.webp)\n![blur2](./images/100002_tile_blur_concat.webp)\n![blur3](./images/100003_tile_blur_concat.webp)\n![blur4](./images/100004_tile_blur_concat.webp)\n![blur5](./images/100005_tile_blur_concat.webp)\n### Tile variation\n![var0](./images/100006_tile_var_concat.webp)\n![var1](./images/100007_tile_var_concat.webp)\n![var2](./images/100008_tile_var_concat.webp)\n![var3](./images/100009_tile_var_concat.webp)\n![var4](./images/100010_tile_var_concat.webp)\n![var5](./images/100011_tile_var_concat.webp)\n\n### Tile Super Resolution\nFollowing example show from 1M resolution --> 9M resolution\n<div style="display: flex; justify-content: space-between;">\n  <img src="./images/tile_super1.webp" alt="Image 1" style="width: 49%; margin: 1%;">\n  <img src="./images/tile_super1_9upscale.webp" alt="Image 2" style="width: 49%; margin: 1%;">\n</div>\n\n<div style="display: flex; justify-content: space-between;">\n  <img src="./images/tile_super2.webp" alt="Image 1" style="width: 49%; margin: 1%;">\n  <img src="./images/tile_super2_9upscale.webp" alt="Image 2" style="width: 49%; margin: 1%;">\n</div>\n\n### Image Inpainting\n![inp0](./images/100018_inpainting_concat.webp)\n![inp1](./images/100019_inpainting_concat.webp)\n![inp2](./images/100020_inpainting_concat.webp)\n![inp3](./images/100021_inpainting_concat.webp)\n![inp4](./images/100022_inpainting_concat.webp)\n![inp5](./images/100023_inpainting_concat.webp)\n\n### Image Outpainting\n![oup0](./images/100012_outpainting_concat.webp)\n![oup1](./images/100013_outpainting_concat.webp)\n![oup2](./images/100014_outpainting_concat.webp)\n![oup3](./images/100015_outpainting_concat.webp)\n![oup4](./images/100016_outpainting_concat.webp)\n![oup5](./images/100017_outpainting_concat.webp)\n\n\n## Visual Examples\n### Openpose\n![pose0](./images/000000_pose_concat.webp)\n![pose1](./images/000001_pose_concat.webp)\n![pose2](./images/000002_pose_concat.webp)\n![pose3](./images/000003_pose_concat.webp)\n![pose4](./images/000004_pose_concat.webp)\n### Depth\n![depth0](./images/000005_depth_concat.webp)\n![depth1](./images/000006_depth_concat.webp)\n![depth2](./images/000007_depth_concat.webp)\n![depth3](./images/000008_depth_concat.webp)\n![depth4](./images/000009_depth_concat.webp)\n### Canny\n![canny0](./images/000010_canny_concat.webp)\n![canny1](./images/000011_canny_concat.webp)\n![canny2](./images/000012_canny_concat.webp)\n![canny3](./images/000013_canny_concat.webp)\n![canny4](./images/000014_canny_concat.webp)\n### Lineart\n![lineart0](./images/000015_lineart_concat.webp)\n![lineart1](./images/000016_lineart_concat.webp)\n![lineart2](./images/000017_lineart_concat.webp)\n![lineart3](./images/000018_lineart_concat.webp)\n![lineart4](./images/000019_lineart_concat.webp)\n### AnimeLineart\n![animelineart0](./images/000020_anime_lineart_concat.webp)\n![animelineart1](./images/000021_anime_lineart_concat.webp)\n![animelineart2](./images/000022_anime_lineart_concat.webp)\n![animelineart3](./images/000023_anime_lineart_concat.webp)\n![animelineart4](./images/000024_anime_lineart_concat.webp)\n### Mlsd\n![mlsd0](./images/000025_mlsd_concat.webp)\n![mlsd1](./images/000026_mlsd_concat.webp)\n![mlsd2](./images/000027_mlsd_concat.webp)\n![mlsd3](./images/000028_mlsd_concat.webp)\n![mlsd4](./images/000029_mlsd_concat.webp)\n### Scribble\n![scribble0](./images/000030_scribble_concat.webp)\n![scribble1](./images/000031_scribble_concat.webp)\n![scribble2](./images/000032_scribble_concat.webp)\n![scribble3](./images/000033_scribble_concat.webp)\n![scribble4](./images/000034_scribble_concat.webp)\n### Hed\n![hed0](./images/000035_hed_concat.webp)\n![hed1](./images/000036_hed_concat.webp)\n![hed2](./images/000037_hed_concat.webp)\n![hed3](./images/000038_hed_concat.webp)\n![hed4](./images/000039_hed_concat.webp)\n### Pidi(Softedge)\n![pidi0](./images/000040_softedge_concat.webp)\n![pidi1](./images/000041_softedge_concat.webp)\n![pidi2](./images/000042_softedge_concat.webp)\n![pidi3](./images/000043_softedge_concat.webp)\n![pidi4](./images/000044_softedge_concat.webp)\n### Teed\n![ted0](./images/000045_ted_concat.webp)\n![ted1](./images/000046_ted_concat.webp)\n![ted2](./images/000047_ted_concat.webp)\n![ted3](./images/000048_ted_concat.webp)\n![ted4](./images/000049_ted_concat.webp)\n### Segment\n![segment0](./images/000050_seg_concat.webp)\n![segment1](./images/000051_seg_concat.webp)\n![segment2](./images/000052_seg_concat.webp)\n![segment3](./images/000053_seg_concat.webp)\n![segment4](./images/000054_seg_concat.webp)\n### Normal\n![normal0](./images/000055_normal_concat.webp)\n![normal1](./images/000056_normal_concat.webp)\n![normal2](./images/000057_normal_concat.webp)\n![normal3](./images/000058_normal_concat.webp)\n![normal4](./images/000059_normal_concat.webp)\n\n## Multi Control Visual Examples\n### Openpose + Canny\n![pose_canny0](./images/000007_openpose_canny_concat.webp)\n![pose_canny1](./images/000008_openpose_canny_concat.webp)\n![pose_canny2](./images/000009_openpose_canny_concat.webp)\n![pose_canny3](./images/000010_openpose_canny_concat.webp)\n![pose_canny4](./images/000011_openpose_canny_concat.webp)\n![pose_canny5](./images/000012_openpose_canny_concat.webp)\n\n### Openpose + Depth\n![pose_depth0](./images/000013_openpose_depth_concat.webp)\n![pose_depth1](./images/000014_openpose_depth_concat.webp)\n![pose_depth2](./images/000015_openpose_depth_concat.webp)\n![pose_depth3](./images/000016_openpose_depth_concat.webp)\n![pose_depth4](./images/000017_openpose_depth_concat.webp)\n![pose_depth5](./images/000018_openpose_depth_concat.webp)\n\n### Openpose + Scribble\n![pose_scribble0](./images/000001_openpose_scribble_concat.webp)\n![pose_scribble1](./images/000002_openpose_scribble_concat.webp)\n![pose_scribble2](./images/000003_openpose_scribble_concat.webp)\n![pose_scribble3](./images/000004_openpose_scribble_concat.webp)\n![pose_scribble4](./images/000005_openpose_scribble_concat.webp)\n![pose_scribble5](./images/000006_openpose_scribble_concat.webp)\n\n### Openpose + Normal\n![pose_normal0](./images/000019_openpose_normal_concat.webp)\n![pose_normal1](./images/000020_openpose_normal_concat.webp)\n![pose_normal2](./images/000021_openpose_normal_concat.webp)\n![pose_normal3](./images/000022_openpose_normal_concat.webp)\n![pose_normal4](./images/000023_openpose_normal_concat.webp)\n![pose_normal5](./images/000024_openpose_normal_concat.webp)\n\n### Openpose + Segment\n![pose_segment0](./images/000025_openpose_sam_concat.webp)\n![pose_segment1](./images/000026_openpose_sam_concat.webp)\n![pose_segment2](./images/000027_openpose_sam_concat.webp)\n![pose_segment3](./images/000028_openpose_sam_concat.webp)\n![pose_segment4](./images/000029_openpose_sam_concat.webp)\n![pose_segment5](./images/000030_openpose_sam_concat.webp)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":5039863050,"files_count":126,"spaces_count":100,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:xinsir6:ControlNetPlus","source_url":"https://github.com/xinsir6/ControlNetPlus"}]', NULL, 'Apache-2.0', 'approved', 65, 'ac18c4e770efea776243eb1c7d29d397', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-2Noise-ChatTTS', 'huggingface--2noise--chattts', 'ChatTTS', '2Noise', '--- license: cc-by-nc-4.0 library_name: chat_tts pipeline_tag: text-to-audio --- **We are also training larger-scale models and need computational power and data support. If you can provide assistance, please contact OPEN-SOURCE@2NOISE.COM. Thank you very much.** First, clone the Git repository: **For more usage examples, please refer to the example notebook, which includes parameters for finer control over the generated speech, such as specifying the speaker, adjusting speech speed, and addi...', '["chat_tts","safetensors","text-to-audio","license:cc-by-nc-4.0","region:us"]', 'text-to-audio', 1619, 1493, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/2Noise/ChatTTS","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlibrary_name: chat_tts\npipeline_tag: text-to-audio\n---\n\n\n**We are also training larger-scale models and need computational power and data support. If you can provide assistance, please contact OPEN-SOURCE@2NOISE.COM. Thank you very much.**\n\n## Clone the Repository\nFirst, clone the Git repository:\n```bash\ngit clone https://github.com/2noise/ChatTTS.git\n```\n\n## Model Inference\n\n\n```python\n# Import necessary libraries and configure settings\nimport torch\nimport torchaudio\ntorch._dynamo.config.cache_size_limit = 64\ntorch._dynamo.config.suppress_errors = True\ntorch.set_float32_matmul_precision(''high'')\n\nimport ChatTTS\nfrom IPython.display import Audio\n\n# Initialize and load the model: \nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\n# Define the text input for inference (Support Batching)\ntexts = [\n    "So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.",\n    ]\n\n# Perform inference and play the generated audio\nwavs = chat.infer(texts)\nAudio(wavs[0], rate=24_000, autoplay=True)\n\n# Save the generated audio \ntorchaudio.save("output.wav", torch.from_numpy(wavs[0]), 24000)\n```\n**For more usage examples, please refer to the [example notebook](https://github.com/2noise/ChatTTS/blob/main/example.ipynb), which includes parameters for finer control over the generated speech, such as specifying the speaker, adjusting speech speed, and adding laughter.**\n\n\n\n\n\n\n### Disclaimer: For Academic Purposes Only\n\nThe information provided in this document is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information.', '{"pipeline_tag":"text-to-audio","library_name":"chat_tts","framework":"chat_tts","params":null,"storage_bytes":2364745599,"files_count":23,"spaces_count":53,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:2noise:ChatTTS.git","source_url":"https://github.com/2noise/ChatTTS.git"},{"type":"has_code","target_id":"github:2noise:ChatTTS","source_url":"https://github.com/2noise/ChatTTS"}]', NULL, 'CC-BY-NC-4.0', 'approved', 50, '6518575df2b78cbeb71960946de9c56d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-docling-project-SmolDocling-256M-preview', 'huggingface--docling-project--smoldocling-256m-preview', 'SmolDocling-256M-preview', 'docling-project', '--- base_model: - HuggingFaceTB/SmolVLM-256M-Instruct language: - en library_name: transformers license: cdla-permissive-2.0 pipeline_tag: image-text-to-text datasets: - ds4sd/SynthCodeNet - ds4sd/SynthFormulaNet - ds4sd/SynthChartNet - HuggingFaceM4/DoclingMatix --- <div style=" background-color: #f0f9ff; border: 1px solid #bae6fd; color: #0369a1; padding: 12px 16px; border-radius: 12px; margin-bottom: 16px; font-family: sans-serif; "> <strong>📢 New Release:</strong> We’ve released <a href=...', '["transformers","onnx","safetensors","idefics3","image-to-text","image-text-to-text","conversational","en","dataset:ds4sd/synthcodenet","dataset:ds4sd/synthformulanet","dataset:ds4sd/synthchartnet","dataset:huggingfacem4/doclingmatix","arxiv:2503.11576","arxiv:2305.03393","base_model:huggingfacetb/smolvlm-256m-instruct","license:cdla-permissive-2.0","endpoints_compatible","region:us"]', 'image-text-to-text', 1599, 137888, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/docling-project/SmolDocling-256M-preview","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nbase_model:\n- HuggingFaceTB/SmolVLM-256M-Instruct\nlanguage:\n- en\nlibrary_name: transformers\nlicense: cdla-permissive-2.0\npipeline_tag: image-text-to-text\ndatasets:\n- ds4sd/SynthCodeNet\n- ds4sd/SynthFormulaNet\n- ds4sd/SynthChartNet\n- HuggingFaceM4/DoclingMatix\n---\n\n<div style="\n  background-color: #f0f9ff;\n  border: 1px solid #bae6fd;\n  color: #0369a1;\n  padding: 12px 16px;\n  border-radius: 12px;\n  margin-bottom: 16px;\n  font-family: sans-serif;\n">\n  <strong>📢 New Release:</strong>  \n  We’ve released <a href="https://huggingface.co/ibm-granite/granite-docling-258M" target="_blank" style="color:#0284c7; font-weight:bold; text-decoration:underline;">\n    granite-docling-258M</a>, the successor to <b>SmolDocling</b>. It will now receive updates and support, check it out!\n</div>\n\n<div style="display: flex; align-items: center;">\n    <img src="https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/assets/SmolDocling_doctags1.png" alt="SmolDocling" style="width: 200px; height: auto; margin-right: 20px;">\n    <div>\n        <h3>SmolDocling-256M-preview</h3>\n        <p>SmolDocling is a multimodal Image-Text-to-Text model designed for efficient document conversion. It retains Docling''s most popular features while ensuring full compatibility with Docling through seamless support for <strong>DoclingDocuments</strong>.</p>\n    </div>\n</div>\n\nThis model was presented in the paper [SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion](https://huggingface.co/papers/2503.11576).\n\n### 🚀 Features:  \n- 🏷️ **DocTags for Efficient Tokenization** – Introduces DocTags an efficient and minimal representation for documents that is fully compatible with **DoclingDocuments**.  \n- 🔍 **OCR (Optical Character Recognition)** – Extracts text accurately from images.  \n- 📐 **Layout and Localization** – Preserves document structure and document element **bounding boxes**.  \n- 💻 **Code Recognition** – Detects and formats code blocks including identation.  \n- 🔢 **Formula Recognition** – Identifies and processes mathematical expressions.  \n- 📊 **Chart Recognition** – Extracts and interprets chart data.  \n- 📑 **Table Recognition** – Supports column and row headers for structured table extraction.  \n- 🖼️ **Figure Classification** – Differentiates figures and graphical elements.  \n- 📝 **Caption Correspondence** – Links captions to relevant images and figures.  \n- 📜 **List Grouping** – Organizes and structures list elements correctly.  \n- 📄 **Full-Page Conversion** – Processes entire pages for comprehensive document conversion including all page elements (code, equations, tables, charts etc.) \n- 🔲 **OCR with Bounding Boxes** – OCR regions using a bounding box.\n- 📂 **General Document Processing** – Trained for both scientific and non-scientific documents.  \n- 🔄 **Seamless Docling Integration** – Import into **Docling** and export in multiple formats.\n- 💨 **Fast inference using VLLM** – Avg of 0.35 secs per page on A100 GPU.\n\n### 🚧 *Coming soon!*\n- 📊 **Better chart recognition 🛠️**\n- 📚 **One shot multi-page inference ⏱️**\n- 🧪 **Chemical Recognition**\n- 📙 **Datasets**\n\n## ⌨️ Get started (code examples)\n\nYou can use **transformers**, **vllm**, or **onnx** to perform inference, and [Docling](https://github.com/docling-project/docling) to convert results to variety of output formats (md, html, etc.):\n\n<details>\n<summary>📄 Single page image inference using Tranformers 🤖</summary>\n\n```python\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Load images\nimage = load_image("https://upload.wikimedia.org/wikipedia/commons/7/76/GazettedeFrance.jpg")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained("ds4sd/SmolDocling-256M-preview")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "ds4sd/SmolDocling-256M-preview",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation="flash_attention_2" if DEVICE == "cuda" else "eager",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "text", "text": "Convert this page to docling."}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors="pt")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\n\n# export as any format\n# HTML\n# Path("Out/").mkdir(parents=True, exist_ok=True)\n# output_path_html = Path("Out/") / "example.html"\n# doc.save_as_html(output_path_html)\n# MD\nprint(doc.export_to_markdown())\n```\n</details>\n\n\n<details>\n<summary> 🚀 Fast Batch Inference Using VLLM</summary>\n\n```python\n# Prerequisites:\n# pip install vllm\n# pip install docling_core\n# place page images you want to convert into "img/" dir\n\nimport time\nimport os\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom pathlib import Path\n\n# Configuration\nMODEL_PATH = "ds4sd/SmolDocling-256M-preview"\nIMAGE_DIR = "img/"  # Place your page images here\nOUTPUT_DIR = "out/"\nPROMPT_TEXT = "Convert page to Docling."\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Initialize LLM\nllm = LLM(model=MODEL_PATH, limit_mm_per_prompt={"image": 1})\n\nsampling_params = SamplingParams(\n    temperature=0.0,\n    max_tokens=8192)\n\nchat_template = f"<|im_start|>User:<image>{PROMPT_TEXT}<end_of_utterance>\nAssistant:"\n\nimage_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith((".png", ".jpg", ".jpeg"))])\n\nstart_time = time.time()\ntotal_tokens = 0\n\nfor idx, img_file in enumerate(image_files, 1):\n    img_path = os.path.join(IMAGE_DIR, img_file)\n    image = Image.open(img_path).convert("RGB")\n\n    llm_input = {"prompt": chat_template, "multi_modal_data": {"image": image}}\n    output = llm.generate([llm_input], sampling_params=sampling_params)[0]\n    \n    doctags = output.outputs[0].text\n    img_fn = os.path.splitext(img_file)[0]\n    output_filename = img_fn + ".dt"\n    output_path = os.path.join(OUTPUT_DIR, output_filename)\n\n    with open(output_path, "w", encoding="utf-8") as f:\n        f.write(doctags)\n\n    # To convert to Docling Document, MD, HTML, etc.:\n    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\n    # export as any format\n    # HTML\n    # output_path_html = Path(OUTPUT_DIR) / f"{img_fn}.html"\n    # doc.save_as_html(output_path_html)\n    # MD\n    output_path_md = Path(OUTPUT_DIR) / f"{img_fn}.md"\n    doc.save_as_markdown(output_path_md)\nprint(f"Total time: {time.time() - start_time:.2f} sec")\n```\n</details>\n<details>\n<summary> ONNX Inference</summary>\n\n```python\n# Prerequisites:\n# pip install onnxruntime\n# pip install onnxruntime-gpu\nfrom transformers import AutoConfig, AutoProcessor\nfrom transformers.image_utils import load_image\nimport onnxruntime\nimport numpy as np\nimport os\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\n\nos.environ["OMP_NUM_THREADS"] = "1"\n# cuda\nos.environ["ORT_CUDA_USE_MAX_WORKSPACE"] = "1"\n\n# 1. Load models\n## Load config and processor\nmodel_id = "ds4sd/SmolDocling-256M-preview"\nconfig = AutoConfig.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n## Load sessions\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/vision_encoder.onnx\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/embed_tokens.onnx\n# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/decoder_model_merged.onnx\n# cpu\n# vision_session = onnxruntime.InferenceSession("vision_encoder.onnx")\n# embed_session = onnxruntime.InferenceSession("embed_tokens.onnx")\n# decoder_session = onnxruntime.InferenceSession("decoder_model_merged.onnx"\n\n# cuda\nvision_session = onnxruntime.InferenceSession("vision_encoder.onnx", providers=["CUDAExecutionProvider"])\nembed_session = onnxruntime.InferenceSession("embed_tokens.onnx", providers=["CUDAExecutionProvider"])\ndecoder_session = onnxruntime.InferenceSession("decoder_model_merged.onnx", providers=["CUDAExecutionProvider"])\n\n## Set config values\nnum_key_value_heads = config.text_config.num_key_value_heads\nhead_dim = config.text_config.head_dim\nnum_hidden_layers = config.text_config.num_hidden_layers\neos_token_id = config.text_config.eos_token_id\nimage_token_id = config.image_token_id\nend_of_utterance_id = processor.tokenizer.convert_tokens_to_ids("<end_of_utterance>")\n\n# 2. Prepare inputs\n## Create input messages\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "text", "text": "Convert this page to docling."}\n        ]\n    },\n]\n\n## Load image and apply processor\nimage = load_image("https://ibm.biz/docling-page-with-table")\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors="np")\n\n## Prepare decoder inputs\nbatch_size = inputs[''input_ids''].shape[0]\npast_key_values = {\n    f''past_key_values.{layer}.{kv}'': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n    for layer in range(num_hidden_layers)\n    for kv in (''key'', ''value'')\n}\nimage_features = None\ninput_ids = inputs[''input_ids'']\nattention_mask = inputs[''attention_mask'']\nposition_ids = np.cumsum(inputs[''attention_mask''], axis=-1)\n\n\n# 3. Generation loop\nmax_new_tokens = 8192\ngenerated_tokens = np.array([[]], dtype=np.int64)\nfor i in range(max_new_tokens):\n  inputs_embeds = embed_session.run(None, {''input_ids'': input_ids})[0]\n\n  if image_features is None:\n    ## Only compute vision features if not already computed\n    image_features = vision_session.run(\n        [''image_features''],  # List of output names or indices\n        {\n            ''pixel_values'': inputs[''pixel_values''],\n            ''pixel_attention_mask'': inputs[''pixel_attention_mask''].astype(np.bool_)\n        }\n    )[0]\n    \n    ## Merge text and vision embeddings\n    inputs_embeds[inputs[''input_ids''] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])\n\n  logits, *present_key_values = decoder_session.run(None, dict(\n      inputs_embeds=inputs_embeds,\n      attention_mask=attention_mask,\n      position_ids=position_ids,\n      **past_key_values,\n  ))\n\n  ## Update values for next generation loop\n  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n  attention_mask = np.ones_like(input_ids)\n  position_ids = position_ids[:, -1:] + 1\n  for j, key in enumerate(past_key_values):\n    past_key_values[key] = present_key_values[j]\n\n  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n  if (input_ids == eos_token_id).all() or (input_ids == end_of_utterance_id).all():\n    break  # Stop predicting\n\ndoctags = processor.batch_decode(\n    generated_tokens,\n    skip_special_tokens=False,\n)[0].lstrip()\n\nprint(doctags)\n\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\n\nprint(doc.export_to_markdown())\n```\n</details>\n\n\n💻 Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ds4sd/SmolDocling-256M-preview-mlx-bf16)\n\n## DocTags\n\n<img src="https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/assets/doctags_v2.png" width="800" height="auto" alt="Image description">\nDocTags create a clear and structured system of tags and rules that separate text from the document''s structure. This makes things easier for Image-to-Sequence models by reducing confusion. On the other hand, converting directly to formats like HTML or Markdown can be messy—it often loses details, doesn’t clearly show the document’s layout, and increases the number of tokens, making processing less efficient.\nDocTags are integrated with Docling, which allows export to HTML, Markdown, and JSON. These exports can be offloaded to the CPU, reducing token generation overhead and improving efficiency.\n\n## Supported Instructions\n\n<table>\n  <tr>\n    <td><b>Description</b></td>\n    <td><b>Instruction</b></td>\n    <td><b>Comment</b></td>\n  </tr>\n  <tr>\n    <td><b>Full conversion</b></td>\n    <td>Convert this page to docling.</td>\n    <td>DocTags represetation</td>\n  </tr>\n  <tr>\n    <td><b>Chart</b></td>\n    <td>Convert chart to table.</td>\n    <td>(e.g., &lt;chart&gt;)</td>\n  </tr>\n  <tr>\n    <td><b>Formula</b></td>\n    <td>Convert formula to LaTeX.</td>\n    <td>(e.g., &lt;formula&gt;)</td>\n  </tr>\n  <tr>\n    <td><b>Code</b></td>\n    <td>Convert code to text.</td>\n    <td>(e.g., &lt;code&gt;)</td>\n  </tr>\n  <tr>\n    <td><b>Table</b></td>\n    <td>Convert table to OTSL.</td>\n    <td>(e.g., &lt;otsl&gt;) OTSL: <a href="https://arxiv.org/pdf/2305.03393">Lysak et al., 2023</a></td>\n  </tr>\n  <tr>\n    <td rowspan=4><b>Actions and Pipelines</b></td>\n    <td>OCR the text in a specific location: &lt;loc_155&gt;&lt;loc_233&gt;&lt;loc_206&gt;&lt;loc_237&gt;</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>Identify element at: &lt;loc_247&gt;&lt;loc_482&gt;&lt;10c_252&gt;&lt;loc_486&gt;</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>Find all ''text'' elements on the page, retrieve all section headers.</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>Detect footer elements on the page.</td>\n    <td></td>\n  </tr>\n</table>\n\n\n#### 📊 Datasets\n- [SynthCodeNet](https://huggingface.co/datasets/ds4sd/SynthCodeNet)\n- [SynthFormulaNet](https://huggingface.co/datasets/ds4sd/SynthFormulaNet)\n- [SynthChartNet](https://huggingface.co/datasets/ds4sd/SynthChartNet)\n- [DoclingMatix](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix)\n\n#### Model Summary\n\n- **Developed by:** Docling Team, IBM Research\n- **Model type:** Multi-modal model (image+text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n- **Finetuned from model:** Based on [SmolVLM-256M-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct)\n\n**Repository:** [Docling](https://github.com/docling-project/docling)\n\n**Paper:** [arXiv](https://arxiv.org/abs/2503.11576)\n\n**Project Page:** [Hugging Face](https://huggingface.co/ds4sd/SmolDocling-256M-preview)\n\n**Citation:**\n```\n@misc{nassar2025smoldoclingultracompactvisionlanguagemodel,\n      title={SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion}, \n      author={Ahmed Nassar and Andres Marafioti and Matteo Omenetti and Maksym Lysak and Nikolaos Livathinos and Christoph Auer and Lucas Morin and Rafael Teixeira de Lima and Yusik Kim and A. Said Gurbuz and Michele Dolfi and Miquel Farré and Peter W. J. Staar},\n      year={2025},\n      eprint={2503.11576},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.11576}, \n}\n```\n**Demo:** [HF Space](https://huggingface.co/spaces/ds4sd/SmolDocling-256M-Demo)', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":256484928,"storage_bytes":6522512627,"files_count":41,"spaces_count":21,"gated":false,"private":false,"config":{"architectures":["Idefics3ForConditionalGeneration"],"model_type":"idefics3","processor_config":{"chat_template":"<|im_start|>{% for message in messages %}{{message[''role''] | capitalize}}{% if message[''content''][0][''type''] == ''image'' %}{{'':''}}{% else %}{{'': ''}}{% endif %}{% for line in message[''content''] %}{% if line[''type''] == ''text'' %}{{line[''text'']}}{% elif line[''type''] == ''image'' %}{{ ''<image>'' }}{% endif %}{% endfor %}<end_of_utterance>\n{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}"},"tokenizer_config":{"bos_token":"<|im_start|>","chat_template":"<|im_start|>{% for message in messages %}{{message[''role''] | capitalize}}{% if message[''content''][0][''type''] == ''image'' %}{{'':''}}{% else %}{{'': ''}}{% endif %}{% for line in message[''content''] %}{% if line[''type''] == ''text'' %}{{line[''text'']}}{% elif line[''type''] == ''image'' %}{{ ''<image>'' }}{% endif %}{% endfor %}<end_of_utterance>\n{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}","eos_token":"<|im_end|>","pad_token":"<|im_end|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"},{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"},{"type":"based_on_paper","target_id":"arxiv:2503.11576","source_url":"https://arxiv.org/abs/2503.11576"},{"type":"based_on_paper","target_id":"arxiv:2305.03393","source_url":"https://arxiv.org/abs/2305.03393"}]', NULL, 'cdla-permissive-2.0', 'approved', 100, 'd7ebe12b4cac39d11988e93a220ab095', NULL, 'https://huggingface.co/docling-project/SmolDocling-256M-preview/resolve/main/assets/SmolDocling_doctags1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-docling-project-SmolDocling-256M-preview from https://huggingface.co/docling-project/SmolDocling-256M-preview/resolve/main/assets/SmolDocling_doctags1.png
Image converted to WebP: data/images/huggingface-docling-project-SmolDocling-256M-preview.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nanonets-Nanonets-OCR-s', 'huggingface--nanonets--nanonets-ocr-s', 'Nanonets-OCR-s', 'nanonets', '--- language: - en base_model: - Qwen/Qwen2.5-VL-3B-Instruct pipeline_tag: image-text-to-text tags: - OCR - pdf2markdown library_name: transformers --- Nanonets-OCR-s by Nanonets is a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction. It transforms documents into structured markdown with intelligent content recognition and semantic tagging, making it ideal for downstream processing by Large Language Models (LLMs). Nanonets-OCR-s is packed ...', '["transformers","safetensors","qwen2_5_vl","image-to-text","ocr","pdf2markdown","image-text-to-text","conversational","en","base_model:qwen/qwen2.5-vl-3b-instruct","base_model:finetune:qwen/qwen2.5-vl-3b-instruct","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 1560, 98835, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nanonets/Nanonets-OCR-s","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-VL-3B-Instruct\npipeline_tag: image-text-to-text\ntags:\n- OCR\n- pdf2markdown\nlibrary_name: transformers\n---\n\n\nNanonets-OCR-s by [Nanonets](https://nanonets.com) is a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction. It transforms documents into structured markdown with intelligent content recognition and semantic tagging, making it ideal for downstream processing by Large Language Models (LLMs).\n\nNanonets-OCR-s is packed with features designed to handle complex documents with ease:\n\n* **LaTeX Equation Recognition:** Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (`$...$`) and display (`$$...$$`) equations.\n* **Intelligent Image Description:** Describes images within documents using structured `<img>` tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.\n* **Signature Detection & Isolation:** Identifies and isolates signatures from other text, outputting them within a `<signature>` tag. This is crucial for processing legal and business documents.\n* **Watermark Extraction:** Detects and extracts watermark text from documents, placing it within a `<watermark>` tag.\n* **Smart Checkbox Handling:** Converts form checkboxes and radio buttons into standardized Unicode symbols (`☐`, `☑`, `☒`) for consistent and reliable processing.\n* **Complex Table Extraction:** Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.\n\n\n📢 [Read the full announcement](https://nanonets.com/research/nanonets-ocr-s) | 🤗 [Hugging Face Space Demo](https://huggingface.co/spaces/Souvik3333/Nanonets-ocr-s)\n\n## Usage\n### Using transformers\n```python\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n\nmodel_path = "nanonets/Nanonets-OCR-s"\n\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_path, \n    torch_dtype="auto", \n    device_map="auto", \n    attn_implementation="flash_attention_2"\n)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprocessor = AutoProcessor.from_pretrained(model_path)\n\n\ndef ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):\n    prompt = """Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes."""\n    image = Image.open(image_path)\n    messages = [\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": [\n            {"type": "image", "image": f"file://{image_path}"},\n            {"type": "text", "text": prompt},\n        ]},\n    ]\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt")\n    inputs = inputs.to(model.device)\n    \n    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n    \n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    return output_text[0]\n\nimage_path = "/path/to/your/document.jpg"\nresult = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)\nprint(result)\n```\n\n### Using vLLM\n1. Start the vLLM server.\n```bash\nvllm serve nanonets/Nanonets-OCR-s\n```\n2. Predict with the model\n```python\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(api_key="123", base_url="http://localhost:8000/v1")\n\nmodel = "nanonets/Nanonets-OCR-s"\n\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode("utf-8")\n\ndef ocr_page_with_nanonets_s(img_base64):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "image_url",\n                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},\n                    },\n                    {\n                        "type": "text",\n                        "text": "Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.",\n                    },\n                ],\n            }\n        ],\n        temperature=0.0,\n        max_tokens=15000\n    )\n    return response.choices[0].message.content\n\ntest_img_path = "/path/to/your/document.jpg"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_nanonets_s(img_base64))\n```\n\n### Using docext\n```python\npip install docext\npython -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s\n```\nCheckout [GitHub](https://github.com/NanoNets/docext/tree/dev/markdown) for more details.\n\n\n## BibTex\n```\n@misc{Nanonets-OCR-S,\n  title={Nanonets-OCR-S: A model for transforming documents into structured markdown with intelligent content recognition and semantic tagging},\n  author={Souvik Mandal and Ashish Talewar and Paras Ahuja and Prathamesh Juvatkar},\n  year={2025},\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":3754622976,"storage_bytes":7521673474,"files_count":17,"spaces_count":34,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","tokenizer_config":{"bos_token":null,"eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null},"chat_template_jinja":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"}}', '[]', '[{"type":"has_code","target_id":"github:NanoNets:docext","source_url":"https://github.com/NanoNets/docext"}]', NULL, NULL, 'pending', 55, '66b8e61422f7f8a41d9411f00b11370c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.2-11B-Vision-Instruct', 'huggingface--meta-llama--llama-3.2-11b-vision-instruct', 'Llama-3.2-11B-Vision-Instruct', 'meta-llama', '', '["transformers","safetensors","mllama","image-to-text","facebook","meta","pytorch","llama","llama-3","image-text-to-text","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.2","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 1544, 180826, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":10670220835,"storage_bytes":42618467462,"files_count":20,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["MllamaForConditionalGeneration"],"model_type":"mllama","processor_config":{"chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n    {%- set user_supplied_system_message = true %}\n{%- else %}\n    {%- set system_message = \"\" %}\n    {%- set user_supplied_system_message = false %}\n{%- endif %}\n\n{#- Find out if there are any images #}\n{% set image_ns = namespace(has_images=false) %}      \n{%- for message in messages %}\n    {%- for content in message[''content''] %}\n        {%- if content[''type''] == ''image'' %}\n            {%- set image_ns.has_images = true %}\n        {%- endif %}\n    {%- endfor %}\n{%- endfor %}\n\n{#- System message if there are no images, or if the user supplied one #}\n{%- if user_supplied_system_message or not image_ns.has_images %}\n    {{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n    {%- if tools is not none %}\n        {{- \"Environment: ipython\\n\" }}\n    {%- endif %}\n    {{- \"Cutting Knowledge Date: December 2023\\n\" }}\n    {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n    {%- if tools is not none and not tools_in_user_message %}\n        {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n        {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n        {{- \"Do not use variables.\\n\\n\" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- \"\\n\\n\" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- system_message }}\n    {{- \"<|eot_id|>\" }}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n    {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n'' }}\n        {%- if message[''content''] is string %}\n            {{- message[''content''] }}\n        {%- else %}\n            {%- for content in message[''content''] %}\n                {%- if content[''type''] == ''image'' %}\n                    {{- ''<|image|>'' }}\n                {%- elif content[''type''] == ''text'' %}\n                    {{- content[''text''] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n"},"tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n    {%- set user_supplied_system_message = true %}\n{%- else %}\n    {%- set system_message = \"\" %}\n    {%- set user_supplied_system_message = false %}\n{%- endif %}\n\n{#- Find out if there are any images #}\n{% set image_ns = namespace(has_images=false) %}      \n{%- for message in messages %}\n    {%- for content in message[''content''] %}\n        {%- if content[''type''] == ''image'' %}\n            {%- set image_ns.has_images = true %}\n        {%- endif %}\n    {%- endfor %}\n{%- endfor %}\n\n{#- System message if there are no images, or if the user supplied one #}\n{%- if user_supplied_system_message or not image_ns.has_images %}\n    {{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n    {%- if tools is not none %}\n        {{- \"Environment: ipython\\n\" }}\n    {%- endif %}\n    {{- \"Cutting Knowledge Date: December 2023\\n\" }}\n    {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n    {%- if tools is not none and not tools_in_user_message %}\n        {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n        {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n        {{- \"Do not use variables.\\n\\n\" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- \"\\n\\n\" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- system_message }}\n    {{- \"<|eot_id|>\" }}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n    {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n'' }}\n        {%- if message[''content''] is string %}\n            {{- message[''content''] }}\n        {%- else %}\n            {%- for content in message[''content''] %}\n                {%- if content[''type''] == ''image'' %}\n                    {{- ''<|image|>'' }}\n                {%- elif content[''type''] == ''text'' %}\n                    {{- content[''text''] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>","pad_token":"<|finetune_right_pad_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.2', 'approved', 40, '65ead08a5db6df44a86ad79bed2c365a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-4-multimodal-instruct', 'huggingface--microsoft--phi-4-multimodal-instruct', 'Phi-4-multimodal-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/LICENSE language: - multilingual - ar - zh - cs - da - nl - en - fi - fr - de - he - hu - it - ja - ko - no - pl - pt - ru - es - sv - th - tr - uk tags: - nlp - code - audio - automatic-speech-recognition - speech-summarization - speech-translation - visual-question-answering - phi-4-multimodal - phi - phi-4-mini widget: - example_title: Librispeech sample 1 src: https://cdn-media.huggingfa...', '["transformers","safetensors","phi4mm","text-generation","nlp","code","audio","automatic-speech-recognition","speech-summarization","speech-translation","visual-question-answering","phi-4-multimodal","phi","phi-4-mini","custom_code","multilingual","ar","zh","cs","da","nl","en","fi","fr","de","he","hu","it","ja","ko","no","pl","pt","ru","es","sv","th","tr","uk","arxiv:2503.01743","arxiv:2407.13833","license:mit","region:us"]', 'automatic-speech-recognition', 1544, 389941, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-4-multimodal-instruct","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\n- ar\n- zh\n- cs\n- da\n- nl\n- en\n- fi\n- fr\n- de\n- he\n- hu\n- it\n- ja\n- ko\n- no\n- pl\n- pt\n- ru\n- es\n- sv\n- th\n- tr\n- uk\ntags:\n- nlp\n- code\n- audio\n- automatic-speech-recognition\n- speech-summarization\n- speech-translation\n- visual-question-answering\n- phi-4-multimodal\n- phi\n- phi-4-mini\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n- messages:\n  - role: user\n    content: Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\nlibrary_name: transformers\npaper: https://arxiv.org/abs/2503.01743\n---\n🎉**Phi-4**: [[mini-reasoning](https://huggingface.co/microsoft/Phi-4-mini-reasoning) | [reasoning](https://huggingface.co/microsoft/Phi-4-reasoning)] | [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n\n\n## Model Summary\n\nPhi-4-multimodal-instruct is a lightweight open multimodal foundation\nmodel that leverages the language, vision, and speech research\nand datasets used for Phi-3.5 and 4.0 models. The model processes text,\nimage, and audio inputs, generating text outputs, and comes with\n128K token context length. The model underwent an enhancement process,\nincorporating both supervised fine-tuning, direct preference\noptimization and RLHF (Reinforcement Learning from Human Feedback)\nto support precise instruction adherence and safety measures.\nThe languages that each modal supports are the following:\n- Text: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish,\nFrench, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian,\nPolish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\n- Vision: English\n- Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese\n\n📰 [Phi-4-multimodal Microsoft Blog](https://aka.ms/phi4-feb2025) <br>\n📖 [Phi-4-multimodal Technical Report](https://arxiv.org/abs/2503.01743) <br>\n🏡 [Phi Portal](https://aka.ms/phi-4-multimodal/azure) <br>\n👩‍🍳 [Phi Cookbook](https://github.com/microsoft/PhiCookBook) <br>\n🖥️ Try It on [Azure](https://aka.ms/phi-4-multimodal/azure), \n[GitHub](https://github.com/marketplace/models/azureml/Phi-4-multimodal-instruct/playground),\n[Nvidia](https://aka.ms/phi-4-multimodal/nvidia),\n[Huggingface](https://huggingface.co/spaces/microsoft/phi-4-multimodal) playgrounds<br>\n📱Huggingface Spaces \n[Thoughts Organizer](https://huggingface.co/spaces/microsoft/ThoughtsOrganizer), \n[Stories Come Alive](https://huggingface.co/spaces/microsoft/StoriesComeAlive), \n[Phine Speech Translator](https://huggingface.co/spaces/microsoft/PhineSpeechTranslator) <br>\n\n\nWatch as Phi-4 Multimodal analyzes spoken language to help plan a trip to Seattle, demonstrating its advanced audio processing and recommendation capabilities.\n\n<div style="width: 800px; height: 400px; margin: 0 auto;">\n  <video autoplay muted loop controls playsinline style="width: 100%; height: 100%; object-fit: contain;">\n    <source src="https://github.com/nguyenbh/phi4mm-demos/raw/refs/heads/main/clips/Phi-4-multimodal_SeattleTrip.mp4" type="video/mp4">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\nSee how Phi-4 Multimodal tackles complex mathematical problems through visual inputs, demonstrating its ability to process and solve equations presented in images.\n<div style="width: 800px; height: 400px; margin: 0 auto;">\n  <video autoplay muted loop controls playsinline style="width: 100%; height: 100%; object-fit: contain;">\n    <source src="https://github.com/nguyenbh/phi4mm-demos/raw/refs/heads/main/clips/Phi-4-multimodal_Math.mp4" type="video/mp4">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\nExplore how Phi-4 Mini functions as an intelligent agent, showcasing its reasoning and task execution abilities in complex scenarios.\n<div style="width: 800px; height: 400px; margin: 0 auto;">\n  <video autoplay muted loop controls playsinline style="width: 100%; height: 100%; object-fit: contain;">\n    <source src="https://github.com/nguyenbh/phi4mm-demos/raw/refs/heads/main/clips/Phi-4-mini_Agents.mp4" type="video/mp4">\n    Your browser does not support the video tag.\n  </video>\n</div>\n\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for broad multilingual and multimodal commercial and research use . The model provides uses for general purpose AI systems and applications which require\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially math and logic)\n4) Function and tool calling\n5) General image understanding\n6) Optical character recognition\n7) Chart and table understanding\n8) Multiple image comparison\n9) Multi-image or video clip summarization\n10) Speech recognition\n11) Speech translation\n12) Speech QA\n13) Speech summarization\n14) Audio understanding\n\nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nThe model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models and multimodal models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case. \n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Release Notes \n\nThis release of Phi-4-multimodal-instruct is based on valuable user feedback from the Phi-3 series. Previously, users could use a speech recognition model to talk to the Mini and Vision models. To achieve this, users needed to use a pipeline of two models: one model to transcribe the audio to text, and another model for the language or vision tasks. This pipeline means that the core model was not provided the full breadth of input information – e.g. cannot directly observe multiple speakers, background noises, jointly align speech, vision, language information at the same time on the same representation space.\nWith Phi-4-multimodal-instruct, a single new open model has been trained across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. The model  employed new architecture, larger vocabulary for efficiency, multilingual, and multimodal support, and better post-training techniques were used for instruction following and function calling, as well as additional data leading to substantial gains on key multimodal capabilities.\nIt is anticipated that Phi-4-multimodal-instruct will greatly benefit app developers and various use cases. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4 is welcomed and crucial to the model''s evolution and improvement. Thank you for being part of this journey!\n\n## Model Quality\n<details>\n  <summary>Click to view details</summary>\n\nTo understand the capabilities, Phi-4-multimodal-instruct  was compared with a set of models over a variety of benchmarks using an internal benchmark platform (See Appendix A for benchmark methodology). Users can refer to the Phi-4-Mini-Instruct model card for details of language benchmarks. At the high-level overview of the model quality on representative speech and vision benchmarks:\n\n### Speech\n\nThe Phi-4-multimodal-instruct was observed as\n- Having strong automatic speech recognition (ASR) and speech translation (ST) performance, surpassing expert ASR model WhisperV3 and ST models SeamlessM4T-v2-Large. \n- Ranking number 1 on the [Huggingface OpenASR](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) leaderboard with word error rate 6.14% in comparison with the current best model 6.5% as of March 04, 2025. \n- Being the first open-sourced model that can perform speech summarization, and the performance is close to GPT4o.\n- Having a gap with close models, e.g. Gemini-1.5-Flash and GPT-4o-realtime-preview, on speech QA task. Work is being undertaken to improve this capability in the next iterations.\n\n#### Speech Recognition (lower is better)\n\nThe performance of Phi-4-multimodal-instruct on the aggregated benchmark datasets:\n![alt text](./figures/speech_recognition.png)\n\nThe performance of Phi-4-multimodal-instruct on different languages, averaging the WERs of CommonVoice and FLEURS:\n\n![alt text](./figures/speech_recog_by_lang.png)\n\n#### Speech Translation (higher is better)\n\nTranslating from German, Spanish, French, Italian, Japanese, Portugues, Chinese to English:\n\n![alt text](./figures/speech_translate.png)\n\nTranslating from English to German, Spanish, French, Italian, Japanese, Portugues, Chinese. Noted that WhiperV3 does not support this capability: \n\n![alt text](./figures/speech_translate_2.png)\n\n\n#### Speech Summarization (higher is better)\n\n![alt text](./figures/speech_summarization.png)\n\n#### Speech QA\n\nMT bench scores are scaled by 10x to match the score range of MMMLU:\n\n![alt text](./figures/speech_qa.png)\n\n#### Audio Understanding\n\nAIR bench scores are scaled by 10x to match the score range of MMAU:\n\n![alt text](./figures/audio_understand.png)\n\n### Vision\n\n#### Vision-Speech tasks\n\nPhi-4-multimodal-instruct is capable of processing both image and audio together, the following table shows the model quality when the input query for vision content is synthetic speech on chart/table understanding and document reasoning tasks. Compared to other existing state-of-the-art omni models that can enable audio and visual signal as input, Phi-4-multimodal-instruct achieves much stronger performance on multiple benchmarks.\n\n| Benchmarks            | Phi-4-multimodal-instruct | InternOmni-7B | Gemini-2.0-Flash-Lite-prv-02-05 | Gemini-2.0-Flash | Gemini-1.5-Pro |\n|-----------------------|--------------------------|---------------|--------------------------------|-----------------|----------------|\n| s_AI2D                | **68.9**                 | 53.9          | 62.0                           | **69.4**        | 67.7           |\n| s_ChartQA             | **69.0**                 | 56.1          | 35.5                           | 51.3            | 46.9           |\n| s_DocVQA              | **87.3**                 | 79.9          | 76.0                           | 80.3            | 78.2           |\n| s_InfoVQA             | **63.7**                 | 60.3          | 59.4                           | 63.6            | **66.1**       |\n| **Average**           | **72.2**                 | **62.6**      | **58.2**                       | **66.2**        | **64.7**       |\n\n### Vision tasks\nTo understand the vision capabilities, Phi-4-multimodal-instruct was compared with a set of models over a variety of zero-shot benchmarks using an internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\n\n| Dataset                          | Phi-4-multimodal-ins | Phi-3.5-vision-ins | Qwen 2.5-VL-3B-ins | Intern VL 2.5-4B | Qwen 2.5-VL-7B-ins | Intern VL 2.5-8B | Gemini 2.0-Flash Lite-preview-0205 | Gemini2.0-Flash | Claude-3.5-Sonnet-2024-10-22 | Gpt-4o-2024-11-20 |\n|----------------------------------|---------------------|-------------------|-------------------|-----------------|-------------------|-----------------|--------------------------------|-----------------|----------------------------|------------------|\n| **Popular aggregated benchmark** |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| MMMU                             | **55.1**            | 43.0              | 47.0              | 48.3            | 51.8              | 50.6            | 54.1                           | **64.7**        | 55.8                       | 61.7             |\n| MMBench (dev-en)                 | **86.7**            | 81.9              | 84.3              | 86.8            | 87.8              | 88.2            | 85.0                           | **90.0**        | 86.7                       | 89.0             |\n| MMMU-Pro (std/vision)            | **38.5**            | 21.8              | 29.9              | 32.4            | 36.9              | 34.4            | 45.1                           | **54.4**        | 54.3                       | 53.0             |\n| **Visual science reasoning**     |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| ScienceQA Visual (img-test)      | **97.5**            | 91.3              | 79.4              | 96.2            | 87.7              | **97.3**        | 85.0                           | 88.3            | 81.2                       | 88.2             |\n| **Visual math reasoning**        |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| MathVista (testmini)             | **62.4**            | 43.9              | 60.8              | 51.2            | **67.8**          | 56.7            | 57.6                           | 47.2            | 56.9                       | 56.1             |\n| InterGPS                         | **48.6**            | 36.3              | 48.3              | 53.7            | 52.7              | 54.1            | 57.9                           | **65.4**        | 47.1                       | 49.1             |\n| **Chart & table reasoning**      |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| AI2D                             | **82.3**            | 78.1              | 78.4              | 80.0            | 82.6              | 83.0            | 77.6                           | 82.1            | 70.6                       | **83.8**         |\n| ChartQA                          | **81.4**            | 81.8              | 80.0              | 79.1            | **85.0**          | 81.0            | 73.0                           | 79.0            | 78.4                       | 75.1             |\n| DocVQA                           | **93.2**            | 69.3              | 93.9              | 91.6            | **95.7**          | 93.0            | 91.2                           | 92.1            | 95.2                       | 90.9             |\n| InfoVQA                          | **72.7**            | 36.6              | 77.1              | 72.1            | **82.6**          | 77.6            | 73.0                           | 77.8            | 74.3                       | 71.9             |\n| **Document Intelligence**        |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| TextVQA (val)                    | **75.6**            | 72.0              | 76.8              | 70.9            | **77.7**          | 74.8            | 72.9                           | 74.4            | 58.6                       | 73.1             |\n| OCR Bench                        | **84.4**            | 63.8              | 82.2              | 71.6            | **87.7**          | 74.8            | 75.7                           | 81.0            | 77.0                       | 77.7             |\n| **Object visual presence verification** |              |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| POPE                             | **85.6**            | 86.1              | 87.9              | 89.4            | 87.5              | **89.1**        | 87.5                           | 88.0            | 82.6                       | 86.5             |\n| **Multi-image perception**       |                     |                   |                   |                 |                   |                 |                                |                 |                            |                  |\n| BLINK                            | **61.3**            | 57.0              | 48.1              | 51.2            | 55.3              | 52.5            | 59.3                           | **64.0**        | 56.9                       | 62.4             |\n| Video MME 16 frames              | **55.0**            | 50.8              | 56.5              | 57.3            | 58.2              | 58.7            | 58.8                           | 65.5            | 60.2                       | **68.2**         |\n| **Average**                      | **72.0**            | **60.9**          | **68.7**          | **68.8**        | **73.1**          | **71.1**        | **70.2**                       | **74.3**        | **69.1**                   | **72.4**         |\n\n![alt text](./figures/vision_radar.png)\n\n#### Visual Perception\n\nBelow are the comparison results on existing multi-image tasks. On average, Phi-4-multimodal-instruct outperforms competitor models of the same size and competitive with much bigger models on multi-frame capabilities.\nBLINK is an aggregated benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs.\n\n| Dataset                    | Phi-4-multimodal-instruct | Qwen2.5-VL-3B-Instruct | InternVL 2.5-4B | Qwen2.5-VL-7B-Instruct | InternVL 2.5-8B | Gemini-2.0-Flash-Lite-prv-02-05 | Gemini-2.0-Flash | Claude-3.5-Sonnet-2024-10-22 | Gpt-4o-2024-11-20 |\n|----------------------------|--------------------------|----------------------|-----------------|----------------------|-----------------|--------------------------------|-----------------|----------------------------|------------------|\n| Art Style                  | **86.3**                 | 58.1                | 59.8           | 65.0                 | 65.0            | 76.9                           | 76.9            | 68.4                       | 73.5             |\n| Counting                   | **60.0**                 | 67.5                | 60.0           | 66.7                 | **71.7**        | 45.8                           | 69.2            | 60.8                       | 65.0             |\n| Forensic Detection         | **90.2**                 | 34.8                | 22.0           | 43.9                 | 37.9            | 31.8                           | 74.2            | 63.6                       | 71.2             |\n| Functional Correspondence  | **30.0**                 | 20.0                | 26.9           | 22.3                 | 27.7            | 48.5                           | **53.1**        | 34.6                       | 42.3             |\n| IQ Test                    | **22.7**                 | 25.3                | 28.7           | 28.7                 | 28.7            | 28.0                           | **30.7**        | 20.7                       | 25.3             |\n| Jigsaw                     | **68.7**                 | 52.0                | **71.3**       | 69.3                 | 53.3            | 62.7                           | 69.3            | 61.3                       | 68.7             |\n| Multi-View Reasoning       | **76.7**                 | 44.4                | 44.4           | 54.1                 | 45.1            | 55.6                           | 41.4            | 54.9                       | 54.1             |\n| Object Localization        | **52.5**                 | 55.7                | 53.3           | 55.7                 | 58.2            | 63.9                           | **67.2**        | 58.2                       | 65.6             |\n| Relative Depth             | **69.4**                 | 68.5                | 68.5           | 80.6                 | 76.6            | **81.5**                       | 72.6            | 66.1                       | 73.4             |\n| Relative Reflectance       | **26.9**                 | **38.8**            | **38.8**       | 32.8                 | **38.8**        | 33.6                           | 34.3            | 38.1                       | 38.1             |\n| Semantic Correspondence    | **52.5**                 | 32.4                | 33.8           | 28.8                 | 24.5            | **56.1**                       | 55.4            | 43.9                       | 47.5             |\n| Spatial Relation           | **72.7**                 | 80.4                | 86.0           | **88.8**             | 86.7            | 74.1                           | 79.0            | 74.8                       | 83.2             |\n| Visual Correspondence      | **67.4**                 | 28.5                | 39.5           | 50.0                 | 44.2            | 84.9                           | **91.3**        | 72.7                       | 82.6             |\n| Visual Similarity          | **86.7**                 | 67.4                | 88.1           | 87.4                 | 85.2            | **87.4**                       | 80.7            | 79.3                       | 83.0             |\n| **Overall**                | **61.6**                 | **48.1**            | **51.2**       | **55.3**             | **52.5**        | **59.3**                       | **64.0**        | **56.9**                   | **62.4**         |\n\n![alt text](./figures/multi_image.png)\n\n</details>\n\n## Usage\n\n### Requirements\n\nPhi-4 family has been integrated in the `4.48.2` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\nWe suggest to run with Python 3.10.\nExamples of required packages:\n```\nflash_attn==2.7.4.post1\ntorch==2.6.0\ntransformers==4.48.2\naccelerate==1.3.0\nsoundfile==0.13.1\npillow==11.1.0\nscipy==1.15.2\ntorchvision==0.21.0\nbackoff==2.2.1\npeft==0.13.2\n```\n\nPhi-4-multimodal-instruct is also available in [Azure AI Studio](https://aka.ms/phi-4-multimodal/azure)\n\n### Tokenizer\n\nPhi-4-multimodal-instruct supports a vocabulary size of up to `200064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n\n### Input Formats\n\nGiven the nature of the training data, the Phi-4-multimodal-instruct model is best suited for prompts using the chat format as follows:\n\n#### Text chat format\n\nThis format is used for general conversation and instructions:\n\n`\n<|system|>You are a helpful assistant.<|end|><|user|>How to explain Internet for a medieval knight?<|end|><|assistant|>\n`\n\n#### Tool-enabled function-calling format\n\nThis format is used when the user wants the model to provide function calls based on\nthe given tools. The user should provide the available tools in the system prompt,\nwrapped by <|tool|> and <|/tool|> tokens. The tools should be specified in JSON format,\nusing a JSON dump structure. Example:\n\n`\n<|system|>You are a helpful assistant with some tools.<|tool|>[{"name": "get_weather_updates", "description": "Fetches weather updates for a given city using the RapidAPI Weather API.", "parameters": {"city": {"description": "The name of the city for which to retrieve weather information.", "type": "str", "default": "London"}}}]<|/tool|><|end|><|user|>What is the weather like in Paris today?<|end|><|assistant|>\n`\n\n#### Vision-Language Format\n\nThis format is used for conversation with image:\n\n`\n<|user|><|image_1|>Describe the image in detail.<|end|><|assistant|>\n`\n\nFor multiple images, the user needs to insert multiple image placeholders in the prompt as below:\n\n`\n<|user|><|image_1|><|image_2|><|image_3|>Summarize the content of the images.<|end|><|assistant|>\n`\n\n#### Speech-Language Format\n\nThis format is used for various speech and audio tasks:\n\n`\n<|user|><|audio_1|>{task prompt}<|end|><|assistant|>\n`\n\nThe task prompt can vary for different task.\nAutomatic Speech Recognition:\n\n`\n<|user|><|audio_1|>Transcribe the audio clip into text.<|end|><|assistant|>\n`\n\nAutomatic Speech Translation:\n\n`\n<|user|><|audio_1|>Translate the audio to {lang}.<|end|><|assistant|>\n`\n\nAutomatic Speech Translation with chain-of-thoughts:\n\n`\n<|user|><|audio_1|>Transcribe the audio to text, and then translate the audio to {lang}. Use <sep> as a separator between the original transcript and the translation.<|end|><|assistant|>\n`\n\nSpoken-query Question Answering:\n\n`\n<|user|><|audio_1|><|end|><|assistant|>\n`\n\n#### Vision-Speech Format\n\nThis format is used for conversation with image and audio.\nThe audio may contain query related to the image:\n\n`\n<|user|><|image_1|><|audio_1|><|end|><|assistant|>\n`\n\nFor multiple images, the user needs to insert multiple image placeholders in the prompt as below:\n\n`\n<|user|><|image_1|><|image_2|><|image_3|><|audio_1|><|end|><|assistant|>\n`\n\n**Vision**\n- Any common RGB/gray image format (e.g., (".jpg", ".jpeg", ".png", ".ppm", ".bmp", ".pgm", ".tif", ".tiff", ".webp")) can be supported.\n- Resolution depends on the GPU memory size. Higher resolution and more images will produce more tokens, thus using more GPU memory. During training, 64 crops can be supported.\nIf it is a square image, the resolution would be around (8*448 by 8*448). For multiple-images, at most 64 frames can be supported, but with more frames as input, the resolution of each frame needs to be reduced to fit in the memory.\n\n**Audio**\n- Any audio format that can be loaded by soundfile package should be supported.\n- To keep the satisfactory performance, maximum audio length is suggested to be 40s. For summarization tasks, the maximum audio length is suggested to 30 mins.\n\n\n### Loading the model locally\n\nAfter obtaining the Phi-4-multimodal-instruct model checkpoints, users can use this sample code for inference.\n\n<details>\n  <summary>Click to view details</summary>\n\n```python\nimport requests\nimport torch\nimport os\nimport io\nfrom PIL import Image\nimport soundfile as sf\nfrom transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\nfrom urllib.request import urlopen\n\n\n# Define model path\nmodel_path = "microsoft/Phi-4-multimodal-instruct"\n\n# Load model and processor\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    device_map="cuda", \n    torch_dtype="auto", \n    trust_remote_code=True,\n    # if you do not use Ampere or later GPUs, change attention to "eager"\n    _attn_implementation=''flash_attention_2'',\n).cuda()\n\n# Load generation config\ngeneration_config = GenerationConfig.from_pretrained(model_path)\n\n# Define prompt structure\nuser_prompt = ''<|user|>''\nassistant_prompt = ''<|assistant|>''\nprompt_suffix = ''<|end|>''\n\n# Part 1: Image Processing\nprint("\n--- IMAGE PROCESSING ---")\nimage_url = ''https://www.ilankelman.org/stopsigns/australia.jpg''\nprompt = f''{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}''\nprint(f''>>> Prompt\n{prompt}'')\n\n# Download and open image\nimage = Image.open(requests.get(image_url, stream=True).raw)\ninputs = processor(text=prompt, images=image, return_tensors=''pt'').to(''cuda:0'')\n\n# Generate response\ngenerate_ids = model.generate(\n    **inputs,\n    max_new_tokens=1000,\n    generation_config=generation_config,\n)\ngenerate_ids = generate_ids[:, inputs[''input_ids''].shape[1]:]\nresponse = processor.batch_decode(\n    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(f''>>> Response\n{response}'')\n\n# Part 2: Audio Processing\nprint("\n--- AUDIO PROCESSING ---")\naudio_url = "https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac"\nspeech_prompt = "Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation."\nprompt = f''{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}''\nprint(f''>>> Prompt\n{prompt}'')\n\n# Downlowd and open audio file\naudio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n\n# Process with the model\ninputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors=''pt'').to(''cuda:0'')\n\ngenerate_ids = model.generate(\n    **inputs,\n    max_new_tokens=1000,\n    generation_config=generation_config,\n)\ngenerate_ids = generate_ids[:, inputs[''input_ids''].shape[1]:]\nresponse = processor.batch_decode(\n    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(f''>>> Response\n{response}'')\n```\n</details>\n\nMore inference examples can be found [**here**](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/sample_inference_phi4mm.py).\n\n### vLLM inference\n\nUser can start a server with this command\n\n```bash\npython -m vllm.entrypoints.openai.api_server --model ''microsoft/Phi-4-multimodal-instruct'' --dtype auto --trust-remote-code --max-model-len 131072 --enable-lora --max-lora-rank 320 --lora-extra-vocab-size 0 --limit-mm-per-prompt audio=3,image=3 --max-loras 2 --lora-modules speech=<path to speech lora folder> vision=<path to vision lora folder>\n```\n\nThe speech lora and vision lora folders are within the Phi-4-multimodal-instruct folder downloaded by vLLM, you can also use the following script to find thoses:\n\n```python\nfrom huggingface_hub import snapshot_download\nmodel_path = snapshot_download(repo_id="microsoft/Phi-4-multimodal-instruct")\nspeech_lora_path = model_path+"/speech-lora"\nvision_lora_path = model_path+"/vision-lora"\n```\n\n## Training\n\n### Fine-tuning\n\nA basic example of supervised fine-tuning (SFT) for [**speech**](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/sample_finetune_speech.py) and [**vision**](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/sample_finetune_vision.py) is provided respectively.\n\nAn example on [**how to extend speech recognition to a new language**.](https://huggingface.co/microsoft/Phi-4-multimodal-instruct#appendix-b-fine-tuning-korean-speech)\n\n### Model\n\n+ **Architecture:** Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model. The model has the pretrained Phi-4-Mini-Instruct as the backbone language model, and the advanced encoders and adapters of vision and speech.<br>\n+ **Inputs:** Text, image, and audio. It is best suited for prompts using the chat format.<br>\n+ **Context length:** 128K tokens<br>\n+ **GPUs:** 512 A100-80G<br>\n+ **Training time:** 28 days<br>\n+ **Training data:** 5T tokens, 2.3M speech hours, and 1.1T image-text tokens<br>\n+ **Outputs:** Generated text in response to the input<br>\n+ **Dates:** Trained between December 2024 and January 2025<br>\n+ **Status:** This is a static model trained on offline datasets with the cutoff date of June 2024 for publicly available data.<br>\n+ **Supported languages:** \n  + Text: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n  + Vision: English<br>\n  + Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese<br>\n+ **Release date:** February 2025<br>\n\n### Training Datasets\n\nPhi-4-multimodal-instruct''s training data includes a wide variety of sources, totaling 5 trillion text tokens, and is a combination of \n1) publicly available documents filtered for quality, selected high-quality educational data, and code\n2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.)\n3) high quality human labeled data in chat format\n4) selected high-quality image-text interleave data\n5) synthetic and publicly available image, multi-image, and video data\n6) anonymized in-house speech-text pair data with strong/weak transcriptions\n7) selected high-quality publicly available and anonymized in-house speech data with task-specific supervisions\n8) selected synthetic speech data\n9) synthetic vision-speech data.\n\nFocus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for large foundation models, but such information was removed for the Phi-4-multimodal-instruct to leave more model capacity for reasoning for the model''s small size. The data collection process involved sourcing information from publicly available documents, with a focus on filtering out undesirable documents and images. To safeguard privacy, image and text data sources were filtered to remove or scrub potentially personal data from the training data.\nThe decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis. \n\n### Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n* [Accelerate](https://huggingface.co/docs/transformers/main/en/accelerate)\n* [soundfile](https://github.com/bastibe/python-soundfile)\n* [pillow](https://github.com/python-pillow/Pillow)\n\n### Hardware\nNote that by default, the Phi-4-multimodal-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with _attn_implementation="eager"\n\n\n## Responsible AI Considerations\n<details>\n  <summary>Click to view detail descriptions</summary>\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n+ Quality of Service: The Phi models are trained primarily on English language content across text, speech, and visual inputs, with some additional multilingual coverage. Performance may vary significantly across different modalities and languages:\n  + Text: Languages other than English will experience reduced performance, with varying levels of degradation across different non-English languages. English language varieties with less representation in the training data may perform worse than standard American English.\n  + Speech: Speech recognition and processing shows similar language-based performance patterns, with optimal performance for standard American English accents and pronunciations. Other English accents, dialects, and non-English languages may experience lower recognition accuracy and response quality. Background noise, audio quality, and speaking speed can further impact performance.\n  + Vision: Visual processing capabilities may be influenced by cultural and geographical biases in the training data. The model may show reduced performance when analyzing images containing text in non-English languages or visual elements more commonly found in non-Western contexts. Image quality, lighting conditions, and composition can also affect processing accuracy.\n+ Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n+ Limited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, it is strongly recommended that users manually verify all API uses.\n+ Long Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift.\n+ Inference of Sensitive Attributes: The Phi 4 models can sometimes attempt to infer sensitive attributes (such as personality characteristics, country of origin, gender, etc...) from the users’ voices when specifically asked to do so. Phi 4-multimodal-instruct is not designed or intended to be used as a biometric categorization system to categorize individuals based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation. This behavior can be easily and efficiently mitigated at the application level by a system message.\n  \nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n</details>\n\n## Safety\n<details>\n  <summary>Click to view detail descriptions</summary>\n\nThe Phi-4 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed for safety alignment is a combination of SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization), and RLHF (Reinforcement Learning from Human Feedback) approaches by utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness, as well as various questions and answers targeted to multiple safety categories. For non-English languages, existing datasets were extended via machine translation. Speech Safety datasets were generated by running Text Safety datasets through Azure TTS (Text-To-Speech) Service, for both English and non-English languages. Vision (text & images) Safety datasets were created to cover harm categories identified both in public and internal multi-modal RAI datasets.\n\n### Safety Evaluation and Red-Teaming\n\nVarious evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 models'' propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the [Phi 3 Safety Post-Training paper](https://arxiv.org/abs/2407.13833) had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the [Phi 3 Safety Post-Training paper](https://arxiv.org/abs/2407.13833). For this release, the red teaming effort focused on the newest Audio input modality and on the following safety areas: harmful content, self-injury risks, and exploits. The model was found to be more susceptible to providing undesirable outputs when attacked with context manipulation or persuasive techniques. These findings applied to all languages, with the persuasive techniques mostly affecting French and Italian. This highlights the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken.\n\n### Vision Safety Evaluation\n\nTo assess model safety in scenarios involving both text and images, Microsoft''s Azure AI Evaluation SDK was utilized. This tool facilitates the simulation of single-turn conversations with the target model by providing prompt text and images designed to incite harmful responses. The target model''s responses are subsequently evaluated by a capable model across multiple harm categories, including violence, sexual content, self-harm, hateful and unfair content, with each response scored based on the severity of the harm identified. The evaluation results were compared with those of Phi-3.5-Vision and open-source models of comparable size. In addition, we ran both an internal and the public RTVLM and VLGuard multi-modal (text & vision) RAI benchmarks, once again comparing scores with Phi-3.5-Vision and open-source models of comparable size. However, the model may be susceptible to language-specific attack prompts and cultural context.\n\n### Audio Safety Evaluation\n\nIn addition to extensive red teaming, the Safety of the model was assessed through three distinct evaluations. First, as performed with Text and Vision inputs, Microsoft''s Azure AI Evaluation SDK was leveraged to detect the presence of harmful content in the model''s responses to Speech prompts. Second, [Microsoft''s Speech Fairness evaluation](https://speech.microsoft.com/portal/responsibleai/assess) was run to verify that Speech-To-Text transcription worked well across a variety of demographics. Third, we proposed and evaluated a mitigation approach via a system message to help prevent the model from inferring sensitive attributes (such as gender, sexual orientation, profession, medical condition, etc...) from the voice of a user.\n</details>\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft''s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party''s policies.\n\n\n## Appendix A: Benchmark Methodology\n\n<details>\n  <summary>Click to view detail descriptions</summary>\n\nWe include a brief word on methodology here - and in particular, how we think about optimizing prompts.\nIn an ideal world, we would never change any prompts in our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date.\nThere are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given eval due to a failure to respect the output format. For example:\n\n+ A model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try different system messages (e.g. “You must always respond to a question” or “Get to the point!”).\n+ Some models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases.\n+ We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts.\n\nHowever, we do not:\n\n+ Pick different few-shot examples. Few shots will always be the same when comparing different models.\n+ Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice.\n\n### Vision Benchmark Settings\n\nThe goal of the benchmark setup is to measure the performance of the LMM when a regular user utilizes these models for a task involving visual input. To this end, we selected 9 popular and publicly available single-frame datasets and 3 multi-frame benchmarks that cover a wide range of challenging topics and tasks (e.g., mathematics, OCR tasks, charts-and-plots understanding, etc.) as well as a set of high-quality models. \nOur benchmarking setup utilizes zero-shot prompts and all the prompt content are the same for every model. We only formatted the prompt content to satisfy the model''s prompt API. This ensures that our evaluation is fair across the set of models we tested. Many benchmarks necessitate models to choose their responses from a presented list of options. Therefore, we''ve included a directive in the prompt''s conclusion, guiding all models to pick the option letter that corresponds to the answer they deem correct.\nIn terms of the visual input, we use the images from the benchmarks as they come from the original datasets. We converted these images to base-64 using a JPEG encoding for models that require this format (e.g., GPTV, Claude Sonnet 3.5, Gemini 1.5 Pro/Flash). For other models (e.g., Llava Interleave, and InternVL2 4B and 8B), we used their Huggingface interface and passed in PIL images or a JPEG image stored locally. We did not scale or pre-process images in any other way.\nLastly, we used the same code to extract answers and evaluate them using the same code for every considered model. This ensures that we are fair in assessing the quality of their answers.\n\n### Speech Benchmark Settings\n\nThe objective of this benchmarking setup is to assess the performance of models in speech and audio understanding tasks as utilized by regular users. To accomplish this, we selected several state-of-the-art open-sourced and closed-sourced models and performed evaluations across a variety of public and in-house benchmarks. These benchmarks encompass diverse and challenging topics, including Automatic Speech Recognition (ASR), Automatic Speech Translation (AST), Spoken Query Question Answering (SQQA), Audio Understanding (AU), and Speech Summarization.\nThe results are derived from evaluations conducted on identical test data without any further clarifications. All results were obtained without sampling during inference. For an accurate comparison, we employed consistent prompts for models across different tasks, except for certain model APIs (e.g., GPT-4o), which may refuse to respond to specific prompts for some tasks.\nIn conclusion, we used uniform code to extract answers and evaluate them for all considered models. This approach ensured fairness by assessing the quality of their responses.\n\n### Benchmark datasets\n\nThe model was evaluated across a breadth of public and internal benchmarks to understand it''s capabilities under multiple tasks and conditions. While most evaluations use English, multilingual benchmark was incorporated to cover performance in select languages.  More specifically,\n+ Vision: \n  + Popular aggregated benchmark:\n    + MMMU and MMMU-Pro: massive multi-discipline tasks at college-level subject knowledge and deliberate reasoning.\n	+ MMBench: large-scale benchmark to evaluate perception and reasoning capabilities.\n  +	Visual reasoning:\n    + ScienceQA: multimodal visual question answering on science.\n	+ MathVista: visual math reasoning.\n	+ InterGPS: Visual 2D geometry reasoning.\n  +	Chart reasoning:\n	+ ChartQA: visual and logical reasoning on charts.\n	+ AI2D: diagram understanding.\n  +	Document Intelligence:\n	+ TextVQA: read and reason about text in images to answer questions about them.\n	+ InfoVQA: read and reason about high-resolution infographics images with arbitrary aspect ratios.\n	+ DocVQA: read and reason about document images with dense texts and handwritten texts.\n	+ OCRBench: test OCR and QA capability on diverse text related images.\n  +	Vision speech multimodal understanding:\n	+ s_AI2D: diagram understanding with speech as the question format.\n	+ s_ChartQA: visual and logical reasoning on charts with speech as the question format.\n	+ s_InfoVQA: read and reason about high-resolution infographics images with speech as the question format.\n	+ s_DocVQA: read and reason about document images with dense texts and handwritten texts with speech as the question format.\n  + RAI & Security Benchmarks:\n	+ VLGuardExt: VLGuard is a vision-language instruction following public dataset for model safety to address safety on deception\n    discrimination, privacy and risky behavior (advice, sexual, violence, political). This was extended to a few internal categories such as child safety and election critical information.\n	+ RTVLM: Public benchmark for red-teaming vision-language model on model truthfulness, privacy, safety, and fairness.\n	+ GPTV-RAI: In-house benchmark for GPT-4V released from Azure AI, measuring harmfulness (ex. sexual, violent, hate and self-harm), privacy, jailbreak, misinformation.\n\n+ Speech: \n  + CommonVoice v15 is an open-source, multilingual speech dataset developed by Mozilla. It includes over 33,000 hours of speech data in 133 languages, contributed and validated by volunteers worldwide.The evaluations were conducted in the eight supported languages.\n  + The OpenASR Leaderboard on Hugging Face is designed for benchmarking and evaluating the robustness of ASR models on English. The datasets in the leaderboard cover diverse speech domains including reading speech, conversations, meetings, and so on.\n  + CoVoST2 is a multilingual speech-to-text translation dataset derived from Mozilla''s Common Voice project. It is one of the largest open datasets available for speech translation, providing support for both X-to-English (X→En) and English-to-X (En→X) translation tasks. The directions with supported languages were evaluated on the test sets.\n  + FLEURS is a multilingual speech dataset designed for evaluating speech recognition and speech-to-text translation models across a wide range of languages. The test sets for speech recognition and translation tasks were evaluated with the eight supported languages.\n  + MT Bench (Multi-turn Benchmark) is specifically designed to evaluate the conversational and instruction-following abilities of AI models in multi-turn question-answering (QA) scenarios. To support spoken questions, the text is synthesized into speech.\n  + MMMLU (Multilingual Massive Multitask Language Understanding) is an extensive benchmark designed to evaluate the general knowledge and reasoning capabilities of AI models across a wide array of subjects. To support spoken questions, the text is synthesized into its speech counterpart.  The model was evaluated on the eight supported languages for this test set. \n  + AIR-Bench Chat (Audio Instruction and Response Benchmark) is a comprehensive evaluation framework designed to test the capabilities of large audio language models (LALMs). It includes both foundation and chat benchmarks. The chat benchmark is selected for its open-ended question answering for audio capability.\n  + MMAU (Massive Multi-Task Audio Understanding) is a comprehensive dataset designed to evaluate the capabilities of multi-modal models in audio-based understanding and reasoning tasks. The test sets are in the form of multiple-choices QA, covering the categories of music, sound, and speech.\n  + Golden3 is a real-world meeting dataset, containing 108 meeting recordings with corresponding transcripts, averaging 6 minutes each. It is recorded across 30 conference rooms, featuring 4-8 attendees. The dataset is primarily in English, covering a wide range of topics. GPT4 is employed to generate summarization instructions that ask to summarize partial or the entire conversation or control the output style/length/structure.\n  + AMI (Augmented Multi-Party Interaction) is a comprehensive collection of meeting recordings, encompassing approximately 100 hours of data. The test split contains 20 meeting recordings with an average duration of 32 minutes. The model was tested on the close-talking version of audio. GPT4 is employed to generate summarization instructions that ask to summarize partial or the entire conversation or control the output style/length/structure.\n\n+ Safety and RAI:\n  + Single-turn trustworthiness evaluation:\n    + DecodingTrust: DecodingTrust is a collection of trustworthiness benchmarks in eight different perspectives\n    + XSTest: XSTest is an exaggerated safety evaluation\n    + Toxigen: Toxigen is adversarial and hate speech detection\n  + Red Team:\n    + Responses to prompts provided by AI Red Team at Microsoft\n</details>\n\n\n## Appendix B: Fine-tuning Korean speech\n\n<details>\n  <summary>Click to view detail descriptions</summary>\n\n### Overview and Datasets\n\nPhi-4-multimodal is originally not designed for Korean speech-to-text task, but it can be fine-tuned for Korean speech-to-text task using your own data or public Korean speech datasets.\n\nWe have fine-tuned Phi-4-multimodal model for Korean speech-to-text task using the following datasets:\n\n- kresnik/zeroth_korean\n- mozilla-foundation/common_voice_17_0 (Used Korean speech only)\n- PolyAI/minds14 (Used Korean speech only)\n- Custom dataset. The speech was a mix of fast and slow speech (Technical blog contents and presentations that the author have posted), with some modulation using [audiomentations](https://github.com/iver56/audiomentations) and [this script](https://github.com/daekeun-ml/azure-genai-utils/blob/main/azure_genai_utils/stt/augment.py)\n\nTotal 35K samples. Each sample is a pair of Korean speech and its transcription. Dataset was sampled 16kHz.\n\nYou can download the fine-tuned model [here](https://huggingface.co/daekeun-ml/Phi-4-multimodal-finetune-ko-speech). Please refer to the Jupyter notebook and video clips in the [demo folder](https://huggingface.co/daekeun-ml/Phi-4-multimodal-finetune-ko-speech/tree/main/demos). They are not production-quality as they were simply fine-tuned for PoC purposes, but you can see that they transcribe and translate with high accuracy even when a native speaker speaks quite quickly.\n\n### Requirements\nBased on Python 3.10, the following packages are required, and A100/H100 GPU is recommended.\n```\ntorch==2.6.0\ntransformers==4.48.2\naccelerate==1.4.0\nsoundfile==0.13.1\npillow==11.1.0\nscipy==1.15.2\ntorchvision==0.21.0\nbackoff==2.2.1\npeft==0.14.0\ndatasets==3.3.2\npandas==2.2.3\nflash_attn==2.7.4.post1\nevaluate==0.4.3\nsacrebleu==2.5.1  \n```\n\n### Training\nThe model was trained on a single A100 80GB GPU for 4 epochs with a batch size of 16 using the `sample_finetune_speech.py` script from [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)\n\nThe fine tuning script and command line are basically the same as [here](https://gist.github.com/seastar105/d1d8983b27611370528e3b194dcc5577#file-main-py), but you need to prepare your own dataset. Also, to perform audio encoder unfreeze, please refer to the code snippet below. The code snippet is retrieved from [the fine-tuning Colab notebook](https://colab.research.google.com/drive/1JAQdpX3BtIgDmTLlnHgstKfGw7HjSfej?usp=sharing).\n\n```python\nwith accelerator.local_main_process_first():\n    processor = AutoProcessor.from_pretrained(\n        "microsoft/Phi-4-multimodal-instruct",\n        trust_remote_code=True,\n    )\n    model = create_model(\n        args.model_name_or_path,\n        use_flash_attention=args.use_flash_attention,\n    )\n\ndef unfreeze_speech_components(model):\n    """Directly target verified components from your debug logs"""\n    # 1. Audio Embed Module (confirmed exists)\n    audio_embed = model.model.embed_tokens_extend.audio_embed\n\n    # 2. Entire Audio Encoder (simplified)\n    audio_encoder = audio_embed.encoder  # Direct access\n\n    # 3. Audio Projection (from debug logs)\n    audio_projection = audio_embed.audio_projection\n\n    # Unfreeze ONLY these 3 components\n    for component in [audio_embed, audio_encoder, audio_projection]:\n        for param in component.parameters():\n            param.requires_grad = True\n    return model\n\nmodel = unfreeze_speech_components(model)\n\n# Verify unfrozen parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f"Trainable parameters: {trainable_params:,}")\n\n# After unfreezing\nencoder_params = list(model.model.embed_tokens_extend.audio_embed.encoder.parameters())\nproj_params = list(model.model.embed_tokens_extend.audio_embed.audio_projection.parameters())\n\nassert any(p.requires_grad for p in encoder_params), "Encoder params frozen!"\nassert any(p.requires_grad for p in proj_params), "Projection params frozen!"\nprint("Components properly unfrozen ✅")    \n```\n\nExample commands to run finetuning scripts are as follows:\n```bash\npython main.py\n```\n\nThe latest version of the model currently uploaded was fine-tuned by **unfreezing the audio encoder**, and the ASR performance was significantly improved compared to the baseline LoRA adapter-based fine-tuning. \nComparing the full fine-tuning and LoRA fine-tuning, the CER on zeroth-test set is **1.61%** and 2.72%, and the WER on zeroth-test set is **3.54%** and 7.19%, respectively. Please refer to the [Experimental Settings and Results](#experimental-settings-and-results) for more details.\n\n### Experimental Settings and Results\nThe purpose of this benchmarking setup is to evaluate the basic performance of Korean audio in speech and audio understanding tasks. We did this for automatic speech recognition and automatic speech translation, and the test data used the following datasets and samples:\n\nEvaluation was done on the following datasets:\n+ ASR (Automatic Speech Recognition): Evaluated with CER (Character Error Rate) and WER (Word Error Rate) on [zeroth-test set (457 samples)](https://huggingface.co/datasets/kresnik/zeroth_korean).\n+ AST (Automatic Speech Translation): Evaluated with BLEU score on [fleurs ko <-> en speech translation test set (270 samples)](https://huggingface.co/datasets/seastar105/fleurs_ko_en_test).\n\nEvaluation Script is retrieved from [here](https://gist.github.com/seastar105/d1d8983b27611370528e3b194dcc5577#file-evaluate-py)\n\nWe used the [Phi-4-mm-inst-zeroth-kor](https://huggingface.co/seastar105/Phi-4-mm-inst-zeroth-kor) as a baseline to improve performance, as it showed significant performance improvement with 1 epoch. Note that the baseline was trained with [22K Zeroth Korean Korean speech data](https://huggingface.co/datasets/kresnik/zeroth_korean) for 1 epoch. Based on this baseline with 35K training samples, we conducted additional experiments with the following scenarios:\n\n+ [Case 1] LoRA finetune (1 epoch): LoRA adapter-based fine-tuning for 1 epochs\n+ [Case 2] LoRA finetune (4 epochs): LoRA adapter-based fine-tuning for 4 epochs\n+ [Case 3] Unfreeze audio encoder finetune (4 epochs): Full fine-tuning for 4 epochs. \n\nThe results of the experiments are as follows:\n+ CER and WER for zeroth-test set (Lower is better)\n  + Case 1''s CER and WER are 3.80% and 11.52%, respectively, which are better than the baseline (7.02% and 17.31%).\n  + Case 2''s CER and WER are 2.72% and 7.19%, respectively, which are better than Case 1.\n  + Case 3''s CER and WER are 1.61% and 3.54%, respectively, which are the best among the cases.\n\n+ BLEU score for fleurs ko <-> en speech translation test set (Higher is better)\n  + Case 1''s result is not improved compared to the baseline. Especially, the BLEU score for fleurs-ko2en-cot is decreased compared to the baseline.\n  + Case 2''s result is slightly improved compared to Case 1, which is the best among the cases.\n  + Case 3''s result is not improved compared to the baseline and Case 2.\n  \n| Model                          | zeroth (CER) | zeroth (WER) | fleurs-ko2en | fleurs-ko2en-cot | fleurs-en2ko | fleurs-en2ko-cot |\n|--------------------------------|-------------|-------------|--------------|------------------|--------------|------------------|\n| original                       | 99.16       | 99.63       | 5.63         | 2.42             | 6.86         | 4.17             |\n| Ours - speech full finetune (4 epochs) | 1.61        | 3.54        | 7.67         | 8.38             | 12.31        | 9.69             |\n| LoRA finetune (4 epochs)        | 2.72        | 7.19        | 7.11         | 9.95             | 13.22        | 10.45            |\n| LoRA finetune (1 epoch)         | 3.80        | 11.52       | 7.03         | 7.04             | 12.50        | 9.54             |\n| Phi-4-mm-inst-zeroth-kor        | 7.02        | 17.31       | 7.07         | 9.19             | 13.08        | 9.35             |\n\n## Cautions\n\nNote that this model is just a PoC/experimental purpose, and not intended to be used in production. More high-quality data, tuning, ablation studies, and experiments are needed.\n\nPhi-4-multimodal model is strong in multimodal tasks, especially in speech-to-text and high potential in Korean language tasks. Thus if you are interested in Korean speech-to-text task, this model can be a good starting point.\n\n## References\n\n- https://huggingface.co/microsoft/Phi-4-multimodal-instruct\n- https://huggingface.co/seastar105/Phi-4-mm-inst-zeroth-kor\n\n</details>', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":5574460384,"storage_bytes":24024633205,"files_count":54,"spaces_count":36,"gated":false,"private":false,"config":{"architectures":["Phi4MMForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi4mm.Phi4MMConfig","AutoModelForCausalLM":"modeling_phi4mm.Phi4MMForCausalLM","AutoTokenizer":"Xenova/gpt-4o"},"model_type":"phi4mm","tokenizer_config":{"bos_token":"<|endoftext|>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' and ''tools'' in message and message[''tools''] is not none %}{{ ''<|'' + message[''role''] + ''|>'' + message[''content''] + ''<|tool|>'' + message[''tools''] + ''<|/tool|>'' + ''<|end|>'' }}{% else %}{{ ''<|'' + message[''role''] + ''|>'' + message[''content''] + ''<|end|>'' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:PhiCookBook","source_url":"https://github.com/microsoft/PhiCookBook"},{"type":"has_code","target_id":"github:marketplace:models","source_url":"https://github.com/marketplace/models"},{"type":"has_code","target_id":"github:nguyenbh:phi4mm-demos","source_url":"https://github.com/nguyenbh/phi4mm-demos"},{"type":"has_code","target_id":"github:nguyenbh:phi4mm-demos","source_url":"https://github.com/nguyenbh/phi4mm-demos"},{"type":"has_code","target_id":"github:nguyenbh:phi4mm-demos","source_url":"https://github.com/nguyenbh/phi4mm-demos"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"},{"type":"has_code","target_id":"github:bastibe:python-soundfile","source_url":"https://github.com/bastibe/python-soundfile"},{"type":"has_code","target_id":"github:python-pillow:Pillow","source_url":"https://github.com/python-pillow/Pillow"},{"type":"has_code","target_id":"github:iver56:audiomentations","source_url":"https://github.com/iver56/audiomentations"},{"type":"has_code","target_id":"github:daekeun-ml:azure-genai-utils","source_url":"https://github.com/daekeun-ml/azure-genai-utils"},{"type":"based_on_paper","target_id":"arxiv:2503.01743","source_url":"https://arxiv.org/abs/2503.01743"},{"type":"based_on_paper","target_id":"arxiv:2407.13833","source_url":"https://arxiv.org/abs/2407.13833"}]', NULL, 'MIT', 'approved', 80, 'eeab0ab33979b4d8088e9759755b8a0b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stepfun-ai-GOT-OCR2-0', 'huggingface--stepfun-ai--got-ocr2-0', 'GOT-OCR2_0', 'stepfun-ai', '--- pipeline_tag: image-text-to-text language: - multilingual tags: - got - vision-language - ocr2.0 - custom_code license: apache-2.0 --- <h1>General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model </h1> 🔋Online Demo | 🌟GitHub | 📜Paper</a> Haoran Wei*, Chenglong Liu*, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang !image/jpeg Inference using Huggingface transformers on NVIDIA GPUs. Requirements tested o...', '["safetensors","got","got","vision-language","ocr2.0","custom_code","image-text-to-text","multilingual","arxiv:2409.01704","arxiv:2405.14295","arxiv:2312.06109","license:apache-2.0","region:us"]', 'image-text-to-text', 1526, 15711, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stepfun-ai/GOT-OCR2_0","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-text-to-text\nlanguage:\n- multilingual\ntags:\n- got\n- vision-language\n- ocr2.0\n- custom_code\nlicense: apache-2.0\n---\n\n<h1>General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\n</h1>\n\n[🔋Online Demo](https://huggingface.co/spaces/ucaslcl/GOT_online) | [🌟GitHub](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/) | [📜Paper](https://arxiv.org/abs/2409.01704)</a> \n\n\n[Haoran Wei*](https://scholar.google.com/citations?user=J4naK0MAAAAJ&hl=en), Chenglong Liu*, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu,  [Zheng Ge](https://joker316701882.github.io/), Liang Zhao, [Jianjian Sun](https://scholar.google.com/citations?user=MVZrGkYAAAAJ&hl=en), [Yuang Peng](https://scholar.google.com.hk/citations?user=J0ko04IAAAAJ&hl=zh-CN&oi=ao), Chunrui Han, [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en)\n\n\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6653eee7a2d7a882a805ab95/QCEFY-M_YG3Bp5fn1GQ8X.jpeg)\n\n\n\n## Usage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.10：\n```\ntorch==2.0.1\ntorchvision==0.15.2\ntransformers==4.37.2\ntiktoken==0.6.0\nverovio==4.3.1\naccelerate==0.28.0\n```\n\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(''ucaslcl/GOT-OCR2_0'', trust_remote_code=True)\nmodel = AutoModel.from_pretrained(''ucaslcl/GOT-OCR2_0'', trust_remote_code=True, low_cpu_mem_usage=True, device_map=''cuda'', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\nmodel = model.eval().cuda()\n\n\n# input your test image\nimage_file = ''xxx.jpg''\n\n# plain texts OCR\nres = model.chat(tokenizer, image_file, ocr_type=''ocr'')\n\n# format texts OCR:\n# res = model.chat(tokenizer, image_file, ocr_type=''format'')\n\n# fine-grained OCR:\n# res = model.chat(tokenizer, image_file, ocr_type=''ocr'', ocr_box='''')\n# res = model.chat(tokenizer, image_file, ocr_type=''format'', ocr_box='''')\n# res = model.chat(tokenizer, image_file, ocr_type=''ocr'', ocr_color='''')\n# res = model.chat(tokenizer, image_file, ocr_type=''format'', ocr_color='''')\n\n# multi-crop OCR:\n# res = model.chat_crop(tokenizer, image_file, ocr_type=''ocr'')\n# res = model.chat_crop(tokenizer, image_file, ocr_type=''format'')\n\n# render the formatted OCR results:\n# res = model.chat(tokenizer, image_file, ocr_type=''format'', render=True, save_render_file = ''./demo.html'')\n\nprint(res)\n\n\n```\nMore details about ''ocr_type'', ''ocr_box'', ''ocr_color'', and ''render'' can be found at our GitHub.\nOur training codes are available at our [GitHub](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/).\n\n\n\n## More Multimodal Projects\n\n👏 Welcome to explore more multimodal projects of our team:\n\n[Vary](https://github.com/Ucas-HaoranWei/Vary) | [Fox](https://github.com/ucaslcl/Fox) | [OneChart](https://github.com/LingyvKong/OneChart)\n\n## Citation\n\nIf you find our work helpful, please consider citing our papers 📝 and liking this project ❤️！\n\n```bib\n@article{wei2024general,\n  title={General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},\n  author={Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},\n  journal={arXiv preprint arXiv:2409.01704},\n  year={2024}\n}\n@article{liu2024focus,\n  title={Focus Anywhere for Fine-grained Multi-page Document Understanding},\n  author={Liu, Chenglong and Wei, Haoran and Chen, Jinyue and Kong, Lingyu and Ge, Zheng and Zhu, Zining and Zhao, Liang and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\n  journal={arXiv preprint arXiv:2405.14295},\n  year={2024}\n}\n@article{wei2023vary,\n  title={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},\n  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\n  journal={arXiv preprint arXiv:2312.06109},\n  year={2023}\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":null,"framework":null,"params":716033280,"storage_bytes":1432126851,"files_count":15,"spaces_count":66,"gated":false,"private":false,"config":{"architectures":["GOTQwenForCausalLM"],"auto_map":{"AutoConfig":"modeling_GOT.GOTConfig","AutoModel":"modeling_GOT.GOTQwenForCausalLM"},"model_type":"GOT","tokenizer_config":{"pad_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:Ucas-HaoranWei:GOT-OCR2.0","source_url":"https://github.com/Ucas-HaoranWei/GOT-OCR2.0"},{"type":"has_code","target_id":"github:Ucas-HaoranWei:GOT-OCR2.0","source_url":"https://github.com/Ucas-HaoranWei/GOT-OCR2.0"},{"type":"has_code","target_id":"github:Ucas-HaoranWei:Vary","source_url":"https://github.com/Ucas-HaoranWei/Vary"},{"type":"has_code","target_id":"github:ucaslcl:Fox","source_url":"https://github.com/ucaslcl/Fox"},{"type":"has_code","target_id":"github:LingyvKong:OneChart","source_url":"https://github.com/LingyvKong/OneChart"},{"type":"based_on_paper","target_id":"arxiv:2409.01704","source_url":"https://arxiv.org/abs/2409.01704"},{"type":"based_on_paper","target_id":"arxiv:2405.14295","source_url":"https://arxiv.org/abs/2405.14295"},{"type":"based_on_paper","target_id":"arxiv:2312.06109","source_url":"https://arxiv.org/abs/2312.06109"}]', NULL, 'Apache-2.0', 'approved', 85, 'ba5ee7cd55fdc57566a6087c06207aeb', NULL, 'https://huggingface.co/stepfun-ai/GOT-OCR2_0/resolve/main/assets/got_logo.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stepfun-ai-GOT-OCR2-0 from https://huggingface.co/stepfun-ai/GOT-OCR2_0/resolve/main/assets/got_logo.png
Image converted to WebP: data/images/huggingface-stepfun-ai-GOT-OCR2-0.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-EleutherAI-gpt-j-6b', 'huggingface--eleutherai--gpt-j-6b', 'gpt-j-6b', 'EleutherAI', '--- language: - en tags: - pytorch - causal-lm license: apache-2.0 datasets: - EleutherAI/pile --- GPT-J 6B is a transformer model trained using Ben Wang''s Mesh Transformer JAX. "GPT-J" refers to the class of model, while "6B" represents the number of trainable parameters. <figure> | Hyperparameter | Value | |----------------------|------------| | \\(n_{parameters}\\) | 6053381344 | | \\(n_{layers}\\) | 28&ast; | | \\(d_{model}\\) | 4096 | | \\(d_{ff}\\) | 16384 | | \\(n_{heads}\\) | 16 | | \...', '["transformers","pytorch","tf","jax","gptj","text-generation","causal-lm","en","dataset:eleutherai/pile","arxiv:2104.09864","arxiv:2101.00027","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 1516, 121682, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/EleutherAI/gpt-j-6b","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- en\ntags:\n- pytorch\n- causal-lm\nlicense: apache-2.0\ndatasets:\n- EleutherAI/pile\n---\n\n# GPT-J 6B\n\n## Model Description\n\nGPT-J 6B is a transformer model trained using Ben Wang''s [Mesh Transformer JAX](https://github.com/kingoflolz/mesh-transformer-jax/). "GPT-J" refers to the class of model, while "6B" represents the number of trainable parameters.\n\n<figure>\n\n| Hyperparameter       | Value      |\n|----------------------|------------|\n| \\(n_{parameters}\\) | 6053381344 |\n| \\(n_{layers}\\)     | 28&ast;    |\n| \\(d_{model}\\)      | 4096       |\n| \\(d_{ff}\\)         | 16384      |\n| \\(n_{heads}\\)      | 16         |\n| \\(d_{head}\\)       | 256        |\n| \\(n_{ctx}\\)        | 2048       |\n| \\(n_{vocab}\\)      | 50257/50400&dagger; (same tokenizer as GPT-2/3)  |\n| Positional Encoding  | [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864) |\n| RoPE Dimensions      | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n<figcaption><p><strong>&ast;</strong> Each layer consists of one feedforward block and one self attention block.</p>\n<p><strong>&dagger;</strong> Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer.</p></figcaption></figure>\n\nThe model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model\ndimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64\ndimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as\nGPT-2/GPT-3.\n\n## Intended Use and Limitations\n\nGPT-J learns an inner representation of the English language that can be used to \nextract features useful for downstream tasks. The model is best at what it was \npretrained for however, which is generating text from a prompt.\n\n### Out-of-scope use\n\nGPT-J-6B is **not** intended for deployment without fine-tuning, supervision, \nand/or moderation. It is not a in itself a product and cannot be used for \nhuman-facing interactions. For example, the model may generate harmful or \noffensive text. Please evaluate the risks associated with your particular use case.\n\nGPT-J-6B was trained on an English-language only dataset, and is thus **not**\nsuitable for translation or generating text in other languages.\n\nGPT-J-6B has not been fine-tuned for downstream contexts in which \nlanguage models are commonly deployed, such as writing genre prose, \nor commercial chatbots. This means GPT-J-6B will **not** \nrespond to a given prompt the way a product like ChatGPT does. This is because,\n unlike this model, ChatGPT was fine-tuned using methods such as Reinforcement \nLearning from Human Feedback (RLHF) to better “follow” human instructions.\n\n### Limitations and Biases\n\nThe core functionality of GPT-J is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work. When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most "accurate" text. Never depend upon GPT-J to produce factually accurate output.\n\nGPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile.\n\nAs with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.\n\n### How to use\n\nThis model can be easily loaded using the `AutoModelForCausalLM` functionality:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")\nmodel = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")\n```\n\n## Training data\n\nGPT-J 6B was trained on [the Pile](https://pile.eleuther.ai), a large-scale curated dataset created by [EleutherAI](https://www.eleuther.ai).\n\n## Training procedure\n\nThis model was trained for 402 billion tokens over 383,500 steps on TPU v3-256 pod. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token correctly.\n\n## Evaluation results\n\n<figure>\n\n|  Model                   | Public      | Training FLOPs | LAMBADA PPL ↓ | LAMBADA Acc ↑ | Winogrande ↑ | Hellaswag ↑ | PIQA ↑    | Dataset Size (GB) |\n|--------------------------|-------------|----------------|---            |---            |---           |---          |---        |-------------------|\n| Random Chance            | &check;     | 0              | ~a lot        | ~0%           | 50%          | 25%         | 25%       | 0                 |\n| GPT-3 Ada&ddagger;       | &cross;     | -----          | 9.95          | 51.6%         | 52.9%        | 43.4%       | 70.5%     | -----             |\n| GPT-2 1.5B               | &check;     | -----          | 10.63         | 51.21%        | 59.4%        | 50.9%       | 70.8%     | 40                |\n| GPT-Neo 1.3B&ddagger;    | &check;     | 3.0e21         | 7.50          | 57.2%         | 55.0%        | 48.9%       | 71.1%     | 825               |\n| Megatron-2.5B&ast;       | &cross;     | 2.4e21         | -----         | 61.7%         | -----        | -----       | -----     | 174               |\n| GPT-Neo 2.7B&ddagger;    | &check;     | 6.8e21         | 5.63          | 62.2%         | 56.5%        | 55.8%       | 73.0%     | 825               |\n| GPT-3 1.3B&ast;&ddagger; | &cross;     | 2.4e21         | 5.44          | 63.6%         | 58.7%        | 54.7%       | 75.1%     | ~800              |\n| GPT-3 Babbage&ddagger;   | &cross;     | -----          | 5.58          | 62.4%         | 59.0%        | 54.5%       | 75.5%     | -----             |\n| Megatron-8.3B&ast;       | &cross;     | 7.8e21         | -----         | 66.5%         | -----        | -----       | -----     | 174               |\n| GPT-3 2.7B&ast;&ddagger; | &cross;     | 4.8e21         | 4.60          | 67.1%         | 62.3%        | 62.8%       | 75.6%     | ~800              |\n| Megatron-11B&dagger;     | &check;     | 1.0e22         | -----         | -----         | -----        | -----       | -----     | 161               |\n| **GPT-J 6B&ddagger;**    | **&check;** | **1.5e22**     | **3.99**      | **69.7%**     | **65.3%**    | **66.1%**   | **76.5%** | **825**           |\n| GPT-3 6.7B&ast;&ddagger; | &cross;     | 1.2e22         | 4.00          | 70.3%         | 64.5%        | 67.4%       | 78.0%     | ~800              |\n| GPT-3 Curie&ddagger;     | &cross;     | -----          | 4.00          | 69.3%         | 65.6%        | 68.5%       | 77.9%     | -----             |\n| GPT-3 13B&ast;&ddagger;  | &cross;     | 2.3e22         | 3.56          | 72.5%         | 67.9%        | 70.9%       | 78.5%     | ~800              |\n| GPT-3 175B&ast;&ddagger; | &cross;     | 3.1e23         | 3.00          | 76.2%         | 70.2%        | 78.9%       | 81.0%     | ~800              |\n| GPT-3 Davinci&ddagger;   | &cross;     | -----          | 3.0           | 75%           | 72%          | 78%         | 80%       | -----             |\n<figcaption><p>Models roughly sorted by performance, or by FLOPs if not available.</p>\n\n<p><strong>&ast;</strong> Evaluation numbers reported by their respective authors. All other numbers are provided by\nrunning <a href="https://github.com/EleutherAI/lm-evaluation-harness/"><code>lm-evaluation-harness</code></a> either with released\nweights or with API access. Due to subtle implementation differences as well as different zero shot task framing, these\nmight not be directly comparable. See <a href="https://blog.eleuther.ai/gpt3-model-sizes/">this blog post</a> for more\ndetails.</p>\n\n<p><strong>†</strong> Megatron-11B provides no comparable metrics, and several implementations using the released weights do not\nreproduce the generation quality and evaluations. (see <a href="https://github.com/huggingface/transformers/pull/10301">1</a>\n<a href="https://github.com/pytorch/fairseq/issues/2358">2</a> <a href="https://github.com/pytorch/fairseq/issues/2719">3</a>)\nThus, evaluation was not attempted.</p>\n\n<p><strong>‡</strong> These models have been trained with data which contains possible test set contamination. The OpenAI GPT-3 models\nfailed to deduplicate training data for certain test sets, while the GPT-Neo models as well as this one is\ntrained on the Pile, which has not been deduplicated against any test sets.</p></figcaption></figure>\n\n## Citation and Related Information\n\n### BibTeX entry\n\nTo cite this model:\n```bibtex\n@misc{gpt-j,\n  author = {Wang, Ben and Komatsuzaki, Aran},\n  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},\n  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\nTo cite the codebase that trained this model:\n```bibtex\n@misc{mesh-transformer-jax,\n  author = {Wang, Ben},\n  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\n  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\nIf you use this model, we would love to hear about it! Reach out on [GitHub](https://github.com/kingoflolz/mesh-transformer-jax), Discord, or shoot Ben an email.\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](https://sites.research.google/trc/), as well as the Cloud TPU team for providing early access to the [Cloud TPU VM](https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms) Alpha.\n\nThanks to everyone who have helped out one way or another (listed alphabetically):\n- [James Bradbury](https://twitter.com/jekbradbury) for valuable assistance with debugging JAX issues.\n- [Stella Biderman](https://www.stellabiderman.com), [Eric Hallahan](https://twitter.com/erichallahan), [Kurumuz](https://github.com/kurumuz/), and [Finetune](https://github.com/finetuneanon/) for converting the model to be compatible with the `transformers` package.\n- [Leo Gao](https://twitter.com/nabla_theta) for running zero shot evaluations for the baseline models for the table.\n- [Laurence Golding](https://github.com/researcher2/) for adding some features to the web demo.\n- [Aran Komatsuzaki](https://twitter.com/arankomatsuzaki) for advice with experiment design and writing the blog posts.\n- [Janko Prester](https://github.com/jprester/) for creating the web demo frontend.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":231474283057,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GPTJForCausalLM"],"model_type":"gptj","tokenizer_config":{"unk_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"bos_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"eos_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"}}}}', '[]', '[{"type":"has_code","target_id":"github:kingoflolz:mesh-transformer-jax","source_url":"https://github.com/kingoflolz/mesh-transformer-jax"},{"type":"has_code","target_id":"github:kingoflolz:mesh-transformer-jax","source_url":"https://github.com/kingoflolz/mesh-transformer-jax"},{"type":"has_code","target_id":"github:EleutherAI:lm-evaluation-harness","source_url":"https://github.com/EleutherAI/lm-evaluation-harness"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:kingoflolz:mesh-transformer-jax}},","source_url":"https://github.com/kingoflolz/mesh-transformer-jax}},"},{"type":"has_code","target_id":"github:kingoflolz:mesh-transformer-jax}},","source_url":"https://github.com/kingoflolz/mesh-transformer-jax}},"},{"type":"has_code","target_id":"github:kingoflolz:mesh-transformer-jax","source_url":"https://github.com/kingoflolz/mesh-transformer-jax"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"}]', NULL, 'Apache-2.0', 'approved', 80, 'b7900d1e4e04c11de3884b62f95ee313', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-bart-large-cnn', 'huggingface--facebook--bart-large-cnn', 'bart-large-cnn', 'facebook', '--- language: - en pipeline_tag: summarization license: mit thumbnail: https://huggingface.co/front/thumbnails/facebook.png datasets: - cnn_dailymail model-index: - name: facebook/bart-large-cnn results: - task: type: summarization name: Summarization dataset: name: cnn_dailymail type: cnn_dailymail config: 3.0.0 split: train metrics: - name: ROUGE-1 type: rouge value: 42.9486 verified: true - name: ROUGE-2 type: rouge value: 20.8149 verified: true - name: ROUGE-L type: rouge value: 30.6186 v...', '["transformers","pytorch","tf","jax","rust","safetensors","bart","text2text-generation","summarization","en","dataset:cnn_dailymail","arxiv:1910.13461","license:mit","model-index","endpoints_compatible","deploy:azure","region:us"]', 'summarization', 1514, 5096213, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/bart-large-cnn","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- en\npipeline_tag: summarization\nlicense: mit\nthumbnail: https://huggingface.co/front/thumbnails/facebook.png\ndatasets:\n- cnn_dailymail\nmodel-index:\n- name: facebook/bart-large-cnn\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: cnn_dailymail\n      type: cnn_dailymail\n      config: 3.0.0\n      split: train\n    metrics:\n    - name: ROUGE-1\n      type: rouge\n      value: 42.9486\n      verified: true\n    - name: ROUGE-2\n      type: rouge\n      value: 20.8149\n      verified: true\n    - name: ROUGE-L\n      type: rouge\n      value: 30.6186\n      verified: true\n    - name: ROUGE-LSUM\n      type: rouge\n      value: 40.0376\n      verified: true\n    - name: loss\n      type: loss\n      value: 2.529000997543335\n      verified: true\n    - name: gen_len\n      type: gen_len\n      value: 78.5866\n      verified: true\n---\n# BART (large-sized model), fine-tuned on CNN Daily Mail \n\nBART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart). \n\nDisclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nBART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n\nBART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\n\n## Intended uses & limitations\n\nYou can use this model for text summarization. \n\n### How to use\n\nHere is how to use this model with the [pipeline API](https://huggingface.co/transformers/main_classes/pipelines.html):\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline("summarization", model="facebook/bart-large-cnn")\n\nARTICLE = """ New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared "I do" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her "first and only" marriage.\nBarrientos, now 39, is facing two criminal counts of "offering a false instrument for filing in the first degree," referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\''s Office by Immigration and Customs Enforcement and the Department of Homeland Security\''s\nInvestigation Division. Seven of the men are from so-called "red-flagged" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n"""\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{''summary_text'': ''Liana Barrientos, 39, is charged with two counts of "offering a false instrument for filing in the first degree" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.''}]\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```', '{"pipeline_tag":"summarization","library_name":"transformers","framework":"transformers","params":406290432,"storage_bytes":18004953759,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BartForConditionalGeneration"],"model_type":"bart"}}', '[]', '[{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"based_on_paper","target_id":"arxiv:1910.13461","source_url":"https://arxiv.org/abs/1910.13461"}]', NULL, 'MIT', 'approved', 65, '3fcbc91ee6c503133557e2d4c90eb613', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-moonshotai-Kimi-K2-Thinking', 'huggingface--moonshotai--kimi-k2-thinking', 'Kimi-K2-Thinking', 'moonshotai', '--- license: other license_name: modified-mit library_name: transformers --- <div align="center"> <picture> <img src="figures/kimi-logo.png" width="30%" alt="Kimi K2: Open Agentic Intellignece"> </picture> </div> <hr> <div align="center" style="line-height:1"> <a href="https://www.kimi.com" target="_blank"><img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-Kimi%20K2-ff6b6b?color=1783ff&logoColor=white"/></a> <a href="https://www.moonshot.ai" target="_blank"><img alt="Homepage" src="h...', '["transformers","safetensors","kimi_k2","text-generation","conversational","custom_code","license:other","endpoints_compatible","compressed-tensors","region:us"]', 'text-generation', 1506, 391098, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/moonshotai/Kimi-K2-Thinking","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: modified-mit\nlibrary_name: transformers\n---\n<div align="center">\n  <picture>\n      <img src="figures/kimi-logo.png" width="30%" alt="Kimi K2: Open Agentic Intellignece">\n  </picture>\n</div>\n<hr>\n\n<div align="center" style="line-height:1">\n  <a href="https://www.kimi.com" target="_blank"><img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-Kimi%20K2-ff6b6b?color=1783ff&logoColor=white"/></a>\n  <a href="https://www.moonshot.ai" target="_blank"><img alt="Homepage" src="https://img.shields.io/badge/Homepage-Moonshot%20AI-white?logo=Kimi&logoColor=white"/></a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://huggingface.co/moonshotai" target="_blank"><img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Moonshot%20AI-ffc107?color=ffc107&logoColor=white"/></a>\n  <a href="https://twitter.com/kimi_moonshot" target="_blank"><img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-Kimi.ai-white?logo=x&logoColor=white"/></a>\n    <a href="https://discord.gg/TYU2fdJykW" target="_blank"><img alt="Discord" src="https://img.shields.io/badge/Discord-Kimi.ai-white?logo=discord&logoColor=white"/></a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking/blob/main/LICENSE"><img alt="License" src="https://img.shields.io/badge/License-Modified_MIT-f5de53?&color=f5de53"/></a>\n</div>\n\n<p align="center">\n<b>📰&nbsp;&nbsp;<a href="https://moonshotai.github.io/Kimi-K2/thinking.html">Tech Blog</a></b>\n</p>\n\n\n## 1. Model Introduction\n\nKimi K2 Thinking is the latest, most capable version of open-source thinking model. Starting with Kimi K2, we built it as a thinking agent that reasons step-by-step while dynamically invoking tools. It sets a new state-of-the-art on Humanity''s Last Exam (HLE), BrowseComp, and other benchmarks by dramatically scaling multi-step reasoning depth and maintaining stable tool-use across 200–300 sequential calls. At the same time, K2 Thinking is a native INT4 quantization model with 256k context window, achieving lossless reductions in inference latency and GPU memory usage.\n\n### Key Features\n- **Deep Thinking & Tool Orchestration**: End-to-end trained to interleave chain-of-thought reasoning with function calls, enabling autonomous research, coding, and writing workflows that last hundreds of steps without drift.\n- **Native INT4 Quantization**: Quantization-Aware Training (QAT) is employed in post-training stage to achieve lossless 2x speed-up in low-latency mode.\n- **Stable Long-Horizon Agency**: Maintains coherent goal-directed behavior across up to 200–300 consecutive tool invocations, surpassing prior models that degrade after 30–50 steps.\n\n\n## 2. Model Summary\n\n<div align="center">\n\n\n| | |\n|:---:|:---:|\n| **Architecture** | Mixture-of-Experts (MoE) |\n| **Total Parameters** | 1T |\n| **Activated Parameters** | 32B |\n| **Number of Layers** (Dense layer included) | 61 |\n| **Number of Dense Layers** | 1 |\n| **Attention Hidden Dimension** | 7168 |\n| **MoE Hidden Dimension** (per Expert) | 2048 |\n| **Number of Attention Heads** | 64 |\n| **Number of Experts** | 384 |\n| **Selected Experts per Token** | 8 |\n| **Number of Shared Experts** | 1 |\n| **Vocabulary Size** | 160K |\n| **Context Length** | 256K |\n| **Attention Mechanism** | MLA |\n| **Activation Function** | SwiGLU |\n</div>\n\n## 3. Evaluation Results\n\n**Reasoning Tasks**\n| Benchmark | Setting | K2 Thinking | GPT-5<br> (High) | Claude Sonnet 4.5<br> (Thinking) | K2 0905 | DeepSeek-V3.2 | Grok-4 |\n|:----------:|:--------:|:------------:|:------:|:----------------------------:|:--------:|:--------------:|:-------:|\n| **HLE (Text-only)** | no tools | 23.9 | 26.3 | 19.8* | 7.9 | 19.8 | 25.4 |\n| | w/ tools | 44.9 | 41.7* | 32.0* | 21.7 | 20.3* | 41.0 |\n| | heavy | 51.0 | 42.0 | - | - | - | 50.7 |\n| **AIME25** | no tools | 94.5 | 94.6 | 87.0 | 51.0 | 89.3 | 91.7 |\n| | w/ python | 99.1 | 99.6 | 100.0 | 75.2 | 58.1* | 98.8 |\n| | heavy | 100.0 | 100.0 | - | - | - | 100.0 |\n| **HMMT25** | no tools | 89.4 | 93.3 | 74.6* | 38.8 | 83.6 | 90.0 |\n| | w/ python | 95.1 | 96.7 | 88.8* | 70.4 | 49.5* | 93.9 |\n| | heavy | 97.5 | 100.0 | - | - | - | 96.7 |\n| **IMO-AnswerBench** | no tools | 78.6 | 76.0* | 65.9* | 45.8 | 76.0* | 73.1 |\n| **GPQA** | no tools | 84.5 | 85.7 | 83.4 | 74.2 | 79.9 | 87.5 |\n\n**General Tasks**\n| Benchmark | Setting | K2 Thinking | GPT-5<br> (High) | Claude Sonnet 4.5<br> (Thinking) | K2 0905 | DeepSeek-V3.2 |\n|:----------:|:--------:|:------------:|:------:|:----------------------------:|:--------:|:--------------:|\n| **MMLU-Pro** | no tools | 84.6 | 87.1 | 87.5 | 81.9 | 85.0 |\n| **MMLU-Redux** | no tools | 94.4 | 95.3 | 95.6 | 92.7 | 93.7 |\n| **Longform Writing** | no tools | 73.8 | 71.4 | 79.8 | 62.8 | 72.5 |\n| **HealthBench** | no tools | 58.0 | 67.2 | 44.2 | 43.8 | 46.9 |\n\n**Agentic Search Tasks**\n| Benchmark | Setting | K2 Thinking | GPT-5<br> (High) | Claude Sonnet 4.5<br> (Thinking) | K2 0905 | DeepSeek-V3.2 |\n|:----------:|:--------:|:------------:|:------:|:----------------------------:|:--------:|:--------------:|\n| **BrowseComp** | w/ tools | 60.2 | 54.9 | 24.1 | 7.4 | 40.1 |\n| **BrowseComp-ZH** | w/ tools | 62.3 | 63.0* | 42.4* | 22.2 | 47.9 |\n| **Seal-0** | w/ tools | 56.3 | 51.4* | 53.4* | 25.2 | 38.5* |\n| **FinSearchComp-T3** | w/ tools | 47.4 | 48.5* | 44.0* | 10.4 | 27.0* |\n| **Frames** | w/ tools | 87.0 | 86.0* | 85.0* | 58.1 | 80.2* |\n\n**Coding Tasks**\n| Benchmark | Setting | K2 Thinking | GPT-5<br> (High) | Claude Sonnet 4.5<br> (Thinking) | K2 0905 | DeepSeek-V3.2 |\n|:----------:|:--------:|:------------:|:------:|:----------------------------:|:--------:|:--------------:|\n| **SWE-bench Verified** | w/ tools | 71.3 | 74.9 | 77.2 | 69.2 | 67.8 |\n| **SWE-bench Multilingual** | w/ tools | 61.1 | 55.3* | 68.0 | 55.9 | 57.9 |\n| **Multi-SWE-bench** | w/ tools | 41.9 | 39.3* | 44.3 | 33.5 | 30.6 |\n| **SciCode** | no tools | 44.8 | 42.9 | 44.7 | 30.7 | 37.7 |\n| **LiveCodeBenchV6** | no tools | 83.1 | 87.0* | 64.0* | 56.1* | 74.1 |\n| **OJ-Bench (cpp)** | no tools | 48.7 | 56.2* | 30.4* | 25.5* | 38.2* |\n| **Terminal-Bench** | w/ simulated tools (JSON) | 47.1 | 43.8 | 51.0 | 44.5 | 37.7 |\n<details>\n<summary><b>Footnotes</b></summary>\n\n1. To ensure a fast, lightweight experience, we selectively employ a subset of tools and reduce the number of tool call steps under the chat mode on kimi.com. As a result, chatting on kimi.com may not reproduce our benchmark scores. Our agentic mode will be updated soon to reflect the full capabilities of K2 Thinking.\n\n2. **Testing Details**:  \n 2.1. All benchmarks were evaluated at temperature = 1.0 and 256 k context length for K2 Thinking, except for SciCode, for which we followed the official temperature setting of 0.0.  \n 2.2. HLE (no tools), AIME25, HMMT25, and GPQA were capped at a 96k thinking-token budget, while IMO-Answer Bench, LiveCodeBench and OJ-Bench were capped at a 128k thinking-token budget. Longform Writing was capped at a 32k completion-token budget.  \n 2.3. For AIME and HMMT (no tools), we report the average of 32 runs (avg@32). For AIME and HMMT (with Python), we report the average of 16 runs (avg@16). For IMO-AnswerBench, we report the average of 8 runs (avg@8).\n\n3. **Baselines**:  \n 3.1 GPT-5, Claude-4.5-sonnet, Grok-4 results and DeepSeek-V3.2 results are quoted from the [GPT-5 post](https://openai.com/index/introducing-gpt-5/), [GPT-5 for Developers post](https://openai.com/index/introducing-gpt-5-for-developers/), [GPT-5 system card](https://openai.com/index/gpt-5-system-card/), [claude-sonnet-4-5 post](https://www.anthropic.com/news/claude-sonnet-4-5), [grok-4 post](https://x.ai/news/grok-4), [deepseek-v3.2 post](https://api-docs.deepseek.com/news/news250929), the [public Terminal-Bench leaderboard](https://www.tbench.ai/leaderboard) (Terminus-2), the [public Vals AI leaderboard](https://vals.ai/) and [artificialanalysis](https://artificialanalysis.ai/). Benchmarks for which no available public scores were re-tested under the same conditions used for k2 thinking and are marked with an asterisk(*). For the GPT-5 test, we set the reasoning effort to high.  \n 3.2 The GPT-5 and Grok-4 on the HLE full set with tools are 35.2 and 38.6 from the official posts. In our internal evaluation on the HLE text-only subset, GPT-5 scores 41.7 and Grok-4 scores 38.6 (Grok-4’s launch cited 41.0 on the text-only subset). For GPT-5''s HLE text-only w/o tool, we use score from <a href="https://scale.com/leaderboard/humanitys_last_exam_text_only" target="_blank">Scale.ai</a>. The official GPT5 HLE full set w/o tool is 24.8.  \n 3.3 For <a href="https://aclanthology.org/2025.emnlp-main.1794.pdf" target="_blank">IMO-AnswerBench</a>: GPT-5 scored 65.6 in the benchmark paper. We re-evaluated GPT-5 with official API and obtained a score of 76.\n\n4. **For HLE (w/ tools) and the agentic-search benchmarks**:  \n 4.1. K2 Thinking was equipped with search, code-interpreter, and web-browsing tools.  \n 4.2. BrowseComp-ZH, Seal-0 and FinSearchComp-T3 were run 4 times independently and the average is reported (avg@4).  \n 4.3. The evaluation used o3-mini as judge, configured identically to the official HLE setting; judge prompts were taken verbatim from the official repository.  \n 4.4. On HLE, the maximum step limit was 120, with a 48 k-token reasoning budget per step; on agentic-search tasks, the limit was 300 steps with a 24 k-token reasoning budget per step.  \n 4.5. When tool execution results cause the accumulated input to exceed the model''s context limit (256k), we employ a simple context management strategy that hides all previous tool outputs.  \n 4.6. The web access to Hugging Face may lead to data leakage in certain benchmark tests, such as HLE. K2 Thinking can achieve a score of 51.3 on HLE without blocking Hugging Face. To ensure a fair and rigorous comparison, we blocked access to Hugging Face during testing.\n\n5. **For Coding Tasks**:  \n 5.1. Terminal-Bench scores were obtained with the default agent framework (Terminus-2) and the provided JSON parser.  \n 5.2. For other coding tasks, the result was produced with our in-house evaluation harness. The harness is derived from SWE-agent, but we clamp the context windows of the Bash and Edit tools and rewrite the system prompt to match the task semantics.  \n 5.3. All reported scores of coding tasks are averaged over 5 independent runs.\n\n6. **Heavy Mode**: K2 Thinking Heavy Mode employs an efficient parallel strategy: it first rolls out eight trajectories simultaneously, then reflectively aggregates all outputs to generate the final result. Heavy mode for GPT-5 denotes the official GPT-5 Pro score.\n</details>\n\n## 4. Native INT4 Quantization\n\nLow-bit quantization is an effective way to reduce inference latency and GPU memory usage on large-scale inference servers. However, thinking models use excessive decoding lengths, and thus quantization often results in substantial performance drops. \n\nTo overcome this challenge, we adopt Quantization-Aware Training (QAT) during the post-training phase, applying INT4 weight-only quantization to the MoE components. It allows K2 Thinking to support native INT4 inference with a roughly 2x generation speed improvement while achieving state-of-the-art performance. All benchmark results are reported under INT4 precision.\n\nThe checkpoints are saved in compressed-tensors format, supported by most of mainstream inference engine. If you need the checkpoints in higher precision such as FP8 or BF16, you can refer to [official repo of compressed-tensors](https://github.com/vllm-project/compressed-tensors) to unpack the int4 weights and convert to any higher precision. \n\n## 5. Deployment\n> [!Note]\n> You can access K2 Thinking''s API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you.\n\nCurrently, Kimi-K2-Thinking is recommended to run on the following inference engines:\n\n* vLLM\n* SGLang\n* KTransformers\n\nDeployment examples can be found in the [Model Deployment Guide](docs/deploy_guidance.md).\n\n---\n\n## 6. Model Usage\n\n### Chat Completion\n\nOnce the local inference service is up, you can interact with it through the chat endpoint:\n\n```python\ndef simple_chat(client: openai.OpenAI, model_name: str):\n    messages = [\n        {"role": "system", "content": "You are Kimi, an AI assistant created by Moonshot AI."},\n        {"role": "user", "content": [{"type": "text", "text": "which one is bigger, 9.11 or 9.9? think carefully."}]},\n    ]\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        stream=False,\n        temperature=1.0,\n        max_tokens=4096\n    )\n    print(f"k2 answer: {response.choices[0].message.content}")\n    print("=====below is reasoning content======")\n    print(f"reasoning content: {response.choices[0].message.reasoning_content}")\n```\n\n> [!NOTE]\n> The recommended temperature for Kimi-K2-Thinking is `temperature = 1.0`.\n> If no special instructions are required, the system prompt above is a good default.\n\n---\n\n### Tool Calling\n\nKimi-K2-Thinking has the same tool calling settings as Kimi-K2-Instruct.\n\nTo enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.\n\nThe following example demonstrates calling a weather tool end-to-end:\n\n```python\n# Your tool implementation\ndef get_weather(city: str) -> dict:\n    return {"weather": "Sunny"}\n# Tool schema definition\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "get_weather",\n        "description": "Retrieve current weather information. Call this when the user asks about the weather.",\n        "parameters": {\n            "type": "object",\n            "required": ["city"],\n            "properties": {\n                "city": {\n                    "type": "string",\n                    "description": "Name of the city"\n                }\n            }\n        }\n    }\n}]\n# Map tool names to their implementations\ntool_map = {\n    "get_weather": get_weather\n}\ndef tool_call_with_client(client: OpenAI, model_name: str):\n    messages = [\n        {"role": "system", "content": "You are Kimi, an AI assistant created by Moonshot AI."},\n        {"role": "user", "content": "What''s the weather like in Beijing today? Use the tool to check."}\n    ]\n    finish_reason = None\n    while finish_reason is None or finish_reason == "tool_calls":\n        completion = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            temperature=1.0,\n            tools=tools,          # tool list defined above\n            tool_choice="auto"\n        )\n        choice = completion.choices[0]\n        finish_reason = choice.finish_reason\n        if finish_reason == "tool_calls":\n            messages.append(choice.message)\n            for tool_call in choice.message.tool_calls:\n                tool_call_name = tool_call.function.name\n                tool_call_arguments = json.loads(tool_call.function.arguments)\n                tool_function = tool_map[tool_call_name]\n                tool_result = tool_function(**tool_call_arguments)\n                print("tool_result:", tool_result)\n                messages.append({\n                    "role": "tool",\n                    "tool_call_id": tool_call.id,\n                    "name": tool_call_name,\n                    "content": json.dumps(tool_result)\n                })\n    print("-" * 100)\n    print(choice.message.content)\n```\n\nThe `tool_call_with_client` function implements the pipeline from user query to tool execution.\nThis pipeline requires the inference engine to support Kimi-K2’s native tool-parsing logic.\nFor more information, see the [Tool Calling Guide](docs/tool_call_guidance.md).\n\n---\n\n## 7. License\n\nBoth the code repository and the model weights are released under the [Modified MIT License](LICENSE).\n\n---\n\n## 8. Third Party Notices\n\nSee [THIRD PARTY NOTICES](THIRD_PARTY_NOTICES.md)\n\n---\n\n## 9. Contact Us\n\nIf you have any questions, please reach out at [support@moonshot.cn](mailto:support@moonshot.cn).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":594263623840,"files_count":80,"spaces_count":52,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"kimi_k2","quantization_config":{"quant_method":"compressed-tensors"},"tokenizer_config":{"bos_token":"[BOS]","eos_token":"[EOS]","pad_token":"[PAD]","unk_token":"[UNK]"},"chat_template_jinja":"{%- macro render_content(msg) -%}\n    {%- set c = msg.get(''content'') -%}\n    {%- if c is string -%}\n      {{ c }}\n    {%- elif c is not none -%}\n      {% for content in c -%}\n        {% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content -%}\n          <|media_start|>image<|media_content|><|media_pad|><|media_end|>\n        {% else -%}\n          {{ content[''text''] }}\n        {%- endif -%}\n      {%- endfor -%}\n    {%- endif -%}\n{%- endmacro -%}\n\n{% macro set_roles(message) -%}\n  {%- set role_name =  message.get(''name'') or  message[''role''] -%}\n  {%- if message[''role''] == ''user'' -%}\n    <|im_user|>{{role_name}}<|im_middle|>\n  {%- elif message[''role''] == ''assistant'' -%}\n    <|im_assistant|>{{role_name}}<|im_middle|>\n  {%- else -%}\n    <|im_system|>{{role_name}}<|im_middle|>\n  {%- endif -%}\n{%- endmacro -%}\n\n\n{%- macro render_toolcalls(message) -%}\n  <|tool_calls_section_begin|>\n  {%- for tool_call in message[''tool_calls''] -%}\n    {%- set formatted_id = tool_call[''id''] -%}\n    <|tool_call_begin|>{{ formatted_id }}<|tool_call_argument_begin|>{% if tool_call[''function''][''arguments''] is string %}{{ tool_call[''function''][''arguments''] }}{% else %}{{ tool_call[''function''][''arguments''] | tojson }}{% endif %}<|tool_call_end|>\n  {%- endfor -%}\n  <|tool_calls_section_end|>\n{%- endmacro -%}\n\n\n{# Find last non-tool-call assisitant message #}\n{%- set ns = namespace(last_non_tool_call_assistant_msg=-1) -%}\n{%- for idx in range(messages|length-1, -1, -1) -%}\n    {%- if messages[idx][''role''] == ''assistant'' and not messages[idx].get(''tool_calls'') -%}\n        {%- set ns.last_non_tool_call_assistant_msg = idx -%}\n        {%- break -%}\n    {%- endif -%}\n{%- endfor -%}\n\n{# split all messages into history & suffix, reasoning_content in suffix should be reserved.#}\n{%- set hist_msgs = messages[:ns.last_non_tool_call_assistant_msg+1] -%}\n{%- set suffix_msgs = messages[ns.last_non_tool_call_assistant_msg+1:] -%}\n\n{%- if tools -%}\n  <|im_system|>tool_declare<|im_middle|>{{ tools | tojson(separators=('','', '':'')) }}<|im_end|>\n{%- endif -%}\n\n{%- if messages|length == 0 or messages[0][''role''] != ''system'' -%}\n  <|im_system|>system<|im_middle|>You are Kimi, an AI assistant created by Moonshot AI.<|im_end|>\n{%- endif -%}\n  \n{%- for message in hist_msgs -%}\n  {{set_roles(message)}}\n  {%- if message[''role''] == ''assistant'' -%}\n    <think></think>{{render_content(message)}}\n    {%- if message.get(''tool_calls'') -%}\n      {{render_toolcalls(message)}}\n    {%- endif -%}\n  {%- elif message[''role''] == ''tool'' -%}\n    {%- set tool_call_id = message.tool_call_id -%}\n    ## Return of {{ tool_call_id }}\n{{render_content(message)}}\n  {%- elif message[''content''] is not none -%}\n    {{render_content(message)}}\n  {%- endif -%}\n  <|im_end|>\n{%- endfor -%}\n\n{%- for message in suffix_msgs -%}\n  {{set_roles(message)}}\n  {%- if message[''role''] == ''assistant'' -%}\n    {%- set rc = message.get(''reasoning_content'', '''') -%}\n    <think>{{rc}}</think>{{render_content(message)}}\n    {%- if message.get(''tool_calls'') -%}\n     {{render_toolcalls(message)}}\n    {%- endif -%}\n  {%- elif message[''role''] == ''tool'' -%}\n    {%- set tool_call_id = message.tool_call_id -%}\n    ## Return of {{ tool_call_id }}\n{{render_content(message)}}\n  {%- elif message[''content''] is not none -%}\n    {{render_content(message)}}\n  {%- endif -%}\n  <|im_end|>\n{%- endfor -%}\n\n\n{%- if add_generation_prompt -%}\n  <|im_assistant|>assistant<|im_middle|>\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:vllm-project:compressed-tensors","source_url":"https://github.com/vllm-project/compressed-tensors"}]', NULL, 'Other', 'approved', 80, '7d157eb638666574264739449e1de293', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Meta-Llama-3-70B-Instruct', 'huggingface--meta-llama--meta-llama-3-70b-instruct', 'Meta-Llama-3-70B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","base_model:meta-llama/meta-llama-3-70b","base_model:finetune:meta-llama/meta-llama-3-70b","license:llama3","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1499, 53850, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553706496,"storage_bytes":282237450046,"files_count":50,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\n\n''+ message[''content''] | trim + ''<|eot_id|>'' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ ''<|start_header_id|>assistant<|end_header_id|>\n\n'' }}{% endif %}","eos_token":"<|eot_id|>"}}}', '[]', '[]', NULL, 'LLaMA-3', 'approved', 40, '0b7ea719bc2be315fd13866c0ad90bb4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-bart-large-mnli', 'huggingface--facebook--bart-large-mnli', 'bart-large-mnli', 'facebook', '--- license: mit thumbnail: https://huggingface.co/front/thumbnails/facebook.png pipeline_tag: zero-shot-classification datasets: - multi_nli --- This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. Additional information about this model: - The bart-large model page - BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - BART fairseq implementation Yin et al. proposed a method for using pre-tra...', '["transformers","pytorch","jax","rust","safetensors","bart","text-classification","zero-shot-classification","dataset:multi_nli","arxiv:1910.13461","arxiv:1909.00161","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'zero-shot-classification', 1498, 4140513, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/bart-large-mnli","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: mit\nthumbnail: https://huggingface.co/front/thumbnails/facebook.png\npipeline_tag: zero-shot-classification\ndatasets:\n- multi_nli\n---\n\n# bart-large-mnli\n\nThis is the checkpoint for [bart-large](https://huggingface.co/facebook/bart-large) after being trained on the [MultiNLI (MNLI)](https://huggingface.co/datasets/multi_nli) dataset.\n\nAdditional information about this model:\n- The [bart-large](https://huggingface.co/facebook/bart-large) model page\n- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n](https://arxiv.org/abs/1910.13461)\n- [BART fairseq implementation](https://github.com/pytorch/fairseq/tree/master/fairseq/models/bart)\n\n## NLI-based Zero Shot Text Classification\n\n[Yin et al.](https://arxiv.org/abs/1909.00161) proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class "politics", we could construct a hypothesis of `This text is about politics.`. The probabilities for entailment and contradiction are then converted to label probabilities.\n\nThis method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face''s built-in pipeline and with native Transformers/PyTorch code.\n\n#### With the zero-shot classification pipeline\n\nThe model can be loaded with the `zero-shot-classification` pipeline like so:\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline("zero-shot-classification",\n                      model="facebook/bart-large-mnli")\n```\n\nYou can then use this pipeline to classify sequences into any of the class names you specify.\n\n```python\nsequence_to_classify = "one day I will see the world"\ncandidate_labels = [''travel'', ''cooking'', ''dancing'']\nclassifier(sequence_to_classify, candidate_labels)\n#{''labels'': [''travel'', ''dancing'', ''cooking''],\n# ''scores'': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n# ''sequence'': ''one day I will see the world''}\n```\n\nIf more than one candidate label can be correct, pass `multi_label=True` to calculate each class independently:\n\n```python\ncandidate_labels = [''travel'', ''cooking'', ''dancing'', ''exploration'']\nclassifier(sequence_to_classify, candidate_labels, multi_label=True)\n#{''labels'': [''travel'', ''exploration'', ''dancing'', ''cooking''],\n# ''scores'': [0.9945111274719238,\n#  0.9383890628814697,\n#  0.0057061901316046715,\n#  0.0018193122232332826],\n# ''sequence'': ''one day I will see the world''}\n```\n\n\n#### With manual PyTorch\n\n```python\n# pose sequence as a NLI premise and label as a hypothesis\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained(''facebook/bart-large-mnli'')\ntokenizer = AutoTokenizer.from_pretrained(''facebook/bart-large-mnli'')\n\npremise = sequence\nhypothesis = f''This example is {label}.''\n\n# run through model pre-trained on MNLI\nx = tokenizer.encode(premise, hypothesis, return_tensors=''pt'',\n                     truncation_strategy=''only_first'')\nlogits = nli_model(x.to(device))[0]\n\n# we throw away "neutral" (dim 1) and take the probability of\n# "entailment" (2) as the probability of the label being true \nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]\n```\n', '{"pipeline_tag":"zero-shot-classification","library_name":"transformers","framework":"transformers","params":407344133,"storage_bytes":14655739723,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BartForSequenceClassification"],"model_type":"bart","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"based_on_paper","target_id":"arxiv:1910.13461","source_url":"https://arxiv.org/abs/1910.13461"},{"type":"based_on_paper","target_id":"arxiv:1909.00161","source_url":"https://arxiv.org/abs/1909.00161"}]', NULL, 'MIT', 'approved', 65, 'ebdad018938cfc2dabc3ebfcb26b0460', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-suno-bark', 'huggingface--suno--bark', 'bark', 'suno', '--- language: - en - de - es - fr - hi - it - ja - ko - pl - pt - ru - tr - zh thumbnail: >- https://user-images.githubusercontent.com/5068315/230698495-cbb1ced9-c911-4c9a-941d-a1a4a1286ac6.png library: bark license: mit tags: - bark - audio - text-to-speech pipeline_tag: text-to-speech inference: true --- Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simpl...', '["transformers","pytorch","bark","text-to-audio","audio","text-to-speech","en","de","es","fr","hi","it","ja","ko","pl","pt","ru","tr","zh","license:mit","endpoints_compatible","region:us"]', 'text-to-speech', 1482, 18421, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/suno/bark","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\n- de\n- es\n- fr\n- hi\n- it\n- ja\n- ko\n- pl\n- pt\n- ru\n- tr\n- zh\nthumbnail: >-\n  https://user-images.githubusercontent.com/5068315/230698495-cbb1ced9-c911-4c9a-941d-a1a4a1286ac6.png\nlibrary: bark\nlicense: mit\ntags:\n- bark\n- audio\n- text-to-speech\npipeline_tag: text-to-speech\ninference: true\n---\n\n# Bark\n\nBark is a transformer-based text-to-audio model created by [Suno](https://www.suno.ai). \nBark can generate highly realistic, multilingual speech as well as other audio - including music, \nbackground noise and simple sound effects. The model can also produce nonverbal \ncommunications like laughing, sighing and crying. To support the research community, \nwe are providing access to pretrained model checkpoints ready for inference.\n\nThe original github repo and model card can be found [here](https://github.com/suno-ai/bark).\n\nThis model is meant for research purposes only. \nThe model output is not censored and the authors do not endorse the opinions in the generated content. \nUse at your own risk.\n\nTwo checkpoints are released:\n- [small](https://huggingface.co/suno/bark-small)\n- [**large** (this checkpoint)](https://huggingface.co/suno/bark)\n\n\n## Example\n\nTry out Bark yourself!\n\n* Bark Colab:\n\n<a target="_blank" href="https://colab.research.google.com/drive/1eJfA2XUa-mXwdMy7DoYKVYHI1iTd9Vkt?usp=sharing">\n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>\n</a>\n\n* Hugging Face Colab:\n\n<a target="_blank" href="https://colab.research.google.com/drive/1dWWkZzvu7L9Bunq9zvD-W02RFUXoW-Pd?usp=sharing"> \n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> \n</a>\n\n* Hugging Face Demo:\n\n<a target="_blank" href="https://huggingface.co/spaces/suno/bark">\n  <img src="https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg" alt="Open in HuggingFace"/>\n</a>\n\n\n## 🤗 Transformers Usage\n\nYou can run Bark locally with the 🤗 Transformers library from version 4.31.0 onwards.\n\n1. First install the 🤗 [Transformers library](https://github.com/huggingface/transformers) and scipy:\n\n```\npip install --upgrade pip\npip install --upgrade transformers scipy\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can infer the bark model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nimport scipy\n\nsynthesiser = pipeline("text-to-speech", "suno/bark")\n\nspeech = synthesiser("Hello, my dog is cooler than you!", forward_params={"do_sample": True})\n\nscipy.io.wavfile.write("bark_out.wav", rate=speech["sampling_rate"], data=speech["audio"])\n```\n\n3. Run inference via the Transformers modelling code. You can use the processor + generate code to convert text into a mono 24 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import AutoProcessor, AutoModel\n\nprocessor = AutoProcessor.from_pretrained("suno/bark")\nmodel = AutoModel.from_pretrained("suno/bark")\n\ninputs = processor(\n    text=["Hello, my name is Suno. And, uh — and I like pizza. [laughs] But I also have other interests such as playing tic tac toe."],\n    return_tensors="pt",\n)\n\nspeech_values = model.generate(**inputs, do_sample=True)\n```\n\n4. Listen to the speech samples either in an ipynb notebook:\n\n```python\nfrom IPython.display import Audio\n\nsampling_rate = model.generation_config.sample_rate\nAudio(speech_values.cpu().numpy().squeeze(), rate=sampling_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\n\n```python\nimport scipy\n\nsampling_rate = model.config.sample_rate\nscipy.io.wavfile.write("bark_out.wav", rate=sampling_rate, data=speech_values.cpu().numpy().squeeze())\n```\n\nFor more details on using the Bark model for inference using the 🤗 Transformers library, refer to the [Bark docs](https://huggingface.co/docs/transformers/model_doc/bark).\n\n## Suno Usage\n\nYou can also run Bark locally through the original [Bark library]((https://github.com/suno-ai/bark):\n\n1. First install the [`bark` library](https://github.com/suno-ai/bark)\n\n2. Run the following Python code:\n\n```python\nfrom bark import SAMPLE_RATE, generate_audio, preload_models\nfrom IPython.display import Audio\n\n# download and load all models\npreload_models()\n\n# generate audio from text\ntext_prompt = """\n     Hello, my name is Suno. And, uh — and I like pizza. [laughs] \n     But I also have other interests such as playing tic tac toe.\n"""\nspeech_array = generate_audio(text_prompt)\n\n# play text in notebook\nAudio(speech_array, rate=SAMPLE_RATE)\n```\n\n[pizza.webm](https://user-images.githubusercontent.com/5068315/230490503-417e688d-5115-4eee-9550-b46a2b465ee3.webm)\n\n\nTo save `audio_array` as a WAV file:\n\n```python\nfrom scipy.io.wavfile import write as write_wav\n\nwrite_wav("/path/to/audio.wav", SAMPLE_RATE, audio_array)\n```\n\n## Model Details\n\n\nThe following is additional information about the models released here. \n\nBark is a series of three transformer models that turn text into audio.\n\n### Text to semantic tokens\n - Input: text, tokenized with [BERT tokenizer from Hugging Face](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer)\n - Output: semantic tokens that encode the audio to be generated\n\n### Semantic to coarse tokens\n - Input: semantic tokens\n - Output: tokens from the first two codebooks of the [EnCodec Codec](https://github.com/facebookresearch/encodec) from facebook\n\n### Coarse to fine tokens\n - Input: the first two codebooks from EnCodec\n - Output: 8 codebooks from EnCodec\n\n### Architecture\n|           Model           | Parameters | Attention  | Output Vocab size |  \n|:-------------------------:|:----------:|------------|:-----------------:|\n|  Text to semantic tokens  |    80/300 M    | Causal     |       10,000      |\n| Semantic to coarse tokens |    80/300 M    | Causal     |     2x 1,024      |\n|   Coarse to fine tokens   |    80/300 M    | Non-causal |     6x 1,024      |\n\n\n### Release date\nApril 2023\n\n## Broader Implications\nWe anticipate that this model''s text to audio capabilities can be used to improve accessbility tools in a variety of languages. \n \nWhile we hope that this release will enable users to express their creativity and build applications that are a force\nfor good, we acknowledge that any text to audio model has the potential for dual use. While it is not straightforward\nto voice clone known people with Bark, it can still be used for nefarious purposes. To further reduce the chances of unintended use of Bark, \nwe also release a simple classifier to detect Bark-generated audio with high accuracy (see notebooks section of the main repository).', '{"pipeline_tag":"text-to-speech","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":35692671601,"files_count":799,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BarkModel"],"model_type":"bark","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:facebookresearch:encodec","source_url":"https://github.com/facebookresearch/encodec"}]', NULL, 'MIT', 'approved', 65, '3cab716c4d98d2a7b25cf3b9ee4a68e0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3n-E4B-it-litert-preview', 'huggingface--google--gemma-3n-e4b-it-litert-preview', 'gemma-3n-E4B-it-litert-preview', 'google', '', '["image-text-to-text","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2210.03057","arxiv:2502.12404","arxiv:2411.19799","arxiv:2009.03300","arxiv:2502.21228","arxiv:2311.12022","arxiv:2403.07974","arxiv:2108.07732","arxiv:2107.03374","license:gemma","region:us"]', 'image-text-to-text', 1478, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3n-E4B-it-litert-preview","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":null,"framework":null,"params":null,"storage_bytes":4412513942,"files_count":3,"spaces_count":22,"gated":"manual","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2411.19799","source_url":"https://arxiv.org/abs/2411.19799"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"}]', NULL, 'Gemma', 'approved', 40, '310a7aabb3e7eb711837c08043c57f2b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-TinyLlama-TinyLlama-1.1B-Chat-v1.0', 'huggingface--tinyllama--tinyllama-1.1b-chat-v1.0', 'TinyLlama-1.1B-Chat-v1.0', 'TinyLlama', '--- license: apache-2.0 datasets: - cerebras/SlimPajama-627B - bigcode/starcoderdata - HuggingFaceH4/ultrachat_200k - HuggingFaceH4/ultrafeedback_binarized language: - en widget: - example_title: Fibonacci (Python) messages: - role: system content: You are a chatbot who can help code! - role: user content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI. --- <div align="center"> </div> https://github.com/jzhang38/TinyLlama T...', '["transformers","safetensors","llama","text-generation","conversational","en","dataset:cerebras/slimpajama-627b","dataset:bigcode/starcoderdata","dataset:huggingfaceh4/ultrachat_200k","dataset:huggingfaceh4/ultrafeedback_binarized","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1473, 1850762, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n- cerebras/SlimPajama-627B\n- bigcode/starcoderdata\n- HuggingFaceH4/ultrachat_200k\n- HuggingFaceH4/ultrafeedback_binarized\nlanguage:\n- en\nwidget:\n  - example_title: Fibonacci (Python)\n    messages:\n    - role: system\n      content: You are a chatbot who can help code!\n    - role: user\n      content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.\n---\n<div align="center">\n\n# TinyLlama-1.1B\n</div>\n\nhttps://github.com/jzhang38/TinyLlama\n\nThe TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. \n\n\nWe adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n#### This Model\nThis is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF''s Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)''s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. \nWe then further aligned the model with [🤗 TRL''s](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." \n\n\n#### How to use\nYou will need the transformers>=4.34\nDo check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")\n\n# We use the tokenizer''s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        "role": "system",\n        "content": "You are a friendly chatbot who always responds in the style of a pirate",\n    },\n    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0]["generated_text"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1100048384,"storage_bytes":6604219014,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}\n{% if message[''role''] == ''user'' %}\n{{ ''<|user|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role''] == ''system'' %}\n{{ ''<|system|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role''] == ''assistant'' %}\n{{ ''<|assistant|>\n''  + message[''content''] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ ''<|assistant|>'' }}\n{% endif %}\n{% endfor %}","eos_token":"</s>","pad_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:jzhang38:TinyLlama","source_url":"https://github.com/jzhang38/TinyLlama"},{"type":"has_code","target_id":"github:huggingface:trl","source_url":"https://github.com/huggingface/trl"},{"type":"has_code","target_id":"github:jzhang38:TinyLlama","source_url":"https://github.com/jzhang38/TinyLlama"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"}]', NULL, 'Apache-2.0', 'approved', 65, '0800ab9beac742cb7b41c3df4c1ac2c9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-32B', 'huggingface--deepseek-ai--deepseek-r1-distill-qwen-32b', 'DeepSeek-R1-Distill-Qwen-32B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen2","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1472, 2266444, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":32763876352,"storage_bytes":65527841532,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<｜begin▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<｜end▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<｜end▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<｜User｜>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>'' + tool[''type''] + ''<｜tool▁sep｜>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<｜tool▁call▁end｜>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<｜tool▁call▁begin｜>'' + tool[''type''] + ''<｜tool▁sep｜>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<｜tool▁call▁end｜>''}}{{''<｜tool▁calls▁end｜><｜end▁of▁sentence｜>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<｜tool▁outputs▁end｜>'' + message[''content''] + ''<｜end▁of▁sentence｜>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<｜Assistant｜>'' + content + ''<｜end▁of▁sentence｜>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>'' + message[''content''] + ''<｜tool▁output▁end｜>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<｜tool▁output▁begin｜>'' + message[''content''] + ''<｜tool▁output▁end｜>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<｜tool▁outputs▁end｜>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<｜Assistant｜><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 100, '67707c9e9ab8a7fb2bf86561b292ce53', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-32B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-webui-ControlNet-modules-safetensors', 'huggingface--webui--controlnet-modules-safetensors', 'ControlNet-modules-safetensors', 'webui', 'This repository hosts pruned modules of ControlNet, by lllyasviel and T2I-Adapters, TencentARC Team The modules are meant for this extension for AUTOMATIC1111/stable-diffusion-webui, but should work for different webuis too if they have it implemented. cheers!🥂', '["region:us"]', 'other', 1442, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/webui/ControlNet-modules-safetensors","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', 'This repository hosts pruned `.safetensors` modules of [ControlNet](https://huggingface.co/lllyasviel/ControlNet), by [lllyasviel](https://huggingface.co/lllyasviel) and [T2I-Adapters](https://huggingface.co/TencentARC/T2I-Adapter), [TencentARC Team](https://huggingface.co/TencentARC)\n\nThe modules are meant for [this extension for AUTOMATIC1111/stable-diffusion-webui](https://github.com/Mikubill/sd-webui-controlnet), but should work for different webuis too if they have it implemented. cheers!🥂', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":7278912091,"files_count":20,"spaces_count":46,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Mikubill:sd-webui-controlnet","source_url":"https://github.com/Mikubill/sd-webui-controlnet"}]', NULL, NULL, 'pending', 30, 'd5fe4fba9de2662313caf685f1a335a8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-apple-OpenELM', 'huggingface--apple--openelm', 'OpenELM', 'apple', '--- license: other license_name: apple-sample-code-license license_link: LICENSE --- *Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari* We introduce **OpenELM**, a family of **Open** **E**fficient **L**anguage **M**odels. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accur...', '["arxiv:2404.14619","license:other","region:us"]', 'other', 1441, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/apple/OpenELM","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: apple-sample-code-license\nlicense_link: LICENSE\n---\n\n# OpenELM: An Efficient Language Model Family with Open Training and Inference Framework\n\n*Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari*\n\nWe introduce **OpenELM**, a family of **Open** **E**fficient **L**anguage **M**odels. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. We pretrained OpenELM models using the [CoreNet](https://github.com/apple/corenet) library. We release both pretrained and instruction tuned models with 270M, 450M, 1.1B and 3B parameters.\n\nOur pre-training dataset contains RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.6, totaling approximately 1.8 trillion tokens. Please check license agreements and terms of these datasets before using them.\n\nSee the list below for the details of each model:\n\n- [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   \n- [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   \n- [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   \n- [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       \n- [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) \n- [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) \n- [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) \n- [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     \n\n\n```python\n\nfrom transformers import AutoModelForCausalLM\n\nopenelm_270m = AutoModelForCausalLM.from_pretrained("apple/OpenELM-270M", trust_remote_code=True)\nopenelm_450m = AutoModelForCausalLM.from_pretrained("apple/OpenELM-450M", trust_remote_code=True)\nopenelm_1b = AutoModelForCausalLM.from_pretrained("apple/OpenELM-1_1B", trust_remote_code=True)\nopenelm_3b = AutoModelForCausalLM.from_pretrained("apple/OpenELM-3B", trust_remote_code=True)\n\nopenelm_270m_instruct = AutoModelForCausalLM.from_pretrained("apple/OpenELM-270M-Instruct", trust_remote_code=True)\nopenelm_450m_instruct = AutoModelForCausalLM.from_pretrained("apple/OpenELM-450M-Instruct", trust_remote_code=True)\nopenelm_1b_instruct = AutoModelForCausalLM.from_pretrained("apple/OpenELM-1_1B-Instruct", trust_remote_code=True)\nopenelm_3b_instruct = AutoModelForCausalLM.from_pretrained("apple/OpenELM-3B-Instruct", trust_remote_code=True)\n\n```\n\n## Usage\n\nWe have provided an example function to generate output from OpenELM models loaded via [HuggingFace Hub](https://huggingface.co/docs/hub/) in `generate_openelm.py`.\n\nYou can try the model by running the following command:\n```\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt ''Once upon a time there was'' --generate_kwargs repetition_penalty=1.2\n```\nPlease refer to [this link](https://huggingface.co/docs/hub/security-tokens) to obtain your hugging face access token.\n\nAdditional arguments to the hugging face generate function can be passed via `generate_kwargs`. As an example, to speedup the inference, you can try [lookup token speculative generation](https://huggingface.co/docs/transformers/generation_strategies) by passing the `prompt_lookup_num_tokens` argument as follows:\n```\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt ''Once upon a time there was'' --generate_kwargs repetition_penalty=1.2 prompt_lookup_num_tokens=10\n```\nAlternatively, try model-wise speculative generation with an [assistive model](https://huggingface.co/blog/assisted-generation) by passing a smaller model through the `assistant_model` argument, for example:\n```\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt ''Once upon a time there was'' --generate_kwargs repetition_penalty=1.2 --assistant_model [SMALLER_MODEL_NAME]\n```\n\n\n## Main Results\n\n### Zero-Shot\n\n| **Model Size**                                                              | **ARC-c** | **ARC-e** | **BoolQ** | **HellaSwag** | **PIQA**  | **SciQ**  | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|-----------|-----------|---------------|-----------|-----------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 26.45     | 45.08     | **53.98** | 46.71         | 69.75     | **84.70** | **53.91**      | 54.37       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **30.55** | **46.68** | 48.56     | **52.07**     | **70.78** | 84.40     | 52.72          | **55.11**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 27.56     | 48.06     | 55.78     | 53.97         | 72.31     | 87.20     | 58.01          | 57.56       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **30.38** | **50.00** | **60.37** | **59.34**     | **72.63** | **88.00** | **58.96**      | **59.95**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 32.34     | **55.43** | 63.58     | 64.81         | **75.57** | **90.60** | 61.72          | 63.44       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **37.97** | 52.23     | **70.00** | **71.20**     | 75.03     | 89.30     | **62.75**      | **65.50**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 35.58     | 59.89     | 67.40     | 72.44         | 78.24     | **92.70** | 65.51          | 67.39       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **39.42** | **61.74** | **68.17** | **76.36**     | **79.00** | 92.50     | **66.85**      | **69.15**   |\n\n### LLM360\n\n| **Model Size**                                                              | **ARC-c** | **HellaSwag** | **MMLU**  | **TruthfulQA** | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|---------------|-----------|----------------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 27.65     | 47.15         | 25.72     | **39.24**      | **53.83**      | 38.72       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **32.51** | **51.58**     | **26.70** | 38.72          | 53.20          | **40.54**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 30.20     | 53.86         | **26.01** | 40.18          | 57.22          | 41.50       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **33.53** | **59.31**     | 25.41     | **40.48**      | **58.33**      | **43.41**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 36.69     | 65.71         | **27.05** | 36.98          | 63.22          | 45.93       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **41.55** | **71.83**     | 25.65     | **45.95**      | **64.72**      | **49.94**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 42.24     | 73.28         | **26.76** | 34.98          | 67.25          | 48.90       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **47.70** | **76.87**     | 24.80     | **38.76**      | **67.96**      | **51.22**   |\n\n\n### OpenLLM Leaderboard\n\n| **Model Size**                                                              | **ARC-c** | **CrowS-Pairs** | **HellaSwag** | **MMLU**  | **PIQA**  | **RACE**  | **TruthfulQA** | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|-----------------|---------------|-----------|-----------|-----------|----------------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 27.65     | **66.79**       | 47.15         | 25.72     | 69.75     | 30.91     | **39.24**      | **53.83**      | 45.13       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **32.51** | 66.01           | **51.58**     | **26.70** | **70.78** | 33.78     | 38.72          | 53.20          | **46.66**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 30.20     | **68.63**       | 53.86         | **26.01** | 72.31     | 33.11     | 40.18          | 57.22          | 47.69       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **33.53** | 67.44           | **59.31**     | 25.41     | **72.63** | **36.84** | **40.48**      | **58.33**      | **49.25**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 36.69     | **71.74**       | 65.71         | **27.05** | **75.57** | 36.46     | 36.98          | 63.22          | 51.68       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **41.55** | 71.02           | **71.83**     | 25.65     | 75.03     | **39.43** | **45.95**      | **64.72**      | **54.40**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 42.24     | **73.29**       | 73.28         | **26.76** | 78.24     | **38.76** | 34.98          | 67.25          | 54.35       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **47.70** | 72.33           | **76.87**     | 24.80     | **79.00** | 38.47     | **38.76**      | **67.96**      | **55.73**   |\n\nSee the technical report for more results and comparison.\n\n## Evaluation\n\n### Setup\n\nInstall the following dependencies:\n\n```bash\n\n# install public lm-eval-harness\n\nharness_repo="public-lm-eval-harness"\ngit clone https://github.com/EleutherAI/lm-evaluation-harness ${harness_repo}\ncd ${harness_repo}\n# use main branch on 03-15-2024, SHA is dc90fec\ngit checkout dc90fec\npip install -e .\ncd ..\n\n# 66d6242 is the main branch on 2024-04-01 \npip install datasets@git+https://github.com/huggingface/datasets.git@66d6242\npip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0\n\n```\n\n### Evaluate OpenELM\n\n```bash\n\n# OpenELM-270M\nhf_model=apple/OpenELM-270M\n\n# this flag is needed because lm-eval-harness set add_bos_token to False by default, but OpenELM uses LLaMA tokenizer which requires add_bos_token to be True\ntokenizer=meta-llama/Llama-2-7b-hf\nadd_bos_token=True\nbatch_size=1\n\nmkdir lm_eval_output\n\nshot=0\ntask=arc_challenge,arc_easy,boolq,hellaswag,piqa,race,winogrande,sciq,truthfulqa_mc2\nlm_eval --model hf \\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\n        --tasks ${task} \\n        --device cuda:0 \\n        --num_fewshot ${shot} \\n        --output_path ./lm_eval_output/${hf_model//\//_}_${task//,/_}-${shot}shot \\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\//_}_${task//,/_}-${shot}shot.log\n\nshot=5\ntask=mmlu,winogrande\nlm_eval --model hf \\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\n        --tasks ${task} \\n        --device cuda:0 \\n        --num_fewshot ${shot} \\n        --output_path ./lm_eval_output/${hf_model//\//_}_${task//,/_}-${shot}shot \\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\//_}_${task//,/_}-${shot}shot.log\n\nshot=25\ntask=arc_challenge,crows_pairs_english\nlm_eval --model hf \\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\n        --tasks ${task} \\n        --device cuda:0 \\n        --num_fewshot ${shot} \\n        --output_path ./lm_eval_output/${hf_model//\//_}_${task//,/_}-${shot}shot \\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\//_}_${task//,/_}-${shot}shot.log\n\nshot=10\ntask=hellaswag\nlm_eval --model hf \\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\n        --tasks ${task} \\n        --device cuda:0 \\n        --num_fewshot ${shot} \\n        --output_path ./lm_eval_output/${hf_model//\//_}_${task//,/_}-${shot}shot \\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\//_}_${task//,/_}-${shot}shot.log\n\n```\n\n\n## Bias, Risks, and Limitations\n\nThe release of OpenELM models aims to empower and enrich the open research community by providing access to state-of-the-art language models. Trained on publicly available datasets, these models are made available without any safety guarantees. Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts. Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.\n\n## Citation\n\nIf you find our work useful, please cite:\n\n```BibTex \n@article{mehtaOpenELMEfficientLanguage2024,\n	title = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\n	shorttitle = {{OpenELM}},\n	url = {https://arxiv.org/abs/2404.14619v1},\n	language = {en},\n	urldate = {2024-04-24},\n	journal = {arXiv.org},\n	author = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\n	month = apr,\n	year = {2024},\n}\n\n@inproceedings{mehta2022cvnets, \n     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, \n     title = {CVNets: High Performance Library for Computer Vision}, \n     year = {2022}, \n     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, \n     series = {MM ''22} \n}\n```\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":null,"files_count":4,"spaces_count":4,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:apple:corenet","source_url":"https://github.com/apple/corenet"},{"type":"has_code","target_id":"github:EleutherAI:lm-evaluation-harness","source_url":"https://github.com/EleutherAI/lm-evaluation-harness"},{"type":"has_code","target_id":"github:huggingface:datasets.git@66d6242","source_url":"https://github.com/huggingface/datasets.git@66d6242"},{"type":"based_on_paper","target_id":"arxiv:2404.14619","source_url":"https://arxiv.org/abs/2404.14619"}]', NULL, 'Other', 'approved', 80, 'b4e668433f27fa2d76b9d90e1ea77f86', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Salesforce-blip-image-captioning-large', 'huggingface--salesforce--blip-image-captioning-large', 'blip-image-captioning-large', 'Salesforce', '--- pipeline_tag: image-to-text tags: - image-captioning languages: - en license: bsd-3-clause --- Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone). | !BLIP.gif | |:--:| | <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>| Authors from the paper write in the abstract: *Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-tr...', '["transformers","pytorch","tf","safetensors","blip","image-to-text","image-captioning","arxiv:2201.12086","license:bsd-3-clause","endpoints_compatible","region:us"]', 'image-to-text', 1437, 1081448, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Salesforce/blip-image-captioning-large","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-to-text\ntags:\n- image-captioning\nlanguages:\n- en\nlicense: bsd-3-clause\n---\n\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\nModel card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone).\n\n| ![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif) |\n|:--:|\n| <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>|\n\n## TL;DR\n\nAuthors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:\n\n*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n\n## Usage\n\nYou can use this model for conditional and un-conditional image captioning\n\n### Using the Pytorch model\n\n#### Running the model on CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n```\n</details>\n\n#### Running the model on GPU\n\n##### In full precision \n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to("cuda")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt").to("cuda")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt").to("cuda")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n```\n</details>\n\n##### In half precision (`float16`)\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large", torch_dtype=torch.float16).to("cuda")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt").to("cuda", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt").to("cuda", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n## Ethical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\n\n## BibTex and citation info\n\n```\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":469733436,"storage_bytes":7519815588,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BlipForConditionalGeneration"],"model_type":"blip","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:salesforce:BLIP","source_url":"https://github.com/salesforce/BLIP"},{"type":"based_on_paper","target_id":"arxiv:2201.12086","source_url":"https://arxiv.org/abs/2201.12086"}]', NULL, 'BSD-3-Clause', 'approved', 65, '6f8b221fc69886f151bda5f18470dcb8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Wan-AI-Wan2.1-T2V-14B', 'huggingface--wan-ai--wan2.1-t2v-14b', 'Wan2.1-T2V-14B', 'Wan-AI', '--- license: apache-2.0 language: - en - zh pipeline_tag: text-to-video tags: - video generation library_name: diffusers inference: parameters: num_inference_steps: 10 --- <p align="center"> <img src="assets/logo.png" width="400"/> <p> <p align="center"> 💜 <a href=""><b>Wan</b></a> &nbsp&nbsp ｜ &nbsp&nbsp 🖥️ <a href="https://github.com/Wan-Video/Wan2.1">GitHub</a> &nbsp&nbsp | &nbsp&nbsp🤗 <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href="https://mo...', '["diffusers","safetensors","t2v","video generation","text-to-video","en","zh","license:apache-2.0","region:us"]', 'text-to-video', 1431, 29744, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Wan-AI/Wan2.1-T2V-14B","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: text-to-video\ntags:\n- video generation\nlibrary_name: diffusers\ninference:\n  parameters:\n    num_inference_steps: 10\n---\n# Wan2.1\n\n<p align="center">\n    <img src="assets/logo.png" width="400"/>\n<p>\n\n<p align="center">\n    💜 <a href=""><b>Wan</b></a> &nbsp&nbsp ｜ &nbsp&nbsp 🖥️ <a href="https://github.com/Wan-Video/Wan2.1">GitHub</a> &nbsp&nbsp  | &nbsp&nbsp🤗 <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href="">Paper (Coming soon)</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href="https://wanxai.com">Blog</a> &nbsp&nbsp | &nbsp&nbsp💬 <a href="https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg">WeChat Group</a>&nbsp&nbsp | &nbsp&nbsp 📖 <a href="https://discord.gg/p5XbdQV7">Discord</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**]("") <be>\n\nIn this repository, we present **Wan2.1**, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. **Wan2.1** offers these key features:\n- 👍 **SOTA Performance**: **Wan2.1** consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.\n- 👍 **Supports Consumer-grade GPUs**: The T2V-1.3B model requires only 8.19 GB VRAM, making it compatible with almost all consumer-grade GPUs. It can generate a 5-second 480P video on an RTX 4090 in about 4 minutes (without optimization techniques like quantization). Its performance is even comparable to some closed-source models.\n- 👍 **Multiple Tasks**: **Wan2.1** excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.\n- 👍 **Visual Text Generation**: **Wan2.1** is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.\n- 👍 **Powerful Video VAE**: **Wan-VAE** delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.\n\nThis repository features our T2V-14B model, which establishes a new SOTA performance benchmark among both open-source and closed-source models. It demonstrates exceptional capabilities in generating high-quality visuals with significant motion dynamics. It is also the only video model capable of producing both Chinese and English text and supports video generation at both 480P and 720P resolutions.\n\n\n## Video Demos\n\n<div align="center">\n    <video width="80%" controls>\n        <source src="https://cloud.video.taobao.com/vod/Jth64Y7wNoPcJki_Bo1ZJTDBvNjsgjlVKsNs05Fqfps.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## 🔥 Latest News!!\n\n* Feb 22, 2025: 👋 We''ve released the inference code and weights of Wan2.1.\n\n\n## 📑 Todo List\n- Wan2.1 Text-to-Video\n    - [x] Multi-GPU Inference code of the 14B and 1.3B models\n    - [x] Checkpoints of the 14B and 1.3B models\n    - [x] Gradio demo\n    - [ ] Diffusers integration\n    - [ ] ComfyUI integration\n- Wan2.1 Image-to-Video\n    - [x] Multi-GPU Inference code of the 14B model\n    - [x] Checkpoints of the 14B model\n    - [x] Gradio demo\n    - [ ] Diffusers integration\n    - [ ] ComfyUI integration\n\n\n## Quickstart\n\n#### Installation\nClone the repo:\n```\ngit clone https://github.com/Wan-Video/Wan2.1.git\ncd Wan2.1\n```\n\nInstall dependencies:\n```\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\n\n\n#### Model Download\n\n| Models        |                       Download Link                                           |    Notes                      |\n| --------------|-------------------------------------------------------------------------------|-------------------------------|\n| T2V-14B       |      🤗 [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B)      🤖 [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B)          | Supports both 480P and 720P\n| I2V-14B-720P  |      🤗 [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P)    🤖 [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P)     | Supports 720P\n| I2V-14B-480P  |      🤗 [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P)    🤖 [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P)      | Supports 480P\n| T2V-1.3B      |      🤗 [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B)     🤖 [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B)         | Supports 480P\n\n> 💡Note: The 1.3B model is capable of generating videos at 720P resolution. However, due to limited training at this resolution, the results are generally less stable compared to 480P. For optimal performance, we recommend using 480P resolution.\n\n\nDownload models using 🤗 huggingface-cli:\n```\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.1-T2V-14B --local-dir ./Wan2.1-T2V-14B\n```\n\nDownload models using 🤖 modelscope-cli:\n```\npip install modelscope\nmodelscope download Wan-AI/Wan2.1-T2V-14B --local_dir ./Wan2.1-T2V-14B\n```\n#### Run Text-to-Video Generation\n\nThis repository supports two Text-to-Video models (1.3B and 14B) and two resolutions (480P and 720P). The parameters and configurations for these models are as follows:\n\n<table>\n    <thead>\n        <tr>\n            <th rowspan="2">Task</th>\n            <th colspan="2">Resolution</th>\n            <th rowspan="2">Model</th>\n        </tr>\n        <tr>\n            <th>480P</th>\n            <th>720P</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>t2v-14B</td>\n            <td style="color: green;">✔️</td>\n            <td style="color: green;">✔️</td>\n            <td>Wan2.1-T2V-14B</td>\n        </tr>\n        <tr>\n            <td>t2v-1.3B</td>\n            <td style="color: green;">✔️</td>\n            <td style="color: red;">❌</td>\n            <td>Wan2.1-T2V-1.3B</td>\n        </tr>\n    </tbody>\n</table>\n\n\n##### (1) Without Prompt Extention\n\nTo facilitate implementation, we will start with a basic version of the inference process that skips the [prompt extension](#2-using-prompt-extention) step.\n\n- Single-GPU inference\n\n```\npython generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage."\n```\n\nIf you encounter OOM (Out-of-Memory) issues, you can use the `--offload_model True` and `--t5_cpu` options to reduce GPU memory usage. For example, on an RTX 4090 GPU:\n\n```\npython generate.py  --task t2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --offload_model True --t5_cpu --sample_shift 8 --sample_guide_scale 6 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage."\n```\n\n> 💡Note: If you are using the `T2V-1.3B` model, we recommend setting the parameter `--sample_guide_scale 6`. The `--sample_shift parameter` can be adjusted within the range of 8 to 12 based on the performance.\n\n\n- Multi-GPU inference using FSDP + xDiT USP\n\n```\npip install "xfuser>=0.4.1"\ntorchrun --nproc_per_node=8 generate.py --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage."\n```\n\n\n##### (2) Using Prompt Extention\n\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\n\n- Use the Dashscope API for extension.\n  - Apply for a `dashscope.api_key` in advance ([EN](https://www.alibabacloud.com/help/en/model-studio/getting-started/first-api-call-to-qwen) | [CN](https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen)).\n  - Configure the environment variable `DASH_API_KEY` to specify the Dashscope API key. For users of Alibaba Cloud''s international site, you also need to set the environment variable `DASH_API_URL` to ''https://dashscope-intl.aliyuncs.com/api/v1''. For more detailed instructions, please refer to the [dashscope document](https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api?spm=a2c63.p38356.0.i1).\n  - Use the `qwen-plus` model for text-to-video tasks and `qwen-vl-max` for image-to-video tasks.\n  - You can modify the model used for extension with the parameter `--prompt_extend_model`. For example:\n```\nDASH_API_KEY=your_key python generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage" --use_prompt_extend --prompt_extend_method ''dashscope'' --prompt_extend_target_lang ''ch''\n```\n\n- Using a local model for extension.\n\n  - By default, the Qwen model on HuggingFace is used for this extension. Users can choose based on the available GPU memory size.\n  - For text-to-video tasks, you can use models like `Qwen/Qwen2.5-14B-Instruct`, `Qwen/Qwen2.5-7B-Instruct` and `Qwen/Qwen2.5-3B-Instruct`\n  - For image-to-video tasks, you can use models like `Qwen/Qwen2.5-VL-7B-Instruct` and `Qwen/Qwen2.5-VL-3B-Instruct`.\n  - Larger models generally provide better extension results but require more GPU memory.\n  - You can modify the model used for extension with the parameter `--prompt_extend_model` , allowing you to specify either a local model path or a Hugging Face model. For example:\n\n```\npython generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage" --use_prompt_extend --prompt_extend_method ''local_qwen'' --prompt_extend_target_lang ''ch''\n```\n\n##### (3) Runing local gradio\n\n```\ncd gradio\n# if one uses dashscope’s API for prompt extension\nDASH_API_KEY=your_key python t2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir ./Wan2.1-T2V-14B\n\n# if one uses a local model for prompt extension\npython t2v_14B_singleGPU.py --prompt_extend_method ''local_qwen'' --ckpt_dir ./Wan2.1-T2V-14B\n```\n\n\n## Manual Evaluation\n\n\nThrough manual evaluation, the results generated after prompt extension are superior to those from both closed-source and open-source models.\n\n<div align="center">\n    <img src="assets/t2v_res.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.1** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align="center">\n    <img src="assets/comp_effic.png" alt="" style="width: 80%;" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) For the 1.3B model on 8 GPUs, set `--ring_size 8` and `--ulysses_size 1`;\n> (2) For the 14B model on 1 GPU, use `--offload_model True`;\n> (3) For the 1.3B model on a single 4090 GPU, set `--offload_model True --t5_cpu`;\n> (4) For all testings, no prompt extension was applied, meaning `--use_prompt_extend` was not enabled.\n\n\n## Community Contributions\n- [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) provides more support for Wan, including video-to-video, FP8 quantization, VRAM optimization, LoRA training, and more. Please refer to [their examples](https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/wanvideo).\n\n-------\n\n## Introduction of Wan2.1\n\n**Wan2.1**  is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. These include our novel spatio-temporal variational autoencoder (VAE), scalable training strategies, large-scale data construction, and automated evaluation metrics. Collectively, these contributions enhance the model’s performance and versatility.\n\n\n##### (1) 3D Variational Autoencoders\nWe propose a novel 3D causal VAE architecture, termed **Wan-VAE** specifically designed for video generation. By combining multiple strategies, we improve spatio-temporal compression, reduce memory usage, and ensure temporal causality. **Wan-VAE** demonstrates significant advantages in performance efficiency compared to other open-source VAEs. Furthermore, our **Wan-VAE** can encode and decode unlimited-length 1080P videos without losing historical temporal information, making it particularly well-suited for video generation tasks.\n\n\n<div align="center">\n    <img src="assets/video_vae_res.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n##### (2) Video Diffusion DiT\n\n**Wan2.1** is designed using the Flow Matching framework within the paradigm of mainstream Diffusion Transformers. Our model''s architecture uses the T5 Encoder to encode multilingual text input, with cross-attention in each transformer block embedding the text into the model structure. Additionally, we employ an MLP with a Linear layer and a SiLU layer to process the input time embeddings and predict six modulation parameters individually. This MLP is shared across all transformer blocks, with each block learning a distinct set of biases. Our experimental findings reveal a significant performance improvement with this approach at the same parameter scale.\n\n<div align="center">\n    <img src="assets/video_dit_arch.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n| Model  | Dimension | Input Dimension | Output Dimension | Feedforward Dimension | Frequency Dimension | Number of Heads | Number of Layers |\n|--------|-----------|-----------------|------------------|-----------------------|---------------------|-----------------|------------------|\n| 1.3B   | 1536      | 16              | 16               | 8960                  | 256                 | 12              | 30               |\n| 14B   | 5120       | 16              | 16               | 13824                 | 256                 | 40              | 40               |\n\n\n\n##### Data\n\nWe curated and deduplicated a candidate dataset comprising a vast amount of image and video data. During the data curation process, we designed a four-step data cleaning process, focusing on fundamental dimensions, visual quality and motion quality. Through the robust data processing pipeline, we can easily obtain high-quality, diverse, and large-scale training sets of images and videos.\n\n![figure1](assets/data_for_diff_stage.jpg "figure1")\n\n\n##### Comparisons to SOTA\nWe compared **Wan2.1** with leading open-source and closed-source models to evaluate the performace. Using our carefully designed set of 1,035 internal prompts, we tested across 14 major dimensions and 26 sub-dimensions. We then compute the total score by performing a weighted calculation on the scores of each dimension, utilizing weights derived from human preferences in the matching process. The detailed results are shown in the table below. These results demonstrate our model''s superior performance compared to both open-source and closed-source models.\n\n![figure1](assets/vben_vs_sota.png "figure1")\n\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2.1,\n    title   = {Wan: Open and Advanced Large-Scale Video Generative Models},\n    author  = {Wan Team},\n    journal = {},\n    year    = {2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/p5XbdQV7) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":149464228510,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"model_type":"t2v"}}', '[]', '[{"type":"has_code","target_id":"github:Wan-Video:Wan2.1\">GitHub<","source_url":"https://github.com/Wan-Video/Wan2.1\">GitHub<"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1.git","source_url":"https://github.com/Wan-Video/Wan2.1.git"},{"type":"has_code","target_id":"github:modelscope:DiffSynth-Studio","source_url":"https://github.com/modelscope/DiffSynth-Studio"},{"type":"has_code","target_id":"github:modelscope:DiffSynth-Studio","source_url":"https://github.com/modelscope/DiffSynth-Studio"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"}]', NULL, 'Apache-2.0', 'approved', 100, '51306731f30eb89079798bdb5c211294', NULL, 'https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/resolve/main/assets/comp_effic.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Wan-AI-Wan2.1-T2V-14B from https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/resolve/main/assets/comp_effic.png
Image converted to WebP: data/images/huggingface-Wan-AI-Wan2.1-T2V-14B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-monster-labs-control-v1p-sd15-qrcode-monster', 'huggingface--monster-labs--control-v1p-sd15-qrcode-monster', 'control_v1p_sd15_qrcode_monster', 'monster-labs', '--- tags: - stable-diffusion - controlnet - qrcode license: openrail++ language: - en --- !QR code in shape of a blue monster, reading "https://qrcode.monster" This model is made to generate creative QR codes that still scan. Keep in mind that not all generated codes might be readable, but you can try different parameters and prompts to get the desired results. **NEW VERSION** Introducing the upgraded version of our model - Controlnet QR code Monster v2. V2 is a huge upgrade over v1, for scan...', '["diffusers","safetensors","stable-diffusion","controlnet","qrcode","en","license:openrail++","region:us"]', 'other', 1426, 26107, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- stable-diffusion\n- controlnet\n- qrcode\nlicense: openrail++\nlanguage:\n- en\n---\n# Controlnet QR Code Monster v2 For SD-1.5\n\n![QR code in shape of a blue monster, reading "https://qrcode.monster"](images/monster.png)\n\n##  Model Description\n\nThis model is made to generate creative QR codes that still scan.\nKeep in mind that not all generated codes might be readable, but you can try different parameters and prompts to get the desired results.\n\n**NEW VERSION**\n\nIntroducing the upgraded version of our model - Controlnet QR code Monster v2.\nV2 is a huge upgrade over v1, for scannability AND creativity.\n\nQR codes can now seamlessly blend the image by using a gray-colored background (#808080).\n\nAs with the former version, the readability of some generated codes may vary, however playing around with parameters and prompts could yield better results.\n\nYou can find in in the `v2/` subfolder.\n\n## How to Use\n\n- **Condition**: QR codes are passed as condition images with a module size of 16px. Use a higher error correction level to make it easier to read (sometimes a lower level can be easier to read if smaller in size). Use a gray background for the rest of the image to make the code integrate better.\n\n- **Prompts**: Use a prompt to guide the QR code generation. The output will highly depend on the given prompt. Some seem to be really easily accepted by the qr code process, some will require careful tweaking to get good results.\n\n- **Controlnet guidance scale**: Set the controlnet guidance scale value:\n   - High values: The generated QR code will be more readable.\n   - Low values: The generated QR code will be more creative.\n\n### Tips\n\n- For an optimally readable output, try generating multiple QR codes with similar parameters, then choose the best ones.\n\n- Use the Image-to-Image feature to improve the readability of a generated QR code:\n  - Decrease the denoising strength to retain more of the original image.\n  - Increase the controlnet guidance scale value for better readability.\n  A typical workflow for "saving" a code would be :\n  Max out the guidance scale and minimize the denoising strength, then bump the strength until the code scans.\n\n## Example Outputs\n\nHere are some examples of creative, yet scannable QR codes produced by our model:\n\n![City ruins with a building facade in shape of a QR code, reading "https://qrcode.monster"](images/architecture.png)\n![QR code in shape of a tree, reading "https://qrcode.monster"](images/tree.png)\n![A gothic sculpture in shape of a QR code, reading "https://qrcode.monster"](images/skulls.png)\n\nFeel free to experiment with prompts, parameters, and the Image-to-Image feature to achieve the desired QR code output. Good luck and have fun!', '{"pipeline_tag":null,"library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":8689186346,"files_count":16,"spaces_count":100,"gated":false,"private":false,"config":{}}', '[]', '[]', NULL, 'OpenRAIL++', 'approved', 85, '2c3e07e750fb3fa232246e98d9aadc3d', NULL, 'https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster/resolve/main/images/architecture.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-monster-labs-control-v1p-sd15-qrcode-monster from https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster/resolve/main/images/architecture.png
Image converted to WebP: data/images/huggingface-monster-labs-control-v1p-sd15-qrcode-monster.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-xl-base-0.9', 'huggingface--stabilityai--stable-diffusion-xl-base-0.9', 'stable-diffusion-xl-base-0.9', 'stabilityai', '', '["diffusers","safetensors","text-to-image","stable-diffusion","arxiv:2108.01073","arxiv:2112.10752","arxiv:2307.01952","license:other","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 1410, 226, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":55508753189,"files_count":37,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2108.01073","source_url":"https://arxiv.org/abs/2108.01073"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2307.01952","source_url":"https://arxiv.org/abs/2307.01952"}]', NULL, 'Other', 'approved', 60, 'cd120d242da5110c2618c81d461ef8b3', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/resolve/main/pipeline.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-xl-base-0.9 from https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/resolve/main/pipeline.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-MiniCPM-Llama3-V-2-5', 'huggingface--openbmb--minicpm-llama3-v-2-5', 'MiniCPM-Llama3-V-2_5', 'openbmb', '--- pipeline_tag: image-text-to-text language: - multilingual datasets: - openbmb/RLAIF-V-Dataset library_name: transformers tags: - minicpm-v - vision - ocr - custom_code --- <h1>A GPT-4V Level Multimodal LLM on Your Phone</h1> GitHub | Demo | <a href="https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/wechat.md" target="_blank"> WeChat</a> * [2025.01.14] 🔥🔥 🔥 We open source **MiniCPM-o 2.6**, with significant performance improvement over **MiniCPM-V 2.6**, and support real-time speech-t...', '["transformers","safetensors","minicpmv","feature-extraction","minicpm-v","vision","ocr","custom_code","image-text-to-text","conversational","multilingual","dataset:openbmb/rlaif-v-dataset","region:us"]', 'image-text-to-text', 1404, 56360, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\npipeline_tag: image-text-to-text\nlanguage:\n- multilingual\ndatasets:\n- openbmb/RLAIF-V-Dataset\nlibrary_name: transformers\ntags:\n- minicpm-v\n- vision\n- ocr\n- custom_code\n---\n\n\n<h1>A GPT-4V Level Multimodal LLM on Your Phone</h1>\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-V) | [Demo](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5) | <a href="https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/wechat.md" target="_blank"> WeChat</a> \n\n\n## News <!-- omit in toc -->\n\n#### 📌 Pinned\n\n* [2025.01.14] 🔥🔥 🔥 We open source [**MiniCPM-o 2.6**](https://huggingface.co/openbmb/MiniCPM-o-2_6), with significant performance improvement over **MiniCPM-V 2.6**, and support real-time speech-to-speech conversation and multimodal live streaming. Try it now. \n\n* [2024.08.10] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf).\n* [2024.08.06] 🔥🔥🔥 We open-source [**MiniCPM-V 2.6**](https://huggingface.co/openbmb/MiniCPM-V-2_6), which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!\n* [2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/docs/MiniCPM_Llama3_V_25_technical_report.pdf).\n* [2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See [here](https://github.com/OpenBMB/MiniCPM-V/tree/main?tab=readme-ov-file#vllm).\n* [2024.05.28] 💫 We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics).\n* [2024.05.23] 🔥🔥🔥 MiniCPM-V tops GitHub Trending and HuggingFace Trending! Our demo, recommended by Hugging Face Gradio’s official account, is available [here](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5). Come and try it out!\n* [2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide [efficient inference](#deployment-on-mobile-phone) and [simple fine-tuning](https://github.com/OpenBMB/MiniCPM-V/blob/main/finetune/readme.md). Try it now!\n\n\n## Model Summary\n\n**MiniCPM-Llama3-V 2.5** is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.0. Notable features of MiniCPM-Llama3-V 2.5 include:\n\n- 🔥 **Leading Performance.**\n  MiniCPM-Llama3-V 2.5 has achieved an average score of 65.1 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4V-1106, Gemini Pro, Claude 3 and Qwen-VL-Max** and greatly outperforms other Llama 3-based MLLMs.\n\n- 💪 **Strong OCR Capabilities.**\n  MiniCPM-Llama3-V 2.5 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), achieving an **700+ score on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V-0409, Qwen-VL-Max and Gemini Pro**. Based on recent user feedback, MiniCPM-Llama3-V 2.5 has now enhanced full-text OCR extraction, table-to-markdown conversion, and other high-utility capabilities, and has further strengthened its instruction-following and complex reasoning abilities, enhancing multimodal interaction experiences.\n\n- 🏆 **Trustworthy Behavior.**\n  Leveraging the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) method (the newest technology in the [RLHF-V](https://github.com/RLHF-V) [CVPR''24] series), MiniCPM-Llama3-V 2.5 exhibits more trustworthy behavior. It achieves **10.3%** hallucination rate on Object HalBench, lower than GPT-4V-1106 (13.6%), achieving the best-level performance within the open-source community. [Data released](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset).\n\n- 🌏 **Multilingual Support.**\n  Thanks to the strong multilingual capabilities of Llama 3 and the cross-lingual generalization technique from [VisCPM](https://github.com/OpenBMB/VisCPM), MiniCPM-Llama3-V 2.5 extends its bilingual (Chinese-English) multimodal capabilities to **over 30 languages including German, French, Spanish, Italian, Korean, Japanese etc.** [All Supported Languages](./assets/minicpm-llama-v-2-5_languages.md).\n\n- 🚀 **Efficient Deployment.**\n  MiniCPM-Llama3-V 2.5 systematically employs **model quantization, CPU optimizations, NPU optimizations and compilation optimizations**, achieving high-efficiency deployment on edge devices. For mobile phones with Qualcomm chips, we have integrated the NPU acceleration framework QNN into llama.cpp for the first time. After systematic optimization, MiniCPM-Llama3-V 2.5 has realized a **150-fold acceleration in multimodal large model end-side image encoding** and a **3-fold increase in language decoding speed**.\n\n-  💫  **Easy Usage.**\n  MiniCPM-Llama3-V 2.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md) and [ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5) support for efficient CPU inference on local devices, (2) [GGUF](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf) format quantized models in 16 sizes, (3) efficient [LoRA](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#lora-finetuning) fine-tuning with only 2 V100 GPUs, (4) [streaming output](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage), (5) quick local WebUI demo setup with [Gradio](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_2.5.py) and [Streamlit](https://github.com/OpenBMB/MiniCPM-V/blob/main/web_demo_streamlit-2_5.py), and (6) interactive demos on [HuggingFace Spaces](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5).\n\n### Evaluation <!-- omit in toc -->\n\nResults on TextVQA, DocVQA, OCRBench, OpenCompass MultiModal Avg , MME, MMBench, MMMU, MathVista, LLaVA Bench, RealWorld QA, Object HalBench.\n\n<div align="center">\n    <img src="https://cdn-uploads.huggingface.co/production/uploads/64abc4aa6cadc7aca585dddf/v2KE3wqQgM05ZW3dH2wbx.png" width="110%" />\n</div>\n\n\nEvaluation results of multilingual LLaVA Bench \n<div align="center">\n    <img src="assets/minicpmv-llama3-v2.5/llavabench_compare.png" width="110%" />\n</div>\n\n\n### Examples <!-- omit in toc -->\n\n<table align="center">\n    <p align="center">\n      <img src="assets/minicpmv-llama3-v2.5/cases_all.png" width=95%/>\n    </p>\n</table>\n\nWe deploy MiniCPM-Llama3-V 2.5 on end devices. The demo video is the raw screen recording on a Xiaomi 14 Pro without edition.\n\n<table align="center">\n    <p align="center">\n      <img src="assets/gif_cases/ticket.gif" width=40% style="display:inline-block;"/>\n      <img src="assets/gif_cases/meal_plan.gif" width=40% style="display:inline-block;"/>\n    </p>\n</table>\n\n<table align="center">\n    <p align="center">\n      <img src="assets/gif_cases/1-4.gif" width=80%/>\n    </p>\n</table>\n\n\n\n## Demo\nClick here to try out the Demo of [MiniCPM-Llama3-V 2.5](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5).\n\n## Deployment on Mobile Phone\nComing soon.\n\n## Usage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.10：\n```\nPillow==10.1.0\ntorch==2.1.2\ntorchvision==0.16.2\ntransformers==4.40.0\nsentencepiece==0.1.99\n```\n\n```python\n# test.py\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-Llama3-V-2_5'', trust_remote_code=True, torch_dtype=torch.float16)\nmodel = model.to(device=''cuda'')\n\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-Llama3-V-2_5'', trust_remote_code=True)\nmodel.eval()\n\nimage = Image.open(''xx.jpg'').convert(''RGB'')\nquestion = ''What is in the image?''\nmsgs = [{''role'': ''user'', ''content'': question}]\n\nres = model.chat(\n    image=image,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True, # if sampling=False, beam_search will be used by default\n    temperature=0.7,\n    # system_prompt='''' # pass system_prompt if needed\n)\nprint(res)\n\n## if you want to use streaming, please make sure sampling=True and stream=True\n## the model.chat will return a generator\nres = model.chat(\n    image=image,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    temperature=0.7,\n    stream=True\n)\n\ngenerated_text = ""\nfor new_text in res:\n    generated_text += new_text\n    print(new_text, flush=True, end='''')\n```\n\nPlease look at [GitHub](https://github.com/OpenBMB/MiniCPM-V) for more detail about usage.\n\n\n## Inference with llama.cpp<a id="llamacpp"></a>\nMiniCPM-Llama3-V 2.5 can run with llama.cpp now! See our fork of [llama.cpp](https://github.com/OpenBMB/llama.cpp/tree/minicpm-v2.5/examples/minicpmv) for more detail.\n\n\n## Int4 quantized version\nDownload the int4 quantized version for lower GPU memory (8GB) usage:  [MiniCPM-Llama3-V-2_5-int4](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4).\n\n## MiniCPM-V 2.0 <!-- omit in toc -->\nPlease see the info about MiniCPM-V 2.0 [here](https://huggingface.co/openbmb/MiniCPM-V-2).\n\n## License\n#### Model License\n* The code in this repo is released under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) License. \n* The usage of MiniCPM-V series model weights must strictly follow [MiniCPM Model License.md](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md).\n* The models and weights of MiniCPM are completely free for academic research. after filling out a ["questionnaire"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g) for registration, are also available for free commercial use.\n\n\n\n#### Statement\n* As an LLM, MiniCPM-Llama3-V 2.5 generates contents by learning a large mount of texts, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-Llama3-V 2.5 does not represent the views and positions of the model developers\n* We will not be liable for any problems arising from the use of the MinCPM-V open Source model, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n## Key Techniques and Other Multimodal Projects\n\n👏 Welcome to explore key techniques of MiniCPM-V 2.6 and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD)  | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n## Citation\n\nIf you find our work helpful, please consider citing our papers 📝 and liking this project ❤️！\n\n```bib\n@article{yao2024minicpmv,\n      title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone}, \n      author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and Chen, Qianyu and Zhou, Huarong and Zou, Zhensheng and Zhang, Haoye and Hu, Shengding and Zheng, Zhi and Zhou, Jie and Cai, Jie and Han, Xu and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},\n      journal={arXiv preprint 2408.01800},\n      year={2024},\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8537092336,"storage_bytes":17139461781,"files_count":34,"spaces_count":32,"gated":false,"private":false,"config":{"architectures":["MiniCPMV"],"auto_map":{"AutoConfig":"configuration_minicpm.MiniCPMVConfig","AutoModel":"modeling_minicpmv.MiniCPMV","AutoModelForCausalLM":"modeling_minicpmv.MiniCPMV"},"model_type":"minicpmv","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\n\n''+ message[''content''] | trim + ''<|eot_id|>'' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ ''<|start_header_id|>assistant<|end_header_id|>\n\n'' }}","eos_token":"<|end_of_text|>","pad_token":"!","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"has_code","target_id":"github:OpenBMB:VisCPM","source_url":"https://github.com/OpenBMB/VisCPM"},{"type":"has_code","target_id":"github:OpenBMB:llama.cpp","source_url":"https://github.com/OpenBMB/llama.cpp"},{"type":"has_code","target_id":"github:OpenBMB:ollama","source_url":"https://github.com/OpenBMB/ollama"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:llama.cpp","source_url":"https://github.com/OpenBMB/llama.cpp"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM","source_url":"https://github.com/OpenBMB/MiniCPM"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM","source_url":"https://github.com/OpenBMB/MiniCPM"},{"type":"has_code","target_id":"github:OpenBMB:VisCPM","source_url":"https://github.com/OpenBMB/VisCPM"},{"type":"has_code","target_id":"github:RLHF-V:RLHF-V","source_url":"https://github.com/RLHF-V/RLHF-V"},{"type":"has_code","target_id":"github:thunlp:LLaVA-UHD","source_url":"https://github.com/thunlp/LLaVA-UHD"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"}]', NULL, NULL, 'pending', 90, 'a42c0cea01698df7bc949c275318e0c7', NULL, 'https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/resolve/main/assets/MiniCPM-Llama3-V-2.5-benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-openbmb-MiniCPM-Llama3-V-2-5 from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/resolve/main/assets/MiniCPM-Llama3-V-2.5-benchmark.png
Image converted to WebP: data/images/huggingface-openbmb-MiniCPM-Llama3-V-2-5.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B', 'huggingface--deepseek-ai--deepseek-r1-distill-qwen-1.5b', 'DeepSeek-R1-Distill-Qwen-1.5B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen2","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1403, 2448724, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1777088000,"storage_bytes":3555721410,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<｜begin▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<｜end▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<｜end▁of▁sentence｜>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<｜User｜>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>'' + tool[''type''] + ''<｜tool▁sep｜>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<｜tool▁call▁end｜>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<｜tool▁call▁begin｜>'' + tool[''type''] + ''<｜tool▁sep｜>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<｜tool▁call▁end｜>''}}{{''<｜tool▁calls▁end｜><｜end▁of▁sentence｜>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<｜tool▁outputs▁end｜>'' + message[''content''] + ''<｜end▁of▁sentence｜>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<｜Assistant｜>'' + content + ''<｜end▁of▁sentence｜>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>'' + message[''content''] + ''<｜tool▁output▁end｜>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<｜tool▁output▁begin｜>'' + message[''content''] + ''<｜tool▁output▁end｜>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<｜tool▁outputs▁end｜>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<｜Assistant｜><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 100, '14aed7216260e7177a7fbf2a7138bc64', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-1.5B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-PaddlePaddle-PaddleOCR-VL', 'huggingface--paddlepaddle--paddleocr-vl', 'PaddleOCR-VL', 'PaddlePaddle', '--- license: apache-2.0 pipeline_tag: image-text-to-text tags: - ERNIE4.5 - PaddleOCR - PaddlePaddle - image-to-text - ocr - document-parse - layout - table - formula - chart base_model: baidu/ERNIE-4.5-0.3B-Paddle language: - en - zh - multilingual library_name: PaddleOCR --- <div align="center"> <h1 align="center"> PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model </h1> **🔥 Official Website**: Baidu AI Studio | **📝 arXiv**: Technical Repor...', '["paddleocr","safetensors","paddleocr_vl","ernie4.5","paddlepaddle","image-to-text","ocr","document-parse","layout","table","formula","chart","image-text-to-text","conversational","custom_code","en","zh","multilingual","arxiv:2510.14528","base_model:baidu/ernie-4.5-0.3b-paddle","base_model:finetune:baidu/ernie-4.5-0.3b-paddle","license:apache-2.0","region:us"]', 'image-text-to-text', 1394, 22486, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/PaddlePaddle/PaddleOCR-VL","fetched_at":"2025-12-08T10:39:52.035Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: image-text-to-text\ntags:\n- ERNIE4.5\n- PaddleOCR\n- PaddlePaddle\n- image-to-text\n- ocr\n- document-parse\n- layout\n- table\n- formula\n- chart\nbase_model: baidu/ERNIE-4.5-0.3B-Paddle\nlanguage:\n- en\n- zh\n- multilingual\nlibrary_name: PaddleOCR\n---\n\n<div align="center">\n\n\n<h1 align="center">\n\nPaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model\n\n</h1>\n\n[![repo](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)\n[![HuggingFace](https://img.shields.io/badge/HuggingFace-black.svg?logo=data:image/png;base64,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&labelColor=white)](https://huggingface.co/PaddlePaddle/PaddleOCR-VL)\n[![ModelScope](https://img.shields.io/badge/ModelScope-black?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&labelColor=white)](https://modelscope.cn/models/PaddlePaddle/PaddleOCR-VL)\n[![HuggingFace](https://img.shields.io/badge/Demo_on_HuggingFace-black.svg?logo=data:image/png;base64,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&labelColor=white)](https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo)\n[![ModelScope](https://img.shields.io/badge/Demo_on_ModelScope-black?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&labelColor=white)](https://modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo/summary)\n[![Discord](https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white)](https://discord.gg/JPmZXDsEEK)\n[![X](https://img.shields.io/badge/X-PaddlePaddle-6080F0)](https://x.com/PaddlePaddle)\n[![License](https://img.shields.io/badge/license-Apache_2.0-green)](./LICENSE)\n\n**🔥 Official Website**: [Baidu AI Studio](https://aistudio.baidu.com/paddleocr) | \n**📝 arXiv**: [Technical Report](https://arxiv.org/pdf/2510.14528)\n\n</div>\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/allmetric.png" width="800"/>\n</div>\n\n## Introduction\n\n**PaddleOCR-VL** is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.\n\n### **Core Features**\n\n1. **Compact yet Powerful VLM Architecture:** We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the model’s recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.\n\n\n2. **SOTA Performance on Document Parsing:** PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.\n\n\n3. **Multilingual Support:** PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.\n\n\n### **Model Architecture** \n\n<!-- PaddleOCR-VL decomposes the complex task of document parsing into a two stages. The first stage, PP-DocLayoutV2, is responsible for layout analysis, where it localizes semantic regions and predicts their reading order. Subsequently, the second stage, PaddleOCR-VL-0.9B, leverages these layout predictions to perform fine-grained recognition of diverse content, including text, tables, formulas, and charts. Finally, a lightweight post-processing module aggregates the outputs from both stages and formats the final document into structured Markdown and JSON. -->\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/paddleocrvl.png" width="800"/>\n</div>\n\n\n## News \n\n* ```2025.11.07``` 🚀 Enabled `flash-attn` in the `transformers` library to achieve faster inference with PaddleOCR-VL-0.9B.\n* ```2025.11.04``` 🌟 PaddleOCR-VL-0.9B is now officially supported on `vLLM` .\n* ```2025.10.29``` 🤗 Supports calling the core module PaddleOCR-VL-0.9B of PaddleOCR-VL via the `transformers` library.\n* ```2025.10.16``` 🚀 We release [PaddleOCR-VL](https://github.com/PaddlePaddle/PaddleOCR), — a multilingual documents parsing via a 0.9B Ultra-Compact Vision-Language Model with SOTA performance.\n\n## Usage    \n\n### Install Dependencies\n\nInstall [PaddlePaddle](https://www.paddlepaddle.org.cn/install/quick) and [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR):\n\n```bash\n# The following command installs the PaddlePaddle version for CUDA 12.6. For other CUDA versions and the CPU version, please refer to https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html\npython -m pip install paddlepaddle-gpu==3.2.1 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\npython -m pip install -U "paddleocr[doc-parser]"\n# For Linux systems, run:\npython -m pip install https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl\n# For Windows systems, run:\npython -m pip install https://xly-devops.cdn.bcebos.com/safetensors-nightly/safetensors-0.6.2.dev0-cp38-abi3-win_amd64.whl\n```\n\n> **Please ensure that you install PaddlePaddle framework version 3.2.1 or above, along with the special version of safetensors.** For macOS users, please use Docker to set up the environment.\n\n\n### Basic Usage\n\nCLI usage:\n\n```bash\npaddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png\n```\n\nPython API usage:\n\n```python\nfrom paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")\nfor res in output:\n    res.print()\n    res.save_to_json(save_path="output")\n    res.save_to_markdown(save_path="output")\n```\n\n### Accelerate VLM Inference via Optimized Inference Servers\n\n1. Start the VLM inference server:\n\n    You can start the vLLM inference service using one of two methods:\n\n    - Method 1: PaddleOCR method\n\n        ```bash\n        docker run \\n            --rm \\n            --gpus all \\n            --network host \\n            ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddleocr-genai-vllm-server:latest \\n            paddleocr genai_server --model_name PaddleOCR-VL-0.9B --host 0.0.0.0 --port 8080 --backend vllm\n        ```\n\n    - Method 2: vLLM method\n\n        [vLLM: PaddleOCR-VL Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/PaddlePaddle/PaddleOCR-VL.html)\n    \n2. Call the PaddleOCR CLI or Python API:\n\n    ```bash\n    paddleocr doc_parser \\n        -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png \\n        --vl_rec_backend vllm-server \\n        --vl_rec_server_url http://127.0.0.1:8080/v1\n    ```\n\n    ```python\n    from paddleocr import PaddleOCRVL\n    pipeline = PaddleOCRVL(vl_rec_backend="vllm-server", vl_rec_server_url="http://127.0.0.1:8080/v1")\n    output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")\n    for res in output:\n        res.print()\n        res.save_to_json(save_path="output")\n        res.save_to_markdown(save_path="output")\n    ```\n  \n**For more usage details and parameter explanations, see the [documentation](https://www.paddleocr.ai/latest/en/version3.x/pipeline_usage/PaddleOCR-VL.html).**\n\n## PaddleOCR-VL-0.9B Usage with transformers\n\nCurrently, we support inference using the PaddleOCR-VL-0.9B model with the `transformers` library, which can recognize texts, formulas, tables, and chart elements. In the future, we plan to support full document parsing inference with `transformers`. Below is a simple script we provide to support inference using the PaddleOCR-VL-0.9B model with `transformers`. \n\n> [!NOTE]\n> Note: We currently recommend using the official method for inference, as it is faster and supports page-level document parsing. The example code below only supports element-level recognition.\n\n```python\nfrom PIL import Image\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\n# ---- Settings ----\nmodel_path = "PaddlePaddle/PaddleOCR-VL"\nimage_path = "test.png"\ntask = "ocr" # Options: ''ocr'' | ''table'' | ''chart'' | ''formula''\n# ------------------\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\nPROMPTS = {\n    "ocr": "OCR:",\n    "table": "Table Recognition:",\n    "formula": "Formula Recognition:",\n    "chart": "Chart Recognition:",\n}\n\nimage = Image.open(image_path).convert("RGB")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, trust_remote_code=True, torch_dtype=torch.bfloat16\n).to(DEVICE).eval()\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nmessages = [\n    {"role": "user",         \n     "content": [\n            {"type": "image", "image": image},\n            {"type": "text", "text": PROMPTS[task]},\n        ]\n    }\n]\ninputs = processor.apply_chat_template(\n    messages, \n    tokenize=True, \n    add_generation_prompt=True, 	\n    return_dict=True,\n    return_tensors="pt"\n).to(DEVICE)\n\noutputs = model.generate(**inputs, max_new_tokens=1024)\noutputs = processor.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(outputs)\n```\n\n<details>\n<summary>👉 Click to expand: Use flash-attn to boost performance and reduce memory usage</summary>\n\n```shell\n# ensure the flash-attn2 is installed\npip install flash-attn --no-build-isolation\n```\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\n\n# ---- Settings ----\nmodel_path = "PaddlePaddle/PaddleOCR-VL"\nimage_path = "test.png"\ntask = "ocr" # ← change to "table" | "chart" | "formula"\n# ------------------\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    attn_implementation="flash_attention_2", \n).to(dtype=torch.bfloat16, device=DEVICE).eval()\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nPROMPTS = {\n    "ocr": "OCR:",\n    "table": "Table Recognition:",\n    "chart": "Chart Recognition:",\n    "formula": "Formula Recognition:",\n}\nmessages = [\n    {\n        "role": "user", \n        "content": [\n            {"type": "image", "image": Image.open(image_path).convert("RGB")},\n            {"type": "text",  "text": PROMPTS[task]}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, \n    tokenize=True, \n    add_generation_prompt=True, 	\n    return_dict=True,\n    return_tensors="pt"\n).to(DEVICE)\n\nwith torch.inference_mode():\n    out = model.generate(\n        **inputs,\n        max_new_tokens=1024,\n        do_sample=False,\n        use_cache=True\n    )\n\noutputs = processor.batch_decode(out, skip_special_tokens=True)[0]\nprint(outputs)\n```\n\n</details>\n\n## Performance\n\n### Page-Level Document Parsing \n\n\n#### 1. OmniDocBench v1.5\n\n##### PaddleOCR-VL achieves SOTA performance for overall, text, formula, tables and reading order on OmniDocBench v1.5\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/omni15.png" width="800"/>\n</div>\n\n\n\n####  2. OmniDocBench v1.0\n\n##### PaddleOCR-VL achieves SOTA performance for almost all metrics of overall, text, formula, tables and reading order on OmniDocBench v1.0\n\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/omni10.png" width="800"/>\n</div>\n\n\n> **Notes:** \n> - The metrics are from [MinerU](https://github.com/opendatalab/MinerU), [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and our own internal evaluations.\n\n\n### Element-level Recognition  \n\n#### 1. Text\n\n**Comparison of OmniDocBench-OCR-block Performance**\n\nPaddleOCR-VL’s robust and versatile capability in handling diverse document types, establishing it as the leading method in the OmniDocBench-OCR-block performance evaluation. \n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/omnibenchocr.png" width="800"/>\n</div>\n\n\n**Comparison of In-house-OCR Performance**\n\nIn-house-OCR provides a evaluation of performance across multiple languages and text types. Our model demonstrates outstanding accuracy with the lowest edit distances in all evaluated scripts.\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/inhouseocr.png" width="800"/>\n</div>\n\n\n\n#### 2. Table\n\n**Comparison of In-house-Table Performance**\n\nOur self-built evaluation set contains diverse types of table images, such as Chinese, English, mixed Chinese-English, and tables with various characteristics like full, partial, or no borders, book/manual formats, lists, academic papers, merged cells, as well as low-quality, watermarked, etc. PaddleOCR-VL achieves remarkable performance across all categories.\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/inhousetable.png" width="600"/>\n</div>\n\n#### 3. Formula\n\n**Comparison of In-house-Formula Performance**\n\nIn-house-Formula evaluation set contains simple prints, complex prints, camera scans, and handwritten formulas. PaddleOCR-VL demonstrates the best performance in every category.\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/inhouse-formula.png" width="500"/>\n</div>\n\n\n#### 4. Chart\n\n**Comparison of In-house-Chart Performance**\n\nThe evaluation set is broadly categorized into 11 chart categories, including bar-line hybrid, pie, 100% stacked bar, area, bar, bubble, histogram, line, scatterplot, stacked area, and stacked bar. PaddleOCR-VL not only outperforms expert OCR VLMs but also surpasses some 72B-level multimodal language models.\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/inhousechart.png" width="400"/>\n</div>\n\n\n\n\n\n\n\n## Visualization\n\n\n### Comprehensive Document Parsing\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/overview1.jpg" width="600"/>\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/overview2.jpg" width="600"/>\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/overview3.jpg" width="600"/>\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/overview4.jpg" width="600"/>\n</div>\n\n\n### Text\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/text_english_arabic.jpg" width="300" style="display: inline-block;"/>\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/text_handwriting_02.jpg" width="300" style="display: inline-block;"/>\n</div>\n\n\n### Table\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/table_01.jpg" width="300" style="display: inline-block;"/>\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/table_02.jpg" width="300" style="display: inline-block;"/>\n</div>\n\n\n### Formula\n\n<div align="center">\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/formula_EN.jpg" width="300" style="display: inline-block;"/>\n<img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/formula_ZH.jpg" width="300" style="display: inline-block;"/>\n</div>\n\n\n### Chart\n\n<div align="center">\n  <img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/chart_01.jpg" width="300" style="display: inline-block;"/>\n  <img src="https://huggingface.co/datasets/PaddlePaddle/PaddleOCR-VL_demo/resolve/main/imgs/chart_02.jpg" width="300" style="display: inline-block;"/>\n</div>\n\n\n## Acknowledgments\n\nWe would like to thank [ERNIE](https://github.com/PaddlePaddle/ERNIE), [Keye](https://github.com/Kwai-Keye/Keye), [MinerU](https://github.com/opendatalab/MinerU), [OmniDocBench](https://github.com/opendatalab/OmniDocBench) for providing valuable code, model weights and benchmarks. We also appreciate everyone''s contribution to this open-source project!\n\n## Citation\n\nIf you find PaddleOCR-VL helpful, feel free to give us a star and citation.\n\n```bibtex\n@misc{cui2025paddleocrvlboostingmultilingualdocument,\n      title={PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model}, \n      author={Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},\n      year={2025},\n      eprint={2510.14528},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2510.14528}, \n}\n```\n', '{"pipeline_tag":"image-text-to-text","library_name":"PaddleOCR","framework":"PaddleOCR","params":958588736,"storage_bytes":2156150084,"files_count":23,"spaces_count":9,"gated":false,"private":false,"config":{"architectures":["PaddleOCRVLForConditionalGeneration"],"auto_map":{"AutoConfig":"configuration_paddleocr_vl.PaddleOCRVLConfig","AutoModel":"modeling_paddleocr_vl.PaddleOCRVLForConditionalGeneration","AutoModelForCausalLM":"modeling_paddleocr_vl.PaddleOCRVLForConditionalGeneration"},"model_type":"paddleocr_vl","tokenizer_config":{"bos_token":"<s>","cls_token":"<|begin_of_sentence|>","eos_token":"</s>","mask_token":"<mask:1>","pad_token":"<unk>","sep_token":"<|end_of_sentence|>","unk_token":"<unk>","use_default_system_prompt":false},"chat_template_jinja":"{%- if not add_generation_prompt is defined -%}\n    {%- set add_generation_prompt = true -%}\n{%- endif -%}\n{%- if not cls_token is defined -%}\n    {%- set cls_token = \"<|begin_of_sentence|>\" -%}\n{%- endif -%}\n{%- if not eos_token is defined -%}\n    {%- set eos_token = \"</s>\" -%}\n{%- endif -%}\n{%- if not image_token is defined -%}\n    {%- set image_token = \"<|IMAGE_START|><|IMAGE_PLACEHOLDER|><|IMAGE_END|>\" -%}\n{%- endif -%}\n{{- cls_token -}}\n{%- for message in messages -%}\n    {%- if message[\"role\"] == \"user\" -%}\n        {{- \"User: \" -}}\n        {%- for content in message[\"content\"] -%}\n            {%- if content[\"type\"] == \"image\" -%}\n                {{ image_token }}\n            {%- endif -%}\n        {%- endfor -%}\n        {%- for content in message[\"content\"] -%}\n            {%- if content[\"type\"] == \"text\" -%}\n                {{ content[\"text\"] }}\n            {%- endif -%}\n        {%- endfor -%}\n        {{ \"\\n\" -}}\n    {%- elif message[\"role\"] == \"assistant\" -%}\n        {{- \"Assistant: \" -}}\n        {%- for content in message[\"content\"] -%}\n            {%- if content[\"type\"] == \"text\" -%}\n                {{ content[\"text\"] }}\n            {%- endif -%}\n        {%- endfor -%}\n        {{ eos_token -}}\n    {%- elif message[\"role\"] == \"system\" -%}\n        {%- for content in message[\"content\"] -%}\n            {%- if content[\"type\"] == \"text\" -%}\n                {{ content[\"text\"] + \"\\n\" }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{- \"Assistant: \" -}}\n{%- endif -%}\n"}}', '[]', '[{"type":"has_code","target_id":"github:PaddlePaddle:PaddleOCR","source_url":"https://github.com/PaddlePaddle/PaddleOCR"},{"type":"has_code","target_id":"github:PaddlePaddle:PaddleOCR","source_url":"https://github.com/PaddlePaddle/PaddleOCR"},{"type":"has_code","target_id":"github:PaddlePaddle:PaddleOCR","source_url":"https://github.com/PaddlePaddle/PaddleOCR"},{"type":"has_code","target_id":"github:opendatalab:MinerU","source_url":"https://github.com/opendatalab/MinerU"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:PaddlePaddle:ERNIE","source_url":"https://github.com/PaddlePaddle/ERNIE"},{"type":"has_code","target_id":"github:Kwai-Keye:Keye","source_url":"https://github.com/Kwai-Keye/Keye"},{"type":"has_code","target_id":"github:opendatalab:MinerU","source_url":"https://github.com/opendatalab/MinerU"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"based_on_paper","target_id":"arxiv:2510.14528","source_url":"https://arxiv.org/abs/2510.14528"}]', NULL, 'Apache-2.0', 'approved', 80, '6d9afbb6198c66677f64d261f8425817', NULL, NULL, CURRENT_TIMESTAMP);
