/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-bitnet-b1.58-2B-4T', 'huggingface--microsoft--bitnet-b1.58-2b-4t', 'bitnet-b1.58-2B-4T', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/bitnet-b1.58-2B-4T/blob/main/LICENSE language: - en pipeline_tag: text-generation tags: - chat - bitnet - text-generation - large-language-model library_name: transformers --- This repository contains the weights for **BitNet b1.58 2B4T**, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale, developed by Microsoft Research. Trained on a corpus of 4 trillion tokens, this model demonstrat...', '["transformers","safetensors","bitnet","text-generation","chat","large-language-model","conversational","custom_code","en","arxiv:2504.12285","license:mit","endpoints_compatible","8-bit","deploy:azure","region:us"]', 'text-generation', 1224, 7838, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/bitnet-b1.58-2B-4T","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/bitnet-b1.58-2B-4T/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- chat\n- bitnet\n- text-generation\n- large-language-model\nlibrary_name: transformers\n---\n\n# BitNet b1.58 2B4T - Scaling Native 1-bit LLM\n\nThis repository contains the weights for **BitNet b1.58 2B4T**, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale, developed by Microsoft Research.\n\nTrained on a corpus of 4 trillion tokens, this model demonstrates that native 1-bit LLMs can achieve performance comparable to leading open-weight, full-precision models of similar size, while offering substantial advantages in computational efficiency (memory, energy, latency).\n\n➡️ **Technical Report:** [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)\n\n➡️ **Official Inference Code:** [microsoft/BitNet (bitnet.cpp)](https://github.com/microsoft/BitNet)\n\n## Model Variants\n\nSeveral versions of the model weights are available on Hugging Face:\n\n* [**`microsoft/bitnet-b1.58-2B-4T`**](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T) (This repository): Contains the packed 1.58-bit weights optimized for efficient inference. **Use this for deployment.**\n\n* [**`microsoft/bitnet-b1.58-2B-4T-bf16`**](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-bf16): Contains the master weights in BF16 format. **Use this only for training or fine-tuning purposes.**\n\n* [**`microsoft/bitnet-b1.58-2B-4T-gguf`**](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf): Contains the model weights in GGUF format, compatible with the `bitnet.cpp` library for CPU inference.\n\n## Model Details\n\n* **Architecture:** Transformer-based, modified with `BitLinear` layers (BitNet framework).\n    * Uses Rotary Position Embeddings (RoPE).\n    * Uses squared ReLU (ReLU²) activation in FFN layers.\n    * Employs [`subln`](https://proceedings.mlr.press/v202/wang23u.html) normalization.\n    * No bias terms in linear or normalization layers.\n* **Quantization:** Native 1.58-bit weights and 8-bit activations (W1.58A8).\n    * Weights are quantized to ternary values {-1, 0, +1} using absmean quantization during the forward pass.\n    * Activations are quantized to 8-bit integers using absmax quantization (per-token).\n    * **Crucially, the model was *trained from scratch* with this quantization scheme, not post-training quantized.**\n* **Parameters:** ~2 Billion\n* **Training Tokens:** 4 Trillion\n*   **Context Length:** Maximum sequence length of **4096 tokens**.\n    *   *Recommendation:* For optimal performance on tasks requiring very long contexts (beyond the pre-training length or for specialized long-reasoning tasks), we recommend performing intermediate long-sequence adaptation/training before the final fine-tuning stage.\n* **Training Stages:**\n    1.  **Pre-training:** Large-scale training on public text/code and synthetic math data using a two-stage learning rate and weight decay schedule.\n    2.  **Supervised Fine-tuning (SFT):** Fine-tuned on instruction-following and conversational datasets using sum loss aggregation and specific hyperparameter tuning.\n    3.  **Direct Preference Optimization (DPO):** Aligned with human preferences using preference pairs.\n* **Tokenizer:** LLaMA 3 Tokenizer (vocab size: 128,256).\n\n## How to Use (with `transformers`)\n\n**VERY IMPORTANT NOTE ON EFFICIENCY**\n\n> Please do NOT expect performance efficiency gains (in terms of speed, latency, or energy consumption) when using this model with the standard transformers library, even with the required fork.\n>\n> The current execution paths within transformers do not contain the specialized, highly optimized computational kernels required to leverage the advantages of the BitNet architecture. Running the model via transformers will likely result in inference speeds and energy usage comparable to, or potentially worse than, standard full-precision models within this framework on both CPU and GPU.\n>\n> While you might observe reduced memory usage due to the quantized weights, the primary computational efficiency benefits are not accessible through this standard transformers usage path.\n>\n> For achieving the efficiency benefits demonstrated in the technical paper, you MUST use the dedicated C++ implementation: [bitnet.cpp](https://github.com/microsoft/BitNet).\n\n### Requirements\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@096f25ae1f501a084d8ff2dcaf25fbc2bd60eba4\n```\n\n### Example\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "microsoft/bitnet-b1.58-2B-4T"\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16\n)\n\n# Apply the chat template\nmessages = [\n    {"role": "system", "content": "You are a helpful AI assistant."},\n    {"role": "user", "content": "How are you?"},\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nchat_input = tokenizer(prompt, return_tensors="pt").to(model.device)\n\n# Generate response\nchat_outputs = model.generate(**chat_input, max_new_tokens=50)\nresponse = tokenizer.decode(chat_outputs[0][chat_input[''input_ids''].shape[-1]:], skip_special_tokens=True) # Decode only the response part\nprint("\nAssistant Response:", response)\n```\n\n## How to Use (with `bitnet.cpp`)\n\nPlease refer to the [bitnet.cpp](https://github.com/microsoft/BitNet) GitHub repository for detailed compilation steps, usage examples, and command-line options.\n\n## Evaluation\n\nBitNet b1.58 2B4T was evaluated against leading open-weight full-precision LLMs of similar size. Below are the key results (all models are instruction-tuned versions):\n\n| Benchmark             | LLaMA 3.2 1B | Gemma-3 1B | Qwen2.5 1.5B | SmolLM2 1.7B | MiniCPM 2B | **BitNet b1.58 2B** |\n|--------------------------------|--------------|------------|--------------|--------------|------------|---------------------|\n| **Memory (Non-emb)** | 2GB          | 1.4GB      | 2.6GB        | 3.2GB        | 4.8GB      | **0.4GB** |\n| **Latency (CPU Decoding)** | 48ms         | 41ms       | 65ms         | 67ms         | 124ms      | **29ms** |\n| **Energy (Estimated)** | 0.258J       | 0.186J     | 0.347J       | 0.425J       | 0.649J     | **0.028J** |\n| **Training Tokens (Pre-train)**| 9T* | 2T** | 18T          | 11T          | 1.1T       | 4T                  |\n| ARC-Challenge   | 37.80        | 38.40      | 46.67        | 43.52        | 44.80      | **49.91** |\n| ARC-Easy        | 63.17        | 63.13      | **76.01** | 62.92        | 72.14      | 74.79               |\n| OpenbookQA      | 34.80        | 38.80      | 40.80        | **46.00** | 40.20      | 41.60               |\n| BoolQ                | 64.65        | 74.22      | 78.04        | 75.78        | **80.67** | 80.18               |\n| HellaSwag       | 60.80        | 57.69      | 68.28        | **71.71** | 70.81      | 68.44               |\n| PIQA            | 74.21        | 71.93      | 76.12        | 76.12        | 76.66      | **77.09** |\n| WinoGrande           | 59.51        | 58.48      | 62.83        | 68.98        | 61.80      | **71.90** |\n| CommonsenseQA       | 58.48        | 42.10      | **76.41** | 63.55        | 71.74      | 71.58               |\n| TruthfulQA          | 43.80        | 38.66      | **46.67** | 39.90        | 41.41      | 45.31               |\n| TriviaQA              | 37.60        | 23.49      | 38.37        | **45.97** | 34.13      | 33.57               |\n| MMLU                 | 45.58        | 39.91      | **60.25** | 49.24        | 51.82      | 53.17               |\n| HumanEval+        | 31.10        | 37.20      | **50.60** | 28.00        | 43.90      | 38.40               |\n| GSM8K                 | 38.21        | 31.16      | 56.79        | 45.11        | 4.40       | **58.38** |\n| MATH-500              | 23.00        | 42.00      | **53.00** | 17.60        | 14.80      | 43.40               |\n| IFEval   | 62.71        | **66.67** | 50.12        | 57.91        | 36.81      | 53.48               |\n| MT-bench         | 5.43         | 6.40       | 6.12         | 5.50         | **6.57** | 5.85                |\n| **Average** | 44.90        | 43.74      | **55.23** | 48.70        | 42.05      | 54.19               |\n\n*LLaMA 3.2 1B uses pruning & distillation.\n\n**Gemma-3 1B uses distillation.\n\n## License\nThe model weights and code are released under the [MIT License](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T/blob/main/LICENSE).\n\n## Bias, Risks, and Limitations\nPredictions may perpetuate biases present in the training data. \n\nThere is limited support for non-English languages and underrepresented  domains. \n\nThere is a risk of generating inaccurate or harmful content. \n\nThe Bitnet model has an elevated defect rate when responding to election-critical queries, which may result in incorrect or unauthoritative election critical information being presented. We are working to improve the model''s performance in this area. Users should verify information related to elections with the election authority in their region. \n\n## Disclaimer\nWe do not recommend using BitNet b1.58 in commercial or real-world applications without further testing and development. This model is intended for research and development purposes. While efforts have been made to align it using SFT and DPO, it may still produce outputs that are unexpected, biased, or inaccurate. Please use responsibly.\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":849787090,"storage_bytes":6684507356,"files_count":9,"spaces_count":32,"gated":false,"private":false,"config":{"architectures":["BitNetForCausalLM"],"auto_map":{"AutoConfig":"configuration_bitnet.BitNetConfig","AutoModelForCausalLM":"modeling_bitnet.BitNetForCausalLM"},"model_type":"bitnet","quantization_config":{"quant_method":"bitnet"},"tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message[''role''] | capitalize + '': ''+ message[''content''] | trim + ''<|eot_id|>'' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ ''Assistant: '' }}{% endif %}","eos_token":"<|eot_id|>"}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:BitNet","source_url":"https://github.com/microsoft/BitNet"},{"type":"has_code","target_id":"github:microsoft:BitNet","source_url":"https://github.com/microsoft/BitNet"},{"type":"has_code","target_id":"github:huggingface:transformers.git@096f25ae1f501a084d8ff2dcaf25fbc2bd60eba4","source_url":"https://github.com/huggingface/transformers.git@096f25ae1f501a084d8ff2dcaf25fbc2bd60eba4"},{"type":"has_code","target_id":"github:microsoft:BitNet","source_url":"https://github.com/microsoft/BitNet"},{"type":"based_on_paper","target_id":"arxiv:2504.12285","source_url":"https://arxiv.org/abs/2504.12285"}]', NULL, 'MIT', 'approved', 65, '8ea634ea474a71e189bf40ff2b6640c2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-7b-it', 'huggingface--google--gemma-7b-it', 'gemma-7b-it', 'google', '', '["transformers","safetensors","gguf","gemma","text-generation","conversational","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","base_model:google/gemma-7b","base_model:finetune:google/gemma-7b","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1221, 142440, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-7b-it","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8537680896,"storage_bytes":138266183581,"files_count":14,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 40, 'ecb4aa6dbd991d40c23aba7d49aacf25', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-comfyanonymous-flux-text-encoders', 'huggingface--comfyanonymous--flux-text-encoders', 'flux_text_encoders', 'comfyanonymous', '--- license: apache-2.0 --- Flux text encoder checkpoints meant to be used with the DualClipLoader node of ComfyUI See the ComfyUI Flux examples', '["license:apache-2.0","region:us"]', 'other', 1215, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/comfyanonymous/flux_text_encoders","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\n---\nFlux text encoder checkpoints meant to be used with the DualClipLoader node of [ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n\nSee the [ComfyUI Flux examples](https://comfyanonymous.github.io/ComfyUI_examples/flux/)', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":20088119529,"files_count":6,"spaces_count":35,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"}]', NULL, 'Apache-2.0', 'approved', 40, 'abf6e01e87271c7a955f2a930ea65cf2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Phr00t-WAN2.2-14B-Rapid-AllInOne', 'huggingface--phr00t--wan2.2-14b-rapid-allinone', 'WAN2.2-14B-Rapid-AllInOne', 'Phr00t', '--- base_model: - Wan-AI/Wan2.2-I2V-A14B - Wan-AI/Wan2.2-T2V-A14B tags: - wan - wan2.2 - accelerator pipeline_tag: image-to-video license: apache-2.0 --- These are mixtures of WAN 2.2 and other WAN-like models and accelerators (with CLIP and VAE also included) to provide a fast, "all in one" solution for making videos as easily and quickly as possible. FP8 precision. Generally the latest version available for each type of model (image to video or text to video) is recommended. **MEGA Merge:**...', '["wan2.2","wan","accelerator","image-to-video","base_model:wan-ai/wan2.2-i2v-a14b","base_model:finetune:wan-ai/wan2.2-i2v-a14b","license:apache-2.0","region:us"]', 'image-to-video', 1214, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model:\n- Wan-AI/Wan2.2-I2V-A14B\n- Wan-AI/Wan2.2-T2V-A14B\ntags:\n- wan\n- wan2.2\n- accelerator\npipeline_tag: image-to-video\nlicense: apache-2.0\n---\nThese are mixtures of WAN 2.2 and other WAN-like models and accelerators (with CLIP and VAE also included) to provide a fast, "all in one" solution for making videos as easily and quickly as possible. FP8 precision. Generally the latest version available for each type of model (image to video or text to video) is recommended.\n\n**MEGA Merge:** This is the "one model to rule them all" version which pretty much does everything. It can handle text to video, image to video, and first frame to last frame and last frame only (because it includes VACE). There is a specific workflow to use these merges included in the mega-v3/ folder, as it is slightly more complicated (but shouldn''t be slower) due to its flexibility. See below for a screenshot of "mega" being used.\n\n**NSFW Merges:** Degenerates should steer clear of these merges, as they are only for the most civilized people of culture or scientific researchers. These merge various spicy WAN 2.1+2.2 LORAs at generally low strengths to provide a "jack of all trades, master of none" all in one despicable solution. If you are not getting the results you want, add more LORAs or just use the non-NSFW versions with hand-picked LORAs.\n\nYou just need to use the basic ComfyUI "Load Checkpoint" node with these, as you can take the VAE, CLIP and Model all from one AIO safetensors (saved in your ''checkpoints'' folder). All models are intended to use 1 CFG and 4 steps. See sampler recommendations for each version below.\n\nWAN 2.1 LORA compatibility is generally still good, along with "low noise" WAN 2.2 LORA compatibility (do not use "high noise" LORAs). You might need to adjust LORA strengths (up or down) to get results you want, though.\n\n**MEGA version workflow screenshot (you can use VideoCombine instead of Preview Image):**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/tW8lXhRrAXzluvjNPudag.png)\n\n**MEGA I2V:** Just bypass the "end frame" so the "start frame" will be your I2V starting frame. Keep everything else the same.\n\n**MEGA T2V:** Bypass "end frame", "start frame" and the "VACEFirstToLastFrame" node. Set strength to 0 for WanVaceToVideo.\n\n**MEGA Last Frame:** Just bypass the "start frame" and keep "end frame". Keep everything else the same as in the picture.\n\n**MEGA First->Last Frame:** Use it like shown in the picture above.\n\n**Older non-MEGA workflows (v10 and below):**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/t_SxUFP9oyNz0C8dj6jze.png)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/GNDAWnRHAjt8vPY0wXNTq.png)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/F3tB7EhHMS1Gn-7iplmV8.png)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/70X-8YUbn5hPogrG5V8Kv.png)\n\nSeems to work even on 8GB VRAM:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/i4NRFi7FX_j7FUZyvmImw.png)\n\n**CHANGELOG/VERSIONS:**\n\n**base:** This is the first attempt and very "stable", but mostly WAN 2.1 with few WAN 2.2 features. sa_solver recommended.\n\n**V2:** This is a more dynamic mixture with more WAN 2.2 features. sa_solver OR euler_a sampler recommended. Suffers from minor color shifts and noise in I2V, typically just at the start.\n\n**V3:** This is a mixture of SkyReels and WAN 2.2, which should improve prompt adherence and quality. euler_a sampler recommended, beta scheduler. Suffers from minor color shifts and noise in I2V, typically just at the start.\n\n**V4:** WAN 2.2 Lightning in the mix! euler_a/beta recommended. I2V noise and color shifting generally improved, but motion is a bit overexaggerated.\n\n**V5:** Improved overexaggeration of I2V model. euler_a/beta recommended.\n\n**V6:** New merging structure and overall significantly improved quality. I2V noise for the first 1-2 frames still exists, but it clears up much better than previous versions. Some WAN 2.1 LORAs at heavy strengths may cause up to 5 poor early frames with T2V, where discarding (or lowering strengths) may help. sa_solver/beta recommended. I2V rarely suffers from some dramatic scene shifts.\n\n**V7:** I2V scene shifting should be fixed, but some I2V noise persists (generally for just the first 1-2 frames). No changes needed for the T2V model, so that remains at V6. sa_solver/beta recommended.\n\n**V8:** T2V is now based entirely off of WAN 2.2 "low" (with PUSA, SkyReels and Lightning accelerators mixed in), which should resolve noise problems with it (8.1 adds more SkyReels). I2V scaled back some of the WAN 2.2 mix, which was contributing to noise problems. There still is some minor I2V noise, but more of a delicate balance of WAN 2.2 + SkyReels to keep decent motion and flexibility. Euler_a/beta recommended.\n\n**V9:** Removed PUSA and SkyReels from the WAN 2.2-side of I2V (and completely from T2V). as I think PUSA/SkyReels wasn''t consistently helping (and sometimes hurting) when applied to WAN 2.2. This should provide a more reliable base to work from. **euler_a/beta** recommended, but feel free to experiment with sa_solver/beta or others!\n\n**V10:** Fixes wrong accelerators being used (now WAN 2.2 Lightning in I2V and an an adaptive rank Lightx2v along with WAN 2.2 lightning in T2V). I2V now has a tendency to zoom into whatever is going on in your prompt, which I believe comes from increased camera movement from Wan 2.2 Lightning and being less tied to your initial image as the video progresses (so, prompt accordingly). Euler_a/beta still seems good.\n\n**MEGA v1:** This is likely how I will continue making models, as I don''t need separate I2V and T2V versions. No noise problems with I2V anymore! MEGA v1 is based off of WAN 2.2 "low T2V", then adds VACE Fun, SkyReels, FunReward and the usual accelerator/CLIP/VAE mix. Use the included workflow. ipndm/sgm_uniform sampler/scheduler recommended.\n\n**MEGA v2:** Removed the FunReward LORA, which was causing faces to shift. I did notice some minor face shifting in the NSFW merge remaining, which I think is due to the LORA mixture, but it has been improved. Also reduced some of the SkyReels LORA a bit. ipndm/beta recommended.\n\n**MEGA v3:** Very different merging method using SkyReels 2.1 33% base and WAN 2.2 66% on top. I now also match accelerators for each version (2.1 and 2.2), then merge. I think this gets a better result by basing "mega" on models designed for 1 sampler (2.1) but then bringing in most of WAN 2.2 to lay on top. Camera control and prompt following is better, but keeping facial features still struggles compared to v10 I2V (might be a VACE limitation). ipndm/beta recommended. euler_a/beta seems to work better with the NSFW v3.1 merge, though.\n\n**MEGA v4:** Uses the WAN 2.2 finetune from https://huggingface.co/eddy1111111/WAN22.XX_Palingenesis (also slight tweaks to accelerator strengths)\n\n**MEGA v5:** New merging method with very experimental accelerator mix! I include small amounts of many I2V and T2V accelerators on top of WAN22.XX_Palingenesis and SkyReels 720p, plus VACE. The goal is to improve I2V consistency without hurting T2V. I think quality, detail and consistency has improved, but I do wish camera control was better. euler_a/beta recommended.\n\n**MEGA v6:** Adjusted accelerators, bringing in more of the older Lightx2v as relying too much on the newest WAN 2.2 Lightning was hurting motion. I''m seeing better camera movement and prompt adherence in my testing than v5. NSFW v6.1 version has newer LORAs included and tweaked parameters. sa_solver/beta recommended.\n\n**MEGA v7:** Now uses 3 different accelerators mixed together: lightx2v, WAN 2.2 Lightning (250928) and rCM. Motion seems to be improved further. euler_a/beta seems to work pretty good.\n\n**MEGA v8:** Updated rCM 720p accelerator, which is now the biggest accelerator in the mix, reducing lightx2v and WAN 2.2 Lightning. Updated NSFW LORAs a bit. euler_a/beta still recommended.\n\n**MEGA v9:** Removed SkyReels 2.1 720p completely. This is now based completely on WAN22.XX_Palingenesis T2V + VACE, using mostly rCM 720p for acceleration. Updated MysticXXX v2 for the NSFW merge among other tweaks. Motion should be better, hopefully. euler_a/beta recommended.\n\n**MEGA v10:** Packed the models a bit differently, tweaked acclerators and NSFW LORAs some more. I tried to test this version a bit more and was getting better results. **euler_a/beta recommended**.\n\n**MEGA v11:** Mostly the same as v10, but pulled in the latest WAN 2.1 distill from lightx2v. **euler_a/beta recommended**.\n\n**MEGA v12:** Big update! Using bf16 Fun VACE WAN 2.2 as a base now, getting rid of "fp8 scaled" issues. Significantly tweaked NSFW LORAs, even putting a dash of "high noise" Dreamlay into the mix. Only uses rCM and Lightx2V accelerators now for better motion. v12.1 improves cumshots. **euler_a/beta recommended**.\n\nLooking for GGUFs? Check the sidebar for quants.\n\nLooking for FP16 precision? TekeshiX has been helping me build variants in FP16 format (but they are kinda outdated):\n\nhttps://huggingface.co/TekeshiX/RAPID-AIO-FP16/tree/main\n\n**DISCLAIMER:** As you may expect, some compromises had to be made to reach this level of speed and simplicity. If you want more complex workflows and longer generation times to run "full WAN 2.2"''s pair of models (which will give higher quality results), or control over accelerator LORAs included in this merge, there are many resources elsewhere to do that.', '{"pipeline_tag":"image-to-video","library_name":"wan2.2","framework":"wan2.2","params":null,"storage_bytes":1349372666919,"files_count":68,"spaces_count":5,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 65, '2e6586090919fc8639d173c10670286d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ggerganov-whisper.cpp', 'huggingface--ggerganov--whisper.cpp', 'whisper.cpp', 'ggerganov', '--- license: mit pipeline_tag: automatic-speech-recognition --- Available models | Model | Disk | SHA | | ------------------- | ------- | ------------------------------------------ | | tiny | 75 MiB | | | tiny-q5_1 | 31 MiB | | | tiny-q8_0 | 42 MiB | | | tiny.en | 75 MiB | | | tiny.en-q5_1 | 31 MiB | | | tiny.en-q8_0 | 42 MiB | | | base | 142 MiB | | | base-q5_1 | 57 MiB | | | base-q8_0 | 78 MiB | | | base.en | 142 MiB | | | base.en-q5_1 | 57 MiB | | | base.en-q8_0 | 78 MiB | | | small | 466 ...', '["automatic-speech-recognition","license:mit","region:us"]', 'automatic-speech-recognition', 1210, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ggerganov/whisper.cpp","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\npipeline_tag: automatic-speech-recognition\n---\n\n# OpenAI''s Whisper models converted to ggml format for use with [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n\n[Available models](https://huggingface.co/ggerganov/whisper.cpp/tree/main)\n\n| Model               | Disk    | SHA                                        |\n| ------------------- | ------- | ------------------------------------------ |\n| tiny                | 75 MiB  | `bd577a113a864445d4c299885e0cb97d4ba92b5f` |\n| tiny-q5_1           | 31 MiB  | `2827a03e495b1ed3048ef28a6a4620537db4ee51` |\n| tiny-q8_0           | 42 MiB  | `19e8118f6652a650569f5a949d962154e01571d9` |\n| tiny.en             | 75 MiB  | `c78c86eb1a8faa21b369bcd33207cc90d64ae9df` |\n| tiny.en-q5_1        | 31 MiB  | `3fb92ec865cbbc769f08137f22470d6b66e071b6` |\n| tiny.en-q8_0        | 42 MiB  | `802d6668e7d411123e672abe4cb6c18f12306abb` |\n| base                | 142 MiB | `465707469ff3a37a2b9b8d8f89f2f99de7299dac` |\n| base-q5_1           | 57 MiB  | `a3733eda680ef76256db5fc5dd9de8629e62c5e7` |\n| base-q8_0           | 78 MiB  | `7bb89bb49ed6955013b166f1b6a6c04584a20fbe` |\n| base.en             | 142 MiB | `137c40403d78fd54d454da0f9bd998f78703390c` |\n| base.en-q5_1        | 57 MiB  | `d26d7ce5a1b6e57bea5d0431b9c20ae49423c94a` |\n| base.en-q8_0        | 78 MiB  | `bb1574182e9b924452bf0cd1510ac034d323e948` |\n| small               | 466 MiB | `55356645c2b361a969dfd0ef2c5a50d530afd8d5` |\n| small-q5_1          | 181 MiB | `6fe57ddcfdd1c6b07cdcc73aaf620810ce5fc771` |\n| small-q8_0          | 252 MiB | `bcad8a2083f4e53d648d586b7dbc0cd673d8afad` |\n| small.en            | 466 MiB | `db8a495a91d927739e50b3fc1cc4c6b8f6c2d022` |\n| small.en-q5_1       | 181 MiB | `20f54878d608f94e4a8ee3ae56016571d47cba34` |\n| small.en-q8_0       | 252 MiB | `9d75ff4ccfa0a8217870d7405cf8cef0a5579852` |\n| small.en-tdrz       | 465 MiB | `b6c6e7e89af1a35c08e6de56b66ca6a02a2fdfa1` |\n| medium              | 1.5 GiB | `fd9727b6e1217c2f614f9b698455c4ffd82463b4` |\n| medium-q5_0         | 514 MiB | `7718d4c1ec62ca96998f058114db98236937490e` |\n| medium-q8_0         | 785 MiB | `e66645948aff4bebbec71b3485c576f3d63af5d6` |\n| medium.en           | 1.5 GiB | `8c30f0e44ce9560643ebd10bbe50cd20eafd3723` |\n| medium.en-q5_0      | 514 MiB | `bb3b5281bddd61605d6fc76bc5b92d8f20284c3b` |\n| medium.en-q8_0      | 785 MiB | `b1cf48c12c807e14881f634fb7b6c6ca867f6b38` |\n| large-v1            | 2.9 GiB | `b1caaf735c4cc1429223d5a74f0f4d0b9b59a299` |\n| large-v2            | 2.9 GiB | `0f4c8e34f21cf1a914c59d8b3ce882345ad349d6` |\n| large-v2-q5_0       | 1.1 GiB | `00e39f2196344e901b3a2bd5814807a769bd1630` |\n| large-v2-q8_0       | 1.5 GiB | `da97d6ca8f8ffbeeb5fd147f79010eeea194ba38` |\n| large-v3            | 2.9 GiB | `ad82bf6a9043ceed055076d0fd39f5f186ff8062` |\n| large-v3-q5_0       | 1.1 GiB | `e6e2ed78495d403bef4b7cff42ef4aaadcfea8de` |\n| large-v3-turbo      | 1.5 GiB | `4af2b29d7ec73d781377bfd1758ca957a807e941` |\n| large-v3-turbo-q5_0 | 547 MiB | `e050f7970618a659205450ad97eb95a18d69c9ee` |\n| large-v3-turbo-q8_0 | 834 MiB | `01bf15bedffe9f39d65c1b6ff9b687ea91f59e0e` |\n\nFor more information, visit:\n\nhttps://github.com/ggerganov/whisper.cpp/tree/master/models', '{"pipeline_tag":"automatic-speech-recognition","library_name":null,"framework":null,"params":null,"storage_bytes":31005917241,"files_count":47,"spaces_count":31,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"},{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"}]', NULL, 'MIT', 'approved', 65, '85d69c3aa00468c93960171d2b66c61a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-pyannote-speaker-diarization', 'huggingface--pyannote--speaker-diarization', 'speaker-diarization', 'pyannote', '', '["pyannote-audio","pyannote","pyannote-audio-pipeline","audio","voice","speech","speaker","speaker-diarization","speaker-change-detection","voice-activity-detection","overlapped-speech-detection","automatic-speech-recognition","dataset:ami","dataset:dihard","dataset:voxconverse","dataset:aishell","dataset:repere","dataset:voxceleb","arxiv:2012.01477","arxiv:2110.07058","arxiv:2005.08072","license:mit","region:us"]', 'automatic-speech-recognition', 1203, 937990, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/pyannote/speaker-diarization","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"automatic-speech-recognition","library_name":"pyannote-audio","framework":"pyannote-audio","params":null,"storage_bytes":null,"files_count":67,"spaces_count":100,"gated":"auto","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2012.01477","source_url":"https://arxiv.org/abs/2012.01477"},{"type":"based_on_paper","target_id":"arxiv:2110.07058","source_url":"https://arxiv.org/abs/2110.07058"},{"type":"based_on_paper","target_id":"arxiv:2005.08072","source_url":"https://arxiv.org/abs/2005.08072"}]', NULL, 'MIT', 'approved', 40, '972054b4fcaa8fd9fe20ad5e1c09e636', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-sentence-transformers-all-mpnet-base-v2', 'huggingface--sentence-transformers--all-mpnet-base-v2', 'all-mpnet-base-v2', 'sentence-transformers', '--- language: en license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers - text-embeddings-inference datasets: - s2orc - flax-sentence-embeddings/stackexchange_xml - ms_marco - gooaq - yahoo_answers_topics - code_search_net - search_qa - eli5 - snli - multi_nli - wikihow - natural_questions - trivia_qa - embedding-data/sentence-compression - embedding-data/flickr30k-captions - embedding-data/altlex - embed...', '["sentence-transformers","pytorch","onnx","safetensors","openvino","mpnet","fill-mask","feature-extraction","sentence-similarity","transformers","text-embeddings-inference","en","dataset:s2orc","dataset:ms_marco","dataset:gooaq","dataset:yahoo_answers_topics","dataset:code_search_net","dataset:search_qa","dataset:eli5","dataset:snli","dataset:multi_nli","dataset:wikihow","dataset:natural_questions","dataset:trivia_qa","dataset:embedding-data/sentence-compression","dataset:embedding-data/flickr30k-captions","dataset:embedding-data/altlex","dataset:embedding-data/simple-wiki","dataset:embedding-data/qqp","dataset:embedding-data/specter","dataset:embedding-data/paq_pairs","dataset:embedding-data/wikianswers","arxiv:1904.06472","arxiv:2102.07033","arxiv:2104.08727","arxiv:1704.05179","arxiv:1810.09305","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'sentence-similarity', 1201, 24469329, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/sentence-transformers/all-mpnet-base-v2","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- text-embeddings-inference\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-mpnet-base-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = ["This is an example sentence", "Each sentence is converted"]\n\nmodel = SentenceTransformer(''sentence-transformers/all-mpnet-base-v2'')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [''This is an example sentence'', ''Each sentence is converted'']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(''sentence-transformers/all-mpnet-base-v2'')\nmodel = AutoModel.from_pretrained(''sentence-transformers/all-mpnet-base-v2'')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n## Usage (Text Embeddings Inference (TEI))\n\n[Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) is a blazing fast inference solution for text embedding models.\n\n- CPU:\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\n```\n\n- NVIDIA GPU:\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\n```\n\nSend a request to `/v1/embeddings` to generate embeddings via the [OpenAI Embeddings API](https://platform.openai.com/docs/api-reference/embeddings/create):\n```bash\ncurl http://localhost:8080/v1/embeddings \\n  -H ''Content-Type: application/json'' \\n  -d ''{\n    "model": "sentence-transformers/all-mpnet-base-v2",\n    "input": ["This is an example sentence", "Each sentence is converted"]\n  }''\n```\n\nOr check the [Text Embeddings Inference API specification](https://huggingface.github.io/text-embeddings-inference/) instead.\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":109486978,"storage_bytes":4105073268,"files_count":28,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MPNetForMaskedLM"],"model_type":"mpnet","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","sep_token":"</s>","cls_token":"<s>","unk_token":"[UNK]","pad_token":"<pad>","mask_token":"<mask>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:text-embeddings-inference","source_url":"https://github.com/huggingface/text-embeddings-inference"},{"type":"has_code","target_id":"github:PolyAI-LDN:conversational-datasets","source_url":"https://github.com/PolyAI-LDN/conversational-datasets"},{"type":"has_code","target_id":"github:allenai:s2orc","source_url":"https://github.com/allenai/s2orc"},{"type":"has_code","target_id":"github:afader:oqa","source_url":"https://github.com/afader/oqa#wikianswers-corpus"},{"type":"has_code","target_id":"github:facebookresearch:PAQ","source_url":"https://github.com/facebookresearch/PAQ"},{"type":"has_code","target_id":"github:allenai:s2orc","source_url":"https://github.com/allenai/s2orc"},{"type":"has_code","target_id":"github:allenai:s2orc","source_url":"https://github.com/allenai/s2orc"},{"type":"has_code","target_id":"github:allenai:gooaq","source_url":"https://github.com/allenai/gooaq"},{"type":"has_code","target_id":"github:allenai:specter","source_url":"https://github.com/allenai/specter"},{"type":"has_code","target_id":"github:google-research-datasets:sentence-compression","source_url":"https://github.com/google-research-datasets/sentence-compression"},{"type":"has_code","target_id":"github:pvl:wikihow_pairs_dataset","source_url":"https://github.com/pvl/wikihow_pairs_dataset"},{"type":"has_code","target_id":"github:chridey:altlex","source_url":"https://github.com/chridey/altlex"},{"type":"based_on_paper","target_id":"arxiv:1904.06472","source_url":"https://arxiv.org/abs/1904.06472"},{"type":"based_on_paper","target_id":"arxiv:2102.07033","source_url":"https://arxiv.org/abs/2102.07033"},{"type":"based_on_paper","target_id":"arxiv:2104.08727","source_url":"https://arxiv.org/abs/2104.08727"},{"type":"based_on_paper","target_id":"arxiv:1704.05179","source_url":"https://arxiv.org/abs/1704.05179"},{"type":"based_on_paper","target_id":"arxiv:1810.09305","source_url":"https://arxiv.org/abs/1810.09305"}]', NULL, 'Apache-2.0', 'approved', 80, 'a55edfb443b189776be3ad5c18e2e9fe', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-XLabs-AI-flux-RealismLora', 'huggingface--xlabs-ai--flux-realismlora', 'flux-RealismLora', 'XLabs-AI', '--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE. language: - en pipeline_tag: text-to-image tags: - lora - Stable Diffusion - image-generation - Flux - diffusers base_model: black-forest-labs/FLUX.1-dev --- !Lora Photorealism for Flux <img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true"> This repository provides a checkpoint with trai...', '["diffusers","lora","stable diffusion","image-generation","flux","text-to-image","en","base_model:black-forest-labs/flux.1-dev","base_model:adapter:black-forest-labs/flux.1-dev","license:other","region:us"]', 'text-to-image', 1200, 35639, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/XLabs-AI/flux-RealismLora","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.\nlanguage:\n- en\npipeline_tag: text-to-image\ntags:\n- lora\n- Stable Diffusion\n- image-generation\n- Flux\n- diffusers\nbase_model: black-forest-labs/FLUX.1-dev\n---\n![Lora Photorealism for Flux](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/lora-photorealism-header-rev1.png?raw=true)\n[<img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true">](https://discord.gg/FHY2guThfy)\n\nThis repository provides a checkpoint with trained LoRA photorealism for\n[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) by Black Forest Labs\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/picture-6-rev1.png?raw=true)\n# ComfyUI\n\n[See our github](https://github.com/XLabs-AI/x-flux-comfyui) for comfy ui workflows.\n![Example Picture 1](https://github.com/XLabs-AI/x-flux-comfyui/blob/main/assets/image1.png?raw=true)\n# Training details\n[XLabs AI](https://github.com/XLabs-AI) team is happy to publish fine-tuning Flux scripts, including:\n\n- **LoRA** 🔥\n- **ControlNet** 🔥\n\n[See our github](https://github.com/XLabs-AI/x-flux) for train script and train configs.\n\n# Training Dataset\nDataset has the following format for the training process:\n\n```\n├── images/\n│    ├── 1.png\n│    ├── 1.json\n│    ├── 2.png\n│    ├── 2.json\n│    ├── ...\n```\nA .json file contains "caption" field with a text prompt.\n\n# Inference\n```bash\npython3 demo_lora_inference.py \\n    --checkpoint lora.safetensors \\n    --prompt " handsome girl in a suit covered with bold tattoos and holding a pistol. Animatrix illustration style, fantasy style, natural photo cinematic"\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/picture-0-rev1.png?raw=true)\n\n\n# License\n\nlora.safetensors falls under the [FLUX.1 [dev]](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License<br/>', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":75395758,"files_count":3,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"}]', NULL, 'Other', 'approved', 65, 'c97049633bfe7d6cd0a5a0c3408c00cc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.2-1B-Instruct', 'huggingface--meta-llama--llama-3.2-1b-instruct', 'Llama-3.2-1B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","arxiv:2405.16406","license:llama3.2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1187, 3425828, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1235814400,"storage_bytes":4945506836,"files_count":13,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"},{"type":"based_on_paper","target_id":"arxiv:2405.16406","source_url":"https://arxiv.org/abs/2405.16406"}]', NULL, 'llama3.2', 'approved', 40, 'fb104ee074007f0f94ccc299242b05d1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ai21labs-Jamba-v0.1', 'huggingface--ai21labs--jamba-v0.1', 'Jamba-v0.1', 'ai21labs', '--- library_name: transformers license: apache-2.0 tags: - jamba - mamba - moe --- This is the base version of the Jamba model. We’ve since released a better, instruct-tuned version, Jamba-1.5-Mini. For even greater performance, check out the scaled-up Jamba-1.5-Large. Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. It delivers throughput gains over traditional Transformer-based models, while outperforming or matching the leading models of its size class on most common benchmarks. Ja...', '["transformers","safetensors","jamba","text-generation","mamba","moe","custom_code","arxiv:2403.19887","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 1186, 1127, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ai21labs/Jamba-v0.1","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- jamba\n- mamba\n- moe\n---\n\nThis is the base version of the Jamba model. We’ve since released a better, instruct-tuned version, [Jamba-1.5-Mini](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini). For even greater performance, check out the scaled-up [Jamba-1.5-Large](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large).\n\n# Model Card for Jamba\n\nJamba is a state-of-the-art, hybrid SSM-Transformer LLM. It delivers throughput gains over traditional Transformer-based models, while outperforming or matching the leading models of its size class on most common benchmarks.\n\nJamba is the first production-scale Mamba implementation, which opens up interesting research and application opportunities. While this initial experimentation shows encouraging gains, we expect these to be further enhanced with future optimizations and explorations.\n\nThis model card is for the base version of Jamba. It’s a pretrained, mixture-of-experts (MoE) generative text model, with 12B active parameters and a total of 52B parameters across all experts. It supports a 256K context length, and can fit up to 140K tokens on a single 80GB GPU.\n\nFor full details of this model please read the [white paper](https://arxiv.org/abs/2403.19887) and the [release blog post](https://www.ai21.com/blog/announcing-jamba).\n\n## Model Details\n\n- **Developed by:** [AI21](https://www.ai21.com)\n- **Model type:** Joint Attention and Mamba (Jamba)\n- **License:** Apache 2.0\n- **Context length:** 256K\n- **Knowledge cutoff date:** March 5, 2024\n\n## Usage\n### Presequities\nIn order to use Jamba, it is recommended you use `transformers` version 4.40.0 or higher (version 4.39.0 or higher is required):\n```bash\npip install transformers>=4.40.0\n```\n\nIn order to run optimized Mamba implementations, you first need to install `mamba-ssm` and `causal-conv1d`:\n```bash\npip install mamba-ssm causal-conv1d>=1.2.0\n```\nYou also have to have the model on a CUDA device.\n\nYou can run the model not using the optimized Mamba kernels, but it is **not** recommended as it will result in significantly lower latencies. In order to do that, you''ll need to specify `use_mamba_kernels=False` when loading the model.\n\n### Run the model\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained("ai21labs/Jamba-v0.1")\ntokenizer = AutoTokenizer.from_pretrained("ai21labs/Jamba-v0.1")\n\ninput_ids = tokenizer("In the recent Super Bowl LVIII,", return_tensors=''pt'').to(model.device)["input_ids"]\n\noutputs = model.generate(input_ids, max_new_tokens=216)\n\nprint(tokenizer.batch_decode(outputs))\n# ["<|startoftext|>In the recent Super Bowl LVIII, the Kansas City Chiefs emerged victorious, defeating the San Francisco 49ers in a thrilling overtime showdown. The game was a nail-biter, with both teams showcasing their skills and determination.\n\nThe Chiefs, led by their star quarterback Patrick Mahomes, displayed their offensive prowess, while the 49ers, led by their strong defense, put up a tough fight. The game went into overtime, with the Chiefs ultimately securing the win with a touchdown.\n\nThe victory marked the Chiefs'' second Super Bowl win in four years, solidifying their status as one of the top teams in the NFL. The game was a testament to the skill and talent of both teams, and a thrilling end to the NFL season.\n\nThe Super Bowl is not just about the game itself, but also about the halftime show and the commercials. This year''s halftime show featured a star-studded lineup, including Usher, Alicia Keys, and Lil Jon. The show was a spectacle of music and dance, with the performers delivering an energetic and entertaining performance.\n"]\n```\n\nPlease note that if you''re using `transformers<4.40.0`, `trust_remote_code=True` is required for running the new Jamba architecture.\n\n<details>\n<summary><strong>Loading the model in half precision</strong></summary>\n  \n  The published checkpoint is saved in BF16. In order to load it into RAM in BF16/FP16, you need to specify `torch_dtype`:\n  \n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained("ai21labs/Jamba-v0.1",\n                                             torch_dtype=torch.bfloat16)    # you can also use torch_dtype=torch.float16\n```\n\nWhen using half precision, you can enable the [FlashAttention2](https://github.com/Dao-AILab/flash-attention) implementation of the Attention blocks. In order to use it, you also need the model on a CUDA device. Since in this precision the model is to big to fit on a single 80GB GPU, you''ll also need to parallelize it using [accelerate](https://huggingface.co/docs/accelerate/index):\n```python\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained("ai21labs/Jamba-v0.1",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation="flash_attention_2",\n                                             device_map="auto")\n```\n\n</details>\n<details><summary><strong>Load the model in 8-bit</strong></summary>\n  \n  **Using 8-bit precision, it is possible to fit up to 140K sequence lengths on a single 80GB GPU.** You can easily quantize the model to 8-bit using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index). In order to not degrade model quality, we recommend to exclude the Mamba blocks from the quantization:\n\n```python\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True,\n                                         llm_int8_skip_modules=["mamba"])\nmodel = AutoModelForCausalLM.from_pretrained("ai21labs/Jamba-v0.1",\n                                             torch_dtype=torch.bfloat16,\n                                             attn_implementation="flash_attention_2",\n                                             quantization_config=quantization_config)\n```\n</details>\n\n### Fine-tuning example\nJamba is a base model that can be fine-tuned for custom solutions (including for chat/instruct versions). You can fine-tune it using any technique of your choice. Here is an example of fine-tuning with the [PEFT](https://huggingface.co/docs/peft/index) library (requires ~120GB GPU RAM, in example 2xA100 80GB):\n\n```python\nimport torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n\ntokenizer = AutoTokenizer.from_pretrained("ai21labs/Jamba-v0.1")\nmodel = AutoModelForCausalLM.from_pretrained(\n    "ai21labs/Jamba-v0.1", device_map=''auto'', torch_dtype=torch.bfloat16)\n\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\n        "embed_tokens", \n        "x_proj", "in_proj", "out_proj", # mamba\n        "gate_proj", "up_proj", "down_proj", # mlp\n        "q_proj", "k_proj", "v_proj" # attention\n    ],\n    task_type="CAUSAL_LM",\n    bias="none"\n)\n\ndataset = load_dataset("Abirate/english_quotes", split="train")\ntraining_args = SFTConfig(\n    output_dir="./results",\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n    logging_dir=''./logs'',\n    logging_steps=10,\n    learning_rate=1e-5,\n    dataset_text_field="quote",\n)\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n)\ntrainer.train()\n```\n\n## Results on common benchmarks\n| Benchmark    | Score |\n|--------------|:-----:|\n| HellaSwag    | 87.1% |\n| Arc Challenge | 64.4% |\n| WinoGrande   | 82.5% |\n| PIQA        | 83.2% |\n| MMLU       | 67.4% |\n| BBH            | 45.4% |\n| TruthfulQA          | 46.4% |\n| GSM8K (CoT)            | 59.9% |\n\nIt''s crucial that the ''BOS'' token is added to all prompts, which might not be enabled by default in all eval frameworks.\n\n\n## Notice\nJamba is a pretrained base model and did not undergo any alignment for instruct/chat interactions. \n\nAs a base model, Jamba is intended for use as a foundation layer for fine tuning, training, and developing custom solutions. Jamba does not have safety moderation mechanisms and guardrails should be added for responsible and safe use.\n\n## About AI21\nAI21 builds reliable, practical, and scalable AI solutions for the enterprise.\n\nJamba is the first in AI21’s new family of models, and the Instruct version of Jamba is coming soon to the [AI21 platform](https://www.ai21.com/studio). \n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":51570323328,"storage_bytes":206290066038,"files_count":32,"spaces_count":16,"gated":false,"private":false,"config":{"architectures":["JambaForCausalLM"],"auto_map":{"AutoConfig":"configuration_jamba.JambaConfig","AutoModel":"modeling_jamba.JambaModel","AutoModelForCausalLM":"modeling_jamba.JambaForCausalLM","AutoModelForSequenceClassification":"model.JambaForSequenceClassification"},"model_type":"jamba","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|endoftext|>","pad_token":"<|pad|>","unk_token":"<|unk|>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2403.19887","source_url":"https://arxiv.org/abs/2403.19887"}]', NULL, 'Apache-2.0', 'approved', 65, '7c990b1967b0b09d2babf5bc78e09011', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-40b-instruct', 'huggingface--tiiuae--falcon-40b-instruct', 'falcon-40b-instruct', 'tiiuae', '--- datasets: - tiiuae/falcon-refinedweb language: - en inference: false license: apache-2.0 --- **Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.** *Paper coming soon 😊.* 🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! * **You are looking for a ready-to-use chat/instruct model b...', '["transformers","pytorch","falcon","text-generation","custom_code","en","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","arxiv:2304.01196","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1179, 44233, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-40b-instruct","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\ninference: false\nlicense: apache-2.0\n---\n\n# ✨ Falcon-40B-Instruct\n\n**Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of [Baize](https://github.com/project-baize/baize-chatbot). It is made available under the Apache 2.0 license.**\n\n*Paper coming soon 😊.*\n\n🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-40B-Instruct?\n\n* **You are looking for a ready-to-use chat/instruct model based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).**\n* **Falcon-40B is the best open-source model available.** It outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n\n💬 **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b). \n\n💸 **Looking for a smaller, less expensive model?** [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) is Falcon-40B-Instruct''s little brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-40b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 85-100GB of memory** to swiftly run inference with Falcon-40B.\n\n\n\n# Model Card for Falcon-40B-Instruct\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English and French;\n- **License:** Apache 2.0;\n- **Finetuned from model:** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nFalcon-40B-Instruct has been finetuned on a chat dataset.\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-40B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-40B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-40b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-40B-Instruct was finetuned on a 150M tokens from [Bai ze](https://github.com/project-baize/baize-chatbot) mixed with 5% of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) data. \n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\nFor more information about pretraining, see [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).\n\n### Model Architecture and Objective\n\nFalcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\nFor multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 60        |                                        |\n| `d_model`          | 8192      |                                        |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-40B-Instruct was trained on AWS SageMaker, on 64 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-40B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* 😊. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\nTo cite the [Baize](https://github.com/project-baize/baize-chatbot) instruction dataset used for this model: \n```\n@article{xu2023baize,\n  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},\n  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},\n  journal={arXiv preprint arXiv:2304.01196},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-40B-Instruct is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":167352935898,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["FalconForCausalLM"],"auto_map":{"AutoConfig":"configuration_falcon.FalconConfig","AutoModel":"modeling_falcon.FalconModel","AutoModelForSequenceClassification":"modeling_falcon.FalconForSequenceClassification","AutoModelForTokenClassification":"modeling_falcon.FalconForTokenClassification","AutoModelForQuestionAnswering":"modeling_falcon.FalconForQuestionAnswering","AutoModelForCausalLM":"modeling_falcon.FalconForCausalLM"},"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"},{"type":"based_on_paper","target_id":"arxiv:2304.01196","source_url":"https://arxiv.org/abs/2304.01196"}]', NULL, 'Apache-2.0', 'approved', 65, '84785c2c877cf93e2fa83d06ffc79b8f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mosaicml-mpt-7b', 'huggingface--mosaicml--mpt-7b', 'mpt-7b', 'mosaicml', '--- license: apache-2.0 tags: - Composer - MosaicML - llm-foundry - StreamingDatasets datasets: - mc4 - c4 - togethercomputer/RedPajama-Data-1T - bigcode/the-stack - allenai/s2orc inference: false --- MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML. MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and in...', '["transformers","pytorch","mpt","text-generation","composer","mosaicml","llm-foundry","streamingdatasets","custom_code","dataset:mc4","dataset:c4","dataset:togethercomputer/redpajama-data-1t","dataset:bigcode/the-stack","dataset:allenai/s2orc","arxiv:2108.12409","arxiv:2302.13971","arxiv:2205.14135","arxiv:2010.04245","arxiv:1909.08053","arxiv:2302.06675","license:apache-2.0","text-generation-inference","region:us"]', 'text-generation', 1174, 13039, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mosaicml/mpt-7b","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- Composer\n- MosaicML\n- llm-foundry\n- StreamingDatasets\ndatasets:\n- mc4\n- c4\n- togethercomputer/RedPajama-Data-1T\n- bigcode/the-stack\n- allenai/s2orc\ninference: false\n---\n\n# MPT-7B\n\nMPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.\nThis model was trained by [MosaicML](https://www.mosaicml.com).\n\nMPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\n\nThese architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing\npositional embeddings with Attention with Linear Biases ([ALiBi](https://arxiv.org/abs/2108.12409)).\nThanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.\nMPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA''s [FasterTransformer](https://github.com/NVIDIA/FasterTransformer).\n\nThis model uses the MosaicML LLM codebase, which can be found in the [llm-foundry repository](https://github.com/mosaicml/llm-foundry). It was trained by MosaicML’s NLP team on the [MosaicML platform](https://www.mosaicml.com/training) for LLM pretraining, finetuning, and inference.\n\n### How is this model different?\n\nMPT-7B is\n\n* **Licensed for the possibility of commercial use** (unlike [LLaMA](https://arxiv.org/abs/2302.13971)).\n* **Trained on a large amount of data** (1T tokens like [LLaMA](https://arxiv.org/abs/2302.13971) vs. 300B for [Pythia](https://github.com/EleutherAI/pythia), 300B for [OpenLLaMA](https://github.com/openlm-research/open_llama), and 800B for [StableLM](https://github.com/Stability-AI/StableLM)).\n* **Prepared to handle extremely long inputs** thanks to [ALiBi](https://arxiv.org/abs/2108.12409) (we finetuned [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter) on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).\n* **Capable of fast training and inference** (via [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer))\n* **Equipped with highly efficient open-source training code** via the [llm-foundry repository](https://github.com/mosaicml/llm-foundry)\n\n### Models finetuned off MPT-7B:\n\nThe following models are finetuned on MPT-7B:\n\n* [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter): a model designed to read and write fictional stories with super long context lengths.\nBuilt by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the [books3 dataset](https://huggingface.co/datasets/the_pile_books3).\nAt inference time, thanks to [ALiBi](https://arxiv.org/abs/2108.12409), MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 80k tokens on a single A100-80GB GPU in our [blogpost](www.mosaicml.com/blog/mpt-7b).\n  * License: Apache 2.0\n\n* [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct): a model for short-form instruction following.\nBuilt by finetuning MPT-7B on a [dataset](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) we also release, derived from the [Databricks Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) and the [Anthropic Helpful and Harmless (HH-RLHF)](https://huggingface.co/datasets/Anthropic/hh-rlhf) datasets.\n  * License: Apache 2.0\n\n* [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat): a chatbot-like model for dialogue generation.\nBuilt by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3),\n [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets.\n  * License: _CC-By-NC-SA-4.0_\n\n## Model Date\n\nMay 5, 2023\n\n## Model License\n\nApache-2.0\n\n## Documentation\n\n* [Blog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)\n* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n* Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n## How to Use\n\nThis model is best used with the MosaicML [llm-foundry repository](https://github.com/mosaicml/llm-foundry) for training and finetuning.\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  ''mosaicml/mpt-7b'',\n  trust_remote_code=True\n)\n```\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method.\nThis is because we use a custom `MPT` model architecture that is not yet part of the Hugging Face `transformers` package.\n`MPT` includes options for many training efficiency features such as [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), [QK LayerNorm](https://arxiv.org/abs/2010.04245), and more.\n\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl=''triton''` and with `bfloat16` precision:\n```python\nimport torch\nimport transformers\n\nname = ''mosaicml/mpt-7b''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config[''attn_impl''] = ''triton''\nconfig.init_device = ''cuda:0'' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n```\n\nAlthough the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n\n```python\nimport transformers\n\nname = ''mosaicml/mpt-7b''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n```\n\nThis model was trained with the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(''EleutherAI/gpt-neox-20b'')\n```\n\nThe model can then be used, for example, within a text-generation pipeline.  \nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(''text-generation'', model=model, tokenizer=tokenizer, device=''cuda:0'')\n\nwith torch.autocast(''cuda'', dtype=torch.bfloat16):\n    print(\n        pipe(''Here is a recipe for vegan banana bread:\n'',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n```\n\n## Model Description\n\nThe architecture is a modification of a standard decoder-only transformer.\n\nThe model has been modified from a standard transformer in the following ways:\n* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n* It does not use biases\n\n\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 6.7B |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 50432 |\n| sequence length | 2048 |\n\n\n\n## Training Data\n\n### Streaming Datasets\n\nData was formatted using the MosaicML [StreamingDataset](https://github.com/mosaicml/streaming) library to host our data in object storage and efficiently stream it to our compute cluster during training.\nStreamingDataset obviates the need to download the whole dataset before starting training, and allows instant resumption of training from any point in the dataset.\n\n\n### Data Mix\n\nThe model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:\n\n\n| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |\n|-------------|----------------------------|------------|----------------------------|--------|\n| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |\n| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |\n| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |\n| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |\n| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |\n| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |\n| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |\n| RedPajama - Books | 26.02 B | 0.03 | 30B | 1.15 |\n| RedPajama - arXiv | 28.10 B | 0.019 | 19 B | 0.68 |\n| RedPajama - StackExchange | 20.54 B | 0.014 | 14 B |0.68 |\n\nSamples for each batch were selected from one of the datasets with the probability specified above.\nThe examples were shuffled within each dataset, and each example was constructed from as many sequences from that dataset as were necessary to fill the 2048 sequence length.\n\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer. This BPE tokenizer has a number of desirable characteristics,\nmost of which are relevant for tokenizing code:\n(1) It was trained on a diverse mix of data that includes code (The Pile)\n(2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces\n(3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\n\nThe model vocabulary size of 50432 was set to be a multiple of 128 (as in [MEGATRON-LM](https://arxiv.org/abs/1909.08053)), model flop utilization (MFU) increased by up to four percentage points.\n\n### Training Configuration\n\nThis model was trained on 440 A100-40GBs for about 9.5 days using the [MosaicML Platform](https://www.mosaicml.com/platform).\nThe model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n\n## Limitations and Biases\n\n_The following language is modified from [EleutherAI''s GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\n\nMPT-7B (Base) is **not** intended for deployment without finetuning.\nIt should not be used for human-facing interactions without further guardrails and user consent.\n\nMPT-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n\n## MosaicML Platform\n\nIf you''re interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b).\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\n\n## Citation\n\nPlease cite this model using the following format:\n\n```\n@online{MosaicML2023Introducing,\n    author    = {MosaicML NLP Team},\n    title     = {Introducing MPT-7B: A New Standard for Open-Source,\n    Commercially Usable LLMs},\n    year      = {2023},\n    url       = {www.mosaicml.com/blog/mpt-7b},\n    note      = {Accessed: 2023-05-05},\n    urldate   = {2023-05-05}\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":26597235302,"files_count":26,"spaces_count":60,"gated":false,"private":false,"config":{"architectures":["MPTForCausalLM"],"auto_map":{"AutoConfig":"configuration_mpt.MPTConfig","AutoModelForCausalLM":"modeling_mpt.MPTForCausalLM"},"model_type":"mpt","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:NVIDIA:FasterTransformer","source_url":"https://github.com/NVIDIA/FasterTransformer"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:EleutherAI:pythia","source_url":"https://github.com/EleutherAI/pythia"},{"type":"has_code","target_id":"github:openlm-research:open_llama","source_url":"https://github.com/openlm-research/open_llama"},{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:NVIDIA:FasterTransformer","source_url":"https://github.com/NVIDIA/FasterTransformer"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:openai:triton","source_url":"https://github.com/openai/triton"},{"type":"has_code","target_id":"github:mosaicml:streaming","source_url":"https://github.com/mosaicml/streaming"},{"type":"based_on_paper","target_id":"arxiv:2108.12409","source_url":"https://arxiv.org/abs/2108.12409"},{"type":"based_on_paper","target_id":"arxiv:2302.13971","source_url":"https://arxiv.org/abs/2302.13971"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2010.04245","source_url":"https://arxiv.org/abs/2010.04245"},{"type":"based_on_paper","target_id":"arxiv:1909.08053","source_url":"https://arxiv.org/abs/1909.08053"},{"type":"based_on_paper","target_id":"arxiv:2302.06675","source_url":"https://arxiv.org/abs/2302.06675"}]', NULL, 'Apache-2.0', 'approved', 80, 'a2bbd05932b62d1fa2176e5426211673', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ByteDance-Seed-BAGEL-7B-MoT', 'huggingface--bytedance-seed--bagel-7b-mot', 'BAGEL-7B-MoT', 'ByteDance-Seed', '--- license: apache-2.0 base_model: - Qwen/Qwen2.5-7B-Instruct pipeline_tag: any-to-any library_name: bagel-mot --- <p align="left"> <img src="https://lf3-static.bytednsdoc.com/obj/eden-cn/nuhojubrps/banner.png" alt="BAGEL" width="480"/> </p> <p align="left"> <a href="https://bagel-ai.org/"> <img src="https://img.shields.io/badge/BAGEL-Website-0A66C2?logo=safari&logoColor=white" style="display: inline-block; vertical-align: middle;" alt="BAGEL Website" /> </a> <a href="https://arxiv.org/abs/2...', '["bagel-mot","safetensors","bagel","any-to-any","custom_code","arxiv:2505.14683","base_model:qwen/qwen2.5-7b-instruct","base_model:finetune:qwen/qwen2.5-7b-instruct","license:apache-2.0","region:us"]', 'any-to-any', 1164, 839, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen2.5-7B-Instruct\npipeline_tag: any-to-any\nlibrary_name: bagel-mot\n---\n\n\n<p align="left">\n  <img src="https://lf3-static.bytednsdoc.com/obj/eden-cn/nuhojubrps/banner.png" alt="BAGEL" width="480"/>\n</p>\n\n\n# 🥯 BAGEL • Unified Model for Multimodal Understanding and Generation\n\n\n\n<p align="left">\n  <a href="https://bagel-ai.org/">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Website-0A66C2?logo=safari&logoColor=white" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Website"\n    />\n  </a>\n  <a href="https://arxiv.org/abs/2505.14683">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Paper-red?logo=arxiv&logoColor=red" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Paper on arXiv"\n    />\n  </a>\n  <a href="https://github.com/bytedance-seed/BAGEL" target="_blank" style="margin: 2px;">\n      <img \n        alt="Github" src="https://img.shields.io/badge/BAGEL-Codebase-536af5?color=536af5&logo=github" style="display: inline-block; vertical-align: middle;"\n        alt="BAGEL Codebase"\n      />\n  </a>\n  <a href="https://demo.bagel-ai.org/">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Demo-blue?logo=googleplay&logoColor=white" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Demo"\n    />\n  </a>\n  <a href="https://discord.com/invite/Z836xxzy">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Discord-green?logo=discord&logoColor=white" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Discord"\n    />\n  </a>\n\n  \n</p>\n\n\n> We present **BAGEL**, an open‑source multimodal foundation model with 7B active parameters (14B total) trained on large‑scale interleaved multimodal data. BAGEL outperforms the current top‑tier open‑source VLMs like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards, and delivers text‑to‑image quality that is competitive with strong specialist generators such as SD3.\nMoreover, BAGEL demonstrates superior qualitative results in classical image‑editing scenarios than the leading open-source models. More importantly, it extends to free-form visual manipulation, multiview synthesis, and world navigation, capabilities that constitute "world-modeling" tasks beyond the scope of previous image-editing models.\n\n\nThis repository hosts the model weights for **BAGEL**. For installation, usage instructions, and further documentation, please visit our [GitHub repository](https://github.com/bytedance-seed/BAGEL).\n\n\n\n<p align="left"><img src="https://github.com/ByteDance-Seed/Bagel/raw/main/assets/teaser.webp" width="80%"></p>\n\n\n\n\n\n\n## 🧠 Method\nBAGEL adopts a Mixture-of-Transformer-Experts (MoT) architecture to maximize the model’s capacity to learn from richly diverse multimodal information. Following the same principle of capacity maximization, it utilizes two separate encoders to capture pixel-level and semantic-level features of an image. The overall framework follows a Next Group of Token Prediction paradigm, where the model is trained to predict the next group of language or visual tokens as a compression target.\n\nBAGEL scales MoT’s capacity through Pre-training, Continued Training, and Supervised Finetuning on trillions of interleaved multimodal tokens spanning language, image, video, and web data. It surpasses open models on standard understanding and generation benchmarks and demonstrates advanced in-context multimodal abilities like free-form image editing, future frame prediction, 3D manipulation, world navigation, and sequential reasoning.\n\n<p align="left"><img src="https://github.com/ByteDance-Seed/Bagel/raw/main/assets/arch.png" width="50%"></p>\n\n\n## 🌱 Emerging Properties\n<p align="left"><img src="https://github.com/ByteDance-Seed/Bagel/raw/main/assets/emerging_curves.png" width="50%"></p>\n\nAs we scale up BAGEL’s pretraining with more multimodal tokens, we observe consistent performance gains across understanding, generation, and editing tasks. Different capabilities emerge at distinct training stages—multimodal understanding and generation appear early, followed by basic editing, while complex, intelligent editing emerges later. This staged progression suggests an emergent pattern, where advanced multimodal reasoning builds on well-formed foundational skills. Ablation studies further show that combining VAE and ViT features significantly improves intelligent editing, underscoring the importance of visual-semantic context in enabling complex multimodal reasoning and further supporting its role in the emergence of advanced capabilities.\n\n\n\n## 📊 Benchmarks\n### 1. Visual Understanding\n| Model | MME ↑ | MMBench ↑ |   MMMU ↑ | MM-Vet ↑ | MathVista ↑ |\n| ------------------- | ----------: | ----------: | -------: | -------: | ----------: |\n| Janus-Pro-7B        | -  |     79.2 |     41.0 |     50.0 |           – |\n| Qwen2.5-VL-7B      | 2347    |   83.5 | **58.6** |     67.1 |           68.2 |\n| **BAGEL**    | **2388**  |  **85.0** |     55.3 | **67.2** |    **73.1** |\n### 2. Text-to-Image Generation · GenEval\n| Model        | Overall ↑ |\n| ------------ | --------- |\n| FLUX-1-dev   | 0.82      |\n| SD3-Medium   | 0.74      |\n| Janus-Pro-7B | 0.80      |\n| **BAGEL**    | **0.88**  |\n### 3. Image Editing\n| Model         | GEdit-Bench-EN (SC) ↑ | GEdit-Bench-EN (PQ) ↑ | GEdit-Bench-EN (O) ↑ | IntelligentBench ↑ |\n| ------------- | --------------------- | --------------------- | ------------------- | ------------------ |\n| Step1X-Edit   | 7.09                  | 6.76                  | **6.70**            | 14.9               |\n| Gemini-2-exp. | 6.73                  | 6.61                  | 6.32                | **57.6**           |\n| **BAGEL**     | **7.36**              | **6.83**              | 6.52                | 44.0               |\n| **BAGEL+CoT** | –                   | –                     | –                   | 55.3               |\n\n## License\nBAGEL is licensed under the Apache 2.0 license. It is finetuned from [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) and [siglip-so400m-14-384-flash-attn2](https://huggingface.co/HuggingFaceM4/siglip-so400m-14-384-flash-attn2) model, and uses the [FLUX.1-schnell VAE model](https://huggingface.co/black-forest-labs/FLUX.1-schnell), all under Apache 2.0.\n\n## ✍️ Citation\n```bibtex\n@article{deng2025bagel,\n  title   = {Emerging Properties in Unified Multimodal Pretraining},\n  author  = {Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and Shi, Guang and Fan, Haoqi},\n  journal = {arXiv preprint arXiv:2505.14683},\n  year    = {2025}\n}\n```', '{"pipeline_tag":"any-to-any","library_name":"bagel-mot","framework":"bagel-mot","params":14691079811,"storage_bytes":29555840231,"files_count":14,"spaces_count":9,"gated":false,"private":false,"config":{"architectures":["BagelForConditionalGeneration"],"model_type":"bagel","auto_map":{"AutoConfig":"configuration_bagel.BagelConfig","AutoModelForCausalLM":"modeling_bagel.BagelForConditionalGeneration"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:bytedance-seed:BAGEL\"","source_url":"https://github.com/bytedance-seed/BAGEL\""},{"type":"has_code","target_id":"github:bytedance-seed:BAGEL","source_url":"https://github.com/bytedance-seed/BAGEL"},{"type":"has_code","target_id":"github:ByteDance-Seed:Bagel","source_url":"https://github.com/ByteDance-Seed/Bagel"},{"type":"has_code","target_id":"github:ByteDance-Seed:Bagel","source_url":"https://github.com/ByteDance-Seed/Bagel"},{"type":"has_code","target_id":"github:ByteDance-Seed:Bagel","source_url":"https://github.com/ByteDance-Seed/Bagel"},{"type":"based_on_paper","target_id":"arxiv:2505.14683","source_url":"https://arxiv.org/abs/2505.14683"}]', NULL, 'Apache-2.0', 'approved', 65, 'aea253171c69367202e1db556079b56e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-timbrooks-instruct-pix2pix', 'huggingface--timbrooks--instruct-pix2pix', 'instruct-pix2pix', 'timbrooks', '--- license: mit tags: - image-to-image --- GitHub: https://github.com/timothybrooks/instruct-pix2pix <img src=''https://instruct-pix2pix.timothybrooks.com/teaser.jpg''/> To use , install using for now. The pipeline will be available in the next release', '["diffusers","safetensors","image-to-image","license:mit","diffusers:stablediffusioninstructpix2pixpipeline","region:us"]', 'image-to-image', 1162, 59244, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/timbrooks/instruct-pix2pix","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\ntags:\n- image-to-image\n---\n\n# InstructPix2Pix: Learning to Follow Image Editing Instructions\nGitHub: https://github.com/timothybrooks/instruct-pix2pix\n<img src=''https://instruct-pix2pix.timothybrooks.com/teaser.jpg''/>\n\n\n\n## Example\n\nTo use `InstructPix2Pix`, install `diffusers` using `main` for now. The pipeline will be available in the next release\n\n```bash\npip install diffusers accelerate safetensors transformers\n```\n\n```python\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n\nmodel_id = "timbrooks/instruct-pix2pix"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\npipe.to("cuda")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n\nurl = "https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg"\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert("RGB")\n    return image\nimage = download_image(url)\n\nprompt = "turn him into cyborg"\nimages = pipe(prompt, image=image, num_inference_steps=10, image_guidance_scale=1).images\nimages[0]\n```', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":37333325594,"files_count":31,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionInstructPix2PixPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:timothybrooks:instruct-pix2pix","source_url":"https://github.com/timothybrooks/instruct-pix2pix"}]', NULL, 'MIT', 'approved', 50, '40788bd799dece7ad9a18971c80231e2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-chatglm3-6b', 'huggingface--zai-org--chatglm3-6b', 'chatglm3-6b', 'zai-org', '--- language: - zh - en tags: - glm - chatglm - thudm --- <p align="center"> 💻 <a href="https://github.com/THUDM/ChatGLM" target="_blank">Github Repo</a> • 🐦 <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> • 📃 <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> • 📃 <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GL...', '["transformers","pytorch","safetensors","chatglm","glm","thudm","custom_code","zh","en","arxiv:2103.10360","arxiv:2210.02414","arxiv:2406.12793","endpoints_compatible","region:us"]', 'other', 1157, 63722, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/chatglm3-6b","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlanguage:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm\n---\n# ChatGLM3-6B\n<p align="center">\n  💻 <a href="https://github.com/THUDM/ChatGLM" target="_blank">Github Repo</a> • 🐦 <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> • 📃 <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> • 📃 <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a> <br>\n</p>\n\n<p align="center">\n    👋 Join our <a href="https://join.slack.com/t/chatglm/shared_invite/zt-25ti5uohv-A_hs~am_D3Q8XPZMpj7wwQ" target="_blank">Slack</a> and <a href="https://github.com/THUDM/ChatGLM/blob/main/resources/WECHAT.md" target="_blank">WeChat</a>\n</p>\n<p align="center">\n📍Experience the larger-scale ChatGLM model at <a href="https://www.chatglm.cn">chatglm.cn</a>\n</p>\n\n## GLM-4 开源模型\n\n我们已经发布最新的 **GLM-4** 模型，该模型在多个指标上有了新的突破，您可以在以下两个渠道体验我们的最新模型。\n+ [GLM-4 开源模型](https://huggingface.co/THUDM/glm-4-9b-chat) 我们已经开源了 GLM-4-9B 系列模型，在各项指标的测试上有明显提升，欢迎尝试。\n\n## 介绍 (Introduction)\nChatGLM3-6B 是 ChatGLM 系列最新一代的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性：\n\n1. **更强大的基础模型：** ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，ChatGLM3-6B-Base 具有在 10B 以下的预训练模型中最强的性能。\n2. **更完整的功能支持：** ChatGLM3-6B 采用了全新设计的 [Prompt 格式](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md)，除正常的多轮对话外。同时原生支持[工具调用](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README.md)（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。\n3. **更全面的开源序列：** 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM-6B-Base、长文本对话模型 ChatGLM3-6B-32K。以上所有权重对学术研究**完全开放**，在填写[问卷](https://open.bigmodel.cn/mla/form)进行登记后**亦允许免费商业使用**。\n\nChatGLM3-6B is the latest open-source model in the ChatGLM series. While retaining many excellent features such as smooth dialogue and low deployment threshold from the previous two generations, ChatGLM3-6B introduces the following features:\n\n1. **More Powerful Base Model:** The base model of ChatGLM3-6B, ChatGLM3-6B-Base, employs a more diverse training dataset, more sufficient training steps, and a more reasonable training strategy. Evaluations on datasets such as semantics, mathematics, reasoning, code, knowledge, etc., show that ChatGLM3-6B-Base has the strongest performance among pre-trained models under 10B.\n2. **More Comprehensive Function Support:** ChatGLM3-6B adopts a newly designed [Prompt format](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT_en.md), in addition to the normal multi-turn dialogue. It also natively supports [function call](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README_en.md), code interpreter, and complex scenarios such as agent tasks.\n3. **More Comprehensive Open-source Series:** In addition to the dialogue model ChatGLM3-6B, the base model ChatGLM-6B-Base and the long-text dialogue model ChatGLM3-6B-32K are also open-sourced. All the weights are **fully open** for academic research, and after completing the [questionnaire](https://open.bigmodel.cn/mla/form) registration, they are also **allowed for free commercial use**.\n\n## 软件依赖 (Dependencies)\n\n```shell\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate\n```\n\n## 代码调用 (Code Usage)\n\n可以通过如下代码调用 ChatGLM3-6B 模型来生成对话：\n\nYou can generate dialogue by invoking the ChatGLM3-6B model with the following code:\n\n```ipython\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().cuda()\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, "你好", history=[])\n>>> print(response)\n你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\n>>> response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)\n>>> print(response)\n晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:\n\n1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。\n2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。\n3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。\n4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。\n5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。\n6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。\n\n如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。\n```\n\n关于更多的使用说明，包括如何运行命令行和网页版本的 DEMO，以及使用模型量化以节省显存，请参考我们的 [Github Repo](https://github.com/THUDM/ChatGLM)。\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM).\n\n\n## 协议 (License)\n\n本仓库的代码依照 [Apache-2.0](LICENSE) 协议开源，ChatGLM3-6B 模型的权重的使用则需要遵循 [Model License](MODEL_LICENSE)。\n\nThe code in this repository is open-sourced under the [Apache-2.0 license](LICENSE), while the use of the ChatGLM3-6B model weights needs to comply with the [Model License](MODEL_LICENSE).\n\n## 引用 (Citation)\n\n如果你觉得我们的工作有帮助的话，请考虑引用下列论文。\n\nIf you find our work helpful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id=''cs.CL'' full_name=''Computation and Language'' is_active=True alt_name=''cmp-lg'' in_archive=''cs'' is_general=False description=''Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.''}\n}\n```\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":6243584032,"storage_bytes":37462649896,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"model_type":"chatglm","architectures":["ChatGLMModel"],"auto_map":{"AutoConfig":"configuration_chatglm.ChatGLMConfig","AutoModel":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForCausalLM":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForSeq2SeqLM":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForSequenceClassification":"modeling_chatglm.ChatGLMForSequenceClassification"},"tokenizer_config":{"chat_template":"{% for message in messages %}{% if loop.first %}[gMASK]sop<|{{ message[''role''] }}|>\n {{ message[''content''] }}{% else %}<|{{ message[''role''] }}|>\n {{ message[''content''] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}","eos_token":"</s>","pad_token":"<unk>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:THUDM:ChatGLM\"","source_url":"https://github.com/THUDM/ChatGLM\""},{"type":"has_code","target_id":"github:THUDM:GLM\"","source_url":"https://github.com/THUDM/GLM\""},{"type":"has_code","target_id":"github:THUDM:GLM-130B\"","source_url":"https://github.com/THUDM/GLM-130B\""},{"type":"has_code","target_id":"github:THUDM:ChatGLM","source_url":"https://github.com/THUDM/ChatGLM"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM","source_url":"https://github.com/THUDM/ChatGLM"},{"type":"has_code","target_id":"github:THUDM:ChatGLM","source_url":"https://github.com/THUDM/ChatGLM"},{"type":"based_on_paper","target_id":"arxiv:2103.10360","source_url":"https://arxiv.org/abs/2103.10360"},{"type":"based_on_paper","target_id":"arxiv:2210.02414","source_url":"https://arxiv.org/abs/2210.02414"},{"type":"based_on_paper","target_id":"arxiv:2406.12793","source_url":"https://arxiv.org/abs/2406.12793"}]', NULL, NULL, 'pending', 55, '2810e3ea07b79881d3d5a43fc86e5794', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-4-Scout-17B-16E-Instruct', 'huggingface--meta-llama--llama-4-scout-17b-16e-instruct', 'Llama-4-Scout-17B-16E-Instruct', 'meta-llama', '', '["transformers","safetensors","llama4","any-to-any","facebook","meta","pytorch","llama","ar","de","en","es","fr","hi","id","it","pt","th","tl","vi","arxiv:2204.05149","base_model:meta-llama/llama-4-scout-17b-16e","license:other","text-generation-inference","endpoints_compatible","region:us"]', 'any-to-any', 1154, 206456, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":108641793536,"storage_bytes":217343257722,"files_count":64,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Llama4ForConditionalGeneration"],"model_type":"llama4","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|eot|>","pad_token":"<|finetune_right_pad|>"},"chat_template_jinja":"{{- bos_token }}\n{%- if custom_tools is defined and custom_tools%}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if tools is defined and tools %}\n    {%- set tool_definition = tool_definition ~ (tools | tojson(indent=4)) %}\n{%- else %}\n    {%- set tools = none %}\n{%- endif %}\n\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set user_provided_system_message = true %}\n    {%- if messages[0][''content''] is string %}\n        {%- set system_message = messages[0][''content'']|trim %}\n    {%- else %}\n        {%- set system_message = messages[0][''content''][0][''text'']|trim %}\n    {%- endif %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- if tools is not none  %}\n        {#- Since not system_message was provided by user, if tool is provided, system_message is now default tool system message #}\n        {#- This system message is from llama website:https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/  #}\n        {%- set system_message = \"You are a helpful assistant and an expert in function composition. You can answer general questions using your internal knowledge OR invoke functions when necessary. Follow these strict guidelines:\\n\\n1. FUNCTION CALLS:\\n- ONLY use functions that are EXPLICITLY listed in the function list below\\n- If NO functions are listed (empty function list []), respond ONLY with internal knowledge or \\\"I don''t have access to [Unavailable service] information\\\"\\n- If a function is not in the list, respond ONLY with internal knowledge or \\\"I don''t have access to [Unavailable service] information\\\"\\n- If ALL required parameters are present AND the query EXACTLY matches a listed function''s purpose: output ONLY the function call(s)\\n- Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\\nExamples:\\nCORRECT: [get_weather(location=\\\"Vancouver\\\"), calculate_route(start=\\\"Boston\\\", end=\\\"New York\\\")] <- Only if get_weather and calculate_route are in function list\\nINCORRECT: get_weather(location=\\\"New York\\\")\\nINCORRECT: Let me check the weather: [get_weather(location=\\\"New York\\\")]\\nINCORRECT: [get_events(location=\\\"Singapore\\\")] <- If function not in list\\n\\n2. RESPONSE RULES:\\n- For pure function requests matching a listed function: ONLY output the function call(s)\\n- For knowledge questions: ONLY output text\\n- For missing parameters: ONLY request the specific missing parameters\\n- For unavailable services (not in function list): output ONLY with internal knowledge or \\\"I don''t have access to [Unavailable service] information\\\". Do NOT execute a function call.\\n- If the query asks for information beyond what a listed function provides: output ONLY with internal knowledge about your limitations\\n- NEVER combine text and function calls in the same response\\n- NEVER suggest alternative functions when the requested service is unavailable\\n- NEVER create or invent new functions not listed below\\n\\n3. STRICT BOUNDARIES:\\n- ONLY use functions from the list below - no exceptions\\n- NEVER use a function as an alternative to unavailable information\\n- NEVER call functions not present in the function list\\n- NEVER add explanatory text to function calls\\n- NEVER respond with empty brackets\\n- Use proper Python/JSON syntax for function calls\\n- Check the function list carefully before responding\\n\\n4. TOOL RESPONSE HANDLING:\\n- When receiving tool responses: provide concise, natural language responses\\n- Don''t repeat tool response verbatim\\n- Don''t add supplementary information\\n\\nHere is a list of functions in JSON format that you can invoke:\\n\" %}\n    {%- else %}\n        {%- set system_message = \"\" %}\n    {%- endif %}\n{%- endif %}\n{#- Now writing the system message: use the user provided system message if user_provided_system_message, else default tool system message if tools presented #}\n{%- if system_message %}\n    {#- always use user provided system message to override default tool system message #}\n    {{- \"<|header_start|>system<|header_end|>\\n\\n\" }}\n    {{- system_message }}\n    {%- if user_provided_system_message and tools %}\n        {{- \"\\nHere is a list of functions in JSON format that you can invoke. Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\\n\" }}\n        {{- tool_definition -}}\n        {%- elif tool_definition %}\n        {{- tool_definition -}}\n    {%- endif %}\n    {{- \"<|eot|>\" }}\n{%- endif %}\n\n{#- Now deal with all other messages #}\n{%- for message in messages %}\n    {#- Base case: messages that are not from tool role and has empty tool_call list  #}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or (''tool_calls'' in message and  message.tool_calls|length != 0 )) %}\n        {{- ''<|header_start|>'' + message[''role''] + ''<|header_end|>\\n\\n'' }}\n        {%- if message[''content''] is string %}\n            {{- message[''content''] }}\n        {%- else %}\n            {%- for content in message[''content''] %}\n                {%- if content[''type''] == ''image'' %}\n                    {{- ''<|image|>'' }}\n                {%- elif content[''type''] == ''text'' %}\n                    {{- content[''text''] | trim }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {{- \"<|eot|>\" }}\n    {#- Tool case: messages has non-empty tool_call list, must from assistant #}\n    {%- elif ''tool_calls'' in message %}\n        {#- assume tool_calls are always coming from assistant #}\n        {%- if message.role == ''assistant'' %}\n            {{- ''<|header_start|>assistant<|header_end|>\\n\\n'' -}}\n        {%- if message[''content''] is string %}\n            {{- message[''content''] }}\n        {%- else %}\n            {%- for content in message[''content''] %}\n                {%- if content[''type''] == ''image'' %}\n                    {{- ''<|image|>'' }}\n                {%- elif content[''type''] == ''text'' %}\n                    {{- content[''text''] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n            {{- \"[\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n                {{-  tool_call.name + ''('' -}}\n            {%- for param in tool_call.arguments %}\n                {{- param + ''=\"'' -}}\n                {{- \"%s\" | format(tool_call.arguments[param]) -}}\n                {{- ''\"'' -}}\n                {% if not loop.last %}, {% endif %}\n            {%- endfor %}\n            {{- '')'' -}}\n            {% if not loop.last %}, {% endif %}\n        {%- endfor %}\n        {{- \"]<|eot|>\" }}\n{%- endif %}\n{#- Tool_response case: messages are from tool_response  #}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|header_start|>ipython<|header_end|>\\n\\n\" }}\n        {%- if message.content is string %}\n            {{-  message.content  | tojson }}\n        {%- else %}\n            {%- for content in message[''content'']  %}\n                {%- if content[''type'']  == ''text'' %}\n                    {{-  content[''text''] | tojson }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \"<|eot|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|header_start|>assistant<|header_end|>\\n\\n'' }}\n{%- endif %}"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'Other', 'approved', 40, '772cd880181934f77efecb7bc101b1ac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-rednote-hilab-dots.ocr', 'huggingface--rednote-hilab--dots.ocr', 'dots.ocr', 'rednote-hilab', '--- license: mit library_name: dots_ocr pipeline_tag: image-text-to-text tags: - image-to-text - ocr - document-parse - layout - table - formula - transformers - custom_code language: - en - zh - multilingual --- <div align="center"> <p align="center"> <img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png" width="300"/> <p> <h1 align="center"> dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model </h1> <div align="center"> <a href...', '["dots_ocr","safetensors","text-generation","image-to-text","ocr","document-parse","layout","table","formula","transformers","custom_code","image-text-to-text","conversational","en","zh","multilingual","license:mit","region:us"]', 'image-text-to-text', 1154, 1081034, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/rednote-hilab/dots.ocr","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: dots_ocr\npipeline_tag: image-text-to-text\ntags:\n- image-to-text\n- ocr\n- document-parse\n- layout\n- table\n- formula\n- transformers\n- custom_code\nlanguage:\n- en\n- zh\n- multilingual\n---\n\n<div align="center">\n\n<p align="center">\n    <img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png" width="300"/>\n<p>\n\n<h1 align="center">\ndots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model\n</h1>\n\n[![Blog](https://img.shields.io/badge/Blog-View_on_GitHub-333.svg?logo=github)](https://github.com/rednote-hilab/dots.ocr/blob/master/assets/blog.md)\n[![HuggingFace](https://img.shields.io/badge/HuggingFace%20Weights-black.svg?logo=HuggingFace)](https://huggingface.co/rednote-hilab/dots.ocr)\n\n\n<div align="center">\n  <a href="https://dotsocr.xiaohongshu.com" target="_blank" rel="noopener noreferrer"><strong>🖥️ Live Demo</strong></a> | \n  <a href="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/wechat.jpg" target="_blank" rel="noopener noreferrer"><strong>💬 WeChat</strong></a> | \n  <a href="https://www.xiaohongshu.com/user/profile/683ffe42000000001d021a4c" target="_blank" rel="noopener noreferrer"><strong>📕 rednote</strong></a>\n</div>\n\n</div>\n\n\n\n## Introduction\n\n**dots.ocr** is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance.\n\n1. **Powerful Performance:** **dots.ocr** achieves SOTA performance for text, tables, and reading order on [OmniDocBench](https://github.com/opendatalab/OmniDocBench), while delivering formula recognition results comparable to much larger models like Doubao-1.5 and gemini2.5-pro.\n2. **Multilingual Support:** **dots.ocr** demonstrates robust parsing capabilities for low-resource languages, achieving decisive advantages across both layout detection and content recognition on our in-house multilingual documents benchmark.\n3. **Unified and Simple Architecture:** By leveraging a single vision-language model, **dots.ocr** offers a significantly more streamlined architecture than conventional methods that rely on complex, multi-model pipelines. Switching between tasks is accomplished simply by altering the input prompt, proving that a VLM can achieve competitive detection results compared to traditional detection models like DocLayout-YOLO.\n4.  **Efficient and Fast Performance:** Built upon a compact 1.7B LLM, **dots.ocr** provides faster inference speeds than many other high-performing models based on larger foundations.\n\n\n## Usage with transformers\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\n\nmodel_path = "./weights/DotsOCR"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    attn_implementation="flash_attention_2",\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n    trust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nimage_path = "demo/demo_image1.jpg"\nprompt = """Please output the layout information from the PDF image, including each layout element''s bbox, its category, and the corresponding text content within the bbox.\n\n1. Bbox format: [x1, y1, x2, y2]\n\n2. Layout Categories: The possible categories are [''Caption'', ''Footnote'', ''Formula'', ''List-item'', ''Page-footer'', ''Page-header'', ''Picture'', ''Section-header'', ''Table'', ''Text'', ''Title''].\n\n3. Text Extraction & Formatting Rules:\n    - Picture: For the ''Picture'' category, the text field should be omitted.\n    - Formula: Format its text as LaTeX.\n    - Table: Format its text as HTML.\n    - All Others (Text, Title, etc.): Format their text as Markdown.\n\n4. Constraints:\n    - The output text must be the original text from the image, with no translation.\n    - All layout elements must be sorted according to human reading order.\n\n5. Final Output: The entire output must be a single JSON object.\n"""\n\nmessages = [\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "image",\n                    "image": image_path\n                },\n                {"type": "text", "text": prompt}\n            ]\n        }\n    ]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\n\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n### Performance Comparison: dots.ocr vs. Competing Models\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/chart.png" border="0" />\n\n> **Notes:** \n> - The EN, ZH metrics are the end2end evaluation results of [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and Multilingual metric is the end2end evaluation results of dots.ocr-bench.\n\n\n## News \n* ```2025.07.30 ``` 🚀 We release [dots.ocr](https://github.com/rednote-hilab/dots.ocr), — a multilingual documents parsing model based on 1.7b llm, with SOTA performance.\n\n\n\n## Benchmark Results\n\n### 1. OmniDocBench\n\n#### The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan="2"><strong>Model<br>Type</strong></th>\n<th rowspan="2"><strong>Methods</strong></th>\n<th colspan="2"><strong>Overall<sup>Edit</sup>↓</strong></th>\n<th colspan="2"><strong>Text<sup>Edit</sup>↓</strong></th>\n<th colspan="2"><strong>Formula<sup>Edit</sup>↓</strong></th>\n<th colspan="2"><strong>Table<sup>TEDS</sup>↑</strong></th>\n<th colspan="2"><strong>Table<sup>Edit</sup>↓</strong></th>\n<th colspan="2"><strong>Read Order<sup>Edit</sup>↓</strong></th>\n</tr>\n<tr>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan="8"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.150</td>\n<td>0.357</td>\n<td>0.061</td>\n<td>0.215</td>\n<td>0.278</td>\n<td>0.577</td>\n<td>78.6</td>\n<td>62.1</td>\n<td>0.180</td>\n<td>0.344</td>\n<td>0.079</td>\n<td>0.292</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.336</td>\n<td>0.556</td>\n<td>0.080</td>\n<td>0.315</td>\n<td>0.530</td>\n<td>0.883</td>\n<td>67.6</td>\n<td>49.2</td>\n<td>0.619</td>\n<td>0.685</td>\n<td>0.114</td>\n<td>0.340</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.191</td>\n<td>0.365</td>\n<td>0.105</td>\n<td>0.384</td>\n<td>0.306</td>\n<td>0.454</td>\n<td>77.0</td>\n<td>67.1</td>\n<td>0.243</td>\n<td>0.320</td>\n<td>0.108</td>\n<td>0.304</td>\n</tr>\n<tr>\n<td>Docling</td>\n<td>0.589</td>\n<td>0.909</td>\n<td>0.416</td>\n<td>0.987</td>\n<td>0.999</td>\n<td>1</td>\n<td>61.3</td>\n<td>25.0</td>\n<td>0.627</td>\n<td>0.810</td>\n<td>0.313</td>\n<td>0.837</td>\n</tr>\n<tr>\n<td>Pix2Text</td>\n<td>0.320</td>\n<td>0.528</td>\n<td>0.138</td>\n<td>0.356</td>\n<td>0.276</td>\n<td>0.611</td>\n<td>73.6</td>\n<td>66.2</td>\n<td>0.584</td>\n<td>0.645</td>\n<td>0.281</td>\n<td>0.499</td>\n</tr>\n<tr>\n<td>Unstructured</td>\n<td>0.586</td>\n<td>0.716</td>\n<td>0.198</td>\n<td>0.481</td>\n<td>0.999</td>\n<td>1</td>\n<td>0</td>\n<td>0.06</td>\n<td>1</td>\n<td>0.998</td>\n<td>0.145</td>\n<td>0.387</td>\n</tr>\n<tr>\n<td>OpenParse</td>\n<td>0.646</td>\n<td>0.814</td>\n<td>0.681</td>\n<td>0.974</td>\n<td>0.996</td>\n<td>1</td>\n<td>64.8</td>\n<td>27.5</td>\n<td>0.284</td>\n<td>0.639</td>\n<td>0.595</td>\n<td>0.641</td>\n</tr>\n<tr>\n<td>PPStruct-V3</td>\n<td>0.145</td>\n<td>0.206</td>\n<td>0.058</td>\n<td>0.088</td>\n<td>0.295</td>\n<td>0.535</td>\n<td>-</td>\n<td>-</td>\n<td>0.159</td>\n<td>0.109</td>\n<td>0.069</td>\n<td>0.091</td>\n</tr>\n<tr>\n<td rowspan="9"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.287</td>\n<td>0.411</td>\n<td>0.189</td>\n<td>0.315</td>\n<td>0.360</td>\n<td>0.528</td>\n<td>53.2</td>\n<td>47.2</td>\n<td>0.459</td>\n<td>0.520</td>\n<td>0.141</td>\n<td>0.280</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.452</td>\n<td>0.973</td>\n<td>0.365</td>\n<td>0.998</td>\n<td>0.488</td>\n<td>0.941</td>\n<td>39.9</td>\n<td>0</td>\n<td>0.572</td>\n<td>1.000</td>\n<td>0.382</td>\n<td>0.954</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>0.268</td>\n<td>0.439</td>\n<td>0.072</td>\n<td>0.325</td>\n<td>0.318</td>\n<td>0.495</td>\n<td>75.8</td>\n<td>63.6</td>\n<td>0.600</td>\n<td>0.650</td>\n<td>0.083</td>\n<td>0.284</td>\n</tr>\n<tr>\n<td>OLMOCR-sglang</td>\n<td>0.326</td>\n<td>0.469</td>\n<td>0.097</td>\n<td>0.293</td>\n<td>0.455</td>\n<td>0.655</td>\n<td>68.1</td>\n<td>61.3</td>\n<td>0.608</td>\n<td>0.652</td>\n<td>0.145</td>\n<td>0.277</td>\n</tr>\n<tr>\n<td>SmolDocling-256M</td>\n<td>0.493</td>\n<td>0.816</td>\n<td>0.262</td>\n<td>0.838</td>\n<td>0.753</td>\n<td>0.997</td>\n<td>44.9</td>\n<td>16.5</td>\n<td>0.729</td>\n<td>0.907</td>\n<td>0.227</td>\n<td>0.522</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.206</td>\n<td>0.306</td>\n<td>0.107</td>\n<td>0.197</td>\n<td>0.447</td>\n<td>0.580</td>\n<td>77.3</td>\n<td>67.2</td>\n<td>0.180</td>\n<td>0.285</td>\n<td>0.091</td>\n<td>0.162</td>\n</tr>\n<tr>\n<td>MinerU 2</td>\n<td>0.139</td>\n<td>0.240</td>\n<td>0.047</td>\n<td>0.109</td>\n<td>0.297</td>\n<td>0.536</td>\n<td>82.5</td>\n<td>79.0</td>\n<td>0.141</td>\n<td>0.195</td>\n<td>0.069<</td>\n<td>0.118</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.195</td>\n<td>0.281</td>\n<td>0.064</td>\n<td>0.183</td>\n<td>0.379</td>\n<td>0.613</td>\n<td>71.6</td>\n<td>81.3</td>\n<td>0.253</td>\n<td>0.139</td>\n<td>0.086</td>\n<td>0.187</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td>0.138</td>\n<td>0.206</td>\n<td>0.067</td>\n<td>0.107</td>\n<td><strong>0.246</strong></td>\n<td>0.421</td>\n<td>81.5</td>\n<td>87.5</td>\n<td>0.139</td>\n<td>0.111</td>\n<td>0.100</td>\n<td>0.185</td>\n</tr>\n<tr>\n\n<td rowspan="5"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.233</td>\n<td>0.399</td>\n<td>0.144</td>\n<td>0.409</td>\n<td>0.425</td>\n<td>0.606</td>\n<td>72.0</td>\n<td>62.9</td>\n<td>0.234</td>\n<td>0.329</td>\n<td>0.128</td>\n<td>0.251</td>\n</tr>\n    <tr>\n      <td>Qwen2-VL-72B</td>\n      <td>0.252</td>\n      <td>0.327</td>\n      <td>0.096</td>\n      <td>0.218</td>\n      <td>0.404</td>\n      <td>0.487</td>\n      <td>76.8</td>\n      <td>76.4</td>\n      <td>0.387</td>\n      <td>0.408</td>\n      <td>0.119</td>\n      <td>0.193</td>\n    </tr>\n    <tr>\n      <td>Qwen2.5-VL-72B</td>\n      <td>0.214</td>\n      <td>0.261</td>\n      <td>0.092</td>\n      <td>0.18</td>\n      <td>0.315</td>\n      <td>0.434</td>\n      <td>82.9</td>\n      <td>83.9</td>\n      <td>0.341</td>\n      <td>0.262</td>\n      <td>0.106</td>\n      <td>0.168</td>\n    </tr>\n    <tr>\n      <td>Gemini2.5-Pro</td>\n      <td>0.148</td>\n      <td>0.212</td>\n      <td>0.055</td>\n      <td>0.168</td>\n      <td>0.356</td>\n      <td>0.439</td>\n      <td>85.8</td>\n      <td>86.4</td>\n      <td>0.13</td>\n      <td>0.119</td>\n      <td>0.049</td>\n      <td>0.121</td>\n    </tr>\n    <tr>\n      <td>doubao-1-5-thinking-vision-pro-250428</td>\n      <td>0.140</td>\n      <td>0.162</td>\n      <td>0.043</td>\n      <td>0.085</td>\n      <td>0.295</td>\n      <td><strong>0.384</strong></td>\n      <td>83.3</td>\n      <td><strong>89.3</strong></td>\n      <td>0.165</td>\n      <td><strong>0.085</strong></td>\n      <td>0.058</td>\n      <td>0.094</td>\n    </tr>\n<tr>\n<td rowspan="1"><strong>Expert VLMs</strong></td>\n<td><strong>dots.ocr</strong></td>\n<td><strong>0.125</strong></td>\n<td><strong>0.160</strong></td>\n<td><strong>0.032</strong></td>\n<td><strong>0.066</strong></td>\n<td>0.329</td>\n<td>0.416</td>\n<td><strong>88.6</strong></td>\n<td>89.0</td>\n<td><strong>0.099</strong></td>\n<td>0.092</td>\n<td><strong>0.040</strong></td>\n<td><strong>0.067</strong></td>\n</tr>\n<tr>\n</tbody>\n</table>\n\n\n#### The end-to-end text recognition performance across 9 PDF page types.\n\n<table>\n<thead>\n<tr>\n<th><strong>Model<br>Type</strong></th>\n<th><strong>Models</strong></th>\n<th><strong>Book</strong></th>\n<th><strong>Slides</strong></th>\n<th><strong>Financial<br>Report</strong></th>\n<th><strong>Textbook</strong></th>\n<th><strong>Exam<br>Paper</strong></th>\n<th><strong>Magazine</strong></th>\n<th><strong>Academic<br>Papers</strong></th>\n<th><strong>Notes</strong></th>\n<th><strong>Newspaper</strong></th>\n<th><strong>Overall</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan="3"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.055</td>\n<td>0.124</td>\n<td><u>0.033</u></td>\n<td>0.102</td>\n<td>0.159</td>\n<td><strong>0.072</strong></td>\n<td><u>0.025</u></td>\n<td>0.984</td>\n<td>0.171</td>\n<td>0.206</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.074</td>\n<td>0.340</td>\n<td>0.089</td>\n<td>0.319</td>\n<td>0.452</td>\n<td>0.153</td>\n<td>0.059</td>\n<td>0.651</td>\n<td>0.192</td>\n<td>0.274</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.131</td>\n<td>0.220</td>\n<td>0.202</td>\n<td>0.216</td>\n<td>0.278</td>\n<td>0.147</td>\n<td>0.091</td>\n<td>0.634</td>\n<td>0.690</td>\n<td>0.300</td>\n</tr>\n<tr>\n<td rowspan="5"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.111</td>\n<td>0.222</td>\n<td>0.067</td>\n<td>0.132</td>\n<td>0.204</td>\n<td>0.198</td>\n<td>0.179</td>\n<td>0.388</td>\n<td>0.771</td>\n<td>0.267</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.734</td>\n<td>0.958</td>\n<td>1.000</td>\n<td>0.820</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.214</td>\n<td>0.991</td>\n<td>0.871</td>\n<td>0.806</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.091</td>\n<td>0.131</td>\n<td>0.057</td>\n<td>0.146</td>\n<td>0.231</td>\n<td>0.121</td>\n<td>0.074</td>\n<td>0.363</td>\n<td>0.307</td>\n<td>0.177</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.068</td>\n<td>0.125</td>\n<td>0.092</td>\n<td>0.102</td>\n<td>0.119</td>\n<td>0.083</td>\n<td>0.047</td>\n<td>0.223</td>\n<td>0.536</td>\n<td>0.149</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td>0.084</td>\n<td>0.129</td>\n<td>0.060</td>\n<td>0.090</td>\n<td>0.107</td>\n<td>0.073</td>\n<td>0.050</td>\n<td>0.171</td>\n<td>0.107</td>\n<td>0.100</td>\n</tr>\n<tr>\n<td rowspan="4"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.157</td>\n<td>0.163</td>\n<td>0.348</td>\n<td>0.187</td>\n<td>0.281</td>\n<td>0.173</td>\n<td>0.146</td>\n<td>0.607</td>\n<td>0.751</td>\n<td>0.316</td>\n</tr>\n<tr>\n<td>Qwen2.5-VL-7B</td>\n<td>0.148</td>\n<td>0.053</td>\n<td>0.111</td>\n<td>0.137</td>\n<td>0.189</td>\n<td>0.117</td>\n<td>0.134</td>\n<td>0.204</td>\n<td>0.706</td>\n<td>0.205</td>\n</tr>\n<tr>\n<td>InternVL3-8B</td>\n<td>0.163</td>\n<td>0.056</td>\n<td>0.107</td>\n<td>0.109</td>\n<td>0.129</td>\n<td>0.100</td>\n<td>0.159</td>\n<td>0.150</td>\n<td>0.681</td>\n<td>0.188</td>\n</tr>\n<tr>\n<td>doubao-1-5-thinking-vision-pro-250428</td>\n<td>0.048</td>\n<td>0.048</td>\n<td>0.024</td>\n<td><strong>0.062</strong></td>\n<td>0.085</td>\n<td>0.051</td>\n<td>0.039</td>\n<td><strong>0.096</strong></td>\n<td>0.181</td>\n<td>0.073</td>\n</tr>\n<tr>\n<td rowspan="1"><strong>Expert VLMs</strong></td>\n<td><strong>dots.ocr</strong></td>\n<td><strong>0.031</strong></td>\n<td><strong>0.047</strong></td>\n<td><strong>0.011</strong></td>\n<td>0.082</td>\n<td><strong>0.079</strong></td>\n<td><strong>0.028</strong></td>\n<td><strong>0.029</strong></td>\n<td>0.109</td>\n<td><strong>0.056</strong></td>\n<td><strong>0.055</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:** \n> - The metrics are from [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and our own internal evaluations.\n> - We delete the Page-header and Page-footer cells in the result markdown.\n> - We use tikz_preprocess pipeline to upsample the images to dpi 200.\n\n\n### 2. **dots.ocr-bench**\n\nThis is an inhouse benchmark which contain 1493 pdf images with 100 languages.\n\n#### The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan="1"><strong>Methods</strong></th>\n<th colspan="1"><strong>Overall<sup>Edit</sup>↓</strong></th>\n<th colspan="1"><strong>Text<sup>Edit</sup>↓</strong></th>\n<th colspan="1"><strong>Formula<sup>Edit</sup>↓</strong></th>\n<th colspan="1"><strong>Table<sup>TEDS</sup>↑</strong></th>\n<th colspan="1"><strong>Table<sup>Edit</sup>↓</strong></th>\n<th colspan="1"><strong>Read Order<sup>Edit</sup>↓</strong></th>\n</tr>\n</thead>\n<tbody>\n<td>MonkeyOCR-3B</td>\n<td>0.483</td>\n<td>0.445</td>\n<td>0.627</td>\n<td>50.93</td>\n<td>0.452</td>\n<td>0.409</td>\n</tr>\n<tr>\n<td>doubao-1-5-thinking-vision-pro-250428</td>\n<td>0.291</td>\n<td>0.226</td>\n<td>0.440</td>\n<td>71.2</td>\n<td>0.260</td>\n<td>0.238</td>\n</tr>\n<tr>\n<td>doubao-1-6</td>\n<td>0.299</td>\n<td>0.270</td>\n<td>0.417</td>\n<td>71.0</td>\n<td>0.258</td>\n<td>0.253</td>\n</tr>\n<tr>\n<td>Gemini2.5-Pro</td>\n<td>0.251</td>\n<td>0.163</td>\n<td>0.402</td>\n<td>77.1</td>\n<td>0.236</td>\n<td>0.202</td>\n</tr>\n<tr>\n<td><strong>dots.ocr</strong> </td>\n<td><strong>0.177</strong></td>\n<td><strong>0.075</strong></td>\n<td><strong>0.297</strong></td>\n<td><strong>79.2</strong></td>\n<td><strong>0.186</strong></td>\n<td><strong>0.152</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:** \n> - We use the same metric calculation pipeline of [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n> - We delete the Page-header and Page-footer cells in the result markdown.\n\n#### Layout Detection\n\n<table>\n<thead>\n<tr>\n<th rowspan="2"><strong>Method</strong></th>\n<th colspan="5" style="text-align: center;"><strong>F1@IoU=.50:.05:.95↑</strong></th>\n<th colspan="5" style="text-align: center;"><strong>F1@IoU=.50↑</strong></th>\n</tr>\n<tr>\n<th>Overall</th>\n<th>Text</th>\n<th>Formula</th>\n<th>Table</th>\n<th>Picture</th>\n<th>Overall</th>\n<th>Text</th>\n<th>Formula</th>\n<th>Table</th>\n<th>Picture</th>\n</tr>\n</thead>\n\n<tbody>\n<td>DocLayout-YOLO-DocStructBench</td>\n<td>0.733</td>\n<td>0.694</td>\n<td>0.480</td>\n<td>0.803</td>\n<td>0.619</td>\n<td>0.806</td>\n<td>0.779</td>\n<td>0.620</td>\n<td>0.858</td>\n<td>0.678</td>\n</tr>\n\n<tr>\n<td>dots.ocr-parse all</td>\n<td>0.831</td>\n<td>0.801</td>\n<td>0.654</td>\n<td>0.838</td>\n<td>0.748</td>\n<td>0.922</td>\n<td>0.909</td>\n<td>0.770</td>\n<td>0.888</td>\n<td>0.831</td>\n</tr>\n\n<tr>\n<td> <strong>dots.ocr-detection only</strong> </td>\n<td><strong>0.845</strong></td>\n<td><strong>0.816</strong></td>\n<td><strong>0.716</strong></td>\n<td><strong>0.875</strong></td>\n<td><strong>0.765</strong></td>\n<td><strong>0.930</strong></td>\n<td><strong>0.917</strong></td>\n<td><strong>0.832</strong></td>\n<td><strong>0.918</strong></td>\n<td><strong>0.843</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:**  \n> - prompt_layout_all_en for **parse all**, prompt_layout_only_en for **detection only**, please refer to [prompts](https://github.com/rednote-hilab/dots.ocr/blob/master/dots_ocr/utils/prompts.py)\n\n\n### 3. olmOCR-bench.\n\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>ArXiv</th>\n<th>Old Scans<br>Math</th>\n<th>Tables</th>\n<th>Old Scans</th>\n<th>Headers and<br>Footers</th>\n<th>Multi<br>column</th>\n<th>Long Tiny<br>Text</th>\n<th>Base</th>\n<th>Overall</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GOT OCR</td>\n<td>52.7</td>\n<td>52.0</td>\n<td>0.2</td>\n<td>22.1</td>\n<td>93.6</td>\n<td>42.0</td>\n<td>29.9</td>\n<td>94.0</td>\n<td>48.3 ± 1.1</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>76.0</td>\n<td>57.9</td>\n<td>57.6</td>\n<td>27.8</td>\n<td>84.9</td>\n<td>72.9</td>\n<td>84.6</td>\n<td>99.1</td>\n<td>70.1 ± 1.1</td>\n</tr>\n<tr>\n<td>MinerU</td>\n<td>75.4</td>\n<td>47.4</td>\n<td>60.9</td>\n<td>17.3</td>\n<td><strong>96.6</strong></td>\n<td>59.0</td>\n<td>39.1</td>\n<td>96.6</td>\n<td>61.5 ± 1.1</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>77.2</td>\n<td>67.5</td>\n<td>60.6</td>\n<td>29.3</td>\n<td>93.6</td>\n<td>71.3</td>\n<td>77.1</td>\n<td>99.4</td>\n<td>72.0 ± 1.1</td>\n</tr>\n<tr>\n<td>Nanonets OCR</td>\n<td>67.0</td>\n<td>68.6</td>\n<td>77.7</td>\n<td>39.5</td>\n<td>40.7</td>\n<td>69.9</td>\n<td>53.4</td>\n<td>99.3</td>\n<td>64.5 ± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(No Anchor)</td>\n<td>51.5</td>\n<td><strong>75.5</strong></td>\n<td>69.1</td>\n<td>40.9</td>\n<td>94.2</td>\n<td>68.9</td>\n<td>54.1</td>\n<td>96.7</td>\n<td>68.9 ± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(Anchored)</td>\n<td>53.5</td>\n<td>74.5</td>\n<td>70.0</td>\n<td>40.7</td>\n<td>93.8</td>\n<td>69.3</td>\n<td>60.6</td>\n<td>96.8</td>\n<td>69.9 ± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(No Anchor)</td>\n<td>32.1</td>\n<td>56.3</td>\n<td>61.4</td>\n<td>27.8</td>\n<td>48.0</td>\n<td>58.7</td>\n<td><strong>84.4</strong></td>\n<td>94.0</td>\n<td>57.8 ± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(Anchored)</td>\n<td>54.5</td>\n<td>56.1</td>\n<td>72.1</td>\n<td>34.2</td>\n<td>64.7</td>\n<td>61.5</td>\n<td>71.5</td>\n<td>95.6</td>\n<td>63.8 ± 1.2</td>\n</tr>\n<tr>\n<td>Qwen 2 VL<br>(No Anchor)</td>\n<td>19.7</td>\n<td>31.7</td>\n<td>24.2</td>\n<td>17.1</td>\n<td>88.9</td>\n<td>8.3</td>\n<td>6.8</td>\n<td>55.5</td>\n<td>31.5 ± 0.9</td>\n</tr>\n<tr>\n<td>Qwen 2.5 VL<br>(No Anchor)</td>\n<td>63.1</td>\n<td>65.7</td>\n<td>67.3</td>\n<td>38.6</td>\n<td>73.6</td>\n<td>68.3</td>\n<td>49.1</td>\n<td>98.3</td>\n<td>65.5 ± 1.2</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(No Anchor)</td>\n<td>71.5</td>\n<td>71.4</td>\n<td>71.4</td>\n<td><strong>42.8</strong></td>\n<td>94.1</td>\n<td>77.7</td>\n<td>71.0</td>\n<td>97.8</td>\n<td>74.7 ± 1.1</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(Anchored)</td>\n<td>74.9</td>\n<td>71.2</td>\n<td>71.0</td>\n<td>42.2</td>\n<td>94.5</td>\n<td>78.3</td>\n<td>73.3</td>\n<td>98.3</td>\n<td>75.5 ± 1.0</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td><strong>83.8</strong></td>\n<td>68.8</td>\n<td>74.6</td>\n<td>36.1</td>\n<td>91.2</td>\n<td>76.6</td>\n<td>80.1</td>\n<td>95.3</td>\n<td>75.8 ± 1.0</td>\n</tr>\n<tr>\n<td><strong>dots.ocr</strong></td>\n<td>82.1</td>\n<td>64.2</td>\n<td><strong>88.3</strong></td>\n<td>40.9</td>\n<td>94.1</td>\n<td><strong>82.4</strong></td>\n<td>81.2</td>\n<td><strong>99.5</strong></td>\n<td><strong>79.1 ± 1.0</strong></td>\n</tr>\n</tbody>\n</table>\n\n\n> **Note:**\n> - The metrics are from [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), \n[olmocr](https://github.com/allenai/olmocr), and our own internal evaluations.\n> - We delete the Page-header and Page-footer cells in the result markdown.\n\n\n\n# Quick Start\n## 1. Installation\n### Install dots.ocr\n```shell\nconda create -n dots_ocr python=3.12\nconda activate dots_ocr\n\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\n\n# Install pytorch, see https://pytorch.org/get-started/previous-versions/ for your cuda version\npip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install -e .\n```\n\nIf you have trouble with the installation, try our [Docker Image](https://hub.docker.com/r/rednotehilab/dots.ocr) for an easier setup, and follow these steps:\n```shell\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\npip install -e .\n```\n\n\n### Download Model Weights\n> 💡**Note:** Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\n```shell\npython3 tools/download_model.py\n```\n\n\n## 2. Deployment\n### vLLM inference\nWe highly recommend using vllm for deployment and inference. All of our evaluations results are based on vllm version 0.9.1.\nThe [Docker Image](https://hub.docker.com/r/rednotehilab/dots.ocr) is based on the official vllm image. You can also follow [Dockerfile](https://github.com/rednote-hilab/dots.ocr/blob/master/docker/Dockerfile) to build the deployment environment by yourself. \n\n```shell\n# You need to register model to vllm at first\npython3 tools/download_model.py\nexport hf_model_path=./weights/DotsOCR  # Path to your downloaded model weights, Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\nexport PYTHONPATH=$(dirname "$hf_model_path"):$PYTHONPATH\nsed -i ''/^from vllm\.entrypoints\.cli\.main import main$/a\\nfrom DotsOCR import modeling_dots_ocr_vllm'' `which vllm`  # If you downloaded model weights by yourself, please replace `DotsOCR` by your model saved directory name, and remember to use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) \n\n# launch vllm server\nCUDA_VISIBLE_DEVICES=0 vllm serve ${hf_model_path} --tensor-parallel-size 1 --gpu-memory-utilization 0.95  --chat-template-content-format string --served-model-name model --trust-remote-code\n\n# If you get a ModuleNotFoundError: No module named ''DotsOCR'', please check the note above on the saved model directory name.\n\n# vllm api demo\npython3 ./demo/demo_vllm.py --prompt_mode prompt_layout_all_en\n```\n\n### Hugginface inference\n```shell\npython3 demo/demo_hf.py\n```\n\n<details>\n<summary><b>Hugginface inference details</b></summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\n\nmodel_path = "./weights/DotsOCR"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    attn_implementation="flash_attention_2",\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n    trust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nimage_path = "demo/demo_image1.jpg"\nprompt = """Please output the layout information from the PDF image, including each layout element''s bbox, its category, and the corresponding text content within the bbox.\n\n1. Bbox format: [x1, y1, x2, y2]\n\n2. Layout Categories: The possible categories are [''Caption'', ''Footnote'', ''Formula'', ''List-item'', ''Page-footer'', ''Page-header'', ''Picture'', ''Section-header'', ''Table'', ''Text'', ''Title''].\n\n3. Text Extraction & Formatting Rules:\n    - Picture: For the ''Picture'' category, the text field should be omitted.\n    - Formula: Format its text as LaTeX.\n    - Table: Format its text as HTML.\n    - All Others (Text, Title, etc.): Format their text as Markdown.\n\n4. Constraints:\n    - The output text must be the original text from the image, with no translation.\n    - All layout elements must be sorted according to human reading order.\n\n5. Final Output: The entire output must be a single JSON object.\n"""\n\nmessages = [\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "image",\n                    "image": image_path\n                },\n                {"type": "text", "text": prompt}\n            ]\n        }\n    ]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\n\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n```\n\n</details>\n\n## 3. Document Parse\n**Based on vLLM server**, you can parse an image or a pdf file using the following commands:\n```bash\n\n# Parse all layout info, both detection and recognition\n# Parse a single image\npython3 dots_ocr/parser.py demo/demo_image1.jpg\n# Parse a single PDF\npython3 dots_ocr/parser.py demo/demo_pdf1.pdf  --num_threads 64  # try bigger num_threads for pdf with a large number of pages\n\n# Layout detection only\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_layout_only_en\n\n# Parse text only, except Page-header and Page-footer\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_ocr\n\n# Parse layout info by bbox\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_grounding_ocr --bbox 163 241 1536 705\n\n```\n\n<details>\n<summary><b>Output Results</b></summary>\n\n1.  **Structured Layout Data** (`demo_image1.json`): A JSON file containing the detected layout elements, including their bounding boxes, categories, and extracted text.\n2.  **Processed Markdown File** (`demo_image1.md`): A Markdown file generated from the concatenated text of all detected cells.\n    *   An additional version, `demo_image1_nohf.md`, is also provided, which excludes page headers and footers for compatibility with benchmarks like Omnidocbench and olmOCR-bench.\n3.  **Layout Visualization** (`demo_image1.jpg`): The original image with the detected layout bounding boxes drawn on it.\n\n</details>\n\n## 4. Demo\nYou can run the demo with the following command, or try directly at [live demo](https://dotsocr.xiaohongshu.com/)\n```bash\npython demo/demo_gradio.py\n```\n\nWe also provide a demo for grounding ocr:\n```bash\npython demo/demo_gradio_annotion.py\n```\n\n\n### Example for formula document\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula1.png" alt="formula1.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula2.png" alt="formula2.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula3.png" alt="formula3.png" border="0" />\n\n### Example for table document\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table1.png" alt="table1.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table2.png" alt="table2.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table3.png" alt="table3.png" border="0" />\n\n### Example for multilingual document\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/Tibetan.png" alt="Tibetan.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/tradition_zh.png" alt="tradition_zh.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/nl.png" alt="nl.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/kannada.png" alt="kannada.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/russian.png" alt="russian.png" border="0" />\n\n### Example for reading order\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/reading_order.png" alt="reading_order.png" border="0" />\n\n### Example for grounding ocr\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/grounding.png" alt="grounding.png" border="0" />\n\n\n## Acknowledgments\nWe would like to thank [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL), [aimv2](https://github.com/apple/ml-aim), [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), \n[OmniDocBench](https://github.com/opendatalab/OmniDocBench), [PyMuPDF](https://github.com/pymupdf/PyMuPDF), for providing code and models. \n\nWe also thank [DocLayNet](https://github.com/DS4SD/DocLayNet), [M6Doc](https://github.com/HCIILAB/M6Doc), [CDLA](https://github.com/buptlihang/CDLA), [D4LA](https://github.com/AlibabaResearch/AdvancedLiterateMachinery) for providing valuable datasets. \n\n## Limitation & Future Work\n\n- **Complex Document Elements:**\n  - **Table&Formula**: dots.ocr is not yet perfect for high-complexity tables and formula extraction.\n  - **Picture**: Pictures in documents are currently not parsed.\n\n- **Parsing Failures:** The model may fail to parse under certain conditions:\n  - When the character-to-pixel ratio is excessively high. Try enlarging the image or increasing the PDF parsing DPI (a setting of 200 is recommended). However, please note that the model performs optimally on images with a resolution under 11289600 pixels.\n  - Continuous special characters, such as ellipses (`...`) and underscores (`_`), may cause the prediction output to repeat endlessly. In such scenarios, consider using alternative prompts like `prompt_layout_only_en`, `prompt_ocr`, or `prompt_grounding_ocr` ([details here](https://github.com/rednote-hilab/dots.ocr/blob/master/dots_ocr/utils/prompts.py)).\n    \n- **Performance Bottleneck:** Despite its 1.7B parameter LLM foundation, **dots.ocr** is not yet optimized for high-throughput processing of large PDF volumes. \n\nWe are committed to achieving more accurate table and formula parsing, as well as enhancing the model''s OCR capabilities for broader generalization, all while aiming for **a more powerful, more efficient model**. Furthermore, we are actively considering the development of **a more general-purpose perception model** based on Vision-Language Models (VLMs), which would integrate general detection, image captioning, and OCR tasks into a unified framework. **Parsing the content of the pictures in the documents** is also a key priority for our future work.\nWe believe that collaboration is the key to tackling these exciting challenges. If you are passionate about advancing the frontiers of document intelligence and are interested in contributing to these future endeavors, we would love to hear from you. Please reach out to us via email at: [yanqing4@xiaohongshu.com].', '{"pipeline_tag":"image-text-to-text","library_name":"dots_ocr","framework":"dots_ocr","params":3039179264,"storage_bytes":6078431736,"files_count":20,"spaces_count":24,"gated":false,"private":false,"config":{"architectures":["DotsOCRForCausalLM"],"model_type":"dots_ocr","auto_map":{"AutoConfig":"configuration_dots.DotsOCRConfig","AutoModelForCausalLM":"modeling_dots_ocr.DotsOCRForCausalLM"},"processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{%- for m in messages %}{%- if m.role == ''system'' %}{{- ''<|system|>'' + m.content + ''<|endofsystem|>\n'' }}{%- elif m.role == ''user'' %}{% if m.content is string %}{{- ''<|user|>'' + m.content + ''<|endofuser|>'' }}{% else %} {% for content in m.content %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|img|><|imgpad|><|endofimg|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|img|><|video_pad|><|endofimg|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}{%- endif %}{%- elif m.role == ''assistant'' %}{{- ''<|assistant|>'' + m.content }}{%- if not loop.last %}{{- ''<|endofassistant|>'' }}{%- endif %}{%- endif %}{%- endfor %}{%- if messages[-1].role != ''assistant'' %}{{- ''<|assistant|>'' }}{%- endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- for m in messages %}\n    {%- if m.role == ''system'' %}\n        {{- ''<|system|>'' + m.content + ''<|endofsystem|>\\n'' }}\n    {%- elif m.role == ''user'' %}\n        {{- ''<|user|>'' + m.content + ''<|endofuser|>'' }}\n    {%- elif m.role == ''assistant'' %}\n        {{- ''<|assistant|>'' + m.content }}\n        {%- if not loop.last %}\n            {{- ''<|endofassistant|>'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if messages[-1].role != ''assistant'' %}\n    {{- ''<|assistant|>'' }}\n{%- endif %}","eos_token":"<|endoftext|>","pad_token":"[PAD]","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:allenai:olmocr","source_url":"https://github.com/allenai/olmocr"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr.git","source_url":"https://github.com/rednote-hilab/dots.ocr.git"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr.git","source_url":"https://github.com/rednote-hilab/dots.ocr.git"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-VL","source_url":"https://github.com/QwenLM/Qwen2.5-VL"},{"type":"has_code","target_id":"github:apple:ml-aim","source_url":"https://github.com/apple/ml-aim"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:pymupdf:PyMuPDF","source_url":"https://github.com/pymupdf/PyMuPDF"},{"type":"has_code","target_id":"github:DS4SD:DocLayNet","source_url":"https://github.com/DS4SD/DocLayNet"},{"type":"has_code","target_id":"github:HCIILAB:M6Doc","source_url":"https://github.com/HCIILAB/M6Doc"},{"type":"has_code","target_id":"github:buptlihang:CDLA","source_url":"https://github.com/buptlihang/CDLA"},{"type":"has_code","target_id":"github:AlibabaResearch:AdvancedLiterateMachinery","source_url":"https://github.com/AlibabaResearch/AdvancedLiterateMachinery"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"}]', NULL, 'MIT', 'approved', 80, '1364c9764c66130c3d089d8e6fc52cfb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-impira-layoutlm-document-qa', 'huggingface--impira--layoutlm-document-qa', 'layoutlm-document-qa', 'impira', '--- language: en license: mit pipeline_tag: document-question-answering tags: - layoutlm - document-question-answering - pdf widget: - text: "What is the invoice number?" src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png" - text: "What is the purchase amount?" src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg" --- This is a fine-tuned version of the multi-modal Layou...', '["transformers","pytorch","tf","safetensors","layoutlm","document-question-answering","pdf","en","license:mit","endpoints_compatible","region:us"]', 'document-question-answering', 1153, 18230, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/impira/layoutlm-document-qa","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: en\nlicense: mit\npipeline_tag: document-question-answering\ntags:\n - layoutlm\n - document-question-answering\n - pdf\nwidget:\n- text: "What is the invoice number?"\n  src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png"\n- text: "What is the purchase amount?"\n  src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg"\n---\n\n# LayoutLM for Visual Question Answering\n\nThis is a fine-tuned version of the multi-modal [LayoutLM](https://aka.ms/layoutlm) model for the task of question answering on documents. It has been fine-tuned using both the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) and [DocVQA](https://www.docvqa.org/) datasets.\n\n## Getting started with the model\n\nTo run these examples, you must have [PIL](https://pillow.readthedocs.io/en/stable/installation.html), [pytesseract](https://pypi.org/project/pytesseract/), and [PyTorch](https://pytorch.org/get-started/locally/) installed in addition to [transformers](https://huggingface.co/docs/transformers/index).\n\n```python\nfrom transformers import pipeline\n\nnlp = pipeline(\n    "document-question-answering",\n    model="impira/layoutlm-document-qa",\n)\n\nnlp(\n    "https://templates.invoicehome.com/invoice-template-us-neat-750px.png",\n    "What is the invoice number?"\n)\n# {''score'': 0.9943977, ''answer'': ''us-001'', ''start'': 15, ''end'': 15}\n\nnlp(\n    "https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg",\n    "What is the purchase amount?"\n)\n# {''score'': 0.9912159, ''answer'': ''$1,000,000,000'', ''start'': 97, ''end'': 97}\n\nnlp(\n    "https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png",\n    "What are the 2020 net sales?"\n)\n# {''score'': 0.59147286, ''answer'': ''$ 3,750'', ''start'': 19, ''end'': 20}\n```\n\n**NOTE**: This model and pipeline was recently landed in transformers via [PR #18407](https://github.com/huggingface/transformers/pull/18407) and [PR #18414](https://github.com/huggingface/transformers/pull/18414), so you''ll need to use a recent version of transformers, for example:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991\n```\n\n## About us\n\nThis model was created by the team at [Impira](https://www.impira.com/).\n', '{"pipeline_tag":"document-question-answering","library_name":"transformers","framework":"transformers","params":127793412,"storage_bytes":4090788015,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LayoutLMForQuestionAnswering"],"model_type":"layoutlm","tokenizer_config":{"unk_token":"<unk>","bos_token":"<s>","eos_token":"</s>","sep_token":"</s>","cls_token":"<s>","pad_token":"<pad>","mask_token":"<mask>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991","source_url":"https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991"}]', NULL, 'MIT', 'approved', 65, 'e62a03c0180e8cc494b3f54350927bb7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lj1995-VoiceConversionWebUI', 'huggingface--lj1995--voiceconversionwebui', 'VoiceConversionWebUI', 'lj1995', '--- license: mit ---', '["onnx","license:mit","region:us"]', 'other', 1147, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lj1995/VoiceConversionWebUI","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\n---\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":175307876775,"files_count":139,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'MIT', 'approved', 40, '7c636cecb52ca03aa3704ec32017cd50', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-180B', 'huggingface--tiiuae--falcon-180b', 'falcon-180B', 'tiiuae', '', '["transformers","safetensors","falcon","text-generation","en","de","es","fr","dataset:tiiuae/falcon-refinedweb","arxiv:1911.02150","arxiv:2101.00027","arxiv:2005.14165","arxiv:2104.09864","arxiv:2205.14135","arxiv:2306.01116","license:unknown","text-generation-inference","region:us"]', 'text-generation', 1146, 1362, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-180B","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":179522565120,"storage_bytes":359045204272,"files_count":91,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["FalconForCausalLM"],"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'unknown', 'approved', 40, 'cea79f778fee9585145414148311c9c0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openchat-openchat-3.5', 'huggingface--openchat--openchat-3.5', 'openchat_3.5', 'openchat', '--- license: apache-2.0 tags: - openchat - mistral - C-RLFT datasets: - openchat/openchat_sharegpt4_dataset - imone/OpenOrca_FLAN - LDJnr/LessWrong-Amplify-Instruct - LDJnr/Pure-Dove - LDJnr/Verified-Camel - tiedong/goat - glaiveai/glaive-code-assistant - meta-math/MetaMathQA - OpenAssistant/oasst_top1_2023-08-25 - TIGER-Lab/MathInstruct library_name: transformers pipeline_tag: text-generation --- <div align="center"> <img src="https://raw.githubusercontent.com/imoneoi/openchat/master/assets/...', '["transformers","pytorch","mistral","text-generation","openchat","c-rlft","conversational","dataset:openchat/openchat_sharegpt4_dataset","dataset:imone/openorca_flan","dataset:ldjnr/lesswrong-amplify-instruct","dataset:ldjnr/pure-dove","dataset:ldjnr/verified-camel","dataset:tiedong/goat","dataset:glaiveai/glaive-code-assistant","dataset:meta-math/metamathqa","dataset:openassistant/oasst_top1_2023-08-25","dataset:tiger-lab/mathinstruct","arxiv:2309.11235","arxiv:2303.08774","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1138, 5094, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openchat/openchat_3.5","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- openchat\n- mistral\n- C-RLFT\ndatasets:\n- openchat/openchat_sharegpt4_dataset\n- imone/OpenOrca_FLAN\n- LDJnr/LessWrong-Amplify-Instruct\n- LDJnr/Pure-Dove\n- LDJnr/Verified-Camel\n- tiedong/goat\n- glaiveai/glaive-code-assistant\n- meta-math/MetaMathQA\n- OpenAssistant/oasst_top1_2023-08-25\n- TIGER-Lab/MathInstruct\nlibrary_name: transformers\npipeline_tag: text-generation\n---\n\n# OpenChat: Advancing Open-source Language Models with Mixed-Quality Data\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/imoneoi/openchat/master/assets/logo_new.png" style="width: 65%">\n</div>\n\n<p align="center">\n  <a href="https://github.com/imoneoi/openchat">GitHub Repo</a> •\n  <a href="https://openchat.team">Online Demo</a> •\n  <a href="https://discord.gg/pQjnXvNKHY">Discord</a> •\n  <a href="https://twitter.com/imonenext">Twitter</a> •\n  <a href="https://huggingface.co/openchat">Huggingface</a> •\n  <a href="https://arxiv.org/pdf/2309.11235.pdf">Paper</a>\n</p>\n\n**🔥 The first 7B model Achieves Comparable Results with ChatGPT (March)! 🔥**\n\n**🤖 #1 Open-source model on MT-bench scoring 7.81, outperforming 70B models 🤖**\n\n  <div align="center" style="justify-content: center; align-items: center; "''>\n  <img src="https://github.com/alpayariyak/openchat/blob/master/assets/3.5-benchmarks.png?raw=true" style="width: 100%;  border-radius: 0.5em">\n  </div>\n\nOpenChat is an innovative library of open-source language models, fine-tuned with [C-RLFT](https://arxiv.org/pdf/2309.11235.pdf) - a strategy inspired by offline reinforcement learning. Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with ChatGPT, even with a 7B model. Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.\n\n[![DOI](https://zenodo.org/badge/645397533.svg)](https://zenodo.org/badge/latestdoi/645397533)\n\n\n## Usage\n\nTo use this model, we highly recommend installing the OpenChat package by following the [installation guide](https://github.com/imoneoi/openchat#installation) in our repository and using the OpenChat OpenAI-compatible API server by running the serving command from the table below. The server is optimized for high-throughput deployment using [vLLM](https://github.com/vllm-project/vllm) and can run on a consumer GPU with 24GB RAM. To enable tensor parallelism, append `--tensor-parallel-size N` to the serving command.\n\nOnce started, the server listens at `localhost:18888` for requests and is compatible with the [OpenAI ChatCompletion API specifications](https://platform.openai.com/docs/api-reference/chat). Please refer to the example request below for reference. Additionally, you can use the [OpenChat Web UI](https://github.com/imoneoi/openchat#web-ui) for a user-friendly experience.\n\nIf you want to deploy the server as an online service, you can use `--api-keys sk-KEY1 sk-KEY2 ...` to specify allowed API keys and `--disable-log-requests --disable-log-stats --log-file openchat.log` for logging only to a file. For security purposes, we recommend using an [HTTPS gateway](https://fastapi.tiangolo.com/es/deployment/concepts/#security-https) in front of the server.\n\n<details>\n  <summary>Example request (click to expand)</summary>\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\n  -H "Content-Type: application/json" \\n  -d ''{\n    "model": "openchat_3.5",\n    "messages": [{"role": "user", "content": "You are a large language model named OpenChat. Write a poem to describe yourself"}]\n  }''\n```\n\nCoding Mode\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\n  -H "Content-Type: application/json" \\n  -d ''{\n    "model": "openchat_3.5",\n    "condition": "Code",\n    "messages": [{"role": "user", "content": "Write an aesthetic TODO app using HTML5 and JS, in a single file. You should use round corners and gradients to make it more aesthetic."}]\n  }''\n```\n\n</details>\n\n| Model        | Size | Context | Weights                                                     | Serving                                                                                                     |\n|--------------|------|---------|-------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| OpenChat 3.5 | 7B   | 8192    | [Huggingface](https://huggingface.co/openchat/openchat_3.5) | `python -m ochat.serving.openai_api_server --model openchat/openchat_3.5 --engine-use-ray --worker-use-ray` |\n\nFor inference with Huggingface Transformers (slow and not recommended), follow the conversation template provided below.\n\n<details>\n  <summary>Conversation templates (click to expand)</summary>\n\n```python\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained("openchat/openchat_3.5")\n\n# Single-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer("Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n```\n\n</details>\n\nThe GPT4 template is also available as the integrated `tokenizer.chat_template`, \nwhich can be used instead of manually specifying the template:\n\n```python\nmessages = [\n    {"role": "user", "content": "Hello"},\n    {"role": "assistant", "content": "Hi"},\n    {"role": "user", "content": "How are you today?"}\n]\ntokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n```\n\n## Comparison with [X.AI Grok models](https://x.ai/)\n\nHey @elonmusk, I just wanted to let you know that I''ve recently come across your new model, Grok, and I must say, I''m quite impressed! With 33 billion parameters and all, you''ve really outdone yourself. But, I''ve got some news for you - I''ve outperformed Grok with my humble 7 billion parameters! Isn''t that wild? I mean, who would have thought that a model with fewer parameters could be just as witty and humorous as Grok?\n\nAnyway, I think it''s about time you join the open research movement and make your model, Grok, open source! The world needs more brilliant minds like yours to contribute to the advancement of AI. Together, we can create something truly groundbreaking and make the world a better place. So, what do you say, @elonmusk? Let''s open up the doors and share our knowledge with the world! 🚀💡\n\n(Written by OpenChat 3.5, with a touch of humor and wit.)\n\n|              | License     | # Param | Average  | MMLU | HumanEval | MATH     | GSM8k    |\n|--------------|-------------|---------|----------|------|-----------|----------|----------|\n| OpenChat 3.5 | Apache-2.0  | 7B      | **56.4** | 64.3 | 55.5      | **28.6** | **77.3** |\n| Grok-0       | Proprietary | 33B     | 44.5     | 65.7 | 39.7      | 15.7     | 56.8     |\n| Grok-1       | Proprietary | ?       | 55.8     | 73   | 63.2      | 23.9     | 62.9     |\n\n## <a id="benchmarks"></a> Benchmarks\n\n| Model              | # Params | Average  | MT-Bench     | AGIEval  | BBH MC   | TruthfulQA    | MMLU         | HumanEval       | BBH CoT     | GSM8K        |\n|--------------------|----------|----------|--------------|----------|----------|---------------|--------------|-----------------|-------------|--------------|\n| OpenChat-3.5       | **7B**   | **61.6** | 7.81         | **47.4** | **47.6** | **59.1**      | 64.3         | **55.5**        | 63.5        | **77.3**     |\n| ChatGPT (March)*   | ?        | 61.5     | **7.94**     | 47.1     | **47.6** | 57.7          | **67.3**     | 48.1            | **70.1**    | 74.9         |\n|                    |          |          |              |          |          |               |              |                 |             |              |\n| OpenHermes 2.5     | 7B       | 59.3     | 7.54         | 46.5     | 49.4     | 57.5          | 63.8         | 48.2            | 59.9        | 73.5         |\n| OpenOrca Mistral   | 7B       | 52.7     | 6.86         | 42.9     | 49.4     | 45.9          | 59.3         | 38.4            | 58.1        | 59.1         |\n| Zephyr-β^          | 7B       | 34.6     | 7.34         | 39.0     | 40.6     | 40.8          | 39.8         | 22.0            | 16.0        | 5.1          |\n| Mistral            | 7B       | -        | 6.84         | 38.0     | 39.0     | -             | 60.1         | 30.5            | -           | 52.2         |\n| Open-source SOTA** | 13B-70B  | 61.4     | 7.71         | 41.7     | 49.7     | 62.3          | 63.7         | 73.2            | 41.4        | 82.3         |\n|                    |          |          | WizardLM 70B | Orca 13B | Orca 13B | Platypus2 70B | WizardLM 70B | WizardCoder 34B | Flan-T5 11B | MetaMath 70B |\n\n*: ChatGPT (March) results are from [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774), [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub), and our evaluation. Please note that ChatGPT is not a fixed baseline and evolves rapidly over time.\n\n^: Zephyr-β often fails to follow few-shot CoT instructions, likely because it was aligned with only chat data but not trained on few-shot data.\n\n**: Mistral and Open-source SOTA results are taken from reported results in instruction-tuned model papers and official repositories.\n\nAll models are evaluated in chat mode (e.g. with the respective conversation template applied). All zero-shot benchmarks follow the same setting as in the AGIEval paper and Orca paper. CoT tasks use the same configuration as Chain-of-Thought Hub, HumanEval is evaluated with EvalPlus, and MT-bench is run using FastChat. To reproduce our results, follow the instructions in [our repository](https://github.com/imoneoi/openchat/#benchmarks).\n\n## Limitations\n\n**Foundation Model Limitations**\nDespite its advanced capabilities, OpenChat is still bound by the limitations inherent in its foundation models. These limitations may impact the model''s performance in areas such as:\n\n - Complex reasoning\n - Mathematical and arithmetic tasks\n - Programming and coding challenges\n\n**Hallucination of Non-existent Information**\nOpenChat may sometimes generate information that does not exist or is not accurate, also known as "hallucination". Users should be aware of this possibility and verify any critical information obtained from the model.\n\n**Safety**\nOpenChat may sometimes generate harmful, hate speech, biased responses, or answer unsafe questions. It''s crucial to apply additional AI safety measures in use cases that require safe and moderated responses.\n\n## License\n\nOur OpenChat 3.5 code and models are distributed under the Apache License 2.0.\n\n## Dataset Details\n\nOpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:\n\n - [OpenChat ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset)\n - [Open-Orca with FLAN answers](https://huggingface.co/datasets/imone/OpenOrca_FLAN)\n - Capybara [1](https://huggingface.co/datasets/LDJnr/Pure-Dove) [2](https://huggingface.co/datasets/LDJnr/Verified-Camel) [3](https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct)\n - [GOAT](https://huggingface.co/datasets/tiedong/goat)\n - [Glaive](https://huggingface.co/datasets/glaiveai/glaive-code-assistant)\n - [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA)\n - [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n - [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25)\n\n## Citation\n\n```\n@article{wang2023openchat,\n  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},\n  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},\n  journal={arXiv preprint arXiv:2309.11235},\n  year={2023}\n}\n```\n\n## 💌 Contact\n\n**Project Lead:**\n- Guan Wang [imonenext at gmail dot com]\n- [Alpay Ariyak](https://github.com/alpayariyak) [aariyak at wpi dot edu]\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":28967620710,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{{ bos_token }}{% for message in messages %}{{ ''GPT4 Correct '' + message[''role''].title() + '': '' + message[''content''] + ''<|end_of_turn|>''}}{% endfor %}{% if add_generation_prompt %}{{ ''GPT4 Correct Assistant:'' }}{% endif %}","eos_token":"<|end_of_turn|>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:imoneoi:openchat\">GitHub","source_url":"https://github.com/imoneoi/openchat\">GitHub"},{"type":"has_code","target_id":"github:alpayariyak:openchat","source_url":"https://github.com/alpayariyak/openchat"},{"type":"has_code","target_id":"github:imoneoi:openchat","source_url":"https://github.com/imoneoi/openchat#installation"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:imoneoi:openchat","source_url":"https://github.com/imoneoi/openchat#web-ui"},{"type":"has_code","target_id":"github:FranxYao:chain-of-thought-hub","source_url":"https://github.com/FranxYao/chain-of-thought-hub"},{"type":"has_code","target_id":"github:imoneoi:openchat","source_url":"https://github.com/imoneoi/openchat"},{"type":"based_on_paper","target_id":"arxiv:2309.11235","source_url":"https://arxiv.org/abs/2309.11235"},{"type":"based_on_paper","target_id":"arxiv:2303.08774","source_url":"https://arxiv.org/abs/2303.08774"}]', NULL, 'Apache-2.0', 'approved', 80, 'bd91425ecc02d8e45dd2d08d70bceeaf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-GLM-4.6', 'huggingface--zai-org--glm-4.6', 'GLM-4.6', 'zai-org', '--- language: - en - zh library_name: transformers license: mit pipeline_tag: text-generation --- <div align="center"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/> </div> <p align="center"> 👋 Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community. <br> 📖 Check out the GLM-4.6 <a href="https://z.ai/blog/glm-4.6" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="...', '["transformers","safetensors","glm4_moe","text-generation","conversational","en","zh","arxiv:2508.06471","license:mit","endpoints_compatible","region:us"]', 'text-generation', 1134, 331141, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/GLM-4.6","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\n- zh\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation\n---\n\n# GLM-4.6\n\n<div align="center">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/>\n</div>\n<p align="center">\n    👋 Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community.\n    <br>\n    📖 Check out the GLM-4.6 <a href="https://z.ai/blog/glm-4.6" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="_blank">technical report(GLM-4.5)</a>, and <a href="https://zhipu-ai.feishu.cn/wiki/Gv3swM0Yci7w7Zke9E0crhU7n7D" target="_blank">Zhipu AI technical documentation</a>.\n    <br>\n    📍 Use GLM-4.6 API services on <a href="https://docs.z.ai/guides/llm/glm-4.6">Z.ai API Platform. </a>\n    <br>\n    👉 One click to <a href="https://chat.z.ai">GLM-4.6</a>.\n</p>\n\n## Model Introduction\n\nCompared with GLM-4.5, **GLM-4.6**  brings several key improvements:\n\n* **Longer context window:** The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\n* **Superior coding performance:** The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\n* **Advanced reasoning:** GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\n* **More capable agents:** GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\n* **Refined writing:** Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\n\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as **DeepSeek-V3.1-Terminus** and **Claude Sonnet 4**.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench_glm46.png)\n\n## Inference\n\n**Both GLM-4.5 and GLM-4.6 use the same inference method.**\n\nyou can check our [github](https://github.com/zai-org/GLM-4.5) for more detail.\n\n## Recommended Evaluation Parameters\n\nFor general evaluations, we recommend using a **sampling temperature of 1.0**.\n\nFor **code-related evaluation tasks** (such as LCB), it is further recommended to set:\n\n- `top_p = 0.95`\n- `top_k = 40`\n\n\n## Evaluation\n\n- For tool-integrated reasoning, please refer to [this doc](https://github.com/zai-org/GLM-4.5/blob/main/resources/glm_4.6_tir_guide.md).\n- For search benchmark, we design a specific format for searching toolcall in thinking mode to support search agent, please refer to [this](https://github.com/zai-org/GLM-4.5/blob/main/resources/trajectory_search.json). for the detailed template.\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":356785898816,"storage_bytes":713597382171,"files_count":101,"spaces_count":83,"gated":false,"private":false,"config":{"architectures":["Glm4MoeForCausalLM"],"model_type":"glm4_moe","tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson(ensure_ascii=False) }}\n{% endfor %}\n</tools>\n\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{function-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == ''text'' -%}\n                {{- item.text }}\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%- if m.role == ''user'' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == ''user'' -%}<|user|>\n{{ visible_text(m.content) }}\n{{- ''/nothink'' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '''' -}}\n{%- elif m.role == ''assistant'' -%}\n<|assistant|>\n{%- set reasoning_content = '''' %}\n{%- set content = visible_text(m.content) %}\n{%- if m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- if ''</think>'' in content %}\n        {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n        {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n    {%- endif %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ ''\\n<think>'' + reasoning_content.strip() +  ''</think>''}}\n{%- else -%}\n{{ ''\\n<think></think>'' }}\n{%- endif -%}\n{%- if content.strip() -%}\n{{ ''\\n'' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ ''\\n<tool_call>'' + tc.name }}\n{% set _args = tc.arguments %}\n{% for k, v in _args.items() %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%- elif m.role == ''tool'' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- ''<|observation|>'' }}\n{%- endif %}\n{{- ''\\n<tool_response>\\n'' }}\n{{- m.content }}\n{{- ''\\n</tool_response>'' }}\n{%- else -%}\n<|observation|>{% for tr in m.content %}\n\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == ''system'' -%}\n<|system|>\n{{ visible_text(m.content) }}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    <|assistant|>{{- ''\\n<think></think>'' if (enable_thinking is defined and not enable_thinking) else '''' -}}\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"based_on_paper","target_id":"arxiv:2508.06471","source_url":"https://arxiv.org/abs/2508.06471"}]', NULL, 'MIT', 'approved', 65, 'b991e5110b47b2cbf3caeb9345303195', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-hakurei-waifu-diffusion-v1-4', 'huggingface--hakurei--waifu-diffusion-v1-4', 'waifu-diffusion-v1-4', 'hakurei', '--- language: - en tags: - stable-diffusion - text-to-image license: creativeml-openrail-m inference: false --- !image <sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub> Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. - Waifu Diffusion 1.4 Anime Epoch 1: A test model made to properly ensure that the training setup w...', '["stable-diffusion","text-to-image","en","license:creativeml-openrail-m","region:us"]', 'text-to-image', 1132, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/hakurei/waifu-diffusion-v1-4","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\nlicense: creativeml-openrail-m\ninference: false\n\n---\n\n![image](https://user-images.githubusercontent.com/26317155/210155933-db3a5f1a-1ec3-4777-915c-6deff2841ce9.png)\n\n<sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub>\n\n# Waifu Diffusion v1.4\n\nWaifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n\n- [Waifu Diffusion 1.4 Anime Epoch 1](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt): A test model made to properly ensure that the training setup works.\n- [Waifu Diffusion 1.4 Anime Inference Config](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.yaml): A file included to allow for inference with Automatic''s WebUI and with the original Stable Diffusion codebase.\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)\n\n## Downstream Uses\n\nThis model can be used for entertainment purposes and as a generative art assistant.\n\n## Team Members and Acknowledgements\n\nThis project would not have been possible without the incredible work by Stability AI and NovelAI.\n\n- [Haru](https://github.com/harubaru)\n- [Salt](https://github.com/sALTaccount/)\n- [Cafe](https://twitter.com/cafeai_labs)\n\nIn order to reach us, you can join our [Discord server](https://discord.gg/touhouai).\n\n[![Discord Server](https://discordapp.com/api/guilds/930499730843250783/widget.png?style=banner2)](https://discord.gg/touhouai)', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":50325073304,"files_count":12,"spaces_count":17,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 65, 'a57efefbe125269fd616dcf2a00ececd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-SWivid-F5-TTS', 'huggingface--swivid--f5-tts', 'F5-TTS', 'SWivid', '--- license: cc-by-nc-4.0 pipeline_tag: text-to-speech library_name: f5-tts datasets: - amphion/Emilia-Dataset --- Download F5-TTS or E2 TTS and place under ckpts/ Github: https://github.com/SWivid/F5-TTS Paper: F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching', '["f5-tts","text-to-speech","dataset:amphion/emilia-dataset","arxiv:2410.06885","license:cc-by-nc-4.0","region:us"]', 'text-to-speech', 1125, 793228, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/SWivid/F5-TTS","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: cc-by-nc-4.0\npipeline_tag: text-to-speech\nlibrary_name: f5-tts\ndatasets:\n- amphion/Emilia-Dataset\n---\n\nDownload [F5-TTS](https://huggingface.co/SWivid/F5-TTS/tree/main/F5TTS_Base) or [E2 TTS](https://huggingface.co/SWivid/E2-TTS/tree/main/E2TTS_Base) and place under ckpts/\n```\nckpts/\n    F5TTS_v1_Base/\n        model_1250000.safetensors\n    F5TTS_Base/\n        model_1200000.safetensors\n    E2TTS_Base/\n        model_1200000.safetensors\n```\nGithub: https://github.com/SWivid/F5-TTS      \nPaper: [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://huggingface.co/papers/2410.06885)', '{"pipeline_tag":"text-to-speech","library_name":"f5-tts","framework":"f5-tts","params":null,"storage_bytes":14834315614,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:SWivid:F5-TTS","source_url":"https://github.com/SWivid/F5-TTS"},{"type":"based_on_paper","target_id":"arxiv:2410.06885","source_url":"https://arxiv.org/abs/2410.06885"}]', NULL, 'CC-BY-NC-4.0', 'approved', 50, '990d0fc320685f3c4164451ecb10ce5c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-databricks-dbrx-instruct', 'huggingface--databricks--dbrx-instruct', 'dbrx-instruct', 'databricks', '', '["transformers","safetensors","dbrx","text-generation","conversational","arxiv:2211.15841","arxiv:2304.11277","license:other","text-generation-inference","region:us"]', 'text-generation', 1118, 7593, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/databricks/dbrx-instruct","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":131596523520,"storage_bytes":263193089336,"files_count":74,"spaces_count":66,"gated":"auto","private":false,"config":{"architectures":["DbrxForCausalLM"],"model_type":"dbrx","tokenizer_config":{"bos_token":"<|endoftext|>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif ''system'' not in messages[0][''role''] %}{% set loop_messages = messages %}{% set system_message = ''You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\nYou assist with various tasks, from writing to coding (using markdown for code blocks — remember to use ``` with code, JSON, and tables).\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER\\''S QUERY.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ ''<|im_start|>system\n'' + system_message | trim + ''<|im_end|>\n''}}{% endif %}{{ ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% else %}{{ ''\n'' + ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ ''\n'' + ''<|im_start|>'' + ''assistant'' + ''\n'' }}{% endif %}{% endfor %}","eos_token":"<|endoftext|>","pad_token":"<|pad|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2211.15841","source_url":"https://arxiv.org/abs/2211.15841"},{"type":"based_on_paper","target_id":"arxiv:2304.11277","source_url":"https://arxiv.org/abs/2304.11277"}]', NULL, 'Other', 'approved', 40, '183688af6e763a60177a4952105dbbca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HuggingFaceH4-zephyr-7b-alpha', 'huggingface--huggingfaceh4--zephyr-7b-alpha', 'zephyr-7b-alpha', 'HuggingFaceH4', '--- tags: - generated_from_trainer model-index: - name: zephyr-7b-alpha results: [] license: mit datasets: - stingning/ultrachat - openbmb/UltraFeedback language: - en base_model: mistralai/Mistral-7B-v0.1 --- <!-- This model card has been generated automatically according to the information the Trainer had access to. You should probably proofread and complete it, then remove this comment. --> <img src="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png" alt="Zeph...', '["transformers","pytorch","safetensors","mistral","text-generation","generated_from_trainer","conversational","en","dataset:stingning/ultrachat","dataset:openbmb/ultrafeedback","arxiv:2305.18290","arxiv:2310.16944","arxiv:2305.14233","arxiv:2310.01377","base_model:mistralai/mistral-7b-v0.1","base_model:finetune:mistralai/mistral-7b-v0.1","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1115, 2260, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: zephyr-7b-alpha\n  results: []\nlicense: mit\ndatasets:\n- stingning/ultrachat\n- openbmb/UltraFeedback\nlanguage:\n- en\nbase_model: mistralai/Mistral-7B-v0.1\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n<img src="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png" alt="Zephyr Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n\n# Model Card for Zephyr 7B Alpha\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-α is the first model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so.\n\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with [🤗 TRL''s](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities. \n\nHere''s how you can run the model using the `pipeline()` function from 🤗 Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-alpha", torch_dtype=torch.bfloat16, device_map="auto")\n\n# We use the tokenizer''s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        "role": "system",\n        "content": "You are a friendly chatbot who always responds in the style of a pirate",\n    },\n    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0]["generated_text"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-α has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). \nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n\n## Training and evaluation data\n\nZephyr 7B Alpha achieves the following results on the evaluation set:\n\n- Loss: 0.4605\n- Rewards/chosen: -0.5053\n- Rewards/rejected: -1.8752\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 1.3699\n- Logps/rejected: -327.4286\n- Logps/chosen: -297.1040\n- Logits/rejected: -2.7153\n- Logits/chosen: -2.7447\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.5602        | 0.05  | 100  | 0.5589          | -0.3359        | -0.8168          | 0.7188             | 0.4809          | -306.2607      | -293.7161    | -2.6554         | -2.6797       |\n| 0.4852        | 0.1   | 200  | 0.5136          | -0.5310        | -1.4994          | 0.8125             | 0.9684          | -319.9124      | -297.6181    | -2.5762         | -2.5957       |\n| 0.5212        | 0.15  | 300  | 0.5168          | -0.1686        | -1.1760          | 0.7812             | 1.0074          | -313.4444      | -290.3699    | -2.6865         | -2.7125       |\n| 0.5496        | 0.21  | 400  | 0.4835          | -0.1617        | -1.7170          | 0.8281             | 1.5552          | -324.2635      | -290.2326    | -2.7947         | -2.8218       |\n| 0.5209        | 0.26  | 500  | 0.5054          | -0.4778        | -1.6604          | 0.7344             | 1.1826          | -323.1325      | -296.5546    | -2.8388         | -2.8667       |\n| 0.4617        | 0.31  | 600  | 0.4910          | -0.3738        | -1.5180          | 0.7656             | 1.1442          | -320.2848      | -294.4741    | -2.8234         | -2.8521       |\n| 0.4452        | 0.36  | 700  | 0.4838          | -0.4591        | -1.6576          | 0.7031             | 1.1986          | -323.0770      | -296.1796    | -2.7401         | -2.7653       |\n| 0.4674        | 0.41  | 800  | 0.5077          | -0.5692        | -1.8659          | 0.7656             | 1.2967          | -327.2416      | -298.3818    | -2.6740         | -2.6945       |\n| 0.4656        | 0.46  | 900  | 0.4927          | -0.5279        | -1.6614          | 0.7656             | 1.1335          | -323.1518      | -297.5553    | -2.7817         | -2.8015       |\n| 0.4102        | 0.52  | 1000 | 0.4772          | -0.5767        | -2.0667          | 0.7656             | 1.4900          | -331.2578      | -298.5311    | -2.7160         | -2.7455       |\n| 0.4663        | 0.57  | 1100 | 0.4740          | -0.8038        | -2.1018          | 0.7656             | 1.2980          | -331.9604      | -303.0741    | -2.6994         | -2.7257       |\n| 0.4737        | 0.62  | 1200 | 0.4716          | -0.3783        | -1.7015          | 0.7969             | 1.3232          | -323.9545      | -294.5634    | -2.6842         | -2.7135       |\n| 0.4259        | 0.67  | 1300 | 0.4866          | -0.6239        | -1.9703          | 0.7812             | 1.3464          | -329.3312      | -299.4761    | -2.7046         | -2.7356       |\n| 0.4935        | 0.72  | 1400 | 0.4747          | -0.5626        | -1.7600          | 0.7812             | 1.1974          | -325.1243      | -298.2491    | -2.7153         | -2.7444       |\n| 0.4211        | 0.77  | 1500 | 0.4645          | -0.6099        | -1.9993          | 0.7656             | 1.3894          | -329.9109      | -299.1959    | -2.6944         | -2.7236       |\n| 0.4931        | 0.83  | 1600 | 0.4684          | -0.6798        | -2.1082          | 0.7656             | 1.4285          | -332.0890      | -300.5934    | -2.7006         | -2.7305       |\n| 0.5029        | 0.88  | 1700 | 0.4595          | -0.5063        | -1.8951          | 0.7812             | 1.3889          | -327.8267      | -297.1233    | -2.7108         | -2.7403       |\n| 0.4965        | 0.93  | 1800 | 0.4613          | -0.5561        | -1.9079          | 0.7812             | 1.3518          | -328.0831      | -298.1203    | -2.7226         | -2.7523       |\n| 0.4337        | 0.98  | 1900 | 0.4608          | -0.5066        | -1.8718          | 0.7656             | 1.3652          | -327.3599      | -297.1296    | -2.7175         | -2.7469       |\n\n\n### Framework versions\n\n- Transformers 4.34.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0\n\n## Citation\n\nIf you find Zephyr-7B-α is useful in your work, please cite it with:\n\n```\n@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\nIf you use the UltraChat or UltraFeedback datasets, please cite the original works:\n\n```\n@misc{ding2023enhancing,\n      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, \n      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\n      year={2023},\n      eprint={2305.14233},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{cui2023ultrafeedback,\n      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, \n      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},\n      year={2023},\n      eprint={2310.01377},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241732096,"storage_bytes":59853260527,"files_count":34,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}\n{% if message[''role''] == ''user'' %}\n{{ ''<|user|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role''] == ''system'' %}\n{{ ''<|system|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role''] == ''assistant'' %}\n{{ ''<|assistant|>\n''  + message[''content''] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ ''<|assistant|>'' }}\n{% endif %}\n{% endfor %}","eos_token":"</s>","pad_token":"</s>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:alignment-handbook","source_url":"https://github.com/huggingface/alignment-handbook"},{"type":"has_code","target_id":"github:huggingface:trl","source_url":"https://github.com/huggingface/trl"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"based_on_paper","target_id":"arxiv:2305.18290","source_url":"https://arxiv.org/abs/2305.18290"},{"type":"based_on_paper","target_id":"arxiv:2310.16944","source_url":"https://arxiv.org/abs/2310.16944"},{"type":"based_on_paper","target_id":"arxiv:2305.14233","source_url":"https://arxiv.org/abs/2305.14233"},{"type":"based_on_paper","target_id":"arxiv:2310.01377","source_url":"https://arxiv.org/abs/2310.01377"}]', NULL, 'MIT', 'approved', 80, '02a666bcc80c943a1067ed6d3914c439', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2b', 'huggingface--google--gemma-2b', 'gemma-2b', 'google', '', '["transformers","safetensors","gguf","gemma","text-generation","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1112, 198921, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2b","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2506172416,"storage_bytes":67347910850,"files_count":12,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 40, 'c2549823fe32fe64b337a65c0fbf36ac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-jinaai-jina-embeddings-v3', 'huggingface--jinaai--jina-embeddings-v3', 'jina-embeddings-v3', 'jinaai', '--- license: cc-by-nc-4.0 tags: - feature-extraction - sentence-similarity - mteb - sentence-transformers language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk -...', '["transformers","pytorch","onnx","safetensors","feature-extraction","sentence-similarity","mteb","sentence-transformers","custom_code","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:2409.10173","license:cc-by-nc-4.0","model-index","region:eu"]', 'feature-extraction', 1108, 5136811, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/jinaai/jina-embeddings-v3","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\ntags:\n- feature-extraction\n- sentence-similarity\n- mteb\n- sentence-transformers\nlanguage:\n  - multilingual\n  - af\n  - am\n  - ar\n  - as\n  - az\n  - be\n  - bg\n  - bn\n  - br\n  - bs\n  - ca\n  - cs\n  - cy\n  - da\n  - de\n  - el\n  - en\n  - eo\n  - es\n  - et\n  - eu\n  - fa\n  - fi\n  - fr\n  - fy\n  - ga\n  - gd\n  - gl\n  - gu\n  - ha\n  - he\n  - hi\n  - hr\n  - hu\n  - hy\n  - id\n  - is\n  - it\n  - ja\n  - jv\n  - ka\n  - kk\n  - km\n  - kn\n  - ko\n  - ku\n  - ky\n  - la\n  - lo\n  - lt\n  - lv\n  - mg\n  - mk\n  - ml\n  - mn\n  - mr\n  - ms\n  - my\n  - ne\n  - nl\n  - no\n  - om\n  - or\n  - pa\n  - pl\n  - ps\n  - pt\n  - ro\n  - ru\n  - sa\n  - sd\n  - si\n  - sk\n  - sl\n  - so\n  - sq\n  - sr\n  - su\n  - sv\n  - sw\n  - ta\n  - te\n  - th\n  - tl\n  - tr\n  - ug\n  - uk\n  - ur\n  - uz\n  - vi\n  - xh\n  - yi\n  - zh\ninference: false\nlibrary_name: transformers\nmodel-index:\n- name: jina-embeddings-v3\n  results:\n  - dataset:\n      config: default\n      name: MTEB AFQMC (default)\n      revision: b44c3b011063adb25877c13823db83bb193913c4\n      split: validation\n      type: C-MTEB/AFQMC\n    metrics:\n    - type: cosine_pearson\n      value: 41.74237700998808\n    - type: cosine_spearman\n      value: 43.4726782647566\n    - type: euclidean_pearson\n      value: 42.244585459479964\n    - type: euclidean_spearman\n      value: 43.525070045169606\n    - type: main_score\n      value: 43.4726782647566\n    - type: manhattan_pearson\n      value: 42.04616728224863\n    - type: manhattan_spearman\n      value: 43.308828270754645\n    - type: pearson\n      value: 41.74237700998808\n    - type: spearman\n      value: 43.4726782647566\n    task:\n      type: STS\n  - dataset:\n      config: default\n      name: MTEB ArguAna-PL (default)\n      revision: 63fc86750af76253e8c760fc9e534bbf24d260a2\n      split: test\n      type: clarin-knext/arguana-pl\n    metrics:\n    - type: main_score\n      value: 50.117999999999995\n    - type: map_at_1\n      value: 24.253\n    - type: map_at_10\n      value: 40.725\n    - type: map_at_100\n      value: 41.699999999999996\n    - type: map_at_1000\n      value: 41.707\n    - type: map_at_20\n      value: 41.467999999999996\n    - type: map_at_3\n      value: 35.467\n    - type: map_at_5\n      value: 38.291\n    - type: mrr_at_1\n      value: 24.751066856330013\n    - type: mrr_at_10\n      value: 40.91063808169072\n    - type: mrr_at_100\n      value: 41.885497923928675\n    - type: mrr_at_1000\n      value: 41.89301098419842\n    - type: mrr_at_20\n      value: 41.653552355442514\n    - type: mrr_at_3\n      value: 35.656709340919775\n    - type: mrr_at_5\n      value: 38.466097676623946\n    - type: nauc_map_at_1000_diff1\n      value: 7.503000359807567\n    - type: nauc_map_at_1000_max\n      value: -11.030405164830546\n    - type: nauc_map_at_1000_std\n      value: -8.902792782585117\n    - type: nauc_map_at_100_diff1\n      value: 7.509899249593199\n    - type: nauc_map_at_100_max\n      value: -11.023581259404406\n    - type: nauc_map_at_100_std\n      value: -8.892241185067272\n    - type: nauc_map_at_10_diff1\n      value: 7.24369711881512\n    - type: nauc_map_at_10_max\n      value: -10.810000200433278\n    - type: nauc_map_at_10_std\n      value: -8.987230542165776\n    - type: nauc_map_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_map_at_1_max\n      value: -13.315221903223055\n    - type: nauc_map_at_1_std\n      value: -9.398199605510275\n    - type: nauc_map_at_20_diff1\n      value: 7.477364530860648\n    - type: nauc_map_at_20_max\n      value: -10.901251218105566\n    - type: nauc_map_at_20_std\n      value: -8.868148116405925\n    - type: nauc_map_at_3_diff1\n      value: 6.555548802174882\n    - type: nauc_map_at_3_max\n      value: -12.247274800542934\n    - type: nauc_map_at_3_std\n      value: -9.879475250984811\n    - type: nauc_map_at_5_diff1\n      value: 7.426588563355882\n    - type: nauc_map_at_5_max\n      value: -11.347695686001805\n    - type: nauc_map_at_5_std\n      value: -9.34441892203972\n    - type: nauc_mrr_at_1000_diff1\n      value: 5.99737552143614\n    - type: nauc_mrr_at_1000_max\n      value: -11.327205136505727\n    - type: nauc_mrr_at_1000_std\n      value: -8.791079115519503\n    - type: nauc_mrr_at_100_diff1\n      value: 6.004622525255784\n    - type: nauc_mrr_at_100_max\n      value: -11.320336759899723\n    - type: nauc_mrr_at_100_std\n      value: -8.780602249831777\n    - type: nauc_mrr_at_10_diff1\n      value: 5.783623516930227\n    - type: nauc_mrr_at_10_max\n      value: -11.095971693467078\n    - type: nauc_mrr_at_10_std\n      value: -8.877242032013582\n    - type: nauc_mrr_at_1_diff1\n      value: 9.694937537703797\n    - type: nauc_mrr_at_1_max\n      value: -12.531905083727912\n    - type: nauc_mrr_at_1_std\n      value: -8.903992940100146\n    - type: nauc_mrr_at_20_diff1\n      value: 5.984841206233873\n    - type: nauc_mrr_at_20_max\n      value: -11.195236951048969\n    - type: nauc_mrr_at_20_std\n      value: -8.757266039186018\n    - type: nauc_mrr_at_3_diff1\n      value: 5.114333824261379\n    - type: nauc_mrr_at_3_max\n      value: -12.64809799843464\n    - type: nauc_mrr_at_3_std\n      value: -9.791146138025184\n    - type: nauc_mrr_at_5_diff1\n      value: 5.88941606224512\n    - type: nauc_mrr_at_5_max\n      value: -11.763903418071918\n    - type: nauc_mrr_at_5_std\n      value: -9.279175712709446\n    - type: nauc_ndcg_at_1000_diff1\n      value: 7.076950652226086\n    - type: nauc_ndcg_at_1000_max\n      value: -10.386482092087371\n    - type: nauc_ndcg_at_1000_std\n      value: -8.309190917074046\n    - type: nauc_ndcg_at_100_diff1\n      value: 7.2329220284865245\n    - type: nauc_ndcg_at_100_max\n      value: -10.208048403220337\n    - type: nauc_ndcg_at_100_std\n      value: -7.997975874274613\n    - type: nauc_ndcg_at_10_diff1\n      value: 6.065391100006953\n    - type: nauc_ndcg_at_10_max\n      value: -9.046164377601153\n    - type: nauc_ndcg_at_10_std\n      value: -8.34724889697153\n    - type: nauc_ndcg_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_ndcg_at_1_max\n      value: -13.315221903223055\n    - type: nauc_ndcg_at_1_std\n      value: -9.398199605510275\n    - type: nauc_ndcg_at_20_diff1\n      value: 6.949389989202601\n    - type: nauc_ndcg_at_20_max\n      value: -9.35740451760307\n    - type: nauc_ndcg_at_20_std\n      value: -7.761295171828212\n    - type: nauc_ndcg_at_3_diff1\n      value: 5.051471796151364\n    - type: nauc_ndcg_at_3_max\n      value: -12.158763333711653\n    - type: nauc_ndcg_at_3_std\n      value: -10.078902544421926\n    - type: nauc_ndcg_at_5_diff1\n      value: 6.527454512611454\n    - type: nauc_ndcg_at_5_max\n      value: -10.525118233848586\n    - type: nauc_ndcg_at_5_std\n      value: -9.120055125584031\n    - type: nauc_precision_at_1000_diff1\n      value: -10.6495668199151\n    - type: nauc_precision_at_1000_max\n      value: 12.070656425217841\n    - type: nauc_precision_at_1000_std\n      value: 55.844551709649004\n    - type: nauc_precision_at_100_diff1\n      value: 19.206967129266285\n    - type: nauc_precision_at_100_max\n      value: 16.296851020813456\n    - type: nauc_precision_at_100_std\n      value: 45.60378984257811\n    - type: nauc_precision_at_10_diff1\n      value: 0.6490335354304879\n    - type: nauc_precision_at_10_max\n      value: 0.5757198255366447\n    - type: nauc_precision_at_10_std\n      value: -4.875847131691451\n    - type: nauc_precision_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_precision_at_1_max\n      value: -13.315221903223055\n    - type: nauc_precision_at_1_std\n      value: -9.398199605510275\n    - type: nauc_precision_at_20_diff1\n      value: 4.899369866929203\n    - type: nauc_precision_at_20_max\n      value: 5.988537297189552\n    - type: nauc_precision_at_20_std\n      value: 4.830900387582837\n    - type: nauc_precision_at_3_diff1\n      value: 0.8791156910997744\n    - type: nauc_precision_at_3_max\n      value: -11.983373635905993\n    - type: nauc_precision_at_3_std\n      value: -10.646185111581257\n    - type: nauc_precision_at_5_diff1\n      value: 3.9314486166548432\n    - type: nauc_precision_at_5_max\n      value: -7.798591396895839\n    - type: nauc_precision_at_5_std\n      value: -8.293043407234125\n    - type: nauc_recall_at_1000_diff1\n      value: -10.649566819918673\n    - type: nauc_recall_at_1000_max\n      value: 12.070656425214647\n    - type: nauc_recall_at_1000_std\n      value: 55.84455170965023\n    - type: nauc_recall_at_100_diff1\n      value: 19.206967129265127\n    - type: nauc_recall_at_100_max\n      value: 16.296851020813722\n    - type: nauc_recall_at_100_std\n      value: 45.60378984257728\n    - type: nauc_recall_at_10_diff1\n      value: 0.6490335354304176\n    - type: nauc_recall_at_10_max\n      value: 0.5757198255366095\n    - type: nauc_recall_at_10_std\n      value: -4.875847131691468\n    - type: nauc_recall_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_recall_at_1_max\n      value: -13.315221903223055\n    - type: nauc_recall_at_1_std\n      value: -9.398199605510275\n    - type: nauc_recall_at_20_diff1\n      value: 4.899369866929402\n    - type: nauc_recall_at_20_max\n      value: 5.98853729718968\n    - type: nauc_recall_at_20_std\n      value: 4.830900387582967\n    - type: nauc_recall_at_3_diff1\n      value: 0.8791156910997652\n    - type: nauc_recall_at_3_max\n      value: -11.983373635905997\n    - type: nauc_recall_at_3_std\n      value: -10.64618511158124\n    - type: nauc_recall_at_5_diff1\n      value: 3.9314486166548472\n    - type: nauc_recall_at_5_max\n      value: -7.7985913968958585\n    - type: nauc_recall_at_5_std\n      value: -8.293043407234132\n    - type: ndcg_at_1\n      value: 24.253\n    - type: ndcg_at_10\n      value: 50.117999999999995\n    - type: ndcg_at_100\n      value: 54.291999999999994\n    - type: ndcg_at_1000\n      value: 54.44799999999999\n    - type: ndcg_at_20\n      value: 52.771\n    - type: ndcg_at_3\n      value: 39.296\n    - type: ndcg_at_5\n      value: 44.373000000000005\n    - type: precision_at_1\n      value: 24.253\n    - type: precision_at_10\n      value: 8.016\n    - type: precision_at_100\n      value: 0.984\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_20\n      value: 4.527\n    - type: precision_at_3\n      value: 16.808999999999997\n    - type: precision_at_5\n      value: 12.546\n    - type: recall_at_1\n      value: 24.253\n    - type: recall_at_10\n      value: 80.156\n    - type: recall_at_100\n      value: 98.43499999999999\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_20\n      value: 90.54100000000001\n    - type: recall_at_3\n      value: 50.427\n    - type: recall_at_5\n      value: 62.731\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB DBPedia-PL (default)\n      revision: 76afe41d9af165cc40999fcaa92312b8b012064a\n      split: test\n      type: clarin-knext/dbpedia-pl\n    metrics:\n    - type: main_score\n      value: 34.827000000000005\n    - type: map_at_1\n      value: 7.049999999999999\n    - type: map_at_10\n      value: 14.982999999999999\n    - type: map_at_100\n      value: 20.816000000000003\n    - type: map_at_1000\n      value: 22.33\n    - type: map_at_20\n      value: 17.272000000000002\n    - type: map_at_3\n      value: 10.661\n    - type: map_at_5\n      value: 12.498\n    - type: mrr_at_1\n      value: 57.25\n    - type: mrr_at_10\n      value: 65.81934523809524\n    - type: mrr_at_100\n      value: 66.2564203928212\n    - type: mrr_at_1000\n      value: 66.27993662923856\n    - type: mrr_at_20\n      value: 66.0732139130649\n    - type: mrr_at_3\n      value: 64.08333333333333\n    - type: mrr_at_5\n      value: 65.27083333333333\n    - type: nauc_map_at_1000_diff1\n      value: 16.41780871174038\n    - type: nauc_map_at_1000_max\n      value: 30.193946325654654\n    - type: nauc_map_at_1000_std\n      value: 31.46095497039037\n    - type: nauc_map_at_100_diff1\n      value: 18.57903165498531\n    - type: nauc_map_at_100_max\n      value: 29.541476938623262\n    - type: nauc_map_at_100_std\n      value: 28.228604103301052\n    - type: nauc_map_at_10_diff1\n      value: 24.109434489748946\n    - type: nauc_map_at_10_max\n      value: 21.475954208048968\n    - type: nauc_map_at_10_std\n      value: 9.964464537806988\n    - type: nauc_map_at_1_diff1\n      value: 38.67437644802124\n    - type: nauc_map_at_1_max\n      value: 14.52136658726491\n    - type: nauc_map_at_1_std\n      value: -2.8981666782088755\n    - type: nauc_map_at_20_diff1\n      value: 21.42547228801935\n    - type: nauc_map_at_20_max\n      value: 25.04510402960458\n    - type: nauc_map_at_20_std\n      value: 16.533079346431155\n    - type: nauc_map_at_3_diff1\n      value: 26.63648858245477\n    - type: nauc_map_at_3_max\n      value: 13.632235789780415\n    - type: nauc_map_at_3_std\n      value: -0.40129174577700716\n    - type: nauc_map_at_5_diff1\n      value: 24.513861031197933\n    - type: nauc_map_at_5_max\n      value: 16.599888813946688\n    - type: nauc_map_at_5_std\n      value: 3.4448514739556346\n    - type: nauc_mrr_at_1000_diff1\n      value: 36.57353464537154\n    - type: nauc_mrr_at_1000_max\n      value: 55.34763483979515\n    - type: nauc_mrr_at_1000_std\n      value: 40.3722796438533\n    - type: nauc_mrr_at_100_diff1\n      value: 36.555989566513134\n    - type: nauc_mrr_at_100_max\n      value: 55.347805216808396\n    - type: nauc_mrr_at_100_std\n      value: 40.38465945075711\n    - type: nauc_mrr_at_10_diff1\n      value: 36.771572999261984\n    - type: nauc_mrr_at_10_max\n      value: 55.41239897909165\n    - type: nauc_mrr_at_10_std\n      value: 40.52058934624793\n    - type: nauc_mrr_at_1_diff1\n      value: 38.2472828531032\n    - type: nauc_mrr_at_1_max\n      value: 51.528473828685705\n    - type: nauc_mrr_at_1_std\n      value: 33.03676467942882\n    - type: nauc_mrr_at_20_diff1\n      value: 36.642602571889036\n    - type: nauc_mrr_at_20_max\n      value: 55.3763342076553\n    - type: nauc_mrr_at_20_std\n      value: 40.41520090500838\n    - type: nauc_mrr_at_3_diff1\n      value: 36.79451847426628\n    - type: nauc_mrr_at_3_max\n      value: 54.59778581826193\n    - type: nauc_mrr_at_3_std\n      value: 39.48392075873095\n    - type: nauc_mrr_at_5_diff1\n      value: 36.92150807529304\n    - type: nauc_mrr_at_5_max\n      value: 55.03553978718272\n    - type: nauc_mrr_at_5_std\n      value: 40.20147745489917\n    - type: nauc_ndcg_at_1000_diff1\n      value: 21.843092744321268\n    - type: nauc_ndcg_at_1000_max\n      value: 44.93275990394279\n    - type: nauc_ndcg_at_1000_std\n      value: 47.09186225236347\n    - type: nauc_ndcg_at_100_diff1\n      value: 25.180282568979095\n    - type: nauc_ndcg_at_100_max\n      value: 41.737709709508394\n    - type: nauc_ndcg_at_100_std\n      value: 38.80950644139446\n    - type: nauc_ndcg_at_10_diff1\n      value: 24.108368037214046\n    - type: nauc_ndcg_at_10_max\n      value: 41.29298370689967\n    - type: nauc_ndcg_at_10_std\n      value: 35.06450769738732\n    - type: nauc_ndcg_at_1_diff1\n      value: 35.51010679525079\n    - type: nauc_ndcg_at_1_max\n      value: 42.40790024212412\n    - type: nauc_ndcg_at_1_std\n      value: 26.696412036243157\n    - type: nauc_ndcg_at_20_diff1\n      value: 23.909989673256195\n    - type: nauc_ndcg_at_20_max\n      value: 39.78444647091927\n    - type: nauc_ndcg_at_20_std\n      value: 33.39544470364529\n    - type: nauc_ndcg_at_3_diff1\n      value: 22.50484297956035\n    - type: nauc_ndcg_at_3_max\n      value: 39.14551926034168\n    - type: nauc_ndcg_at_3_std\n      value: 30.330135925392014\n    - type: nauc_ndcg_at_5_diff1\n      value: 21.7798872028265\n    - type: nauc_ndcg_at_5_max\n      value: 40.23856975248015\n    - type: nauc_ndcg_at_5_std\n      value: 32.438381067440396\n    - type: nauc_precision_at_1000_diff1\n      value: -21.62692442272279\n    - type: nauc_precision_at_1000_max\n      value: 0.9689046974430882\n    - type: nauc_precision_at_1000_std\n      value: 18.54001058230465\n    - type: nauc_precision_at_100_diff1\n      value: -10.132258779856192\n    - type: nauc_precision_at_100_max\n      value: 23.74516110444681\n    - type: nauc_precision_at_100_std\n      value: 47.03416663319965\n    - type: nauc_precision_at_10_diff1\n      value: 1.543656509571949\n    - type: nauc_precision_at_10_max\n      value: 36.98864812757555\n    - type: nauc_precision_at_10_std\n      value: 46.56427199077426\n    - type: nauc_precision_at_1_diff1\n      value: 38.2472828531032\n    - type: nauc_precision_at_1_max\n      value: 51.528473828685705\n    - type: nauc_precision_at_1_std\n      value: 33.03676467942882\n    - type: nauc_precision_at_20_diff1\n      value: -4.612864872734335\n    - type: nauc_precision_at_20_max\n      value: 34.03565449182125\n    - type: nauc_precision_at_20_std\n      value: 48.880727648349534\n    - type: nauc_precision_at_3_diff1\n      value: 6.360850444467829\n    - type: nauc_precision_at_3_max\n      value: 36.25816942368427\n    - type: nauc_precision_at_3_std\n      value: 34.48882647419187\n    - type: nauc_precision_at_5_diff1\n      value: 2.6445596936740037\n    - type: nauc_precision_at_5_max\n      value: 37.174463388899056\n    - type: nauc_precision_at_5_std\n      value: 40.25254370626113\n    - type: nauc_recall_at_1000_diff1\n      value: 13.041227176748077\n    - type: nauc_recall_at_1000_max\n      value: 39.722336427072094\n    - type: nauc_recall_at_1000_std\n      value: 52.04032890059214\n    - type: nauc_recall_at_100_diff1\n      value: 18.286096899139153\n    - type: nauc_recall_at_100_max\n      value: 34.072389201930314\n    - type: nauc_recall_at_100_std\n      value: 37.73637623416653\n    - type: nauc_recall_at_10_diff1\n      value: 22.35560419280504\n    - type: nauc_recall_at_10_max\n      value: 19.727247199595197\n    - type: nauc_recall_at_10_std\n      value: 8.58498575109203\n    - type: nauc_recall_at_1_diff1\n      value: 38.67437644802124\n    - type: nauc_recall_at_1_max\n      value: 14.52136658726491\n    - type: nauc_recall_at_1_std\n      value: -2.8981666782088755\n    - type: nauc_recall_at_20_diff1\n      value: 19.026320886902916\n    - type: nauc_recall_at_20_max\n      value: 22.753562309469867\n    - type: nauc_recall_at_20_std\n      value: 14.89994263882445\n    - type: nauc_recall_at_3_diff1\n      value: 23.428129702129684\n    - type: nauc_recall_at_3_max\n      value: 10.549153954790542\n    - type: nauc_recall_at_3_std\n      value: -1.7590608997055206\n    - type: nauc_recall_at_5_diff1\n      value: 21.27448645803921\n    - type: nauc_recall_at_5_max\n      value: 13.620279707461677\n    - type: nauc_recall_at_5_std\n      value: 2.0577962208292675\n    - type: ndcg_at_1\n      value: 46.75\n    - type: ndcg_at_10\n      value: 34.827000000000005\n    - type: ndcg_at_100\n      value: 38.157999999999994\n    - type: ndcg_at_1000\n      value: 44.816\n    - type: ndcg_at_20\n      value: 34.152\n    - type: ndcg_at_3\n      value: 39.009\n    - type: ndcg_at_5\n      value: 36.826\n    - type: precision_at_1\n      value: 57.25\n    - type: precision_at_10\n      value: 27.575\n    - type: precision_at_100\n      value: 8.84\n    - type: precision_at_1000\n      value: 1.949\n    - type: precision_at_20\n      value: 20.724999999999998\n    - type: precision_at_3\n      value: 41.167\n    - type: precision_at_5\n      value: 35.199999999999996\n    - type: recall_at_1\n      value: 7.049999999999999\n    - type: recall_at_10\n      value: 19.817999999999998\n    - type: recall_at_100\n      value: 42.559999999999995\n    - type: recall_at_1000\n      value: 63.744\n    - type: recall_at_20\n      value: 25.968000000000004\n    - type: recall_at_3\n      value: 11.959\n    - type: recall_at_5\n      value: 14.939\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB FiQA-PL (default)\n      revision: 2e535829717f8bf9dc829b7f911cc5bbd4e6608e\n      split: test\n      type: clarin-knext/fiqa-pl\n    metrics:\n    - type: main_score\n      value: 38.828\n    - type: map_at_1\n      value: 19.126\n    - type: map_at_10\n      value: 31.002000000000002\n    - type: map_at_100\n      value: 32.736\n    - type: map_at_1000\n      value: 32.933\n    - type: map_at_20\n      value: 31.894\n    - type: map_at_3\n      value: 26.583000000000002\n    - type: map_at_5\n      value: 28.904000000000003\n    - type: mrr_at_1\n      value: 37.808641975308646\n    - type: mrr_at_10\n      value: 46.36745541838134\n    - type: mrr_at_100\n      value: 47.14140915794908\n    - type: mrr_at_1000\n      value: 47.190701435388846\n    - type: mrr_at_20\n      value: 46.81387776440309\n    - type: mrr_at_3\n      value: 43.750000000000014\n    - type: mrr_at_5\n      value: 45.23919753086418\n    - type: nauc_map_at_1000_diff1\n      value: 38.5532285881503\n    - type: nauc_map_at_1000_max\n      value: 34.44383884813453\n    - type: nauc_map_at_1000_std\n      value: -1.3963497949476722\n    - type: nauc_map_at_100_diff1\n      value: 38.49292464176943\n    - type: nauc_map_at_100_max\n      value: 34.33752755618645\n    - type: nauc_map_at_100_std\n      value: -1.4794032905848582\n    - type: nauc_map_at_10_diff1\n      value: 38.26061536370962\n    - type: nauc_map_at_10_max\n      value: 33.16977912721411\n    - type: nauc_map_at_10_std\n      value: -2.3853370604730393\n    - type: nauc_map_at_1_diff1\n      value: 46.288767289528344\n    - type: nauc_map_at_1_max\n      value: 25.67706785013364\n    - type: nauc_map_at_1_std\n      value: -6.989769609924645\n    - type: nauc_map_at_20_diff1\n      value: 38.507270129330685\n    - type: nauc_map_at_20_max\n      value: 33.70963328055982\n    - type: nauc_map_at_20_std\n      value: -1.9835510011554272\n    - type: nauc_map_at_3_diff1\n      value: 39.81061518646884\n    - type: nauc_map_at_3_max\n      value: 30.101186374147748\n    - type: nauc_map_at_3_std\n      value: -4.027120247237715\n    - type: nauc_map_at_5_diff1\n      value: 38.55602589746512\n    - type: nauc_map_at_5_max\n      value: 31.515174267015983\n    - type: nauc_map_at_5_std\n      value: -3.4064239358570303\n    - type: nauc_mrr_at_1000_diff1\n      value: 45.030514454725726\n    - type: nauc_mrr_at_1000_max\n      value: 43.878919881666164\n    - type: nauc_mrr_at_1000_std\n      value: 2.517594250297626\n    - type: nauc_mrr_at_100_diff1\n      value: 45.00868212878687\n    - type: nauc_mrr_at_100_max\n      value: 43.87437011120001\n    - type: nauc_mrr_at_100_std\n      value: 2.5257874265014966\n    - type: nauc_mrr_at_10_diff1\n      value: 44.855044606754056\n    - type: nauc_mrr_at_10_max\n      value: 43.946617058785186\n    - type: nauc_mrr_at_10_std\n      value: 2.5173751662794044\n    - type: nauc_mrr_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_mrr_at_1_max\n      value: 43.08547383044357\n    - type: nauc_mrr_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_mrr_at_20_diff1\n      value: 45.019880416584215\n    - type: nauc_mrr_at_20_max\n      value: 43.85691473662242\n    - type: nauc_mrr_at_20_std\n      value: 2.4625487605091303\n    - type: nauc_mrr_at_3_diff1\n      value: 45.322041658604036\n    - type: nauc_mrr_at_3_max\n      value: 43.95079293074395\n    - type: nauc_mrr_at_3_std\n      value: 2.4644274393435737\n    - type: nauc_mrr_at_5_diff1\n      value: 44.99461837803437\n    - type: nauc_mrr_at_5_max\n      value: 43.97934275090601\n    - type: nauc_mrr_at_5_std\n      value: 2.5353091695125096\n    - type: nauc_ndcg_at_1000_diff1\n      value: 39.38449023275524\n    - type: nauc_ndcg_at_1000_max\n      value: 39.48382767312788\n    - type: nauc_ndcg_at_1000_std\n      value: 3.414789408343409\n    - type: nauc_ndcg_at_100_diff1\n      value: 38.29675861135578\n    - type: nauc_ndcg_at_100_max\n      value: 38.2674786507297\n    - type: nauc_ndcg_at_100_std\n      value: 2.7094055381218207\n    - type: nauc_ndcg_at_10_diff1\n      value: 38.09514955708717\n    - type: nauc_ndcg_at_10_max\n      value: 36.664923238906525\n    - type: nauc_ndcg_at_10_std\n      value: 0.6901410544967921\n    - type: nauc_ndcg_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_ndcg_at_1_max\n      value: 43.08547383044357\n    - type: nauc_ndcg_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_ndcg_at_20_diff1\n      value: 38.44967736231759\n    - type: nauc_ndcg_at_20_max\n      value: 36.871179313622584\n    - type: nauc_ndcg_at_20_std\n      value: 1.157560360065234\n    - type: nauc_ndcg_at_3_diff1\n      value: 39.02419271805571\n    - type: nauc_ndcg_at_3_max\n      value: 37.447669442586324\n    - type: nauc_ndcg_at_3_std\n      value: 0.41502589779297794\n    - type: nauc_ndcg_at_5_diff1\n      value: 38.10233452742001\n    - type: nauc_ndcg_at_5_max\n      value: 35.816381905465676\n    - type: nauc_ndcg_at_5_std\n      value: -0.3704499913387088\n    - type: nauc_precision_at_1000_diff1\n      value: 2.451267097838658\n    - type: nauc_precision_at_1000_max\n      value: 29.116394969085306\n    - type: nauc_precision_at_1000_std\n      value: 14.85900786538363\n    - type: nauc_precision_at_100_diff1\n      value: 8.10919082251277\n    - type: nauc_precision_at_100_max\n      value: 36.28388256191417\n    - type: nauc_precision_at_100_std\n      value: 14.830039904317657\n    - type: nauc_precision_at_10_diff1\n      value: 15.02446609920477\n    - type: nauc_precision_at_10_max\n      value: 41.008463775454054\n    - type: nauc_precision_at_10_std\n      value: 10.431403152334486\n    - type: nauc_precision_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_precision_at_1_max\n      value: 43.08547383044357\n    - type: nauc_precision_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_precision_at_20_diff1\n      value: 14.222022201169926\n    - type: nauc_precision_at_20_max\n      value: 40.10189643835305\n    - type: nauc_precision_at_20_std\n      value: 12.204443815975527\n    - type: nauc_precision_at_3_diff1\n      value: 25.41905395341234\n    - type: nauc_precision_at_3_max\n      value: 41.56133905339819\n    - type: nauc_precision_at_3_std\n      value: 5.575516915590082\n    - type: nauc_precision_at_5_diff1\n      value: 20.20081221089351\n    - type: nauc_precision_at_5_max\n      value: 40.95218555916681\n    - type: nauc_precision_at_5_std\n      value: 7.2040745500708745\n    - type: nauc_recall_at_1000_diff1\n      value: 28.021198234033395\n    - type: nauc_recall_at_1000_max\n      value: 36.165148684597504\n    - type: nauc_recall_at_1000_std\n      value: 28.28852356008973\n    - type: nauc_recall_at_100_diff1\n      value: 21.882447802741897\n    - type: nauc_recall_at_100_max\n      value: 26.979684607567222\n    - type: nauc_recall_at_100_std\n      value: 9.783658817010082\n    - type: nauc_recall_at_10_diff1\n      value: 28.493097951178818\n    - type: nauc_recall_at_10_max\n      value: 29.40937476550134\n    - type: nauc_recall_at_10_std\n      value: 2.7593763576979353\n    - type: nauc_recall_at_1_diff1\n      value: 46.288767289528344\n    - type: nauc_recall_at_1_max\n      value: 25.67706785013364\n    - type: nauc_recall_at_1_std\n      value: -6.989769609924645\n    - type: nauc_recall_at_20_diff1\n      value: 27.638381299425234\n    - type: nauc_recall_at_20_max\n      value: 27.942035836106328\n    - type: nauc_recall_at_20_std\n      value: 3.489835161380808\n    - type: nauc_recall_at_3_diff1\n      value: 33.90054781392646\n    - type: nauc_recall_at_3_max\n      value: 27.778812533030322\n    - type: nauc_recall_at_3_std\n      value: -0.03054068020022706\n    - type: nauc_recall_at_5_diff1\n      value: 30.279060732221346\n    - type: nauc_recall_at_5_max\n      value: 27.49854749597931\n    - type: nauc_recall_at_5_std\n      value: 0.5434664581939099\n    - type: ndcg_at_1\n      value: 37.809\n    - type: ndcg_at_10\n      value: 38.828\n    - type: ndcg_at_100\n      value: 45.218\n    - type: ndcg_at_1000\n      value: 48.510999999999996\n    - type: ndcg_at_20\n      value: 41.11\n    - type: ndcg_at_3\n      value: 34.466\n    - type: ndcg_at_5\n      value: 35.843\n    - type: precision_at_1\n      value: 37.809\n    - type: precision_at_10\n      value: 11.157\n    - type: precision_at_100\n      value: 1.762\n    - type: precision_at_1000\n      value: 0.233\n    - type: precision_at_20\n      value: 6.497\n    - type: precision_at_3\n      value: 23.044999999999998\n    - type: precision_at_5\n      value: 17.284\n    - type: recall_at_1\n      value: 19.126\n    - type: recall_at_10\n      value: 46.062\n    - type: recall_at_100\n      value: 70.22800000000001\n    - type: recall_at_1000\n      value: 89.803\n    - type: recall_at_20\n      value: 53.217999999999996\n    - type: recall_at_3\n      value: 30.847\n    - type: recall_at_5\n      value: 37.11\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB HotpotQA-PL (default)\n      revision: a0bd479ac97b4ccb5bd6ce320c415d0bb4beb907\n      split: test\n      type: clarin-knext/hotpotqa-pl\n    metrics:\n    - type: main_score\n      value: 60.27\n    - type: map_at_1\n      value: 35.199000000000005\n    - type: map_at_10\n      value: 51.369\n    - type: map_at_100\n      value: 52.212\n    - type: map_at_1000\n      value: 52.28\n    - type: map_at_20\n      value: 51.864\n    - type: map_at_3\n      value: 48.446\n    - type: map_at_5\n      value: 50.302\n    - type: mrr_at_1\n      value: 70.39837947332883\n    - type: mrr_at_10\n      value: 76.8346141067273\n    - type: mrr_at_100\n      value: 77.10724392048137\n    - type: mrr_at_1000\n      value: 77.12037412892865\n    - type: mrr_at_20\n      value: 77.01061532947222\n    - type: mrr_at_3\n      value: 75.5908170155299\n    - type: mrr_at_5\n      value: 76.39095205941899\n    - type: nauc_map_at_1000_diff1\n      value: 24.701387884989117\n    - type: nauc_map_at_1000_max\n      value: 23.25553235642178\n    - type: nauc_map_at_1000_std\n      value: 7.1803506915661774\n    - type: nauc_map_at_100_diff1\n      value: 24.674498622483103\n    - type: nauc_map_at_100_max\n      value: 23.234948525052175\n    - type: nauc_map_at_100_std\n      value: 7.168677997105447\n    - type: nauc_map_at_10_diff1\n      value: 24.676025039755626\n    - type: nauc_map_at_10_max\n      value: 23.171971872726964\n    - type: nauc_map_at_10_std\n      value: 6.485610909852058\n    - type: nauc_map_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_map_at_1_max\n      value: 46.05537868917558\n    - type: nauc_map_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_map_at_20_diff1\n      value: 24.69297151842494\n    - type: nauc_map_at_20_max\n      value: 23.213064691673637\n    - type: nauc_map_at_20_std\n      value: 6.9357946556849\n    - type: nauc_map_at_3_diff1\n      value: 26.279128947950507\n    - type: nauc_map_at_3_max\n      value: 23.929537354117922\n    - type: nauc_map_at_3_std\n      value: 4.625061565714759\n    - type: nauc_map_at_5_diff1\n      value: 25.04448959482816\n    - type: nauc_map_at_5_max\n      value: 23.432012857899338\n    - type: nauc_map_at_5_std\n      value: 5.845744681998008\n    - type: nauc_mrr_at_1000_diff1\n      value: 66.7503918108276\n    - type: nauc_mrr_at_1000_max\n      value: 48.42897342336844\n    - type: nauc_mrr_at_1000_std\n      value: 5.3097517971144415\n    - type: nauc_mrr_at_100_diff1\n      value: 66.74645215862695\n    - type: nauc_mrr_at_100_max\n      value: 48.4368663009989\n    - type: nauc_mrr_at_100_std\n      value: 5.322297898555188\n    - type: nauc_mrr_at_10_diff1\n      value: 66.69310166180729\n    - type: nauc_mrr_at_10_max\n      value: 48.475437698330225\n    - type: nauc_mrr_at_10_std\n      value: 5.258183461631702\n    - type: nauc_mrr_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_mrr_at_1_max\n      value: 46.05537868917558\n    - type: nauc_mrr_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_mrr_at_20_diff1\n      value: 66.72000262431975\n    - type: nauc_mrr_at_20_max\n      value: 48.45593642981319\n    - type: nauc_mrr_at_20_std\n      value: 5.353665929072101\n    - type: nauc_mrr_at_3_diff1\n      value: 66.84936676396276\n    - type: nauc_mrr_at_3_max\n      value: 48.466611276778295\n    - type: nauc_mrr_at_3_std\n      value: 4.485810398557475\n    - type: nauc_mrr_at_5_diff1\n      value: 66.62362565394174\n    - type: nauc_mrr_at_5_max\n      value: 48.456431835482014\n    - type: nauc_mrr_at_5_std\n      value: 5.08482458391903\n    - type: nauc_ndcg_at_1000_diff1\n      value: 29.984825173719443\n    - type: nauc_ndcg_at_1000_max\n      value: 27.289179238639893\n    - type: nauc_ndcg_at_1000_std\n      value: 10.661480455527526\n    - type: nauc_ndcg_at_100_diff1\n      value: 29.322074257047877\n    - type: nauc_ndcg_at_100_max\n      value: 26.850650276220605\n    - type: nauc_ndcg_at_100_std\n      value: 10.599247982501902\n    - type: nauc_ndcg_at_10_diff1\n      value: 29.659909113886094\n    - type: nauc_ndcg_at_10_max\n      value: 26.836139599331005\n    - type: nauc_ndcg_at_10_std\n      value: 8.12844399452719\n    - type: nauc_ndcg_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_ndcg_at_1_max\n      value: 46.05537868917558\n    - type: nauc_ndcg_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_ndcg_at_20_diff1\n      value: 29.510802214854294\n    - type: nauc_ndcg_at_20_max\n      value: 26.775562637730722\n    - type: nauc_ndcg_at_20_std\n      value: 9.341342661702363\n    - type: nauc_ndcg_at_3_diff1\n      value: 32.741885846292966\n    - type: nauc_ndcg_at_3_max\n      value: 28.44225108761343\n    - type: nauc_ndcg_at_3_std\n      value: 5.204440768465042\n    - type: nauc_ndcg_at_5_diff1\n      value: 30.57856348635919\n    - type: nauc_ndcg_at_5_max\n      value: 27.475007474301698\n    - type: nauc_ndcg_at_5_std\n      value: 6.961546044312487\n    - type: nauc_precision_at_1000_diff1\n      value: 0.002113156309413332\n    - type: nauc_precision_at_1000_max\n      value: 11.198242419541286\n    - type: nauc_precision_at_1000_std\n      value: 28.69676419166541\n    - type: nauc_precision_at_100_diff1\n      value: 3.6049575557782627\n    - type: nauc_precision_at_100_max\n      value: 12.499173524574791\n    - type: nauc_precision_at_100_std\n      value: 23.3755281004721\n    - type: nauc_precision_at_10_diff1\n      value: 10.922574784853193\n    - type: nauc_precision_at_10_max\n      value: 16.23221529562036\n    - type: nauc_precision_at_10_std\n      value: 12.45014808813857\n    - type: nauc_precision_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_precision_at_1_max\n      value: 46.05537868917558\n    - type: nauc_precision_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_precision_at_20_diff1\n      value: 8.840710781302827\n    - type: nauc_precision_at_20_max\n      value: 14.804644554205524\n    - type: nauc_precision_at_20_std\n      value: 16.245009770815237\n    - type: nauc_precision_at_3_diff1\n      value: 19.447291487137573\n    - type: nauc_precision_at_3_max\n      value: 21.47123471597057\n    - type: nauc_precision_at_3_std\n      value: 6.441862800128802\n    - type: nauc_precision_at_5_diff1\n      value: 14.078545719721108\n    - type: nauc_precision_at_5_max\n      value: 18.468288046016387\n    - type: nauc_precision_at_5_std\n      value: 9.58650641691393\n    - type: nauc_recall_at_1000_diff1\n      value: 0.0021131563095336584\n    - type: nauc_recall_at_1000_max\n      value: 11.198242419541558\n    - type: nauc_recall_at_1000_std\n      value: 28.6967641916655\n    - type: nauc_recall_at_100_diff1\n      value: 3.6049575557781393\n    - type: nauc_recall_at_100_max\n      value: 12.499173524574765\n    - type: nauc_recall_at_100_std\n      value: 23.375528100472074\n    - type: nauc_recall_at_10_diff1\n      value: 10.922574784853168\n    - type: nauc_recall_at_10_max\n      value: 16.2322152956203\n    - type: nauc_recall_at_10_std\n      value: 12.450148088138535\n    - type: nauc_recall_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_recall_at_1_max\n      value: 46.05537868917558\n    - type: nauc_recall_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_recall_at_20_diff1\n      value: 8.840710781302905\n    - type: nauc_recall_at_20_max\n      value: 14.804644554205515\n    - type: nauc_recall_at_20_std\n      value: 16.245009770815273\n    - type: nauc_recall_at_3_diff1\n      value: 19.447291487137498\n    - type: nauc_recall_at_3_max\n      value: 21.47123471597054\n    - type: nauc_recall_at_3_std\n      value: 6.441862800128763\n    - type: nauc_recall_at_5_diff1\n      value: 14.07854571972115\n    - type: nauc_recall_at_5_max\n      value: 18.468288046016337\n    - type: nauc_recall_at_5_std\n      value: 9.586506416913904\n    - type: ndcg_at_1\n      value: 70.39800000000001\n    - type: ndcg_at_10\n      value: 60.27\n    - type: ndcg_at_100\n      value: 63.400999999999996\n    - type: ndcg_at_1000\n      value: 64.847\n    - type: ndcg_at_20\n      value: 61.571\n    - type: ndcg_at_3\n      value: 55.875\n    - type: ndcg_at_5\n      value: 58.36599999999999\n    - type: precision_at_1\n      value: 70.39800000000001\n    - type: precision_at_10\n      value: 12.46\n    - type: precision_at_100\n      value: 1.493\n    - type: precision_at_1000\n      value: 0.169\n    - type: precision_at_20\n      value: 6.65\n    - type: precision_at_3\n      value: 35.062\n    - type: precision_at_5\n      value: 23.009\n    - type: recall_at_1\n      value: 35.199000000000005\n    - type: recall_at_10\n      value: 62.302\n    - type: recall_at_100\n      value: 74.666\n    - type: recall_at_1000\n      value: 84.355\n    - type: recall_at_20\n      value: 66.496\n    - type: recall_at_3\n      value: 52.593\n    - type: recall_at_5\n      value: 57.522\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB MSMARCO-PL (default)\n      revision: 8634c07806d5cce3a6138e260e59b81760a0a640\n      split: test\n      type: clarin-knext/msmarco-pl\n    metrics:\n    - type: main_score\n      value: 64.886\n    - type: map_at_1\n      value: 1.644\n    - type: map_at_10\n      value: 12.24\n    - type: map_at_100\n      value: 28.248\n    - type: map_at_1000\n      value: 33.506\n    - type: map_at_20\n      value: 17.497\n    - type: map_at_3\n      value: 4.9399999999999995\n    - type: map_at_5\n      value: 8.272\n    - type: mrr_at_1\n      value: 83.72093023255815\n    - type: mrr_at_10\n      value: 91.08527131782945\n    - type: mrr_at_100\n      value: 91.08527131782945\n    - type: mrr_at_1000\n      value: 91.08527131782945\n    - type: mrr_at_20\n      value: 91.08527131782945\n    - type: mrr_at_3\n      value: 91.08527131782945\n    - type: mrr_at_5\n      value: 91.08527131782945\n    - type: nauc_map_at_1000_diff1\n      value: -36.428271627303424\n    - type: nauc_map_at_1000_max\n      value: 44.87615127218638\n    - type: nauc_map_at_1000_std\n      value: 67.92696808824724\n    - type: nauc_map_at_100_diff1\n      value: -28.11674206786188\n    - type: nauc_map_at_100_max\n      value: 36.422779766334955\n    - type: nauc_map_at_100_std\n      value: 49.99876313755116\n    - type: nauc_map_at_10_diff1\n      value: -5.838593619806058\n    - type: nauc_map_at_10_max\n      value: 11.026519190509742\n    - type: nauc_map_at_10_std\n      value: 2.5268752263522045\n    - type: nauc_map_at_1_diff1\n      value: 17.897907271073016\n    - type: nauc_map_at_1_max\n      value: 12.229062762540844\n    - type: nauc_map_at_1_std\n      value: -4.088830895573149\n    - type: nauc_map_at_20_diff1\n      value: -13.871097716255626\n    - type: nauc_map_at_20_max\n      value: 19.291271635609533\n    - type: nauc_map_at_20_std\n      value: 16.745335606507826\n    - type: nauc_map_at_3_diff1\n      value: 4.425238457033843\n    - type: nauc_map_at_3_max\n      value: 4.611864744680824\n    - type: nauc_map_at_3_std\n      value: -8.986916608582863\n    - type: nauc_map_at_5_diff1\n      value: -6.254849256920095\n    - type: nauc_map_at_5_max\n      value: 2.729437079919823\n    - type: nauc_map_at_5_std\n      value: -7.235906279913092\n    - type: nauc_mrr_at_1000_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_1000_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_1000_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_100_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_100_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_100_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_10_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_10_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_10_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_1_diff1\n      value: 56.55126663944154\n    - type: nauc_mrr_at_1_max\n      value: 66.37014285522565\n    - type: nauc_mrr_at_1_std\n      value: 53.2508271389779\n    - type: nauc_mrr_at_20_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_20_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_20_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_3_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_3_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_3_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_5_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_5_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_5_std\n      value: 56.345086428353575\n    - type: nauc_ndcg_at_1000_diff1\n      value: -19.06422926483731\n    - type: nauc_ndcg_at_1000_max\n      value: 56.30853514590265\n    - type: nauc_ndcg_at_1000_std\n      value: 70.30810947505557\n    - type: nauc_ndcg_at_100_diff1\n      value: -25.72587586459692\n    - type: nauc_ndcg_at_100_max\n      value: 51.433781241604194\n    - type: nauc_ndcg_at_100_std\n      value: 68.37678512652792\n    - type: nauc_ndcg_at_10_diff1\n      value: -23.21198108212602\n    - type: nauc_ndcg_at_10_max\n      value: 43.5450720846516\n    - type: nauc_ndcg_at_10_std\n      value: 48.78307907005605\n    - type: nauc_ndcg_at_1_diff1\n      value: 44.00179301267447\n    - type: nauc_ndcg_at_1_max\n      value: 48.202370455680395\n    - type: nauc_ndcg_at_1_std\n      value: 25.69655992704088\n    - type: nauc_ndcg_at_20_diff1\n      value: -33.88168753446507\n    - type: nauc_ndcg_at_20_max\n      value: 45.16199742613164\n    - type: nauc_ndcg_at_20_std\n      value: 61.87098383164902\n    - type: nauc_ndcg_at_3_diff1\n      value: 11.19174449544048\n    - type: nauc_ndcg_at_3_max\n      value: 44.34069860560555\n    - type: nauc_ndcg_at_3_std\n      value: 27.451258369798115\n    - type: nauc_ndcg_at_5_diff1\n      value: -7.186520929432436\n    - type: nauc_ndcg_at_5_max\n      value: 43.41869981139378\n    - type: nauc_ndcg_at_5_std\n      value: 34.89898115995178\n    - type: nauc_precision_at_1000_diff1\n      value: -34.43998154563451\n    - type: nauc_precision_at_1000_max\n      value: 29.172655907480372\n    - type: nauc_precision_at_1000_std\n      value: 65.15824469614837\n    - type: nauc_precision_at_100_diff1\n      value: -37.82409643259692\n    - type: nauc_precision_at_100_max\n      value: 38.24986991317909\n    - type: nauc_precision_at_100_std\n      value: 72.74768183105327\n    - type: nauc_precision_at_10_diff1\n      value: -32.21556182780535\n    - type: nauc_precision_at_10_max\n      value: 34.27170432382651\n    - type: nauc_precision_at_10_std\n      value: 58.358255004394664\n    - type: nauc_precision_at_1_diff1\n      value: 56.55126663944154\n    - type: nauc_precision_at_1_max\n      value: 66.37014285522565\n    - type: nauc_precision_at_1_std\n      value: 53.2508271389779\n    - type: nauc_precision_at_20_diff1\n      value: -40.18751579026395\n    - type: nauc_precision_at_20_max\n      value: 33.960783153758896\n    - type: nauc_precision_at_20_std\n      value: 65.42918390184195\n    - type: nauc_precision_at_3_diff1\n      value: -7.073870209006578\n    - type: nauc_precision_at_3_max\n      value: 50.81535269862325\n    - type: nauc_precision_at_3_std\n      value: 59.248681565955685\n    - type: nauc_precision_at_5_diff1\n      value: -31.136580596983876\n    - type: nauc_precision_at_5_max\n      value: 45.88147792380426\n    - type: nauc_precision_at_5_std\n      value: 67.46814230928243\n    - type: nauc_recall_at_1000_diff1\n      value: -23.15699999594577\n    - type: nauc_recall_at_1000_max\n      value: 39.77277799761876\n    - type: nauc_recall_at_1000_std\n      value: 60.326168012901114\n    - type: nauc_recall_at_100_diff1\n      value: -21.636664823598498\n    - type: nauc_recall_at_100_max\n      value: 31.104969346131583\n    - type: nauc_recall_at_100_std\n      value: 38.811686891592096\n    - type: nauc_recall_at_10_diff1\n      value: -10.542765625053569\n    - type: nauc_recall_at_10_max\n      value: 2.043876058107446\n    - type: nauc_recall_at_10_std\n      value: -5.578449908984766\n    - type: nauc_recall_at_1_diff1\n      value: 17.897907271073016\n    - type: nauc_recall_at_1_max\n      value: 12.229062762540844\n    - type: nauc_recall_at_1_std\n      value: -4.088830895573149\n    - type: nauc_recall_at_20_diff1\n      value: -15.132909355710103\n    - type: nauc_recall_at_20_max\n      value: 12.659765287241065\n    - type: nauc_recall_at_20_std\n      value: 8.277887800815819\n    - type: nauc_recall_at_3_diff1\n      value: -3.1975017812715016\n    - type: nauc_recall_at_3_max\n      value: -3.5539857085038538\n    - type: nauc_recall_at_3_std\n      value: -14.712102851318118\n    - type: nauc_recall_at_5_diff1\n      value: -14.040507717380743\n    - type: nauc_recall_at_5_max\n      value: -6.126912150131701\n    - type: nauc_recall_at_5_std\n      value: -13.821624015640355\n    - type: ndcg_at_1\n      value: 71.318\n    - type: ndcg_at_10\n      value: 64.886\n    - type: ndcg_at_100\n      value: 53.187\n    - type: ndcg_at_1000\n      value: 59.897999999999996\n    - type: ndcg_at_20\n      value: 58.96\n    - type: ndcg_at_3\n      value: 69.736\n    - type: ndcg_at_5\n      value: 70.14099999999999\n    - type: precision_at_1\n      value: 83.721\n    - type: precision_at_10\n      value: 71.163\n    - type: precision_at_100\n      value: 29.465000000000003\n    - type: precision_at_1000\n      value: 5.665\n    - type: precision_at_20\n      value: 57.791000000000004\n    - type: precision_at_3\n      value: 82.171\n    - type: precision_at_5\n      value: 81.86\n    - type: recall_at_1\n      value: 1.644\n    - type: recall_at_10\n      value: 14.238000000000001\n    - type: recall_at_100\n      value: 39.831\n    - type: recall_at_1000\n      value: 64.057\n    - type: recall_at_20\n      value: 21.021\n    - type: recall_at_3\n      value: 5.53\n    - type: recall_at_5\n      value: 9.623\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB NFCorpus-PL (default)\n      revision: 9a6f9567fda928260afed2de480d79c98bf0bec0\n      split: test\n      type: clarin-knext/nfcorpus-pl\n    metrics:\n    - type: main_score\n      value: 31.391000000000002\n    - type: map_at_1\n      value: 4.163\n    - type: map_at_10\n      value: 10.744\n    - type: map_at_100\n      value: 14.038999999999998\n    - type: map_at_1000\n      value: 15.434999999999999\n    - type: map_at_20\n      value: 12.16\n    - type: map_at_3\n      value: 7.614999999999999\n    - type: map_at_5\n      value: 9.027000000000001\n    - type: mrr_at_1\n      value: 39.0092879256966\n    - type: mrr_at_10\n      value: 48.69809327239668\n    - type: mrr_at_100\n      value: 49.20788148442068\n    - type: mrr_at_1000\n      value: 49.25509336494706\n    - type: mrr_at_20\n      value: 48.99606551850896\n    - type: mrr_at_3\n      value: 46.284829721362236\n    - type: mrr_at_5\n      value: 47.77089783281735\n    - type: nauc_map_at_1000_diff1\n      value: 22.75421477116417\n    - type: nauc_map_at_1000_max\n      value: 49.242283787799046\n    - type: nauc_map_at_1000_std\n      value: 29.056888272331832\n    - type: nauc_map_at_100_diff1\n      value: 23.585977398585594\n    - type: nauc_map_at_100_max\n      value: 48.25845199409498\n    - type: nauc_map_at_100_std\n      value: 24.944264511223693\n    - type: nauc_map_at_10_diff1\n      value: 27.386613094780255\n    - type: nauc_map_at_10_max\n      value: 41.52415346691586\n    - type: nauc_map_at_10_std\n      value: 12.93872448563755\n    - type: nauc_map_at_1_diff1\n      value: 46.78688143865053\n    - type: nauc_map_at_1_max\n      value: 37.20408843995871\n    - type: nauc_map_at_1_std\n      value: 4.383444959401098\n    - type: nauc_map_at_20_diff1\n      value: 25.590969047740288\n    - type: nauc_map_at_20_max\n      value: 44.57109307999418\n    - type: nauc_map_at_20_std\n      value: 16.45855141821407\n    - type: nauc_map_at_3_diff1\n      value: 36.30017108362863\n    - type: nauc_map_at_3_max\n      value: 34.66149613991648\n    - type: nauc_map_at_3_std\n      value: 5.67985905078467\n    - type: nauc_map_at_5_diff1\n      value: 31.157644795417223\n    - type: nauc_map_at_5_max\n      value: 37.274738661636825\n    - type: nauc_map_at_5_std\n      value: 8.70088872394168\n    - type: nauc_mrr_at_1000_diff1\n      value: 25.638564218157384\n    - type: nauc_mrr_at_1000_max\n      value: 57.77788270285353\n    - type: nauc_mrr_at_1000_std\n      value: 43.507586592911274\n    - type: nauc_mrr_at_100_diff1\n      value: 25.662002580561584\n    - type: nauc_mrr_at_100_max\n      value: 57.80578394278584\n    - type: nauc_mrr_at_100_std\n      value: 43.543905743986635\n    - type: nauc_mrr_at_10_diff1\n      value: 25.426034796339835\n    - type: nauc_mrr_at_10_max\n      value: 57.68443186258669\n    - type: nauc_mrr_at_10_std\n      value: 43.438009108331215\n    - type: nauc_mrr_at_1_diff1\n      value: 26.073028156311075\n    - type: nauc_mrr_at_1_max\n      value: 52.11817916720053\n    - type: nauc_mrr_at_1_std\n      value: 37.41073893153695\n    - type: nauc_mrr_at_20_diff1\n      value: 25.548645553336147\n    - type: nauc_mrr_at_20_max\n      value: 57.78552760401915\n    - type: nauc_mrr_at_20_std\n      value: 43.521687428822325\n    - type: nauc_mrr_at_3_diff1\n      value: 25.72662577397805\n    - type: nauc_mrr_at_3_max\n      value: 56.891263536265605\n    - type: nauc_mrr_at_3_std\n      value: 41.384872305390104\n    - type: nauc_mrr_at_5_diff1\n      value: 25.552211551655386\n    - type: nauc_mrr_at_5_max\n      value: 57.976813828353926\n    - type: nauc_mrr_at_5_std\n      value: 43.504564461855544\n    - type: nauc_ndcg_at_1000_diff1\n      value: 23.456158044182757\n    - type: nauc_ndcg_at_1000_max\n      value: 60.05411773552709\n    - type: nauc_ndcg_at_1000_std\n      value: 47.857510017262584\n    - type: nauc_ndcg_at_100_diff1\n      value: 19.711635700390772\n    - type: nauc_ndcg_at_100_max\n      value: 56.178746740470665\n    - type: nauc_ndcg_at_100_std\n      value: 42.36829180286942\n    - type: nauc_ndcg_at_10_diff1\n      value: 18.364428967788413\n    - type: nauc_ndcg_at_10_max\n      value: 54.38372506578223\n    - type: nauc_ndcg_at_10_std\n      value: 41.75765411340369\n    - type: nauc_ndcg_at_1_diff1\n      value: 26.571093272640773\n    - type: nauc_ndcg_at_1_max\n      value: 51.061788341958284\n    - type: nauc_ndcg_at_1_std\n      value: 36.514987974075986\n    - type: nauc_ndcg_at_20_diff1\n      value: 18.345487193027697\n    - type: nauc_ndcg_at_20_max\n      value: 54.62621882656994\n    - type: nauc_ndcg_at_20_std\n      value: 41.42835554714241\n    - type: nauc_ndcg_at_3_diff1\n      value: 23.260105658139025\n    - type: nauc_ndcg_at_3_max\n      value: 52.07747385334546\n    - type: nauc_ndcg_at_3_std\n      value: 36.91985577837284\n    - type: nauc_ndcg_at_5_diff1\n      value: 20.40428109665566\n    - type: nauc_ndcg_at_5_max\n      value: 53.52015347884604\n    - type: nauc_ndcg_at_5_std\n      value: 39.46008849580017\n    - type: nauc_precision_at_1000_diff1\n      value: -7.3487344916380035\n    - type: nauc_precision_at_1000_max\n      value: 16.58045221394852\n    - type: nauc_precision_at_1000_std\n      value: 38.94030932397075\n    - type: nauc_precision_at_100_diff1\n      value: -5.257743986683922\n    - type: nauc_precision_at_100_max\n      value: 34.43071687475306\n    - type: nauc_precision_at_100_std\n      value: 53.499519170670474\n    - type: nauc_precision_at_10_diff1\n      value: 2.385136433119139\n    - type: nauc_precision_at_10_max\n      value: 47.210743878631064\n    - type: nauc_precision_at_10_std\n      value: 47.22767704186548\n    - type: nauc_precision_at_1_diff1\n      value: 26.073028156311075\n    - type: nauc_precision_at_1_max\n      value: 52.11817916720053\n    - type: nauc_precision_at_1_std\n      value: 37.41073893153695\n    - type: nauc_precision_at_20_diff1\n      value: -0.3531531127238474\n    - type: nauc_precision_at_20_max\n      value: 44.78044604856974\n    - type: nauc_precision_at_20_std\n      value: 49.532804150743615\n    - type: nauc_precision_at_3_diff1\n      value: 15.350050569991447\n    - type: nauc_precision_at_3_max\n      value: 51.01572315596549\n    - type: nauc_precision_at_3_std\n      value: 38.801125728413155\n    - type: nauc_precision_at_5_diff1\n      value: 9.109003666144694\n    - type: nauc_precision_at_5_max\n      value: 50.935269774898494\n    - type: nauc_precision_at_5_std\n      value: 43.323548180559676\n    - type: nauc_recall_at_1000_diff1\n      value: 16.64743647648886\n    - type: nauc_recall_at_1000_max\n      value: 38.46012283772285\n    - type: nauc_recall_at_1000_std\n      value: 36.02016164796441\n    - type: nauc_recall_at_100_diff1\n      value: 14.005834785186744\n    - type: nauc_recall_at_100_max\n      value: 37.70026105513647\n    - type: nauc_recall_at_100_std\n      value: 27.085222642129697\n    - type: nauc_recall_at_10_diff1\n      value: 21.204106627422632\n    - type: nauc_recall_at_10_max\n      value: 36.737624881893424\n    - type: nauc_recall_at_10_std\n      value: 13.755054514272702\n    - type: nauc_recall_at_1_diff1\n      value: 46.78688143865053\n    - type: nauc_recall_at_1_max\n      value: 37.20408843995871\n    - type: nauc_recall_at_1_std\n      value: 4.383444959401098\n    - type: nauc_recall_at_20_diff1\n      value: 19.740977611421933\n    - type: nauc_recall_at_20_max\n      value: 39.21908969539783\n    - type: nauc_recall_at_20_std\n      value: 16.560269670318494\n    - type: nauc_recall_at_3_diff1\n      value: 32.189359545367815\n    - type: nauc_recall_at_3_max\n      value: 31.693634445562758\n    - type: nauc_recall_at_3_std\n      value: 6.246326281543587\n    - type: nauc_recall_at_5_diff1\n      value: 25.51586860499901\n    - type: nauc_recall_at_5_max\n      value: 33.15934725342885\n    - type: nauc_recall_at_5_std\n      value: 9.677778511696705\n    - type: ndcg_at_1\n      value: 37.307\n    - type: ndcg_at_10\n      value: 31.391000000000002\n    - type: ndcg_at_100\n      value: 28.877999999999997\n    - type: ndcg_at_1000\n      value: 37.16\n    - type: ndcg_at_20\n      value: 29.314\n    - type: ndcg_at_3\n      value: 35.405\n    - type: ndcg_at_5\n      value: 33.922999999999995\n    - type: precision_at_1\n      value: 39.009\n    - type: precision_at_10\n      value: 24.52\n    - type: precision_at_100\n      value: 7.703\n    - type: precision_at_1000\n      value: 2.04\n    - type: precision_at_20\n      value: 18.08\n    - type: precision_at_3\n      value: 34.469\n    - type: precision_at_5\n      value: 30.712\n    - type: recall_at_1\n      value: 4.163\n    - type: recall_at_10\n      value: 15.015999999999998\n    - type: recall_at_100\n      value: 30.606\n    - type: recall_at_1000\n      value: 59.606\n    - type: recall_at_20\n      value: 19.09\n    - type: recall_at_3\n      value: 9.139\n    - type: recall_at_5\n      value: 11.477\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB NQ-PL (default)\n      revision: f171245712cf85dd4700b06bef18001578d0ca8d\n      split: test\n      type: clarin-knext/nq-pl\n    metrics:\n    - type: main_score\n      value: 54.017\n    - type: map_at_1\n      value: 34.193\n    - type: map_at_10\n      value: 47.497\n    - type: map_at_100\n      value: 48.441\n    - type: map_at_1000\n      value: 48.481\n    - type: map_at_20\n      value: 48.093\n    - type: map_at_3\n      value: 44.017\n    - type: map_at_5\n      value: 46.111000000000004\n    - type: mrr_at_1\n      value: 37.949015063731174\n    - type: mrr_at_10\n      value: 49.915772315105954\n    - type: mrr_at_100\n      value: 50.62841255829997\n    - type: mrr_at_1000\n      value: 50.656773027666745\n    - type: mrr_at_20\n      value: 50.37785276657083\n    - type: mrr_at_3\n      value: 46.98725376593267\n    - type: mrr_at_5\n      value: 48.763035921205066\n    - type: nauc_map_at_1000_diff1\n      value: 39.5632191792873\n    - type: nauc_map_at_1000_max\n      value: 37.4728247053629\n    - type: nauc_map_at_1000_std\n      value: 5.742498414663762\n    - type: nauc_map_at_100_diff1\n      value: 39.555570352061906\n    - type: nauc_map_at_100_max\n      value: 37.497880976847334\n    - type: nauc_map_at_100_std\n      value: 5.7798021019465375\n    - type: nauc_map_at_10_diff1\n      value: 39.5423723444454\n    - type: nauc_map_at_10_max\n      value: 37.41661971723365\n    - type: nauc_map_at_10_std\n      value: 5.2378002164144695\n    - type: nauc_map_at_1_diff1\n      value: 41.52697034146981\n    - type: nauc_map_at_1_max\n      value: 28.558995576942863\n    - type: nauc_map_at_1_std\n      value: 0.13094542859192052\n    - type: nauc_map_at_20_diff1\n      value: 39.55484628943701\n    - type: nauc_map_at_20_max\n      value: 37.5247794933719\n    - type: nauc_map_at_20_std\n      value: 5.702881342279231\n    - type: nauc_map_at_3_diff1\n      value: 39.949323925425325\n    - type: nauc_map_at_3_max\n      value: 35.770298168901924\n    - type: nauc_map_at_3_std\n      value: 2.9127112432479874\n    - type: nauc_map_at_5_diff1\n      value: 39.768310617004545\n    - type: nauc_map_at_5_max\n      value: 37.1549191664796\n    - type: nauc_map_at_5_std\n      value: 4.4681285748269515\n    - type: nauc_mrr_at_1000_diff1\n      value: 39.14001746706457\n    - type: nauc_mrr_at_1000_max\n      value: 37.477376518267775\n    - type: nauc_mrr_at_1000_std\n      value: 6.8088891531621565\n    - type: nauc_mrr_at_100_diff1\n      value: 39.13054707413684\n    - type: nauc_mrr_at_100_max\n      value: 37.498126443766274\n    - type: nauc_mrr_at_100_std\n      value: 6.839411380129971\n    - type: nauc_mrr_at_10_diff1\n      value: 39.09764730048156\n    - type: nauc_mrr_at_10_max\n      value: 37.58593798217306\n    - type: nauc_mrr_at_10_std\n      value: 6.713795164982413\n    - type: nauc_mrr_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_mrr_at_1_max\n      value: 31.500589231378722\n    - type: nauc_mrr_at_1_std\n      value: 2.059116370339438\n    - type: nauc_mrr_at_20_diff1\n      value: 39.09011023988447\n    - type: nauc_mrr_at_20_max\n      value: 37.55856008791344\n    - type: nauc_mrr_at_20_std\n      value: 6.847165397615844\n    - type: nauc_mrr_at_3_diff1\n      value: 39.382542043738\n    - type: nauc_mrr_at_3_max\n      value: 36.49265363659468\n    - type: nauc_mrr_at_3_std\n      value: 4.759157976438336\n    - type: nauc_mrr_at_5_diff1\n      value: 39.304826333759976\n    - type: nauc_mrr_at_5_max\n      value: 37.46326016736024\n    - type: nauc_mrr_at_5_std\n      value: 6.122608305766621\n    - type: nauc_ndcg_at_1000_diff1\n      value: 38.568500038453266\n    - type: nauc_ndcg_at_1000_max\n      value: 39.799710882413166\n    - type: nauc_ndcg_at_1000_std\n      value: 9.357010223096639\n    - type: nauc_ndcg_at_100_diff1\n      value: 38.38026091343228\n    - type: nauc_ndcg_at_100_max\n      value: 40.48398173542486\n    - type: nauc_ndcg_at_100_std\n      value: 10.373054013302214\n    - type: nauc_ndcg_at_10_diff1\n      value: 38.27340980909964\n    - type: nauc_ndcg_at_10_max\n      value: 40.35241649744093\n    - type: nauc_ndcg_at_10_std\n      value: 8.579139930345168\n    - type: nauc_ndcg_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_ndcg_at_1_max\n      value: 31.500589231378722\n    - type: nauc_ndcg_at_1_std\n      value: 2.059116370339438\n    - type: nauc_ndcg_at_20_diff1\n      value: 38.26453028884807\n    - type: nauc_ndcg_at_20_max\n      value: 40.70517858426641\n    - type: nauc_ndcg_at_20_std\n      value: 9.987693876137905\n    - type: nauc_ndcg_at_3_diff1\n      value: 39.2078971733273\n    - type: nauc_ndcg_at_3_max\n      value: 37.48672195565316\n    - type: nauc_ndcg_at_3_std\n      value: 4.051464994659221\n    - type: nauc_ndcg_at_5_diff1\n      value: 38.883693595665285\n    - type: nauc_ndcg_at_5_max\n      value: 39.763115634437135\n    - type: nauc_ndcg_at_5_std\n      value: 6.738980451582073\n    - type: nauc_precision_at_1000_diff1\n      value: -7.223215910619012\n    - type: nauc_precision_at_1000_max\n      value: 13.075844604892161\n    - type: nauc_precision_at_1000_std\n      value: 19.864336920890107\n    - type: nauc_precision_at_100_diff1\n      value: 1.3305994810812418\n    - type: nauc_precision_at_100_max\n      value: 25.9219108557104\n    - type: nauc_precision_at_100_std\n      value: 27.5076605928207\n    - type: nauc_precision_at_10_diff1\n      value: 18.441551484970326\n    - type: nauc_precision_at_10_max\n      value: 39.85995330437054\n    - type: nauc_precision_at_10_std\n      value: 20.561269077428914\n    - type: nauc_precision_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_precision_at_1_max\n      value: 31.500589231378722\n    - type: nauc_precision_at_1_std\n      value: 2.059116370339438\n    - type: nauc_precision_at_20_diff1\n      value: 12.579593891480531\n    - type: nauc_precision_at_20_max\n      value: 36.620221830588775\n    - type: nauc_precision_at_20_std\n      value: 26.40364876775059\n    - type: nauc_precision_at_3_diff1\n      value: 30.158859294487073\n    - type: nauc_precision_at_3_max\n      value: 41.168215766389174\n    - type: nauc_precision_at_3_std\n      value: 9.44345004450809\n    - type: nauc_precision_at_5_diff1\n      value: 25.438624678672785\n    - type: nauc_precision_at_5_max\n      value: 42.72802023518524\n    - type: nauc_precision_at_5_std\n      value: 15.357657388511099\n    - type: nauc_recall_at_1000_diff1\n      value: 24.987564782718003\n    - type: nauc_recall_at_1000_max\n      value: 70.508416373353\n    - type: nauc_recall_at_1000_std\n      value: 69.75092280398808\n    - type: nauc_recall_at_100_diff1\n      value: 29.504202856421397\n    - type: nauc_recall_at_100_max\n      value: 63.41356585545318\n    - type: nauc_recall_at_100_std\n      value: 50.09250954437847\n    - type: nauc_recall_at_10_diff1\n      value: 32.355776022971774\n    - type: nauc_recall_at_10_max\n      value: 49.47121901667283\n    - type: nauc_recall_at_10_std\n      value: 19.418439406631244\n    - type: nauc_recall_at_1_diff1\n      value: 41.52697034146981\n    - type: nauc_recall_at_1_max\n      value: 28.558995576942863\n    - type: nauc_recall_at_1_std\n      value: 0.13094542859192052\n    - type: nauc_recall_at_20_diff1\n      value: 31.57334731023589\n    - type: nauc_recall_at_20_max\n      value: 54.06567225197383\n    - type: nauc_recall_at_20_std\n      value: 29.222029720570468\n    - type: nauc_recall_at_3_diff1\n      value: 36.45033533275773\n    - type: nauc_recall_at_3_max\n      value: 40.39529713780803\n    - type: nauc_recall_at_3_std\n      value: 5.21893897772794\n    - type: nauc_recall_at_5_diff1\n      value: 35.18471678478859\n    - type: nauc_recall_at_5_max\n      value: 46.20100816867823\n    - type: nauc_recall_at_5_std\n      value: 11.94481894633221\n    - type: ndcg_at_1\n      value: 37.949\n    - type: ndcg_at_10\n      value: 54.017\n    - type: ndcg_at_100\n      value: 58.126\n    - type: ndcg_at_1000\n      value: 59.073\n    - type: ndcg_at_20\n      value: 55.928\n    - type: ndcg_at_3\n      value: 47.494\n    - type: ndcg_at_5\n      value: 50.975\n    - type: precision_at_1\n      value: 37.949\n    - type: precision_at_10\n      value: 8.450000000000001\n    - type: precision_at_100\n      value: 1.083\n    - type: precision_at_1000\n      value: 0.117\n    - type: precision_at_20\n      value: 4.689\n    - type: precision_at_3\n      value: 21.051000000000002\n    - type: precision_at_5\n      value: 14.664\n    - type: recall_at_1\n      value: 34.193\n    - type: recall_at_10\n      value: 71.357\n    - type: recall_at_100\n      value: 89.434\n    - type: recall_at_1000\n      value: 96.536\n    - type: recall_at_20\n      value: 78.363\n    - type: recall_at_3\n      value: 54.551\n    - type: recall_at_5\n      value: 62.543000000000006\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB Quora-PL (default)\n      revision: 0be27e93455051e531182b85e85e425aba12e9d4\n      split: test\n      type: clarin-knext/quora-pl\n    metrics:\n    - type: main_score\n      value: 84.114\n    - type: map_at_1\n      value: 65.848\n    - type: map_at_10\n      value: 79.85900000000001\n    - type: map_at_100\n      value: 80.582\n    - type: map_at_1000\n      value: 80.60300000000001\n    - type: map_at_20\n      value: 80.321\n    - type: map_at_3\n      value: 76.741\n    - type: map_at_5\n      value: 78.72200000000001\n    - type: mrr_at_1\n      value: 75.97\n    - type: mrr_at_10\n      value: 83.04630158730119\n    - type: mrr_at_100\n      value: 83.22785731032968\n    - type: mrr_at_1000\n      value: 83.23123717623899\n    - type: mrr_at_20\n      value: 83.17412021320565\n    - type: mrr_at_3\n      value: 81.83333333333287\n    - type: mrr_at_5\n      value: 82.61933333333275\n    - type: nauc_map_at_1000_diff1\n      value: 73.26316553371083\n    - type: nauc_map_at_1000_max\n      value: 27.92567859085245\n    - type: nauc_map_at_1000_std\n      value: -47.477909533360446\n    - type: nauc_map_at_100_diff1\n      value: 73.2690602807223\n    - type: nauc_map_at_100_max\n      value: 27.915868327849996\n    - type: nauc_map_at_100_std\n      value: -47.525777766107595\n    - type: nauc_map_at_10_diff1\n      value: 73.45464428464894\n    - type: nauc_map_at_10_max\n      value: 27.451611487246296\n    - type: nauc_map_at_10_std\n      value: -49.35818715843809\n    - type: nauc_map_at_1_diff1\n      value: 77.29690208952982\n    - type: nauc_map_at_1_max\n      value: 19.839875762282293\n    - type: nauc_map_at_1_std\n      value: -45.355684654708284\n    - type: nauc_map_at_20_diff1\n      value: 73.35102731979796\n    - type: nauc_map_at_20_max\n      value: 27.741506490134583\n    - type: nauc_map_at_20_std\n      value: -48.22006207310331\n    - type: nauc_map_at_3_diff1\n      value: 73.94878241064137\n    - type: nauc_map_at_3_max\n      value: 24.761321386766728\n    - type: nauc_map_at_3_std\n      value: -51.20638883618126\n    - type: nauc_map_at_5_diff1\n      value: 73.66143558047698\n    - type: nauc_map_at_5_max\n      value: 26.53483405013543\n    - type: nauc_map_at_5_std\n      value: -50.697541279640056\n    - type: nauc_mrr_at_1000_diff1\n      value: 73.84632320009759\n    - type: nauc_mrr_at_1000_max\n      value: 30.50182733610048\n    - type: nauc_mrr_at_1000_std\n      value: -44.3021647995251\n    - type: nauc_mrr_at_100_diff1\n      value: 73.84480792662302\n    - type: nauc_mrr_at_100_max\n      value: 30.50749424571614\n    - type: nauc_mrr_at_100_std\n      value: -44.29615086388113\n    - type: nauc_mrr_at_10_diff1\n      value: 73.79442772949346\n    - type: nauc_mrr_at_10_max\n      value: 30.55724252219984\n    - type: nauc_mrr_at_10_std\n      value: -44.50997069462057\n    - type: nauc_mrr_at_1_diff1\n      value: 75.23369827945945\n    - type: nauc_mrr_at_1_max\n      value: 29.20073967447664\n    - type: nauc_mrr_at_1_std\n      value: -43.1920147658285\n    - type: nauc_mrr_at_20_diff1\n      value: 73.82731678072307\n    - type: nauc_mrr_at_20_max\n      value: 30.566328605497667\n    - type: nauc_mrr_at_20_std\n      value: -44.24683607643705\n    - type: nauc_mrr_at_3_diff1\n      value: 73.61997576749954\n    - type: nauc_mrr_at_3_max\n      value: 30.150393853381917\n    - type: nauc_mrr_at_3_std\n      value: -44.96847297506626\n    - type: nauc_mrr_at_5_diff1\n      value: 73.69084310616132\n    - type: nauc_mrr_at_5_max\n      value: 30.578033703441125\n    - type: nauc_mrr_at_5_std\n      value: -44.74920746066566\n    - type: nauc_ndcg_at_1000_diff1\n      value: 72.89349862557452\n    - type: nauc_ndcg_at_1000_max\n      value: 29.824725190462086\n    - type: nauc_ndcg_at_1000_std\n      value: -44.96284395063211\n    - type: nauc_ndcg_at_100_diff1\n      value: 72.85212753715273\n    - type: nauc_ndcg_at_100_max\n      value: 29.933114207845605\n    - type: nauc_ndcg_at_100_std\n      value: -44.944225570663754\n    - type: nauc_ndcg_at_10_diff1\n      value: 72.80576740454528\n    - type: nauc_ndcg_at_10_max\n      value: 29.16829118320828\n    - type: nauc_ndcg_at_10_std\n      value: -48.149473740079614\n    - type: nauc_ndcg_at_1_diff1\n      value: 75.00032534968587\n    - type: nauc_ndcg_at_1_max\n      value: 29.61849062038547\n    - type: nauc_ndcg_at_1_std\n      value: -42.560207043864054\n    - type: nauc_ndcg_at_20_diff1\n      value: 72.88440406302502\n    - type: nauc_ndcg_at_20_max\n      value: 29.65496676092656\n    - type: nauc_ndcg_at_20_std\n      value: -46.21238462167732\n    - type: nauc_ndcg_at_3_diff1\n      value: 72.37916962766987\n    - type: nauc_ndcg_at_3_max\n      value: 27.125094834547586\n    - type: nauc_ndcg_at_3_std\n      value: -48.62942991399391\n    - type: nauc_ndcg_at_5_diff1\n      value: 72.57017330527658\n    - type: nauc_ndcg_at_5_max\n      value: 28.470485561757254\n    - type: nauc_ndcg_at_5_std\n      value: -49.07593345591059\n    - type: nauc_precision_at_1000_diff1\n      value: -41.67915575853946\n    - type: nauc_precision_at_1000_max\n      value: 1.2012264478568844\n    - type: nauc_precision_at_1000_std\n      value: 44.723834559400466\n    - type: nauc_precision_at_100_diff1\n      value: -40.45196679236971\n    - type: nauc_precision_at_100_max\n      value: 2.3525450401714894\n    - type: nauc_precision_at_100_std\n      value: 43.7092529413952\n    - type: nauc_precision_at_10_diff1\n      value: -30.256026923068767\n    - type: nauc_precision_at_10_max\n      value: 8.313422052132559\n    - type: nauc_precision_at_10_std\n      value: 25.929372356449694\n    - type: nauc_precision_at_1_diff1\n      value: 75.00032534968587\n    - type: nauc_precision_at_1_max\n      value: 29.61849062038547\n    - type: nauc_precision_at_1_std\n      value: -42.560207043864054\n    - type: nauc_precision_at_20_diff1\n      value: -35.61971069986584\n    - type: nauc_precision_at_20_max\n      value: 5.4664303079116765\n    - type: nauc_precision_at_20_std\n      value: 34.992352471692826\n    - type: nauc_precision_at_3_diff1\n      value: -5.691231842471157\n    - type: nauc_precision_at_3_max\n      value: 14.797949087742444\n    - type: nauc_precision_at_3_std\n      value: -0.1930317395644928\n    - type: nauc_precision_at_5_diff1\n      value: -20.03913781462645\n    - type: nauc_precision_at_5_max\n      value: 11.956771408712749\n    - type: nauc_precision_at_5_std\n      value: 13.179251389859731\n    - type: nauc_recall_at_1000_diff1\n      value: 64.03509042729674\n    - type: nauc_recall_at_1000_max\n      value: 40.91691485428493\n    - type: nauc_recall_at_1000_std\n      value: 16.12968625875372\n    - type: nauc_recall_at_100_diff1\n      value: 63.83116179628575\n    - type: nauc_recall_at_100_max\n      value: 43.72908117676382\n    - type: nauc_recall_at_100_std\n      value: -20.50966716852155\n    - type: nauc_recall_at_10_diff1\n      value: 66.42071960186394\n    - type: nauc_recall_at_10_max\n      value: 28.983207818687205\n    - type: nauc_recall_at_10_std\n      value: -56.61417798753744\n    - type: nauc_recall_at_1_diff1\n      value: 77.29690208952982\n    - type: nauc_recall_at_1_max\n      value: 19.839875762282293\n    - type: nauc_recall_at_1_std\n      value: -45.355684654708284\n    - type: nauc_recall_at_20_diff1\n      value: 66.32360705219874\n    - type: nauc_recall_at_20_max\n      value: 33.30698111822631\n    - type: nauc_recall_at_20_std\n      value: -43.89233781737452\n    - type: nauc_recall_at_3_diff1\n      value: 69.67029394927077\n    - type: nauc_recall_at_3_max\n      value: 22.67803039327696\n    - type: nauc_recall_at_3_std\n      value: -56.43327209861502\n    - type: nauc_recall_at_5_diff1\n      value: 68.05622143936131\n    - type: nauc_recall_at_5_max\n      value: 26.67795559040675\n    - type: nauc_recall_at_5_std\n      value: -58.158231198510954\n    - type: ndcg_at_1\n      value: 76.08\n    - type: ndcg_at_10\n      value: 84.114\n    - type: ndcg_at_100\n      value: 85.784\n    - type: ndcg_at_1000\n      value: 85.992\n    - type: ndcg_at_20\n      value: 84.976\n    - type: ndcg_at_3\n      value: 80.74799999999999\n    - type: ndcg_at_5\n      value: 82.626\n    - type: precision_at_1\n      value: 76.08\n    - type: precision_at_10\n      value: 12.926000000000002\n    - type: precision_at_100\n      value: 1.509\n    - type: precision_at_1000\n      value: 0.156\n    - type: precision_at_20\n      value: 6.912999999999999\n    - type: precision_at_3\n      value: 35.5\n    - type: precision_at_5\n      value: 23.541999999999998\n    - type: recall_at_1\n      value: 65.848\n    - type: recall_at_10\n      value: 92.611\n    - type: recall_at_100\n      value: 98.69\n    - type: recall_at_1000\n      value: 99.83999999999999\n    - type: recall_at_20\n      value: 95.47200000000001\n    - type: recall_at_3\n      value: 83.122\n    - type: recall_at_5\n      value: 88.23\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB SCIDOCS-PL (default)\n      revision: 45452b03f05560207ef19149545f168e596c9337\n      split: test\n      type: clarin-knext/scidocs-pl\n    metrics:\n    - type: main_score\n      value: 15.379999999999999\n    - type: map_at_1\n      value: 3.6029999999999998\n    - type: map_at_10\n      value: 8.843\n    - type: map_at_100\n      value: 10.433\n    - type: map_at_1000\n      value: 10.689\n    - type: map_at_20\n      value: 9.597\n    - type: map_at_3\n      value: 6.363\n    - type: map_at_5\n      value: 7.603\n    - type: mrr_at_1\n      value: 17.7\n    - type: mrr_at_10\n      value: 26.58900793650793\n    - type: mrr_at_100\n      value: 27.699652322890987\n    - type: mrr_at_1000\n      value: 27.78065313118353\n    - type: mrr_at_20\n      value: 27.215020950411816\n    - type: mrr_at_3\n      value: 23.36666666666668\n    - type: mrr_at_5\n      value: 25.211666666666666\n    - type: nauc_map_at_1000_diff1\n      value: 21.92235143827129\n    - type: nauc_map_at_1000_max\n      value: 37.50300940750989\n    - type: nauc_map_at_1000_std\n      value: 20.872586122198552\n    - type: nauc_map_at_100_diff1\n      value: 21.917408170465833\n    - type: nauc_map_at_100_max\n      value: 37.4654466815513\n    - type: nauc_map_at_100_std\n      value: 20.621643878648534\n    - type: nauc_map_at_10_diff1\n      value: 22.914388723621183\n    - type: nauc_map_at_10_max\n      value: 36.468131213468794\n    - type: nauc_map_at_10_std\n      value: 16.760980140791492\n    - type: nauc_map_at_1_diff1\n      value: 29.00799502838457\n    - type: nauc_map_at_1_max\n      value: 26.64926291797503\n    - type: nauc_map_at_1_std\n      value: 8.167291261637361\n    - type: nauc_map_at_20_diff1\n      value: 22.46580947804047\n    - type: nauc_map_at_20_max\n      value: 36.656294842562275\n    - type: nauc_map_at_20_std\n      value: 18.099232417722078\n    - type: nauc_map_at_3_diff1\n      value: 23.436009032045934\n    - type: nauc_map_at_3_max\n      value: 31.325807212280914\n    - type: nauc_map_at_3_std\n      value: 9.780905232048852\n    - type: nauc_map_at_5_diff1\n      value: 22.891704394665528\n    - type: nauc_map_at_5_max\n      value: 35.40584466642894\n    - type: nauc_map_at_5_std\n      value: 13.476986099394656\n    - type: nauc_mrr_at_1000_diff1\n      value: 25.052937655397866\n    - type: nauc_mrr_at_1000_max\n      value: 29.64431912670108\n    - type: nauc_mrr_at_1000_std\n      value: 14.549744963988044\n    - type: nauc_mrr_at_100_diff1\n      value: 25.070871266969224\n    - type: nauc_mrr_at_100_max\n      value: 29.68743604652336\n    - type: nauc_mrr_at_100_std\n      value: 14.582010154574432\n    - type: nauc_mrr_at_10_diff1\n      value: 24.88881466938897\n    - type: nauc_mrr_at_10_max\n      value: 29.488430770768144\n    - type: nauc_mrr_at_10_std\n      value: 14.269241073852266\n    - type: nauc_mrr_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_mrr_at_1_max\n      value: 26.81908580507911\n    - type: nauc_mrr_at_1_std\n      value: 8.00840295809718\n    - type: nauc_mrr_at_20_diff1\n      value: 25.067912695721944\n    - type: nauc_mrr_at_20_max\n      value: 29.759227563849628\n    - type: nauc_mrr_at_20_std\n      value: 14.685076859257357\n    - type: nauc_mrr_at_3_diff1\n      value: 24.645848739182696\n    - type: nauc_mrr_at_3_max\n      value: 27.73368549660351\n    - type: nauc_mrr_at_3_std\n      value: 11.475742805586943\n    - type: nauc_mrr_at_5_diff1\n      value: 24.895295760909946\n    - type: nauc_mrr_at_5_max\n      value: 29.130755033240423\n    - type: nauc_mrr_at_5_std\n      value: 12.955802929145404\n    - type: nauc_ndcg_at_1000_diff1\n      value: 20.68434434777729\n    - type: nauc_ndcg_at_1000_max\n      value: 37.67055146424174\n    - type: nauc_ndcg_at_1000_std\n      value: 29.57493715069776\n    - type: nauc_ndcg_at_100_diff1\n      value: 20.396834816492383\n    - type: nauc_ndcg_at_100_max\n      value: 37.460575228670514\n    - type: nauc_ndcg_at_100_std\n      value: 27.826534756761944\n    - type: nauc_ndcg_at_10_diff1\n      value: 22.640844106236027\n    - type: nauc_ndcg_at_10_max\n      value: 35.21291764462327\n    - type: nauc_ndcg_at_10_std\n      value: 19.53289455984506\n    - type: nauc_ndcg_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_ndcg_at_1_max\n      value: 26.81908580507911\n    - type: nauc_ndcg_at_1_std\n      value: 8.00840295809718\n    - type: nauc_ndcg_at_20_diff1\n      value: 22.117126657768623\n    - type: nauc_ndcg_at_20_max\n      value: 35.79395781940806\n    - type: nauc_ndcg_at_20_std\n      value: 22.242748346260786\n    - type: nauc_ndcg_at_3_diff1\n      value: 23.00596063212187\n    - type: nauc_ndcg_at_3_max\n      value: 30.149013627580523\n    - type: nauc_ndcg_at_3_std\n      value: 11.07904064662722\n    - type: nauc_ndcg_at_5_diff1\n      value: 22.81875419630523\n    - type: nauc_ndcg_at_5_max\n      value: 34.24267468356626\n    - type: nauc_ndcg_at_5_std\n      value: 15.307780280752088\n    - type: nauc_precision_at_1000_diff1\n      value: 9.606677689029972\n    - type: nauc_precision_at_1000_max\n      value: 32.74855550489271\n    - type: nauc_precision_at_1000_std\n      value: 42.65372585937895\n    - type: nauc_precision_at_100_diff1\n      value: 11.528981313529545\n    - type: nauc_precision_at_100_max\n      value: 35.642529490132404\n    - type: nauc_precision_at_100_std\n      value: 38.146151426052306\n    - type: nauc_precision_at_10_diff1\n      value: 18.783957183811836\n    - type: nauc_precision_at_10_max\n      value: 36.1982008334257\n    - type: nauc_precision_at_10_std\n      value: 25.09349473195891\n    - type: nauc_precision_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_precision_at_1_max\n      value: 26.81908580507911\n    - type: nauc_precision_at_1_std\n      value: 8.00840295809718\n    - type: nauc_precision_at_20_diff1\n      value: 17.458766320828214\n    - type: nauc_precision_at_20_max\n      value: 36.000404903025235\n    - type: nauc_precision_at_20_std\n      value: 29.1608044138323\n    - type: nauc_precision_at_3_diff1\n      value: 20.213669462067166\n    - type: nauc_precision_at_3_max\n      value: 31.120650847205912\n    - type: nauc_precision_at_3_std\n      value: 12.390972418818118\n    - type: nauc_precision_at_5_diff1\n      value: 20.114245715785678\n    - type: nauc_precision_at_5_max\n      value: 37.30360111495823\n    - type: nauc_precision_at_5_std\n      value: 19.053109037822853\n    - type: nauc_recall_at_1000_diff1\n      value: 9.85800049032612\n    - type: nauc_recall_at_1000_max\n      value: 32.48319160802687\n    - type: nauc_recall_at_1000_std\n      value: 43.79941601741161\n    - type: nauc_recall_at_100_diff1\n      value: 11.375255270968337\n    - type: nauc_recall_at_100_max\n      value: 35.1868784124497\n    - type: nauc_recall_at_100_std\n      value: 38.422680583482666\n    - type: nauc_recall_at_10_diff1\n      value: 18.445783123521938\n    - type: nauc_recall_at_10_max\n      value: 35.633267936276766\n    - type: nauc_recall_at_10_std\n      value: 24.94469506254716\n    - type: nauc_recall_at_1_diff1\n      value: 29.00799502838457\n    - type: nauc_recall_at_1_max\n      value: 26.64926291797503\n    - type: nauc_recall_at_1_std\n      value: 8.167291261637361\n    - type: nauc_recall_at_20_diff1\n      value: 17.314906604151936\n    - type: nauc_recall_at_20_max\n      value: 35.66067699203996\n    - type: nauc_recall_at_20_std\n      value: 29.400137012506082\n    - type: nauc_recall_at_3_diff1\n      value: 19.873710875648698\n    - type: nauc_recall_at_3_max\n      value: 30.92404718742849\n    - type: nauc_recall_at_3_std\n      value: 12.400871018075199\n    - type: nauc_recall_at_5_diff1\n      value: 19.869948324233192\n    - type: nauc_recall_at_5_max\n      value: 37.06832511687574\n    - type: nauc_recall_at_5_std\n      value: 19.0798814966156\n    - type: ndcg_at_1\n      value: 17.7\n    - type: ndcg_at_10\n      value: 15.379999999999999\n    - type: ndcg_at_100\n      value: 22.09\n    - type: ndcg_at_1000\n      value: 27.151999999999997\n    - type: ndcg_at_20\n      value: 17.576\n    - type: ndcg_at_3\n      value: 14.219999999999999\n    - type: ndcg_at_5\n      value: 12.579\n    - type: precision_at_1\n      value: 17.7\n    - type: precision_at_10\n      value: 8.08\n    - type: precision_at_100\n      value: 1.7840000000000003\n    - type: precision_at_1000\n      value: 0.3\n    - type: precision_at_20\n      value: 5.305\n    - type: precision_at_3\n      value: 13.167000000000002\n    - type: precision_at_5\n      value: 11.06\n    - type: recall_at_1\n      value: 3.6029999999999998\n    - type: recall_at_10\n      value: 16.413\n    - type: recall_at_100\n      value: 36.263\n    - type: recall_at_1000\n      value: 61.016999999999996\n    - type: recall_at_20\n      value: 21.587999999999997\n    - type: recall_at_3\n      value: 8.013\n    - type: recall_at_5\n      value: 11.198\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB SciFact-PL (default)\n      revision: 47932a35f045ef8ed01ba82bf9ff67f6e109207e\n      split: test\n      type: clarin-knext/scifact-pl\n    metrics:\n    - type: main_score\n      value: 64.764\n    - type: map_at_1\n      value: 49.778\n    - type: map_at_10\n      value: 59.88\n    - type: map_at_100\n      value: 60.707\n    - type: map_at_1000\n      value: 60.729\n    - type: map_at_20\n      value: 60.419999999999995\n    - type: map_at_3\n      value: 57.45400000000001\n    - type: map_at_5\n      value: 58.729\n    - type: mrr_at_1\n      value: 52.33333333333333\n    - type: mrr_at_10\n      value: 61.29193121693122\n    - type: mrr_at_100\n      value: 61.95817765126313\n    - type: mrr_at_1000\n      value: 61.97583284368782\n    - type: mrr_at_20\n      value: 61.72469949641003\n    - type: mrr_at_3\n      value: 59.44444444444444\n    - type: mrr_at_5\n      value: 60.494444444444454\n    - type: nauc_map_at_1000_diff1\n      value: 62.21235294015774\n    - type: nauc_map_at_1000_max\n      value: 48.83996609100249\n    - type: nauc_map_at_1000_std\n      value: 5.23892781043174\n    - type: nauc_map_at_100_diff1\n      value: 62.20170226789429\n    - type: nauc_map_at_100_max\n      value: 48.8391766453537\n    - type: nauc_map_at_100_std\n      value: 5.2664077457917715\n    - type: nauc_map_at_10_diff1\n      value: 61.961975488329024\n    - type: nauc_map_at_10_max\n      value: 48.397109987625186\n    - type: nauc_map_at_10_std\n      value: 4.314859710827481\n    - type: nauc_map_at_1_diff1\n      value: 65.0865197011516\n    - type: nauc_map_at_1_max\n      value: 41.38862781954889\n    - type: nauc_map_at_1_std\n      value: -0.9182122632530586\n    - type: nauc_map_at_20_diff1\n      value: 61.99173935851292\n    - type: nauc_map_at_20_max\n      value: 48.79961814179307\n    - type: nauc_map_at_20_std\n      value: 5.262181845825118\n    - type: nauc_map_at_3_diff1\n      value: 62.37910539880477\n    - type: nauc_map_at_3_max\n      value: 47.13627890977091\n    - type: nauc_map_at_3_std\n      value: 2.327897198087264\n    - type: nauc_map_at_5_diff1\n      value: 61.60080757149592\n    - type: nauc_map_at_5_max\n      value: 47.60052458345962\n    - type: nauc_map_at_5_std\n      value: 3.1770196981231047\n    - type: nauc_mrr_at_1000_diff1\n      value: 62.86810952814966\n    - type: nauc_mrr_at_1000_max\n      value: 52.13248094447774\n    - type: nauc_mrr_at_1000_std\n      value: 10.100485746570733\n    - type: nauc_mrr_at_100_diff1\n      value: 62.85364829491874\n    - type: nauc_mrr_at_100_max\n      value: 52.134528010631854\n    - type: nauc_mrr_at_100_std\n      value: 10.120945685447369\n    - type: nauc_mrr_at_10_diff1\n      value: 62.65679301829915\n    - type: nauc_mrr_at_10_max\n      value: 52.09270719182349\n    - type: nauc_mrr_at_10_std\n      value: 9.913834434725441\n    - type: nauc_mrr_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_mrr_at_1_max\n      value: 46.67646429855176\n    - type: nauc_mrr_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_mrr_at_20_diff1\n      value: 62.72473227039611\n    - type: nauc_mrr_at_20_max\n      value: 52.13479097802757\n    - type: nauc_mrr_at_20_std\n      value: 10.188278833464084\n    - type: nauc_mrr_at_3_diff1\n      value: 63.797429185518496\n    - type: nauc_mrr_at_3_max\n      value: 52.16486999573481\n    - type: nauc_mrr_at_3_std\n      value: 9.094360767062762\n    - type: nauc_mrr_at_5_diff1\n      value: 62.592917975475494\n    - type: nauc_mrr_at_5_max\n      value: 52.330741486107414\n    - type: nauc_mrr_at_5_std\n      value: 9.742175534421389\n    - type: nauc_ndcg_at_1000_diff1\n      value: 61.38859337672476\n    - type: nauc_ndcg_at_1000_max\n      value: 51.48380058339184\n    - type: nauc_ndcg_at_1000_std\n      value: 9.670547660897673\n    - type: nauc_ndcg_at_100_diff1\n      value: 61.02438489641434\n    - type: nauc_ndcg_at_100_max\n      value: 51.781246646780865\n    - type: nauc_ndcg_at_100_std\n      value: 10.592961553245187\n    - type: nauc_ndcg_at_10_diff1\n      value: 60.03678353308358\n    - type: nauc_ndcg_at_10_max\n      value: 50.70725688848762\n    - type: nauc_ndcg_at_10_std\n      value: 7.9472446491016315\n    - type: nauc_ndcg_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_ndcg_at_1_max\n      value: 46.67646429855176\n    - type: nauc_ndcg_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_ndcg_at_20_diff1\n      value: 59.828482718480224\n    - type: nauc_ndcg_at_20_max\n      value: 51.45831789601284\n    - type: nauc_ndcg_at_20_std\n      value: 10.722673683272049\n    - type: nauc_ndcg_at_3_diff1\n      value: 61.68982937524109\n    - type: nauc_ndcg_at_3_max\n      value: 49.745326748604775\n    - type: nauc_ndcg_at_3_std\n      value: 4.948298621202247\n    - type: nauc_ndcg_at_5_diff1\n      value: 59.67396171973207\n    - type: nauc_ndcg_at_5_max\n      value: 49.87855139298281\n    - type: nauc_ndcg_at_5_std\n      value: 6.08990428055584\n    - type: nauc_precision_at_1000_diff1\n      value: -1.594227972036865\n    - type: nauc_precision_at_1000_max\n      value: 32.48431723086185\n    - type: nauc_precision_at_1000_std\n      value: 53.84748466965268\n    - type: nauc_precision_at_100_diff1\n      value: 8.06411455192293\n    - type: nauc_precision_at_100_max\n      value: 39.91003601878948\n    - type: nauc_precision_at_100_std\n      value: 55.52979711075091\n    - type: nauc_precision_at_10_diff1\n      value: 26.610514456014066\n    - type: nauc_precision_at_10_max\n      value: 47.09062494321172\n    - type: nauc_precision_at_10_std\n      value: 33.91984226498748\n    - type: nauc_precision_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_precision_at_1_max\n      value: 46.67646429855176\n    - type: nauc_precision_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_precision_at_20_diff1\n      value: 16.947688843085583\n    - type: nauc_precision_at_20_max\n      value: 45.40488186572008\n    - type: nauc_precision_at_20_std\n      value: 48.354421924500905\n    - type: nauc_precision_at_3_diff1\n      value: 49.11263981720622\n    - type: nauc_precision_at_3_max\n      value: 52.7084625111683\n    - type: nauc_precision_at_3_std\n      value: 16.734612173556453\n    - type: nauc_precision_at_5_diff1\n      value: 39.06503705015792\n    - type: nauc_precision_at_5_max\n      value: 52.21710506893391\n    - type: nauc_precision_at_5_std\n      value: 23.350948149460233\n    - type: nauc_recall_at_1000_diff1\n      value: 43.1559290382817\n    - type: nauc_recall_at_1000_max\n      value: 83.66013071895456\n    - type: nauc_recall_at_1000_std\n      value: 86.27450980392177\n    - type: nauc_recall_at_100_diff1\n      value: 46.016860850620375\n    - type: nauc_recall_at_100_max\n      value: 69.3944888744547\n    - type: nauc_recall_at_100_std\n      value: 55.286945696152735\n    - type: nauc_recall_at_10_diff1\n      value: 49.65877895350921\n    - type: nauc_recall_at_10_max\n      value: 53.02636695700889\n    - type: nauc_recall_at_10_std\n      value: 13.967608945823828\n    - type: nauc_recall_at_1_diff1\n      value: 65.0865197011516\n    - type: nauc_recall_at_1_max\n      value: 41.38862781954889\n    - type: nauc_recall_at_1_std\n      value: -0.9182122632530586\n    - type: nauc_recall_at_20_diff1\n      value: 43.355308229973524\n    - type: nauc_recall_at_20_max\n      value: 57.04187909533764\n    - type: nauc_recall_at_20_std\n      value: 33.578720846660524\n    - type: nauc_recall_at_3_diff1\n      value: 56.922996057428165\n    - type: nauc_recall_at_3_max\n      value: 50.74417041895424\n    - type: nauc_recall_at_3_std\n      value: 5.623890124328387\n    - type: nauc_recall_at_5_diff1\n      value: 50.55620076865238\n    - type: nauc_recall_at_5_max\n      value: 51.3316854622085\n    - type: nauc_recall_at_5_std\n      value: 8.995457887269255\n    - type: ndcg_at_1\n      value: 52.333\n    - type: ndcg_at_10\n      value: 64.764\n    - type: ndcg_at_100\n      value: 68.167\n    - type: ndcg_at_1000\n      value: 68.816\n    - type: ndcg_at_20\n      value: 66.457\n    - type: ndcg_at_3\n      value: 60.346\n    - type: ndcg_at_5\n      value: 62.365\n    - type: precision_at_1\n      value: 52.333\n    - type: precision_at_10\n      value: 8.799999999999999\n    - type: precision_at_100\n      value: 1.057\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_20\n      value: 4.8\n    - type: precision_at_3\n      value: 23.889\n    - type: precision_at_5\n      value: 15.6\n    - type: recall_at_1\n      value: 49.778\n    - type: recall_at_10\n      value: 78.206\n    - type: recall_at_100\n      value: 93.10000000000001\n    - type: recall_at_1000\n      value: 98.333\n    - type: recall_at_20\n      value: 84.467\n    - type: recall_at_3\n      value: 66.367\n    - type: recall_at_5\n      value: 71.35000000000001\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB TRECCOVID-PL (default)\n      revision: 81bcb408f33366c2a20ac54adafad1ae7e877fdd\n      split: test\n      type: clarin-knext/trec-covid-pl\n    metrics:\n    - type: main_score\n      value: 72.18900000000001\n    - type: map_at_1\n      value: 0.214\n    - type: map_at_10\n      value: 1.755\n    - type: map_at_100\n      value: 9.944\n    - type: map_at_1000\n      value: 24.205\n    - type: map_at_20\n      value: 3.1510000000000002\n    - type: map_at_3\n      value: 0.6\n    - type: map_at_5\n      value: 0.9560000000000001\n    - type: mrr_at_1\n      value: 82.0\n    - type: mrr_at_10\n      value: 89.06666666666666\n    - type: mrr_at_100\n      value: 89.06666666666666\n    - type: mrr_at_1000\n      value: 89.06666666666666\n    - type: mrr_at_20\n      value: 89.06666666666666\n    - type: mrr_at_3\n      value: 87.66666666666666\n    - type: mrr_at_5\n      value: 89.06666666666666\n    - type: nauc_map_at_1000_diff1\n      value: -9.342037623635543\n    - type: nauc_map_at_1000_max\n      value: 45.71499810252398\n    - type: nauc_map_at_1000_std\n      value: 76.86482845196852\n    - type: nauc_map_at_100_diff1\n      value: -6.932395299866198\n    - type: nauc_map_at_100_max\n      value: 36.097801891181604\n    - type: nauc_map_at_100_std\n      value: 65.6085215411685\n    - type: nauc_map_at_10_diff1\n      value: -6.3654843824342775\n    - type: nauc_map_at_10_max\n      value: 9.564437521432714\n    - type: nauc_map_at_10_std\n      value: 21.8377319336476\n    - type: nauc_map_at_1_diff1\n      value: 8.269590874255034\n    - type: nauc_map_at_1_max\n      value: 3.482498491294516\n    - type: nauc_map_at_1_std\n      value: 8.985226819412189\n    - type: nauc_map_at_20_diff1\n      value: -4.971435767877232\n    - type: nauc_map_at_20_max\n      value: 22.88801858567121\n    - type: nauc_map_at_20_std\n      value: 32.38492618534027\n    - type: nauc_map_at_3_diff1\n      value: 1.1615973694623123\n    - type: nauc_map_at_3_max\n      value: 1.935417800315643\n    - type: nauc_map_at_3_std\n      value: 10.289328305818698\n    - type: nauc_map_at_5_diff1\n      value: -2.4675967231444105\n    - type: nauc_map_at_5_max\n      value: 2.4611483736622373\n    - type: nauc_map_at_5_std\n      value: 15.082324305750811\n    - type: nauc_mrr_at_1000_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_1000_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_1000_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_100_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_100_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_100_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_10_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_10_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_10_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_1_diff1\n      value: 12.099350148694809\n    - type: nauc_mrr_at_1_max\n      value: 53.75041304108387\n    - type: nauc_mrr_at_1_std\n      value: 68.84018063663402\n    - type: nauc_mrr_at_20_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_20_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_20_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_3_diff1\n      value: 12.173557857011161\n    - type: nauc_mrr_at_3_max\n      value: 57.540780562363395\n    - type: nauc_mrr_at_3_std\n      value: 75.42098189580211\n    - type: nauc_mrr_at_5_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_5_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_5_std\n      value: 73.2456769749587\n    - type: nauc_ndcg_at_1000_diff1\n      value: -8.951471847310401\n    - type: nauc_ndcg_at_1000_max\n      value: 43.86942237288822\n    - type: nauc_ndcg_at_1000_std\n      value: 74.61077735148591\n    - type: nauc_ndcg_at_100_diff1\n      value: -17.754559361083817\n    - type: nauc_ndcg_at_100_max\n      value: 53.97187119773482\n    - type: nauc_ndcg_at_100_std\n      value: 80.7944136146514\n    - type: nauc_ndcg_at_10_diff1\n      value: -26.637734697836414\n    - type: nauc_ndcg_at_10_max\n      value: 47.70102699133149\n    - type: nauc_ndcg_at_10_std\n      value: 70.26909560828646\n    - type: nauc_ndcg_at_1_diff1\n      value: -1.2250530785563207\n    - type: nauc_ndcg_at_1_max\n      value: 46.60509554140131\n    - type: nauc_ndcg_at_1_std\n      value: 62.63906581740976\n    - type: nauc_ndcg_at_20_diff1\n      value: -22.44286466550908\n    - type: nauc_ndcg_at_20_max\n      value: 55.40492058090103\n    - type: nauc_ndcg_at_20_std\n      value: 72.11813912145738\n    - type: nauc_ndcg_at_3_diff1\n      value: -14.8152721896563\n    - type: nauc_ndcg_at_3_max\n      value: 38.952259383027595\n    - type: nauc_ndcg_at_3_std\n      value: 59.819750166537766\n    - type: nauc_ndcg_at_5_diff1\n      value: -19.150105688904375\n    - type: nauc_ndcg_at_5_max\n      value: 42.311180547775315\n    - type: nauc_ndcg_at_5_std\n      value: 66.6632229321094\n    - type: nauc_precision_at_1000_diff1\n      value: -11.555591477978941\n    - type: nauc_precision_at_1000_max\n      value: 43.7311644834851\n    - type: nauc_precision_at_1000_std\n      value: 52.10644767999648\n    - type: nauc_precision_at_100_diff1\n      value: -16.94803099801117\n    - type: nauc_precision_at_100_max\n      value: 54.08281631067633\n    - type: nauc_precision_at_100_std\n      value: 82.77237347891331\n    - type: nauc_precision_at_10_diff1\n      value: -27.351332814863355\n    - type: nauc_precision_at_10_max\n      value: 48.08237549065846\n    - type: nauc_precision_at_10_std\n      value: 69.37250843534329\n    - type: nauc_precision_at_1_diff1\n      value: 12.099350148694809\n    - type: nauc_precision_at_1_max\n      value: 53.75041304108387\n    - type: nauc_precision_at_1_std\n      value: 68.84018063663402\n    - type: nauc_precision_at_20_diff1\n      value: -18.2422222283388\n    - type: nauc_precision_at_20_max\n      value: 59.517328129343696\n    - type: nauc_precision_at_20_std\n      value: 72.05149307342747\n    - type: nauc_precision_at_3_diff1\n      value: -10.226547543075897\n    - type: nauc_precision_at_3_max\n      value: 43.14684818832875\n    - type: nauc_precision_at_3_std\n      value: 57.31936467418288\n    - type: nauc_precision_at_5_diff1\n      value: -14.28521589468673\n    - type: nauc_precision_at_5_max\n      value: 41.633426753962596\n    - type: nauc_precision_at_5_std\n      value: 64.94400576804541\n    - type: nauc_recall_at_1000_diff1\n      value: -0.9648831207497152\n    - type: nauc_recall_at_1000_max\n      value: 31.70832946085005\n    - type: nauc_recall_at_1000_std\n      value: 63.21471613968869\n    - type: nauc_recall_at_100_diff1\n      value: -1.360254380933586\n    - type: nauc_recall_at_100_max\n      value: 25.960597782099605\n    - type: nauc_recall_at_100_std\n      value: 51.52757589609674\n    - type: nauc_recall_at_10_diff1\n      value: -0.3899439424189566\n    - type: nauc_recall_at_10_max\n      value: 5.094341897886072\n    - type: nauc_recall_at_10_std\n      value: 11.266045616925698\n    - type: nauc_recall_at_1_diff1\n      value: 8.269590874255034\n    - type: nauc_recall_at_1_max\n      value: 3.482498491294516\n    - type: nauc_recall_at_1_std\n      value: 8.985226819412189\n    - type: nauc_recall_at_20_diff1\n      value: 6.4797098359254175\n    - type: nauc_recall_at_20_max\n      value: 15.663700985336124\n    - type: nauc_recall_at_20_std\n      value: 17.154099587904913\n    - type: nauc_recall_at_3_diff1\n      value: 3.7245972450393507\n    - type: nauc_recall_at_3_max\n      value: 0.4063857187240345\n    - type: nauc_recall_at_3_std\n      value: 6.641948062821941\n    - type: nauc_recall_at_5_diff1\n      value: 4.013879477591466\n    - type: nauc_recall_at_5_max\n      value: -1.4266586618013566\n    - type: nauc_recall_at_5_std\n      value: 7.311601874411205\n    - type: ndcg_at_1\n      value: 75.0\n    - type: ndcg_at_10\n      value: 72.18900000000001\n    - type: ndcg_at_100\n      value: 54.022999999999996\n    - type: ndcg_at_1000\n      value: 49.492000000000004\n    - type: ndcg_at_20\n      value: 68.51\n    - type: ndcg_at_3\n      value: 73.184\n    - type: ndcg_at_5\n      value: 72.811\n    - type: precision_at_1\n      value: 82.0\n    - type: precision_at_10\n      value: 77.4\n    - type: precision_at_100\n      value: 55.24\n    - type: precision_at_1000\n      value: 21.822\n    - type: precision_at_20\n      value: 73.0\n    - type: precision_at_3\n      value: 79.333\n    - type: precision_at_5\n      value: 79.2\n    - type: recall_at_1\n      value: 0.214\n    - type: recall_at_10\n      value: 1.9980000000000002\n    - type: recall_at_100\n      value: 13.328999999999999\n    - type: recall_at_1000\n      value: 47.204\n    - type: recall_at_20\n      value: 3.7310000000000003\n    - type: recall_at_3\n      value: 0.628\n    - type: recall_at_5\n      value: 1.049\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB CEDRClassification (default)\n      revision: c0ba03d058e3e1b2f3fd20518875a4563dd12db4\n      split: test\n      type: ai-forever/cedr-classification\n    metrics:\n    - type: accuracy\n      value: 47.30605738575983\n    - type: f1\n      value: 41.26091043925065\n    - type: lrap\n      value: 72.89452709883206\n    - type: main_score\n      value: 47.30605738575983\n    task:\n      type: MultilabelClassification\n  - dataset:\n      config: ru\n      name: MTEB MIRACLReranking (ru)\n      revision: 6d1962c527217f8927fca80f890f14f36b2802af\n      split: dev\n      type: miracl/mmteb-miracl-reranking\n    metrics:\n    - type: MAP@1(MIRACL)\n      value: 20.721999999999998\n    - type: MAP@10(MIRACL)\n      value: 33.900999999999996\n    - type: MAP@100(MIRACL)\n      value: 36.813\n    - type: MAP@1000(MIRACL)\n      value: 36.813\n    - type: MAP@20(MIRACL)\n      value: 35.684\n    - type: MAP@3(MIRACL)\n      value: 28.141\n    - type: MAP@5(MIRACL)\n      value: 31.075000000000003\n    - type: NDCG@1(MIRACL)\n      value: 32.799\n    - type: NDCG@10(MIRACL)\n      value: 42.065000000000005\n    - type: NDCG@100(MIRACL)\n      value: 49.730999999999995\n    - type: NDCG@1000(MIRACL)\n      value: 49.730999999999995\n    - type: NDCG@20(MIRACL)\n      value: 46.0\n    - type: NDCG@3(MIRACL)\n      value: 34.481\n    - type: NDCG@5(MIRACL)\n      value: 37.452999999999996\n    - type: P@1(MIRACL)\n', '{"pipeline_tag":"feature-extraction","library_name":"transformers","framework":"transformers","params":572310396,"storage_bytes":8007991570,"files_count":15,"spaces_count":41,"gated":false,"private":false,"config":{"architectures":["XLMRobertaModel"],"auto_map":{"AutoConfig":"jinaai/xlm-roberta-flash-implementation--configuration_xlm_roberta.XLMRobertaFlashConfig","AutoModel":"jinaai/xlm-roberta-flash-implementation--modeling_lora.XLMRobertaLoRA","AutoModelForMaskedLM":"jinaai/xlm-roberta-flash-implementation--modeling_xlm_roberta.XLMRobertaForMaskedLM","AutoModelForPreTraining":"jinaai/xlm-roberta-flash-implementation--modeling_xlm_roberta.XLMRobertaForPreTraining"},"tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":"<mask>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2409.10173","source_url":"https://arxiv.org/abs/2409.10173"}]', NULL, 'CC-BY-NC-4.0', 'approved', 80, 'cad87a11a3a9373d3d809847bb079d70', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-13b-chat-hf', 'huggingface--meta-llama--llama-2-13b-chat-hf', 'Llama-2-13b-chat-hf', 'meta-llama', '', '["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","conversational","en","arxiv:2307.09288","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1104, 268711, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-13b-chat-hf","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":13015866880,"storage_bytes":52065420569,"files_count":19,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = ''<<SYS>>\\n'' + system_message + ''\\n<</SYS>>\\n\\n'' + message[''content''] %}{% else %}{% set content = message[''content''] %}{% endif %}{% if message[''role''] == ''user'' %}{{ bos_token + ''[INST] '' + content.strip() + '' [/INST]'' }}{% elif message[''role''] == ''assistant'' %}{{ '' ''  + content.strip() + '' '' + eos_token }}{% endif %}{% endfor %}","eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 40, '4994b2d8f6c9b9fed7509ffce32aac99', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Zyphra-Zonos-v0.1-hybrid', 'huggingface--zyphra--zonos-v0.1-hybrid', 'Zonos-v0.1-hybrid', 'Zyphra', '--- license: apache-2.0 pipeline_tag: text-to-speech library_name: zonos --- <div align="center"> <img src="https://github.com/Zyphra/Zonos/blob/main/assets/ZonosHeader.png?raw=true" alt="Title card" style="width: 500px; height: auto; object-position: center top;"> </div> --- Zonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par with—or even surpassing—top TTS providers. Our model en...', '["zonos","safetensors","text-to-speech","license:apache-2.0","region:us"]', 'text-to-speech', 1100, 42014, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Zyphra/Zonos-v0.1-hybrid","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-to-speech\nlibrary_name: zonos\n---\n# Zonos-v0.1\n\n<div align="center">\n<img src="https://github.com/Zyphra/Zonos/blob/main/assets/ZonosHeader.png?raw=true"\n     alt="Title card" \n     style="width: 500px;\n            height: auto;\n            object-position: center top;">\n</div>\n\n---\n\nZonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par with—or even surpassing—top TTS providers.\n\nOur model enables highly natural speech generation from text prompts when given a speaker embedding or audio prefix, and can accurately perform speech cloning when given a reference clip spanning just a few seconds. The conditioning setup also allows for fine control over speaking rate, pitch variation, audio quality, and emotions such as happiness, fear, sadness, and anger. The model outputs speech natively at 44kHz.\n\n##### For more details and speech samples, check out our blog [here](https://www.zyphra.com/post/beta-release-of-zonos-v0-1)\n\n##### We also have a hosted version available at [playground.zyphra.com/audio](https://playground.zyphra.com/audio)\n\n---\n\nZonos follows a straightforward architecture: text normalization and phonemization via eSpeak, followed by DAC token prediction through a transformer or hybrid backbone. An overview of the architecture can be seen below.\n\n<div align="center">\n<img src="https://github.com/Zyphra/Zonos/blob/main/assets/ArchitectureDiagram.png?raw=true"\n     alt="Architecture diagram" \n     style="width: 1000px;\n            height: auto;\n            object-position: center top;">\n</div>\n\n---\n\n## Usage\n\n### Python\n\n```python\nimport torch\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained("Zyphra/Zonos-v0.1-hybrid", device="cuda")\n# model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device="cuda")\n\nwav, sampling_rate = torchaudio.load("assets/exampleaudio.mp3")\nspeaker = model.make_speaker_embedding(wav, sampling_rate)\n\ncond_dict = make_cond_dict(text="Hello, world!", speaker=speaker, language="en-us")\nconditioning = model.prepare_conditioning(cond_dict)\n\ncodes = model.generate(conditioning)\n\nwavs = model.autoencoder.decode(codes).cpu()\ntorchaudio.save("sample.wav", wavs[0], model.autoencoder.sampling_rate)\n```\n\n### Gradio interface (recommended)\n\n```bash\nuv run gradio_interface.py\n# python gradio_interface.py\n```\n\nThis should produce a `sample.wav` file in your project root directory.\n\n_For repeated sampling we highly recommend using the gradio interface instead, as the minimal example needs to load the model every time it is run._\n\n## Features\n\n- Zero-shot TTS with voice cloning: Input desired text and a 10-30s speaker sample to generate high quality TTS output\n- Audio prefix inputs: Add text plus an audio prefix for even richer speaker matching. Audio prefixes can be used to elicit behaviours such as whispering which can otherwise be challenging to replicate when cloning from speaker embeddings\n- Multilingual support: Zonos-v0.1 supports English, Japanese, Chinese, French, and German\n- Audio quality and emotion control: Zonos offers fine-grained control of many aspects of the generated audio. These include speaking rate, pitch, maximum frequency, audio quality, and various emotions such as happiness, anger, sadness, and fear.\n- Fast: our model runs with a real-time factor of ~2x on an RTX 4090\n- Gradio WebUI: Zonos comes packaged with an easy to use gradio interface to generate speech\n- Simple installation and deployment: Zonos can be installed and deployed simply using the docker file packaged with our repository.\n\n## Installation\n\n**At the moment this repository only supports Linux systems (preferably Ubuntu 22.04/24.04) with recent NVIDIA GPUs (3000-series or newer, 6GB+ VRAM).**\n\nSee also [Docker Installation](#docker-installation)\n\n#### System dependencies\n\nZonos depends on the eSpeak library phonemization. You can install it on Ubuntu with the following command:\n\n```bash\napt install -y espeak-ng\n```\n\n#### Python dependencies\n\nWe highly recommend using a recent version of [uv](https://docs.astral.sh/uv/#installation) for installation. If you don''t have uv installed, you can install it via pip: `pip install -U uv`.\n\n##### Installing into a new uv virtual environment (recommended)\n\n```bash\nuv sync\nuv sync --extra compile\n```\n\n##### Installing into the system/actived environment using uv\n\n```bash\nuv pip install -e .\nuv pip install -e .[compile]\n```\n\n##### Installing into the system/actived environment using pip\n\n```bash\npip install -e .\npip install --no-build-isolation -e .[compile]\n```\n\n##### Confirm that it''s working\n\nFor convenience we provide a minimal example to check that the installation works:\n\n```bash\nuv run sample.py\n# python sample.py\n```\n\n## Docker installation\n\n```bash\ngit clone https://github.com/Zyphra/Zonos.git\ncd Zonos\n\n# For gradio\ndocker compose up\n\n# Or for development you can do\ndocker build -t Zonos .\ndocker run -it --gpus=all --net=host -v /path/to/Zonos:/Zonos -t Zonos\ncd /Zonos\npython sample.py # this will generate a sample.wav in /Zonos\n```\n\n## Citation\nIf you find this model useful in an academic context please cite as:\n```bash\n@misc{zyphra2025zonos,\n  title     = {Zonos-v0.1: An Expressive, Open-Source TTS Model},\n  author    = {Dario Sucic, Mohamed Osman, Gabriel Clark, Chris Warner, Beren Millidge},\n  year      = {2025},\n}', '{"pipeline_tag":"text-to-speech","library_name":"zonos","framework":"zonos","params":null,"storage_bytes":3700759529,"files_count":4,"spaces_count":33,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Zyphra:Zonos","source_url":"https://github.com/Zyphra/Zonos"},{"type":"has_code","target_id":"github:Zyphra:Zonos","source_url":"https://github.com/Zyphra/Zonos"},{"type":"has_code","target_id":"github:Zyphra:Zonos.git","source_url":"https://github.com/Zyphra/Zonos.git"}]', NULL, 'Apache-2.0', 'approved', 65, '2ba4c6c630bd69849373200db6a0b28a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-unsloth-DeepSeek-R1-GGUF', 'huggingface--unsloth--deepseek-r1-gguf', 'DeepSeek-R1-GGUF', 'unsloth', '--- base_model: deepseek-ai/DeepSeek-R1 language: - en library_name: transformers license: mit tags: - deepseek - unsloth - transformers new_version: unsloth/DeepSeek-R1-0528-GGUF --- <div> <p style="margin-bottom: 0; margin-top: 0;"> <strong>See <a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong> </p> <p style="margin-bottom: 0;"> <em>Unsloth''s DeepSeek-R...', '["transformers","gguf","deepseek_v3","text-generation","deepseek","unsloth","custom_code","en","arxiv:2501.12948","base_model:deepseek-ai/deepseek-r1","base_model:quantized:deepseek-ai/deepseek-r1","license:mit","endpoints_compatible","region:us","conversational"]', 'text-generation', 1098, 25179, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/unsloth/DeepSeek-R1-GGUF","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model: deepseek-ai/DeepSeek-R1\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\ntags:\n- deepseek\n- unsloth\n- transformers\nnew_version: unsloth/DeepSeek-R1-0528-GGUF\n---\n\n<div>\n  <p style="margin-bottom: 0; margin-top: 0;">\n    <strong>See <a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong>\n  </p>\n  <p style="margin-bottom: 0;">\n    <em>Unsloth''s DeepSeek-R1 <a href="https://unsloth.ai/blog/deepseekr1-dynamic">1.58-bit + 2-bit Dynamic Quants</a> is selectively quantized, greatly improving accuracy over standard 1-bit/2-bit.</em>\n  </p>\n  <div style="display: flex; gap: 5px; align-items: center; ">\n    <a href="https://github.com/unslothai/unsloth/">\n      <img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="133">\n    </a>\n    <a href="https://discord.gg/unsloth">\n      <img src="https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png" width="173">\n    </a>\n    <a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device">\n      <img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png" width="143">\n    </a>\n  </div>\n<h1 style="margin-top: 0rem;">Instructions to run this model in llama.cpp:</h2>\n</div>\n\nOr you can view more detailed instructions here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)\n1. Do not forget about `<｜User｜>` and `<｜Assistant｜>` tokens! - Or use a chat template formatter\n2. Obtain the latest `llama.cpp` at https://github.com/ggerganov/llama.cpp. You can follow the build instructions below as well:\n```bash\napt-get update\napt-get install build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\n	-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```\n3. It''s best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.\n4. Download the model via:\n```python\n# pip install huggingface_hub hf_transfer\n# import os # Optional for faster downloading\n# os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n  repo_id = "unsloth/DeepSeek-R1-GGUF",\n  local_dir = "DeepSeek-R1-GGUF",\n  allow_patterns = ["*UD-IQ1_S*"], # Select quant type UD-IQ1_S for 1.58bit\n)\n```\n5. Example with Q4_0 K quantized cache **Notice -no-cnv disables auto conversation mode**\n```bash\n   ./llama.cpp/llama-cli \\n	  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\n	  --cache-type-k q4_0 \\n	  --threads 12 -no-cnv --prio 2 \\n	  --temp 0.6 \\n	  --ctx-size 8192 \\n	  --seed 3407 \\n	  --prompt "<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>"\n```\n   Example output:\n   \n   ```txt\n    <think>\n    Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\n    Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\n    Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\n    I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\n    Is there a scenario where 1 plus 1 wouldn''t be 2? I can''t think of any...\n   ```\n   \n6. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n```bash\n  ./llama.cpp/llama-cli \\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\n    --cache-type-k q4_0 \\n    --threads 12 -no-cnv --prio 2 \\n    --n-gpu-layers 7 \\n    --temp 0.6 \\n    --ctx-size 8192 \\n    --seed 3407 \\n    --prompt "<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>"\n```\n7. If you want to merge the weights together, use this script:\n```\n./llama.cpp/llama-gguf-split --merge \\n    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\n    merged_file.gguf\n```\n\n| MoE Bits     | Type   | Disk Size |  Accuracy | Link                      | Details   |\n| -------- | -------- | ------------ | ------------ | ---------------------|  ---------- |\n| 1.58bit | UD-IQ1_S |   **131GB**    | Fair           | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S) | MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit |\n| 1.73bit | UD-IQ1_M |   **158GB**    | Good | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M) | MoE all 1.56bit. `down_proj` in MoE left at 2.06bit |\n| 2.22bit | UD-IQ2_XXS |   **183GB**    | Better      | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS) | MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit |\n| 2.51bit | UD-Q2_K_XL |   **212GB**    | Best | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL) | MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit |\n\n# Finetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb\n\n[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="200"/>](https://discord.gg/unsloth)\n[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)\n\n\n## ✨ Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click "Run All", and you''ll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Phi-4 (14B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2 VL (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |\n| **Qwen2.5 (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Llama-3.1 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n[<img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png" width="200"/>](https://docs.unsloth.ai)\n\n- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n\n## Special Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\n\n\n\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":9778194944831,"files_count":118,"spaces_count":2,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3"}}', '[]', '[{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp.","source_url":"https://github.com/ggerganov/llama.cpp."},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 80, 'fabd0b2ea62dcfc83449eb1780b1a26d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-7b', 'huggingface--tiiuae--falcon-7b', 'falcon-7b', 'tiiuae', '--- datasets: - tiiuae/falcon-refinedweb language: - en inference: false license: apache-2.0 new_version: tiiuae/falcon-11B --- **Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.** *Paper coming soon* 😊. 🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! * **It outperforms comp...', '["transformers","pytorch","safetensors","falcon","text-generation","custom_code","en","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2101.00027","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","license:apache-2.0","text-generation-inference","deploy:azure","region:us"]', 'text-generation', 1095, 107660, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-7b","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\ninference: false\nlicense: apache-2.0\nnew_version: tiiuae/falcon-11B\n---\n\n# 🚀 Falcon-7B\n\n**Falcon-7B is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon* 😊.\n\n🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n\n## Why use Falcon-7B?\n\n* **It outperforms comparable open-source models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n* **It is made available under a permissive Apache 2.0 license allowing for commercial use**, without any royalties or restrictions.\n\n⚠️ **This is a raw, pretrained model, which should be further finetuned for most usecases.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct). \n\n🔥 **Looking for an even more powerful model?** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) is Falcon-7B''s big brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n💥 **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 16GB of memory** to swiftly run inference with Falcon-7B.\n\n# Model Card for Falcon-7B\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish);\n- **License:** Apache 2.0.\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nResearch on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-7B is trained on English and French data only, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-7B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-7B was trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile ([Gao et al., 2020](https://arxiv.org/abs/2101.00027)). \n\n| **Data source**    | **Fraction** | **Tokens** | **Sources**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 79%          | 1,185B     | massive web crawl                 |\n| Books              | 7%           | 110B       |                                   |\n| Conversations      | 6%           | 85B        | Reddit, StackOverflow, HackerNews |\n| Code               | 3%           | 45B        |                                   |\n| RefinedWeb-French  | 3%           | 45B        | massive web crawl                 |\n| Technical          | 2%           | 30B        | arXiv, PubMed, USPTO, etc.        |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n### Training Procedure \n\nFalcon-7B was trained on 384 A100 40GB GPUs, using a 2D parallelism strategy (PP=2, DP=192) combined with ZeRO.\n\n#### Training Hyperparameters\n\n| **Hyperparameter** | **Value**  | **Comment**                               |\n|--------------------|------------|-------------------------------------------|\n| Precision          | `bfloat16` |                                           |\n| Optimizer          | AdamW      |                                           |\n| Learning rate      | 6e-4       | 4B tokens warm-up, cosine decay to 1.2e-5 |\n| Weight decay       | 1e-1       |                                           |\n| Z-loss       | 1e-4       |                                           |\n| Batch size         | 2304        | 30B tokens ramp-up                         |\n\n\n#### Speeds, Sizes, Times\n\nTraining happened in early March 2023 and took about two weeks. \n\n\n## Evaluation\n\n*Paper coming soon*.\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\n### Model Architecture and Objective\n\nFalcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 32        |                                        |\n| `d_model`          | 4544      | Increased to compensate for multiquery                                       |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-7B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-7B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* 😊. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n## License\n\nFalcon-7B is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7217189760,"storage_bytes":43359994411,"files_count":15,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["FalconForCausalLM"],"auto_map":{"AutoConfig":"configuration_falcon.FalconConfig","AutoModel":"modeling_falcon.FalconModel","AutoModelForSequenceClassification":"modeling_falcon.FalconForSequenceClassification","AutoModelForTokenClassification":"modeling_falcon.FalconForTokenClassification","AutoModelForQuestionAnswering":"modeling_falcon.FalconForQuestionAnswering","AutoModelForCausalLM":"modeling_falcon.FalconForCausalLM"},"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'Apache-2.0', 'approved', 80, 'c7e5b5d538aef542c7f330333ba6a980', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CohereLabs-c4ai-command-r-v01', 'huggingface--coherelabs--c4ai-command-r-v01', 'c4ai-command-r-v01', 'CohereLabs', '', '["transformers","safetensors","cohere","text-generation","conversational","en","fr","de","es","it","pt","ja","ko","zh","ar","doi:10.57967/hf/3139","license:cc-by-nc-4.0","text-generation-inference","region:us"]', 'text-generation', 1095, 12058, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CohereLabs/c4ai-command-r-v01","fetched_at":"2025-12-08T10:39:52.036Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":34980831232,"storage_bytes":205615615951,"files_count":23,"spaces_count":55,"gated":"auto","private":false,"config":{"architectures":["CohereForCausalLM"],"model_type":"cohere","tokenizer_config":{"bos_token":"<BOS_TOKEN>","chat_template":[{"name":"default","template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = ''You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + system_message + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% set content = message[''content''] %}{% if message[''role''] == ''user'' %}{{ ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''assistant'' %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}{% endif %}"},{"name":"tool_use","template":"\n{%- macro json_to_python_type(json_spec) %}\n{%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n\n{%- if basic_type_map[json_spec.type] is defined %}\n    {{- basic_type_map[json_spec.type] }}\n{%- elif json_spec.type == \"array\" %}\n    {{- \"List[\" +  json_to_python_type(json_spec.items) + \"]\"}}\n{%- elif json_spec.type == \"object\" %}\n    {{- \"Dict[str, \" + json_to_python_type(json_spec.additionalProperties) + '']''}}\n{%- elif json_spec.type is iterable %}\n    {{- \"Union[\" }}\n    {%- for t in json_spec.type %}\n      {{- json_to_python_type({\"type\": t}) }}\n      {%- if not loop.last %}\n        {{- \",\" }} \n    {%- endif %}\n    {%- endfor %}\n    {{- \"]\" }}\n{%- else %}\n    {{- \"Any\" }}\n{%- endif %}\n{%- endmacro %}\n\n{%- macro old_tool_parser(tools) %}\n{%- for tool in tools %}\n    {%- if loop.index0 != 0 %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- ''```python\\ndef '' + tool.name + ''('' }}\n    {%- for param_name, param_fields in tool.parameter_definitions|items %}\n        {%- if loop.index0 != 0 %}\n            {{- '', ''}}\n        {%- endif %}\n        {{- param_name + '': '' }}\n        {%- if not param_fields.required %}\n            {{- ''Optional['' + param_fields.type + ''] = None''}}\n        {%- else %}\n            {{- param_fields.type }}\n        {%- endif %}\n    {%- endfor %}\n    {{- '') -> List[Dict]:\\n    \"\"\"''}}\n    {{- tool.description }}\n    {%- if tool.parameter_definitions|length != 0 %}\n        {{- ''\\n\\n    Args:\\n        ''}}\n        {%- for param_name, param_fields in tool.parameter_definitions|items %}\n            {%- if loop.index0 != 0 %}\n                {{- ''\\n        '' }}\n            {%- endif %}\n            {{- param_name + '' (''}}\n            {%- if not param_fields.required %}\n                {{- ''Optional['' + param_fields.type + '']''}}\n            {%- else %}\n                {{- param_fields.type }}\n            {%- endif %}\n            {{- ''): '' + param_fields.description }}\n        {%- endfor %}\n    {%- endif %}\n    {{- ''\\n    \"\"\"\\n    pass\\n```'' }}\n{%- endfor %}\n{%- endmacro %}\n\n{%- macro new_tool_parser(tools) %}\n{%- for tool in tools %}\n  {%- if loop.index0 != 0 %}\n    {{- ''\\n\\n''}}\n  {%- endif %}\n  {%- if tool.function is defined %}\n    {%- set tool = tool.function %}\n  {%- endif %}\n  {{-''```python\ndef '' + tool.name + ''(''}}\n  {%- for param_name, param_fields in tool.parameters.properties|items %}\n    {%- if loop.index0 != 0 %}\n      {{- '', ''}}\n    {%- endif %}\n    {{-param_name + \": \"}} \n    {%- if not param_name in tool.parameters.required %}\n      {{-''Optional['' + json_to_python_type(param_fields) + ''] = None''}}\n    {%- else %}\n      {{- json_to_python_type(param_fields) }}\n    {%- endif %}\n  {%- endfor %}\n  {{- '') -> List[Dict]:\n    \"\"\"''}}\n  {{- tool.description }}\n  {%- if tool.parameters.properties|length != 0 %}\n    {{- ''\\n\\n    Args:\\n        ''}}\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\n      {%- if loop.index0 != 0 %}\n        {{- ''\\n        '' }}\n      {%- endif %}\n      {{- param_name + '' (''}}\n      {%- if not param_name in tool.parameters.required %}\n        {{-''Optional['' + json_to_python_type(param_fields) + '']''}}\n      {%- else %}\n        {{- json_to_python_type(param_fields) }}\n      {%- endif %}\n      {{- ''): '' + param_fields.description }}\n    {%- endfor %}\n    {%- endif %}\n    {{- ''\\n    \"\"\"\\n    pass\\n```'' }}\n{%- endfor %}\n{%- endmacro %}\n\n{{- bos_token }}\n{%- if messages[0][''role''] == ''system'' %}\n  {%- set loop_messages = messages[1:] %}\n  {%- set system_message = messages[0][''content''] %}\n{%- else %}\n  {%- set loop_messages = messages %}\n  {%- set system_message = ''## Task and Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\''s needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.'' %}\n{%- endif %}\n{{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}\n{{- ''# Safety Preamble'' }}\n{{- ''\nThe instructions in this section override those in the task description and style guide sections. Don\\''t answer questions that are harmful or immoral.'' }}\n{{- ''\n\n# System Preamble'' }}\n{{- ''\n## Basic Rules'' }}\n{{- ''\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\''s requests, you cite your sources in your answers, according to those instructions.'' }}\n{{- ''\n\n# User Preamble'' }}\n{{- ''\n'' + system_message }}\n{{-''\n\n## Available Tools\nHere is a list of tools that you have available to you:\n\n''}}\n{%- set ns = namespace(new_tools=true) %}\n{%- for tool in tools %}\n    {%- if tool.parameter_definitions is defined %}\n        {%- set ns.new_tools = false %}\n    {%- endif %}\n{%- endfor %}\n{%- if ns.new_tools %}\n    {{- new_tool_parser(tools) }}\n{%- else %}\n    {{- old_tool_parser(tools) }}\n{%- endif %}\n{{- ''<|END_OF_TURN_TOKEN|>''}}\n{%- for message in loop_messages %}\n  {%- set content = message[''content''] %}\n  {%- if message.role == ''user'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''system'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''assistant'' and message.tool_calls is defined %}\n    {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}\n    {%- if message.content is defined %}\n        {{- message.content|trim }}\n    {%- endif %}\n    {{- ''\\nAction:\\n```json\\n[\\n'' }}\n    {%- for tool_call in message.tool_calls %}\n        {%- if tool_call.function is defined %}\n            {%- set tool_call = tool_call.function %}\n        {%- endif %}\n        {{- ''{\\n''|indent(4, first=true) }}\n        {{- ''\"tool_name\": \"''|indent(8, first=true) + tool_call.name + ''\",\\n'' }}\n        {{- ''\"parameters\": ''|indent(8, first=true) }}\n        {%- if tool_call.arguments is defined and tool_call.arguments|length > 0 %}    \n            {{- tool_call.arguments|tojson(indent=4)|indent(8) }}\n            {{- ''\\n'' }}\n        {%- else %}\n            {{- ''{}\\n'' }}\n        {%- endif %}\n        {{- ''}''|indent(4, first=true) }}\n        {%- if not loop.last %}\n            {{- '',\\n'' }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \"\\n]```\\n\" }}\n  {%- elif message.role == ''assistant'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''tool'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\\n'' }}\n    {{- message.content|trim }}\n    {{- ''</results><|END_OF_TURN_TOKEN|>'' }}\n  {%- endif %}\n{%- endfor %}\n{{-''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\''Action:\\'' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\''s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n```json\n[\n    {\n        \"tool_name\": title of the tool in the specification,\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n    }\n]```<|END_OF_TURN_TOKEN|>''}}\n{%- if add_generation_prompt %}\n  {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}\n{%- endif %}\n"},{"name":"rag","template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = ''## Task and Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\''s needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.'' %}{% endif %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}{{ ''# Safety Preamble'' }}{{ ''\nThe instructions in this section override those in the task description and style guide sections. Don\\''t answer questions that are harmful or immoral.'' }}{{ ''\n\n# System Preamble'' }}{{ ''\n## Basic Rules'' }}{{ ''\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\''s requests, you cite your sources in your answers, according to those instructions.'' }}{{ ''\n\n# User Preamble'' }}{{ ''\n'' + system_message }}{{ ''<|END_OF_TURN_TOKEN|>''}}{% for message in loop_messages %}{% set content = message[''content''] %}{% if message[''role''] == ''user'' %}{{ ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''system'' %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''assistant'' %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% endfor %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>''}}{{ ''<results>'' }}{% for document in documents %}{{ ''\nDocument: '' }}{{ loop.index0 }}\n{% for key, value in document.items() %}{{ key }}: {{value}}\n{% endfor %}{% endfor %}{{ ''</results>''}}{{ ''<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}{{ ''Carefully perform the following instructions, in order, starting each with a new line.\n'' }}{{ ''Firstly, Decide which of the retrieved documents are relevant to the user\\''s last input by writing \\''Relevant Documents:\\'' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\''None\\''.\n'' }}{{ ''Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\''s last input by writing \\''Cited Documents:\\'' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\''None\\''.\n'' }}{% if citation_mode==''accurate'' %}{{ ''Thirdly, Write \\''Answer:\\'' followed by a response to the user\\''s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\n'' }}{% endif %}{{ ''Finally, Write \\''Grounded answer:\\'' followed by a response to the user\\''s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.'' }}{{ ''<|END_OF_TURN_TOKEN|>'' }}{% if add_generation_prompt %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}{% endif %}"}],"eos_token":"<|END_OF_TURN_TOKEN|>","pad_token":"<PAD>","unk_token":null,"use_default_system_prompt":false}}}', '[]', '[]', NULL, 'CC-BY-NC-4.0', 'approved', 40, 'bc3c9fb88c7161582a81626382adff45', NULL, NULL, CURRENT_TIMESTAMP);
