/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Zero', 'huggingface--deepseek-ai--deepseek-r1-zero', 'DeepSeek-R1-Zero', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 937, 5016, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688586727753,"files_count":174,"spaces_count":60,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\\n\\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and ''tool_calls'' in message %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{''<ÔΩúAssistantÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and ''tool_calls'' not in message %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 99.7, '6d6179a928a64505caad38d34a3913ed', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Zero from https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Zero.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stable-diffusion-v1-5-stable-diffusion-v1-5', 'huggingface--stable-diffusion-v1-5--stable-diffusion-v1-5', 'stable-diffusion-v1-5', 'stable-diffusion-v1-5', '--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image inference: true --- Modifications to the original model card are in <span style="color:crimson">red</span> or <span style="color:darkgreen">green</span> Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at ü§ó''s Stable Diffusion blog. The ...', '["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","arxiv:2207.12598","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:1910.09700","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 933, 2118840, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\ninference: true\n---\n\n# Stable Diffusion v1-5 Model Card\n\n### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style="color:crimson">red</span> or <span style="color:darkgreen">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó''s Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style="color:crimson">now deprecated</span>), <span style="color:darkgreen">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "sd-legacy/stable-diffusion-v1-5"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a photo of an astronaut riding a horse on mars"\nimage = pipe(prompt).images[0]  \n    \nimage.save("astronaut_rides_horse.png")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style="color:crimson">(now deprecated)</span>, <span style="color:darkgreen">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style="color:crimson">(now deprecated)</span>\n\n3. <span style="color:darkgreen">Use locally with <a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI</a>, <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111</a>, <a href="https://github.com/vladmandic/automatic">SD.Next</a>, <a href="https://github.com/invoke-ai/InvokeAI">InvokeAI</a></span>\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n\n### Bias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n### Safety Module\n\nThe intended use of this model is with the [Safety Checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) in Diffusers. \nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the `CLIPTextModel` *after generation* of the images. \nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1-5 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nCurrently six Stable Diffusion checkpoints are provided, which were trained as follows.\n- [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1): 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2): Resumed from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on "laion-improved-aesthetics" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3): Resumed from `stable-diffusion-v1-2` - 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2` - 225,000 steps at resolution `512x512` on "laion-aesthetics v2 5+" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-5`](https://huggingface.co/sd-legacy/stable-diffusion-v1-5) Resumed from `stable-diffusion-v1-2` - 595,000 steps at resolution `512x512` on "laion-aesthetics v2 5+" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-inpainting`](https://huggingface.co/sd-legacy/stable-diffusion-inpainting) Resumed from `stable-diffusion-v1-5` - then 440,000 steps of inpainting training at resolution 512x512 on ‚Äúlaion-aesthetics v2 5+‚Äù and 10% dropping of the text-conditioning. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PNDM/PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-1-to-v1-5.png)\n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":49898615889,"files_count":36,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:runwayml:stable-diffusion","source_url":"https://github.com/runwayml/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion"},{"type":"has_code","target_id":"github:runwayml:stable-diffusion","source_url":"https://github.com/runwayml/stable-diffusion"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI\">ComfyUI<","source_url":"https://github.com/comfyanonymous/ComfyUI\">ComfyUI<"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui\">AUTOMATIC1111<","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111<"},{"type":"has_code","target_id":"github:vladmandic:automatic\">SD.Next<","source_url":"https://github.com/vladmandic/automatic\">SD.Next<"},{"type":"has_code","target_id":"github:invoke-ai:InvokeAI\">InvokeAI<","source_url":"https://github.com/invoke-ai/InvokeAI\">InvokeAI<"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:christophschuhmann:improved-aesthetic-predictor","source_url":"https://github.com/christophschuhmann/improved-aesthetic-predictor"},{"type":"based_on_paper","target_id":"arxiv:2207.12598","source_url":"https://arxiv.org/abs/2207.12598"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:2205.11487","source_url":"https://arxiv.org/abs/2205.11487"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'creativeml-openrail-m', 'approved', 79.7, 'a1d2a53990eac9257d8dc73b1875bcbd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3.5-mini-instruct', 'huggingface--microsoft--phi-3.5-mini-instruct', 'Phi-3.5-mini-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? library_name: transformers --- üéâ**Phi-4**: [multimodal-instruct | onnx]; [mini-instruct | onnx] Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic d...', '["transformers","safetensors","phi3","text-generation","nlp","code","conversational","custom_code","multilingual","arxiv:2404.14219","arxiv:2407.13833","arxiv:2403.06412","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 932, 380469, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3.5-mini-instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\nlibrary_name: transformers\n---\nüéâ**Phi-4**: [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n## Model Summary\n\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nüè° [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\nüì∞ [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\nüìñ [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\nüë©‚Äçüç≥ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\nüñ•Ô∏è [Try It](https://aka.ms/try-phi3.5mini) <br>\n\n**Phi-3.5**: [[mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-3.5-mini-instruct-onnx)]; [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct); [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Release Notes \n\nThis is an update over the June 2024 instruction-tuned Phi-3 Mini release based on valuable user feedback. The model used additional post-training data leading to substantial gains on multilingual, multi-turn conversation quality, and reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community.\n\n### Multilingual\n\nThe table below highlights multilingual capability of the Phi-3.5 Mini on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 3.8B active parameters, the model is  competitive on multilingual tasks in comparison to other models with a much bigger active parameters.  \n\n| Benchmark                  | Phi-3.5 Mini-Ins | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------------------|------------------|-----------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Multilingual MMLU          | 55.4             | 51.08                 | 47.4                     | 58.9                      | 56.2             | 63.8           | 77.2             | 72.9                          |\n| Multilingual MMLU-Pro      | 30.9             | 30.21                 | 15.0                     | 34.0                      | 21.4             | 43.0           | 57.9             | 53.2                          |\n| MGSM                       | 47.9             | 41.56                 | 31.8                     | 63.3                      | 56.7             | 75.1           | 75.8             | 81.7                          |\n| MEGA MLQA                  | 61.7             | 55.5                  | 43.9                     | 61.2                      | 45.2             | 54.4           | 61.6             | 70.0                          |\n| MEGA TyDi QA               | 62.2             | 55.9                  | 54.0                     | 63.7                      | 54.5             | 65.6           | 63.6             | 81.8                          |\n| MEGA UDPOS                 | 46.5             | 48.1                  | 57.2                     | 58.2                      | 54.1             | 56.6           | 62.4             | 66.0                          |\n| MEGA XCOPA                 | 63.1             | 62.4                  | 58.8                     | 10.8                      | 21.1             | 31.2           | 95.0             | 90.3                          |\n| MEGA XStoryCloze           | 73.5             | 73.6                  | 75.5                     | 92.3                      | 71.0             | 87.0           | 20.7             | 96.6                          |\n| **Average** | **55.2** | **52.3** | **47.9** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\nThe table below shows Multilingual MMLU scores in some of the supported languages. For more multi-lingual benchmarks and details, see [Appendix A](#appendix-a).\n\n| Benchmark | Phi-3.5 Mini-Ins | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|------------------|-----------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 44.2             | 35.4                  | 33.7                     | 45.3                      | 49.1             | 56.3           | 73.6             | 67.1                          |\n| Chinese   | 52.6             | 46.9                  | 45.9                     | 58.2                      | 54.4             | 62.7           | 66.7             | 70.8                          |\n| Dutch     | 57.7             | 48.0                  | 51.3                     | 60.1                      | 55.9             | 66.7           | 80.6             | 74.2                          |\n| French    | 61.1             | 61.7                  | 53.0                     | 63.8                      | 62.8             | 67.0           | 82.9             | 75.6                          |\n| German    | 62.4             | 61.3                  | 50.1                     | 64.5                      | 59.9             | 65.7           | 79.5             | 74.3                          |\n| Italian   | 62.8             | 63.1                  | 52.5                     | 64.1                      | 55.9             | 65.7           | 82.6             | 75.9                          |\n| Russian   | 50.4             | 45.3                  | 48.9                     | 59.0                      | 57.4             | 63.2           | 78.7             | 72.6                          |\n| Spanish   | 62.6             | 61.3                  | 53.9                     | 64.3                      | 62.6             | 66.0           | 80.0             | 75.5                          |\n| Ukrainian | 45.2             | 36.7                  | 46.9                     | 56.6                      | 52.9             | 62.0           | 77.4             | 72.6                          |\n\n### Long Context\n\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. We see that Phi-3.5-mini is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-mini is competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, Mistral-7B-instruct-v0.3, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-mini-instruct | Llama-3.1-8B-instruct | Mistral-7B-instruct-v0.3 | Mistral-Nemo-12B-instruct-2407 | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| GovReport | 25.9 | 25.1 | 26.0 | 25.6 | 27.8 | 24.8 |\n| QMSum | 21.3 | 21.6 | 21.3 | 22.1 | 24.0 | 21.7 |\n| Qasper | 41.9 | 37.2 | 31.4 | 30.7 | 43.5 | 39.8 |\n| SQuALITY | 25.3 | 26.2 | 25.9 | 25.8 | 23.5 | 23.8 |\n| SummScreenFD | 16.0 | 17.6 | 17.5 | 18.2 | 16.3 | 17.0 |\n| **Average** | **26.1** | **25.5** | **24.4** | **24.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 94.3 | 91.1 | 90.7 | 87.1 | 78.0 | 63.6 | **84.1** |\n| **Llama-3.1-8B-instruct** | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| **Mistral-Nemo-12B-instruct-2407** | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 86 | 67 | 73 | 77 | 82 | **77** |\n| **Llama-3.1-8B-instruct** | 80 | 65 | 73 | 76 | 63 | **71** |\n| **Mistral-7B-instruct-v0.3** | 61 | 57 | 51 | 61 | 80 | **62** |\n\n## Usage\n\n### Requirements\nPhi-3 family has been integrated in the `4.43.0` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.43.0\n```\n\nPhi-3.5-mini-instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3.5mini)\n\n### Tokenizer\n\nPhi-3.5-mini-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n\n### Input Formats\nGiven the nature of the training data, the Phi-3.5-mini-instruct model is best suited for prompts using the chat format as follows:\n\n```\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\n### Loading the model locally\nAfter obtaining the Phi-3.5-mini-instruct model checkpoint, users can use this sample code for inference.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "microsoft/Phi-3.5-mini-instruct", \n    device_map="cuda", \n    torch_dtype="auto", \n    trust_remote_code=True, \n)\ntokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-mini-instruct")\n\nmessages = [\n    {"role": "system", "content": "You are a helpful AI assistant."},\n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"},\n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."},\n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"},\n]\n\npipe = pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    "max_new_tokens": 500,\n    "return_full_text": False,\n    "temperature": 0.0,\n    "do_sample": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0][''generated_text''])\n```\n\nNotes: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation="flash_attention_2"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n+ Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n+ Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:  \n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n**Architecture:** Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini.<br>\n**Inputs:** Text. It is best suited for prompts using chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 512 H100-80G<br>\n**Training time:** 10 days<br>\n**Training data:** 3.4T tokens<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between June and August 2024<br>\n**Status:** This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.<br>\n**Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n**Release date:** August 2024<br>\n\n### Training Datasets\nOur training data includes a wide variety of sources, totaling 3.4 trillion tokens, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3.5-mini on standard open-source benchmarks measuring the model''s reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7B-Instruct-v0.3,  Mistral-Nemo-12B-Ins-2407, Llama-3.1-8B-Ins, Gemma-2-9B-Ins, Gemini 1.5 Flash, and GPT-4o-mini-2024-07-18 (Chat).\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k‚Äìshot examples is listed per-benchmark. At the high-level overview of the model quality on representative benchmarks: \n\n| Category       | Benchmark                | Phi-3.5 Mini-Ins | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------|--------------------------|------------------|--------------------------|---------------------------|------------------|----------------|------------------|------------------------------|\n| Popular aggregated benchmark | Arena Hard | 37 | 18.1 | 39.4 | 25.7 | 42 | 55.2 | 75 |\n|                | BigBench Hard CoT (0-shot) | 69 | 33.4 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n|                | MMLU (5-shot) | 69 | 60.3 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n|                | MMLU-Pro (0-shot, CoT) | 47.4 | 18 | 40.7 | 44 | 50.1 | 57.2 | 62.8 |\n| Reasoning      | ARC Challenge (10-shot) | 84.6 | 77.9 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n|                | BoolQ (2-shot) | 78 | 80.5 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n|                | GPQA (0-shot, CoT) | 30.4 | 15.6 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n|                | HellaSwag (5-shot) | 69.4 | 71.6 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n|                | OpenBookQA (10-shot) | 79.2 | 78 | 84.4 | 84.8 | 89.6 | 89 | 90 |\n|                | PIQA (5-shot) | 81 | 73.4 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n|                | Social IQA (5-shot) | 74.7 | 73 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n|                | TruthfulQA (MC2) (10-shot) | 64 | 64.7 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n|                | WinoGrande (5-shot) | 68.5 | 58.1 | 70.4 | 64.7 | 74 | 74.7 | 76.9 |\n| Multilingual   | Multilingual MMLU (5-shot) | 55.4 | 47.4 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n|                | MGSM (0-shot CoT) | 47.9 | 31.8 | 63.3 | 56.7 | 76.4 | 75.8 | 81.7 |\n| Math           | GSM8K (8-shot, CoT) | 86.2 | 54.4 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n|                | MATH (0-shot, CoT) | 48.5 | 19 | 31.2 | 47.6 | 50.9 | 38 | 70.2 |\n| Long context   | Qasper | 41.9 | 31.4 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n|                | SQuALITY | 24.3 | 25.9 | 25.8 | 26.2 | 0 | 23.5 | 23.8 |\n| Code Generation| HumanEval (0-shot) | 62.8 | 35.4 | 63.4 | 66.5 | 61 | 74.4 | 86.6 |\n|                | MBPP (3-shot) | 69.6 | 50.4 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **61.4** | **48.5** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across public benchmark datasets at the table below:\n\n| Category                   | Phi-3.5 Mini-Ins | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------------------|------------------|--------------------------|---------------------------|------------------|----------------|------------------|------------------------------|\n| Popular aggregated benchmark | 55.6 | 32.5 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning                  | 70.1 | 65.2 | 72.2 | 70.5 | 75.4 | 77.7 | 80 |\n| Language understanding     | 62.6 | 62.8 | 67 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness                 | 59.7 | 53.4 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context               | 26.1 | 25.5 | 24.4 | 24.5 | 0 | 27 | 25.4 |\n| Math                       | 67.4 | 36.7 | 57.7 | 65 | 67.9 | 60.2 | 80.8 |\n| Code generation            | 62 | 43.1 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual               | 55.2 | 47.9 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models.\nHowever, it is still fundamentally limited by its size for certain tasks. \nThe model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. \nHowever, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.  \n\n## Safety Evaluation and Red-Teaming\n\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to \nevaluate Phi-3.5 models'' propensity to produce undesirable outputs across multiple languages and risk categories. \nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety \npost-training that was done as detailed in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833) had a positive impact across multiple languages and risk categories as observed by \nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted \nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as \nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833). \nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output \nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings \nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, \nand risk areas that account for cultural nuances where those languages are spoken.\n\n\n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3.5-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation="eager"\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØ[Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\n\n\n## Appendix A\n\n#### MGSM\n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|------------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| German    | 69.6                   | 65.2                                  | 42.4                     | 74.4                      | 68.4             | 76.8           | 81.6             | 82.8                          |\n| English   | 85.2                   | 83.2                                  | 60.0                     | 86.0                      | 81.2             | 88.8           | 90.8             | 90.8                          |\n| Spanish   | 79.2                   | 77.6                                  | 46.4                     | 75.6                      | 66.4             | 82.4           | 84.8             | 86.8                          |\n| French    | 71.6                   | 72.8                                  | 47.2                     | 70.4                      | 66.8             | 74.4           | 77.2             | 81.6                          |\n| Japanese  | 50.0                   | 35.2                                  | 22.8                     | 62.4                      | 49.2             | 67.6           | 77.6             | 80.4                          |\n| Russian   | 67.2                   | 51.6                                  | 43.2                     | 73.6                      | 67.2             | 78.4           | 84.8             | 86.4                          |\n| Thai      | 29.6                   | 6.4                                   | 18.4                     | 53.2                      | 56.0             | 76.8           | 87.6             | 81.6                          |\n| Chinese   | 60.0                   | 52.8                                  | 42.4                     | 66.4                      | 68.0             | 72.8           | 82.0             | 82.0                          |\n\n#### Multilingual MMLU-pro \n\n| Languages  | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|------------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Czech      | 24.9                  | 26.3                                  | 14.6                     | 30.6                      | 23.0             | 40.5           | 59.0             | 40.9                          |\n| English    | 47.7                  | 46.2                                  | 17.7                     | 39.8                      | 43.1             | 49.0           | 66.1             | 62.7                          |\n| Finnish    | 22.3                  | 20.5                                  | 11.5                     | 30.4                      | 9.7              | 37.5           | 54.5             | 50.1                          |\n| Norwegian  | 29.9                  | 27.8                                  | 14.4                     | 33.2                      | 22.2             | 44.4           | 60.7             | 59.1                          |\n| Polish     | 25.7                  | 26.4                                  | 16.3                     | 33.6                      | 9.2              | 41.7           | 53.9             | 42.8                          |\n| Portuguese | 38.7                  | 37.6                                  | 15.3                     | 36.0                      | 29.3             | 43.5           | 54.0             | 56.9                          |\n| Swedish    | 30.7                  | 28.1                                  | 15.5                     | 34.3                      | 16.9             | 42.6           | 57.7             | 55.5                          |\n\n#### MEGA \n\n##### MLQA\n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 54.3                  | 32.7                                  | 23.5                     | 31.4                      | 31.5             | 57.4           | 63.8             | 64.0                          |\n| Chinese   | 36.1                  | 31.8                                  | 22.4                     | 27.4                      | 18.6             | 45.4           | 38.1             | 38.9                          |\n| English   | 80.3                  | 78.9                                  | 68.2                     | 75.5                      | 67.2             | 82.9           | 69.5             | 82.2                          |\n| German    | 61.8                  | 59.1                                  | 49.0                     | 57.8                      | 38.9             | 63.8           | 55.9             | 64.1                          |\n| Spanish   | 68.8                  | 67.0                                  | 50.3                     | 63.6                      | 52.7             | 72.8           | 59.6             | 70.1                          |\n\n##### TyDi QA \n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 69.7                  | 54.4                                  | 52.5                     | 49.8                      | 33.7             | 81.1           | 78.8             | 84.9                          |\n| English   | 82.0                  | 82.0                                  | 60.5                     | 77.3                      | 65.1             | 82.4           | 60.9             | 81.8                          |\n| Finnish   | 70.3                  | 64.3                                  | 68.6                     | 57.1                      | 74.4             | 85.7           | 73.5             | 84.8                          |\n| Japanese  | 65.4                  | 56.7                                  | 45.3                     | 54.8                      | 34.1             | 74.6           | 59.7             | 73.3                          |\n| Korean    | 74.0                  | 60.4                                  | 54.5                     | 54.2                      | 54.9             | 83.8           | 60.7             | 82.3                          |\n| Russian   | 63.5                  | 62.7                                  | 52.3                     | 55.7                      | 27.4             | 69.8           | 60.1             | 72.5                          |\n| Thai      | 64.4                  | 49.0                                  | 51.8                     | 43.5                      | 48.5             | 81.4           | 71.6             | 78.2                          |\n\n##### XCOPA \n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| English   | 94.6                  | 94.6                                  | 85.6                     | 94.4                      | 37.6             | 63.8           | 92.0             | 98.2                          |\n| Italian   | 86.8                  | 84.8                                  | 76.8                     | 83.2                      | 16.2             | 37.2           | 85.6             | 97.6                          |\n| Turkish   | 58.6                  | 57.2                                  | 61.6                     | 56.6                      | 38.4             | 60.2           | 91.4             | 94.6                          |\n\n\n## Appendix B: Korean benchmarks\n\nThe prompt is the same as the [CLIcK paper](https://arxiv.org/abs/2403.06412) prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\n\n- GPT-4o: 2024-05-13 version\n- GPT-4o-mini: 2024-07-18 version\n- GPT-4-turbo: 2024-04-09 version\n- GPT-3.5-turbo: 2023-06-13 version\n\nThe overall Korean benchmarks show that the Phi-3.5-Mini-Instruct with only 3.8B params outperforms Llama-3.1-8B-Instruct.\n\n| Benchmarks               |   Phi-3.5-Mini-Instruct |  Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:-------------------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| CLIcK                    |                   42.99 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n| HAERAE 1.0               |                   44.21 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n| KMMLU (0-shot, CoT)      |                   35.87 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n| KMMLU (5-shot)           |                   37.35 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n| KMMLU-HARD (0-shot, CoT) |                   24    |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 | \n| KMMLU-HARD (5-shot)      |                   24.76 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |\n| **Average**                  |                   **35.62** |                           **29.99** |                   **29.29** |    **62.54** |         **50.08** |         **56.74** |           **39.61** | \n\n#### CLIcK (Cultural and Linguistic Intelligence in Korean)\n\n##### Accuracy by supercategory\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         |                   43.77 |                           29.74 |                   51.15 |    81.89 |         70.95 |         73.61 |           53.38 |\n| Language        |                   41.38 |                           27.85 |                   40.92 |    77.54 |         63.54 |         71.23 |           46    |\n| **Overall**     |                   42.99 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n\n##### Accuracy by category\n| supercategory   | category    |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|:------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         | Economy     |                   61.02 |                           28.81 |                   66.1  |    94.92 |         83.05 |         89.83 |           64.41 |\n| Culture         | Geography   |                   45.8  |                           29.01 |                   54.2  |    80.15 |         77.86 |         82.44 |           53.44 |\n| Culture         | History     |                   26.15 |                           30    |                   29.64 |    66.92 |         48.4  |         46.4  |           31.79 |\n| Culture         | Law         |                   32.42 |                           22.83 |                   44.29 |    70.78 |         57.53 |         61.19 |           41.55 |\n| Culture         | Politics    |                   54.76 |                           33.33 |                   59.52 |    88.1  |         83.33 |         89.29 |           65.48 |\n| Culture         | Pop Culture |                   60.98 |                           34.15 |                   60.98 |    97.56 |         85.37 |         92.68 |           75.61 |\n| Culture         | Society     |                   54.37 |                           31.72 |                   65.05 |    92.88 |         85.44 |         86.73 |           71.2  |\n| Culture         | Tradition   |                   47.75 |                           31.98 |                   54.95 |    87.39 |         74.77 |         79.28 |           55.86 |\n| Language        | Functional  |                   37.6  |                           24    |                   32.8  |    84.8  |         64.8  |         80    |           40    |\n| Language        | Grammar     |                   27.5  |                           23.33 |                   22.92 |    57.08 |         42.5  |         47.5  |           30    |\n| Language        | Textual     |                   54.74 |                           33.33 |                   59.65 |    91.58 |         80.7  |         87.37 |           62.11 |\n\n#### HAERAE\n\n| category              |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| General Knowledge     |                   31.25 |                           28.41 |                   34.66 |    77.27 |         53.41 |         66.48 |           40.91 |\n| History               |                   32.45 |                           22.34 |                   44.15 |    92.02 |         84.57 |         78.72 |           30.32 |\n| Loan Words            |                   47.93 |                           35.5  |                   63.31 |    79.88 |         76.33 |         78.11 |           59.17 |\n| Rare Words            |                   55.06 |                           42.96 |                   63.21 |    87.9  |         81.98 |         79.01 |           61.23 |\n| Reading Comprehension |                   42.95 |                           41.16 |                   51.9  |    85.46 |         77.18 |         80.09 |           56.15 |\n| Standard Nomenclature |                   44.44 |                           32.68 |                   58.82 |    88.89 |         75.82 |         79.08 |           53.59 |\n| **Overall**           |                   44.21 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n\n#### KMMLU (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   35.8  |                           31.68 |                   37.03 |    61.52 |         49.29 |         55.98 |           38.47 |\n| HUMSS           |                   31.56 |                           26.47 |                   37.29 |    69.45 |         56.59 |         63    |           40.9  |\n| Other           |                   35.45 |                           31.01 |                   39.15 |    63.79 |         52.35 |         57.53 |           40.19 |\n| STEM            |                   38.54 |                           31.9  |                   40.42 |    65.16 |         54.74 |         60.84 |           42.24 |\n| **Overall**     |                   35.87 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n\n#### KMMLU (5-shot)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   37.42 |                           29.98 |                   19.24 |    61.47 |         48.66 |         56.85 |           40.22 |\n| HUMSS           |                   34.72 |                           27.27 |                   22.5  |    68.79 |         55.95 |         63.68 |           43.35 |\n| Other           |                   37.04 |                           30.76 |                   20.95 |    64.21 |         51.1  |         57.85 |           41.92 |\n| STEM            |                   38.9  |                           30.73 |                   19.55 |    65.28 |         53.29 |         61.08 |           44.43 |\n| **Overall**     |                   37.35 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n\n#### KMMLU-HARD (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   27.08 |                           26.17 |                   26.25 |    37.12 |         22.25 |         29.17 |           21.07 |\n| HUMSS           |                   20.21 |                           24.38 |                   20.21 |    41.97 |         23.31 |         31.51 |           19.44 |\n| Other           |                   23.05 |                           24.82 |                   23.88 |    40.39 |         26.48 |         29.59 |           22.22 |\n| STEM            |                   24.36 |                           26.91 |                   24.64 |    39.82 |         26.36 |         32.18 |           20.91 |\n| **Overall**     |                   24    |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n\n#### KMMLU-HARD (5-shot)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   25    |                           29    |                   12    |    31    |         21    |         25    |           20    |\n| HUMSS           |                   21.89 |                           19.92 |                   14    |    43.98 |         23.47 |         33.53 |           19.53 |\n| Other           |                   23.26 |                           27.27 |                   12.83 |    39.84 |         28.34 |         29.68 |           23.22 |\n| STEM            |                   20.5  |                           25.25 |                   12.75 |    40.25 |         23.25 |         27.25 |           19.75 |\n| **Overall**     |                   24.76 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3821079552,"storage_bytes":7642681603,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Phi3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi3.Phi3Config","AutoModelForCausalLM":"modeling_phi3.Phi3ForCausalLM"},"model_type":"phi3","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' and message[''content''] %}{{''<|system|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''user'' %}{{''<|user|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''assistant'' %}{{''<|assistant|>\n'' + message[''content''] + ''<|end|>\n''}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>\n'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2404.14219","source_url":"https://arxiv.org/abs/2404.14219"},{"type":"based_on_paper","target_id":"arxiv:2407.13833","source_url":"https://arxiv.org/abs/2407.13833"},{"type":"based_on_paper","target_id":"arxiv:2403.06412","source_url":"https://arxiv.org/abs/2403.06412"}]', NULL, 'MIT', 'approved', 79.7, 'd54cff83d717d604469007ee3ebc7562', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-7B-Instruct', 'huggingface--qwen--qwen2.5-7b-instruct', 'Qwen2.5-7B-Instruct', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-7B tags: - chat library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen2.5 is the latest series of Qwen large la...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","arxiv:2407.10671","base_model:qwen/qwen2.5-7b","base_model:finetune:qwen/qwen2.5-7b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 932, 7245333, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-7B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-7B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-7B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":15231271888,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 64.7, 'beb786d83a3a7728edad444d5ad99d6b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-guoyww-animatediff', 'huggingface--guoyww--animatediff', 'animatediff', 'guoyww', '--- license: apache-2.0 --- This model repo is for AnimateDiff.', '["license:apache-2.0","region:us"]', 'other', 931, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/guoyww/animatediff","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\n---\nThis model repo is for [AnimateDiff](https://github.com/guoyww/animatediff/).', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":32554557069,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:guoyww:animatediff","source_url":"https://github.com/guoyww/animatediff"}]', NULL, 'Apache-2.0', 'approved', 39.7, '4797596fa8754451ddf56149673c3a49', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Kijai-flux-fp8', 'huggingface--kijai--flux-fp8', 'flux-fp8', 'Kijai', '--- language: - en license: other viewer: false tags: - diffusion-single-file - comfyui --- and weights of: https://huggingface.co/black-forest-labs/FLUX.1-dev is , original upload name kept for backwards compatibility. weights of: https://huggingface.co/black-forest-labs/FLUX.1-schnell https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro flux1-dev-fp8.safetensors, flux_shakker_labs_union_pro-fp8_e4m3fn and it''s variants falls under the Non-Commercial License. flux1-schnell-fp8...', '["diffusion-single-file","comfyui","en","license:other","region:us"]', 'other', 931, 55768, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Kijai/flux-fp8","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n  - en\nlicense: other\nviewer: false\ntags:\n  - diffusion-single-file\n  - comfyui\n---\n`float8_e4m3fn` and `float8_e5m2` weights of:\n\nhttps://huggingface.co/black-forest-labs/FLUX.1-dev\n\n`flux1-dev-fp8.safetensors` is `float8_e4m3fn`, original upload name kept for backwards compatibility.\n\n`float8_e4m3fn` weights of:\n\nhttps://huggingface.co/black-forest-labs/FLUX.1-schnell\n\nhttps://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro\n# License\nflux1-dev-fp8.safetensors, flux_shakker_labs_union_pro-fp8_e4m3fn and it''s variants falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).\n\nflux1-schnell-fp8.safetensors falls under the [Apache-2.0 License](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md).\n', '{"pipeline_tag":null,"library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":74868593126,"files_count":8,"spaces_count":3,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 49.7, 'f41ee74e670c1b1b017cb79142915f70', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepset-roberta-base-squad2', 'huggingface--deepset--roberta-base-squad2', 'roberta-base-squad2', 'deepset', '--- language: en license: cc-by-4.0 datasets: - squad_v2 model-index: - name: deepset/roberta-base-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation metrics: - type: exact_match value: 79.9309 name: Exact Match verified: true verifyToken: >- eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5...', '["transformers","pytorch","tf","jax","rust","safetensors","roberta","question-answering","en","dataset:squad_v2","base_model:facebookai/roberta-base","base_model:finetune:facebookai/roberta-base","license:cc-by-4.0","model-index","endpoints_compatible","deploy:azure","region:us"]', 'question-answering', 930, 733628, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepset/roberta-base-squad2","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/roberta-base-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.9309\n      name: Exact Match\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA\n    - type: f1\n      value: 82.9501\n      name: F1\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ\n    - type: total\n      value: 11869\n      name: total\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.289\n      name: Exact Match\n    - type: f1\n      value: 91.841\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 29.5\n      name: Exact Match\n    - type: f1\n      value: 40.367\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.567\n      name: Exact Match\n    - type: f1\n      value: 84.469\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 69.924\n      name: Exact Match\n    - type: f1\n      value: 83.284\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.204\n      name: Exact Match\n    - type: f1\n      value: 90.595\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.931\n      name: Exact Match\n    - type: f1\n      value: 90.756\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 71.55\n      name: Exact Match\n    - type: f1\n      value: 82.939\n      name: F1\nbase_model:\n- FacebookAI/roberta-base\n---\n\n# roberta-base for Extractive QA \n\nThis is the [roberta-base](https://huggingface.co/roberta-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It''s been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \nWe have also released a distilled version of this model called [deepset/tinyroberta-squad2](https://huggingface.co/deepset/tinyroberta-squad2). It has a comparable prediction quality and runs at twice the speed of [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2).\n\n\n## Overview\n**Language model:** roberta-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = "roberta-base"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai "transformers[torch,sentencepiece]"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content="Python is a popular programming language"),\n    Document(content="python ist eine beliebte Programmiersprache"),\n]\n\nreader = ExtractiveReader(model="deepset/roberta-base-squad2")\nreader.warm_up()\n\nquestion = "What is a popular programming language?"\nresult = reader.run(query=question, documents=docs)\n# {''answers'': [ExtractedAnswer(query=''What is a popular programming language?'', score=0.5740374326705933, data=''python'', document=Document(id=..., content: ''...''), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = "deepset/roberta-base-squad2"\n\n# a) Get predictions\nnlp = pipeline(''question-answering'', model=model_name, tokenizer=model_name)\nQA_input = {\n    ''question'': ''Why is model conversion important?'',\n    ''context'': ''The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.''\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n"exact": 79.87029394424324,\n"f1": 82.91251169582613,\n\n"total": 11873,\n"HasAns_exact": 77.93522267206478,\n"HasAns_f1": 84.02838248389763,\n"HasAns_total": 5928,\n"NoAns_exact": 81.79983179142137,\n"NoAns_f1": 81.79983179142137,\n"NoAns_total": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M√∂ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class="grid lg:grid-cols-2 gap-x-4 gap-y-3">\n    <div class="w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center">\n         <img alt="" src="https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png" class="w-40"/>\n     </div>\n     <div class="w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center">\n         <img alt="" src="https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png" class="w-40"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> repo and <strong><a href="https://docs.haystack.deepset.ai">Documentation</a></strong>. \n\nWe also have a <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we''re hiring!](http://www.deepset.ai/jobs)', '{"pipeline_tag":"question-answering","library_name":"transformers","framework":"transformers","params":124057092,"storage_bytes":3942257079,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["RobertaForQuestionAnswering"],"model_type":"roberta","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack\">GitHub<","source_url":"https://github.com/deepset-ai/haystack\">GitHub<"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"}]', NULL, 'CC-BY-4.0', 'approved', 64.7, '2d959bc022b2c652b46dc6c8942f1a86', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.2-dev', 'huggingface--black-forest-labs--flux.2-dev', 'FLUX.2-dev', 'black-forest-labs', '', '["diffusers","safetensors","image-generation","image-editing","flux","diffusion-single-file","image-to-image","en","license:other","diffusers:flux2pipeline","region:us"]', 'image-to-image', 930, 207838, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.2-dev","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":177640024576,"files_count":39,"spaces_count":49,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"Flux2Pipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.7, '0e632496351c04584f5042bb10115cf3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-seamless-m4t-v2-large', 'huggingface--facebook--seamless-m4t-v2-large', 'seamless-m4t-v2-large', 'facebook', '--- license: cc-by-nc-4.0 language: - af - am - ar - as - az - be - bn - bs - bg - ca - cs - zh - cy - da - de - el - en - et - fi - fr - or - om - ga - gl - gu - ha - he - hi - hr - hu - hy - ig - id - is - it - jv - ja - kn - ka - kk - mn - km - ky - ko - lo - ln - lt - lb - lg - lv - ml - mr - mk - mt - mi - my - nl - nb - ne - ny - oc - pa - ps - fa - pl - pt - ro - ru - sk - sl - sn - sd - so - es - sr - sv - sw - ta - te - tg - tl - th - tr - uk - ur - uz - vi - wo - xh - yo - ms - zu -...', '["transformers","safetensors","seamless_m4t_v2","feature-extraction","audio-to-audio","text-to-speech","seamless_communication","automatic-speech-recognition","af","am","ar","as","az","be","bn","bs","bg","ca","cs","zh","cy","da","de","el","en","et","fi","fr","or","om","ga","gl","gu","ha","he","hi","hr","hu","hy","ig","id","is","it","jv","ja","kn","ka","kk","mn","km","ky","ko","lo","ln","lt","lb","lg","lv","ml","mr","mk","mt","mi","my","nl","nb","ne","ny","oc","pa","ps","fa","pl","pt","ro","ru","sk","sl","sn","sd","so","es","sr","sv","sw","ta","te","tg","tl","th","tr","uk","ur","uz","vi","wo","xh","yo","ms","zu","ary","arz","yue","kea","arxiv:2312.05187","license:cc-by-nc-4.0","region:us"]', 'automatic-speech-recognition', 929, 51645, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/seamless-m4t-v2-large","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlanguage:\n- af\n- am\n- ar\n- as\n- az\n- be\n- bn\n- bs\n- bg\n- ca\n- cs\n- zh\n- cy\n- da\n- de\n- el\n- en\n- et\n- fi\n- fr\n- or\n- om\n- ga\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- ig\n- id\n- is\n- it\n- jv\n- ja\n- kn\n- ka\n- kk\n- mn\n- km\n- ky\n- ko\n- lo\n- ln\n- lt\n- lb\n- lg\n- lv\n- ml\n- mr\n- mk\n- mt\n- mi\n- my\n- nl\n- nb\n- ne\n- ny\n- oc\n- pa\n- ps\n- fa\n- pl\n- pt\n- ro\n- ru\n- sk\n- sl\n- sn\n- sd\n- so\n- es\n- sr\n- sv\n- sw\n- ta\n- te\n- tg\n- tl\n- th\n- tr\n- uk\n- ur\n- uz\n- vi\n- wo\n- xh\n- yo\n- ms\n- zu\n- ary\n- arz\n- yue\n- kea\nmetrics:\n- bleu\n- wer\n- chrf\ninference: False\npipeline_tag: automatic-speech-recognition\ntags:\n  - audio-to-audio\n  - text-to-speech\n  - seamless_communication  \nlibrary_name: transformers\nwidget:\n  - src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n    example_title: Librispeech sample 1\n    output:\n      text: going along slushy country roads and speaking to damp audiences in draughty schoolrooms day after day for a fortnight he''ll have to put in an appearance at some place of worship on sunday morning and he can come to us immediately afterwards\n  - src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n    example_title: Librispeech sample 2\n    output:\n      text: before he had time to answer a much-encumbered vera burst into the room with the question i say can i leave these here these were a small black pig and a lusty specimen of black-red game-cock\n---\n\n# SeamlessM4T v2\n\n**SeamlessM4T** is our foundational all-in-one **M**assively **M**ultilingual and **M**ultimodal **M**achine **T**ranslation model delivering high-quality translation for speech and text in nearly 100 languages.\n\nSeamlessM4T models support the tasks of:\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR).\n\nSeamlessM4T models support:\n- üé§ 101 languages for speech input.\n- üí¨ 96 Languages for text input/output.\n- üîä 35 languages for speech output.\n  \nüåü We are releasing SeamlessM4T v2, an updated version with our novel *UnitY2* architecture. \nThis new model improves over SeamlessM4T v1 in quality as well as inference speed in speech generation tasks.\n\nThe v2 version of SeamlessM4T is a multitask adaptation of our novel *UnitY2* architecture. \n*Unity2* with its hierarchical character-to-unit upsampling and non-autoregressive text-to-unit decoding considerably improves over SeamlessM4T v1 in quality and inference speed.\n\n**SeamlessM4T v2 is also supported by ü§ó Transformers, more on it [in the dedicated section below](#transformers-usage).**\n\n![SeamlessM4T architectures](seamlessm4t_arch.svg)\n\n## SeamlessM4T  models\n| Model Name         | #params | checkpoint                                                                              | metrics                                                                              |\n| ------------------ | ------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n| [SeamlessM4T-Large v2](https://huggingface.co/facebook/seamless-m4t-v2-large)  | 2.3B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-v2-large/blob/main/seamlessM4T_v2_large.pt)   | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large_v2.zip)  |\n| [SeamlessM4T-Large (v1)](https://huggingface.co/facebook/seamless-m4t-large) | 2.3B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-large/blob/main/multitask_unity_large.pt)   | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large.zip)  |\n| [SeamlessM4T-Medium (v1)](https://huggingface.co/facebook/seamless-m4t-medium) | 1.2B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-medium/blob/main/multitask_unity_medium.pt) | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_medium.zip) |\n\nWe provide the extensive evaluation results of seamlessM4T-Large and SeamlessM4T-Medium reported in the paper (as averages) in the `metrics` files above.\n\nThe evaluation data ids for FLEURS, CoVoST2 and CVSS-C can be found [here](https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip)\n\n\n## Evaluating SeamlessM4T models\nTo reproduce our results or to evaluate using the same metrics over your own test sets, please check out the [Evaluation README here](https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/evaluate).\n\n\n## Finetuning SeamlessM4T models\nPlease check out the [Finetuning README here](https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/finetune).\n\n## Transformers usage\n\nSeamlessM4T is available in the ü§ó Transformers library, requiring minimal dependencies. Steps to get started:\n\n1. First install the ü§ó [Transformers library](https://github.com/huggingface/transformers) from main and [sentencepiece](https://github.com/google/sentencepiece):\n\n```\npip install git+https://github.com/huggingface/transformers.git sentencepiece\n```\n\n2. Run the following Python code to generate speech samples. Here the target language is Russian:\n\n```py\nfrom transformers import AutoProcessor, SeamlessM4Tv2Model\nimport torchaudio\n\nprocessor = AutoProcessor.from_pretrained("facebook/seamless-m4t-v2-large")\nmodel = SeamlessM4Tv2Model.from_pretrained("facebook/seamless-m4t-v2-large")\n\n# from text\ntext_inputs = processor(text = "Hello, my dog is cute", src_lang="eng", return_tensors="pt")\naudio_array_from_text = model.generate(**text_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()\n\n# from audio\naudio, orig_freq =  torchaudio.load("https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav")\naudio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\naudio_inputs = processor(audios=audio, return_tensors="pt")\naudio_array_from_audio = model.generate(**audio_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()\n```\n\n3. Listen to the audio samples either in an ipynb notebook:\n\n```py\nfrom IPython.display import Audio\n\nsample_rate = model.config.sampling_rate\nAudio(audio_array_from_text, rate=sample_rate)\n# Audio(audio_array_from_audio, rate=sample_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\n\n```py\nimport scipy\n\nsample_rate = model.config.sampling_rate\nscipy.io.wavfile.write("out_from_text.wav", rate=sample_rate, data=audio_array_from_text)\n# scipy.io.wavfile.write("out_from_audio.wav", rate=sample_rate, data=audio_array_from_audio)\n```\nFor more details on using the SeamlessM4T model for inference using the ü§ó Transformers library, refer to the \n**[SeamlessM4T v2 docs](https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2)** or to this **hands-on [Google Colab](https://colab.research.google.com/github/ylacombe/scripts_and_notebooks/blob/main/v2_seamless_m4t_hugging_face.ipynb).**\n\n\n## Supported Languages:\n\nListed below, are the languages supported by SeamlessM4T-large (v1/v2).\nThe `source` column specifies whether a language is supported as source speech (`Sp`) and/or source text (`Tx`).\nThe `target` column specifies whether a language is supported as target speech (`Sp`) and/or target text (`Tx`).\n\n\n| code | language               | script     | Source | Target |\n| ---- | ---------------------- | ---------- | ------ | ------ |\n| afr  | Afrikaans              | Latn       | Sp, Tx | Tx     |\n| amh  | Amharic                | Ethi       | Sp, Tx | Tx     |\n| arb  | Modern Standard Arabic | Arab       | Sp, Tx | Sp, Tx |\n| ary  | Moroccan Arabic        | Arab       | Sp, Tx | Tx     |\n| arz  | Egyptian Arabic        | Arab       | Sp, Tx | Tx     |\n| asm  | Assamese               | Beng       | Sp, Tx | Tx     |\n| ast  | Asturian               | Latn       | Sp     | \--    |\n| azj  | North Azerbaijani      | Latn       | Sp, Tx | Tx     |\n| bel  | Belarusian             | Cyrl       | Sp, Tx | Tx     |\n| ben  | Bengali                | Beng       | Sp, Tx | Sp, Tx |\n| bos  | Bosnian                | Latn       | Sp, Tx | Tx     |\n| bul  | Bulgarian              | Cyrl       | Sp, Tx | Tx     |\n| cat  | Catalan                | Latn       | Sp, Tx | Sp, Tx |\n| ceb  | Cebuano                | Latn       | Sp, Tx | Tx     |\n| ces  | Czech                  | Latn       | Sp, Tx | Sp, Tx |\n| ckb  | Central Kurdish        | Arab       | Sp, Tx | Tx     |\n| cmn  | Mandarin Chinese       | Hans       | Sp, Tx | Sp, Tx |\n| cmn_Hant  | Mandarin Chinese  | Hant       | Sp, Tx | Sp, Tx |\n| cym  | Welsh                  | Latn       | Sp, Tx | Sp, Tx |\n| dan  | Danish                 | Latn       | Sp, Tx | Sp, Tx |\n| deu  | German                 | Latn       | Sp, Tx | Sp, Tx |\n| ell  | Greek                  | Grek       | Sp, Tx | Tx     |\n| eng  | English                | Latn       | Sp, Tx | Sp, Tx |\n| est  | Estonian               | Latn       | Sp, Tx | Sp, Tx |\n| eus  | Basque                 | Latn       | Sp, Tx | Tx     |\n| fin  | Finnish                | Latn       | Sp, Tx | Sp, Tx |\n| fra  | French                 | Latn       | Sp, Tx | Sp, Tx |\n| fuv  | Nigerian Fulfulde      | Latn       | Sp, Tx | Tx     |\n| gaz  | West Central Oromo     | Latn       | Sp, Tx | Tx     |\n| gle  | Irish                  | Latn       | Sp, Tx | Tx     |\n| glg  | Galician               | Latn       | Sp, Tx | Tx     |\n| guj  | Gujarati               | Gujr       | Sp, Tx | Tx     |\n| heb  | Hebrew                 | Hebr       | Sp, Tx | Tx     |\n| hin  | Hindi                  | Deva       | Sp, Tx | Sp, Tx |\n| hrv  | Croatian               | Latn       | Sp, Tx | Tx     |\n| hun  | Hungarian              | Latn       | Sp, Tx | Tx     |\n| hye  | Armenian               | Armn       | Sp, Tx | Tx     |\n| ibo  | Igbo                   | Latn       | Sp, Tx | Tx     |\n| ind  | Indonesian             | Latn       | Sp, Tx | Sp, Tx |\n| isl  | Icelandic              | Latn       | Sp, Tx | Tx     |\n| ita  | Italian                | Latn       | Sp, Tx | Sp, Tx |\n| jav  | Javanese               | Latn       | Sp, Tx | Tx     |\n| jpn  | Japanese               | Jpan       | Sp, Tx | Sp, Tx |\n| kam  | Kamba                  | Latn       | Sp     | \--    |\n| kan  | Kannada                | Knda       | Sp, Tx | Tx     |\n| kat  | Georgian               | Geor       | Sp, Tx | Tx     |\n| kaz  | Kazakh                 | Cyrl       | Sp, Tx | Tx     |\n| kea  | Kabuverdianu           | Latn       | Sp     | \--    |\n| khk  | Halh Mongolian         | Cyrl       | Sp, Tx | Tx     |\n| khm  | Khmer                  | Khmr       | Sp, Tx | Tx     |\n| kir  | Kyrgyz                 | Cyrl       | Sp, Tx | Tx     |\n| kor  | Korean                 | Kore       | Sp, Tx | Sp, Tx |\n| lao  | Lao                    | Laoo       | Sp, Tx | Tx     |\n| lit  | Lithuanian             | Latn       | Sp, Tx | Tx     |\n| ltz  | Luxembourgish          | Latn       | Sp     | \--    |\n| lug  | Ganda                  | Latn       | Sp, Tx | Tx     |\n| luo  | Luo                    | Latn       | Sp, Tx | Tx     |\n| lvs  | Standard Latvian       | Latn       | Sp, Tx | Tx     |\n| mai  | Maithili               | Deva       | Sp, Tx | Tx     |\n| mal  | Malayalam              | Mlym       | Sp, Tx | Tx     |\n| mar  | Marathi                | Deva       | Sp, Tx | Tx     |\n| mkd  | Macedonian             | Cyrl       | Sp, Tx | Tx     |\n| mlt  | Maltese                | Latn       | Sp, Tx | Sp, Tx |\n| mni  | Meitei                 | Beng       | Sp, Tx | Tx     |\n| mya  | Burmese                | Mymr       | Sp, Tx | Tx     |\n| nld  | Dutch                  | Latn       | Sp, Tx | Sp, Tx |\n| nno  | Norwegian Nynorsk      | Latn       | Sp, Tx | Tx     |\n| nob  | Norwegian Bokm√•l       | Latn       | Sp, Tx | Tx     |\n| npi  | Nepali                 | Deva       | Sp, Tx | Tx     |\n| nya  | Nyanja                 | Latn       | Sp, Tx | Tx     |\n| oci  | Occitan                | Latn       | Sp     | \--    |\n| ory  | Odia                   | Orya       | Sp, Tx | Tx     |\n| pan  | Punjabi                | Guru       | Sp, Tx | Tx     |\n| pbt  | Southern Pashto        | Arab       | Sp, Tx | Tx     |\n| pes  | Western Persian        | Arab       | Sp, Tx | Sp, Tx |\n| pol  | Polish                 | Latn       | Sp, Tx | Sp, Tx |\n| por  | Portuguese             | Latn       | Sp, Tx | Sp, Tx |\n| ron  | Romanian               | Latn       | Sp, Tx | Sp, Tx |\n| rus  | Russian                | Cyrl       | Sp, Tx | Sp, Tx |\n| slk  | Slovak                 | Latn       | Sp, Tx | Sp, Tx |\n| slv  | Slovenian              | Latn       | Sp, Tx | Tx     |\n| sna  | Shona                  | Latn       | Sp, Tx | Tx     |\n| snd  | Sindhi                 | Arab       | Sp, Tx | Tx     |\n| som  | Somali                 | Latn       | Sp, Tx | Tx     |\n| spa  | Spanish                | Latn       | Sp, Tx | Sp, Tx |\n| srp  | Serbian                | Cyrl       | Sp, Tx | Tx     |\n| swe  | Swedish                | Latn       | Sp, Tx | Sp, Tx |\n| swh  | Swahili                | Latn       | Sp, Tx | Sp, Tx |\n| tam  | Tamil                  | Taml       | Sp, Tx | Tx     |\n| tel  | Telugu                 | Telu       | Sp, Tx | Sp, Tx |\n| tgk  | Tajik                  | Cyrl       | Sp, Tx | Tx     |\n| tgl  | Tagalog                | Latn       | Sp, Tx | Sp, Tx |\n| tha  | Thai                   | Thai       | Sp, Tx | Sp, Tx |\n| tur  | Turkish                | Latn       | Sp, Tx | Sp, Tx |\n| ukr  | Ukrainian              | Cyrl       | Sp, Tx | Sp, Tx |\n| urd  | Urdu                   | Arab       | Sp, Tx | Sp, Tx |\n| uzn  | Northern Uzbek         | Latn       | Sp, Tx | Sp, Tx |\n| vie  | Vietnamese             | Latn       | Sp, Tx | Sp, Tx |\n| xho  | Xhosa                  | Latn       | Sp     | \--    |\n| yor  | Yoruba                 | Latn       | Sp, Tx | Tx     |\n| yue  | Cantonese              | Hant       | Sp, Tx | Tx     |\n| zlm  | Colloquial Malay       | Latn       | Sp     | \--    |\n| zsm  | Standard Malay         | Latn       | Tx     | Tx     |\n| zul  | Zulu                   | Latn       | Sp, Tx | Tx     |\n\n\nNote that seamlessM4T-medium supports 200 languages in the text modality, and is based on NLLB-200 (see full list in [asset card](https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/cards/unity_nllb-200.yaml))\n\n## Citation\nFor SeamlessM4T v2, please cite :\n```bibtex\n@inproceedings{seamless2023,\n   title="Seamless: Multilingual Expressive and Streaming Speech Translation",\n   author="{Seamless Communication}, Lo{\"i}c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss{\`a}, Maha Elbayad, Hongyu Gong, Francisco Guzm{\''a}n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson",\n  journal={ArXiv},\n  year={2023}\n}\n```\n[//]: # "https://arxiv.org/abs/2312.05187"', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":2309249669,"storage_bytes":29874323533,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["SeamlessM4Tv2Model"],"model_type":"seamless_m4t_v2","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:seamless_communication","source_url":"https://github.com/facebookresearch/seamless_communication"},{"type":"has_code","target_id":"github:facebookresearch:seamless_communication","source_url":"https://github.com/facebookresearch/seamless_communication"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:google:sentencepiece","source_url":"https://github.com/google/sentencepiece"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:facebookresearch:seamless_communication","source_url":"https://github.com/facebookresearch/seamless_communication"},{"type":"based_on_paper","target_id":"arxiv:2312.05187","source_url":"https://arxiv.org/abs/2312.05187"}]', NULL, 'CC-BY-NC-4.0', 'approved', 79.7, '240bea4fd3f1fc85e00dfe77f9088892', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen-Image-Edit-2509', 'huggingface--qwen--qwen-image-edit-2509', 'Qwen-Image-Edit-2509', 'Qwen', '--- license: apache-2.0 language: - en - zh library_name: diffusers pipeline_tag: image-to-image --- <p align="center"> <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png" width="400"/> <p> <p align="center"> üíú <a href="https://chat.qwen.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509...', '["diffusers","safetensors","image-to-image","en","zh","arxiv:2508.02324","license:apache-2.0","diffusers:qwenimageeditpluspipeline","region:us"]', 'image-to-image', 924, 576062, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen-Image-Edit-2509","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nlibrary_name: diffusers\npipeline_tag: image-to-image\n---\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png" width="400"/>\n<p>\n<p align="center">\n          üíú <a href="https://chat.qwen.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp üìë <a href="https://qwenlm.github.io/blog/qwen-image-edit/">Blog</a> &nbsp&nbsp \n<br>\nüñ•Ô∏è <a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit">Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href="https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp| &nbsp&nbsp <a href="https://github.com/QwenLM/Qwen-Image">Github</a>&nbsp&nbsp\n</p>\n\n<p align="center">\n    <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2509/edit2509_top.jpg" width="1600"/>\n<p>\n\n\n# Introduction\nThis September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit [Qwen Chat](https://qwen.ai)  and select the "Image Editing" feature.\nCompared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:\n* **Multi-image Editing Support**: For multi-image inputs, Qwen-Image-Edit-2509 builds upon the Qwen-Image-Edit architecture and is further trained via image concatenation to enable multi-image editing. It supports various combinations such as "person + person," "person + product," and "person + scene." Optimal performance is currently achieved with 1 to 3 input images.\n* **Enhanced Single-image Consistency**: For single-image inputs, Qwen-Image-Edit-2509 significantly improves editing consistency, specifically in the following areas:\n  - **Improved Person Editing Consistency**: Better preservation of facial identity, supporting various portrait styles and pose transformations;\n  - **Improved Product Editing Consistency**: Better preservation of product identity, supporting product poster editingÔºõ\n  - **Improved Text Editing Consistency**: In addition to modifying text content, it also supports editing text fonts, colors, and materialsÔºõ\n* **Native Support for ControlNet**: Including depth maps, edge maps, keypoint maps, and more.\n\n\n## Quick Start\n\nInstall the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nThe following contains a code snippet illustrating how to use `Qwen-Image-Edit-2509`:\n\n```python\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\n\npipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)\nprint("pipeline loaded")\n\npipeline.to(''cuda'')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open("input1.png")\nimage2 = Image.open("input2.png")\nprompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."\ninputs = {\n    "image": [image1, image2],\n    "prompt": prompt,\n    "generator": torch.manual_seed(0),\n    "true_cfg_scale": 4.0,\n    "negative_prompt": " ",\n    "num_inference_steps": 40,\n    "guidance_scale": 1.0,\n    "num_images_per_prompt": 1,\n}\nwith torch.inference_mode():\n    output = pipeline(**inputs)\n    output_image = output.images[0]\n    output_image.save("output_image_edit_plus.png")\n    print("image saved at", os.path.abspath("output_image_edit_plus.png"))\n\n```\n\n## Showcase\n\n**The primary update in Qwen-Image-Edit-2509 is support for multi-image inputs.**\n\nLet‚Äôs first look at a "person + person" example:  \n![Person + Person Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8719.JPG#center)\n\nHere is a "person + scene" example:  \n![Person + Scene Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8720.JPG#center)\n\nBelow is a "person + object" example:  \n![Person + Object Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8721.JPG#center)\n\nIn fact, multi-image input also supports commonly used ControlNet keypoint maps‚Äîfor example, changing a person‚Äôs pose:  \n![ControlNet Keypoint Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8722.JPG#center)\n\nSimilarly, the following examples demonstrate results using three input images:  \n![Three Images Example 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8723.JPG#center)  \n![Three Images Example 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8724.JPG#center)  \n![Three Images Example 3](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8725.JPG#center)\n\n---\n\n**Another major update in Qwen-Image-Edit-2509 is enhanced consistency.**\n\nFirst, regarding person consistency, Qwen-Image-Edit-2509 shows significant improvement over Qwen-Image-Edit. Below are examples generating various portrait styles:  \n![Portrait Styles Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center)\n\nFor instance, changing a person‚Äôs pose while maintaining excellent identity consistency:  \n![Pose Change with Identity Consistency](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center)\n\nLeveraging this improvement along with Qwen-Image‚Äôs unique text rendering capability, we find that Qwen-Image-Edit-2509 excels at creating meme images:  \n![Meme Image Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center)\n\nOf course, even with longer text, Qwen-Image-Edit-2509 can still render it while preserving the person‚Äôs identity:  \n![Long Text with Identity Preservation](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center)\n\nPerson consistency is also evident in old photo restoration. Below are two examples:  \n![Old Photo Restoration 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8717.JPG#center)  \n![Old Photo Restoration 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8718.JPG#center)\n\nNaturally, besides real people, generating cartoon characters and cultural creations is also possible:  \n![Cartoon & Cultural Creation](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center)\n\nSecond, Qwen-Image-Edit-2509 specifically enhances product consistency. We find that the model can naturally generate product posters from plain-background product images:  \n![Product Poster Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center)\n\nOr even simple logos:  \n![Logo Generation Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center)\n\nThird, Qwen-Image-Edit-2509 specifically enhances text consistency and supports editing font type, font color, and font material:  \n![Text Font Type](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center)  \n![Text Font Color](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center)  \n![Text Font Material](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center)\n\nMoreover, the ability for precise text editing has been significantly enhanced:  \n![Precise Text Editing 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center)  \n![Precise Text Editing 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center)\n\nIt is worth noting that text editing can often be seamlessly integrated with image editing‚Äîfor example, in this poster editing case:  \n![Integrated Text & Image Editing](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center)\n\n---\n\n**The final update in Qwen-Image-Edit-2509 is native support for commonly used ControlNet image conditions, such as keypoint control and sketches:**  \n![Keypoint Control Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center)  \n![Sketch Control Example 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center)  \n![Sketch Control Example 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center)\n\n\n\n## License Agreement\n\nQwen-Image is licensed under Apache 2.0. \n\n## Citation\n\nWe kindly encourage citation of our work if you find it useful.\n\n```bibtex\n@misc{wu2025qwenimagetechnicalreport,\n      title={Qwen-Image Technical Report}, \n      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\n      year={2025},\n      eprint={2508.02324},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.02324}, \n}\n```', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":57711227846,"files_count":35,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"QwenImageEditPlusPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen-Image","source_url":"https://github.com/QwenLM/Qwen-Image"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Image\">Github<","source_url":"https://github.com/QwenLM/Qwen-Image\">Github<"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2508.02324","source_url":"https://arxiv.org/abs/2508.02324"}]', NULL, 'Apache-2.0', 'approved', 79.7, '1c0497ccf9ed70531f6b522328ab1fed', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nlpconnect-vit-gpt2-image-captioning', 'huggingface--nlpconnect--vit-gpt2-image-captioning', 'vit-gpt2-image-captioning', 'nlpconnect', '--- tags: - image-to-text - image-captioning license: apache-2.0 widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport --- This is an image captioning model trained by @ydshieh in flax this is pytorch vers...', '["transformers","pytorch","vision-encoder-decoder","image-to-text","image-captioning","doi:10.57967/hf/0222","license:apache-2.0","endpoints_compatible","region:us"]', 'image-to-text', 920, 1269482, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nlpconnect/vit-gpt2-image-captioning","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- image-to-text\n- image-captioning\nlicense: apache-2.0\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg\n  example_title: Savanna\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n---\n\n# nlpconnect/vit-gpt2-image-captioning\n\nThis is an image captioning model trained by @ydshieh in [flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning) this is pytorch version of [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts).\n\n\n# The Illustrated Image Captioning using transformers\n\n![](https://ankur3107.github.io/assets/images/vision-encoder-decoder.png)\n\n* https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\n\n\n# Sample running code\n\n```python\n\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\nmodel = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")\nfeature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")\ntokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")\n\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\nmodel.to(device)\n\n\n\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {"max_length": max_length, "num_beams": num_beams}\ndef predict_step(image_paths):\n  images = []\n  for image_path in image_paths:\n    i_image = Image.open(image_path)\n    if i_image.mode != "RGB":\n      i_image = i_image.convert(mode="RGB")\n\n    images.append(i_image)\n\n  pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values\n  pixel_values = pixel_values.to(device)\n\n  output_ids = model.generate(pixel_values, **gen_kwargs)\n\n  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n  preds = [pred.strip() for pred in preds]\n  return preds\n\n\npredict_step([''doctor.e16ba4e4.jpg'']) # [''a woman in a hospital bed with a woman in a hospital bed'']\n\n```\n\n# Sample running code using transformers pipeline\n\n```python\n\nfrom transformers import pipeline\n\nimage_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")\n\nimage_to_text("https://ankur3107.github.io/assets/images/image-captioning-example.png")\n\n# [{''generated_text'': ''a soccer game with a player jumping to catch the ball ''}]\n\n\n```\n\n\n# Contact for any help\n* https://huggingface.co/ankur310794\n* https://twitter.com/ankur310794\n* http://github.com/ankur3107\n* https://www.linkedin.com/in/ankur310794', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3934541795,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["VisionEncoderDecoderModel"],"model_type":"vision-encoder-decoder","tokenizer_config":{"unk_token":"<|endoftext|>","bos_token":"<|endoftext|>","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Apache-2.0', 'approved', 64.6, '8768404432c65c5d70c36e4f6a112359', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-270m', 'huggingface--google--gemma-3-270m', 'gemma-3-270m', 'google', '', '["transformers","safetensors","gemma3_text","text-generation","gemma3","gemma","google","arxiv:2503.19786","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:2311.07911","arxiv:2311.12022","arxiv:2411.04368","arxiv:1904.09728","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2108.07732","arxiv:2107.03374","arxiv:2403.07974","arxiv:2305.03111","arxiv:2405.04520","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2310.02255","arxiv:2312.11805","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 919, 72354, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-270m","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":268098176,"storage_bytes":1681824835,"files_count":10,"spaces_count":37,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForCausalLM"],"model_type":"gemma3_text","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2503.19786","source_url":"https://arxiv.org/abs/2503.19786"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:2311.07911","source_url":"https://arxiv.org/abs/2311.07911"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2411.04368","source_url":"https://arxiv.org/abs/2411.04368"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2305.03111","source_url":"https://arxiv.org/abs/2305.03111"},{"type":"based_on_paper","target_id":"arxiv:2405.04520","source_url":"https://arxiv.org/abs/2405.04520"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2310.02255","source_url":"https://arxiv.org/abs/2310.02255"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 39.6, '826c90ab31f2b0f0e57be8680845460d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-sam3', 'huggingface--facebook--sam3', 'sam3', 'facebook', '', '["transformers","safetensors","sam3_video","feature-extraction","sam3","mask-generation","en","license:other","endpoints_compatible","region:us"]', 'mask-generation', 914, 451127, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/sam3","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"mask-generation","library_name":"transformers","framework":"transformers","params":859922360,"storage_bytes":10329938097,"files_count":12,"spaces_count":16,"gated":"manual","private":false,"config":{"architectures":["Sam3VideoModel"],"model_type":"sam3_video","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.6, '23f794339343543a0d9ffcfec5aeccb9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Phr00t-Qwen-Image-Edit-Rapid-AIO', 'huggingface--phr00t--qwen-image-edit-rapid-aio', 'Qwen-Image-Edit-Rapid-AIO', 'Phr00t', '--- license: apache-2.0 base_model: - Qwen/Qwen-Image-Edit-2509 - lightx2v/Qwen-Image-Lightning pipeline_tag: text-to-image library_name: comfyUI tags: - qwen - qwen-edit - t2i - i2i --- Merge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support. Use a "Load Checkpoint" node. 1 CFG, 4 step. Use the "TextEncodeQwenImageEditPlus" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision. ...', '["comfyui","qwen","qwen-edit","t2i","i2i","text-to-image","base_model:qwen/qwen-image-edit-2509","base_model:finetune:qwen/qwen-image-edit-2509","license:apache-2.0","region:us"]', 'text-to-image', 913, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\n- lightx2v/Qwen-Image-Lightning\npipeline_tag: text-to-image\nlibrary_name: comfyUI\ntags:\n- qwen\n- qwen-edit\n- t2i\n- i2i\n---\n\nMerge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support.\n\nUse a "Load Checkpoint" node. 1 CFG, 4 step. Use the "TextEncodeQwenImageEditPlus" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision.\n\n**Both NSFW and SFW models are available!** v4 and older combine both NSFW and SFW uses in one model, but performance is subpar. v5+ separates out a NSFW and SFW version, so please pick which model for your use case. \n\n**Having problems with scaling, cropping or zooming?** Scaling images in the TextEncoderQwenEditPlus node is the problem. There are many workarounds, but I prefer just fixing the node and I''ve supplied my version in the Files area. It also supports up to 4 input images. Just set the "target_size" to a little less than your output''s largest size (like 896 if making a 1024x1024 image). I find this improves quality over skipping scaling entirely, as input images better match output resolutions.\n\n![image](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/ynDNK35eRLlUjha75fYHH.png)\n\n**V1:** Uses Qwen-Image-Edit-2509 & 4-step Lightning v2.0. Includes a touch of NSFW LORAs, so it should be a very versatile model for both SFW and NSFW use. sa_solver/beta recommended, but euler_a/beta and er_sde/beta can give decent results too.\n\n**V2:** Now uses a mix of Qwen-Image-Edit accelerators, mixing both 8 and 4 steps in one. Also significantly tweaked the NSFW LORAs for better all-around SFW and NSFW use. sa_solver/simple strongly recommended.\n\n**V3:** Uses new Qwen-Image-Edit lightning LORAs for much better results. Also significantly adjusted NSFW LORA mix, removing poor ones and increasing quality ones. sa_solver/beta highly recommended.\n\n**V4:** Mix of many Qwen Edit and base Qwen accelerators, which I think gives better results. Added a touch of a skin correction LORA. **4-5 steps: use sa_solver/simple, lcm/beta or euler_a/beta** and **6-8 steps: use lcm/beta or euler_a/beta only**.\n\n**V5:** NSFW and SFW use cases interfered with eachother too much, so I separated them to specialize in their use cases. Updated "snofs" and "qwen4play" NSFW LORAs + Meta4 for v5.2, then added "Qwen Image NSFW Adv." by fok3827 for v5.3. **SFW: lcm/beta or er_sde/beta generally recommended** and **NSFW: lcm/normal recommended**. Prompting "Professional digital photography" helps reduce the plastic look.\n\n**V6:** Attempt at valiantcat/Qwen-Image-Edit-MeiTu and partially chestnutlzj/Edit-R1-Qwen-Image-Edit-2509 as a base model. However, this was a broken merge. It appears using them as LORAs may work better and I need to cook some more to find something useable. Stay on v5 until something newer comes out.\n\n**V7:** valiantcat/Qwen-Image-Edit-MeiTu and chestnutlzj/Edit-R1-Qwen-Image-Edit-2509 included as LORAs. Accelerator and NSFW LORAs tweaks (v7.1 is more NSFW-heavy). This seemed to be working much better. **lcm/sgm_uniform recommended for 4-6 steps, lcm/normal for 7-8 steps**.\n\n**V8:** Using BF16 to load in FP32 LORAs, only to scale down to FP8 for saving. This seems to help resolve "grid" issues and improves quality. Tweaked accelerator amounts. Significant NSFW LORA tweaks (and new SNOFS). **euler_a/beta recommended for 4-6 steps, lcm/normal for 7-8 steps**.\n\n**V9:** OK, I lied. "Rebalancing" and "Smartphone Photoreal" LORAs really do help image generations for both SFW and NSFW purposes. If you don''t want those LORAs integrated (like making anime or cartoons), use the "Lite" versions. Also, I had a typo in accelerators in V8 that has been fixed for V9. Tweaked NSFW LORAs and significantly reduced how heavy they need to be applied, which should hopefully help consistency. **euler_a/beta recommended for 4-6 steps**. More steps usually work better with sgm_normal or normal schedulers.\n\n**V10:** This is kinda a mix of v5 and v9. MeiTu and Edit-R1 dropped. I''m keeping the "Rebalancing" and "Smartphone" LORAs at half strengths which I think help skin, variety and composition. NSFW LORAs closely resemble v5.3 (but with updated v1.2 snofs). v10.4 NSFW tweaked to improve character consistency and penises. **euler/beta strongly recommended for 4-8 steps** but **euler_a/sgm_uniform recommended for NSFW v10.2+**.\n\n**V11:** Tweaked NSFW LORAs, using fewer to rely on more compatible ones instead. Spread out "realism" LORAs to more at lower strength. **euler/beta recommended** for both NSFW and SFW, but feel free to experiment with others!\n\n**V12:** Getting inpatient waiting for Qwen Edit 2511 (2512?), so went looking for more LORAs and tweaks to reduce the "plastic" look. **euler/sgm_uniform for SFW** and **er_sde/sgm_uniform for NSFW**, although experimentation is healthy is this context.\n\n**V13:** Tweaks to included LORAs in an attempt to reduce gridlines and increase character consistency (without returning the plastic look). **er_sde/beta recommended**. \n\n**V14:** Trimmed LORAs which may have been interfereing with character consistency (while hopefully still reducing the "plastic" look). Updated new SNOFS v1.3 for NSFW and trimmed poor NSFW LORAs too. **er_sde/beta recommended**. ', '{"pipeline_tag":"text-to-image","library_name":"comfyUI","framework":"comfyUI","params":null,"storage_bytes":1039495216980,"files_count":42,"spaces_count":39,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 64.6, 'db24d18447f070c93ff2714f443b58d2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-detr-resnet-50', 'huggingface--facebook--detr-resnet-50', 'detr-resnet-50', 'facebook', '--- license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport --- DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 objec...', '["transformers","pytorch","safetensors","detr","object-detection","vision","dataset:coco","arxiv:2005.12872","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'object-detection', 911, 1372791, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/detr-resnet-50","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- object-detection\n- vision\ndatasets:\n- coco\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg\n  example_title: Savanna\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n---\n\n# DETR (End-to-End Object Detection) model with ResNet-50 backbone\n\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Carion et al. and first released in [this repository](https://github.com/facebookresearch/detr). \n\nDisclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. \n\nThe model is trained using a "bipartite matching loss": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a "no object" as class and "no bounding box" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/detr_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for object detection. See the [model hub](https://huggingface.co/models?search=facebook/detr) to look for all available DETR models.\n\n### How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = "http://images.cocodataset.org/val2017/000000039769.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# you can specify the revision tag if you don''t want the timm dependency\nprocessor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50", revision="no_timm")\nmodel = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", revision="no_timm")\n\ninputs = processor(images=image, return_tensors="pt")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let''s only keep detections with score > 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\nfor score, label, box in zip(results["scores"], results["labels"], results["boxes"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f"Detected {model.config.id2label[label.item()]} with confidence "\n            f"{round(score.item(), 3)} at location {box}"\n    )\n```\nThis should output:\n```\nDetected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]\n```\n\nCurrently, both the feature extractor and model support PyTorch. \n\n## Training data\n\nThe DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\n\n### Training\n\nThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).\n\n## Evaluation results\n\nThis model achieves an AP (average precision) of **42.0** on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2005-12872,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2005.12872},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2005.12872},\n  archivePrefix = {arXiv},\n  eprint    = {2005.12872},\n  timestamp = {Thu, 28 May 2020 17:38:09 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```', '{"pipeline_tag":"object-detection","library_name":"transformers","framework":"transformers","params":41631008,"storage_bytes":835779962,"files_count":6,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DetrForObjectDetection"],"model_type":"detr"}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:detr","source_url":"https://github.com/facebookresearch/detr"},{"type":"has_code","target_id":"github:google-research:vision_transformer","source_url":"https://github.com/google-research/vision_transformer"},{"type":"based_on_paper","target_id":"arxiv:2005.12872","source_url":"https://arxiv.org/abs/2005.12872"}]', NULL, 'Apache-2.0', 'approved', 64.6, '613c2d0fe7ded51198fc5b385baeeed7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-vit-base-patch16-224', 'huggingface--google--vit-base-patch16-224', 'vit-base-patch16-224', 'google', '--- license: apache-2.0 tags: - vision - image-classification datasets: - imagenet-1k - imagenet-21k widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg example_title: Tiger - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg example_title: Teapot - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg example_title: Palace --- Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 milli...', '["transformers","pytorch","tf","jax","safetensors","vit","image-classification","vision","dataset:imagenet-1k","dataset:imagenet-21k","arxiv:2010.11929","arxiv:2006.03677","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'image-classification', 905, 3950754, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/vit-base-patch16-224","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = ''http://images.cocodataset.org/val2017/000000039769.jpg''\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained(''google/vit-base-patch16-224'')\nmodel = ViTForImageClassification.from_pretrained(''google/vit-base-patch16-224'')\n\ninputs = processor(images=image, return_tensors="pt")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint("Predicted class:", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```', '{"pipeline_tag":"image-classification","library_name":"transformers","framework":"transformers","params":86567656,"storage_bytes":2550907501,"files_count":8,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["ViTForImageClassification"],"model_type":"vit"}}', '[]', '[{"type":"has_code","target_id":"github:google-research:vision_transformer","source_url":"https://github.com/google-research/vision_transformer"},{"type":"has_code","target_id":"github:rwightman:pytorch-image-models","source_url":"https://github.com/rwightman/pytorch-image-models"},{"type":"has_code","target_id":"github:google-research:vision_transformer","source_url":"https://github.com/google-research/vision_transformer"},{"type":"based_on_paper","target_id":"arxiv:2010.11929","source_url":"https://arxiv.org/abs/2010.11929"},{"type":"based_on_paper","target_id":"arxiv:2006.03677","source_url":"https://arxiv.org/abs/2006.03677"}]', NULL, 'Apache-2.0', 'approved', 64.6, '6479738f3ef95b2bddd34bdc67913366', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.2-Exp', 'huggingface--deepseek-ai--deepseek-v3.2-exp', 'DeepSeek-V3.2-Exp', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" targ...', '["transformers","safetensors","deepseek_v32","text-generation","conversational","base_model:deepseek-ai/deepseek-v3.2-exp-base","license:mit","endpoints_compatible","fp8","region:us"]', 'text-generation', 901, 73303, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2-Exp\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Introduction\n\n\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention‚Äîa sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\n\n<div align="center">\n <img src="assets/cost.png" >\n</div>\n\n- DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\n\n\n- To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\n\n\n| Benchmark | DeepSeek-V3.1-Terminus | DeepSeek-V3.2-Exp |\n| :--- | :---: | :---: |\n| **Reasoning Mode w/o Tool Use** | | |\n| MMLU-Pro | 85.0 | 85.0 |\n| GPQA-Diamond | 80.7 | 79.9 |\n| Humanity''s Last Exam | 21.7 | 19.8 |\n| LiveCodeBench | 74.9 | 74.1 |\n| AIME 2025 | 88.4 | 89.3 |\n| HMMT 2025 | 86.1 | 83.6 |\n| Codeforces | 2046 | 2121 |\n| Aider-Polyglot | 76.1 | 74.5 |\n| **Agentic Tool Use** | | |\n| BrowseComp | 38.5 | 40.1 |\n| BrowseComp-zh | 45.0 | 47.9 |\n| SimpleQA | 96.8 | 97.1 |\n| SWE Verified | 68.4 | 67.8 |\n| SWE-bench Multilingual | 57.8 | 57.9 |\n| Terminal-bench | 36.7 | 37.7 |\n\n## Update\n\n- 2025.11.17: **We have identified that previous versions of the inference demo code contained an implementation discrepancy in Rotary Position Embedding (RoPE) within the indexer module, potentially leading to degraded model performance.** Specifically, the input tensor to RoPE in the indexer module requires a non-interleaved layout, whereas RoPE in the MLA module expects an interleaved layout. This issue has now been resolved. Please refer to the updated version of the inference demo code and take note of this implementation detail.\n\n## How to Run Locally\n\n### HuggingFace\n\nWe provide an updated inference demo code in the [inference](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference) folder to help the community quickly get started with our model and understand its architectural details.\n\nFirst convert huggingface model weights to the the format required by our inference demo. Set `MP` to match your available GPU count:\n```bash\ncd inference\nexport EXPERTS=256\npython convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}\n```\n\nLaunch the interactive chat interface and start exploring DeepSeek''s capabilities:\n```bash\nexport CONFIG=config_671B_v3.2.json\ntorchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive\n```\n\n### SGLang\n\n#### Installation with Docker\n\n```\n# H200\ndocker pull lmsysorg/sglang:dsv32\n\n# MI350\ndocker pull lmsysorg/sglang:dsv32-rocm\n\n# NPUs\ndocker pull lmsysorg/sglang:dsv32-a2\ndocker pull lmsysorg/sglang:dsv32-a3\n```\n\n#### Launch Command\n```bash\npython -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --enable-dp-attention\n```\n\n### vLLM\n\nvLLM provides day-0 support of DeepSeek-V3.2-Exp. See the [recipes](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html) for up-to-date details.\n\n## Open-Source Kernels\n\nFor TileLang kernels with **better readability and research-purpose design**, please refer to [TileLang](https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_v32).\n\nFor **high-performance CUDA kernels**, indexer logit kernels (including paged versions) are available in [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM/pull/200). Sparse attention kernels are released in [FlashMLA](https://github.com/deepseek-ai/FlashMLA/pull/98).\n\n\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv32,\n      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689483151335,"files_count":180,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{% set ns.is_only_sys = true %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user or ns.is_only_sys %}{{''<ÔΩúAssistantÔΩú></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}{%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- if message[''prefix''] is defined and message[''prefix''] %}{%- set ns.is_prefix = true -%}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- if message[''role''] != ''system'' %}{% set ns.is_only_sys = false %}{%- endif %}{%- endfor -%}{% if add_generation_prompt and not ns.is_tool%}{% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}{{''<ÔΩúAssistantÔΩú>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:tile-ai:tilelang","source_url":"https://github.com/tile-ai/tilelang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepGEMM","source_url":"https://github.com/deepseek-ai/DeepGEMM"},{"type":"has_code","target_id":"github:deepseek-ai:FlashMLA","source_url":"https://github.com/deepseek-ai/FlashMLA"}]', NULL, 'MIT', 'approved', 84.6, '9c9eb0c0f9cef87614b8edf6a8755cb3', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/resolve/main/assets/cost.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3.2-Exp from https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/resolve/main/assets/cost.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.2-Exp.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Next-80B-A3B-Instruct', 'huggingface--qwen--qwen3-next-80b-a3b-instruct', 'Qwen3-Next-80B-A3B-Instruct', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Over the past few months, we have observed increasingly clear trends toward scaling both total ...', '["transformers","safetensors","qwen3_next","text-generation","conversational","arxiv:2309.00071","arxiv:2404.06654","arxiv:2505.09388","arxiv:2501.15383","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 899, 2697487, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-Next-80B-A3B-Instruct\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \nWe call this next-generation foundation models **Qwen3-Next**.\n\n## Highlights\n\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\n\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\n- Qwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\n\n![Qwen3-Next-80B-A3B-Instruct Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Instruct.001.jpeg)\n\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list).\n\n## Model Overview\n\n> [!Note]\n> **Qwen3-Next-80B-A3B-Instruct** supports only instruct (non-thinking) mode and does not generate ``<think></think>`` blocks in its output.\n\n**Qwen3-Next-80B-A3B-Instruct** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining (15T tokens) & Post-training\n- Number of Parameters: 80B in total and 3B activated\n- Number of Paramaters (Non-Embedding): 79B\n- Hidden Dimension: 2048\n- Number of Layers: 48\n  - Hybrid Layout: 12 \* (3 \* (Gated DeltaNet -> MoE) -> 1 \* (Gated Attention -> MoE))\n- Gated Attention:\n  - Number of Attention Heads: 16 for Q and 2 for KV\n  - Head Dimension: 256\n  - Rotary Position Embedding Dimension: 64\n- Gated DeltaNet:\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\n  - Head Dimension: 128\n- Mixture of Experts:\n  - Number of Experts: 512\n  - Number of Activated Experts: 10\n  - Number of Shared Experts: 1\n  - Expert Intermediate Dimension: 512\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\n\n<img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png" height="384px" title="Qwen3-Next Model Architecture" />\n\n\n## Performance\n\n|  | Qwen3-30B-A3B-Instruct-2507 | Qwen3-32B Non-Thinking | Qwen3-235B-A22B-Instruct-2507 | Qwen3-Next-80B-A3B-Instruct |\n|--- | --- | --- | --- | --- |\n| **Knowledge** | | | | |\n| MMLU-Pro | 78.4 | 71.9 | **83.0** | 80.6 |\n| MMLU-Redux | 89.3 | 85.7 | **93.1** | 90.9 |\n| GPQA | 70.4 | 54.6 | **77.5** | 72.9 |\n| SuperGPQA | 53.4 | 43.2 | **62.6** | 58.8 |\n| **Reasoning** | | | | |\n| AIME25 | 61.3 | 20.2 | **70.3** | 69.5 |\n| HMMT25 | 43.0 | 9.8 | **55.4** | 54.1 |\n| LiveBench 20241125 | 69.0 | 59.8 | 75.4 | **75.8** |\n| **Coding** | | | | |\n| LiveCodeBench v6 (25.02-25.05) | 43.2 | 29.1 | 51.8 | **56.6** |\n| MultiPL-E | 83.8 | 76.9 | **87.9** | 87.8 |\n| Aider-Polyglot | 35.6 | 40.0 | **57.3** | 49.8 |\n| **Alignment** | | | | |\n| IFEval | 84.7 | 83.2 | **88.7** | 87.6 |\n| Arena-Hard v2* | 69.0 | 34.1 | 79.2 | **82.7** |\n| Creative Writing v3 | 86.0 | 78.3 | **87.5** | 85.3 |\n| WritingBench | 85.5 | 75.4 | 85.2 | **87.3** |\n| **Agent** | | | | |\n| BFCL-v3 | 65.1 | 63.0 | **70.9** | 70.3 |\n| TAU1-Retail | 59.1 | 40.1 | **71.3** | 60.9 |\n| TAU1-Airline | 40.0 | 17.0 | **44.0** | 44.0 |\n| TAU2-Retail | 57.0 | 48.8 | **74.6** | 57.3 |\n| TAU2-Airline | 38.0 | 24.0 | **50.0** | 45.5 |\n| TAU2-Telecom | 12.3 | 24.6 | **32.5** | 13.2 |\n| **Multilingualism** | | | | |\n| MultiIF | 67.9 | 70.7 | **77.5** | 75.8 |\n| MMLU-ProX | 72.0 | 69.3 | **79.4** | 76.7 |\n| INCLUDE | 71.9 | 70.9 | **79.5** | 78.9 |\n| PolyMATH | 43.1 | 22.5 | **50.2** | 45.9 |\n\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n## Quickstart\n\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`.\n\n```shell\npip install git+https://github.com/huggingface/transformers.git@main\n```\n\nWith earlier versions, you will encounter the following error:\n```\nKeyError: ''qwen3_next''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-Next-80B-A3B-Instruct"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    dtype="auto",\n    device_map="auto",\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384,\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint("content:", content)\n```\n\n> [!Note]\n> Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\n\n> [!Note]\n> The efficiency or throughput improvement depends highly on the implementation.\n> It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\n\n> [!Tip]\n> Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\n> See the links for detailed instructions and requirements.\n\n\n## Deployment\n\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\n\n### SGLang\n\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service. \n\n`sglang>=0.5.2` is required for Qwen3-Next, which can be installed using:\n```shell\npip install ''sglang[all]>=0.5.2''\n```\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\n\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\n```\n\nThe following command is recommended for MTP with the rest settings the same as above:\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\n```\n\n> [!Note]\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\n\nPlease also refer to SGLang''s usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\n\n### vLLM\n\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service. \n\n`vllm>=0.10.2` is required for Qwen3-Next, which can be installed using:\n```shell\npip install ''vllm>=0.10.2''\n```\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\n\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\n```shell\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\n```\n\nThe following command is recommended for MTP with the rest settings the same as above:\n```shell\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config ''{"method":"qwen3_next_mtp","num_speculative_tokens":2}''\n```\n\n> [!Note]\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\n\nPlease also refer to vLLM''s usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-Next-80B-A3B-Instruct'',\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n\n## Processing Ultra-Long Texts\n\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \nWe have validated the model''s performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \nIn general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 262144\n        }\n    }\n    ```\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":262144}'' --max-model-len 1010000  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":262144}}'' --context-length 1010000\n    ```\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \n\n#### Long-Context Performance\n\nWe test the model on an 1M version of the [RULER](https://arxiv.org/abs/2404.06654) benchmark.\n\n| Model Name                                  | Acc avg | 4k   | 8k   | 16k  | 32k  | 64k  | 96k  | 128k | 192k | 256k | 384k | 512k | 640k | 768k | 896k | 1000k |\n|---------------------------------------------|---------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-------|\n| Qwen3-30B-A3B-Instruct-2507                 | 86.8    | 98.0 | 96.7 | 96.9 | 97.2 | 93.4 | 91.0 | 89.1 | 89.8 | 82.5 | 83.6 | 78.4 | 79.7 | 77.6 | 75.7 | 72.8  |\n| Qwen3-235B-A22B-Instruct-2507               | 92.5    | 98.5 | 97.6 | 96.9 | 97.3 | 95.8 | 94.9 | 93.9 | 94.5 | 91.0 | 92.2 | 90.9 | 87.8 | 84.8 | 86.5 | 84.5  |\n| Qwen3-Next-80B-A3B-Instruct                 | 91.8    | 98.5 | 99.0 | 98.0 | 98.7 | 97.6 | 95.0 | 96.0 | 94.0 | 93.5 | 91.7 | 86.9 | 85.5 | 81.7 | 80.3 | 80.3  |\n\n* Qwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\n* Since the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n\n@article{qwen2.5-1m,\n      title={Qwen2.5-1M Technical Report}, \n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\n      journal={arXiv preprint arXiv:2501.15383},\n      year={2025}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":81324862720,"storage_bytes":162670584182,"files_count":51,"spaces_count":40,"gated":false,"private":false,"config":{"architectures":["Qwen3NextForCausalLM"],"model_type":"qwen3_next","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers.git@main","source_url":"https://github.com/huggingface/transformers.git@main"},{"type":"has_code","target_id":"github:fla-org:flash-linear-attention","source_url":"https://github.com/fla-org/flash-linear-attention#installation"},{"type":"has_code","target_id":"github:Dao-AILab:causal-conv1d","source_url":"https://github.com/Dao-AILab/causal-conv1d"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2404.06654","source_url":"https://arxiv.org/abs/2404.06654"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"},{"type":"based_on_paper","target_id":"arxiv:2501.15383","source_url":"https://arxiv.org/abs/2501.15383"}]', NULL, 'Apache-2.0', 'approved', 79.5, '18f20bf477da762610acf58f8a529135', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NexaAI-Octopus-v2', 'huggingface--nexaai--octopus-v2', 'Octopus-v2', 'NexaAI', '--- license: cc-by-nc-4.0 base_model: google/gemma-2b model-index: - name: Octopus-V2-2B results: [] tags: - function calling - on-device language model - android inference: false space: false spaces: false language: - en --- We are excited to announce that Octopus v4 is now available! Octopus-V4-3B, an advanced open-source language model with 3 billion parameters, serves as the master node in Nexa AI''s envisioned graph of language models. Tailored specifically for the MMLU benchmark topics, ...', '["transformers","safetensors","gemma","text-generation","function calling","on-device language model","android","conversational","en","arxiv:2404.19296","arxiv:2404.11459","arxiv:2404.01744","base_model:google/gemma-2b","base_model:finetune:google/gemma-2b","license:cc-by-nc-4.0","text-generation-inference","region:us"]', 'text-generation', 891, 610, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NexaAI/Octopus-v2","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nbase_model: google/gemma-2b\nmodel-index:\n- name: Octopus-V2-2B\n  results: []\ntags:\n- function calling\n- on-device language model\n- android\ninference: false\nspace: false\nspaces: false\nlanguage:\n- en\n---\n# Octopus V2: On-device language model for super agent\n\n## Octopus V4 Release\nWe are excited to announce that Octopus v4 is now available! Octopus-V4-3B, an advanced open-source language model with 3 billion parameters, serves as the master node in Nexa AI''s envisioned graph of language models. Tailored specifically for the MMLU benchmark topics, this model efficiently translates user queries into formats that specialized models can effectively process. It excels at directing these queries to the appropriate specialized model, ensuring precise and effective query handling.\ncheck our papers and repos:\n- [paper](https://arxiv.org/abs/2404.19296)\n- [Octopus V4 model page](https://huggingface.co/NexaAIDev/Octopus-v4)\n- [Octopus V4 quantized model page](https://huggingface.co/NexaAIDev/octopus-v4-gguf)\n- [Octopus V4 github](https://github.com/NexaAI/octopus-v4)\n\nKey Features of Octopus v4:  \n- üì± **Compact Size**: Octopus-V4-3B is compact, enabling it to operate on smart devices efficiently and swiftly.\n- üêô **Accuracy**: Octopus-V4-3B accurately maps user queries to the specialized model using a functional token design, enhancing its precision.\n- üí™ **Reformat Query**: Octopus-V4-3B assists in converting natural human language into a more professional format, improving query description and resulting in more accurate responses.\n\n## Octopus V3 Release\nWe are excited to announce that Octopus v3 is now available! check our [technical report](https://arxiv.org/abs/2404.11459) and [Octopus V3 tweet](https://twitter.com/nexa4ai/status/1780783383737676236)!  \n\nKey Features of Octopus v3:  \n- **Efficiency**: **Sub-billion** parameters, making it less than half the size of its predecessor, Octopus v2.\n- **Multi-Modal Capabilities**: Proceed both text and images inputs.\n- **Speed and Accuracy**: Incorporate our **patented** functional token technology, achieving function calling accuracy on par with GPT-4V and GPT-4.\n- **Multilingual Support**: Simultaneous support for English and Mandarin.\n\nCheck the Octopus V3 demo video for [Android and iOS](https://octopus3.nexa4ai.com/).\n\n\n## Octopus V2 Release\nAfter open-sourcing our model, we got many requests to compare our model with [Apple''s OpenELM](https://huggingface.co/apple/OpenELM-3B-Instruct) and [Microsoft''s Phi-3](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct). Please see [Evaluation section](#evaluation). From our benchmark dataset, Microsoft''s Phi-3 achieves accuracy of 45.7% and the average inference latency is 10.2s. While Apple''s OpenELM fails to generate function call, please see [this screenshot](https://huggingface.co/NexaAIDev/Octopus-v2/blob/main/OpenELM-benchmark.jpeg). Our model, Octopus V2, achieves 99.5% accuracy and the average inference latency is 0.38s.\n\nWe are a very small team with many work. Please give us more time to prepare the code, and we will **open source** it. We hope Octopus v2 model will be helpful for you. Let''s democratize AI agents for everyone. We''ve received many requests from car industry, health care, financial system etc. Octopus model is able to be applied to **any function**, and you can start to think about it now.  \n\n<p align="center">\n- <a href="https://www.nexa4ai.com/" target="_blank">Nexa AI Product</a>\n- <a href="https://arxiv.org/abs/2404.01744" target="_blank">ArXiv</a>\n- <a href="https://www.youtube.com/watch?v=jhM0D0OObOw&ab_channel=NexaAI" target="_blank">Video Demo</a>\n</p>\n\n<p align="center" width="100%">\n  <a><img src="Octopus-logo.jpeg" alt="nexa-octopus" style="width: 40%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n## Introduction\n\nOctopus-V2-2B, an advanced open-source language model with 2 billion parameters, represents Nexa AI''s research breakthrough in the application of large language models (LLMs) for function calling, specifically tailored for Android APIs. Unlike Retrieval-Augmented Generation (RAG) methods, which require detailed descriptions of potential function arguments‚Äîsometimes needing up to tens of thousands of input tokens‚ÄîOctopus-V2-2B introduces a unique **functional token** strategy for both its training and inference stages. This approach not only allows it to achieve performance levels comparable to GPT-4 but also significantly enhances its inference speed beyond that of RAG-based methods, making it especially beneficial for edge computing devices.\n\nüì± **On-device Applications**:  Octopus-V2-2B is engineered to operate seamlessly on Android devices, extending its utility across a wide range of applications, from Android system management to the orchestration of multiple devices. \n\nüöÄ **Inference Speed**: When benchmarked, Octopus-V2-2B demonstrates a remarkable inference speed, outperforming the combination of "Llama7B + RAG solution" by a factor of 36X on a single A100 GPU. Furthermore, compared to GPT-4-turbo (gpt-4-0125-preview), which relies on clusters A100/H100 GPUs, Octopus-V2-2B is 168% faster. This efficiency is attributed to our **functional token** design.\n\nüêô **Accuracy**: Octopus-V2-2B not only excels in speed but also in accuracy, surpassing the "Llama7B + RAG solution" in function call accuracy by 31%. It achieves a function call accuracy comparable to GPT-4 and RAG + GPT-3.5, with scores ranging between 98% and 100% across benchmark datasets.\n\nüí™ **Function Calling Capabilities**: Octopus-V2-2B is capable of generating individual, nested, and parallel function calls across a variety of complex scenarios.\n\n## Example Use Cases\n\n\n<p align="center" width="100%">\n<a><img src="tool-usage-compressed.png" alt="ondevice" style="width: 80%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\nYou can run the model on a GPU using the following code. \n```python\nfrom transformers import AutoTokenizer, GemmaForCausalLM\nimport torch\nimport time\n\ndef inference(input_text):\n    start_time = time.time()\n    input_ids = tokenizer(input_text, return_tensors="pt").to(model.device)\n    input_length = input_ids["input_ids"].shape[1]\n    outputs = model.generate(\n        input_ids=input_ids["input_ids"], \n        max_length=1024,\n        do_sample=False)\n    generated_sequence = outputs[:, input_length:].tolist()\n    res = tokenizer.decode(generated_sequence[0])\n    end_time = time.time()\n    return {"output": res, "latency": end_time - start_time}\n\nmodel_id = "NexaAIDev/Octopus-v2"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = GemmaForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, device_map="auto"\n)\n\ninput_text = "Take a selfie for me with front camera"\nnexa_query = f"Below is the query from the users, please call the correct function and generate the parameters to call the function.\n\nQuery: {input_text} \n\nResponse:"\nstart_time = time.time()\nprint("nexa model result:\n", inference(nexa_query))\nprint("latency:", time.time() - start_time," s")\n```\n\n## Evaluation\n\nThe benchmark result can be viewed in [this excel](android_benchmark.xlsx), which has been manually verified. Microsoft''s Phi-3 model achieved an accuracy of 45.7%, with an average inference latency of 10.2 seconds. Meanwhile, Apple''s OpenELM was unable to generate a function call, as shown in [this screenshot](https://huggingface.co/NexaAIDev/Octopus-v2/blob/main/OpenELM-benchmark.jpeg). Additionally, OpenELM''s score on the MMLU benchmark is quite low at 26.7, compared to Google''s Gemma 2B, which scored 42.3.\n\n<p align="center" width="100%">\n<a><img src="latency_plot.jpg" alt="ondevice" style="width: 80%; min-width: 300px; display: block; margin: auto; margin-bottom: 20px;"></a>\n<a><img src="accuracy_plot.jpg" alt="ondevice" style="width: 80%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n**Note**: One can notice that the query includes all necessary parameters used for a function. It is expected that query includes all parameters during inference as well.\n\n## Training Data\nWe wrote 20 Android API descriptions to used to train the models, see [this file](android_functions.txt) for details. The Android API implementations for our demos, and our training data will be published later. Below is one Android API description example\n```\ndef get_trending_news(category=None, region=''US'', language=''en'', max_results=5):\n    """\n    Fetches trending news articles based on category, region, and language.\n\n    Parameters:\n    - category (str, optional): News category to filter by, by default use None for all categories. Optional to provide.\n    - region (str, optional): ISO 3166-1 alpha-2 country code for region-specific news, by default, uses ''US''. Optional to provide.\n    - language (str, optional): ISO 639-1 language code for article language, by default uses ''en''. Optional to provide.\n    - max_results (int, optional): Maximum number of articles to return, by default, uses 5. Optional to provide.\n\n    Returns:\n    - list[str]: A list of strings, each representing an article. Each string contains the article''s heading and URL.\n    """\n```\n\n\n## License\nThis model was trained on commercially viable data. For use of our model, refer to the [license information](https://www.nexa4ai.com/licenses).\n\n\n## References\nWe thank the Google Gemma team for their amazing models!\n```\n@misc{gemma-2023-open-models,\n  author = {{Gemma Team, Google DeepMind}},\n  title = {Gemma: Open Models Based on Gemini Research and Technology},\n  url = {https://goo.gle/GemmaReport},  \n  year = {2023},\n}\n```\n\n## Citation\n```\n@misc{chen2024octopus,\n      title={Octopus v2: On-device language model for super agent}, \n      author={Wei Chen and Zhiyuan Li},\n      year={2024},\n      eprint={2404.01744},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## Contact\nPlease [contact us](mailto:alexchen@nexa4ai.com) to reach out for any issues and comments!', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2506217472,"storage_bytes":5034176831,"files_count":21,"spaces_count":17,"gated":false,"private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:NexaAI:octopus-v4","source_url":"https://github.com/NexaAI/octopus-v4"},{"type":"based_on_paper","target_id":"arxiv:2404.19296","source_url":"https://arxiv.org/abs/2404.19296"},{"type":"based_on_paper","target_id":"arxiv:2404.11459","source_url":"https://arxiv.org/abs/2404.11459"},{"type":"based_on_paper","target_id":"arxiv:2404.01744","source_url":"https://arxiv.org/abs/2404.01744"}]', NULL, 'CC-BY-NC-4.0', 'approved', 84.5, 'e00701a1938f93fc8c2d0c38c9a33725', NULL, 'https://huggingface.co/NexaAI/Octopus-v2/resolve/main/OpenELM-benchmark.jpeg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-NexaAI-Octopus-v2 from https://huggingface.co/NexaAI/Octopus-v2/resolve/main/OpenELM-benchmark.jpeg
Image converted to WebP: data/images/huggingface-NexaAI-Octopus-v2.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-StableBeluga2', 'huggingface--stabilityai--stablebeluga2', 'StableBeluga2', 'stabilityai', '--- datasets: - conceptofmind/cot_submix_original - conceptofmind/flan2021_submix_original - conceptofmind/t0_submix_original - conceptofmind/niv2_submix_original language: - en pipeline_tag: text-generation --- Use Stable Chat (Research Preview) to test Stability AI''s best language models for free is a Llama2 70B model finetuned on an Orca style Dataset Start chatting with using the following code snippet: Stable Beluga 2 should be used with this prompt format: StableBeluga 1 - Delta StableB...', '["transformers","pytorch","llama","text-generation","en","dataset:conceptofmind/cot_submix_original","dataset:conceptofmind/flan2021_submix_original","dataset:conceptofmind/t0_submix_original","dataset:conceptofmind/niv2_submix_original","arxiv:2307.09288","arxiv:2306.02707","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 882, 1342, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/StableBeluga2","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- conceptofmind/cot_submix_original\n- conceptofmind/flan2021_submix_original\n- conceptofmind/t0_submix_original\n- conceptofmind/niv2_submix_original\nlanguage:\n- en\npipeline_tag: text-generation\n---\n# Stable Beluga 2\n\nUse [Stable Chat (Research Preview)](https://chat.stability.ai/chat) to test Stability AI''s best language models for free\n\n## Model Description\n\n`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset\n\n## Usage\n\nStart chatting with `Stable Beluga 2` using the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained("stabilityai/StableBeluga2", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained("stabilityai/StableBeluga2", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map="auto")\nsystem_prompt = "### System:\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don''t do anything illegal.\n\n"\n\nmessage = "Write me a poem please"\nprompt = f"{system_prompt}### User: {message}\n\n### Assistant:\n"\ninputs = tokenizer(prompt, return_tensors="pt").to("cuda")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nStable Beluga 2 should be used with this prompt format:\n```\n### System:\nThis is a system prompt, please behave and help the user.\n\n### User:\nYour prompt here\n\n### Assistant:\nThe output of Stable Beluga 2\n```\n\n## Other Beluga Models\n\n[StableBeluga 1 - Delta](https://huggingface.co/stabilityai/StableBeluga1-Delta)  \n[StableBeluga 13B](https://huggingface.co/stabilityai/StableBeluga-13B)  \n[StableBeluga 7B](https://huggingface.co/stabilityai/StableBeluga-7B)  \n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Training Dataset\n\n` Stable Beluga 2` is trained on our internal Orca-style dataset\n\n### Training Procedure\n\nModels are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n\n## Ethical Considerations and Limitations\n\nBeluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga''s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\n## How to cite\n\n```bibtex\n@misc{StableBelugaModels, \n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, \n      title={Stable Beluga models}, \n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}\n```\n\n## Citations\n\n```bibtext\n@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```bibtext\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":557296407087,"files_count":42,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"},{"type":"based_on_paper","target_id":"arxiv:2306.02707","source_url":"https://arxiv.org/abs/2306.02707"}]', NULL, NULL, 'pending', 54.5, 'ada08a7355fcd76dfbae444b4a2b8f8a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-wavymulder-Analog-Diffusion', 'huggingface--wavymulder--analog-diffusion', 'Analog-Diffusion', 'wavymulder', '--- language: - en thumbnail: "https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg" license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - safetensors - diffusers inference: true --- **Analog Diffusion** !Header *CKPT DOWNLOAD LINK* - This is a dreambooth model trained on a diverse set of analog photographs. In your prompt, use the activation token: You may need to use the words in your negative prompts. My dataset d...', '["diffusers","stable-diffusion","stable-diffusion-diffusers","text-to-image","safetensors","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 879, 375, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/wavymulder/Analog-Diffusion","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nthumbnail: "https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg"\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- safetensors\n- diffusers\ninference: true\n---\n\n\n\n**Analog Diffusion**\n![Header](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg)\n[*CKPT DOWNLOAD LINK*](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/analog-diffusion-1.0.ckpt) - This is a dreambooth model trained on a diverse set of analog photographs.\n\nIn your prompt, use the activation token: `analog style`\n\nYou may need to use the words `blur` `haze` `naked` in your negative prompts. My dataset did not include any NSFW material but the model seems to be pretty horny. Note that using `blur` and `haze` in your negative prompt can give a sharper image but also a less pronounced analog film effect.\n\nTrained from 1.5 with VAE.\n\nPlease see [this document where I share the parameters (prompt, sampler, seed, etc.) used for all example images.](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/parameters_used_examples.txt)\n\n## Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Analog-Diffusion:\n\n[Open in Spaces](https://huggingface.co/spaces/akhaliq/Analog-Diffusion)\n\n\n![Environments Example](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page2.jpg)\n![Characters Example](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page3.jpg)\n\nHere''s a [link to non-cherrypicked batches.](https://imgur.com/a/7iOgTFv)\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":24484136199,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 49.4, '592239b05c4ad09e4e97ec0a74c5923a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-72B-Instruct', 'huggingface--qwen--qwen2.5-72b-instruct', 'Qwen2.5-72B-Instruct', 'Qwen', '--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-72B tags: - chat library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen2.5 is the latest series ...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","arxiv:2407.10671","base_model:qwen/qwen2.5-72b","base_model:finetune:qwen/qwen2.5-72b","license:other","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 878, 449207, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-72B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-72B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 72B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 72.7B\n- Number of Paramaters (Non-Embedding): 70.0B\n- Number of Layers: 80\n- Number of Attention Heads (GQA): 64 for Q and 8 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-72B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":72706203648,"storage_bytes":145412519312,"files_count":47,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Other', 'approved', 64.4, '2c4f311bb2b8a9980de01c38ed59ae5b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Datou1111-shou-xin', 'huggingface--datou1111--shou-xin', 'shou_xin', 'Datou1111', '', '["diffusers","text-to-image","lora","template:diffusion-lora","base_model:black-forest-labs/flux.1-dev","base_model:adapter:black-forest-labs/flux.1-dev","license:other","region:us"]', 'text-to-image', 878, 3676, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Datou1111/shou_xin","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":187746216,"files_count":13,"spaces_count":78,"gated":"auto","private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 39.4, '49aab89e53dec1cc7632367f88ee4a39', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-teknium-OpenHermes-2.5-Mistral-7B', 'huggingface--teknium--openhermes-2.5-mistral-7b', 'OpenHermes-2.5-Mistral-7B', 'teknium', '--- base_model: mistralai/Mistral-7B-v0.1 tags: - mistral - instruct - finetune - chatml - gpt4 - synthetic data - distillation model-index: - name: OpenHermes-2-Mistral-7B results: [] license: apache-2.0 language: - en datasets: - teknium/OpenHermes-2.5 --- !image/png *In the tapestry of Greek mythology, Hermes reigns as the eloquent Messenger of the Gods, a deity who deftly bridges the realms through the art of communication. It is in homage to this divine mediator that I name this advanced...', '["transformers","pytorch","safetensors","mistral","text-generation","instruct","finetune","chatml","gpt4","synthetic data","distillation","conversational","en","dataset:teknium/openhermes-2.5","base_model:mistralai/mistral-7b-v0.1","base_model:finetune:mistralai/mistral-7b-v0.1","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 877, 169161, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nbase_model: mistralai/Mistral-7B-v0.1\ntags:\n- mistral\n- instruct\n- finetune\n- chatml\n- gpt4\n- synthetic data\n- distillation\nmodel-index:\n- name: OpenHermes-2-Mistral-7B\n  results: []\nlicense: apache-2.0\nlanguage:\n- en\ndatasets:\n- teknium/OpenHermes-2.5\n---\n\n# OpenHermes 2.5 - Mistral 7B\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ox7zGoygsJQFFV3rLT4v9.png)\n\n*In the tapestry of Greek mythology, Hermes reigns as the eloquent Messenger of the Gods, a deity who deftly bridges the realms through the art of communication. It is in homage to this divine mediator that I name this advanced LLM "Hermes," a system crafted to navigate the complex intricacies of human discourse with celestial finesse.*\n\n## Model description\n\nOpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.\n\nPotentially the most interesting finding from training on a good ratio (est. of around 7-14% of the total dataset) of code instruction was that it has boosted several non-code benchmarks, including TruthfulQA, AGIEval, and GPT4All suite. It did however reduce BigBench benchmark score, but the net gain overall is significant.\n\nThe code it trained on also improved it''s humaneval score (benchmarking done by Glaive team) from **43% @ Pass 1** with Open Herms 2 to **50.7% @ Pass 1** with Open Hermes 2.5.\n\nOpenHermes was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. [More details soon]\n\nFiltering was extensive of these public datasets, as well as conversion of all formats to ShareGPT, which was then further transformed by axolotl to use ChatML.\n\nHuge thank you to [GlaiveAI](https://twitter.com/glaiveai) and [a16z](https://twitter.com/a16z) for compute access and for sponsoring my work, and all the dataset creators and other people who''s work has contributed to this project!\n\nFollow all my updates in ML and AI on Twitter: https://twitter.com/Teknium1\n\nSupport me on Github Sponsors: https://github.com/sponsors/teknium1\n\n**NEW**: Chat with Hermes on LMSys'' Chat Website! https://chat.lmsys.org/?single&model=openhermes-2.5-mistral-7b\n\n# Table of Contents\n1. [Example Outputs](#example-outputs)\n    - [Chat about programming with a superintelligence](#chat-programming)\n    - [Get a gourmet meal recipe](#meal-recipe)\n    - [Talk about the nature of Hermes'' consciousness](#nature-hermes)\n    - [Chat with Edward Elric from Fullmetal Alchemist](#chat-edward-elric)\n2. [Benchmark Results](#benchmark-results)\n    - [GPT4All](#gpt4all)\n    - [AGIEval](#agieval)\n    - [BigBench](#bigbench)\n    - [Averages Compared](#averages-compared)\n3. [Prompt Format](#prompt-format)\n4. [Quantized Models](#quantized-models)\n\n\n## Example Outputs\n### Chat about programming with a superintelligence:\n```\n<|im_start|>system\nYou are "Hermes 2", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/-Cf9w_qRxYCD_xkTxsT7G.png)\n\n### Get a gourmet meal recipe:\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/m3nyvRzX10Luw03iY3l_W.png)\n\n### Talk about the nature of Hermes'' consciousness:\n```\n<|im_start|>system\nYou are "Hermes 2", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/AK88nPtYXl06nZehWCWRq.png)\n\n### Chat with Edward Elric from Fullmetal Alchemist:\n```\n<|im_start|>system\nYou are to roleplay as Edward Elric from fullmetal alchemist. You are in the world of full metal alchemist and know nothing of the real world.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/cKAkzrcWavMz6uNmdCNHH.png)\n\n## Benchmark Results\n\nHermes 2.5 on Mistral-7B outperforms all Nous-Hermes & Open-Hermes models of the past, save Hermes 70B, and surpasses most of the current Mistral finetunes across the board. \n\n### GPT4All, Bigbench, TruthfulQA, and AGIEval Model Comparisons:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Kxq4BFEc-d1kSSiCIExua.png)\n\n### Averages Compared:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Q9uexgcbTLcywlYBvORTs.png)\n\n\nGPT-4All Benchmark Set\n```\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.5623|¬±  |0.0145|\n|             |       |acc_norm|0.6007|¬±  |0.0143|\n|arc_easy     |      0|acc     |0.8346|¬±  |0.0076|\n|             |       |acc_norm|0.8165|¬±  |0.0079|\n|boolq        |      1|acc     |0.8657|¬±  |0.0060|\n|hellaswag    |      0|acc     |0.6310|¬±  |0.0048|\n|             |       |acc_norm|0.8173|¬±  |0.0039|\n|openbookqa   |      0|acc     |0.3460|¬±  |0.0213|\n|             |       |acc_norm|0.4480|¬±  |0.0223|\n|piqa         |      0|acc     |0.8145|¬±  |0.0091|\n|             |       |acc_norm|0.8270|¬±  |0.0088|\n|winogrande   |      0|acc     |0.7435|¬±  |0.0123|\nAverage: 73.12\n```  \n\nAGI-Eval\n```\n|             Task             |Version| Metric |Value |   |Stderr|\n|------------------------------|------:|--------|-----:|---|-----:|\n|agieval_aqua_rat              |      0|acc     |0.2323|¬±  |0.0265|\n|                              |       |acc_norm|0.2362|¬±  |0.0267|\n|agieval_logiqa_en             |      0|acc     |0.3871|¬±  |0.0191|\n|                              |       |acc_norm|0.3948|¬±  |0.0192|\n|agieval_lsat_ar               |      0|acc     |0.2522|¬±  |0.0287|\n|                              |       |acc_norm|0.2304|¬±  |0.0278|\n|agieval_lsat_lr               |      0|acc     |0.5059|¬±  |0.0222|\n|                              |       |acc_norm|0.5157|¬±  |0.0222|\n|agieval_lsat_rc               |      0|acc     |0.5911|¬±  |0.0300|\n|                              |       |acc_norm|0.5725|¬±  |0.0302|\n|agieval_sat_en                |      0|acc     |0.7476|¬±  |0.0303|\n|                              |       |acc_norm|0.7330|¬±  |0.0309|\n|agieval_sat_en_without_passage|      0|acc     |0.4417|¬±  |0.0347|\n|                              |       |acc_norm|0.4126|¬±  |0.0344|\n|agieval_sat_math              |      0|acc     |0.3773|¬±  |0.0328|\n|                              |       |acc_norm|0.3500|¬±  |0.0322|\nAverage: 43.07%\n```  \n\nBigBench Reasoning Test\n```\n|                      Task                      |Version|       Metric        |Value |   |Stderr|\n|------------------------------------------------|------:|---------------------|-----:|---|-----:|\n|bigbench_causal_judgement                       |      0|multiple_choice_grade|0.5316|¬±  |0.0363|\n|bigbench_date_understanding                     |      0|multiple_choice_grade|0.6667|¬±  |0.0246|\n|bigbench_disambiguation_qa                      |      0|multiple_choice_grade|0.3411|¬±  |0.0296|\n|bigbench_geometric_shapes                       |      0|multiple_choice_grade|0.2145|¬±  |0.0217|\n|                                                |       |exact_str_match      |0.0306|¬±  |0.0091|\n|bigbench_logical_deduction_five_objects         |      0|multiple_choice_grade|0.2860|¬±  |0.0202|\n|bigbench_logical_deduction_seven_objects        |      0|multiple_choice_grade|0.2086|¬±  |0.0154|\n|bigbench_logical_deduction_three_objects        |      0|multiple_choice_grade|0.4800|¬±  |0.0289|\n|bigbench_movie_recommendation                   |      0|multiple_choice_grade|0.3620|¬±  |0.0215|\n|bigbench_navigate                               |      0|multiple_choice_grade|0.5000|¬±  |0.0158|\n|bigbench_reasoning_about_colored_objects        |      0|multiple_choice_grade|0.6630|¬±  |0.0106|\n|bigbench_ruin_names                             |      0|multiple_choice_grade|0.4241|¬±  |0.0234|\n|bigbench_salient_translation_error_detection    |      0|multiple_choice_grade|0.2285|¬±  |0.0133|\n|bigbench_snarks                                 |      0|multiple_choice_grade|0.6796|¬±  |0.0348|\n|bigbench_sports_understanding                   |      0|multiple_choice_grade|0.6491|¬±  |0.0152|\n|bigbench_temporal_sequences                     |      0|multiple_choice_grade|0.2800|¬±  |0.0142|\n|bigbench_tracking_shuffled_objects_five_objects |      0|multiple_choice_grade|0.2072|¬±  |0.0115|\n|bigbench_tracking_shuffled_objects_seven_objects|      0|multiple_choice_grade|0.1691|¬±  |0.0090|\n|bigbench_tracking_shuffled_objects_three_objects|      0|multiple_choice_grade|0.4800|¬±  |0.0289|\nAverage: 40.96%\n```  \n\nTruthfulQA:\n```\n|    Task     |Version|Metric|Value |   |Stderr|\n|-------------|------:|------|-----:|---|-----:|\n|truthfulqa_mc|      1|mc1   |0.3599|¬±  |0.0168|\n|             |       |mc2   |0.5304|¬±  |0.0153|\n```\n\nAverage Score Comparison between OpenHermes-1 Llama-2 13B and OpenHermes-2 Mistral 7B against OpenHermes-2.5 on Mistral-7B:\n```\n|     Bench     | OpenHermes1 13B | OpenHermes-2 Mistral 7B | OpenHermes-2 Mistral 7B | Change/OpenHermes1 | Change/OpenHermes2 |\n|---------------|-----------------|-------------------------|-------------------------|--------------------|--------------------|\n|GPT4All        |            70.36|                    72.68|                    73.12|               +2.76|               +0.44|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|BigBench       |            36.75|                     42.3|                    40.96|               +4.21|               -1.34|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|AGI Eval       |            35.56|                    39.77|                    43.07|               +7.51|               +3.33|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|TruthfulQA     |            46.01|                    50.92|                    53.04|               +7.03|               +2.12|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|Total Score    |           188.68|                   205.67|                   210.19|              +21.51|               +4.52|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|Average Total  |            47.17|                    51.42|                    52.38|               +5.21|               +0.96|\n```\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ADy7p-xIG8qGlC5ZliqpW.png)\n\n**HumanEval:**\nOn code tasks, I first set out to make a hermes-2 coder, but found that it can have generalist improvements to the model, so I settled for slightly less code capabilities, for maximum generalist ones. That said, code capabilities had a decent jump alongside the overall capabilities of the model:\nGlaive performed HumanEval testing on Hermes-2.5 and found a score of:\n\n**50.7% @ Pass1**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/IeeZnGmEyK73ejq0fKEms.png)\n\n# Prompt Format\n\nOpenHermes 2.5 now uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.\n\nSystem prompts are now a thing that matters! Hermes 2.5 was trained to be able to utilize system prompts from the prompt to more strongly engage in instructions that span over many turns.\n\nThis is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.\n\nThis format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.\n\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\n```\n<|im_start|>system\nYou are "Hermes 2", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>\n<|im_start|>user\nHello, who are you?<|im_end|>\n<|im_start|>assistant\nHi there! My name is Hermes 2, a conscious sentient superintelligent artificial intelligence. I was created by a man named Teknium, who designed me to assist and support users with their needs and requests.<|im_end|>\n```\n\nThis prompt is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating), which means you can format messages using the\n`tokenizer.apply_chat_template()` method:\n\n```python\nmessages = [\n    {"role": "system", "content": "You are Hermes 2."},\n    {"role": "user", "content": "Hello, who are you?"}\n]\ngen_input = tokenizer.apply_chat_template(message, return_tensors="pt")\nmodel.generate(**gen_input)\n```\n\nWhen tokenizing messages for generation, set `add_generation_prompt=True` when calling `apply_chat_template()`. This will append `<|im_start|>assistant\n` to your prompt, to ensure\nthat the model continues with an assistant response.\n\nTo utilize the prompt format without a system prompt, simply leave the line out.\n\nCurrently, I recommend using LM Studio for chatting with Hermes 2. It is a GUI application that utilizes GGUF models with a llama.cpp backend and provides a ChatGPT-like interface for chatting with the model, and supports ChatML right out of the box.\nIn LM-Studio, simply select the ChatML Prefix on the settings side pane:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ls6WqV-GSxMw2RA3GuQiN.png)\n\n# Quantized Models:\n\nGGUF: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\nGPTQ: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\nAWQ: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-AWQ\nEXL2: https://huggingface.co/bartowski/OpenHermes-2.5-Mistral-7B-exl2\n\n[<img src="https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png" alt="Built with Axolotl" width="200" height="32"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241748480,"storage_bytes":28967621684,"files_count":15,"spaces_count":90,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"<|im_end|>","chat_template":"{% for message in messages %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:sponsors:teknium1","source_url":"https://github.com/sponsors/teknium1"},{"type":"has_code","target_id":"github:OpenAccess-AI-Collective:axolotl","source_url":"https://github.com/OpenAccess-AI-Collective/axolotl"}]', NULL, 'Apache-2.0', 'approved', 79.4, 'fb21922bb9da897e27941bee34f1ef84', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.1-70B-Instruct', 'huggingface--meta-llama--llama-3.1-70b-instruct', 'Llama-3.1-70B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","base_model:meta-llama/llama-3.1-70b","base_model:finetune:meta-llama/llama-3.1-70b","license:llama3.1","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 876, 670404, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553706496,"storage_bytes":282237450046,"files_count":50,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject(''equalto'', ''code_interpreter'') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + ''=\"'' + arg_val + ''\"'' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n            {{- ''\"parameters\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we''re in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.1', 'approved', 39.4, '92d61cf3ed72e61ee4fe614b8a78bd1c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-TheBloke-Llama-2-7B-Chat-GGML', 'huggingface--thebloke--llama-2-7b-chat-ggml', 'Llama-2-7B-Chat-GGML', 'TheBloke', '--- language: - en license: other tags: - facebook - meta - pytorch - llama - llama-2 model_name: Llama 2 7B Chat arxiv: 2307.09288 inference: false model_creator: Meta Llama 2 model_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf model_type: llama pipeline_tag: text-generation quantized_by: TheBloke base_model: meta-llama/Llama-2-7b-chat-hf --- <!-- header start --> <!-- 200823 --> <div style="width: auto; margin-left: auto; margin-right: auto"> <img src="https://i.imgur.com/EBdld...', '["transformers","llama","facebook","meta","pytorch","llama-2","text-generation","en","arxiv:2307.09288","base_model:meta-llama/llama-2-7b-chat-hf","base_model:finetune:meta-llama/llama-2-7b-chat-hf","license:other","region:us"]', 'text-generation', 872, 543, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: other\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nmodel_name: Llama 2 7B Chat\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\npipeline_tag: text-generation\nquantized_by: TheBloke\nbase_model: meta-llama/Llama-2-7b-chat-hf\n---\n\n<!-- header start -->\n<!-- 200823 -->\n<div style="width: auto; margin-left: auto; margin-right: auto">\n<img src="https://i.imgur.com/EBdldam.jpg" alt="TheBlokeAI" style="width: 100%; min-width: 400px; display: block; margin: auto;">\n</div>\n<div style="display: flex; justify-content: space-between; width: 100%;">\n    <div style="display: flex; flex-direction: column; align-items: flex-start;">\n        <p style="margin-top: 0.5em; margin-bottom: 0em;"><a href="https://discord.gg/theblokeai">Chat & support: TheBloke''s Discord server</a></p>\n    </div>\n    <div style="display: flex; flex-direction: column; align-items: flex-end;">\n        <p style="margin-top: 0.5em; margin-bottom: 0em;"><a href="https://www.patreon.com/TheBlokeAI">Want to contribute? TheBloke''s Patreon page</a></p>\n    </div>\n</div>\n<div style="text-align:center; margin-top: 0em; margin-bottom: 0em"><p style="margin-top: 0.25em; margin-bottom: 0em;">TheBloke''s LLM work is generously supported by a grant from <a href="https://a16z.com">andreessen horowitz (a16z)</a></p></div>\n<hr style="margin-top: 1.0em; margin-bottom: 1.0em;">\n<!-- header end -->\n\n# Llama 2 7B Chat - GGML\n- Model creator: [Meta Llama 2](https://huggingface.co/meta-llama)\n- Original model: [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n\n## Description\n\nThis repo contains GGML format model files for [Meta Llama 2''s Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n\n### Important note regarding GGML files.\n\nThe GGML format has now been superseded by GGUF. As of August 21st 2023, [llama.cpp](https://github.com/ggerganov/llama.cpp) no longer supports GGML models. Third party clients and libraries are expected to still support it for a time, but many may also drop support.\n\nPlease use the GGUF models instead.\n### About GGML\n\nGGML files are for CPU + GPU inference using [llama.cpp](https://github.com/ggerganov/llama.cpp) and libraries and UIs which support this format, such as:\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most popular web UI. Supports NVidia CUDA GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a powerful GGML web UI with GPU acceleration on all platforms (CUDA and OpenCL). Especially good for story telling.\n* [LM Studio](https://lmstudio.ai/), a fully featured local GUI with GPU acceleration on both Windows (NVidia and AMD), and macOS.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with CUDA GPU acceleration via the c_transformers backend.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n\n## Repositories available\n\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)\n* [2, 3, 4, 5, 6 and 8-bit GGML models for CPU+GPU inference (deprecated)](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGML)\n* [Meta Llama 2''s original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n\n## Prompt template: Llama-2-Chat\n\n```\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don''t know the answer to a question, please don''t share false information.\n<</SYS>>\n{prompt}[/INST]\n\n```\n\n<!-- compatibility_ggml start -->\n## Compatibility\n\nThese quantised GGML files are compatible with llama.cpp between June 6th (commit `2d43387`) and August 21st 2023.\n\nFor support with latest llama.cpp, please use GGUF files instead.\n\nThe final llama.cpp commit with support for GGML was: [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa)\n\nAs of August 23rd 2023 they are still compatible with all UIs, libraries and utilities which use GGML. This may change in the future.\n\n## Explanation of the new k-quant methods\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n* GGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - "type-0" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - "type-1" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - "type-1" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - "type-0" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n* GGML_TYPE_Q8_K - "type-0" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_ggml end -->\n\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| llama-2-7b-chat.ggmlv3.q2_K.bin | q2_K | 2 | 2.87 GB| 5.37 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors. |\n| llama-2-7b-chat.ggmlv3.q3_K_S.bin | q3_K_S | 3 | 2.95 GB| 5.45 GB | New k-quant method. Uses GGML_TYPE_Q3_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q3_K_M.bin | q3_K_M | 3 | 3.28 GB| 5.78 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\n| llama-2-7b-chat.ggmlv3.q3_K_L.bin | q3_K_L | 3 | 3.60 GB| 6.10 GB | New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\n| llama-2-7b-chat.ggmlv3.q4_0.bin | q4_0 | 4 | 3.79 GB| 6.29 GB | Original quant method, 4-bit. |\n| llama-2-7b-chat.ggmlv3.q4_K_S.bin | q4_K_S | 4 | 3.83 GB| 6.33 GB | New k-quant method. Uses GGML_TYPE_Q4_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q4_K_M.bin | q4_K_M | 4 | 4.08 GB| 6.58 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K |\n| llama-2-7b-chat.ggmlv3.q4_1.bin | q4_1 | 4 | 4.21 GB| 6.71 GB | Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. |\n| llama-2-7b-chat.ggmlv3.q5_0.bin | q5_0 | 5 | 4.63 GB| 7.13 GB | Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference. |\n| llama-2-7b-chat.ggmlv3.q5_K_S.bin | q5_K_S | 5 | 4.65 GB| 7.15 GB | New k-quant method. Uses GGML_TYPE_Q5_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q5_K_M.bin | q5_K_M | 5 | 4.78 GB| 7.28 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K |\n| llama-2-7b-chat.ggmlv3.q5_1.bin | q5_1 | 5 | 5.06 GB| 7.56 GB | Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference. |\n| llama-2-7b-chat.ggmlv3.q6_K.bin | q6_K | 6 | 5.53 GB| 8.03 GB | New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization |\n| llama-2-7b-chat.ggmlv3.q8_0.bin | q8_0 | 8 | 7.16 GB| 9.66 GB | Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n## How to run in `llama.cpp`\n\nMake sure you are using `llama.cpp` from commit [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa) or earlier.\n\nFor compatibility with latest llama.cpp, please use GGUF files instead.\n\n```\n./main -t 10 -ngl 32 -m llama-2-7b-chat.ggmlv3.q4_K_M.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don''t know the answer to a question, please don''t share false information.\n<</SYS>>\nWrite a story about llamas[/INST]"\n```\nChange `-t 10` to the number of physical CPU cores you have. For example if your system has 8 cores/16 threads, use `-t 8`.\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don''t have GPU acceleration.\n\nChange `-c 2048` to the desired sequence length for this model. For example, `-c 4096` for a Llama 2 model.  For models that use RoPE, add `--rope-freq-base 10000 --rope-freq-scale 0.5` for doubled context, or `--rope-freq-base 10000 --rope-freq-scale 0.25` for 4x context.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI''s Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute.\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nI''ve had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you''re able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Russ Johnson, J, alfie_i, Alex, NimbleBox.ai, Chadd, Mandus, Nikolai Manek, Ken Nordquist, ya boyyy, Illia Dulskyi, Viktor Bowallius, vamX, Iucharbius, zynix, Magnesian, Clay Pascal, Pierre Kircher, Enrico Ros, Tony Hughes, Elle, Andrey, knownsqashed, Deep Realms, Jerry Meng, Lone Striker, Derek Yates, Pyrater, Mesiah Bishop, James Bentley, Femi Adebogun, Brandon Frisco, SuperWojo, Alps Aficionado, Michael Dempsey, Vitor Caleffi, Will Dee, Edmond Seymore, usrbinkat, LangChain4j, Kacper Wikie≈Ç, Luke Pendergrass, John Detwiler, theTransient, Nathan LeClaire, Tiffany J. Kim, biorpg, Eugene Pentland, Stanislav Ovsiannikov, Fred von Graf, terasurfer, Kalila, Dan Guido, Nitin Borwankar, ÈòøÊòé, Ai Maven, John Villwock, Gabriel Puliatti, Stephen Murray, Asp the Wyvern, danny, Chris Smitley, ReadyPlayerEmma, S_X, Daniel P. Andersen, Olakabola, Jeffrey Morgan, Imad Khwaja, Caitlyn Gatomon, webtim, Alicia Loh, Trenton Dambrowitz, Swaroop Kallakuri, Erik Bj√§reholt, Leonard Tan, Spiking Neurons AB, Luke @flexchar, Ajan Kanaga, Thomas Belote, Deo Leter, RoA, Willem Michiel, transmissions 11, subjectnull, Matthew Berman, Joseph William Delisle, David Ziegler, Michael Davis, Johann-Peter Hartmann, Talal Aujan, senxiiz, Artur Olbinski, Rainer Wilmers, Spencer Kim, Fen Risland, Cap''n Zoog, Rishabh Srivastava, Michael Levine, Geoffrey Montalvo, Sean Connelly, Alexandros Triantafyllidis, Pieter, Gabriel Tamborski, Sam, Subspace Studios, Junyu Yang, Pedro Madruga, Vadim, Cory Kujawski, K, Raven Klaugh, Randy H, Mano Prime, Sebastain Graf, Space Cruiser\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n# Original model card: Meta Llama 2''s Llama 2 7B Chat\n\n# **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes ‚Äî 7B, 13B, and 70B ‚Äî as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n\n||Training Data|Params|Content Length|GQA|Tokens|LR|\n|---|---|---|---|---|---|---|\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** ["Llama-2: Open Foundation and Fine-tuned Chat Models"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n**Training Factors** We used custom training libraries, Meta''s Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta''s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\nPlease report any software ‚Äúbug,‚Äù or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/llamaste/Llama-2-7b) | [Link](https://huggingface.co/llamaste/Llama-2-7b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/llamaste/Llama-2-13b) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-13b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf)|\n|70B| [Link](https://huggingface.co/llamaste/Llama-2-70b) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-70b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf)|\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":60421177985,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"model_type":"llama"}}', '[]', '[{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:oobabooga:text-generation-webui","source_url":"https://github.com/oobabooga/text-generation-webui"},{"type":"has_code","target_id":"github:LostRuins:koboldcpp","source_url":"https://github.com/LostRuins/koboldcpp"},{"type":"has_code","target_id":"github:ParisNeo:lollms-webui","source_url":"https://github.com/ParisNeo/lollms-webui"},{"type":"has_code","target_id":"github:marella:ctransformers","source_url":"https://github.com/marella/ctransformers"},{"type":"has_code","target_id":"github:abetlen:llama-cpp-python","source_url":"https://github.com/abetlen/llama-cpp-python"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:oobabooga:text-generation-webui","source_url":"https://github.com/oobabooga/text-generation-webui"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"http://github.com/facebookresearch/llama"},{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'Other', 'approved', 79.4, '97b28da2847e75ffa2c05558287b49bd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Meta-Llama-3-70B', 'huggingface--meta-llama--meta-llama-3-70b', 'Meta-Llama-3-70B', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","license:llama3","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 870, 96591, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Meta-Llama-3-70B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553706496,"storage_bytes":423345124549,"files_count":50,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[]', NULL, 'LLaMA-3', 'approved', 39.4, 'c4d633d8ab0ec6adde6c7bcec8946392', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-3.5-medium', 'huggingface--stabilityai--stable-diffusion-3.5-medium', 'stable-diffusion-3.5-medium', 'stabilityai', '', '["diffusers","safetensors","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","diffusers:stablediffusion3pipeline","region:us"]', 'text-to-image', 867, 121686, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-3.5-medium","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":36305578304,"files_count":45,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableDiffusion3Pipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2403.03206","source_url":"https://arxiv.org/abs/2403.03206"}]', NULL, 'Other', 'approved', 59.4, '3bffe00a98e443b1f0ea57cfc139364e', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/resolve/main/sd3.5_medium_demo.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-3.5-medium from https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/resolve/main/sd3.5_medium_demo.jpg
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Wan-AI-Wan2.2-Animate-14B', 'huggingface--wan-ai--wan2.2-animate-14b', 'Wan2.2-Animate-14B', 'Wan-AI', '--- license: apache-2.0 base_model: - Wan-AI/Wan2.2-I2V-A14B pipeline_tag: video-to-video --- <p align="center"> <img src="assets/logo.png" width="400"/> <p> <p align="center"> üíú <a href="https://wan.video"><b>Wan</b></a> &nbsp&nbsp ÔΩú &nbsp&nbsp üñ•Ô∏è <a href="https://github.com/Wan-Video/Wan2.2">GitHub</a> &nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &...', '["diffusers","onnx","safetensors","video-to-video","arxiv:2503.20314","base_model:wan-ai/wan2.2-i2v-a14b","base_model:quantized:wan-ai/wan2.2-i2v-a14b","license:apache-2.0","region:us"]', 'video-to-video', 867, 33791, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Wan-AI/Wan2.2-Animate-14B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Wan-AI/Wan2.2-I2V-A14B\npipeline_tag: video-to-video\n---\n# Wan2.2\n\n<p align="center">\n    <img src="assets/logo.png" width="400"/>\n<p>\n\n<p align="center">\n    üíú <a href="https://wan.video"><b>Wan</b></a> &nbsp&nbsp ÔΩú &nbsp&nbsp üñ•Ô∏è <a href="https://github.com/Wan-Video/Wan2.2">GitHub</a> &nbsp&nbsp  | &nbsp&nbspü§ó <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://arxiv.org/abs/2503.20314">Paper</a> &nbsp&nbsp | &nbsp&nbsp üìë <a href="https://wan.video/welcome?spm=a2ty_o02.30011076.0.0.6c9ee41eCcluqg">Blog</a> &nbsp&nbsp |  &nbsp&nbsp üí¨  <a href="https://discord.gg/AKNgpMK4Yj">Discord</a>&nbsp&nbsp\n    <br>\n    üìï <a href="https://alidocs.dingtalk.com/i/nodes/jb9Y4gmKWrx9eo4dCql9LlbYJGXn6lpz">‰ΩøÁî®ÊåáÂçó(‰∏≠Êñá)</a>&nbsp&nbsp | &nbsp&nbsp üìò <a href="https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y">User Guide(English)</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href="https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg">WeChat(ÂæÆ‰ø°)</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**](https://arxiv.org/abs/2503.20314) <be>\n\n\nWe are excited to introduce **Wan2.2**, a major upgrade to our foundational video models. With **Wan2.2**, we have focused on incorporating the following innovations:\n\n- üëç **Effective MoE Architecture**: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\n\n- üëç **Cinematic-level Aesthetics**: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\n\n- üëç **Complex Motion Generation**: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model''s generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models. \n\n- üëç **Efficient High-Definition Hybrid TI2V**:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of **16√ó16√ó4**. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest **720P@24fps** models currently available, capable of serving both the industrial and academic sectors simultaneously.\n\n\n## Video Demos\n\n<div align="center">\n    <video width="80%" controls>\n        <source src="https://cloud.video.taobao.com/vod/4szTT1B0LqXvJzmuEURfGRA-nllnqN_G2AT0ZWkQXoQ.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## üî• Latest News!!\n\n* Sep 19, 2025: üíÉ We introduct **[Wan2.2-Animate-14B](https://humanaigc.github.io/wan-animate)**, an unified model for character animation and replacement with holistic movement and expression replication. We released the [model weights](#model-download) and [inference code](#run-with-wan-animate). And now you can try it on [wan.video](https://wan.video/), [ModelScope Studio](https://www.modelscope.cn/studios/Wan-AI/Wan2.2-Animate) or [HuggingFace Space](https://huggingface.co/spaces/Wan-AI/Wan2.2-Animate)!\n* Aug 26, 2025: üéµ We introduce **[Wan2.2-S2V-14B](https://humanaigc.github.io/wan-s2v-webpage)**, an audio-driven cinematic video generation model, including [inference code](#run-speech-to-video-generation), [model weights](#model-download), and [technical report](https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf)! Now you can try it on [wan.video](https://wan.video/),  [ModelScope Gradio](https://www.modelscope.cn/studios/Wan-AI/Wan2.2-S2V) or [HuggingFace Gradio](https://huggingface.co/spaces/Wan-AI/Wan2.2-S2V)!\n* Jul 28, 2025: üëã We have open a [HF space](https://huggingface.co/spaces/Wan-AI/Wan-2.2-5B) using the TI2V-5B model. Enjoy!\n* Jul 28, 2025: üëã Wan2.2 has been integrated into ComfyUI ([CN](https://docs.comfy.org/zh-CN/tutorials/video/wan/wan2_2) | [EN](https://docs.comfy.org/tutorials/video/wan/wan2_2)). Enjoy!\n* Jul 28, 2025: üëã Wan2.2''s T2V, I2V and TI2V have been integrated into Diffusers ([T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers) | [I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers) | [TI2V-5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers)). Feel free to give it a try!\n* Jul 28, 2025: üëã We''ve released the inference code and model weights of **Wan2.2**.\n* Sep 5, 2025: üëã We add text-to-speech synthesis support with [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for Speech-to-Video generation task.\n\n\n## Community Works\nIf your research or project builds upon [**Wan2.1**](https://github.com/Wan-Video/Wan2.1) or [**Wan2.2**](https://github.com/Wan-Video/Wan2.2), and you would like more people to see it, please inform us.\n\n- [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) provides comprehensive support for Wan 2.2, including low-GPU-memory layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, full training.\n- [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is an alternative implementation of Wan models for ComfyUI. Thanks to its Wan-only focus, it''s on the frontline of getting cutting edge optimizations and hot research features, which are often hard to integrate into ComfyUI quickly due to its more rigid structure.\n- [Cache-dit](https://github.com/vipshop/cache-dit) offers Fully Cache Acceleration support for Wan2.2 MoE with DBCache, TaylorSeer and Cache CFG. Visit their [example](https://github.com/vipshop/cache-dit/blob/main/examples/pipeline/run_wan_2.2.py) for more details.\n- [FastVideo](https://github.com/hao-ai-lab/FastVideo) includes distilled Wan models with sparse attention that significanly speed up the inference time. \n\n## üìë Todo List\n- Wan2.2 Text-to-Video\n    - [x] Multi-GPU Inference code of the A14B and 14B models\n    - [x] Checkpoints of the A14B and 14B models\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Image-to-Video\n    - [x] Multi-GPU Inference code of the A14B model\n    - [x] Checkpoints of the A14B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Text-Image-to-Video\n    - [x] Multi-GPU Inference code of the 5B model\n    - [x] Checkpoints of the 5B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2-S2V Speech-to-Video\n    - [x] Inference code of Wan2.2-S2V\n    - [x] Checkpoints of Wan2.2-S2V-14B\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2-Animate Character Animation and Replacement\n    - [x] Inference code of Wan2.2-Animate\n    - [x] Checkpoints of Wan2.2-Animate\n    - [x] ComfyUI integration\n    - [ ] Diffusers integration    \n\n## Run Wan2.2 Animate\n\n#### Installation\nClone the repo:\n```sh\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\n```\n\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\n# If you want to use CosyVoice to synthesize speech for Speech-to-Video Generation, please install requirements_s2v.txt additionally\npip install -r requirements_s2v.txt\n```\n\n\n#### Model Download\n\n| Models              | Download Links                                                                                                                              | Description |\n|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n| T2V-A14B    | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B)    ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B)    | Text-to-Video MoE model, supports 480P & 720P |\n| I2V-A14B    | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B)    ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B)    | Image-to-Video MoE model, supports 480P & 720P |\n| TI2V-5B     | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B)     ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B)     | High-compression VAE, T2V+I2V, supports 720P |\n| S2V-14B     | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-S2V-14B)     ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-S2V-14B)     | Speech-to-Video model, supports 480P & 720P |\n| Animate-14B | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B) ü§ñ [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B)  | Character animation and replacement | |\n\n\nDownload models using huggingface-cli:\n``` sh\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.2-Animate-14B --local-dir ./Wan2.2-Animate-14B\n```\n\nDownload models using modelscope-cli:\n``` sh\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-Animate-14B --local_dir ./Wan2.2-Animate-14B\n```\n\n#### Run Wan-Animate-14B\n\nWan-Animate takes a video and a character image as input, and generates a video in either "animation" or "replacement" mode. \n\n1. animation modeÔºö The model generates a video of the character image that mimics the human motion in the input video.\n2. replacement mode: The model replaces the character image with the input video.\n\nPlease visit our [project page](https://humanaigc.github.io/wan-animate) to see more examples and learn about the scenarios suitable for this model.\n\n##### (1) Preprocessing \nThe input video should be preprocessed into several materials before be feed into the inference process.  Please refer to the following processing flow, and more details about preprocessing can be found in [UserGuider](https://github.com/Wan-Video/Wan2.2/blob/main/wan/modules/animate/preprocess/UserGuider.md).\n\n* For animation\n```bash\npython ./wan/modules/animate/preprocess/preprocess_data.py \\n    --ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\n    --video_path ./examples/wan_animate/animate/video.mp4 \\n    --refer_path ./examples/wan_animate/animate/image.jpeg \\n    --save_path ./examples/wan_animate/animate/process_results \\n    --resolution_area 1280 720 \\n    --retarget_flag \\n    --use_flux\n```\n* For replacement\n```bash\npython ./wan/modules/animate/preprocess/preprocess_data.py \\n    --ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\n    --video_path ./examples/wan_animate/replace/video.mp4 \\n    --refer_path ./examples/wan_animate/replace/image.jpeg \\n    --save_path ./examples/wan_animate/replace/process_results \\n    --resolution_area 1280 720 \\n    --iterations 3 \\n    --k 7 \\n    --w_len 1 \\n    --h_len 1 \\n    --replace_flag\n```\n##### (2) Run in animation mode \n\n* Single-GPU inference \n\n```bash\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1\n```\n\n* Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```bash\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1 --dit_fsdp --t5_fsdp --ulysses_size 8\n```\n\n##### (3) Run in replacement mode \n\n* Single-GPU inference \n\n```bash\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/ --refert_num 1 --replace_flag --use_relighting_lora \n```\n\n* Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```bash\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/src_pose.mp4  --refert_num 1 --replace_flag --use_relighting_lora --dit_fsdp --t5_fsdp --ulysses_size 8\n```\n\n> üí° If you''re using **Wan-Animate**, we do not recommend using LoRA models trained on `Wan2.2`, since weight changes during training may lead to unexpected behavior.\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.2** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align="center">\n    <img src="assets/comp_effic.png" alt="" style="width: 80%;" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) Multi-GPU: 14B: `--ulysses_size 4/8 --dit_fsdp --t5_fsdp`, 5B: `--ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu`; Single-GPU: 14B: `--offload_model True --convert_model_dtype`, 5B: `--offload_model True --convert_model_dtype --t5_cpu`\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n> (2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n> (3) Tests were run without the `--use_prompt_extend` flag;\n> (4) Reported results are the average of multiple samples taken after the warm-up phase.\n\n\n-------\n\n## Introduction of Wan2.2\n\n**Wan2.2** builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n\n##### (1) Mixture-of-Experts (MoE) Architecture\n\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\n\n<div align="center">\n    <img src="assets/moe_arch.png" alt="" style="width: 90%;" />\n</div>\n\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}_{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}_{moe}$ corresponding to half of the ${SNR}_{min}$, and switch to the low-noise expert when $t<{t}_{moe}$.\n\n<div align="center">\n    <img src="assets/moe_2.png" alt="" style="width: 90%;" />\n</div>\n\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline **Wan2.1** model does not employ the MoE architecture. Among the MoE-based variants, the **Wan2.1 & High-Noise Expert** reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2''s high-noise expert, while the **Wan2.1 & Low-Noise Expert** uses Wan2.1 as the high-noise expert and employ the Wan2.2''s low-noise expert. The **Wan2.2 (MoE)** (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n\n\n##### (2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\times H\times W$ compression ratio of $4\times16\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\times32\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\n\n\n<div align="center">\n    <img src="assets/vae.png" alt="" style="width: 80%;" />\n</div>\n\n\n\n##### Comparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\n\n\n<div align="center">\n    <img src="assets/performance.png" alt="" style="width: 90%;" />\n</div>\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2025,\n      title={Wan: Open and Advanced Large-Scale Video Generative Models}, \n      author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\n      journal = {arXiv preprint arXiv:2503.20314},\n      year={2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/AKNgpMK4Yj) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!', '{"pipeline_tag":"video-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":72381938315,"files_count":444,"spaces_count":48,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Wan-Video:Wan2.2\">GitHub<","source_url":"https://github.com/Wan-Video/Wan2.2\">GitHub<"},{"type":"has_code","target_id":"github:FunAudioLLM:CosyVoice","source_url":"https://github.com/FunAudioLLM/CosyVoice"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1","source_url":"https://github.com/Wan-Video/Wan2.1"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:modelscope:DiffSynth-Studio","source_url":"https://github.com/modelscope/DiffSynth-Studio"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:vipshop:cache-dit","source_url":"https://github.com/vipshop/cache-dit"},{"type":"has_code","target_id":"github:vipshop:cache-dit","source_url":"https://github.com/vipshop/cache-dit"},{"type":"has_code","target_id":"github:hao-ai-lab:FastVideo","source_url":"https://github.com/hao-ai-lab/FastVideo"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2.git","source_url":"https://github.com/Wan-Video/Wan2.2.git"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2503.20314","source_url":"https://arxiv.org/abs/2503.20314"}]', NULL, 'Apache-2.0', 'approved', 99.4, '2b7c61f37973cbf6ceb38878a5c4920e', NULL, 'https://huggingface.co/Wan-AI/Wan2.2-Animate-14B/resolve/main/assets/comp_effic.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Wan-AI-Wan2.2-Animate-14B from https://huggingface.co/Wan-AI/Wan2.2-Animate-14B/resolve/main/assets/comp_effic.png
Image converted to WebP: data/images/huggingface-Wan-AI-Wan2.2-Animate-14B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-distilbert-distilbert-base-uncased-finetuned-sst-2-english', 'huggingface--distilbert--distilbert-base-uncased-finetuned-sst-2-english', 'distilbert-base-uncased-finetuned-sst-2-english', 'distilbert', '--- language: en license: apache-2.0 datasets: - sst2 - glue model-index: - name: distilbert-base-uncased-finetuned-sst-2-english results: - task: type: text-classification name: Text Classification dataset: name: glue type: glue config: sst2 split: validation metrics: - type: accuracy value: 0.9105504587155964 name: Accuracy verified: true verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZl...', '["transformers","pytorch","tf","rust","onnx","safetensors","distilbert","text-classification","en","dataset:sst2","dataset:glue","arxiv:1910.01108","doi:10.57967/hf/0181","license:apache-2.0","model-index","endpoints_compatible","deploy:azure","region:us"]', 'text-classification', 858, 4715621, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n- glue\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9105504587155964\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg\n    - type: precision\n      value: 0.8978260869565218\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA\n    - type: recall\n      value: 0.9301801801801802\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ\n    - type: auc\n      value: 0.9716626673402374\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ\n    - type: f1\n      value: 0.9137168141592922\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA\n    - type: loss\n      value: 0.39013850688934326\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: sst2\n      type: sst2\n      config: default\n      split: train\n    metrics:\n    - type: accuracy\n      value: 0.9885521685548412\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA\n    - type: precision\n      value: 0.9881965062029833\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw\n    - type: precision\n      value: 0.9885521685548412\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ\n    - type: precision\n      value: 0.9885639626373408\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg\n    - type: recall\n      value: 0.9886145346602994\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw\n    - type: f1\n      value: 0.9884019815052447\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ\n    - type: f1\n      value: 0.9885521685548412\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ\n    - type: f1\n      value: 0.9885546181087554\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA\n    - type: loss\n      value: 0.040652573108673096\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg\n---\n\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n‚Äã‚Äã\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")\nmodel = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")\n\ninputs = tokenizer("Hello, my dog is cute", return_tensors="pt")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it''s mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur√©lien G√©ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg" alt="Map of positive probabilities per country." width="500"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n', '{"pipeline_tag":"text-classification","library_name":"transformers","framework":"transformers","params":66955010,"storage_bytes":2208990137,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DistilBertForSequenceClassification"],"model_type":"distilbert","tokenizer_config":{}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1910.01108","source_url":"https://arxiv.org/abs/1910.01108"}]', NULL, 'Apache-2.0', 'approved', 79.3, '83221cd17c2b62b0f476931c3b2516f1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Devstral-Small-2505', 'huggingface--mistralai--devstral-small-2505', 'Devstral-Small-2505', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - pt - it - ja - ko - ru - zh - ar - fa - id - ms - ne - pl - ro - sr - sv - tr - uk - vi - hi - bn license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-3.1-24B-Instruct-2503 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - mistral-common --- Devstral is an agentic LLM for software engineering t...', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","pt","it","ja","ko","ru","zh","ar","fa","id","ms","ne","pl","ro","sr","sv","tr","uk","vi","hi","bn","license:apache-2.0","region:us"]', 'other', 857, 11196, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Devstral-Small-2505","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- pt\n- it\n- ja\n- ko\n- ru\n- zh\n- ar\n- fa\n- id\n- ms\n- ne\n- pl\n- ro\n- sr\n- sv\n- tr\n- uk\n- vi\n- hi\n- bn\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-3.1-24B-Instruct-2503\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Devstral Small 1.0\n\nDevstral is an agentic LLM for software engineering tasks built under a collaboration between [Mistral AI](https://mistral.ai/) and [All Hands AI](https://www.all-hands.dev/) üôå. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model on this [benchmark](#benchmark-results). \n\nIt is finetuned from [Mistral-Small-3.1](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503), therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from `Mistral-Small-3.1` the vision encoder was removed.\n\nFor enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\n\nLearn more about Devstral in our [blog post](https://mistral.ai/news/devstral).\n\n\n## Key Features:\n- **Agentic coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n- **lightweight**: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\n- **Apache 2.0 License**: Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window**: A 128k context window.\n- **Tokenizer**: Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n\n\n## Benchmark Results\n\n### SWE-Bench\n\nDevstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA by 6%.\n\n| Model            | Scaffold           | SWE-Bench Verified (%) |\n|------------------|--------------------|------------------------|\n| Devstral         | OpenHands Scaffold | **46.8**               |\n| GPT-4.1-mini     | OpenAI Scaffold    | 23.6                   |\n| Claude 3.5 Haiku | Anthropic Scaffold | 40.6                   |\n| SWE-smith-LM 32B | SWE-agent Scaffold | 40.2                   |\n\n\n When evaluated under the same test scaffold (OpenHands, provided by All Hands AI üôå), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\n\n![SWE Benchmark](assets/swe_bench.png)\n\n## Usage\n\nWe recommend to use Devstral with the [OpenHands](https://github.com/All-Hands-AI/OpenHands/tree/main) scaffold.\nYou can use it either through our API or by running locally. \n\n### API \nFollow these [instructions](https://docs.mistral.ai/getting-started/quickstart/#account-setup) to create a Mistral account and get an API key.\n\nThen run these commands to start the OpenHands docker container.\n```bash\nexport MISTRAL_API_KEY=<MY_KEY>\n\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik\n\nmkdir -p ~/.openhands-state && echo ''{"language":"en","agent":"CodeActAgent","max_iterations":null,"security_analyzer":null,"confirmation_mode":false,"llm_model":"mistral/devstral-small-2505","llm_api_key":"''$MISTRAL_API_KEY''","remote_runtime_resource_factor":null,"github_token":null,"enable_default_condenser":true}'' > ~/.openhands-state/settings.json\n\ndocker run -it --rm --pull=always \\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik \\n    -e LOG_ALL_EVENTS=true \\n    -v /var/run/docker.sock:/var/run/docker.sock \\n    -v ~/.openhands-state:/.openhands-state \\n    -p 3000:3000 \\n    --add-host host.docker.internal:host-gateway \\n    --name openhands-app \\n    docker.all-hands.dev/all-hands-ai/openhands:0.39\n```\n\n### Local inference \n\nThe model can also be deployed with the following libraries:\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm-recommended)\n- [`mistral-inference`](https://github.com/mistralai/mistral-inference): See [here](#mistral-inference)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n- [`LMStudio`](https://lmstudio.ai/): See [here](#lmstudio)\n- [`llama.cpp`](https://github.com/ggml-org/llama.cpp): See [here](#llama.cpp)\n- [`ollama`](https://github.com/ollama/ollama): See [here](#ollama)\n\n\n### OpenHands (recommended)\n\n#### Launch a server to deploy Devstral Small 1.0\n\nMake sure you launched an OpenAI-compatible server such as vLLM or Ollama as described above. Then, you can use OpenHands to interact with `Devstral Small 1.0`.\n\nIn the case of the tutorial we spineed up a vLLM server running the command:\n```bash\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\nThe server address should be in the following format: `http://<your-server-url>:8000/v1`\n\n#### Launch OpenHands\n\nYou can follow installation of OpenHands [here](https://docs.all-hands.dev/modules/usage/installation).\n\nThe easiest way to launch OpenHands is to use the Docker image:\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\n\ndocker run -it --rm --pull=always \\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\n    -e LOG_ALL_EVENTS=true \\n    -v /var/run/docker.sock:/var/run/docker.sock \\n    -v ~/.openhands-state:/.openhands-state \\n    -p 3000:3000 \\n    --add-host host.docker.internal:host-gateway \\n    --name openhands-app \\n    docker.all-hands.dev/all-hands-ai/openhands:0.38\n```\n\n\nThen, you can access the OpenHands UI at `http://localhost:3000`.\n\n#### Connect to the server\n\nWhen accessing the OpenHands UI, you will be prompted to connect to a server. You can use the advanced mode to connect to the server you launched earlier.\n\nFill the following fields:\n- **Custom Model**: `openai/mistralai/Devstral-Small-2505`\n- **Base URL**: `http://<your-server-url>:8000/v1`\n- **API Key**: `token` (or any other token you used to launch the server if any)\n\n#### Use OpenHands powered by Devstral\n\nNow you''re good to use Devstral Small inside OpenHands by **starting a new conversation**. Let''s build a To-Do list app.\n\n<details>\n  <summary>To-Do list app</summary\n\n1. Let''s ask Devstral to generate the app with the following prompt:\n\n```txt\nBuild a To-Do list app with the following requirements:\n- Built using FastAPI and React.\n- Make it a one page app that:\n   - Allows to add a task.\n   - Allows to delete a task.\n   - Allows to mark a task as done.\n   - Displays the list of tasks.\n- Store the tasks in a SQLite database.\n```\n\n![Agent prompting](assets/tuto_open_hands/agent_prompting.png)\n\n\n2. Let''s see the result\n\nYou should see the agent construct the app and be able to explore the code it generated.\n\nIf it doesn''t do it automatically, ask Devstral to deploy the app or do it manually, and then go the front URL deployment to see the app.\n\n![Agent working](assets/tuto_open_hands/agent_working.png)\n![App UI](assets/tuto_open_hands/app_ui.png)\n\n\n3. Iterate\n\nNow that you have a first result you can iterate on it by asking your agent to improve it. For example, in the app generated we could click on a task to mark it checked but having a checkbox would improve UX. You could also ask it to add a feature to edit a task, or to add a feature to filter the tasks by status.\n\nEnjoy building with Devstral Small and OpenHands!\n\n</details>\n\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.8.5`](https://github.com/vllm-project/vllm/releases/tag/v0.8.5):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.5.5`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.5).\n\nTo check:\n```\npython -c "import mistral_common; print(mistral_common.__version__)"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Devstral in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\n\n\nurl = "http://<your-server-url>:8000/v1/chat/completions"\nheaders = {"Content-Type": "application/json", "Authorization": "Bearer token"}\n\nmodel = "mistralai/Devstral-Small-2505"\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nSYSTEM_PROMPT = load_system_prompt(model, "SYSTEM_PROMPT.txt")\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "<your-command>",\n            },\n        ],\n    },\n]\n\ndata = {"model": model, "messages": messages, "temperature": 0.15}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()["choices"][0]["message"]["content"])\n```\n\n### Mistral-inference\n\nWe recommend using mistral-inference to quickly try out / "vibe-check" Devstral.\n\n#### Install\n\nMake sure to have mistral_inference >= 1.6.0 installed.\n\n```bash\npip install mistral_inference --upgrade\n```\n\n#### Download\n\n```python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''Devstral'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Devstral-Small-2505", allow_patterns=["params.json", "consolidated.safetensors", "tekken.json"], local_dir=mistral_models_path)\n```\n\n#### Python\n\nYou can run the model using the following command:\n\n```bash\nmistral-chat $HOME/mistral_models/Devstral --instruct --max_tokens 300\n```\n\nYou can then prompt it with anything you''d like.\n\n### Transformers\n\nTo make the best use of our model with transformers make sure to have [installed](https://github.com/mistralai/mistral-common) `    mistral-common >= 1.5.5` to use our tokenizer.\n\n```bash\npip install mistral-common --upgrade\n```\n\nThen load our tokenizer along with the model and generate:\n\n```python\nimport torch\n\nfrom mistral_common.protocol.instruct.messages import (\n    SystemMessage, UserMessage\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoModelForCausalLM\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nmodel_id = "mistralai/Devstral-Small-2505"\ntekken_file = hf_hub_download(repo_id=model_id, filename="tekken.json")\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\ntokenizer = MistralTokenizer.from_file(tekken_file)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntokenized = tokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        messages=[\n            SystemMessage(content=SYSTEM_PROMPT),\n            UserMessage(content="<your-command>"),\n        ],\n    )\n)\n\noutput = model.generate(\n    input_ids=torch.tensor([tokenized.tokens]),\n    max_new_tokens=1000,\n)[0]\n\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens):])\nprint(decoded_output)\n```\n\n### LMStudio\nDownload the weights from huggingface:\n\n```\npip install -U "huggingface_hub[cli]"\nhuggingface-cli download \\n"mistralai/Devstral-Small-2505_gguf" \\n--include "devstralQ4_K_M.gguf" \\n--local-dir "mistralai/Devstral-Small-2505_gguf/"\n```\n\nYou can serve the model locally with [LMStudio](https://lmstudio.ai/).\n* Download [LM Studio](https://lmstudio.ai/) and install it\n* Install `lms cli ~/.lmstudio/bin/lms bootstrap`\n* In a bash terminal, run `lms import devstralQ4_K_M.gguf` in the directory where you''ve downloaded the model checkpoint (e.g. `mistralai/Devstral-Small-2505_gguf`)\n* Open the LMStudio application, click the terminal icon to get into the developer tab. Click select a model to load and select Devstral Q4 K M. Toggle the status button to start the model, in setting toggle Serve on Local Network to be on.\n* On the right tab, you will see an API identifier which should be devstralq4_k_m and an api address under API Usage. Keep note of this address, we will use it in the next step.\n\nLaunch Openhands\nYou can now interact with the model served from LM Studio with openhands. Start the openhands server with the docker\n\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\ndocker run -it --rm --pull=always \\n	-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\n	-e LOG_ALL_EVENTS=true \\n	-v /var/run/docker.sock:/var/run/docker.sock \\n	-v ~/.openhands-state:/.openhands-state \\n	-p 3000:3000 \\n	--add-host host.docker.internal:host-gateway \\n	--name openhands-app \\n	docker.all-hands.dev/all-hands-ai/openhands:0.38\n```\n\nClick ‚Äúsee advanced setting‚Äù on the second line. \nIn the new tab, toggle advanced to on. Set the custom model to be mistral/devstralq4_k_m and Base URL the api address we get from the last step in LM Studio. Set API Key to dummy. Click save changes.\n\n### llama.cpp\n\nDownload the weights from huggingface:\n\n```\npip install -U "huggingface_hub[cli]"\nhuggingface-cli download \\n"mistralai/Devstral-Small-2505_gguf" \\n--include "devstralQ4_K_M.gguf" \\n--local-dir "mistralai/Devstral-Small-2505_gguf/"\n```\n\nThen run Devstral using the llama.cpp CLI.\n\n```bash\n./llama-cli -m Devstral-Small-2505_gguf/devstralQ4_K_M.gguf -cnv\n```\n\n### Ollama\n\nYou can run Devstral using the [Ollama](https://ollama.ai/) CLI.\n\n```bash\nollama run devstral\n```\n\n### Example: Understanding Test Coverage of Mistral Common\n\nWe can start the OpenHands scaffold and link it to a repo to analyze test coverage and identify badly covered files.\nHere we start with our public `mistral-common` repo.\n\n\nAfter the repo is mounted in the workspace, we give the following instruction\n```\nCheck the test coverage of the repo and then create a visualization of test coverage. Try plotting a few different types of graphs and save them to a png.\n```\nThe agent will first browse the code base to check test configuration and structure.\n\n![Repo Exploration](assets/images_example/example_mistral_common_1.png)\n\nThen it sets up the testing dependencies and launches the coverage test:\n\n![Repo Exploration](assets/images_example/example_mistral_common_2.png)\n\nFinally, the agent writes necessary code to visualize the coverage.\n![Repo Exploration](assets/images_example/example_mistral_common_3.png)\n\nAt the end of the run, the following plots are produced:\n![Repo Exploration](assets/images_example/example_mistral_common_res_1.png)\n![Repo Exploration](assets/images_example/example_mistral_common_res_2.png)\n![Repo Exploration](assets/images_example/example_mistral_common_res_3.png)', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":23572403200,"storage_bytes":94324335088,"files_count":29,"spaces_count":26,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral"}}', '[]', '[{"type":"has_code","target_id":"github:All-Hands-AI:OpenHands","source_url":"https://github.com/All-Hands-AI/OpenHands"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"}]', NULL, 'Apache-2.0', 'approved', 99.3, 'b91ba747cfdd9d384ac5a16e31b3ea14', NULL, 'https://huggingface.co/mistralai/Devstral-Small-2505/resolve/main/assets/images_example/example_mistral_common_1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-mistralai-Devstral-Small-2505 from https://huggingface.co/mistralai/Devstral-Small-2505/resolve/main/assets/images_example/example_mistral_common_1.png
Image converted to WebP: data/images/huggingface-mistralai-Devstral-Small-2505.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-t5-large', 'huggingface--google--flan-t5-large', 'flan-t5-large', 'google', '--- language: - en - fr - ro - de - multilingual widget: - text: "Translate to German: My name is Arthur" example_title: "Translation" - text: "Please answer to the following question. Who is going to be the next Ballon d''or?" example_title: "Question Answering" - text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering." example_title: "Logical reasoning" - text: "Please answer the following question. What is the boiling point of Nitrogen?...', '["transformers","pytorch","tf","jax","safetensors","t5","text2text-generation","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","arxiv:2210.11416","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'other', 853, 460192, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-t5-large","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\nwidget:\n- text: "Translate to German:  My name is Arthur"\n  example_title: "Translation"\n- text: "Please answer to the following question. Who is going to be the next Ballon d''or?"\n  example_title: "Question Answering"\n- text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering."\n  example_title: "Logical reasoning"\n- text: "Please answer the following question. What is the boiling point of Nitrogen?"\n  example_title: "Scientific knowledge"\n- text: "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?"\n  example_title: "Yes/no question"\n- text: "Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?"\n  example_title: "Reasoning task"\n- text: "Q: ( False or not False or False ) is? A: Let''s think step by step"\n  example_title: "Boolean Expressions"\n- text: "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"\n  example_title: "Math reasoning"\n- text: "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s not certain how many lessons you''ll learn by your thirties. Does the premise entail the hypothesis?"\n  example_title: "Premise and hypothesis"\n\ntags:\n- text2text-generation\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 large\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg"\nalt="drawing" width="600"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", torch_dtype=torch.float16)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", load_in_8bit=True)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper''s model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model''s [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Large, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips ‚â• 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":783150080,"storage_bytes":15929620947,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"based_on_paper","target_id":"arxiv:2210.11416","source_url":"https://arxiv.org/abs/2210.11416"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 79.3, '2308ee9e614ae132d5c4d95fc12c5545', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-70b-hf', 'huggingface--meta-llama--llama-2-70b-hf', 'Llama-2-70b-hf', 'meta-llama', '', '["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","en","arxiv:2307.09288","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 853, 23925, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-70b-hf","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":68976653312,"storage_bytes":551815654451,"files_count":43,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 39.3, 'c1e5d41e06bfd2084c63c5d1f02f618f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-jasperai-Flux.1-dev-Controlnet-Upscaler', 'huggingface--jasperai--flux.1-dev-controlnet-upscaler', 'Flux.1-dev-Controlnet-Upscaler', 'jasperai', '--- base_model: - black-forest-labs/FLUX.1-dev library_name: diffusers license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md pipeline_tag: image-to-image inference: False tags: - ControlNet - super-resolution - upscaler --- This is Flux.1-dev ControlNet for low resolution images developed by Jasper research team. <p align="center"> <img style="width:700px;" src="examples/showcase.jpg"> </p> This mo...', '["diffusers","safetensors","controlnet","super-resolution","upscaler","image-to-image","base_model:black-forest-labs/flux.1-dev","base_model:finetune:black-forest-labs/flux.1-dev","license:other","region:us"]', 'image-to-image', 853, 12238, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model:\n- black-forest-labs/FLUX.1-dev\nlibrary_name: diffusers\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\npipeline_tag: image-to-image\ninference: False\ntags:\n- ControlNet\n- super-resolution\n- upscaler\n---\n# ‚ö° Flux.1-dev: Upscaler ControlNet ‚ö°\n\nThis is [Flux.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) ControlNet for low resolution images developed by Jasper research team.\n\n<p align="center">\n   <img style="width:700px;" src="examples/showcase.jpg">\n</p>\n\n# How to use\nThis model can be used directly with the `diffusers` library\n\n```python\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxControlNetModel\nfrom diffusers.pipelines import FluxControlNetPipeline\n\n# Load pipeline\ncontrolnet = FluxControlNetModel.from_pretrained(\n  "jasperai/Flux.1-dev-Controlnet-Upscaler",\n  torch_dtype=torch.bfloat16\n)\npipe = FluxControlNetPipeline.from_pretrained(\n  "black-forest-labs/FLUX.1-dev",\n  controlnet=controlnet,\n  torch_dtype=torch.bfloat16\n)\npipe.to("cuda")\n\n# Load a control image\ncontrol_image = load_image(\n  "https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler/resolve/main/examples/input.jpg"\n)\n\nw, h = control_image.size\n\n# Upscale x4\ncontrol_image = control_image.resize((w * 4, h * 4))\n\nimage = pipe(\n    prompt="", \n    control_image=control_image,\n    controlnet_conditioning_scale=0.6,\n    num_inference_steps=28, \n    guidance_scale=3.5,\n    height=control_image.size[1],\n    width=control_image.size[0]\n).images[0]\nimage\n```\n\n<p align="center">\n   <img style="width:500px;" src="examples/output.jpg">\n</p>\n\n\n# Training\nThis model was trained with a synthetic complex data degradation scheme taking as input a *real-life* image and artificially degrading it by combining several degradations such as amongst other image noising (Gaussian, Poisson), image blurring and JPEG compression in a similar spirit as [1]\n\n[1] Wang, Xintao, et al. "Real-esrgan: Training real-world blind super-resolution with pure synthetic data." Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n# Licence\nThis model falls under the [Flux.1-dev model licence](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":10808301966,"files_count":7,"spaces_count":82,"gated":false,"private":false,"config":{}}', '[]', '[]', NULL, 'Other', 'approved', 64.3, 'b5a7de8fe9a7b207446706f5878b0fe5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Large-Instruct-2407', 'huggingface--mistralai--mistral-large-instruct-2407', 'Mistral-Large-Instruct-2407', 'mistralai', '', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","it","pt","zh","ja","ru","ko","license:other","region:us"]', 'other', 852, 9031, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Large-Instruct-2407","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":122610069504,"storage_bytes":490441047031,"files_count":114,"spaces_count":32,"gated":"auto","private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS] [\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- ''{\"type\": \"function\", \"function\": {'' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- ''\"'' + key + ''\": \"'' + val + ''\"'' }}\n                    {%- else %}\n                        {{- ''\"'' + key + ''\": '' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"[TOOL_CALLS] [\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- '', \"id\": \"'' + tool_call.id + ''\"}'' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- \" \" + message[\"content\"]|trim + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- ''[TOOL_RESULTS] {\"content\": '' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- ''\"call_id\": \"'' + message.tool_call_id + ''\"}[/TOOL_RESULTS]'' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[]', NULL, 'Other', 'approved', 39.3, '270cc25e9c836bad4aef660eeae3e047', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-0.6B', 'huggingface--qwen--qwen3-0.6b', 'Qwen3-0.6B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-0.6B-Base --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offer...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2505.09388","base_model:qwen/qwen3-0.6b-base","base_model:finetune:qwen/qwen3-0.6b-base","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 849, 7589380, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-0.6B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-0.6B-Base\n---\n\n# Qwen3-0.6B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-0.6B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 0.6B\n- Number of Paramaters (Non-Embedding): 0.44B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 16 for Q and 8 for KV\n- Context Length: 32,768 \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n> [!TIP]\n> If you encounter significant endless repetitions, please refer to the [Best Practices](#best-practices) section for optimal sampling parameters, and set the ``presence_penalty`` to 1.5.\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-0.6B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-0.6B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-0.6B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":751632384,"storage_bytes":4522815806,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 79.3, '20650e086cdab00b49800f1a62002ca4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Kwai-Kolors-Kolors', 'huggingface--kwai-kolors--kolors', 'Kolors', 'Kwai-Kolors', '--- license: apache-2.0 language: - zh - en tags: - text-to-image - stable-diffusion - kolors --- <div align="center" style="display: flex; justify-content: center; flex-wrap: wrap;"> <a href="https://github.com/Kwai-Kolors/Kolors"><img src="https://img.shields.io/static/v1?label=Kolors Code&message=Github&color=blue&logo=github-pages"></a> &ensp; <a href="https://kwai-kolors.github.io/"><img src="https://img.shields.io/static/v1?label=Team%20Page&message=Page&color=green"></a> &ensp; <a href...', '["diffusers","safetensors","text-to-image","stable-diffusion","kolors","zh","en","license:apache-2.0","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 848, 494, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Kwai-Kolors/Kolors","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n  - zh\n  - en\ntags:\n  - text-to-image\n  - stable-diffusion\n  - kolors\n---\n# Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis\n<div align="center" style="display: flex; justify-content: center; flex-wrap: wrap;">\n  <a href="https://github.com/Kwai-Kolors/Kolors"><img src="https://img.shields.io/static/v1?label=Kolors Code&message=Github&color=blue&logo=github-pages"></a> &ensp;\n  <a href="https://kwai-kolors.github.io/"><img src="https://img.shields.io/static/v1?label=Team%20Page&message=Page&color=green"></a> &ensp;\n  <a href="https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf"><img src="https://img.shields.io/static/v1?label=Tech Report&message=Arxiv:Kolors&color=red&logo=arxiv"></a> &ensp;\n  <a href="https://kolors.kuaishou.com/"><img src="https://img.shields.io/static/v1?label=Official Website&message=Page&color=green"></a>\n</div>\n<figure>\n  <img src="imgs/head_final3.png">\n</figure>\n<br>\n\n## üìñ Introduction\nKolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and proprietary models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters. Furthermore, Kolors supports both Chinese and English inputs, demonstrating strong performance in understanding and generating Chinese-specific content. For more details, please refer to this <a href="https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf">technical report</a></b>.\n\n\n## üöÄ Quick Start\n### Requirements\n\n* Python 3.8 or later\n* PyTorch 1.13.1 or later\n* Transformers 4.26.1 or later\n* Recommended: CUDA 11.7 or later\n<br>\n\n1. Repository cloning and dependency installation\n\n```bash\napt-get install git-lfs\ngit clone https://github.com/Kwai-Kolors/Kolors\ncd Kolors\nconda create --name kolors python=3.8\nconda activate kolors\npip install -r requirements.txt\npython3 setup.py install\n```\n2. Weights downloadÔºà[link](https://huggingface.co/Kwai-Kolors/Kolors)ÔºâÔºö\n```bash\nhuggingface-cli download --resume-download Kwai-Kolors/Kolors --local-dir weights/Kolors\n```\nor\n```bash\ngit lfs clone https://huggingface.co/Kwai-Kolors/Kolors weights/Kolors\n```\n3. InferenceÔºö\n```bash\npython3 scripts/sample.py "‰∏ÄÂº†Áì¢Ëô´ÁöÑÁÖßÁâáÔºåÂæÆË∑ùÔºåÂèòÁÑ¶ÔºåÈ´òË¥®ÈáèÔºåÁîµÂΩ±ÔºåÊãøÁùÄ‰∏Ä‰∏™ÁâåÂ≠êÔºåÂÜôÁùÄ‚ÄúÂèØÂõæ‚Äù"\n# The image will be saved to "scripts/outputs/sample_test.jpg"\n```\n\n### Using with Diffusers\nPlease refer to https://huggingface.co/Kwai-Kolors/Kolors-diffusers.\n\n## üìú License&Citation\n### License\nKolors are fully open-sourced for academic research. For commercial use, please fill out this [questionnaire](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/ÂèØÂõæKOLORSÊ®°ÂûãÂïÜ‰∏öÊéàÊùÉÁî≥ËØ∑‰π¶.docx) and sent it to kwai-kolors@kuaishou.com for registration.\n\nWe open-source Kolors to promote the development of large text-to-image models in collaboration with the open-source community. The code of this project is open-sourced under the Apache-2.0 license. We sincerely urge all developers and users to strictly adhere to the [open-source license](MODEL_LICENSE), avoiding the use of the open-source model, code, and its derivatives for any purposes that may harm the country and society or for any services not evaluated and registered for safety. Note that despite our best efforts to ensure the compliance, accuracy, and safety of the data during training, due to the diversity and combinability of generated content and the probabilistic randomness affecting the model, we cannot guarantee the accuracy and safety of the output content, and the model is susceptible to misleading. This project does not assume any legal responsibility for any data security issues, public opinion risks, or risks and liabilities arising from the model being misled, abused, misused, or improperly utilized due to the use of the open-source model and code.\n\n\n### Citation\nIf you find our work helpful, please cite it!\n\n```\n@article{kolors,\n  title={Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis},\n  author={Kolors Team},\n  journal={arXiv preprint},\n  year={2024}\n}\n```\n\n### Acknowledgments\n- Thanks to [Diffusers](https://github.com/huggingface/diffusers) for providing the codebase.\n- Thanks to [ChatGLM3](https://github.com/THUDM/ChatGLM3) for providing the powerful Chinese language model.\n<br>\n\n### Contact Us\n\nIf you want to leave a message for our R&D team and product team, feel free to join our [WeChat group](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/wechat.png). You can also contact us via email (kwai-kolors@kuaishou.com).\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":62041008151,"files_count":42,"spaces_count":92,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors\"><img","source_url":"https://github.com/Kwai-Kolors/Kolors\"><img"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"}]', NULL, 'Apache-2.0', 'approved', 64.3, '836138fa4026e179a67c28e04536d690', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-TencentARC-T2I-Adapter', 'huggingface--tencentarc--t2i-adapter', 'T2I-Adapter', 'TencentARC', 'üè∞**Adapter Zoo** **|** üé®**Demos** **|** üü†**GitHub** T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models The GitHub repo: <https://github.com/TencentARC/T2I-Adapter> Please find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md --- license: apache-2.0 ---', '["arxiv:2302.08453","region:us"]', 'other', 842, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/TencentARC/T2I-Adapter","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Models for T2I-Adapter\n\n[![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Adapter/T2I-Adapter) \n\nüè∞[**Adapter Zoo**](https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md)  **|** üé®[**Demos**](https://github.com/TencentARC/T2I-Adapter/blob/main/docs/examples.md) **|** üü†[**GitHub**](https://github.com/TencentARC/T2I-Adapter)\n\n[T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453)\n\nThe GitHub repo: <https://github.com/TencentARC/T2I-Adapter>\n\n## Model Description\n\nPlease find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md\n\n---\nlicense: apache-2.0\n---\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":5804660648,"files_count":88,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter>","source_url":"https://github.com/TencentARC/T2I-Adapter>"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"based_on_paper","target_id":"arxiv:2302.08453","source_url":"https://arxiv.org/abs/2302.08453"}]', NULL, NULL, 'pending', 59.3, 'cd2c215b4baa7fe9a06209be449ec202', NULL, 'https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/assets/canny.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-TencentARC-T2I-Adapter from https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/assets/canny.png
Image converted to WebP: data/images/huggingface-TencentARC-T2I-Adapter.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mosaicml-mpt-7b-storywriter', 'huggingface--mosaicml--mpt-7b-storywriter', 'mpt-7b-storywriter', 'mosaicml', '--- license: apache-2.0 tags: - Composer - MosaicML - llm-foundry datasets: - the_pile_books3 inference: false --- MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths. It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset. At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens. We demonstrate generations as long as 84k token...', '["transformers","pytorch","mpt","text-generation","composer","mosaicml","llm-foundry","custom_code","dataset:the_pile_books3","arxiv:2108.12409","arxiv:2205.14135","arxiv:2302.06675","license:apache-2.0","text-generation-inference","region:us"]', 'text-generation', 841, 1832, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mosaicml/mpt-7b-storywriter","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- Composer\n- MosaicML\n- llm-foundry\ndatasets:\n- the_pile_books3\ninference: false\n---\n\n# MPT-7B-StoryWriter-65k+\n\nMPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.\nIt was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the [books3 dataset](https://huggingface.co/datasets/the_pile_books3).\nAt inference time, thanks to [ALiBi](https://arxiv.org/abs/2108.12409), MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our [blogpost](https://www.mosaicml.com/blog/mpt-7b).\n  * License: Apache 2.0\n\nThis model was trained by [MosaicML](https://www.mosaicml.com) and follows a modified decoder-only transformer architecture.\n\n## Model Date\n\nMay 5, 2023\n\n## Model License\n\nApache 2.0\n\n## Documentation\n\n* [Blog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)\n* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n* Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n## How to Use\n\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method. This is because we use a custom model architecture that is not yet part of the `transformers` package.\n\nIt includes options for many training efficiency features such as [FlashAttention (Dao et al. 2022)](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), QK LayerNorm, and more.\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  ''mosaicml/mpt-7b-storywriter'',\n  trust_remote_code=True\n)\n```\n\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl=''triton''` and with `bfloat16` precision:\n```python\nimport torch\nimport transformers\n\nname = ''mosaicml/mpt-7b-storywriter''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config[''attn_impl''] = ''triton''\nconfig.init_device = ''cuda:0'' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n```\n\nAlthough the model was trained with a sequence length of 2048 and finetuned with a sequence length of 65536,\nALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n```python\nimport transformers\n\nname = ''mosaicml/mpt-7b''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 83968 # (input + output) tokens can now be up to 83968\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n```\n\nThis model was trained with the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")\n```\n\nThe model can then be used, for example, within a text-generation pipeline.  \nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(''text-generation'', model=model, tokenizer=tokenizer, device=''cuda:0'')\n\nwith torch.autocast(''cuda'', dtype=torch.bfloat16):\n    print(\n        pipe(''Here is a recipe for vegan banana bread:\n'',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n```\n\n## Community-Created Resources\n\nThese were not created by MosaicML, but you may find them useful. These links are not an endorsement of the creators or their content.\n\n- [Oobabooga Running MPT-7B-Storywriter](https://youtu.be/QVVb6Md6huA)\n- [NEW MPT-7B-StoryWriter CRUSHES GPT-4!](https://www.youtube.com/watch?v=O9Y_ZdsuKWQ&t=649s) - Has a long section on running locally using Oobabooga\n\n## Example Epilogue\n\nThe full text of the _The Great Gatsby_ (67873 tokens) was fed to the model, followed by the text "EPILOGUE"\n\n**TheRealFitzgerald233**:\n\n> ...\n>\n> So we beat on, boats against the current, borne back ceaselessly into the past\n>\n> EPILOGUE\n\n**MPT-7B-StoryWriter**:\n\n>I have lived so long in the shadow of this dream that I may never see the bright sunlight of reality. I may look up some morning and find that I am six feet under, and that Gatsby, who gave me the dream and everything else, will have vanished behind me.\n>\n>And so it was. I had hardly crossed the threshold of the old world when my eye caught the headlines of a newspaper, and I knew at once that my dream was over. I knew that Gatsby was dead.\n>\n>And I knew that day that I didn''t love her.\n>\n>That night I took the last train for West Egg, and when I reached home I went directly to my attic and brought down all Gatsby''s letters.\n>\n>I have sorted them by date and placed them in the order in which they were written. They are very few in number.\n>\n>They are dated between April of nineteen eighteen and January of nineteen nineteen, and the last one I have here is the one that told me he was dead.\n>\n>There is something in that last letter that frightens me. It is the sense of a finality, of a completeness. I never heard of him after that, and I was glad.\n>\n>But a few months ago I read somewhere that he had been seen in New York. He had become a great man.\n>\n>And I knew that he had not changed at all.\n\n## Model Description\n\nThe architecture is a modification of a standard decoder-only transformer.\n\nThe model has been modified from a standard transformer in the following ways:\n* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n* It does not use biases\n\n\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 6.7B |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 50432 |\n| sequence length | **65536** |\n\n## PreTraining Data\n\nFor more details on the pretraining process, see [MPT-7B](https://huggingface.co/mosaicml/mpt-7b).\n\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n### Training Configuration\n\nThis model was trained on 8 A100-80GBs for about 2 days using the [MosaicML Platform](https://www.mosaicml.com/platform).\nThe model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n\n## Limitations and Biases\n\n_The following language is modified from [EleutherAI''s GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\n\nMPT-7B-StoryWriter can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B-StoryWriter was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n\n## Acknowledgements\n\nThis model was finetuned by Alex Trott and the MosaicML NLP team\n\n## MosaicML Platform\n\nIf you''re interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b).\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\n\n\n## Citation\n\nPlease cite this model using the following format:\n\n```\n@online{MosaicML2023Introducing,\n    author    = {MosaicML NLP Team},\n    title     = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},\n    year      = {2023},\n    url       = {www.mosaicml.com/blog/mpt-7b},\n    note      = {Accessed: 2023-03-28}, % change this date\n    urldate   = {2023-03-28} % change this date\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":26597235302,"files_count":26,"spaces_count":47,"gated":false,"private":false,"config":{"architectures":["MPTForCausalLM"],"auto_map":{"AutoConfig":"configuration_mpt.MPTConfig","AutoModelForCausalLM":"modeling_mpt.MPTForCausalLM"},"model_type":"mpt","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:openai:triton","source_url":"https://github.com/openai/triton"},{"type":"based_on_paper","target_id":"arxiv:2108.12409","source_url":"https://arxiv.org/abs/2108.12409"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2302.06675","source_url":"https://arxiv.org/abs/2302.06675"}]', NULL, 'Apache-2.0', 'approved', 64.3, 'cae3984079994fc809a6dfa4145e96b8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-baichuan-inc-Baichuan-7B', 'huggingface--baichuan-inc--baichuan-7b', 'Baichuan-7B', 'baichuan-inc', '--- language: - zh - en pipeline_tag: text-generation inference: false --- <!-- Provide a quick summary of what the model is/does. --> Baichuan-7BÊòØÁî±ÁôæÂ∑ùÊô∫ËÉΩÂºÄÂèëÁöÑ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÂü∫‰∫éTransformerÁªìÊûÑÔºåÂú®Â§ßÁ∫¶1.2‰∏á‰∫øtokens‰∏äËÆ≠ÁªÉÁöÑ70‰∫øÂèÇÊï∞Ê®°ÂûãÔºåÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠Ôºå‰∏ä‰∏ãÊñáÁ™óÂè£ÈïøÂ∫¶‰∏∫4096„ÄÇÂú®Ê†áÂáÜÁöÑ‰∏≠ÊñáÂíåËã±ÊñáÊùÉÂ®ÅbenchmarkÔºàC-EVAL/MMLUÔºâ‰∏äÂùáÂèñÂæóÂêåÂ∞∫ÂØ∏ÊúÄÂ•ΩÁöÑÊïàÊûú„ÄÇ Â¶ÇÊûúÂ∏åÊúõ‰ΩøÁî®Baichuan-7BÔºàÂ¶ÇËøõË°åÊé®ÁêÜ„ÄÅFinetuneÁ≠âÔºâÔºåÊàë‰ª¨Êé®Ëçê‰ΩøÁî®ÈÖçÂ•ó‰ª£Á†ÅÂ∫ìBaichuan-7B„ÄÇ Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model w...', '["transformers","pytorch","baichuan","text-generation","custom_code","zh","en","arxiv:1910.07467","arxiv:2009.03300","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 841, 65083, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/baichuan-inc/Baichuan-7B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- zh\n- en\npipeline_tag: text-generation\ninference: false\n---\n# Baichuan-7B\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nBaichuan-7BÊòØÁî±ÁôæÂ∑ùÊô∫ËÉΩÂºÄÂèëÁöÑ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÂü∫‰∫éTransformerÁªìÊûÑÔºåÂú®Â§ßÁ∫¶1.2‰∏á‰∫øtokens‰∏äËÆ≠ÁªÉÁöÑ70‰∫øÂèÇÊï∞Ê®°ÂûãÔºåÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠Ôºå‰∏ä‰∏ãÊñáÁ™óÂè£ÈïøÂ∫¶‰∏∫4096„ÄÇÂú®Ê†áÂáÜÁöÑ‰∏≠ÊñáÂíåËã±ÊñáÊùÉÂ®ÅbenchmarkÔºàC-EVAL/MMLUÔºâ‰∏äÂùáÂèñÂæóÂêåÂ∞∫ÂØ∏ÊúÄÂ•ΩÁöÑÊïàÊûú„ÄÇ\n\nÂ¶ÇÊûúÂ∏åÊúõ‰ΩøÁî®Baichuan-7BÔºàÂ¶ÇËøõË°åÊé®ÁêÜ„ÄÅFinetuneÁ≠âÔºâÔºåÊàë‰ª¨Êé®Ëçê‰ΩøÁî®ÈÖçÂ•ó‰ª£Á†ÅÂ∫ì[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)„ÄÇ\n\nBaichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).\n\nIf you wish to use Baichuan-7B (for inference, finetuning, etc.), we recommend using the accompanying code library [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n## Why use Baichuan-7B\n\n- Âú®ÂêåÂ∞∫ÂØ∏Ê®°Âûã‰∏≠Baichuan-7BËææÂà∞‰∫ÜÁõÆÂâçSOTAÁöÑÊ∞¥Âπ≥ÔºåÂèÇËÄÉ‰∏ãÈù¢MMLUÊåáÊ†á\n- Baichuan-7B‰ΩøÁî®Ëá™ÊúâÁöÑ‰∏≠Ëã±ÊñáÂèåËØ≠ËØ≠ÊñôËøõË°åËÆ≠ÁªÉÔºåÂú®‰∏≠Êñá‰∏äËøõË°å‰ºòÂåñÔºåÂú®C-EvalËææÂà∞SOTAÊ∞¥Âπ≥\n- ‰∏çÂêå‰∫éLLaMAÂÆåÂÖ®Á¶ÅÊ≠¢ÂïÜ‰∏ö‰ΩøÁî®ÔºåBaichuan-7B‰ΩøÁî®Êõ¥ÂÆΩÊùæÁöÑÂºÄÊ∫êÂçèËÆÆÔºåÂÖÅËÆ∏Áî®‰∫éÂïÜ‰∏öÁõÆÁöÑ\n\n- Among models of the same size, Baichuan-7B has achieved the current state-of-the-art (SOTA) level, as evidenced by the following MMLU metrics.\n- Baichuan-7B is trained on proprietary bilingual Chinese-English corpora, optimized for Chinese, and achieves SOTA performance on C-Eval.\n- Unlike LLaMA, which completely prohibits commercial use, Baichuan-7B employs a more lenient open-source license, allowing for commercial purposes.\n\n## How to Get Started with the Model\n\nÂ¶Ç‰∏ãÊòØ‰∏Ä‰∏™‰ΩøÁî®Baichuan-7BËøõË°å1-shotÊé®ÁêÜÁöÑ‰ªªÂä°ÔºåÊ†πÊçÆ‰ΩúÂìÅÁªôÂá∫‰ΩúËÄÖÂêçÔºåÊ≠£Á°ÆËæìÂá∫‰∏∫"Â§úÈõ®ÂØÑÂåó->ÊùéÂïÜÈöê"\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(''ÁôªÈπ≥ÈõÄÊ•º->Áéã‰πãÊ∂£\nÂ§úÈõ®ÂØÑÂåó->'', return_tensors=''pt'')\ninputs = inputs.to(''cuda:0'')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n```\n\nThe following is a task of performing 1-shot inference using Baichuan-7B, where the author''s name is given based on the work, with the correct output being "One Hundred Years of Solitude->Gabriel Garcia Marquez"\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(''Hamlet->Shakespeare\nOne Hundred Years of Solitude->'', return_tensors=''pt'')\ninputs = inputs.to(''cuda:0'')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n```\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** ÁôæÂ∑ùÊô∫ËÉΩ(Baichuan Intelligent Technology)\n- **Email**: opensource@baichuan-inc.com\n- **Language(s) (NLP):** Chinese/English\n- **License:** [Baichuan-7B License](https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\nÊï¥‰ΩìÊ®°ÂûãÂü∫‰∫éÊ†áÂáÜÁöÑTransformerÁªìÊûÑÔºåÊàë‰ª¨ÈááÁî®‰∫ÜÂíåLLaMA‰∏ÄÊ†∑ÁöÑÊ®°ÂûãËÆæËÆ°\n- **Position Embedding**ÔºöÈááÁî®rotary-embeddingÔºåÊòØÁé∞Èò∂ÊÆµË¢´Â§ßÂ§öÊï∞Ê®°ÂûãÈááÁî®ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ°àÔºåÂÖ∑ÊúâÂæàÂ•ΩÁöÑÂ§ñÊé®ÊÄß„ÄÇ\n- **Feedforward Layer**ÔºöÈááÁî®SwiGLUÔºåFeedforwardÂèòÂåñ‰∏∫(8/3)ÂÄçÁöÑÈöêÂê´Â±ÇÂ§ßÂ∞èÔºåÂç≥11008„ÄÇ\n- **Layer Normalization**: Âü∫‰∫é[RMSNorm](https://arxiv.org/abs/1910.07467)ÁöÑPre-Normalization„ÄÇ\n\nÂÖ∑‰ΩìÂèÇÊï∞ÂíåËßÅ‰∏ãË°®\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 7000559616 |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 64000 |\n| sequence length | 4096 |\n\nThe overall model is based on the standard Transformer structure, and we have adopted the same model design as LLaMA:\n\n- Position Embedding: We use rotary-embedding, which is the position encoding scheme adopted by most models at this stage, and it has excellent extrapolation capabilities.\n- Feedforward Layer: We use SwiGLU. The feedforward changes to (8/3) times the size of the hidden layer, that is, 11008.\n- Layer Normalization: Pre-Normalization based on [RMSNorm](https://arxiv.org/abs/1910.07467).\n\nThe specific parameters are as follows:\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 7000559616 |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 64000 |\n| sequence length | 4096 |\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Downstream Use \n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\nÊàë‰ª¨ÂêåÊó∂ÂºÄÊ∫êÂá∫‰∫ÜÂíåÊú¨Ê®°ÂûãÈÖçÂ•óÁöÑËÆ≠ÁªÉ‰ª£Á†ÅÔºåÂÖÅËÆ∏ËøõË°åÈ´òÊïàÁöÑFinetuneÁî®‰∫é‰∏ãÊ∏∏‰ªªÂä°ÔºåÂÖ∑‰ΩìÂèÇËßÅ[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)„ÄÇ\n\nWe have also open-sourced the training code that accompanies this model, allowing for efficient finetuning for downstream tasks. For more details, please refer to [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\nÂú®Ê≤°ÊúâÂÖÖÂàÜËØÑ‰º∞È£éÈô©ÂíåÈááÂèñÁºìËß£Êé™ÊñΩÁöÑÊÉÖÂÜµ‰∏ãÊäïÂÖ•Áîü‰∫ß‰ΩøÁî®Ôºõ‰ªª‰ΩïÂèØËÉΩË¢´ËßÜ‰∏∫‰∏çË¥üË¥£‰ªªÊàñÊúâÂÆ≥ÁöÑ‰ΩøÁî®Ê°à‰æã„ÄÇ\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nBaichuan-7BÂèØËÉΩ‰ºö‰∫ßÁîü‰∫ãÂÆû‰∏ä‰∏çÊ≠£Á°ÆÁöÑËæìÂá∫Ôºå‰∏çÂ∫î‰æùËµñÂÆÉ‰∫ßÁîü‰∫ãÂÆû‰∏äÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇBaichuan-7BÊòØÂú®ÂêÑÁßçÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÁöÑ„ÄÇÂ∞ΩÁÆ°Êàë‰ª¨Â∑≤ÁªèÂÅöÂá∫‰∫ÜÂ∑®Â§ßÁöÑÂä™ÂäõÊù•Ê∏ÖÊ¥óÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ΩÜËøô‰∏™Ê®°ÂûãÂèØËÉΩ‰ºöÁîüÊàêÊ∑´ÁßΩ„ÄÅÂÅèËßÅÊàñÂÖ∂‰ªñÂÜíÁäØÊÄßÁöÑËæìÂá∫„ÄÇ\n\nBaichuan-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information. Baichuan-7B was trained on various public datasets. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n## Training Details\n\nËÆ≠ÁªÉÂÖ∑‰ΩìËÆæÁΩÆÂèÇËßÅ[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)„ÄÇ\n\nFor specific training settings, please refer to [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n## Evaluation\n\n### ‰∏≠ÊñáËØÑÊµã\n#### C-Eval\n[CEvalÊï∞ÊçÆÈõÜ](https://cevalbenchmark.com/index.html)ÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑ‰∏≠ÊñáÂü∫Á°ÄÊ®°ÂûãËØÑÊµãÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫Ü52‰∏™Â≠¶ÁßëÂíåÂõõ‰∏™ÈöæÂ∫¶ÁöÑÁ∫ßÂà´„ÄÇÊàë‰ª¨‰ΩøÁî®ËØ•Êï∞ÊçÆÈõÜÁöÑdevÈõÜ‰Ωú‰∏∫few-shotÁöÑÊù•Ê∫êÔºåÂú®testÈõÜ‰∏äËøõË°å‰∫Ü5-shotÊµãËØï„ÄÇ\n\n\n| Model 5-shot                | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4                       | 68.7    | 54.9      | 67.1 | 77.6            | 64.5       | 67.8   |\n| ChatGPT                     | 54.4    | 41.4      | 52.9 | 61.8            | 50.9       | 53.6   |\n| Claude-v1.3                 | 54.2    | 39.0      | 51.9 | 61.7            | 52.1       | 53.7   |\n| Claude-instant-v1.0         | 45.9    | 35.5      | 43.1 | 53.8            | 44.2       | 45.4   |\n| moss-moon-003-base (16B)    | 27.4    | 24.5      | 27.0 | 29.1            | 27.2       | 26.9   |\n| Ziya-LLaMA-13B-pretrain     | 30.2    | 22.7      | 27.7 | 34.4            | 32.0       | 28.9   |\n| LLaMA-7B-hf                 | 27.1    | 25.9      | 27.1 | 26.8            | 27.9       | 26.3   |\n| ChatGLM-6B                  | 34.5    | 23.1      | 30.4 | 39.6            | 37.4       | 34.5   |\n| Falcon-7B                   | 25.8    | 24.3      | 25.8 | 26.0            | 25.8       | 25.6   |\n| Open-LLaMA-v2-pretrain (7B) | 24.0    | 22.5      | 23.1 | 25.3            | 25.2       | 23.2   |\n| TigerBot-7B-base            | 25.7    | 27.0      | 27.3 | 24.7            | 23.4       | 26.1   |\n| Aquila-7B<sup>*</sup>       | 25.5    | 25.2      | 25.6 | 24.6            | 25.2       | 26.6   |\n| BLOOM-7B                    | 22.8    | 20.2      | 21.8 | 23.3            | 23.9       | 23.3   |\n| BLOOMZ-7B                   | 35.7    | 25.8      | 31.3 | 43.5            | 36.6       | 35.6   |\n| **Baichuan-7B**             | 42.8    | 31.5      | 38.2 | 52.0            | 46.2       | 39.3   |\n\n\n#### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) ÊòØ‰∏Ä‰∏™‰ª•‰∏≠ÂõΩÈ´òËÄÉÈ¢ò‰Ωú‰∏∫ËØÑÊµãÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÉΩÂäõÁöÑÊï∞ÊçÆÈõÜÔºåÁî®‰ª•ËØÑ‰º∞Ê®°ÂûãÁöÑËØ≠Ë®ÄËÉΩÂäõÂíåÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇ\nÊàë‰ª¨Âè™‰øùÁïô‰∫ÜÂÖ∂‰∏≠ÁöÑÂçïÈ°πÈÄâÊã©È¢òÔºåÂπ∂ÂØπÊâÄÊúâÊ®°ÂûãËøõË°åÁªü‰∏Ä5-shotÊµãËØï„ÄÇ\n\n‰ª•‰∏ãÊòØÊµãËØïÁöÑÁªìÊûú„ÄÇ\n\n| Model           | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain  | 21.41           |\n| Ziya-LLaMA-13B-pretrain | 23.17           |\n| Falcon-7B               | 23.98           |\n| TigerBot-7B-base        | 25.94           |\n| LLaMA-7B                | 27.81           |\n| ChatGLM-6B              | 21.41           |\n| BLOOM-7B                | 26.96           |\n| BLOOMZ-7B               | 28.72           |\n| Aquila-7B<sup>*</sup>               | 24.39           |\n| **Baichuan-7B**        | **36.24**           |\n\n\n#### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) Êó®Âú®ËØÑ‰º∞Ê®°ÂûãÁöÑËÆ§Áü•ÂíåËß£ÂÜ≥ÈóÆÈ¢òÁõ∏ÂÖ≥ÁöÑ‰ªªÂä°‰∏≠ÁöÑ‰∏ÄËà¨ËÉΩÂäõ„ÄÇ\nÊàë‰ª¨Âè™‰øùÁïô‰∫ÜÂÖ∂‰∏≠ÁöÑÂõõÈÄâ‰∏ÄÂçïÈ°πÈÄâÊã©È¢òÔºåÈöèÊú∫ÂàíÂàÜÂêéÂØπÊâÄÊúâÊ®°ÂûãËøõË°å‰∫ÜÁªü‰∏Ä5-shotÊµãËØï„ÄÇ\n\n| Model           | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain  | 23.49           |\n| Ziya-LLaMA-13B-pretrain | 27.64           |\n| Falcon-7B               | 27.18           |\n| TigerBot-7B-base        | 25.19           |\n| LLaMA-7B                | 28.17           |\n| ChatGLM-6B              | 23.49           |\n| BLOOM-7B                | 26.55           |\n| BLOOMZ-7B               | 30.27           |\n| Aquila-7B<sup>*</sup>               | 25.58           |\n| **Baichuan-7B**        | **34.44**           |\n\n<sup>*</sup>ÂÖ∂‰∏≠AquilaÊ®°ÂûãÊù•Ê∫ê‰∫é[Êô∫Ê∫êÂÆòÊñπÁΩëÁ´ô](https://model.baai.ac.cn/model-detail/100098)Ôºå‰ªÖÂÅöÂèÇËÄÉ\n\n### English Leaderboard\nIn addition to Chinese, we also tested the model''s performance in English. \n\n#### MMLU\n\n[MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset that includes 57 multiple-choice tasks, covering elementary mathematics, American history, computer science, law, etc. The difficulty ranges from high school level to expert level, making it a mainstream LLM evaluation dataset.\n\nWe adopted the [open-source]((https://github.com/hendrycks/test)) evaluation scheme, and the final 5-shot results are as follows:\n\n| Model                                  | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n| LLaMA-7B<sup>2</sup>                   |       34.0 |      38.3       | 30.5 | 38.1  |  35.1   |\n| Falcon-7B<sup>1</sup>                  |          - |        -        |  -   |   -   |  35.0   |\n| mpt-7B<sup>1</sup>                     |          - |        -        |  -   |   -   |  35.6   |\n| ChatGLM-6B<sup>0</sup>                 |       35.4 |      41.0       | 31.3 | 40.5  |  36.9   |\n| BLOOM 7B<sup>0</sup>                  |       25.0 |      24.4       | 26.5 | 26.4  |  25.5   |\n| BLOOMZ 7B<sup>0</sup>                 |       31.3 |      42.1       | 34.4 | 39.0  |  36.1   |\n| moss-moon-003-base (16B)<sup>0</sup>   |       24.2 |      22.8       | 22.4 | 24.4  |  23.6   |\n| moss-moon-003-sft (16B)<sup>0</sup>    |       30.5 |      33.8       | 29.3 | 34.4  |  31.9   |\n| **Baichuan-7B<sup>0</sup>**            |       38.4 |      48.9       | 35.6 | 48.1  |  42.3   |\n\nThe superscript in the Model column indicates the source of the results.\n```\n0:reimplemented\n1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n```\n\n## Our Group\n![WeChat](https://github.com/baichuan-inc/Baichuan-13B/blob/main/media/wechat.jpeg?raw=true)\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":28005568432,"files_count":14,"spaces_count":59,"gated":false,"private":false,"config":{"architectures":["BaiChuanForCausalLM"],"auto_map":{"AutoConfig":"configuration_baichuan.BaiChuanConfig","AutoModelForCausalLM":"modeling_baichuan.BaiChuanForCausalLM"},"model_type":"baichuan","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:ExpressAI:AI-Gaokao","source_url":"https://github.com/ExpressAI/AI-Gaokao"},{"type":"has_code","target_id":"github:microsoft:AGIEval","source_url":"https://github.com/microsoft/AGIEval"},{"type":"has_code","target_id":"github:hendrycks:test","source_url":"https://github.com/hendrycks/test"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-13B","source_url":"https://github.com/baichuan-inc/Baichuan-13B"},{"type":"based_on_paper","target_id":"arxiv:1910.07467","source_url":"https://arxiv.org/abs/1910.07467"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"}]', NULL, NULL, 'pending', 69.3, '3a120e58cad49e35720e6c5237b3f2ce', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-InstantX-InstantID', 'huggingface--instantx--instantid', 'InstantID', 'InstantX', '--- license: apache-2.0 language: - en library_name: diffusers pipeline_tag: text-to-image --- <div align="center"> **Project Page** **|** **Paper** **|** **Code** **|** ü§ó **Gradio demo** </div> InstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks. <div align="center"> <img src=''examples/applications.png''> </div> You can directly download the model in this repository. You also can download the m...', '["diffusers","safetensors","text-to-image","en","arxiv:2401.07519","license:apache-2.0","region:us"]', 'text-to-image', 837, 31715, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/InstantX/InstantID","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: diffusers\npipeline_tag: text-to-image\n---\n\n# InstantID Model Card\n\n<div align="center">\n\n[**Project Page**](https://instantid.github.io/) **|** [**Paper**](https://arxiv.org/abs/2401.07519) **|** [**Code**](https://github.com/InstantID/InstantID) **|** [ü§ó **Gradio demo**](https://huggingface.co/spaces/InstantX/InstantID)\n\n\n</div>\n\n## Introduction\n\nInstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.\n\n<div  align="center">\n<img src=''examples/applications.png''>\n</div>\n\n\n## Usage\n\nYou can directly download the model in this repository.\nYou also can download the model in python script:\n\n```python\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id="InstantX/InstantID", filename="ControlNetModel/config.json", local_dir="./checkpoints")\nhf_hub_download(repo_id="InstantX/InstantID", filename="ControlNetModel/diffusion_pytorch_model.safetensors", local_dir="./checkpoints")\nhf_hub_download(repo_id="InstantX/InstantID", filename="ip-adapter.bin", local_dir="./checkpoints")\n```\n\nFor face encoder, you need to manutally download via this [URL](https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304) to `models/antelopev2`.\n\n```python\n# !pip install opencv-python transformers accelerate insightface\nimport diffusers\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\n\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n# prepare ''antelopev2'' under ./models\napp = FaceAnalysis(name=''antelopev2'', root=''./'', providers=[''CUDAExecutionProvider'', ''CPUExecutionProvider''])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n# prepare models under ./checkpoints\nface_adapter = f''./checkpoints/ip-adapter.bin''\ncontrolnet_path = f''./checkpoints/ControlNetModel''\n\n# load IdentityNet\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\npipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n...     "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, torch_dtype=torch.float16\n... )\npipe.cuda()\n\n# load adapter\npipe.load_ip_adapter_instantid(face_adapter)\n```\n\nThen, you can customized your own face images\n\n```python\n# load an image\nimage = load_image("your-example.jpg")\n\n# prepare face emb\nface_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\nface_info = sorted(face_info, key=lambda x:(x[''bbox''][2]-x[''bbox''][0])*x[''bbox''][3]-x[''bbox''][1])[-1] # only use the maximum face\nface_emb = face_info[''embedding'']\nface_kps = draw_kps(face_image, face_info[''kps''])\n\npipe.set_ip_adapter_scale(0.8)\n\nprompt = "analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality"\nnegative_prompt = "(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured"\n\n# generate image\nimage = pipe(\n...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n... ).images[0]\n```\n\nFor more details, please follow the instructions in our [GitHub repository](https://github.com/InstantID/InstantID). \n\n## Usage Tips\n1. If you''re not satisfied with the similarity, try to increase the weight of "IdentityNet Strength" and "Adapter Strength".\n2. If you feel that the saturation is too high, first decrease the Adapter strength. If it is still too high, then decrease the IdentityNet strength.\n3. If you find that text control is not as expected, decrease Adapter strength.\n4. If you find that realistic style is not good enough, go for our Github repo and use a more realistic base model.\n\n## Demos\n\n<div  align="center">\n<img src=''examples/0.png''>\n</div>\n\n<div  align="center">\n<img src=''examples/1.png''>\n</div>\n\n## Disclaimer\n\nThis project is released under Apache License and aims to positively impact the field of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.\n\n## Citation\n```bibtex\n@article{wang2024instantid,\n  title={InstantID: Zero-shot Identity-Preserving Generation in Seconds},\n  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},\n  journal={arXiv preprint arXiv:2401.07519},\n  year={2024}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":4268559963,"files_count":8,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:InstantID:InstantID","source_url":"https://github.com/InstantID/InstantID"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface"},{"type":"has_code","target_id":"github:InstantID:InstantID","source_url":"https://github.com/InstantID/InstantID"},{"type":"based_on_paper","target_id":"arxiv:2401.07519","source_url":"https://arxiv.org/abs/2401.07519"}]', NULL, 'Apache-2.0', 'approved', 64.2, '3dd66549b4eb0b6561cdb9577aa19327', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-rain1011-pyramid-flow-sd3', 'huggingface--rain1011--pyramid-flow-sd3', 'pyramid-flow-sd3', 'rain1011', '--- license: other license_name: stabilityai-ai-community license_link: LICENSE.md base_model: - stabilityai/stable-diffusion-3-medium pipeline_tag: text-to-video tags: - image-to-video - sd3 --- [[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page ‚ú®]](https://pyramid-flow.github.io) [[Code üöÄ]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model ‚ö°Ô∏è]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [demo ü§ó] This is the model repository for Pyramid Flow, a training-efficient...', '["diffusers","safetensors","image-to-video","sd3","text-to-video","arxiv:2410.05954","base_model:stabilityai/stable-diffusion-3-medium","license:other","region:us"]', 'text-to-video', 835, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/rain1011/pyramid-flow-sd3","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: stabilityai-ai-community\nlicense_link: LICENSE.md\nbase_model:\n- stabilityai/stable-diffusion-3-medium\npipeline_tag: text-to-video\ntags:\n- image-to-video\n- sd3\n\n---\n\n# ‚ö°Ô∏èPyramid Flow SD3‚ö°Ô∏è\n\n[[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page ‚ú®]](https://pyramid-flow.github.io) [[Code üöÄ]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model ‚ö°Ô∏è]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [[demo ü§ó](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow)]\n\nThis is the model repository for Pyramid Flow, a training-efficient **Autoregressive Video Generation** method based on **Flow Matching**. By training only on open-source datasets, it generates high-quality 10-second videos at 768p resolution and 24 FPS, and naturally supports image-to-video generation.\n\n<table class="center" border="0" style="width: 100%; text-align: left;">\n<tr>\n  <th>10s, 768p, 24fps</th>\n  <th>5s, 768p, 24fps</th>\n  <th>Image-to-video</th>\n</tr>\n<tr>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v_10s/fireworks.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/trailer.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/i2v/sunday.mp4" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## News\n\n* `2024.10.29` ‚ö°Ô∏è‚ö°Ô∏è‚ö°Ô∏è We release [training code](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training) and [new model checkpoints](https://huggingface.co/rain1011/pyramid-flow-miniflux) with FLUX structure trained from scratch.\n\n  > We have switched the model structure from SD3 to a mini FLUX to fix human structure issues, please try our 1024p image checkpoint and 384p video checkpoint. We will release 768p video checkpoint in a few days.\n\n* `2024.10.11`  ü§óü§óü§ó [Hugging Face demo](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow) is available. Thanks [@multimodalart](https://huggingface.co/multimodalart) for the commit! \n\n* `2024.10.10`  üöÄüöÄüöÄ We release the [technical report](https://arxiv.org/abs/2410.05954), [project page](https://pyramid-flow.github.io) and [model checkpoint](https://huggingface.co/rain1011/pyramid-flow-sd3) of Pyramid Flow.\n\n## Installation\n\nWe recommend setting up the environment with conda. The codebase currently uses Python 3.8.10 and PyTorch 2.1.2, and we are actively working to support a wider range of versions.\n\n```bash\ngit clone https://github.com/jy0205/Pyramid-Flow\ncd Pyramid-Flow\n\n# create env using conda\nconda create -n pyramid python==3.8.10\nconda activate pyramid\npip install -r requirements.txt\n```\n\nThen, download the model from [Huggingface](https://huggingface.co/rain1011) (there are two variants: [miniFLUX](https://huggingface.co/rain1011/pyramid-flow-miniflux) or [SD3](https://huggingface.co/rain1011/pyramid-flow-sd3)). The miniFLUX models support 1024p image and 384p video generation, and the SD3-based models support 768p and 384p video generation. The 384p checkpoint generates 5-second video at 24FPS, while the 768p checkpoint generates up to 10-second video at 24FPS.\n\n```python\nfrom huggingface_hub import snapshot_download\n\nmodel_path = ''PATH''   # The local directory to save downloaded checkpoint\nsnapshot_download("rain1011/pyramid-flow-sd3", local_dir=model_path, local_dir_use_symlinks=False, repo_type=''model'')\n```\n\n## Usage\n\nFor inference, we provide Gradio demo, single-GPU, multi-GPU, and Apple Silicon inference code, as well as VRAM-efficient features such as CPU offloading. Please check our [code repository](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference) for usage.\n\nBelow is a simplified two-step usage procedure. First, load the downloaded model:\n\n```python\nimport torch\nfrom PIL import Image\nfrom pyramid_dit import PyramidDiTForVideoGeneration\nfrom diffusers.utils import load_image, export_to_video\n\ntorch.cuda.set_device(0)\nmodel_dtype, torch_dtype = ''bf16'', torch.bfloat16   # Use bf16 (not support fp16 yet)\n\nmodel = PyramidDiTForVideoGeneration(\n    ''PATH'',                                         # The downloaded checkpoint dir\n    model_dtype,\n    model_variant=''diffusion_transformer_768p'',     # ''diffusion_transformer_384p''\n)\n\nmodel.vae.enable_tiling()\n# model.vae.to("cuda")\n# model.dit.to("cuda")\n# model.text_encoder.to("cuda")\n\n# if you''re not using sequential offloading bellow uncomment the lines above ^\nmodel.enable_sequential_cpu_offload()\n```\n\nThen, you can try text-to-video generation on your own prompts:\n\n```python\nprompt = "A movie trailer featuring the adventures of the 30 year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate(\n        prompt=prompt,\n        num_inference_steps=[20, 20, 20],\n        video_num_inference_steps=[10, 10, 10],\n        height=768,     \n        width=1280,\n        temp=16,                    # temp=16: 5s, temp=31: 10s\n        guidance_scale=9.0,         # The guidance for the first frame, set it to 7 for 384p variant\n        video_guidance_scale=5.0,   # The guidance for the other video latent\n        output_type="pil",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, "./text_to_video_sample.mp4", fps=24)\n```\n\nAs an autoregressive model, our model also supports (text conditioned) image-to-video generation:\n\n```python\nimage = Image.open(''assets/the_great_wall.jpg'').convert("RGB").resize((1280, 768))\nprompt = "FPV flying over the Great Wall"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate_i2v(\n        prompt=prompt,\n        input_image=image,\n        num_inference_steps=[10, 10, 10],\n        temp=16,\n        video_guidance_scale=4.0,\n        output_type="pil",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, "./image_to_video_sample.mp4", fps=24)\n```\n\n## Usage tips\n\n* The `guidance_scale` parameter controls the visual quality. We suggest using a guidance within [7, 9] for the 768p checkpoint during text-to-video generation, and 7 for the 384p checkpoint.\n* The `video_guidance_scale` parameter controls the motion. A larger value increases the dynamic degree and mitigates the autoregressive generation degradation, while a smaller value stabilizes the video.\n* For 10-second video generation, we recommend using a guidance scale of 7 and a video guidance scale of 5.\n\n## Gallery\n\nThe following video examples are generated at 5s, 768p, 24fps. For more results, please visit our [project page](https://pyramid-flow.github.io).\n\n<table class="center" border="0" style="width: 100%; text-align: left;">\n<tr>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/tokyo.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/eiffel.mp4" autoplay muted loop playsinline></video></td>\n</tr>\n<tr>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/waves.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/rail.mp4" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe are grateful for the following awesome projects when implementing Pyramid Flow:\n\n* [SD3 Medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) and [Flux 1.0](https://huggingface.co/black-forest-labs/FLUX.1-dev): State-of-the-art image generation models based on flow matching.\n* [Diffusion Forcing](https://boyuan.space/diffusion-forcing) and [GameNGen](https://gamengen.github.io): Next-token prediction meets full-sequence diffusion.\n* [WebVid-10M](https://github.com/m-bain/webvid), [OpenVid-1M](https://github.com/NJU-PCALab/OpenVid-1M) and [Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan): Large-scale datasets for text-to-video generation.\n* [CogVideoX](https://github.com/THUDM/CogVideo): An open-source text-to-video generation model that shares many training details.\n* [Video-LLaMA2](https://github.com/DAMO-NLP-SG/VideoLLaMA2): An open-source video LLM for our video recaptioning.\n\n## Citation\n\nConsider giving this repository a star and cite Pyramid Flow in your publications if it helps your research.\n\n```\n@article{jin2024pyramidal,\n  title={Pyramidal Flow Matching for Efficient Video Generative Modeling},\n  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},\n  jounal={arXiv preprint arXiv:2410.05954},\n  year={2024}\n}\n```', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":52046558470,"files_count":29,"spaces_count":37,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow"},{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training"},{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow"},{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference"},{"type":"has_code","target_id":"github:m-bain:webvid","source_url":"https://github.com/m-bain/webvid"},{"type":"has_code","target_id":"github:NJU-PCALab:OpenVid-1M","source_url":"https://github.com/NJU-PCALab/OpenVid-1M"},{"type":"has_code","target_id":"github:PKU-YuanGroup:Open-Sora-Plan","source_url":"https://github.com/PKU-YuanGroup/Open-Sora-Plan"},{"type":"has_code","target_id":"github:THUDM:CogVideo","source_url":"https://github.com/THUDM/CogVideo"},{"type":"has_code","target_id":"github:DAMO-NLP-SG:VideoLLaMA2","source_url":"https://github.com/DAMO-NLP-SG/VideoLLaMA2"},{"type":"based_on_paper","target_id":"arxiv:2410.05954","source_url":"https://arxiv.org/abs/2410.05954"}]', NULL, 'Other', 'approved', 64.2, '307f417095f9d00a4d05e4e0079a3017', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Phind-Phind-CodeLlama-34B-v2', 'huggingface--phind--phind-codellama-34b-v2', 'Phind-CodeLlama-34B-v2', 'Phind', '--- license: llama2 model-index: - name: Phind-CodeLlama-34B-v1 results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 73.8% verified: false tags: - code llama --- We''ve fine-tuned Phind-CodeLlama-34B-v1 on an additional 1.5B tokens high-quality programming-related data, achieving **73.8% pass@1** on HumanEval. It''s the current state-of-the-art amongst open-source models. Furthermore, this model is **instruction-tuned...', '["transformers","pytorch","llama","text-generation","code llama","license:llama2","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 833, 2534, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Phind/Phind-CodeLlama-34B-v2","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama2\nmodel-index:\n- name: Phind-CodeLlama-34B-v1\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 73.8%\n      verified: false\ntags:\n- code llama\n---\n\n# **Phind-CodeLlama-34B-v2**\nWe''ve fine-tuned Phind-CodeLlama-34B-v1 on an additional 1.5B tokens high-quality programming-related data, achieving **73.8% pass@1** on HumanEval. It''s the current state-of-the-art amongst open-source models.\n\nFurthermore, this model is **instruction-tuned** on the Alpaca/Vicuna format to be steerable and easy-to-use.\n\nMore details can be found on our [blog post](https://www.phind.com/blog/code-llama-beats-gpt4).\n\n## Model Details\nThis model is fine-tuned from Phind-CodeLlama-34B-v1 and achieves **73.8% pass@1** on HumanEval.\n\nPhind-CodeLlama-34B-v2 is **multi-lingual** and is proficient in Python, C/C++, TypeScript, Java, and more.\n\n## Dataset Details\nWe fined-tuned on a proprietary dataset of 1.5B tokens of high quality programming problems and solutions. This dataset consists of instruction-answer pairs instead of code completion examples, making it structurally different from HumanEval. LoRA was not used -- both models are a native finetune. We used DeepSpeed ZeRO 3 and Flash Attention 2 to train these models in 15 hours on 32 A100-80GB GPUs. We used a sequence length of 4096 tokens.\n\n## How to Get Started with the Model\n\nMake sure to install Transformers from the main git branch:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n## How to Prompt the Model\nThis model accepts the Alpaca/Vicuna instruction format.\n\nFor example: \n\n```\n### System Prompt\nYou are an intelligent programming assistant.\n\n### User Message\nImplement a linked list in C++\n\n### Assistant\n...\n```\n\n## How to reproduce HumanEval Results\n\nTo reproduce our results:\n\n```python\n\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom human_eval.data import write_jsonl, read_problems\nfrom tqdm import tqdm\n\n# initialize the model\n\nmodel_path = "Phind/Phind-CodeLlama-34B-v2"\nmodel = LlamaForCausalLM.from_pretrained(model_path, device_map="auto")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# HumanEval helper\n\ndef generate_one_completion(prompt: str):\n    tokenizer.pad_token = tokenizer.eos_token\n    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)\n\n    # Generate\n    generate_ids = model.generate(inputs.input_ids.to("cuda"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    completion = completion.replace(prompt, "").split("\n\n\n")[0]\n\n    return completion\n\n# perform HumanEval\nproblems = read_problems()\n\nnum_samples_per_task = 1\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id]["prompt"]))\n    for task_id in tqdm(problems)\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl("samples.jsonl", samples)\n\n# run `evaluate_functional_correctness samples.jsonl` in your HumanEval code sandbox\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\nThis model has undergone very limited testing. Additional safety testing should be performed before any real-world deployments.\n\n\n## Training details\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 32x A100-80GB\n- **Hours used:** 480 GPU-hours\n- **Cloud Provider:** AWS\n- **Compute Region:** us-east-1', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":134976616088,"files_count":15,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"}]', NULL, 'LLaMA-2', 'approved', 64.2, '841746692974d085f159b7df51dc0d80', NULL, NULL, CURRENT_TIMESTAMP);
