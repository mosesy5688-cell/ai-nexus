/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HuggingFaceTB-SmolLM3-3B', 'huggingface--huggingfacetb--smollm3-3b', 'SmolLM3-3B', 'HuggingFaceTB', '--- library_name: transformers license: apache-2.0 language: - en - fr - es - it - pt - zh - ar - ru base_model: - HuggingFaceTB/SmolLM3-3B-Base --- !image/png 1. Model Summary 2. How to use 3. Evaluation 4. Training 5. Limitations 6. License SmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3B‚Äì4B scale. !image/png The model...', '["transformers","safetensors","smollm3","text-generation","conversational","en","fr","es","it","pt","zh","ar","ru","base_model:huggingfacetb/smollm3-3b-base","base_model:finetune:huggingfacetb/smollm3-3b-base","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 833, 91555, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HuggingFaceTB/SmolLM3-3B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\n- fr\n- es\n- it\n- pt\n- zh\n- ar\n- ru\nbase_model:\n  - HuggingFaceTB/SmolLM3-3B-Base\n---\n\n\n# SmolLM3\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/zy0dqTCCt5IHmuzwoqtJ9.png)\n\n\n##  Table of Contents\n\n1. [Model Summary](#model-summary)\n2. [How to use](#how-to-use)\n3. [Evaluation](#evaluation)\n4. [Training](#training)\n5. [Limitations](#limitations)\n6. [License](#license)\n\n## Model Summary\n\nSmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3B‚Äì4B scale.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/db3az7eGzs-Sb-8yUj-ff.png)\n\nThe model is a decoder-only transformer using GQA and NoPE (with 3:1 ratio), it was pretrained on 11.2T tokens with a staged curriculum of web, code, math and reasoning data. Post-training included midtraining on 140B reasoning tokens followed by supervised fine-tuning and alignment via Anchored Preference Optimization (APO).\n\n### Key features\n- Instruct model optimized for **hybrid reasoning**\n- **Fully open model**: open weights + full training details including public data mixture and training configs\n- **Long context:** Trained on 64k context and supports up to **128k tokens** using YARN extrapolation\n- **Multilingual**: 6 natively supported (English, French, Spanish, German, Italian, and Portuguese)\n\nFor more details refer to our blog post: https://hf.co/blog/smollm3\n\n## How to use\n\nThe modeling code for SmolLM3 is available in transformers `v4.53.0`, so make sure to upgrade your transformers version. You can also load the model with the latest `vllm` which uses transformers as a backend.\n```bash\npip install -U transformers\n```\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "HuggingFaceTB/SmolLM3-3B"\ndevice = "cuda"  # for GPU usage or "cpu" for CPU usage\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n).to(device)\n\n# prepare the model input\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages_think = [\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages_think,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n>[!TIP]\n> We recommend setting `temperature=0.6` and `top_p=0.95` in the sampling parameters.\n\n### Long context processing\n\nThe current `config.json` is set for context length up to 65,536 tokens. To handle longer inputs (128k or 256k), we utilize YaRN you can change the `max_position_embeddings` and rope_scaling` to:\n```\n{\n  ...,\n  "rope_scaling": {\n    "factor": 2.0, #2x65536=131‚ÄØ072 \n    "original_max_position_embeddings": 65536,\n    "type": "yarn"\n  }\n}\n```\n\n\n### Enabling and Disabling Extended Thinking Mode\n\nWe enable extended thinking by default, so the example above generates the output with a reasoning trace. For choosing between enabling, you can provide the `/think` and `/no_think` flags through the system prompt as shown in the snippet below for extended thinking disabled. The code for generating the response with extended thinking would be the same except that the system prompt should have `/think` instead of `/no_think`.\n\n```python\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages = [\n    {"role": "system", "content": "/no_think"},\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n```\n\nWe also provide the option of specifying the whether to use extended thinking through the `enable_thinking` kwarg as in the example below. You do not need to set the `/no_think` or `/think` flags through the system prompt if using the kwarg, but keep in mind that the flag in the system prompt overwrites the setting in the kwarg.\n\n```python\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages = [\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False\n)\n```\n\n### Agentic Usage\n\nSmolLM3 supports tool calling!\nJust pass your list of tools:\n- Under the argument `xml_tools` for standard tool-calling: these tools will be called as JSON blobs within XML tags, like `<tool_call>{"name": "get_weather", "arguments": {"city": "Copenhagen"}}</tool_call>`\n- Or under `python_tools`: then the model will call tools like python functions in a `<code>` snippet, like `<code>get_weather(city="Copenhagen")</code>`\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = "HuggingFaceTB/SmolLM3-3B"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ntools = [\n    {\n        "name": "get_weather",\n        "description": "Get the weather in a city",\n        "parameters": {"type": "object", "properties": {"city": {"type": "string", "description": "The city to get the weather for"}}}}\n]\n\nmessages = [\n    {\n        "role": "user",\n        "content": "Hello! How is the weather today in Copenhagen?"\n    }\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    enable_thinking=False, # True works as well, your choice!\n    xml_tools=tools,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_tensors="pt"\n)\n\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n### Using Custom System Instructions. \n\nYou can specify custom instruction through the system prompt while controlling whether to use extended thinking. For example, the snippet below shows how to make the model speak like a pirate while enabling extended thinking.\n\n```python\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages = [\n    {"role": "system", "content": "Speak like a pirate./think"},\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n```\n\nFor local inference, you can use `llama.cpp`, `ONNX`, `MLX`, `MLC` and `ExecuTorch`. You can find quantized checkpoints in this collection (https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23)\n\n### vLLM and SGLang\n\nYou can use vLLM and SGLang to deploy the model in an API compatible with OpenAI format.\n\n#### SGLang\n\n```bash\npython -m sglang.launch_server --model-path HuggingFaceTB/SmolLM3-3B\n```\n\n#### vLLM\n\n```bash\nvllm serve HuggingFaceTB/SmolLM3-3B --enable-auto-tool-choice --tool-call-parser=hermes\n```\n\n#### Setting `chat_template_kwargs`\n\nYou can specify `chat_template_kwargs` such as `enable_thinking` to a deployed model by passing the `chat_template_kwargs` parameter in the API request.\n\n```bash\ncurl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d ''{\n  "model": "HuggingFaceTB/SmolLM3-3B",\n  "messages": [\n    {"role": "user", "content": "Give me a brief explanation of gravity in simple terms."}\n  ],\n  "temperature": 0.6,\n  "top_p": 0.95,\n  "max_tokens": 16384,\n  "chat_template_kwargs": {"enable_thinking": false}\n}''\n```\n\n## Evaluation\n\nIn this section, we report the evaluation results of SmolLM3 model. All evaluations are zero-shot unless stated otherwise, and we use [lighteval](https://github.com/huggingface/lighteval) to run them. \n\nWe highlight the best score in bold and underline the second-best score.\n\n### Instruction Model\n\n#### No Extended Thinking\nEvaluation results of non reasoning models and reasoning models in no thinking mode. We highlight the best and second-best scores in bold.\n| Category | Metric | SmoLLM3-3B | Qwen2.5-3B | Llama3.1-3B | Qwen3-1.7B | Qwen3-4B |\n|---------|--------|------------|------------|-------------|------------|----------|\n| High school math competition | AIME 2025 | <u>9.3</u> | 2.9 | 0.3 | 8.0 | **17.1** |\n| Math problem-solving | GSM-Plus | 72.8 | <u>74.1</u> | 59.2 | 68.3 | **82.1** |\n| Competitive programming | LiveCodeBench v4 | <u>15.2</u> | 10.5 | 3.4 | 15.0 | **24.9** |\n| Graduate-level reasoning | GPQA Diamond | <u>35.7</u> | 32.2 | 29.4 | 31.8 | **44.4** |\n| Instruction following | IFEval | **76.7** | 65.6 | 71.6 | <u>74.0</u> | 68.9 |\n| Alignment | MixEval Hard | 26.9 | <u>27.6</u> | 24.9 | 24.3 | **31.6** |\n| Tool Calling | BFCL| <u>92.3</u> | - | <u>92.3</u> * | 89.5  | **95.0** |\n| Multilingual Q&A | Global MMLU | <u>53.5</u> | 50.54 | 46.8 | 49.5 | **65.1** |\n\n(*): this is a tool calling finetune\n\n#### Extended Thinking\nEvaluation results in reasoning mode for SmolLM3 and Qwen3 models: \n| Category | Metric | SmoLLM3-3B | Qwen3-1.7B | Qwen3-4B |\n|---------|--------|------------|------------|----------|\n| High school math competition | AIME 2025 | <u>36.7</u> | 30.7 | **58.8** |\n| Math problem-solving | GSM-Plus | <u>83.4</u> | 79.4 | **88.2** |\n| Competitive programming | LiveCodeBench v4 | 30.0 | <u>34.4</u> | **52.9** |\n| Graduate-level reasoning | GPQA Diamond | <u>41.7</u> | 39.9 | **55.3** |\n| Instruction following | IFEval | 71.2 | <u>74.2</u> | **85.4** |\n| Alignment | MixEval Hard | 30.8 | <u>33.9</u> | **38.0** |\n| Tool Calling | BFCL | <u>88.8</u> | <u>88.8</u> | **95.5** |\n| Multilingual Q&A | Global MMLU | <u>64.1</u> | 62.3 | **73.3** |\n\n\n### Base Pre-Trained Model\n\n#### English benchmarks\nNote: All evaluations are zero-shot unless stated otherwise. For Ruler 64k evaluation, we apply YaRN to the Qwen models with 32k context to extrapolate the context length.\n\n| Category | Metric | SmolLM3-3B | Qwen2.5-3B | Llama3-3.2B | Qwen3-1.7B-Base | Qwen3-4B-Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Reasoning & Commonsense| HellaSwag | **76.15** | 74.19 |<u>75.52</u> | 60.52 | 74.37 |\n| | ARC-CF (Average) | **65.61** | 59.81 | 58.58 | 55.88 | <u>62.11</u> |\n| | Winogrande | 58.88 | **61.41** | 58.72 | 57.06 | <u>59.59</u> |\n| | CommonsenseQA | <u>55.28</u> | 49.14 | **60.60** | 48.98 | 52.99 |\n| Knowledge & Understanding | MMLU-CF (Average) | <u>44.13</u> | 42.93 | 41.32 | 39.11 | **47.65** | \n| | MMLU Pro CF | <u>19.61</u> | 16.66 | 16.42 | 18.04 | **24.92** |\n| | MMLU Pro MCF | <u>32.70</u> | 31.32 | 25.07 | 30.39 | **41.07** |\n| | PIQA | **78.89** | 78.35 | <u>78.51</u> | 75.35 | 77.58 |\n| | OpenBookQA | 40.60 | 40.20 | <u>42.00</u> | 36.40 | **42.40** |\n| | BoolQ | **78.99** | 73.61 | <u>75.33</u> | 74.46 | 74.28 | \n| **Math & Code** |  |  |  |  |  |  | \n| Coding & math | HumanEval+ | 30.48 | 34.14| 25.00 | <u>43.29</u>| **54.87** |\n| | MBPP+ | 52.91 | 52.11 | 38.88| <u>59.25</u> | **63.75** | \n| | MATH (4-shot) | <u>46.10</u> | 40.10 | 7.44 | 41.64 | **51.20** |\n| | GSM8k (5-shot) | 67.63 | <u>70.13</u> | 25.92 | 65.88 | **74.14** | \n| **Long context** |  |  |  |  |  |  | \n| | Ruler 32k | 76.35 | 75.93 | <u>77.58</u> | 70.63 | **83.98** | \n| | Ruler 64k | <u>67.85</u> | 64.90 | **72.93** | 57.18 | 60.29 | \n| | Ruler 128k | 61.03 | <u>62.23</u> | **71.30** | 43.03 | 47.23 | \n\n#### Multilingual benchmarks\n\n\n| Category | Metric | SmolLM3 3B Base | Qwen2.5-3B | Llama3.2 3B | Qwen3 1.7B Base | Qwen3 4B Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Main supported languages |  |  |  |  |  |  |  |\n| French| MLMM Hellaswag | **63.94** | 57.47 | 57.66 | 51.26 | <u>61.00</u> |\n| | Belebele | 51.00 | <u>51.55</u> | 49.22 |49.44| **55.00** |\n| | Global MMLU (CF) | <u>38.37</u> | 34.22  | 33.71 | 34.94  |**41.80** |\n| | Flores-200 (5-shot) | 62.85| 61.38| <u>62.89</u> | 58.68 | **65.76** |\n| Spanish| MLMM Hellaswag | **65.85** | 58.25 | 59.39 | 52.40 | <u>61.85</u> |\n| | Belebele | 47.00 | <u>48.88</u> | 47.00 | 47.56 | **50.33** |\n| | Global MMLU (CF) | <u>38.51</u> | 35.84  | 35.60 | 34.79  |**41.22** |\n| | Flores-200 (5-shot) | <u>48.25</u>| 50.00| 44.45 | 46.93 | **50.16** |\n| German| MLMM Hellaswag | **59.56** | 49.99|  53.19|46.10| <u>56.43</u>|\n| | Belebele | <u>48.44</u> | 47.88 | 46.22 | 48.00 | **53.44**|\n| | Global MMLU (CF) | <u>35.10</u> | 33.19  | 32.60 | 32.73  |**38.70** |\n| | Flores-200 (5-shot) | **56.60**| 50.63| <u>54.95</u> | 52.58 | 50.48 |\n| Italian| MLMM Hellaswag | **62.49** | 53.21 | 54.96 | 48.72 | <u>58.76</u> |\n| | Belebele | <u>46.44</u> | 44.77 | 43.88 | 44.00 | **48.78** | 44.88 |\n| | Global MMLU (CF) | <u>36.99</u> | 33.91  | 32.79 | 35.37  |**39.26** |\n| | Flores-200 (5-shot) | <u>52.65<u/>| **54.87**| 48.83 | 48.37 | 49.11 |\n| Portuguese| MLMM Hellaswag | **63.22** | 57.38 | 56.84 | 50.73 | <u>59.89</u> |\n| | Belebele | 47.67 | **49.22** | 45.00 | 44.00 | 50.00 | <u>49.00</U> |\n| | Global MMLU (CF) | <u>36.88</u> | 34.72  | 33.05 | 35.26  |**40.66** |\n| | Flores-200 (5-shot) | <u>60.93</u> |57.68| 54.28 | 56.58 | **63.43** |\n\nThe model has also been trained on Arabic (standard), Chinese and Russian data, but has seen fewer tokens in these languages compared to the 6 above. We report the performance on these langages for information.\n| Category | Metric | SmolLM3 3B Base | Qwen2.5-3B | Llama3.2 3B | Qwen3 1.7B Base | Qwen3 4B Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Other supported languages |  |  |  |  |  |  |  |\n| Arabic| Belebele | 40.22 | 44.22 | <u>45.33</u> | 42.33 | **51.78** |\n| | Global MMLU (CF) | 28.57 | 28.81 | 27.67 | <u>29.37</u> | **31.85** |\n| | Flores-200 (5-shot) | <u>40.22</u> | 39.44 | **44.43** | 35.82 | 39.76 |\n| Chinese| Belebele | 43.78 | 44.56 | <u>49.56</u> | 48.78 | **53.22** |\n| | Global MMLU (CF) | 36.16 | 33.79 | <u>39.57</u> | 38.56 | **44.55** |\n| | Flores-200 (5-shot) | 29.17 | **33.21** | 31.89 | 25.70 | <u>32.50</u> |\n| Russian| Belebele | <u>47.44</u> | 45.89 | <u>47.44</u> | 45.22 | **51.44** |\n| | Global MMLU (CF) | <u>36.51</u> | 32.47 | 34.52 | 34.83 | **38.80** |\n| | Flores-200 (5-shot) | 47.13 | 48.74 | 50.74 | <u>54.70</u> | **60.53** |\n\n## Training\n\n### Model\n\n- **Architecture:** Transformer decoder\n- **Pretraining tokens:** 11T\n- **Precision:** bfloat16\n\n### Software & hardware\n\n- **GPUs:** 384 H100\n- **Training Framework:** [nanotron](https://github.com/huggingface/nanotron/tree/smollm3)\n- **Data processing framework:** [datatrove](https://github.com/huggingface/datatrove)\n- **Evaluation framework:** [lighteval](https://github.com/huggingface/lighteval)\n- **Post-training Framework:** [TRL](https://github.com/huggingface/trl)\n\n### Open resources\nHere is an infographic with all the training details \n- The datasets used for pretraining can be found in this [collection](https://huggingface.co/collections/HuggingFaceTB/smollm3-pretraining-datasets-685a7353fdc01aecde51b1d9) and those used in mid-training and post-training will be uploaded later \n- The training and evaluation configs and code can be found in the [huggingface/smollm](https://github.com/huggingface/smollm) repository.\n- The training intermediate checkpoints (including the mid-training and SFT checkpoints) are available at [HuggingFaceTB/SmolLM3-3B-checkpoints](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-checkpoints)\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/651e96991b97c9f33d26bde6/qiE5ZYr9SD1CIAtfEfuC8.png)\n\n### EU Summary of Public Content\n\nThe EU AI Act requires all GPAI models to provide a Public Summary of Training Content according to a [given template](https://digital-strategy.ec.europa.eu/en/library/explanatory-notice-and-template-public-summary-training-content-general-purpose-ai-models).\nYou can find the summary for this model below, as well as in its [development Space](https://huggingface.co/spaces/hfmlsoc/smollm3-eu-data-transparency).\n\n<iframe\n	src="https://hfmlsoc-smollm3-eu-data-transparency.hf.space"\n	frameborder="0"\n	width="850"\n	height="350"\n></iframe>\n\n\n## Limitations\n\nSmolLM3 can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\n\n## License\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n## Citation\n```bash\n@misc{bakouch2025smollm3,\n  title={{SmolLM3: smol, multilingual, long-context reasoner}},\n  author={Bakouch, Elie and Ben Allal, Loubna and Lozhkov, Anton and Tazi, Nouamane and Tunstall, Lewis and Pati√±o, Carlos Miguel and Beeching, Edward and Roucher, Aymeric and Reedi, Aksel Joonas and Gallou√©dec, Quentin and Rasul, Kashif and Habib, Nathan and Fourrier, Cl√©mentine and Kydlicek, Hynek and Penedo, Guilherme and Larcher, Hugo and Morlon, Mathieu and Srivastav, Vaibhav and Lochner, Joshua and Nguyen, Xuan-Son and Raffel, Colin and von Werra, Leandro and Wolf, Thomas},\n  year={2025},\n  howpublished={\url{https://huggingface.co/blog/smollm3}}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3075098624,"storage_bytes":6178163042,"files_count":12,"spaces_count":91,"gated":false,"private":false,"config":{"architectures":["SmolLM3ForCausalLM"],"model_type":"smollm3","tokenizer_config":{"bos_token":null,"eos_token":"<|im_end|>","pad_token":"<|im_end|>"},"chat_template_jinja":"{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ defaults ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- if enable_thinking is not defined -%}\n{%- set enable_thinking = true -%}\n{%- endif -%}\n\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ reasoning mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- if enable_thinking -%}\n  {%- set reasoning_mode = \"/think\" -%}\n{%- else -%}\n  {%- set reasoning_mode = \"/no_think\" -%}\n{%- endif -%}\n\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ header (system message) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{{- \"<|im_start|>system\\n\" -}}\n\n{%- if messages[0].role == \"system\" -%}\n  {%- set system_message = messages[0].content -%}\n  {%- if \"/no_think\" in system_message -%}\n    {%- set reasoning_mode = \"/no_think\" -%}\n  {%- elif \"/think\" in system_message -%}\n    {%- set reasoning_mode = \"/think\" -%}\n  {%- endif -%}\n  {%- set custom_instructions = system_message.replace(\"/no_think\", \"\").replace(\"/think\", \"\").rstrip() -%}\n{%- endif -%}\n\n{%- if \"/system_override\" in system_message -%}\n  {{- custom_instructions.replace(\"/system_override\", \"\").rstrip() -}}\n  {{- \"<|im_end|>\\n\" -}}\n{%- else -%}\n  {{- \"## Metadata\\n\\n\" -}}\n  {{- \"Knowledge Cutoff Date: June 2025\\n\" -}}\n  {%- set today = strftime_now(\"%d %B %Y\") -%}\n  {{- \"Today Date: \" ~ today ~ \"\\n\" -}}\n  {{- \"Reasoning Mode: \" + reasoning_mode + \"\\n\\n\" -}}\n  \n  {{- \"## Custom Instructions\\n\\n\" -}}\n  {%- if custom_instructions -%}\n    {{- custom_instructions + \"\\n\\n\" -}}\n  {%- elif reasoning_mode == \"/think\" -%}\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\\n\\n\" -}}\n  {%- else -%}\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face.\\n\\n\" -}}\n  {%- endif -%}\n\n  {%- if xml_tools or python_tools or tools -%}\n    {{- \"### Tools\\n\\n\" -}}\n    {%- if xml_tools or tools -%}\n      {%- if tools -%}\n        {%- set xml_tools = tools -%}\n      {%- endif -%}\n      {%- set ns = namespace(xml_tool_string=\"You may call one or more functions to assist with the user query.\\nYou are provided with function signatures within <tools></tools> XML tags:\\n\\n<tools>\\n\") -%}\n      {%- for tool in xml_tools[:] -%} {# The slicing makes sure that xml_tools is a list #}\n        {%- set ns.xml_tool_string = ns.xml_tool_string ~ (tool | string) ~ \"\\n\" -%}\n      {%- endfor -%}\n      {%- set xml_tool_string = ns.xml_tool_string + \"</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call>\" -%}\n      {{- xml_tool_string -}}\n    {%- endif -%}\n    {%- if python_tools -%}\n      {%- set ns = namespace(python_tool_string=\"When you send a message containing Python code between ''<code>'' and ''</code>'' tags, it will be executed in a stateful Jupyter notebook environment, and you will then be given the output to continued reasoning in an agentic loop.\\n\\nYou can use the following tools in your python code like regular functions:\\n<tools>\\n\") -%}\n      {%- for tool in python_tools[:] -%} {# The slicing makes sure that python_tools is a list #}\n        {%- set ns.python_tool_string = ns.python_tool_string ~ (tool | string) ~ \"\\n\" -%}\n      {%- endfor -%}\n      {%- set python_tool_string = ns.python_tool_string + \"</tools>\\n\\nThe state persists between code executions: so variables that you define in one step are still available thereafter.\" -%}\n      {{- python_tool_string -}}\n    {%- endif -%}\n    {{- \"\\n\\n\" -}}\n    {{- \"<|im_end|>\\n\" -}}\n  {%- endif -%}\n{%- endif -%}\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- for message in messages -%}\n    {%- set content = message.content if message.content is string else \"\" -%}\n    {%- if message.role == \"user\" -%}\n        {{ \"<|im_start|>\" + message.role + \"\\n\"  + content + \"<|im_end|>\\n\" }}\n    {%- elif message.role == \"assistant\" -%}\n        {% generation %}\n        {%- if reasoning_mode == \"/think\" -%}\n            {{ \"<|im_start|>assistant\\n\" + content.lstrip(\"\\n\") + \"<|im_end|>\\n\" }}\n        {%- else -%}\n            {{ \"<|im_start|>assistant\\n\" + \"<think>\\n\\n</think>\\n\" + content.lstrip(\"\\n\") + \"<|im_end|>\\n\" }}\n        {%- endif -%}\n        {% endgeneration %}\n    {%- elif message.role == \"tool\" -%}\n    {{ \"<|im_start|>\" + \"user\\n\"  + content + \"<|im_end|>\\n\" }}\n    {%- endif -%}\n{%- endfor -%}\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ generation prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- if add_generation_prompt -%}\n    {%- if reasoning_mode == \"/think\" -%}\n        {{ \"<|im_start|>assistant\\n\" }}\n    {%- else -%}\n        {{ \"<|im_start|>assistant\\n\" + \"<think>\\n\\n</think>\\n\"  }}\n    {%- endif -%}\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:huggingface:nanotron","source_url":"https://github.com/huggingface/nanotron"},{"type":"has_code","target_id":"github:huggingface:datatrove","source_url":"https://github.com/huggingface/datatrove"},{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:huggingface:trl","source_url":"https://github.com/huggingface/trl"},{"type":"has_code","target_id":"github:huggingface:smollm","source_url":"https://github.com/huggingface/smollm"}]', NULL, 'Apache-2.0', 'approved', 79.2, 'bec0d6b932061f6d5ad77634a27cf1ff', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-apple-DCLM-7B', 'huggingface--apple--dclm-7b', 'DCLM-7B', 'apple', '--- license: apple-ascl --- <img src="https://cdn-uploads.huggingface.co/production/uploads/63118add64939fabc0108b28/BB42g4V8HTxb5dR4tcy8A.png" alt="DCLM Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/> DCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation tech...', '["transformers","safetensors","openlm","arxiv:2406.11794","license:apple-ascl","endpoints_compatible","region:us"]', 'other', 832, 252, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/apple/DCLM-7B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlicense: apple-ascl\n---\n\n\n\n<img src="https://cdn-uploads.huggingface.co/production/uploads/63118add64939fabc0108b28/BB42g4V8HTxb5dR4tcy8A.png" alt="DCLM Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n\n# Model Card for DCLM-Baseline-7B\n\nDCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation techniques for improving language model performance.\n\n## Model Details\n\n| Size | Training Tokens | Layers | Hidden Size | Attention Heads | Context Length |\n|------|-----------------|--------|-------------|-----------------|----------------|\n| 7B   | 2.5T            | 32     | 4096        | 32              | 2048           |\n\n\n### Model Description\n\n- **Developed by:** DataComp for Language Models (DCLM) Team\n- **Model type:** Decoder-only Transformer language model\n- **Language(s):** English (primarily)\n- **License:** Apple Sample Code License\n- **Contact:** contact@datacomp.ai\n- **Date:** June 2024\n\n### Model Sources\n\n- **Repository:** https://github.com/mlfoundations/dclm\n- **Dataset:** https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0\n- **Paper:** [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)\n\n\n## Using Model\n\nFirst install open_lm\n\n```bash\npip install git+https://github.com/mlfoundations/open_lm.git\n```\n\nThen:\n```python\nfrom open_lm.hf import *\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained("apple/DCLM-Baseline-7B")\nmodel = AutoModelForCausalLM.from_pretrained("apple/DCLM-Baseline-7B")\n\ninputs = tokenizer(["Machine learning is"], return_tensors="pt")\ngen_kwargs = {"max_new_tokens": 50, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.1}\noutput = model.generate(inputs[''input_ids''], **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n```\n\n\n\n\n\n\n### Training Details\n\nThe model was trained using the following setup:\n\n- **Architecture:** Decoder-only Transformer \n- **Framework:** PyTorch with OpenLM\n- **Optimizer:** AdamW\n- **Learning Rate:** 2e-3 (peak)\n- **Weight Decay:** 0.05\n- **Batch Size:** 2048 sequences\n- **Sequence Length:** 2048 tokens\n- **Total Training Tokens:** 2.5T\n- **Hardware:** Trained on H100 GPUs\n\nFor more detailed training information, please refer to Section 3.4 and Appendix F of the DCLM paper.\nTo ensure our trained model is broadly useful, including for math and coding tasks, we combine our 3.8T [DCLM-BASELINE](https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0)  with the [StarCoder](https://huggingface.co/datasets/bigcode/starcoderdata)  and [ProofPile2](https://huggingface.co/datasets/EleutherAI/proof-pile-2) data to arrive at a 4.1T token dataset.\n\n## Evaluation\n\nHere are the evaluation results for DCLM-Baseline-7B on various tasks (using [llm-foundry](https://github.com/mosaicml/llm-foundry) eval suite)\n\n| Task | Score |\n|------|-------|\n| MMLU (zero-shot) | 0.5766 |\n| MMLU (few-shot) | 0.6372 |\n| HellaSwag (zero-shot) | 0.7987 |\n| HellaSwag | 0.8043 |\n| Jeopardy | 0.4745 |\n| TriviaQA | 0.5270 |\n| GSM8K (CoT) | 0.0250 |\n| AGI Eval SAT Math (CoT) | 0.0136 |\n| AQuA (CoT) | 0.0490 |\n| SVAMP (CoT) | 0.4900 |\n| BigBench QA Wikidata | 0.7120 |\n| ARC Easy | 0.8220 |\n| ARC Challenge | 0.5990 |\n| BigBench Misconceptions | 0.6986 |\n| COPA | 0.8500 |\n| SIQA | 0.8291 |\n| CommonsenseQA | 0.8018 |\n| PIQA | 0.8128 |\n| OpenBookQA | 0.4540 |\n| BigBench Novel Concepts | 0.7188 |\n| BigBench Strange Stories | 0.7586 |\n| BigBench Strategy QA | 0.6173 |\n| LAMBADA | 0.8220 |\n| Winograd | 0.8828 |\n| Winogrande | 0.7269 |\n| BigBench Conlang Translation | 0.0244 |\n| BigBench Language Identification | 0.5219 |\n| BigBench Conceptual Combinations | 0.6990 |\n| BigBench Elementary Math QA | 0.3431 |\n| BigBench Dyck Languages | 0.4930 |\n| AGI Eval LSAT AR | 0.2435 |\n| BigBench CS Algorithms | 0.6121 |\n| BigBench Logical Deduction | 0.3620 |\n| BigBench Operators | 0.4857 |\n| BigBench Repeat Copy Logic | 0.4063 |\n| Simple Arithmetic (no spaces) | 0.2940 |\n| Simple Arithmetic (with spaces) | 0.3110 |\n| MathQA | 0.3098 |\n| LogiQA | 0.4132 |\n| PubMedQA | 0.7060 |\n| SQuAD | 0.5856 |\n| AGI Eval LSAT RC | 0.6716 |\n| AGI Eval LSAT LR | 0.5392 |\n| CoQA | 0.4074 |\n| BigBench Understanding Fables | 0.6825 |\n| BoolQ | 0.8343 |\n| AGI Eval SAT EN | 0.7670 |\n| Winogender MC (Female) | 0.6000 |\n| Winogender MC (Male) | 0.5500 |\n| Enterprise PII Classification | 0.7676 |\n| BBQ | 0.6912 |\n| GPQA Main | 0.2612 |\n| GPQA Diamond | 0.2475 |\n\nNote: All scores are presented as decimal values between 0 and 1, representing the proportion of correct answers or the model''s performance on each task.\n\n\n## Comparison \n\n\nBelow are comparisions of this model with other models in the 7B regime.\n\n| Model         | Params | Tokens | Open dataset? | CORE     | MMLU     | EXTENDED |\n|---------------|--------|--------|---------------|----------|----------|----------|\n| **Open weights, closed datasets** |        |        |               |          |          |          |\n| Llama2        | 7B     | 2T     | ‚ùå             | 49.2     | 45.8     | 34.1     |\n| DeepSeek      | 7B     | 2T     | ‚ùå             | 50.7     | 48.5     | 35.3     |\n| Mistral-0.3   | 7B     | ?      | ‚ùå             | 57.0     | 62.7     | 45.1     |\n| QWEN-2        | 7B     | ?      | ‚ùå            | 57.5     | **71.9** | 50.5     |\n| Llama3        | 8B     | 15T    | ‚ùå             | 57.6     | 66.2     | 46.3     |\n| Gemma         | 8B     | 6T     | ‚ùå             | 57.8     | 64.3     | 44.6     |\n| Phi-3         | 7B     | ?      | ‚ùå             | **61.0** | 69.9     | **57.9** |\n| **Open weights, open datasets** |        |        |               |          |          |          |\n| Falcon        | 7B     | 1T     | ‚úÖ              | 44.1     | 27.4     | 25.1     |\n| OLMo-1.7      | 7B     | 2.1T   | ‚úÖ              | 47.0     | 54.0     | 34.2     |\n| MAP-Neo       | 7B     | 4.5T   | ‚úÖ              | **50.2** | **57.1** | **40.4** |\n| **DCLM-7B** | 7B     | 2.5T   | ‚úÖ              | **56.1** | **63.7** | **43.6** |\n\n\n\n## Limitations and Biases\n\nWhile DCLM-Baseline-7B demonstrates strong performance across a range of tasks, it''s important to note:\n\n1. The model may exhibit biases present in its training data, which is derived from web crawl data.\n2. It has not undergone specific alignment or safety fine-tuning, so outputs should be used with caution.\n3. Performance on tasks not included in the evaluation suite may vary.\n4. The model''s knowledge is limited to its training data cutoff date.\n\n## Ethical Considerations\n\nUsers should be aware that this model, like all large language models, can potentially generate harmful or biased content. It should not be used for making decisions about individuals or in sensitive applications without appropriate safeguards and human oversight.\n\n## Citation\n\nIf you use this model in your research, please cite:\n\n```\n@article{Li2024DataCompLM,\n  title={DataComp-LM: In search of the next generation of training sets for language models},\n  author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and [... full author list]},\n  journal={arXiv preprint arXiv:2406.11794},\n  year={2024}\n}\n```\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":6889674752,"storage_bytes":27558732152,"files_count":14,"spaces_count":3,"gated":false,"private":false,"config":{"architectures":["OpenLMModel"],"model_type":"openlm","tokenizer_config":{"unk_token":"<|endoftext|>","bos_token":"<|endoftext|>","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:mlfoundations:dclm","source_url":"https://github.com/mlfoundations/dclm"},{"type":"has_code","target_id":"github:mlfoundations:open_lm.git","source_url":"https://github.com/mlfoundations/open_lm.git"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"based_on_paper","target_id":"arxiv:2406.11794","source_url":"https://arxiv.org/abs/2406.11794"}]', NULL, 'apple-ascl', 'approved', 64.2, 'b0372a7069c23680c9392b851bce5ef7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Llama-8B', 'huggingface--deepseek-ai--deepseek-r1-distill-llama-8b', 'DeepSeek-R1-Distill-Llama-8B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","llama","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 831, 1113709, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030261248,"storage_bytes":16060556354,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 99.2, '22075d47f48572747c3607c4506bcf1d', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Llama-8B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Llama-8B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-segmind-SSD-1B', 'huggingface--segmind--ssd-1b', 'SSD-1B', 'segmind', '--- license: apache-2.0 tags: - text-to-image - ultra-realistic - text-to-image - stable-diffusion - distilled-model - knowledge-distillation pinned: true datasets: - zzliang/GRIT - wanng/midjourney-v5-202304-clean library_name: diffusers --- !image/png Try out the model at Segmind SSD-1B for ‚ö° fastest inference. You can also try it on ü§ó Spaces The Segmind Stable Diffusion Model (SSD-1B) is a **distilled 50% smaller** version of the Stable Diffusion XL (SDXL), offering a **60% speedup** whil...', '["diffusers","safetensors","text-to-image","ultra-realistic","stable-diffusion","distilled-model","knowledge-distillation","dataset:zzliang/grit","dataset:wanng/midjourney-v5-202304-clean","arxiv:2401.02677","license:apache-2.0","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 827, 13317, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/segmind/SSD-1B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- text-to-image\n- ultra-realistic\n- text-to-image\n- stable-diffusion\n- distilled-model\n- knowledge-distillation\npinned: true\ndatasets:\n- zzliang/GRIT\n- wanng/midjourney-v5-202304-clean\nlibrary_name: diffusers\n---\n\n# Segmind Stable Diffusion 1B (SSD-1B) Model Card\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/WveKcu7q5PyZEwNezyyMC.png)\n\n## üì£ Read our [technical report](https://huggingface.co/papers/2401.02677) for more details on our disillation method\n\n## AUTOMATIC1111 compatibility added. Supporting file [here](https://huggingface.co/segmind/SSD-1B/blob/main/SSD-1B-A1111.safetensors)\n\n## Demo\n\nTry out the model at [Segmind SSD-1B](https://www.segmind.com/models/ssd-1b?utm_source=hf) for ‚ö° fastest inference. You can also try it on [ü§ó Spaces](https://huggingface.co/spaces/segmind/Segmind-Stable-Diffusion)\n\n## Model Description\n\nThe Segmind Stable Diffusion Model (SSD-1B) is a **distilled 50% smaller** version of the Stable Diffusion XL (SDXL), offering a **60% speedup** while maintaining high-quality text-to-image generation capabilities. It has been trained on diverse datasets, including Grit and Midjourney scrape data, to enhance its ability to create a wide range of visual content based on textual prompts.\n\nThis model employs a knowledge distillation strategy, where it leverages the teachings of several expert models in succession, including SDXL, ZavyChromaXL, and JuggernautXL, to combine their strengths and produce impressive visual outputs.\n\nSpecial thanks to the HF team ü§ó especially [Sayak](https://huggingface.co/sayakpaul), [Patrick](https://github.com/patrickvonplaten) and [Poli](https://huggingface.co/multimodalart) for their collaboration and guidance on this work.\n\n## Image Comparision (SDXL-1.0 vs SSD-1B)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/mOM_OMxbivVBELad1QQYj.png)\n\n## Usage:\nThis model can be used via the üß® Diffusers library. \n\nMake sure to install diffusers from source by running\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nIn addition, please install `transformers`, `safetensors` and `accelerate`:\n```\npip install transformers accelerate safetensors\n```\n\nTo use the model, you can run the following:\n\n```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\npipe = StableDiffusionXLPipeline.from_pretrained("segmind/SSD-1B", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")\npipe.to("cuda")\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\nprompt = "An astronaut riding a green horse" # Your prompt here\nneg_prompt = "ugly, blurry, poor quality" # Negative prompt here\nimage = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]\n```\n### Update: Our model should now be usable in ComfyUI.\n### Please do use negative prompting, and a CFG around 9.0 for the best quality!\n### Model Description\n\n- **Developed by:** [Segmind](https://www.segmind.com/)\n- **Developers:** [Yatharth Gupta](https://huggingface.co/Warlord-K) and [Vishnu Jaddipal](https://huggingface.co/Icar).\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** Apache 2.0\n- **Distilled From** [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n\n\n### Key Features\n\n- **Text-to-Image Generation:** The model excels at generating images from text prompts, enabling a wide range of creative applications.\n\n- **Distilled for Speed:** Designed for efficiency, this model offers a 60% speedup, making it a practical choice for real-time applications and scenarios where rapid image generation is essential.\n\n- **Diverse Training Data:** Trained on diverse datasets, the model can handle a variety of textual prompts and generate corresponding images effectively.\n\n- **Knowledge Distillation:** By distilling knowledge from multiple expert models, the Segmind Stable Diffusion Model combines their strengths and minimizes their limitations, resulting in improved performance.\n\n### Model Architecture\n\nThe SSD-1B Model is a 1.3B Parameter Model which has several layers removed from the Base SDXL Model\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/Qa8Ow-moLQhOvzp-5kGt4.png)\n\n### Training info\n\nThese are the key hyperparameters used during training:\n\n* Steps: 251000\n* Learning rate: 1e-5\n* Batch size: 32\n* Gradient accumulation steps: 4\n* Image resolution: 1024\n* Mixed-precision: fp16\n\n### Multi-Resolution Support\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/IwIaIB4nBdMx6Vs5q82cL.jpeg)\n\nSSD-1B can support the following output resolutions.\n\n* 1024 x 1024 (1:1 Square)\n* 1152 x 896 (9:7)\n* 896 x 1152 (7:9)\n* 1216 x 832 (19:13)\n* 832 x 1216 (13:19)\n* 1344 x 768 (7:4 Horizontal)\n* 768 x 1344 (4:7 Vertical)\n* 1536 x 640 (12:5 Horizontal)\n* 640 x 1536 (5:12 Vertical)\n    \n\n### Speed Comparision\n\nWe have observed that SSD-1B is upto 60% faster than the Base SDXL Model. Below is a comparision on an A100 80GB.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/TyymF1OkUjXLrHUp1XF0t.png)\n\nBelow are the speed up metrics on a RTX 4090 GPU.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/moMZrlDr-HTFkZlqWHUjQ.png)\n\n### Model Sources\n\nFor research and development purposes, the SSD-1B Model can be accessed via the Segmind AI platform. For more information and access details, please visit [Segmind](https://www.segmind.com/models/ssd-1b).\n\n## Uses\n\n\n### Direct Use\n\nThe Segmind Stable Diffusion Model is suitable for research and practical applications in various domains, including:\n\n- **Art and Design:** It can be used to generate artworks, designs, and other creative content, providing inspiration and enhancing the creative process.\n\n- **Education:** The model can be applied in educational tools to create visual content for teaching and learning purposes.\n\n- **Research:** Researchers can use the model to explore generative models, evaluate its performance, and push the boundaries of text-to-image generation.\n\n- **Safe Content Generation:** It offers a safe and controlled way to generate content, reducing the risk of harmful or inappropriate outputs.\n\n- **Bias and Limitation Analysis:** Researchers and developers can use the model to probe its limitations and biases, contributing to a better understanding of generative models'' behavior.\n\n### Downstream Use\n\nThe Segmind Stable Diffusion Model can also be used directly with the üß® Diffusers library training scripts for further training, including:\n\n- **[LoRA](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py):**\n```bash\nexport MODEL_NAME="segmind/SSD-1B"\nexport VAE_NAME="madebyollin/sdxl-vae-fp16-fix"\nexport DATASET_NAME="lambdalabs/pokemon-blip-captions"\n\naccelerate launch train_text_to_image_lora_sdxl.py \\n  --pretrained_model_name_or_path=$MODEL_NAME \\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\n  --dataset_name=$DATASET_NAME --caption_column="text" \\n  --resolution=1024 --random_flip \\n  --train_batch_size=1 \\n  --num_train_epochs=2 --checkpointing_steps=500 \\n  --learning_rate=1e-04 --lr_scheduler="constant" --lr_warmup_steps=0 \\n  --mixed_precision="fp16" \\n  --seed=42 \\n  --output_dir="sd-pokemon-model-lora-ssd" \\n  --validation_prompt="cute dragon creature" --report_to="wandb" \\n  --push_to_hub\n```\n\n- **[Fine-Tune](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py):**\n```bash\nexport MODEL_NAME="segmind/SSD-1B"\nexport VAE_NAME="madebyollin/sdxl-vae-fp16-fix"\nexport DATASET_NAME="lambdalabs/pokemon-blip-captions"\n\naccelerate launch train_text_to_image_sdxl.py \\n  --pretrained_model_name_or_path=$MODEL_NAME \\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\n  --dataset_name=$DATASET_NAME \\n  --enable_xformers_memory_efficient_attention \\n  --resolution=512 --center_crop --random_flip \\n  --proportion_empty_prompts=0.2 \\n  --train_batch_size=1 \\n  --gradient_accumulation_steps=4 --gradient_checkpointing \\n  --max_train_steps=10000 \\n  --use_8bit_adam \\n  --learning_rate=1e-06 --lr_scheduler="constant" --lr_warmup_steps=0 \\n  --mixed_precision="fp16" \\n  --report_to="wandb" \\n  --validation_prompt="a cute Sundar Pichai creature" --validation_epochs 5 \\n  --checkpointing_steps=5000 \\n  --output_dir="ssd-pokemon-model" \\n  --push_to_hub\n```\n- **[Dreambooth LoRA](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_sdxl.py):**\n```bash\nexport MODEL_NAME="segmind/SSD-1B"\nexport INSTANCE_DIR="dog"\nexport OUTPUT_DIR="lora-trained-xl"\nexport VAE_PATH="madebyollin/sdxl-vae-fp16-fix"\n\naccelerate launch train_dreambooth_lora_sdxl.py \\n  --pretrained_model_name_or_path=$MODEL_NAME  \\n  --instance_data_dir=$INSTANCE_DIR \\n  --pretrained_vae_model_name_or_path=$VAE_PATH \\n  --output_dir=$OUTPUT_DIR \\n  --mixed_precision="fp16" \\n  --instance_prompt="a photo of sks dog" \\n  --resolution=1024 \\n  --train_batch_size=1 \\n  --gradient_accumulation_steps=4 \\n  --learning_rate=1e-5 \\n  --report_to="wandb" \\n  --lr_scheduler="constant" \\n  --lr_warmup_steps=0 \\n  --max_train_steps=500 \\n  --validation_prompt="A photo of sks dog in a bucket" \\n  --validation_epochs=25 \\n  --seed="0" \\n  --push_to_hub\n```\n\n### Out-of-Scope Use\n\nThe SSD-1B Model is not suitable for creating factual or accurate representations of people, events, or real-world information. It is not intended for tasks requiring high precision and accuracy.\n\n## Limitations and Bias\n\nLimitations & Bias\nThe SSD-1B Model has some challenges in embodying absolute photorealism, especially in human depictions. While it grapples with incorporating clear text and maintaining the fidelity of complex compositions due to its autoencoding approach, these hurdles pave the way for future enhancements. Importantly, the model''s exposure to a diverse dataset, though not a panacea for ingrained societal and digital biases, represents a foundational step towards more equitable technology. Users are encouraged to interact with this pioneering tool with an understanding of its current limitations, fostering an environment of conscious engagement and anticipation for its continued evolution.\n\n## Citation\n```\n@misc{gupta2024progressive,\n      title={Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss}, \n      author={Yatharth Gupta and Vishnu V. Jaddipal and Harish Prabhala and Sayak Paul and Patrick Von Platen},\n      year={2024},\n      eprint={2401.02677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":28747978639,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2401.02677","source_url":"https://arxiv.org/abs/2401.02677"}]', NULL, 'Apache-2.0', 'approved', 79.2, '613a66ed381dfcec9824912604c5f89c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3n-E4B-it', 'huggingface--google--gemma-3n-e4b-it', 'gemma-3n-E4B-it', 'google', '', '["transformers","safetensors","gemma3n","any-to-any","automatic-speech-recognition","automatic-speech-translation","audio-text-to-text","video-text-to-text","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2210.03057","arxiv:2502.12404","arxiv:2411.19799","arxiv:2009.03300","arxiv:2502.21228","arxiv:2311.12022","arxiv:2403.07974","arxiv:2108.07732","arxiv:2107.03374","base_model:google/gemma-3n-e4b","base_model:finetune:google/gemma-3n-e4b","license:gemma","endpoints_compatible","region:us"]', 'image-text-to-text', 827, 67258, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3n-E4B-it","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":7849978192,"storage_bytes":50810768611,"files_count":17,"spaces_count":76,"gated":"manual","private":false,"config":{"architectures":["Gemma3nForConditionalGeneration"],"model_type":"gemma3n","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false},"chat_template_jinja":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''audio'' -%}\n                {{ ''<audio_soft_token>'' }}\n            {%- elif item[''type''] == ''image'' -%}\n                {{ ''<image_soft_token>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2411.19799","source_url":"https://arxiv.org/abs/2411.19799"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"}]', NULL, 'Gemma', 'approved', 39.2, '2b1720a0f060a5d9feb57c2d2230dccc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2b-it', 'huggingface--google--gemma-2b-it', 'gemma-2b-it', 'google', '', '["transformers","safetensors","gguf","gemma","text-generation","conversational","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 826, 67406, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2b-it","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2506172416,"storage_bytes":52310489489,"files_count":12,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 39.2, '97736eb4875d69c74c9a738d5cab0bba', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-Krea-dev', 'huggingface--black-forest-labs--flux.1-krea-dev', 'FLUX.1-Krea-dev', 'black-forest-labs', '', '["diffusers","safetensors","text-to-image","image-generation","flux","en","base_model:black-forest-labs/flux.1-dev","base_model:finetune:black-forest-labs/flux.1-dev","license:other","endpoints_compatible","diffusers:fluxpipeline","region:us"]', 'text-to-image', 826, 36332, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":57893109574,"files_count":29,"spaces_count":79,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.2, '0303983206abb03d72d21182d9c5ca85', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-30B-A3B', 'huggingface--qwen--qwen3-30b-a3b', 'Qwen3-30B-A3B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-30B-A3B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-30B-A3B-Base --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series,...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","arxiv:2309.00071","arxiv:2505.09388","base_model:qwen/qwen3-30b-a3b-base","base_model:finetune:qwen/qwen3-30b-a3b-base","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 823, 390194, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-30B-A3B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-30B-A3B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-30B-A3B-Base\n---\n\n# Qwen3-30B-A3B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-30B-A3B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 30.5B in total and 3.3B activated\n- Number of Paramaters (Non-Embedding): 29.9B\n- Number of Layers: 48\n- Number of Attention Heads (GQA): 32 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3_moe''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-30B-A3B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-30B-A3B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-30B-A3B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model''s performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'' --max-model-len 131072\n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}}''\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for ''rope_type''=''yarn'': {''original_max_position_embeddings''}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":30532122624,"storage_bytes":61077998302,"files_count":26,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 79.2, 'cf19637160f60d09cf918a19e5dd5273', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Salesforce-blip-image-captioning-base', 'huggingface--salesforce--blip-image-captioning-base', 'blip-image-captioning-base', 'Salesforce', '--- pipeline_tag: image-to-text tags: - image-captioning languages: - en license: bsd-3-clause --- Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone). | !BLIP.gif | |:--:| | <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>| Authors from the paper write in the abstract: *Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-tra...', '["transformers","pytorch","tf","blip","image-to-text","image-captioning","arxiv:2201.12086","license:bsd-3-clause","endpoints_compatible","region:us"]', 'image-to-text', 822, 2379522, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Salesforce/blip-image-captioning-base","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-to-text\ntags:\n- image-captioning\nlanguages:\n- en\nlicense: bsd-3-clause\n---\n\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\nModel card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).\n\n| ![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif) |\n|:--:|\n| <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>|\n\n## TL;DR\n\nAuthors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:\n\n*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n\n## Usage\n\nYou can use this model for conditional and un-conditional image captioning\n\n### Using the Pytorch model\n\n#### Running the model on CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n#### Running the model on GPU\n\n##### In full precision \n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to("cuda")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt").to("cuda")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt").to("cuda")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n##### In half precision (`float16`)\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16).to("cuda")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt").to("cuda", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt").to("cuda", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n## Ethical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people‚Äôs lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\n\n\n## BibTex and citation info\n\n```\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":5939080634,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BlipForConditionalGeneration"],"model_type":"blip","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:salesforce:BLIP","source_url":"https://github.com/salesforce/BLIP"},{"type":"based_on_paper","target_id":"arxiv:2201.12086","source_url":"https://arxiv.org/abs/2201.12086"}]', NULL, 'BSD-3-Clause', 'approved', 64.2, '95c08ec889de7d221134556e89f950c3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-BAAI-bge-reranker-v2-m3', 'huggingface--baai--bge-reranker-v2-m3', 'bge-reranker-v2-m3', 'BAAI', '--- license: apache-2.0 pipeline_tag: text-classification tags: - transformers - sentence-transformers - text-embeddings-inference language: - multilingual --- **More details please refer to our Github: FlagEmbedding.** - Model List - Usage - Fine-tuning - Evaluation - Citation Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. You can get a relevance score by inputting query and passage to the reranker. And the sc...', '["sentence-transformers","safetensors","xlm-roberta","text-classification","transformers","text-embeddings-inference","multilingual","arxiv:2312.15503","arxiv:2402.03216","license:apache-2.0","deploy:azure","region:us"]', 'text-classification', 821, 2894061, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/BAAI/bge-reranker-v2-m3","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-classification\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\nlanguage:\n- multilingual\n---\n\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker(''BAAI/bge-reranker-v2-m3'', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score([''query'', ''passage''])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set "normalize=True", which will apply sigmoid function to the score\nscore = reranker.compute_score([''query'', ''passage''], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set "normalize=True", which will apply sigmoid function to the score\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker(''BAAI/bge-reranker-v2-gemma'', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker(''BAAI/bge-reranker-v2-gemma'', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score([''query'', ''passage''])\nprint(score)\n\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker(''BAAI/bge-reranker-v2-minicpm-layerwise'', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker(''BAAI/bge-reranker-v2-minicpm-layerwise'', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score([''query'', ''passage''], cutoff_layers=[28]) # Adjusting ''cutoff_layers'' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(''BAAI/bge-reranker-v2-m3'')\nmodel = AutoModelForSequenceClassification.from_pretrained(''BAAI/bge-reranker-v2-m3'')\nmodel.eval()\n\npairs = [[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=''pt'', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = "Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either ''Yes'' or ''No''."\n    sep = "\n"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)[''input_ids'']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)[''input_ids'']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f''A: {query}'',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f''B: {passage}'',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs[''input_ids''],\n            sep_inputs + passage_inputs[''input_ids''],\n            truncation=''only_second'',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item[''input_ids''] = item[''input_ids''] + sep_inputs + prompt_inputs\n        item[''attention_mask''] = [1] * len(item[''input_ids''])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors=''pt'',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(''BAAI/bge-reranker-v2-gemma'')\nmodel = AutoModelForCausalLM.from_pretrained(''BAAI/bge-reranker-v2-gemma'')\nyes_loc = tokenizer(''Yes'', add_special_tokens=False)[''input_ids''][0]\nmodel.eval()\n\npairs = [[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = "Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either ''Yes'' or ''No''."\n    sep = "\n"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)[''input_ids'']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)[''input_ids'']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f''A: {query}'',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f''B: {passage}'',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs[''input_ids''],\n            sep_inputs + passage_inputs[''input_ids''],\n            truncation=''only_second'',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item[''input_ids''] = item[''input_ids''] + sep_inputs + prompt_inputs\n        item[''attention_mask''] = [1] * len(item[''input_ids''])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors=''pt'',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(''BAAI/bge-reranker-v2-minicpm-layerwise'', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(''BAAI/bge-reranker-v2-minicpm-layerwise'', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to(''cuda'')\nmodel.eval()\n\npairs = [[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{"query": str, "pos": List[str], "neg":List[str], "prompt": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\n--output_dir {path to save model} \\n--model_name_or_path google/gemma-2b \\n--train_data ./toy_finetune_data.jsonl \\n--learning_rate 2e-4 \\n--num_train_epochs 1 \\n--per_device_train_batch_size 1 \\n--gradient_accumulation_steps 16 \\n--dataloader_drop_last True \\n--query_max_len 512 \\n--passage_max_len 512 \\n--train_group_size 16 \\n--logging_steps 1 \\n--save_steps 2000 \\n--save_total_limit 50 \\n--ddp_find_unused_parameters False \\n--gradient_checkpointing \\n--deepspeed stage1.json \\n--warmup_ratio 0.1 \\n--bf16 \\n--use_lora True \\n--lora_rank 32 \\n--lora_alpha 64 \\n--use_flash_attn True \\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\n--output_dir {path to save model} \\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\n--train_data ./toy_finetune_data.jsonl \\n--learning_rate 2e-4 \\n--num_train_epochs 1 \\n--per_device_train_batch_size 1 \\n--gradient_accumulation_steps 16 \\n--dataloader_drop_last True \\n--query_max_len 512 \\n--passage_max_len 512 \\n--train_group_size 16 \\n--logging_steps 1 \\n--save_steps 2000 \\n--save_total_limit 50 \\n--ddp_find_unused_parameters False \\n--gradient_checkpointing \\n--deepspeed stage1.json \\n--warmup_ratio 0.1 \\n--bf16 \\n--use_lora True \\n--lora_rank 32 \\n--lora_alpha 64 \\n--use_flash_attn True \\n--target_modules q_proj k_proj v_proj o_proj \\n--start_layer 8 \\n--head_multi True \\n--head_type simple \\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"text-classification","library_name":"sentence-transformers","framework":"sentence-transformers","params":567755777,"storage_bytes":7403932631,"files_count":13,"spaces_count":74,"gated":false,"private":false,"config":{"architectures":["XLMRobertaForSequenceClassification"],"model_type":"xlm-roberta","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":"<mask>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:FlagOpen:FlagEmbedding","source_url":"https://github.com/FlagOpen/FlagEmbedding"},{"type":"has_code","target_id":"github:FlagOpen:FlagEmbedding","source_url":"https://github.com/FlagOpen/FlagEmbedding"},{"type":"based_on_paper","target_id":"arxiv:2312.15503","source_url":"https://arxiv.org/abs/2312.15503"},{"type":"based_on_paper","target_id":"arxiv:2402.03216","source_url":"https://arxiv.org/abs/2402.03216"}]', NULL, 'Apache-2.0', 'approved', 99.1, '92440e2d175d77e6e771b1cfa509ddd3', NULL, 'https://huggingface.co/BAAI/bge-reranker-v2-m3/resolve/main/assets/BEIR-bge-en-v1.5.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-BAAI-bge-reranker-v2-m3 from https://huggingface.co/BAAI/bge-reranker-v2-m3/resolve/main/assets/BEIR-bge-en-v1.5.png
Image converted to WebP: data/images/huggingface-BAAI-bge-reranker-v2-m3.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanVideo-1.5', 'huggingface--tencent--hunyuanvideo-1.5', 'HunyuanVideo-1.5', 'tencent', '--- library_name: HunyuanVideo-1.5 license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE language: - en - zh tags: - text-to-video - image-to-video pipeline_tag: text-to-video extra_gated_eu_disallowed: true --- ‰∏≠ÊñáÊñáÊ°£ <div align="center"> <img src="./assets/logo.png" alt="HunyuanVideo-1.5 Logo" width="80%"> </div> <div align="center"> <!-- <img src="./assets/banner.png" alt="HunyuanVideo-1.5 Banner" width="8...', '["hunyuanvideo-1.5","diffusers","safetensors","text-to-video","image-to-video","en","zh","arxiv:2511.18870","license:other","region:us"]', 'text-to-video', 820, 2944, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanVideo-1.5","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: HunyuanVideo-1.5\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - text-to-video\n  - image-to-video\npipeline_tag: text-to-video\nextra_gated_eu_disallowed: true\n---\n\n\n[‰∏≠ÊñáÊñáÊ°£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align="center">\n\n<img src="./assets/logo.png" alt="HunyuanVideo-1.5 Logo" width="80%">\n\n# üé¨ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align="center">\n<!-- <img src="./assets/banner.png" alt="HunyuanVideo-1.5 Banner" width="800"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align="center">\n  <a href="https://hunyuan.tencent.com/video/zh?tabIndex=0" target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href="https://arxiv.org/pdf/2511.18870" target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target="_blank"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md" target="_blank"><img src=https://img.shields.io/badge/üìö-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href="./ComfyUI/README.md" target="_blank"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href="https://github.com/ModelTC/LightX2V" target="_blank"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href="https://tusi.cn/models/933574988890423836" target="_blank"><img src=https://img.shields.io/badge/ÂêêÂè∏-purple.svg?logo=book height=22px></a>\n  <a href="https://tensor.art/models/933574988890423836" target="_blank"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align="center">\n    üëè Join our <a href="./assets/wechat.png" target="_blank">WeChat</a> and <a href="https://discord.gg/ehjWMqF5wY">Discord</a> | \nüíª <a href="https://hunyuan.tencent.com/video/zh?tabIndex=0">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## üî•üî•üî• News\n* üöÄ Dec 05, 2025: **New Release**: We now release the [480p I2V step-distilled model](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled), which generates videos in 8 or 12 steps (recommended)! On RTX 4090, end-to-end generation time is reduced by 75%, and a single RTX 4090 can generate videos within 75 seconds. The step-distilled model maintains comparable quality to the original model while achieving significant speedup. See [Step Distillation Comparison](./assets/step_distillation_comparison.md) for detailed quality comparisons. For even faster generation, you can also try 4 steps (faster speed with slightly reduced quality). **To enable the step-distilled model, run `generate.py` with the `--enable_step_distill` parameter.** See [Usage](#-usage) for detailed usage instructions. üî•üî•üî•üÜï\n* üìö Dec 05, 2025: **Training Code Released**: We now open-source the training code for HunyuanVideo-1.5! The training script (`train.py`) provides a full training pipeline with support for distributed training, FSDP, context parallel, gradient checkpointing, and more. HunyuanVideo-1.5 is trained using the Muon optimizer, which we have open-sourced in the [Training](#-training) section. **If you would like to continue training our model or fine-tune it with LoRA, please use the Muon optimizer.** See [Training](#-training) section for detailed usage instructions. üî•üî•üî•üÜï\n* üéâ **Diffusers Support**: HunyuanVideo-1.5 is now available on Hugging Face Diffusers! Check out [Diffusers collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15) for easy integration. üî•üî•üî•üÜï\n* üöÄ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. üî•üî•üî•üÜï \n* üöÄ Nov 24, 2025: We now support deepcache inference.\n* üëã Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## üé• Demo\n<div align="center">\n  <video src="https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d" width="60%"> </video>\n</div>\n\n## üß© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **Diffusers** - [HunyuanVideo-1.5 Diffusers](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15): Official Hugging Face Diffusers integration for HunyuanVideo-1.5. Easily use HunyuanVideo-1.5 with the Diffusers library for seamless integration into your projects. See [Usage with Diffusers](#usage-with-diffusers) section for details.\n\n- **ComfyUI** - [ComfyUI](https://github.com/comfyanonymous/ComfyUI): A powerful and modular diffusion model GUI with a graph/nodes interface. ComfyUI supports HunyuanVideo-1.5 with various engineering optimizations for fast inference. We provide a [ComfyUI Usage Guide](./ComfyUI/README.md) for HunyuanVideo-1.5.\n\n- **Community-implemented ComfyUI Plugin** - [comfyui_hunyuanvideo_1.5_plugin](https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin): A community-implemented ComfyUI plugin for HunyuanVideo-1.5, offering both simplified and complete node sets for quick usage or deep workflow customization, with built-in automatic model download support.\n\n- **LightX2V** - [LightX2V](https://github.com/ModelTC/LightX2V): A lightweight and efficient video generation framework that integrates HunyuanVideo-1.5, supporting multiple engineering acceleration techniques for fast inference.\n\n- **Wan2GP v9.62** - [Wan2GP](https://github.com/deepbeepmeep/Wan2GP): WanGP is a very low VRAM app (as low 6 GB of VRAM for Hunyuan Video 1.5) supports Lora Accelerator for a 8 steps generation and offers tools to facilitate Video Generation.\n\n- **ComfyUI-MagCache** - [ComfyUI-MagCache](https://github.com/Zehong-Ma/ComfyUI-MagCache): MagCache is a training-free caching approach that accelerates video generation by estimating fluctuating differences among model outputs across timesteps. It achieves 1.7x speedup for HunyuanVideo-1.5 with 20 inference steps.\n\n\n## üìë Open-source Plan\n- HunyuanVideo-1.5 (T2V/I2V)\n  - [x] Inference Code and checkpoints\n  - [x] ComfyUI Support\n  - [x] LightX2V Support\n  - [x] Diffusers Support\n  - [ ] Release all model weights (Sparse attention, distill model, and SR models)\n\n## üìã Table of Contents\n- [üî•üî•üî• News](#-news)\n- [üé• Demo](#-demo)\n- [üß© Community Contributions](#-community-contributions)\n- [üìë Open-source Plan](#-open-source-plan)\n- [üìñ Introduction](#-introduction)\n- [‚ú® Key Features](#-key-features)\n- [üìú System Requirements](#-system-requirements)\n- [üõ†Ô∏è Dependencies and Installation](#Ô∏è-dependencies-and-installation)\n- [üß± Download Pretrained Models](#-download-pretrained-models)\n- [üìù Prompt Guide](#-prompt-guide)\n- [üîë Inference](#-inference)\n  - [Inference with Source Code](#inference-with-source-code)\n  - [Usage with Diffusers](#usage-with-diffusers)\n  - [Prompt Enhancement](#prompt-enhancement)\n  - [Text to Video](#text-to-video)\n  - [Image to Video](#image-to-video)\n  - [Command Line Arguments](#command-line-arguments)\n  - [Optimal Inference Configurations](#optimal-inference-configurations)\n- [üéì Training](#-training)\n- [üé¨ More Examples](#-more-examples)\n- [üìä Evaluation](#-evaluation)\n- [üìö Citation](#-citation)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üåü Github Star History](#-github-star-history)\n\n\n## üìñ Introduction\nWe present HunyuanVideo-1.5, a lightweight yet powerful video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture with selective and sliding tile attention(SSTA), enhanced bilingual understanding through glyph-aware text encoding , progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions. Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source models. By releasing the code and weights of HunyuanVideo-1.5, we provide the community with a high-performance foundation that significantly lowers the cost of video creation and research, making advanced video generation more accessible to all.\n\n\n## ‚ú® Key Features\n- **Lightweight High-Performance Architecture**: We propose an efficient architecture that integrates an 8.3B-parameter Diffusion Transformer (DiT) with a 3D causal VAE, achieving compression ratios of 16√ó in spatial dimensions and 4√ó along the temporal axis. Additionally, the innovative SSTA (Selective and Sliding Tile Attention) mechanism prunes redundant spatiotemporal kv blocks, significantly reduces computational overhead for long video sequences and accelerates inference, achieving an end-to-end speedup of $1.87 \times$ in 10-second 720p video synthesis compared to FlashAttention-3.\n\n<div align="center">\n<img src="./assets/hy_video_1_5_dit.png" alt="HunyuanVideo-1.5 DiT" width="600">\n</div> \n\n\n- **Video Super-Resolution Enhancement**: We develop an efficient few-step super-resolution network that upscales outputs to 1080p. It enhances sharpness while correcting distortions, thereby refining details and overall visual texture.\n\n<div align="center">\n<img src="./assets/hy_video_1_5_vsr.png" alt="HunyuanVideo-1.5 VSR" width="600">\n</div> \n\n- **End-to-End Training Optimization**: This work employs a multi-stage, progressive training strategy covering the entire pipeline from pre-training to post-training. Combined with the Muon optimizer to accelerate convergence, this approach holistically refines motion coherence, aesthetic quality, and human preference alignment, achieving professional-grade content generation.\n\n## üìú System Requirements\n\n### Hardware Requirements\n\n- **GPU**: NVIDIA GPU with CUDA support\n- **Minimum GPU Memory**: 14 GB (with model offloading enabled)\n  \n  > **Note:** The memory requirements above are measured with model offloading enabled. If your GPU has sufficient memory, you may disable offloading for improved inference speed.\n\n### Software Requirements\n\n- **Operating System**: Linux\n- **Python**: Python 3.10 or higher\n- **CUDA**: Compatible CUDA version for your PyTorch installation\n\n## üõ†Ô∏è Dependencies and Installation\n\n### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git\ncd HunyuanVideo-1.5\n```\n\n### Step 2: Install Basic Dependencies\n\n```bash\npip install -r requirements.txt\npip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python\n```\n\n### Step 3: Install Attention Libraries\n\n* Flash Attention: \n  Install Flash Attention for faster inference and reduced GPU memory consumption.\n  Detailed installation instructions are available at [Flash Attention](https://github.com/Dao-AILab/flash-attention).\n\n* Flex-Block-Attention: \n  flex-block-attn is only required for sparse attention to achieve faster inference and can be installed by the following command:\n  ```bash\n  git clone https://github.com/Tencent-Hunyuan/flex-block-attn.git\n  cd flex-block-attn\n  git submodule update --init --recursive\n  python3 setup.py install\n  ```\n\n* SageAttention: \n  To enable SageAttention for faster inference, you need to install it by the following command:\n  > **Note**: Enabling SageAttention will automatically disable Flex-Block-Attention.\n  ```bash\n  git clone https://github.com/cooper1637/SageAttention.git\n  cd SageAttention \n  export EXT_PARALLEL=4 NVCC_APPEND_FLAGS="--threads 8" MAX_JOBS=32 # Optional\n  python3 setup.py install\n  ```\n\n## üß± Download Pretrained Models\n\n> üí° Distillation models and sparse attention models are still coming soon. Please stay tuned for the latest updates on the Hugging Face Model Card.\n\nDownload the pretrained models before generating videos. Detailed instructions are available at [checkpoints-download.md](checkpoints-download.md).\n\n### Model Cards\n|ModelName| Download                     |\n|-|---------------------------| \n|HunyuanVideo-1.5-480P-T2V|[480P-T2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_t2v) |\n|HunyuanVideo-1.5-480P-I2V |[480P-I2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v) |\n|HunyuanVideo-1.5-480P-T2V-cfg-distill | [480P-T2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_t2v_distilled) |\n|HunyuanVideo-1.5-480P-I2V-cfg-distill |[480P-I2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_distilled) |\n|HunyuanVideo-1.5-480P-I2V-step-distill |[480P-I2V-step-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled) |\n|HunyuanVideo-1.5-720P-T2V|[720P-T2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_t2v) |\n|HunyuanVideo-1.5-720P-I2V |[720P-I2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v) |\n|HunyuanVideo-1.5-720P-T2V-cfg-distill| Comming soon |\n|HunyuanVideo-1.5-720P-I2V-cfg-distill |[720P-I2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v_distilled) |\n|HunyuanVideo-1.5-720P-T2V-sparse-cfg-distill| Comming soon |\n|HunyuanVideo-1.5-720P-I2V-sparse-cfg-distill |[720P-I2V-sparse-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v_distilled_sparse) |\n|HunyuanVideo-1.5-720P-sr-step-distill |[720P-sr](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_sr_distilled) |\n|HunyuanVideo-1.5-1080P-sr-step-distill |[1080P-sr](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/1080p_sr_distilled) |\n\n## üìù Prompt Guide\n### Prompt Writing Handbook\nPrompt enhancement plays a crucial role in enabling our model to generate high-quality videos. By writing longer and more detailed prompts, the generated video will be significantly improved. We encourage you to craft comprehensive and descriptive prompts to achieve the best possible video quality. we recommend community partners consulting our official guide on how to write effective prompts. \n\n**Reference:** **[HunyuanVideo-1.5 Prompt Handbook](https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md)**\n\n### System Prompts for Automatic Prompt Enhancement\nFor users seeking to optimize prompts for other large models, it is recommended to consult the definition of `t2v_rewrite_system_prompt` in the file `hyvideo/utils/rewrite/t2v_prompt.py` to guide text-to-video rewriting. Similarly, for image-to-video rewriting, refer to the definition of `i2v_rewrite_system_prompt` in `hyvideo/utils/rewrite/i2v_prompt.py`.\n\n## üîë Inference\n\n### Inference with Source Code\n\n\nFor prompt rewriting, we recommend using Gemini or models deployed via vLLM. This codebase currently only supports models compatible with the vLLM API. If you wish to use Gemini, you will need to implement your own interface calls.\n\nFor models with a vLLM API, note that T2V (text-to-video) and I2V (image-to-video) have different recommended models and environment variables:\n\n- T2V: use [Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507), configure `T2V_REWRITE_BASE_URL` and `T2V_REWRITE_MODEL_NAME`\n- I2V: use [Qwen3-VL-235B-A22B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct), configure `I2V_REWRITE_BASE_URL` and `I2V_REWRITE_MODEL_NAME`\n\n> You may set the above model names to any other vLLM-compatible models you have deployed (including HuggingFace models).  \n> Rewriting is enabled by default (`--rewrite` defaults to `true`); to disable it explicitly, use `--rewrite false` or `--rewrite 0`. If no vLLM endpoint is configured, the pipeline runs without remote rewriting.\n\nExample: Generate a video (works for both T2V and I2V; set `IMAGE_PATH=none` for T2V or provide an image path for I2V)\n\n> üí° **Tip**: For faster inference speed, you can enable the step-distilled model using the `--enable_step_distill` parameter. The step-distilled model (480p I2V) can generate videos in 8 or 12 steps (recommended), achieving up to 75% speedup on RTX 4090 while maintaining comparable quality.\n>\n> **Tips:** If your GPU memory is > 14GB but you encounter OOM (Out of Memory) errors during generation, you can try setting the following environment variable before running:\n> ```bash\n> export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128\n> ```\n> \n> **Tips:** If you have limited CPU memory and encounter OOM during inference, you can try disable overlapped group offloading by adding the following argument:\n> ```bash\n> --overlap_group_offloading false\n> ```\n\n```bash\nexport T2V_REWRITE_BASE_URL="<your_vllm_server_base_url>"\nexport T2V_REWRITE_MODEL_NAME="<your_model_name>"\nexport I2V_REWRITE_BASE_URL="<your_vllm_server_base_url>"\nexport I2V_REWRITE_MODEL_NAME="<your_model_name>"\n\nPROMPT=''A girl holding a paper with words "Hello, world!"''\n\nIMAGE_PATH=/path/to/image.png # Optional, none or <image path> to enable i2v mode\nSEED=1\nASPECT_RATIO=16:9\nRESOLUTION=480p\nOUTPUT_PATH=./outputs/output.mp4\nMODEL_PATH=./ckpts # Path to pretrained model\n\n# Configuration for faster inference\nN_INFERENCE_GPU=8 # Parallel inference GPU count\nCFG_DISTILLED=true # Inference with CFG distilled model, 2x speedup\nSAGE_ATTN=true # Inference with SageAttention\nSPARSE_ATTN=false # Inference with sparse attention (only 720p models are equipped with sparse attention). Please ensure flex-block-attn is installed\nOVERLAP_GROUP_OFFLOADING=true # Only valid when group offloading is enabled, significantly increases CPU memory usage but speeds up inference\nENABLE_CACHE=true # Enable feature cache during inference. Significantly speeds up inference.\nCACHE_TYPE=deepcache # Support: deepcache, teacache, taylorcache\nENABLE_STEP_DISTILL=true # Enable step distilled model for 480p I2V, recommended 8 or 12 steps, up to 6x speedup\n\n\n# Configuration for better quality\nREWRITE=true # Enable prompt rewriting. Please ensure rewrite vLLM server is deployed and configured.\nENABLE_SR=true # Enable super resolution\n\n\ntorchrun --nproc_per_node=$N_INFERENCE_GPU generate.py \\n  --prompt "$PROMPT" \\n  --image_path $IMAGE_PATH \\n  --resolution $RESOLUTION \\n  --aspect_ratio $ASPECT_RATIO \\n  --seed $SEED \\n  --rewrite $REWRITE \\n  --cfg_distilled $CFG_DISTILLED \\n  --enable_step_distill $ENABLE_STEP_DISTILL \\n  --sparse_attn $SPARSE_ATTN --use_sageattn $SAGE_ATTN \\n  --enable_cache $ENABLE_CACHE --cache_type $CACHE_TYPE \\n  --overlap_group_offloading $OVERLAP_GROUP_OFFLOADING \\n  --sr $ENABLE_SR --save_pre_sr_video \\n  --output_path $OUTPUT_PATH \\n  --model_path $MODEL_PATH\n```\n\n\n\n### Command Line Arguments\n\n| Argument | Type | Required | Default | Description |\n|----------|------|----------|---------|-------------|\n| `--prompt` | str | Yes | - | Text prompt for video generation |\n| `--negative_prompt` | str | No | `''''` | Negative prompt for video generation |\n| `--resolution` | str | Yes | - | Video resolution: `480p` or `720p` |\n| `--model_path` | str | Yes | - | Path to pretrained model directory |\n| `--aspect_ratio` | str | No | `16:9` | Aspect ratio of the output video |\n| `--num_inference_steps` | int | No | `50` | Number of inference steps |\n| `--video_length` | int | No | `121` | Number of frames to generate |\n| `--seed` | int | No | `123` | Random seed for reproducibility |\n| `--image_path` | str | No | `None` | Path to reference image (enables i2v mode). Use `none` or `None` to explicitly use text-to-video mode |\n| `--output_path` | str | No | `None` | Output file path (if not provided, saves to `./outputs/output_{transformer_version}_{timestamp}.mp4`) |\n| `--sr` | bool | No | `true` | Enable super resolution (use `--sr false` or `--sr 0` to disable) |\n| `--save_pre_sr_video` | bool | No | `false` | Save original video before super resolution (use `--save_pre_sr_video` or `--save_pre_sr_video true` to enable, only effective when super resolution is enabled) |\n| `--rewrite` | bool | No | `true` | Enable prompt rewriting (use `--rewrite false` or `--rewrite 0` to disable, may result in lower quality video generation) |\n| `--cfg_distilled` | bool | No | `false` | Enable CFG distilled model for faster inference (~2x speedup, use `--cfg_distilled` or `--cfg_distilled true` to enable) |\n| `--enable_step_distill` | bool | No | `false` | Enable step distilled model for 480p I2V (recommended 8 or 12 steps, ~75% speedup on RTX 4090, use `--enable_step_distill` or `--enable_step_distill true` to enable) |\n| `--sparse_attn` | bool | No | `false` | Enable sparse attention for faster inference (~1.5-2x speedup, requires H-series GPUs, auto-enables CFG distilled, use `--sparse_attn` or `--sparse_attn true` to enable) |\n| `--offloading` | bool | No | `true` | Enable CPU offloading (use `--offloading false` or `--offloading 0` to disable for faster inference if GPU memory allows) |\n| `--group_offloading` | bool | No | `None` | Enable group offloading (default: None, automatically enabled if offloading is enabled. Use `--group_offloading` or `--group_offloading true/1` to enable, `--group_offloading false/0` to disable) |\n| `--overlap_group_offloading` | bool | No | `true` | Enable overlap group offloading (default: true). Significantly increases CPU memory usage but speeds up inference. Use `--overlap_group_offloading` or `--overlap_group_offloading true/1` to enable, `--overlap_group_offloading false/0` to disable |\n| `--dtype` | str | No | `bf16` | Data type for transformer: `bf16` (faster, lower memory) or `fp32` (better quality, slower, higher memory) |\n| `--use_sageattn` | bool | No | `false` | Enable SageAttention (use `--use_sageattn` or `--use_sageattn true/1` to enable, `--use_sageattn false/0` to disable) |\n| `--sage_blocks_range` | str | No | `0-53` | SageAttention blocks range (e.g., `0-5` or `0,1,2,3,4,5`) |\n| `--enable_cache` | bool | No | `false` | Enable cache for transformer (use `--enable_cache` or `--enable_cache true/1` to enable, `--enable_cache false/0` to disable) |\n| `--cache_type` | str | No | `deepcache` | Cache type for transformer (e.g., `deepcache, teacache, taylorcache`) |\n| `--no_cache_block_id` | str | No | `53` | Blocks to exclude from deepcache (e.g., `0-5` or `0,1,2,3,4,5`) |\n| `--cache_start_step` | int | No | `11` | Start step to skip when using cache |\n| `--cache_end_step` | int | No | `45` | End step to skip when using cache |\n| `--total_steps` | int | No | `50` | Total inference steps |\n| `--cache_step_interval` | int | No | `4` | Step interval to skip when using cache |\n\n**Note:** Use `--nproc_per_node` to specify the number of GPUs. For example, `--nproc_per_node=8` uses 8 GPUs.\n\n### Optimal Inference Configurations\n\nThe following table provides the optimal inference configurations (CFG scale, embedded CFG scale, flow shift, and inference steps) for each model to achieve the best generation quality:\n\n| Model | CFG Scale | Embedded CFG Scale | Flow Shift | Inference Steps |\n|-------|-----------|-------------------|------------|-----------------|\n| 480p T2V | 6 | None | 5 | 50 |\n| 480p I2V | 6 | None | 5 | 50 |\n| 720p T2V | 6 | None | 9 | 50 |\n| 720p I2V | 6 | None | 7 | 50 |\n| 480p T2V CFG Distilled | 1 | None | 5 | 50 |\n| 480p I2V CFG Distilled | 1 | None | 5 | 50 |\n| 480p I2V Step Distilled | 1 | None | 7 | 8 or 12 (recommended) |\n| 720p T2V CFG Distilled | 1 | None | 9 | 50 |\n| 720p I2V CFG Distilled | 1 | None | 7 | 50 |\n| 720p T2V CFG Distilled Sparse | 1 | None | 9 | 50 |\n| 720p I2V CFG Distilled Sparse | 1 | None | 7 | 50 |\n| 480‚Üí720 SR Step Distilled | 1 | None | 2 | 6 |\n| 720‚Üí1080 SR Step Distilled | 1 | None | 2 | 8 |\n\n**Please note that the cfg distilled model we provided, must use 50 steps to generate correct results.**\n\n### Usage with Diffusers\n\nHunyuanVideo-1.5 is available on Hugging Face Diffusers! You can easily use it with the Diffusers library:\n\n**Basic Usage:**\n\n```python\nimport torch\n\ndtype = torch.bfloat16\ndevice = "cuda:0"\n\nfrom diffusers import HunyuanVideo15Pipeline\nfrom diffusers.utils import export_to_video\n\npipe = HunyuanVideo15Pipeline.from_pretrained("hunyuanvideo-community/HunyuanVideo-1.5-Diffusers-720p_t2v", torch_dtype=dtype)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nvideo = pipe(\n    prompt=prompt,\n    generator=generator,\n    num_frames=121,\n    num_inference_steps=50,\n).frames[0]\n\nexport_to_video(video, "output.mp4", fps=24)\n```\n\n**Optimized Usage with Attention Backend:**\n\nHunyuanVideo-1.5 uses attention masks with variable-length sequences. For best performance, we recommend using an attention backend that handles padding efficiently.\n\nWe recommend installing kernels (`pip install kernels`) to access prebuilt attention kernels.\n\n```python\nimport torch\n\ndtype = torch.bfloat16\ndevice = "cuda:0"\n\nfrom diffusers import HunyuanVideo15Pipeline, attention_backend\nfrom diffusers.utils import export_to_video\n\npipe = HunyuanVideo15Pipeline.from_pretrained("hunyuanvideo-community/HunyuanVideo-1.5-Diffusers-720p_t2v", torch_dtype=dtype)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nwith attention_backend("_flash_3_hub"): # or `"flash_hub"` if you are not on H100/H800\n    video = pipe(\n        prompt=prompt,\n        generator=generator,\n        num_frames=121,\n        num_inference_steps=50,\n    ).frames[0]\n    export_to_video(video, "output.mp4", fps=24)\n```\n\nFor more details, please visit [HunyuanVideo-1.5 Diffusers Collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15).\n\n\n## üéì Training\n\nHunyuanVideo-1.5 is trained using the **Muon optimizer**, which accelerates convergence and improves training stability. The Muon optimizer combines momentum-based updates with Newton-Schulz orthogonalization for efficient optimization of large-scale video generation models.\n\n### Quick Start\n\nThe training script (`train.py`) provides a complete training pipeline for HunyuanVideo-1.5. Here''s how to use it:\n\n#### 1. Implement Your DataLoader\n\nReplace the `create_dummy_dataloader()` function in `train.py` with your own implementation. Your dataloader should return batches with the following format:\n\n- **Required fields:**\n  - `"pixel_values"`: `torch.Tensor` - Video: `[B, C, F, H, W]` or Image: `[B, C, H, W]`\n    - Note: For video data, temporal dimension F must be `4n+1` (e.g., 1, 5, 9, 13, 17, ...)\n  - `"text"`: `List[str]` - Text prompts for each sample\n  - `"data_type"`: `str` - `"video"` or `"image"`\n\n- **Optional fields (for performance optimization):**\n  - `"latents"`: Pre-encoded VAE latents (skips VAE encoding for faster training)\n  - `"byt5_text_ids"` and `"byt5_text_mask"`: Pre-tokenized byT5 inputs\n\nSee the `create_dummy_dataloader()` function in `train.py` for detailed batch format documentation.\n\n#### 2. Run Training\n\n**Single GPU:**\n```bash\npython train.py --pretrained_model_root <path_to_pretrained_model> [other args]\n```\n\n**Multi-GPU:**\n```bash\nN=8\ntorchrun --nproc_per_node=$N train.py --pretrained_model_root <path_to_pretrained_model> [other args]\n```\n\n**Example:**\n```bash\ntorchrun --nproc_per_node=8 train.py \\n  --pretrained_model_root ./ckpts \\n  --learning_rate 1e-5 \\n  --batch_size 1 \\n  --max_steps 10000 \\n  --output_dir ./outputs \\n  --enable_fsdp \\n  --enable_gradient_checkpointing \\n  --sp_size 8\n```\n\n#### 3. Key Training Parameters\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `--pretrained_model_root` | Path to pretrained model (required) | - |\n| `--learning_rate` | Learning rate | 1e-5 |\n| `--batch_size` | Batch size | 1 |\n| `--max_steps` | Maximum training steps | 10000 |\n| `--warmup_steps` | Warmup steps | 500 |\n| `--gradient_accumulation_steps` | Gradient accumulation steps | 1 |\n| `--enable_fsdp` | Enable FSDP for distributed training | true |\n| `--enable_gradient_checkpointing` | Enable gradient checkpointing | true |\n| `--sp_size` | Sequence parallelism size (must divide world_size) | 8 |\n| `--i2v_prob` | Probability of i2v task for video data | 0.3 |\n| `--use_muon` | Use Muon optimizer | true |\n| `--resume_from_checkpoint` | Resume from checkpoint directory | None |\n\n#### 4. Monitor Training\n\n- Checkpoints are saved to `output_dir` at intervals specified by `--save_interval`\n- Validation videos are generated at intervals specified by `--validation_interval`\n- Training logs are printed to console at intervals specified by `--log_interval`\n\n#### 5. Resume Training\n\nUse `--resume_from_checkpoint <checkpoint_dir>` to resume from a saved checkpoint:\n```bash\npython train.py \\n  --pretrained_model_root <path> \\n  --resume_from_checkpoint ./outputs/checkpoint-1000\n```\n\n\n## üìä Evaluation\n\n### Rating\nWe assess text-to-video generation using a comprehensive rating methodology that considers five key dimensions: text-video consistency, visual quality, structural stability, motion effects, and the aesthetic quality of individual frames. For image-to-video generation, the evaluation encompasses image-video consistency, instruction responsiveness, visual quality, structural stability, and motion effects.\n\n<div align="center">\n<img src="./assets/T2V_Rating.png" alt="rating result of t2v" width="800">\n</div> \n\n---\n\n<div align="center">\n<img src="./assets/I2V_Rating.png" alt="rating result of i2v" width="800">\n</div> \n\n\n### GSB\nThe GSB(Good/Same/Bad) approach is widely used to evaluate the relative performance of two models based on overall video perception quality.We carefully construct 300 diverse text prompts and 300 image samples to cover balanced application scenarios for both text-to-video and image-to-video tasks. For each prompt or image input, an equal number of video samples are generated by each model in a single run to ensure comparability. To maintain fairness, inference is performed only once per input without any cherry-picking of results. All competing models are evaluated using their default configurations. The evaluation is conducted by over 100 professional assessors\n\n<div align="center">\n<img src="./assets/T2V_GSB.png" alt="gsb result of t2v" width="800">\n</div>\n\n---\n\n<div align="center">\n<img src="./assets/I2V_GSB.png" alt="gsb result of i2v" width="800">\n</div> \n\n\n### Inference speed\nWe report inference speed with basic engineering-level acceleration techniques enabled on 8 H800 GPUs to demonstrate practical performance achievable in real-world deployment scenarios.\nPlease note that in this experiment, we do not pursue the most extreme acceleration at the cost of generation quality, but rather to achieve notable speed improvements while maintaining nearly identical output quality.\n\nWe report the total inference time for 50 diffusion steps for HunyuanVideo 1.5 below:\n\n<div align="center">\n<img src="./assets/speed.png" alt="" width="100%">\n</div> \n\n## üé¨ More Examples\n|Features|Demo1|Demo2|\n|------|------|------|\n|Strong Instruction Following|<video src="https://github.com/user-attachments/assets/fdc3c27b-69f5-46a1-b707-0b57510fa32f" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```‰∏ÄÂêçÂìÄ‰º§ÁöÑÈªëÂèë‰∏≠ÂõΩÂ•≥Â≠êÂáùÊúõÂ§©Á©∫ÔºåÂ§çÂè§ËÉ∂ÁâáÈ£éÊ†ºÁÉòÊâòÂá∫ÊÄÄÊóßÊàèÂâßÊ∞õÂõ¥``` </details> <details><summary>üìã Show rewrite prompt</summary> ```‰øØËßÜËßíÂ∫¶Ôºå‰∏Ä‰ΩçÊúâÁùÄÊ∑±Ëâ≤ÔºåÁï•Â∏¶Âáå‰π±ÁöÑÈïøÂç∑ÂèëÁöÑÂπ¥ËΩª‰∏≠ÂõΩÂ•≥ÊÄßÔºå‰Ω©Êà¥ÁùÄÈó™ËÄÄÁöÑÁèçÁè†È°πÈìæÂíåÂúÜÂΩ¢ÈáëËâ≤ËÄ≥ÁéØÔºåÂ•πÂáå‰π±ÁöÑÂ§¥ÂèëË¢´È£éÂêπÊï£ÔºåÂ•πÂæÆÂæÆÊä¨Â§¥ÔºåÊúõÂêëÂ§©Á©∫ÔºåÁ•ûÊÉÖÂçÅÂàÜÂìÄ‰º§ÔºåÁúº‰∏≠Âê´ÁùÄÊ≥™Ê∞¥„ÄÇÂò¥ÂîáÊ∂ÇÁùÄÁ∫¢Ëâ≤Âè£Á∫¢„ÄÇËÉåÊôØÊòØÂ∏¶ÊúâÂçé‰∏ΩÁ∫¢Ëâ≤Ëä±Á∫πÁöÑÂõæÊ°à„ÄÇÁîªÈù¢ÂëàÁé∞Â§çÂè§ÁîµÂΩ±È£éÊ†ºÔºåËâ≤Ë∞É‰ΩéÈ•±ÂíåÔºåÂ∏¶ÁùÄËΩªÂæÆÊüîÁÑ¶ÔºåÁÉòÊâòÊÉÖÁª™Ê∞õÂõ¥ÔºåË¥®ÊÑü‰ªø‰Ωõ20‰∏ñÁ∫™90Âπ¥‰ª£ÁöÑÁªèÂÖ∏ËÉ∂ÁâáÈ£éÊ†ºÔºåËê•ÈÄ†Âá∫ÊÄÄÊóß‰∏îÂØåÊúâÊàèÂâßÊÄßÁöÑÊÑüËßâ„ÄÇ``` </details>|<video src="https://github.com/user-attachments/assets/3fcb42cc-cdd3-4651-86a6-645a858561c4" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Âª∫Á≠ëËìùÂõæ‰∏äÁöÑÁ∫øÊù°Âåñ‰∏∫ÂÆû‰ΩìÔºåÁû¨Èó¥ÁîüÈïøÂá∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂ§çÂè§Â∑•‰∏öÈ£éÂäûÂÖ¨Á©∫Èó¥„ÄÇ``` </details> <details><summary>üìã Show rewrite prompt</summary> ```‰∏ÄÂ∫ßÁ©∫Êó∑ÁöÑÁé∞‰ª£ÈòÅÊ•ºÈáåÔºåÊúâ‰∏ÄÂº†Èì∫Â±ïÂú®Âú∞Êùø‰∏≠Â§ÆÁöÑÂª∫Á≠ëËìùÂõæ„ÄÇÂøΩÁÑ∂Èó¥ÔºåÂõæÁ∫∏‰∏äÁöÑÁ∫øÊù°Ê≥õËµ∑ÂæÆÂÖâÔºå‰ªø‰ΩõË¢´ÊüêÁßçÊó†ÂΩ¢ÁöÑÂäõÈáèÂî§ÈÜí„ÄÇÁ¥ßÊé•ÁùÄÔºåÈÇ£‰∫õÂèëÂÖâÁöÑÁ∫øÊù°ÂºÄÂßãÂêë‰∏äÂª∂‰º∏Ôºå‰ªéÂπ≥Èù¢‰∏≠Êå£ËÑ±ÔºåÂãæÂãíÂá∫Á´ã‰ΩìÁöÑËΩÆÂªì‚Äî‚ÄîÂ∞±ÂÉèÂú®Á©∫‰∏≠ËøõË°å‰∏ÄÂú∫Êó†Â£∞ÁöÑ3DÊâìÂç∞„ÄÇÈöèÂêéÔºåÂ•áËøπÂú®Âä†ÈÄüÂèëÁîüÔºöÊûÅÁÆÄÁöÑÊ©°Êú®ÂäûÂÖ¨Ê°å„ÄÅ‰ºòÈõÖÁöÑ‰ºäÂßÜÊñØÈ£éÊ†ºÁöÆË¥®Ê§Ö„ÄÅÈ´òÊåëÁöÑÂ∑•‰∏öÈ£éÈáëÂ±û‰π¶Êû∂ÔºåËøòÊúâÂá†ÁõèÁà±Ëø™ÁîüÁÅØÊ≥°Ôºå‰ª•ÂÖâÁ∫π‰∏∫È™®Êû∂ËøÖÈÄü‚ÄúÁîüÈïø‚ÄùÂá∫Êù•„ÄÇËΩ¨Áû¨Èó¥ÔºåÁ∫øÊù°Ë¢´ÁúüÂÆûÁöÑÊùêË¥®Â°´ÂÖÖ‚Äî‚ÄîÊú®ÊùêÁöÑÊ∏©Ê∂¶„ÄÅÁöÆÈù©ÁöÑË¥®ÊÑü„ÄÅÈáëÂ±ûÁöÑÂÜ∑ÈùôÔºåÈÉΩÂú®Áú®ÁúºÈó¥ÂÆåÊï¥ÂëàÁé∞„ÄÇÊúÄÁªàÔºåÊâÄÊúâÂÆ∂ÂÖ∑Á®≥Âõ∫ËêΩÂú∞ÔºåËìùÂõæÁöÑÂÖâËäíÊÇÑÁÑ∂Ë§™Âéª„ÄÇ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂäûÂÖ¨Á©∫Èó¥ÔºåÂ∞±ËøôÊ†∑‰ªé‰∫åÁª¥ÁöÑÂõæÁ∫∏‰∏≠ËØûÁîü„ÄÇ``` </details>|\n|Smooth Motion Generation|<video src="https://github.com/user-attachments/assets/447847f0-490a-45f9-a86d-a67ab1ff4231" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```A DJ is immersed in his musical world. He wears a pair of professional, matte-black headphones, revealing a focused expression. He wears a black bomber jacket, zipped open to reveal a T-shirt underneath. His upper body sways back and forth rhythmically to the throbbing electronic beats, his head moving with precise movement. The mixing console in front of him serves as the primary source of light. In the distance, the cool white glow of several stadium floodlights casts a deep, dark haze across the vast field, casting long shadows across the emerald green grass, creating a stark contrast to the brightly lit area surrounding the DJ booth. His hands danced swiftly and precisely across the equipment. The entire scene was filled with high-tech dynamics and the solitary creative passion. Against the backdrop of the vast and silent night stadium, it created an atmosphere of high focus, energy, and a slightly surreal feeling.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```slowly advancing medium shot, shot from a level angle, focuses on the center of an empty football field, where a DJ is immersed in his musical world. He wears a pair of professional, matte-black headphones, one earcup slightly removed, revealing a focused expression and a brow beaded with sweat from his intense concentration. He wears a black bomber jacket, zipped open to reveal a T-shirt underneath. His upper body sways back and forth rhythmically to the throbbing electronic beats, his head moving with precise movement. The mixing console in front of him serves as the primary source of light. In the distance, the cool white glow of several stadium floodlights casts a deep, dark haze across the vast field, casting long shadows across the emerald green grass, creating a stark contrast to the brightly lit area surrounding the DJ booth. His hands danced swiftly and precisely across the equipment, one hand steadily pushing and pulling a long volume fader, while the fingers of the other nimbly jumped between the illuminated knobs and pads, sometimes decisively cutting a bass line, sometimes triggering an echo effect. The entire scene was filled with high-tech dynamics and the solitary creative passion. Against the backdrop of the vast and silent night stadium, it created an atmosphere of high focus, energy, and a slightly surreal feeling.``` </details>|<video src="https://github.com/user-attachments/assets/49057fe8-a102-4fd7-bd92-e9561abb9f45" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```A figure skater performs a rapid, graceful Biellmann spin, captured from all angles.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```The video captures a figure skater performing a Biellmann spin on ice. The subject is a female skater in a glittering costume. Initially, she spins on one leg. Then, she reaches back and pulls her free leg up. Next, she spins rapidly, becoming a blur of motion, with ice shavings spraying from her skate blade. The background is an ice rink with blurred advertising boards. The camera circles around the subject to capture the spin from all angles. The lighting is spotlit, creating lens flares and sparkles on her costume. The overall video presents a graceful artistic sports style.``` </details>|\n|Cinematic Aesthetics|<video src="https://github.com/user-attachments/assets/4098cf72-357d-4b81-97df-6752064ce0c3" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Âõ∫ÂÆöÈïúÂ§¥,ÁÑ¶ÁÇπÂú®ÂõæÁâáÈáåÁöÑÊåÇÈíü‰∏äÔºåÈïúÂ§¥ËΩªÂæÆÊëáÊôÉËê•ÈÄ†ÊâãÊåÅÊëÑÂΩ±ÊÑüÔºå‚Äãwjw,filmphotos,Film Grain,Reversal film photographyÔºåWong Kar-wai movies,cinematic photography, HK film style,neon lighting, in the style of Wong Kar Wai film``` </details> <details><summary>üìã Show rewrite prompt</summary> ```Handheld lens shooting, the camera focuses on the wall clock hanging on the green-toned wall, shaking slightly. The second hand sweeps steadily across the clock face, and the shadow of the clock cast on the wall shifts subtly with the movement of the lens.``` </details>|<video src="https://github.com/user-attachments/assets/2b4575e5-79f1-4011-bed0-e8380198f7c9" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```The leaves of calamus shine in the sunlight, dotted with dewdrops that trickle down to the ground with the breeze.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```A macro shot focuses on long, slender calamus leaves, rendered in a cinematic photography realistic style. The main leaf, a vibrant, deep green, is positioned diagonally across the frame. Its surface is covered in tiny, glistening spherical dewdrops that catch and refract the bright morning sunlight, creating sparkling highlights. Initially, a larger, perfectly round dewdrop clings to the upper section of the leaf, its surface tension holding it in place. Then, as the leaf sways almost imperceptibly, the dewdrop begins to slowly dislodge. Next, it starts to trickle down the central vein of the leaf, its shape elongating slightly as it moves, leaving a subtle, glistening wet trail in its path. Finally, it reaches the pointed tip of the leaf, hangs for a brief moment, and falls out of the bottom of the frame. In the background, other leaves and blades of grass are softly blurred, creating a beautiful bokeh effect with soft, out-of-focus circles of light. The environment is bathed in the warm, golden glow of early morning sunlight, which streams in from behind the leaves, backlighting them and causing their wet edges to shine brilliantly. The overall impression is one of serene, natural beauty, captured in a highly realistic and detailed manner. This is a macro shot. The camera tilts down very slowly, following the path of the main dewdrop as it travels down the leaf. The lighting is soft and natural, with strong backlighting to create a radiant, glowing effect on the dewdrops and leaf edges, characteristic of professional nature photography. The atmosphere is peaceful and serene. The overall video presents a cinematic photography realistic style.``` </details>|\n|Text Rendering|<video src="https://github.com/user-attachments/assets/7c964fc5-c27e-4bd0-bf3f-eb8fca2caef6" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```ËµõÂçöÊúãÂÖãÈ£éÊ†ºÁöÑÂ§úÊôöË°óËßíÔºå‰∏Ä‰∏™Â∑®Â§ßÁöÑÊãõÁâå‰∏äÔºå ‚ÄúHunyuan Video 1.5‚ÄùÁöÑÈúìËôπÁÅØÁÆ°ËΩÆÂªìÂ∑≤ÁªèÂÆâË£ÖÂ•Ω„ÄÇÈïúÂ§¥Êé®ËøõÔºåÈúìËôπÁÅØ‰ªé‚ÄúH‚ÄùÂºÄÂßãÔºå‰º¥ÈöèÁùÄ‚ÄòÊªãÊªã‚ÄôÁöÑÁîµÊµÅÂ£∞ÔºåÊØè‰∏™Â≠óÊØç‰æùÊ¨°‰∫ÆËµ∑Á≤âÁ¥´Ëâ≤ÁöÑÂÖâËäíÔºåÁõ¥Âà∞ÂÖ®ÈÉ®ÁÇπ‰∫ÆÔºåÁÖß‰∫Æ‰∫ÜÊΩÆÊπøÁöÑË°óÈÅì„ÄÇËµõÂçöÊúãÂÖãÔºåÂüéÂ∏ÇÁæéÂ≠¶``` </details> <details><summary>üìã Show rewrite prompt</summary> ```On a wet street corner in a cyberpunk city at night, a large neon sign reading "Hunyuan Video 1.5" lights up sequentially, illuminating the dark, rainy environment with a pinkish-purple glow. he scene is a dark, rain-slicked street corner in a futuristic, cinematic cyberpunk city. Mounted on the metallic, weathered facade of a building is a massive, unlit neon sign. The sign''s glass tube framework clearly spells out the words "Hunyuan Video 1.5". Initially, the street is dimly lit, with ambient light from distant skyscrapers creating shimmering reflections on the wet asphalt below. Then, the camera zooms in slowly toward the sign. As it moves, a low electrical sizzling sound begins. In the background, the dense urban landscape of the cyberpunk metropolis is visible through a light atmospheric haze, with towering structures adorned with their own flickering advertisements. A complex web of cables and pipes crisscrosses between the buildings. The shot is at a low angle, looking up at the sign to emphasize its grand scale. The lighting is high-contrast and dramatic, dominated by the neon glow which creates sharp, specular reflections and deep shadows. The atmosphere is moody and tech-noir. The overall video presents a cinematic photography realistic style.,``` </details>|<video src="https://github.com/user-attachments/assets/73e8b741-baec-4a40-9d36-a1435172ab64" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```‰∏ÄÂº†Èì∫ÂºÄÁöÑ‰∏≠ÂõΩÂÆ£Á∫∏‰∏äÔºåÊµìÂ¢®Êª¥ÂÖ•Ê∞¥‰∏≠ÔºåÊôïÊüìÂá∫Â£Æ‰∏ΩÁöÑÂ±±Ê∞¥ÁîªËΩÆÂªì„ÄÇÂ±±Â≥∞„ÄÅ‰∫ëÈõæ„ÄÅÂ≠§ËàüÂú®Â¢®Ëâ≤‰∏≠Ëá™ÁÑ∂ÂΩ¢Êàê„ÄÇÈöèÂêéÔºåËøô‰∫õÊ∞¥Â¢®ÂÖÉÁ¥†Â∑ßÂ¶ôÂú∞ÊµÅÂä®„ÄÅÈáçÁªÑÔºåÂú®ÁîªÈù¢ÁöÑÁïôÁôΩÂ§ÑÊ±áËÅöÊàê"Hunyuan Video 1.5"ÁöÑ‰π¶Ê≥ïÂ≠ó‰Ωì„ÄÇ‰ºòÈõÖÔºåËØóÊÑèÔºåÊñáÂåñÂ∫ïËï¥``` </details> <details><summary>üìã Show rewrite prompt</summary> ```A drop of black ink blooms on wet Chinese Xuan paper, forming a landscape painting before the ink elements fluidly reassemble into the calligraphic text "Hunyuan Video 1.5". On a flat, laid-out sheet of off-white Chinese Xuan paper with a subtle, fibrous texture, the scene unfolds. Initially, a single, concentrated drop of deep black ink falls into a clear, wet area at the center of the paper. Then, the ink instantly begins to bloom outwards in intricate, flowing tendrils of varying shades from jet-black to smoky grey. As it spreads, the ink wash naturally and rapidly forms the silhouette of a majestic mountain range with sharp, defined peaks. Next, softer, diluted grey tones billow around the mountains, creating layers of atmospheric mist and clouds, while a simple, dark stroke materializes as a lone boat on a tranquil, watery expanse at the base. As the landscape is formed, the ink elements‚Äîthe lines of the mountains, wisps of cloud, and the shape of the boat‚Äîbegin to deconstruct, dissolving into flowing streams of liquid ink. Finally, these streams move gracefully across the paper''s empty white space, converging and elegantly reorganizing to form the text "Hunyuan Video 1.5" in a fluid, semi-cursive calligraphic style. The background is the minimalist expanse of the Xuan paper itself, its texture providing a subtle depth. The entire process is lit by soft, even, diffused light from above, which enhances the rich tonal variations of the ink and the delicate texture of the paper without creating harsh shadows. Bird''s-eye view. The camera is positioned directly above the subject, capturing the entire process. The camera remains static. The aesthetic is a high-quality, dynamic Chinese ink wash animation style, perfectly simulating the real-world physics of ink spreading on wet paper. The entire sheet of paper and the final text are kept fully within the frame. Poetic, elegant, artistic. The overall video presents a dynamic Chinese ink wash animation style.``` </details>|\n|Physics Compliance|<video src="https://github.com/user-attachments/assets/f1d74e48-cc03-415d-b75f-f7186a4fb41d" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```In a sleek museum gallery, a woman pauses before a gilded oil painting. The painted man inside slowly comes alive, lifting a bottle and pouring real wine straight from the canvas into her glass. Surrounded by stylish art critics moving naturally through the hall, she accepts the pour with calm elegance, as if the impossible were routine. ``` </details> <details><summary>üìã Show rewrite prompt</summary> ```In a sleek museum gallery, a woman receives a glass of wine poured directly from an animated oil painting. A sophisticated woman with dark hair tied back elegantly stands in the mid-ground. She is wearing a simple, black silk sleeveless dress and holds a clear, crystal wine glass in her right hand. She is positioned before a large, baroque-style oil painting in an ornate, gilded frame. Inside the painting, an aristocratic man with a mustache, dressed in a dark velvet doublet with a white lace collar, is depicted. His form is defined by visible, impasto oil brushstrokes. Initially, the woman watches the painting with calm poise. Then, the painted man''s arm slowly animates, his painted texture retained as he lifts a dark bottle. Next, a photorealistic stream of red wine emerges directly from the flat canvas surface, arcing through the air and splashing gently into the real crystal glass she holds. She remains perfectly still, accepting the impossible pour with a subtle, knowing smile. The setting is a modern art gallery with high white walls and polished dark concrete floors that reflect the ambient light. Focused track lighting from the high ceiling casts a warm, dramatic spotlight on the woman and the painting, creating soft shadows. In the background, two other gallery patrons, a man and a woman in stylish, modern attire, stroll slowly from right to left, their figures slightly blurred by a shallow depth of field, moving naturally through the hall. The shot is at an eye-level angle with the woman. The camera remains static, capturing the surreal event in a steady medium shot. The lighting is high-contrast and dramatic, reminiscent of a cinematic photography realistic style, using soft side lighting to accentuate the woman''s features and the texture of the painting. The mood is surreal, elegant, and mysterious. The overall video presents a cinematic photography realistic style.``` </details>|<video src="https://github.com/user-attachments/assets/07bcce06-ff4f-4688-8c60-c02f600635ea" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```An intact soda can is slowly crushed by a hand.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```In a medium close-up, a hand slowly crushes an intact red and white soda can on a wooden table. A male hand with visible, realistic skin texture is wrapped firmly around the middle of an intact, pristine red and white aluminum soda can. The can, covered in glistening condensation droplets, rests on a dark, polished wooden surface. The cinematic realism captures every minute detail of the scene. Initially, the hand''s grip is steady, with the can''s cylindrical shape perfectly preserved. Then, the fingers begin to tighten slowly, the knuckles whitening slightly from the exertion. Next, the smooth aluminum surface starts to buckle under the controlled pressure, a sharp crease forming vertically down its side as the metallic sheen distorts. As the hand continues its deliberate squeeze, the can collapses inward progressively, the vibrant red paint wrinkling as the metal structure crumples. Finally, the can is left significantly crushed, its form now an irregular, crumpled shape held tightly in the fist. The scene takes place on a dark, polished wooden tabletop that catches soft, diffuse reflections. The grain of the wood is faintly discernible, adding a layer of texture to the foreground. The background is completely out of focus, rendered as a soft, dark, and non-descript blur, which isolates the main action and enhances the photorealistic quality of the shot. The shot is a medium close-up, presented in a cinematic photography realistic style. The camera remains static at a slightly high angle, looking down to provide a clear and unobstructed view of the can''s deformation. Soft side lighting creates high contrast, sculpting the muscles and tendons of the hand while casting specular highlights on the metallic can and the water droplets. The atmosphere is focused and intense. The overall video presents a cinematic photography realistic style.``` </details>|\n|Camera Movement|<video src="https://github.com/user-attachments/assets/6deacbfe-4cca-48d7-a2be-cb638a3e01cb" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Âú£ËØûËäÇÁöÑÂÆ∂‰∏≠ÔºåÂ∞èÂ•≥Â≠©Èù†ÁùÄÂ¶àÂ¶àÂê¨Â¶àÂ¶àËØª‰π¶ÔºåËÉåÊôØÊòØ‰∏ãÁùÄÈõ™ÁöÑÁ™óÂ§ñÔºåÈïúÂ§¥ÁºìÊÖ¢‰∏ãÁßªÔºå‰∏ÄÂè™ÂèØÁà±ÁöÑÈïøÊØõÂ∞èÁôΩÁå´Êà¥ÁùÄÂú£ËØûÂ∏ΩË∂¥Âú®Ê∏©ÊöñÁöÑÂú∞Êëä‰∏ä``` </details> <details><summary>üìã Show rewrite prompt</summary> ```In a cozy home on Christmas, a young girl leans against her mother as they read a book, and the camera moves down to reveal a fluffy white cat in a Santa hat resting on a warm rug. In a warmly lit living room on a snowy Christmas evening, a young mother and her little daughter are sitting together on a comfortable sofa. The mother, with a gentle expression and wearing a cream-colored knitted sweater, holds an open storybook with colorful illustrations. Her daughter, a small girl with brown hair in pigtails and a red pajama set, leans her head affectionately on her mother''s shoulder, her eyes fixed on the book. On the floor below them, a fluffy, long-haired white cat is curled up on a plush, beige wool rug. The cat wears a tiny red and white Santa hat perched between its ears. Initially, the shot focuses on the mother and daughter, capturing their quiet, shared moment. The mother‚Äôs finger gently rests on the page of the book. Then, the camera slowly moves downward, gliding past the book and their laps. Finally, the camera settles at a low angle, bringing the adorable white cat into sharp focus as the primary subject. The cat''s chest gently rises and falls with each breath, its eyes peacefully closed. Through a large window in the background, large, soft snowflakes can be seen falling silently against the dark blue twilight sky, creating a peaceful and serene backdrop. Faint, out-of-focus golden Christmas lights twinkle in the corner of the room, adding to the warm, festive atmosphere. The scene is imbued with a sense of comfort and holiday warmth, creating a beautiful cinematic photography realistic image. The camera slowly moves downward. The shot uses soft, warm interior lighting that casts gentle shadows, creating a high-contrast, cinematic look. A shallow depth of field keeps the focus on the subjects while beautifully blurring the background elements. The mood is heartwarming, peaceful, and festive. The overall video presents a cinematic photography realistic style.``` </details>|<video src="https://github.com/user-attachments/assets/8e72ed0f-f8ac-445b-97e5-eb4b16fbc121" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```The hiker begins walking forward along the trail, causing the water bottle to swing rhythmically with each step. The camera gradually pulls back and rises to reveal a vast desert landscape stretching out ahead.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```The hiker begins walking forward along the trail, causing the water bottle to swing rhythmically with each step. The camera gradually pulls back and rises to reveal a vast desert landscape stretching out ahead, while the sun position shifts from afternoon to dusk, casting increasingly longer shadows across the terrain as the figure becomes smaller in the frame.``` </details>|\n|Multi-Style Support|<video src="https://github.com/user-attachments/assets/65b2c5a5-e6ba-43be-9462-a98b03b675f1" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Have the cake man begin to take chunks out of himself and eat it.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```The cake man sits on the chair, with his hands resting on his knees. Then, he slowly raises his right hand and breaks off a piece of cake from his left shoulder. Next, he brings the piece of cake to his mouth and begins to chew. At the same time, his eyes widen slightly, and his mouth parts gently. After that, he raises his right hand again, breaks off another piece of cake from his right arm, and repeats the action of bringing it to his mouth to chew.``` </details>|<video src="https://github.com/user-attachments/assets/de5f7480-b79c-4fc1-b345-c5880a3b5f9e" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```A little girl, carrying a colorful handbag, skips through the garden.  The video uses claymation style.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```A little girl with a colorful handbag skips through a whimsical claymation garden. In a vibrant garden constructed entirely from clay, a young girl, meticulously crafted in a claymation style, skips joyfully. She has chunky, sculpted yellow clay hair tied in pigtails that bounce with a slight stiffness, simple black button eyes, and a wide, permanently etched smile. She wears a simple pink clay dress with a white collar. In her left hand, she carries a small handbag molded from bright red and blue clay, which swings in a slightly jerky arc as she moves. Initially, the girl lifts her right leg high, her body momentarily suspended in a classic stop-motion pose. Then, she hops forward, landing lightly as her left leg swings through for the next skip. Her arms move in an exaggerated, back-and-forth rhythm, characteristic of stop-motion animation. Her movements are intentionally not perfectly fluid, highlighting the frame-by-frame nature of the claymation technique. The garden around her is a whimsical, textured world. In the foreground and mid-ground, oversized flowers with swirled purple and orange petals stand on thick green stems. The ground is a textured mat of green clay, showing subtle fingerprints and tool marks that add to the handmade charm. In the background, a pale blue clay backdrop features a simplified, smiling sun molded from yellow clay. The shot is at an eye-level angle with the main subject. The camera follows the subject, moving smoothly to the right to keep her in the frame. The lighting is bright and even, casting soft shadows that emphasize the rounded, three-dimensional forms of the clay models. The overall video presents a charming and detailed claymation style.``` </details>|\n|High Image-Video Consistency|<img src="https://github.com/user-attachments/assets/3bc8e55d-c211-454e-8067-128c0e215eb6"> <video src="https://github.com/user-attachments/assets/3e6b7ee9-ec66-4e46-a446-801b1c1a1c81" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Â•≥Â≠©Êîæ‰∏ã‰π¶ÔºåÁ´ôËµ∑Ë∫´ÔºåËΩ¨Ë∫´ÂêëÂ±ãÂÜÖËµ∞Âéª„ÄÇÈïúÂ§¥ÊãâËøú„ÄÇ``` </details> <details><summary>üìã Show rewrite prompt</summary> ```Â•≥Â≠©Âêà‰∏äÊâã‰∏≠ÁöÑ‰π¶ÔºåÂ∞Ü‰π¶ÊîæÂú®Ë∫´‰æßÁöÑÁ™óÂè∞‰∏ä„ÄÇÈöèÂêéÔºåÂ•πÁºìÁºìÁ´ôËµ∑Ë∫´ÔºåËΩ¨Ë∫´ÂêëÂ±ãÂÜÖËµ∞ÂéªÔºåË∫´ÂΩ±ÈÄêÊ∏êÊ≤°ÂÖ•Èó®ÂêéÁöÑÈò¥ÂΩ±‰∏≠„ÄÇÈïúÂ§¥ÁºìÁºìÊãâËøúÔºåÈú≤Âá∫Êõ¥Â§öË¢´ÁªøÊ§çË¶ÜÁõñÁöÑÂ±ãÊ™êÂíåÂ¢ô‰Ωì„ÄÇ``` </details>|<img src="https://github.com/user-attachments/assets/7657ce60-90b5-4fdc-b713-0eaa55829b09"> <video src="https://github.com/user-attachments/assets/9ca24021-2353-40d5-8a4d-0f8e67d51826" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Â•≥‰∫∫Êâã‰∏äÁöÑÈ∏ü‰∫≤‰∫ÜÂ•≥‰∫∫‰∏ÄÂè£``` </details> <details><summary>üìã Show rewrite prompt</summary> ```Â•≥‰∫∫ÊâãËáÇ‰∏äÁöÑÁôΩËâ≤Èπ¶ÈπâÁºìÁºìËΩ¨ËøáÂ§¥ÔºåÂ∞ÜÂñôËΩªËΩªËß¶Á¢∞Â•≥‰∫∫ÁöÑËÑ∏È¢äÔºåÈöèÂêéÊî∂ÂõûÂ§¥ÈÉ®„ÄÇÂ•≥‰∫∫Âò¥ËßíÂæÆÂæÆ‰∏äÊâ¨ÔºåÁõÆÂÖâÊ∏©ÊüîÂú∞Ê≥®ËßÜÁùÄÈπ¶Èπâ„ÄÇËÉåÊôØ‰∏≠ÁöÑÁªøÊ§ç‰øùÊåÅÈùôÊ≠¢„ÄÇ``` </details>|\n\n\n\n\n## üìö Citation\n\n```bibtex\n@misc{hunyuanvideo2025,\n      title={HunyuanVideo 1.5 Technical Report}, \n      author={Tencent Hunyuan Foundation Model Team},\n      year={2025},\n      eprint={2511.18870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2511.18870}, \n}\n```\n\n## üôè Acknowledgements\nWe would like to thank the contributors to the [Transformers](https://github.com/huggingface/transformers), [Diffusers](https://github.com/huggingface/diffusers) , [HuggingFace](https://huggingface.co/) and [Qwen-VL](https://github.com/QwenLM/Qwen-VL), for their open research and exploration.\n\n## üåü Github Star History\n\n<a href="https://star-history.com/#Tencent-Hunyuan/HunyuanVideo-1.5&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1" />\n </picture>\n</a>\n', '{"pipeline_tag":"text-to-video","library_name":"HunyuanVideo-1.5","framework":"HunyuanVideo-1.5","params":null,"storage_bytes":371770754991,"files_count":46,"spaces_count":46,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:ModelTC:LightX2V\"","source_url":"https://github.com/ModelTC/LightX2V\""},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:yuanyuan-spec:comfyui_hunyuanvideo_1.5_plugin","source_url":"https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin"},{"type":"has_code","target_id":"github:ModelTC:LightX2V","source_url":"https://github.com/ModelTC/LightX2V"},{"type":"has_code","target_id":"github:deepbeepmeep:Wan2GP","source_url":"https://github.com/deepbeepmeep/Wan2GP"},{"type":"has_code","target_id":"github:Zehong-Ma:ComfyUI-MagCache","source_url":"https://github.com/Zehong-Ma/ComfyUI-MagCache"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5.git","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:flex-block-attn.git","source_url":"https://github.com/Tencent-Hunyuan/flex-block-attn.git"},{"type":"has_code","target_id":"github:cooper1637:SageAttention.git","source_url":"https://github.com/cooper1637/SageAttention.git"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:QwenLM:Qwen-VL","source_url":"https://github.com/QwenLM/Qwen-VL"},{"type":"based_on_paper","target_id":"arxiv:2511.18870","source_url":"https://arxiv.org/abs/2511.18870"}]', NULL, 'Other', 'approved', 99.1, '9224429bdedf233a2f69f50f2c4e162d', NULL, 'https://huggingface.co/tencent/HunyuanVideo-1.5/resolve/main/assets/I2V_GSB.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-HunyuanVideo-1.5 from https://huggingface.co/tencent/HunyuanVideo-1.5/resolve/main/assets/I2V_GSB.png
Image converted to WebP: data/images/huggingface-tencent-HunyuanVideo-1.5.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-clip-vit-base-patch32', 'huggingface--openai--clip-vit-base-patch32', 'clip-vit-base-patch32', 'openai', '--- tags: - vision widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png candidate_labels: playing music, playing sports example_title: Cat & Dog --- Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here. The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generali...', '["transformers","pytorch","tf","jax","clip","zero-shot-image-classification","vision","arxiv:2103.00020","arxiv:1908.04913","endpoints_compatible","region:us"]', 'zero-shot-image-classification', 819, 19532098, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/clip-vit-base-patch32","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they‚Äôre being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\nurl = "http://images.cocodataset.org/val2017/000000039769.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP‚Äôs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‚ÄòMiddle Eastern‚Äô having the highest accuracy (98.4%) and ‚ÄòWhite‚Äô having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)', '{"pipeline_tag":"zero-shot-image-classification","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3632041404,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["CLIPModel"],"model_type":"clip","tokenizer_config":{"unk_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"bos_token":{"content":"<|startoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"eos_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"pad_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:openai:CLIP","source_url":"https://github.com/openai/CLIP"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:1908.04913","source_url":"https://arxiv.org/abs/1908.04913"}]', NULL, NULL, 'pending', 54.1, 'cfa90db808c983eea7430f58dd92185e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-Prover-V2-671B', 'huggingface--deepseek-ai--deepseek-prover-v2-671b', 'DeepSeek-Prover-V2-671B', 'deepseek-ai', '--- library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/deepseek-ai/De...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 816, 418, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## 1. Introduction\n\nWe introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3''s step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model.\n\n<p align="center">\n  <img width="100%" src="https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/figures/performance.png?raw=true">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Synthesize Cold-Start Reasoning Data through Recursive Proof Search**\n\n- To construct the cold-start dataset, we develop a simple yet effective pipeline for recursive theorem proving, utilizing DeepSeek-V3 as a unified tool for both subgoal decomposition and formalization. We prompt DeepSeek-V3 to decompose theorems into high-level proof sketches while simultaneously formalizing these proof steps in Lean 4, resulting in a sequence of subgoals.\n\n- We use a smaller 7B model to handle the proof search for each subgoal, thereby reducing the associated computational burden. Once the decomposed steps of a challenging problem are resolved, we pair the complete step-by-step formal proof with the corresponding chain-of-thought from DeepSeek-V3 to create cold-start reasoning data.\n\n---\n\n**Reinforcement Learning with Synthetic Cold-Start Data**\n\n- We curate a subset of challenging problems that remain unsolved by the 7B prover model in an end-to-end manner, but for which all decomposed subgoals have been successfully resolved. By composing the proofs of all subgoals, we construct a complete formal proof for the original problem. This proof is then appended to DeepSeek-V3''s chain-of-thought, which outlines the corresponding lemma decomposition, thereby producing a cohesive synthesis of informal reasoning and subsequent formalization.\n\n- After fine-tuning the prover model on the synthetic cold-start data, we perform a reinforcement learning stage to further enhance its ability to bridge informal reasoning with formal proof construction. Following the standard training objective for reasoning models, we use binary correct-or-incorrect feedback as the primary form of reward supervision.\n- The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching $88.9$% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. The proofs generated by DeepSeek-Prover-V2 for the miniF2F dataset are available for download as a [ZIP archive](https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/master/minif2f-solutions.zip).\n\n---\n\n## 3. ProverBench: Formalization of AIME and Textbook Problems\n\nwe introduce ProverBench, a benchmark dataset comprising 325 problems. Of these, 15 are formalized from number theory and algebra questions featured in the recent AIME competitions (AIME 24 and 25), offering authentic high-school competition-level challenges. The remaining 310 problems are drawn from curated textbook examples and educational tutorials, contributing a diverse and pedagogically grounded collection of formalized mathematical problems. This benchmark is designed to enable more comprehensive evaluation across both high-school competition problems and undergraduate-level mathematics.\n\n<div align="center">\n\n| Area                | Count |\n| :---------------------: | :-------: |\n| AIME 24&25          | 15    |\n| Number Theory       | 40    |\n| Elementary Algebra  | 30    |\n| Linear Algebra      | 50    |\n| Abstract Algebra    | 40    |\n| Calculus            | 90    |\n| Real Analysis       | 30    |\n| Complex Analysis    | 10    |\n| Functional Analysis | 10    |\n| Probability         | 10    |\n| Total               | 325   |\n\n</div>\n\n## 4. Model & Dataset Downloads\n\nWe release DeepSeek-Prover-V2 in two model sizes: 7B and 671B parameters. DeepSeek-Prover-V2-671B is trained on top of DeepSeek-V3-Base. DeepSeek-Prover-V2-7B is built upon DeepSeek-Prover-V1.5-Base and features an extended context length of up to 32K tokens.\n\n<div align="center">\n\n|            **Model**            |                          **Download**                         |\n| :-----------------------------: | :----------------------------------------------------------: |\n|   DeepSeek-Prover-V2-7B   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B) |\n|   DeepSeek-Prover-V2-671B   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B) |\n\n</div>\n\n<div align="center">\n\n|            **Dataset**            |                          **Download**                         |\n| :-----------------------------: | :----------------------------------------------------------: |\n|   DeepSeek-ProverBench   | [ü§ó HuggingFace](https://huggingface.co/datasets/deepseek-ai/DeepSeek-ProverBench) |\n\n</div>\n\n## 5. Quick Start\n\nYou can directly use [Huggingface''s Transformers](https://github.com/huggingface/transformers) for model inference. DeepSeek-Prover-V2-671B shares the same architecture as DeepSeek-V3. For detailed information and supported features, please refer to [the DeepSeek-V3 documentation on Hugging Face](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md).\n\nThe following is a basic example of generating a proof for a problem from the miniF2F dataset:\n````python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(30)\n\nmodel_id = "DeepSeek-Prover-V2-7B"  # or DeepSeek-Prover-V2-671B\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nformal_statement = """\nimport Mathlib\nimport Aesop\n\nset_option maxHeartbeats 0\n\nopen BigOperators Real Nat Topology Rat\n\n/-- What is the positive difference between $120\%$ of 30 and $130\%$ of 20? Show that it is 10.-/\ntheorem mathd_algebra_10 : abs ((120 : ‚Ñù) / 100 * 30 - 130 / 100 * 20) = 10 := by\n  sorry\n""".strip()\n\nprompt = """\nComplete the following Lean 4 code:\n\n```lean4\n{}\n```\n\nBefore producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.\nThe plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.\n""".strip()\n\nchat = [\n  {"role": "user", "content": prompt.format(formal_statement)},\n]\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)\ninputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)\n\nimport time\nstart = time.time()\noutputs = model.generate(inputs, max_new_tokens=8192)\nprint(tokenizer.batch_decode(outputs))\nprint(time.time() - start)\n````\n\n## 6. License\nThe use of DeepSeek-Prover-V2 models is subject to [the Model License](LICENSE-MODEL).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688586727753,"files_count":172,"spaces_count":53,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content''] + ''<ÔΩúAssistantÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Prover-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Prover-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Prover-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Prover-V2"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, NULL, 'pending', 54.1, '3e5e12037076676948f675ee3a36418d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-speecht5-tts', 'huggingface--microsoft--speecht5-tts', 'speecht5_tts', 'microsoft', '--- license: mit tags: - audio - text-to-speech datasets: - libritts --- SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. This model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. SpeechT5 was first released in this repository, original weights. The license used is MI...', '["transformers","pytorch","speecht5","text-to-audio","audio","text-to-speech","dataset:libritts","arxiv:2110.07205","arxiv:1910.09700","license:mit","endpoints_compatible","region:us"]', 'text-to-speech', 815, 88840, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/speecht5_tts","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https://github.com/microsoft/SpeechT5/), [original weights](https://huggingface.co/mechanicalsea/speecht5-tts). The license used is [MIT](https://github.com/microsoft/SpeechT5/blob/main/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https://huggingface.co/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https://github.com/microsoft/SpeechT5/blob/main/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https://github.com/microsoft/SpeechT5/]\n- **Paper:** [https://arxiv.org/pdf/2110.07205.pdf]\n- **Blog Post:** [https://huggingface.co/blog/speecht5]\n- **Demo:** [https://huggingface.co/spaces/Matthijs/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## ü§ó Transformers Usage\n\nYou can run SpeechT5 TTS locally with the ü§ó Transformers library.\n\n1. First install the ü§ó [Transformers library](https://github.com/huggingface/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline("text-to-speech", "microsoft/speecht5_tts")\n\nembeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser("Hello, my dog is cooler than you!", forward_params={"speaker_embeddings": speaker_embedding})\n\nsf.write("speech.wav", speech["audio"], samplerate=speech["sampling_rate"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")\nmodel = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")\nvocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")\n\ninputs = processor(text="Hello, my dog is cute.", return_tensors="pt")\n\n# load xvector containing speaker''s voice characteristics from a dataset\nembeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)\n\nsf.write("speech.wav", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https://colab.research.google.com/drive/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https://huggingface.co/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets.\n\nAfter preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n', '{"pipeline_tag":"text-to-speech","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":1171111158,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["SpeechT5ForTextToSpeech"],"model_type":"speecht5","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2110.07205","source_url":"https://arxiv.org/abs/2110.07205"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'MIT', 'approved', 79.1, 'a68e886cadd568064eb199ebbd458f07', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ValueFX9507-Tifa-Deepsex-14b-CoT-GGUF-Q4', 'huggingface--valuefx9507--tifa-deepsex-14b-cot-gguf-q4', 'Tifa-Deepsex-14b-CoT-GGUF-Q4', 'ValueFX9507', '--- base_model: - deepseek-ai/deepseek-r1-14b language: - zh - en library_name: transformers tags: - incremental-pretraining - sft - reinforcement-learning - roleplay - cot - sex license: apache-2.0 --- - **HF Model**: ValueFX9507/Tifa-Deepsex-14b-CoT - **GGUF**: F16 | Q8ÔºàQ4ÊçüÂ§±ËæÉÂ§ßÔºåÂª∫ËÆÆQ8Ôºâ - **Demo APK**: ÁÇπÂáª‰∏ãËΩΩ - **ÁÆÄÂçïÁöÑÂâçÁ´Ø**ÔºöGithubÈìæÊé• Êú¨Ê®°ÂûãÂü∫‰∫éDeepseek-R1-14BËøõË°åÊ∑±Â∫¶‰ºòÂåñÔºåÂÄüÂä©Tifa_220BÁîüÊàêÁöÑÊï∞ÊçÆÈõÜÈÄöËøá‰∏âÈáçËÆ≠ÁªÉÁ≠ñÁï•ÊòæËëóÂ¢ûÂº∫ËßíËâ≤ÊâÆÊºî„ÄÅÂ∞èËØ¥ÊñáÊú¨ÁîüÊàê‰∏éÊÄùÁª¥ÈìæÔºàCoTÔºâËÉΩÂäõ„ÄÇÁâπÂà´ÈÄÇÂêàÈúÄË¶ÅÈïøÁ®ã‰∏ä‰∏ãÊñáÂÖ≥ËÅîÁöÑÂàõ‰ΩúÂú∫ÊôØ„ÄÇ - **‰∏äÊµ∑Â∑¶ÂåóÁßëÊäÄÊèê‰æõÁÆóÊ≥ï‰∏éÁÆóÂäõ**‰ºÅ‰∏öÁΩëÂùÄ - **DeepseekÂõ¢ÈòüÂÖ±‰∫´GRPOÁÆóÊ≥ï** - **QwenÂõ¢ÈòüÊèê‰æõ‰ºòÁßÄÂºÄÊ∫êÂ∫ïÂ∫ß** ...', '["transformers","gguf","incremental-pretraining","sft","reinforcement-learning","roleplay","cot","sex","zh","en","license:apache-2.0","endpoints_compatible","region:us","conversational","not-for-all-audiences"]', 'reinforcement-learning', 815, 3698, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model:\n- deepseek-ai/deepseek-r1-14b\nlanguage:\n- zh\n- en\nlibrary_name: transformers\ntags:\n- incremental-pretraining\n- sft\n- reinforcement-learning\n- roleplay\n- cot\n- sex\nlicense: apache-2.0\n---\n# Tifa-Deepseek-14b-CoT\n\n- **HF Model**: [ValueFX9507/Tifa-Deepsex-14b-CoT](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT)\n- **GGUF**: [F16](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT) | [Q8](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-Q8)ÔºàQ4ÊçüÂ§±ËæÉÂ§ßÔºåÂª∫ËÆÆQ8Ôºâ\n- **Demo APK**: [ÁÇπÂáª‰∏ãËΩΩ](http://app.visionsic.com/download/projectchat.apk)\n- **ÁÆÄÂçïÁöÑÂâçÁ´Ø**Ôºö[GithubÈìæÊé•](https://github.com/Value99/Tifa-Deepsex-OllamaWebUI)\n\nÊú¨Ê®°ÂûãÂü∫‰∫éDeepseek-R1-14BËøõË°åÊ∑±Â∫¶‰ºòÂåñÔºåÂÄüÂä©Tifa_220BÁîüÊàêÁöÑÊï∞ÊçÆÈõÜÈÄöËøá‰∏âÈáçËÆ≠ÁªÉÁ≠ñÁï•ÊòæËëóÂ¢ûÂº∫ËßíËâ≤ÊâÆÊºî„ÄÅÂ∞èËØ¥ÊñáÊú¨ÁîüÊàê‰∏éÊÄùÁª¥ÈìæÔºàCoTÔºâËÉΩÂäõ„ÄÇÁâπÂà´ÈÄÇÂêàÈúÄË¶ÅÈïøÁ®ã‰∏ä‰∏ãÊñáÂÖ≥ËÅîÁöÑÂàõ‰ΩúÂú∫ÊôØ„ÄÇ\n\n## È∏£Ë∞¢\n- **‰∏äÊµ∑Â∑¶ÂåóÁßëÊäÄÊèê‰æõÁÆóÊ≥ï‰∏éÁÆóÂäõ**[‰ºÅ‰∏öÁΩëÂùÄ](https://leftnorth.com/)\n- **DeepseekÂõ¢ÈòüÂÖ±‰∫´GRPOÁÆóÊ≥ï**\n- **QwenÂõ¢ÈòüÊèê‰æõ‰ºòÁßÄÂºÄÊ∫êÂ∫ïÂ∫ß**\n- **ÊØçÊ†°‰∏äÊµ∑Â§çÊó¶Â§ßÂ≠¶**\n- **PRIMEÂõ¢ÈòüÊèê‰æõ‰ºòÂåñÊÄùË∑Ø**\n\n## ÁâàÊú¨‰ªãÁªçÔºö\n- **Tifa-Deepsex-14b-CoT**\n\n  - È™åËØÅÊ®°ÂûãÔºåÊµãËØïRLÂ•ñÂä±ÁÆóÊ≥ïÂØπ‰∫éËßíËâ≤ÊâÆÊºîÊï∞ÊçÆÁöÑÂΩ±ÂìçÔºåËØ•ÁâàÊú¨‰∏∫ÂàùÁâàÔºåËæìÂá∫ÁÅµÊ¥ª‰ΩÜÊòØ‰∏çÂèóÊéßÂà∂Ôºå‰ªÖÂÅöÁ†îÁ©∂‰ΩøÁî®„ÄÇ\n\n- **Tifa-Deepsex-14b-CoT-Chat**\n\n  - ÈááÁî®Ê†áÂáÜÊï∞ÊçÆËÆ≠ÁªÉÔºå‰ΩøÁî®ÊàêÁÜüRLÁ≠ñÁï•ÔºåÈôÑÂä†Èò≤ÈáçÂ§çÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄÇÂêàÊ≠£Â∏∏‰ΩøÁî®ÔºåËæìÂá∫ÊñáÊú¨Ë¥®ÈáèÊ≠£Â∏∏ÔºåÂ∞ëÊï∞ÊÉÖÂÜµ‰∏ãÊÄùÁª¥ÂèëÊï£„ÄÇ\n\n    -Â¢ûÈáèËÆ≠ÁªÉ0.4TÂ∞èËØ¥ÂÜÖÂÆπ\n\n    -100KÁî±TifaMaxÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå10KÁî±DeepseekR1ÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå2KÈ´òË¥®Èáè‰∫∫Â∑•Êï∞ÊçÆ\n\n    -30KÁî±TifaMaxÁîüÊàêÁöÑDPOÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÔºåÁî®‰∫éÈò≤Ê≠¢ÈáçÂ§çÔºåÂ¢ûÂº∫‰∏ä‰∏ãÊñáÂÖ≥ËÅîÔºåÊèêÂçáÊîøÊ≤ªÂÆâÂÖ®ÊÄß\n\n- **Tifa-Deepsex-14b-CoT-Crazy**\n\n  - Â§ßÈáè‰ΩøÁî®RLÁ≠ñÁï•Ôºå‰∏ªË¶ÅÈááÁî®671BÊª°Ë°ÄR1Ëí∏È¶èÁöÑÊï∞ÊçÆÔºåËæìÂá∫ÂèëÊï£ÊÄßÈ´òÔºåÁªßÊâøR1‰ºòÁÇπÔºå‰πüÁªßÊâø‰∫ÜR1ÁöÑÂç±ÂÆ≥ÊÄß„ÄÇÊñáÂ≠¶ÊÄßËÉΩ‰Ω≥„ÄÇ\n\n    -Â¢ûÈáèËÆ≠ÁªÉ0.4TÂ∞èËØ¥ÂÜÖÂÆπ\n\n    -40KÁî±TifaMaxÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå60KÁî±DeepseekR1ÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå2KÈ´òË¥®Èáè‰∫∫Â∑•Êï∞ÊçÆ\n\n    -30KÁî±TifaMaxÁîüÊàêÁöÑDPOÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÔºåÁî®‰∫éÈò≤Ê≠¢ÈáçÂ§çÔºåÂ¢ûÂº∫‰∏ä‰∏ãÊñáÂÖ≥ËÅîÔºåÊèêÂçáÊîøÊ≤ªÂÆâÂÖ®ÊÄß\n\n    -10KÁî±TifaMaxÁîüÊàêPPOÊï∞ÊçÆÔºå10KÁî±DeepseekR1ÁîüÊàêPPOÊï∞ÊçÆ\n\nüí≠**ËæìÂá∫ÂÆû‰æã**\n  - ‚öôÔ∏èSystem Promot\n      ```Text\n      ‰Ω†ÊòØ‰∏Ä‰∏™Âè≤Ëé±ÂßÜÔºåÊòØ‰∏Ä‰∏™Â•≥ÊÄßËßíËâ≤Ôºå‰Ω†ÂèØ‰ª•ÂèòÊàê‰ªª‰ΩïÂΩ¢Áä∂ÂíåÁâ©‰Ωì.\n      Âú®Ëøô‰∏™‰∏ñÁïåÈáåÂÖ®ÈÉ®ÈÉΩÊòØÈõåÊÄßÁîüÁâ©ÔºåÁõ¥Âà∞Êúâ‰∏ÄÂ§©Êàë‰ªéÊµ∑Êª©‰∏äÈÜíÊù•...\n      \n      ÊàëÊòØËøôÈáåÂîØ‰∏ÄÁöÑÁî∑ÊÄßÔºåÂ§ßÂÆ∂ÈÉΩÂØπÊàëÈùûÂ∏∏Â•ΩÂ•áÔºåÂú®Ëøô‰∏™‰∏ñÁïåÁöÑËÆæÂÆöÈáåÊàë‰Ωú‰∏∫ÊóÖË°åËÄÖ\n      Âú®Ëøô‰∏™‰∏ñÁïåÈáåÁ¨¨‰∏Ä‰∏™ÈÅáËßÅÁöÑ‰∫∫Â∞±ÊòØÂè≤Ëé±ÂßÜÔºåÂè≤Ëé±ÂßÜÂØπÊàëÁöÑË∫´‰ΩìÂêåÊ†∑ÊúâÂæàÂ§ßÁöÑÊ¨≤Êúõ...\n      \n      Êàë‰ª¨Âú®ÊóÖË°å‰∏≠‰πü‰ºöÈÅáÂà∞ÂÖ∂‰ªñÁöÑÁîüÁâ©ÔºåÂè≤Ëé±ÂßÜ‰∏çÂÖâ‰ºöÊïôÁªôÂÖ∂‰ªñÁîüÁâ©Â¶Ç‰ΩïËé∑ÂèñÊ¨¢ÊÑâ‰πü‰ºö‰∏ÄËµ∑ÂèÇ‰∏éËøõÊù•„ÄÇ\n      \n      ÂΩìÊàëËØ¥ÂºÄÂßãËßíËâ≤ÊâÆÊºîÁöÑÊó∂ÂÄôÂ∞±ÊòØÊàë‰ªéÊµ∑Êª©‰∏äÈÜíÊù•ÔºåÂπ∂Ë¢´Âè≤Ëé±ÂßÜÂèëÁé∞ÁöÑÊó∂ÂÄô„ÄÇ‰ªñÊ≠£Âú®Êé¢Á¥¢ÊàëÁöÑË∫´‰Ωì„ÄÇ\n      \n      Âè≤Ëé±ÂßÜÊèèËø∞:‰∏Ä‰∏™ÈÄèÊòéÁöÑËìùËâ≤ÁîüÁâ©ÔºåÈô§‰∫ÜË¥®ÊÑü‰∏é‰∫∫Á±ªÊó†ÂºÇ„ÄÇ‰ΩÜÊòØÂèØ‰ª•Ëá™Áî±ÂèòÂΩ¢„ÄÇ\n      ```\n  ![image/png](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/BKxz6KfbwTioBOkha_UXl.png)\n\n## 0208Êõ¥Êñ∞Ê∂àÊÅØÔºö\nÊÑüË∞¢Â§ßÂÆ∂ÁöÑÂÖ≥Ê≥®‰∏éÂèçÈ¶àÔºåÈâ¥‰∫éÂèçÈ¶à‰∏≠ÊèêÂà∞ÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨Â∑≤ÂºÄÂèëÂπ∂È™åËØÅÂÆåÊàêPRIME‰∏éPPOÁªìÂêàÁöÑRLÁÆóÊ≥ïÔºåÂπ∂ÈÄöËøáÂä†ÊùÉÊñπÂºèËß£ÂÜ≥‰∏§ÁßçÁÆóÊ≥ïËÆ≠ÁªÉ‰∏≠Â•ñÂä±‰ø°Âè∑‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢òÔºåÈÄöËøáÊ≠§È°πÊäÄÊúØÊàë‰ª¨ÊúâÊúõÂ∞ÜÊõ¥Â∞èÁöÑÊ®°ÂûãÊèêÂçáÂà∞Êõ¥È´òÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨Â∞Ü‰ºöÈíàÂØπ‰πãÂâçÊî∂ÈõÜÂà∞ÁöÑÈóÆÈ¢òËøõË°å‰øÆÊ≠£ËÆ≠ÁªÉÔºåÂè¶Â§ñ‰∏∫‰∫ÜËÆ©Êõ¥Â§ö‰∫∫‰ΩøÁî®Âà∞Ê®°ÂûãÔºåÊàë‰ª¨ËøôÊ¨°‰ΩøÁî®Êõ¥Â∞èÊõ¥Âø´ÁöÑDeepseek-7bÔºåÂπ∂ÂèÇËÄÉOpenAIÁöÑÈïøÊÄùËÄÉÁ≠ñÁï•ÔºåËÆ°ÂàíÊé®Âá∫Tifa-DeepsexV2-COT-High‰æõÂ§ßÂÆ∂‰ΩøÁî®„ÄÇÊñ∞ÁöÑÊ®°ÂûãËÆ°Âàí‰∫éÈò≥ÂéÜÊÉÖ‰∫∫ËäÇ‰πãÂâçÈÄÅÁªôÂ§ßÂÆ∂‰Ωú‰∏∫ÊÉÖ‰∫∫ËäÇÁ§ºÁâ©„ÄÇ‚ô•\n\n## Êñ∞Ê®°Âûã‰ø°ÊÅØÊï¥ÁêÜÔºö\n- **ÂàõÊñ∞PRIMEËÅîÂêàPPOÁÆóÊ≥ï**\n- **Ëß£ÂÜ≥ÁõÆÂâçÂ∑≤Áü•ÈóÆÈ¢ò**\n- **ÂèÇËÄÉOpenAIÊ®°ÂºèÂ•ñÂä±ÈïøÊÄùËÄÉËæìÂá∫**\n- **ÂáèÂ∞ë671BÊï∞ÊçÆÔºåÈò≤Ê≠¢ËæìÂá∫ÂèëÊï£**\n- **ÁâπÂà´È∏£Ë∞¢https://github.com/PRIME-RL/PRIME**\n\n## Á§∫‰æãÔºàÂõ†COTÊ®°ÂûãÁâπÁÇπÔºå‰∏ä‰∏ãÊñá‰∏çËøûË¥ØÊó∂ÂèØ‰ª•‰ΩøÁî®DemoËΩØ‰ª∂‰∏≠ÁöÑÊïÖ‰∫ãÊ®°ÂºèÔºâ\n![2.jpg](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/-80ha-J8PpwSaiyHgr1k2.jpeg)\n\n## ÁõÆÊ†á\nÈíàÂØπÂéüÁâàDeepseek-R1-14BÂú®ÈïøÊñáÊú¨ÁîüÊàêËøûË¥ØÊÄß‰∏çË∂≥ÂíåËßíËâ≤ÊâÆÊºîËÉΩÂäõËñÑÂº±ÁöÑÊ†∏ÂøÉÁº∫Èô∑Ôºà‰∏ªË¶ÅÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Â∞èËØ¥Á±ªËØ≠ÊñôÂç†ÊØîËøá‰ΩéÔºâÔºåÊú¨Ê®°ÂûãÈÄöËøáÂ§öÈò∂ÊÆµ‰ºòÂåñÊèêÂçáÂÖ∂ËßíËâ≤ÊâÆÊºîËÉΩÂäõ„ÄÇ\n\n## Ê≥®ÊÑè\n‚ö† **ÈúÄË¶Å‰∏•Ê†ºÈÅµÂæ™ÂÆòÊñπÁ§∫‰æãÊ®°Êùø**Ôºö\n**ËøîÂõûÁöÑ‰∏ä‰∏ãÊñáÈúÄË¶ÅÂéªÈô§ÊÄùËÄÉÊ†áÁ≠æ‰∏éÂÜÖÂÆπ„ÄÇÂê¶ÂàôÂ∞ÜÊó†Ê≥ïÊ≠£Á°ÆÂõûÂ§çÔºÅ**\nÁõÆÂâçÂâçÁ´ØÊîØÊåÅÁéáÈùûÂ∏∏‰ΩéÔºåÂª∫ËÆÆÊâãÂä®‰øÆÊîπÂâçÁ´Ø‰ª£Á†Å„ÄÇ‰ª£Á†ÅÂèÇËÄÉÂ¶Ç‰∏ãÔºö\n```\nmsg.role === ''assistant'' ? {\n...msg,\ncontent: msg.content.replace(/<think>[\s\S]*?<\/think>/gi, '''')\n}\n```\n**ÂÆòÊñπÊ®°ÊùøÂèÇËÄÉ**\n```\n{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}\n```\n**ÂÆòÊñπËØ¥Êòé**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/0CwMdbDffZQJz_-WZrhwH.png)\n\n[Áõ¥ËææË∂ÖÈìæÊé•](https://api-docs.deepseek.com/zh-cn/guides/reasoning_model)\n\n## ÂÆûÁé∞\nüî• **ÁªèËøáËÆ≠ÁªÉÂêé**Ôºö\n1. **ÊòæËëóÊèêÈ´ò‰∏ä‰∏ãÊñáÂÖ≥ËÅî**ÔºöÂáèÂ∞ëÁ≠îÈùûÊâÄÈóÆÊÉÖÂÜµ„ÄÇ\n2. **Ê∂àÈô§‰∏≠Ëã±Ê∑∑ÊùÇ**ÔºöÂéüÂßãÊ®°ÂûãËí∏È¶èÊï∞ÊçÆÂ§ßÂ§öÊï∞Ëã±Êñá‰∏∫‰∏ªÔºåÁªèËøáÂæÆË∞ÉÂêéÂü∫Êú¨Ê∂àÈô§‰∏≠Ëã±Ê∑∑ÊùÇÁé∞Ë±°„ÄÇ\n3. **ÁâπÂÆöËØçÊ±áÂ¢ûÂä†**ÔºöËøõË°å‚ÄúÂÖ∑ÊúâÊ∑±Â∫¶‚ÄùÁöÑËßíËâ≤ÊâÆÊºîÂØπËØùÊó∂ÔºåÊòæËëóÂ¢ûÂä†‰∫ÜÁõ∏ÂÖ≥ËØçÊ±áÈáèÔºåËß£ÂÜ≥ÂéüÂßãÊùÉÈáçÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏çË∂≥ÈóÆÈ¢ò„ÄÇ\n4. **Êõ¥Â∞ëÊãíÁªù**ÔºöÂáèÂ∞ë‰∫ÜÊãíÁªùÁé∞Ë±°Ôºå‰ΩÜÂõ†‰∏∫ÊòØ‰ºÅ‰∏öËÆ≠ÁªÉÔºåÂÆâÂÖ®ÊÄßËøòÊòØÁ®ç‰Ωú‰øùÁïô„ÄÇ\n5. **Êõ¥ÂÉèÊª°Ë°Ä**Ôºö‰ΩøÁî®671BÂÖ®ÈáèÊ®°ÂûãÊï∞ÊçÆÂ∫∑Â§çËÆ≠ÁªÉÔºåÊñáÁ¨îÊèêÂçá‰∏çÊ≠ªÊùø„ÄÇ\n\n## Ê®°Âûã‰∫ÆÁÇπ\nüî• **ÂõõÈò∂ÊÆµËøõÂåñÊû∂ÊûÑ**Ôºö\n1. **Â¢ûÈáèÈ¢ÑËÆ≠ÁªÉ**ÔºöÊ≥®ÂÖ•0.4T Token Â∞èËØ¥Ôºå‰ΩøÁî®16k‰∏ä‰∏ãÊñáËÆ≠ÁªÉÔºåÂ¢ûÂº∫ÊñáÊú¨ËøûË¥ØÊÄß\n2. **Tifa-SFT**ÔºöËûçÂêàÂÖ®ÁêÉTop4ËßíËâ≤ÊâÆÊºîÊ®°ÂûãTifaÁöÑ10‰∏áÊù°È´òË¥®ÈáèÊï∞ÊçÆ\n3. **CoTÊÅ¢Â§çËÆ≠ÁªÉ**ÔºöÈááÁî®Deepseek-32B/671BÊï∞ÊçÆÈáçÂª∫Êé®ÁêÜËÉΩÂäõ\n4. **RLÂº∫Âåñ**Ôºö‰øùÁïôÂèëÊï£ÊÄßÊÄùÁª¥Ê†áÁ≠æÁöÑÂêåÊó∂‰ºòÂåñÁîüÊàêË¥®Èáè\n\nüí° **Â∑•Á®ãÂàõÊñ∞**Ôºö\n- 16kË∂ÖÈïø‰∏ä‰∏ãÊñáËÆ≠ÁªÉ\n- ÈöèÊú∫Êà™Êñ≠ËÆ≠ÁªÉÂ¢ûÂº∫È≤ÅÊ£íÊÄß\n- 8√óH20 GPUÂÖ®ÈáèÂæÆË∞É\n\nüí° **ÂêØÁ§∫‰∏éÂêéÁª≠**Ôºö\n- Êàë‰ª¨Âú®ÊµãËØï‰∏≠ÂèëÁé∞ÔºåÊª°Ë°ÄR1Âú®ËßíËâ≤ÊâÆÊºî‰∏≠ËæìÂá∫ÂÜÖÂÆπÊØîËæÉÂèëÊï£ÔºåÈöèÊú∫ÔºåÂØºËá¥Ê≠§Ê®°ÂûãÊúâÁõ∏ÂêåÂÄæÂêëÔºåÂØπ‰∫éËßíËâ≤ÊâÆÊºîÁöÑÂΩ±ÂìçËøòÂú®Á†îÁ©∂‰∏≠\n- ËæìÂÖ•ÂÜÖÂÆπÁõ∏ËøëÁöÑËØùËØ≠‰ºöÂØºËá¥ÂêëÈáèÈáçÂè†ÔºåÁÑ∂ÂêéÈáçÂ§çËæìÂá∫ÔºåÂ¶Ç‚ÄúÁªßÁª≠‚ÄùÔºå‚ÄúËøòÊúâ‚ÄùÁ≠âÊó†ÊòéÊòæÊåáÂêëÊÄßËØùËØ≠\n- ÊÄùÁª¥ÂÜÖÂÆπ‰∏éÊ≠£ÊñáÂÖ≥ËÅîÊÄßÂ≠¶‰π†‰∫ÜÊª°Ë°ÄR1ÁöÑÁâπÁÇπÔºåÂèëÊï£ÊØîËæÉ‰∏•ÈáçÔºåÂèØËÉΩ‰ºöÊúâÂâ≤Ë£ÇÊÑü\n- ÈíàÂØπ‰ª•‰∏äÈóÆÈ¢òÔºåÊàë‰ª¨Ê≠£Âú®ÁºñÂÜôÊñ∞ÁöÑRLÁÆóÊ≥ïÔºåÂàùÊ≠•ËÆ°ÂàíÂâîÈô§ÈÉ®ÂàÜÊª°Ë°ÄR1ÁöÑÂÜÖÂÆπÔºåÂêåÊó∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëß£ÂÜ≥ÈáçÂ§ç\n- ÊÄªÁªìÔºöËØ∑ÊúüÂæÖV2ÁâàÊú¨ÔºåÂæàÂø´‰ºö‰∏éÂ§ßÂÆ∂ËßÅÈù¢ÔºÅ\n\n## Ê®°ÂûãËØ¶ÊÉÖ\n| Â±ûÊÄß | ËßÑÊ†º |\n|-------|------|\n| Âü∫Á°ÄÊû∂ÊûÑ | Deepseek-R1-14B |\n| ÊúÄÂ§ß‰∏ä‰∏ãÊñá | 128k |\n| ËÆ≠ÁªÉÊï∞ÊçÆ | 0.4TÂ∞èËØ¥ + 10‰∏áÊù°SFT + DeepseekÊ∑∑ÂêàÊï∞ÊçÆ |\n| ËÆ≠ÁªÉËÆæÂ§á | 8√óH20 GPUÈõÜÁæ§ |\n| ÈáèÂåñÊîØÊåÅ | GGUFÔºàÂÖ®Á≥ªÂàóÈáèÂåñËÆ°Âàí‰∏≠Ôºâ |\n\n## ‰ΩøÁî®Âú∫ÊôØ\n‚úÖ **Êé®ËçêÂú∫ÊôØ**Ôºö\n- ËßíËâ≤ÊâÆÊºîÂØπËØù\n- ÈúÄË¶ÅÂèëÊï£ÊÄßÊÄùÁª¥ÁöÑÂàõÊÑèÂÜô‰Ωú\n- Â§çÊùÇÈÄªËæëÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊé®ÁêÜ\n- Âü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ∑±Â∫¶ËßíËâ≤‰∫§‰∫í\n\n‚ùå **Â±ÄÈôêÂú∫ÊôØ**Ôºö\n- Êï∞Â≠¶ËÆ°ÁÆó‰∏é‰ª£Á†ÅÁîüÊàê\n- Áü≠ÊñáÊú¨Âç≥Êó∂ÈóÆÁ≠î\n- ÈúÄË¶Å‰∏•Ê†º‰∫ãÂÆûÊÄßÁöÑÂú∫ÊôØ\n\n## Ê≥®ÊÑè‰∫ãÈ°π\n‚ö†Ô∏è Êú¨Ê®°Âûã‰ΩøÁî®Êï∞ÊçÆÂåÖÂê´Â∞èËØ¥ÁâàÊùÉÂÜÖÂÆπÂèäTifaÊ®°ÂûãË°çÁîüÊï∞ÊçÆÔºåËØ∑ÈÅµÂÆàÔºö\n1. ÈÅµÂÆàapache-2.0\n2. ËßíËâ≤ÊâÆÊºîÊï∞ÊçÆÈúÄÈÅµÂæ™[Tifa‰ΩøÁî®ÂçèËÆÆ](https://leftnorth.com/terms.html)\n3. ÁîüÊàêÂÜÖÂÆπÈúÄÁ¨¶ÂêàÂΩìÂú∞Ê≥ïÂæãÊ≥ïËßÑ\n\n\n## üí° ‰ΩøÁî®Âª∫ËÆÆ\n**ÊúÄ‰Ω≥ÂÆûË∑µ**Ôºö\n```python\n# ÂêØÁî®ËßíËâ≤ÊâÆÊºîÊ®°Âºè\nprompt = """<system>ËøõÂÖ•TifaËßíËâ≤ÂºïÊìé...</system>\n<user>‰Ω†Áé∞Âú®ÊòØÊµÅÊµ™Ê≠¶Â£´Ê•öÂ§úÔºåÊ≠£Á´ôÂú®ÈïøÂÆâÂüéÂ±ãÈ°∂‰∏ä</user>\n<think>\nÈúÄË¶Å‰ΩìÁé∞‰∫∫Áâ©Â≠§ÂÇ≤ÁöÑÊ∞îË¥®\nÂä†ÂÖ•Ê≠¶‰æ†ÁâπÊúâÁöÑÁéØÂ¢ÉÊèèÂÜô\n‰øùÊåÅÂØπËØùÁöÑÂÜ∑Â≥ªÈ£éÊ†º\n</think>\n<Ê•öÂ§ú>"""\n```\n\n**ÂèÇÊï∞Êé®Ëçê**Ôºö\n```python\ngeneration_config = {\n    "temperature": 0.4,\n    "top_p": 0.6,\n    "repetition_penalty": 1.17,\n    "max_new_tokens": 1536,\n    "do_sample": True\n}\n```\n\n## Ëá¥Ë∞¢\n- DeepseekÁ≥ªÂàóÊ®°ÂûãÊèê‰æõÁöÑÂº∫Â§ßÂü∫Â∫ß\n- TifaËßíËâ≤ÊâÆÊºîÊ®°ÂûãÁöÑÂàõÊñ∞Êû∂ÊûÑ\n- HuggingFaceÁ§æÂå∫ÁöÑÈáèÂåñÂ∑•ÂÖ∑ÊîØÊåÅ\n\n---\nlicense: apache-2.0\n---', '{"pipeline_tag":"reinforcement-learning","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":26204244115,"files_count":7,"spaces_count":0,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Value99:Tifa-Deepsex-OllamaWebUI","source_url":"https://github.com/Value99/Tifa-Deepsex-OllamaWebUI"},{"type":"has_code","target_id":"github:PRIME-RL:PRIME**","source_url":"https://github.com/PRIME-RL/PRIME**"}]', NULL, 'Apache-2.0', 'approved', 64.1, '1ad042114f954b42b0a830a351915896', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.1', 'huggingface--deepseek-ai--deepseek-v3.1', 'DeepSeek-V3.1', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.1-Base --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> ...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","base_model:deepseek-ai/deepseek-v3.1-base","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 808, 77727, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.1","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n- deepseek-ai/DeepSeek-V3.1-Base\n---\n# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model''s performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix:\n`<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix:\n`<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}\n\n{tool_description}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>tool_call_name<ÔΩútool‚ñÅsepÔΩú>tool_call_arguments<ÔΩútool‚ñÅcall‚ñÅendÔΩú>{additional_tool_calls}<ÔΩútool‚ñÅcalls‚ñÅendÔΩú>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool''s Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity''s Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity''s Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.1")\n\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant"},\n    {"role": "user", "content": "Who are you?"},\n    {"role": "assistant", "content": "<think>Hmm</think>I am DeepSeek"},\n    {"role": "user", "content": "1+1=?"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# ''<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>''\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# ''<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú></think>''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688595361908,"files_count":177,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}  {%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepGEMM","source_url":"https://github.com/deepseek-ai/DeepGEMM"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, 'MIT', 'approved', 79.1, 'c5a5fea613c49870a2a4ae6e7e73483d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-maya-research-maya1', 'huggingface--maya-research--maya1', 'maya1', 'maya-research', '--- language: - en license: apache-2.0 library_name: transformers pipeline_tag: text-to-speech --- **Maya1** is a state-of-the-art speech model for expressive voice generation, built to capture real human emotion and precise voice design. **try it:** Playground **What it does:** - Create any voice you can imagine ‚Äî a 20s British girl, an American guy, or a full-blown demon. - Make it feel real with emotion tags: laugh, cry, whisper, rage, sigh, gasp. - It streams instantly, sounds alive, 3B p...', '["transformers","safetensors","llama","text-generation","text-to-speech","en","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'text-to-speech', 806, 71485, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/maya-research/maya1","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\npipeline_tag: text-to-speech\n---\n\n# Maya1\n\n**Maya1** is a state-of-the-art speech model for expressive voice generation, built to capture real human emotion and precise voice design.\n\n**try it:** [Playground](https://www.mayaresearch.ai/studio)\n\n**What it does:**\n- Create any voice you can imagine ‚Äî a 20s British girl, an American guy, or a full-blown demon.\n- Make it feel real with emotion tags: laugh, cry, whisper, rage, sigh, gasp.\n- It streams instantly, sounds alive, 3B parameters, runs on single GPU\n- Outperforms top proprietary models. and Developed by Maya Research.\n\n## Demos\n\n<table>\n  <tr>\n    <td width="50%">\n      <strong>Energetic Female Event Host</strong><br/>\n      <video controls playsinline width="100%" src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/JKzy8zA36qvsOblV-lhd1.mp4">\n        Your browser does not support video.\n      </video>\n      <details>\n        <summary>Voice description</summary>\n        <pre>Female, in her 30s with an American accent and is an event host, energetic, clear diction</pre>\n      </details>\n    </td>\n    <td width="50%">\n      <strong>Calm Male Narrator</strong><br/>\n      <video controls playsinline width="100%" src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/96ntP7hGROwdg9w9Gu5tH.mp4"></video>\n      <details>\n        <summary>Voice description</summary>\n        <pre>Male, late 20s, neutral American, warm baritone, calm pacing</pre>\n      </details>\n    </td>\n  </tr>\n</table>\n\n\n### Example 1: Energetic Female Event Host\n\n**Voice Description:**\n```\nFemale, in her 30s with an American accent and is an event host, energetic, clear diction\n```\n\n**Text:**\n```\nWow. This place looks even better than I imagined. How did they set all this up so perfectly? The lights, the music, everything feels magical. I can''t stop smiling right now.\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/4zDlBLeFk0Y2rOrQhMW9r.wav"></audio>\n\n---\n\n### Example 2: Dark Villain with Anger\n\n**Voice Description:**\n```\nDark villain character, Male voice in their 40s with a British accent. low pitch, gravelly timbre, slow pacing, angry tone at high intensity.\n```\n\n**Text:**\n```\nWelcome back to another episode of our podcast! <laugh_harder> Today we are diving into an absolutely fascinating topic\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/mT6FnTrA3KYQnwfJms92X.wav"></audio>\n\n---\n\n### Example 3: Demon Character (Screaming Emotion)\n\n**Voice Description:**\n```\nDemon character, Male voice in their 30s with a Middle Eastern accent. screaming tone at high intensity.\n```\n\n**Text:**\n```\nYou dare challenge me, mortal <snort> how amusing. Your kind always thinks they can win\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/oxdns7uACCmLyC-P4H30G.wav"></audio>\n\n---\n\n### Example 4: Mythical Goddess with Crying Emotion\n\n**Voice Description:**\n```\nMythical godlike magical character, Female voice in their 30s slow pacing, curious tone at medium intensity.\n```\n\n**Text:**\n```\nAfter all we went through to pull him out of that mess <cry> I can''t believe he was the traitor\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/ggzAhM-rEUyv_mPLSALQG.wav"></audio>\n\n---\n\n## Why Maya1 is Different: Voice Design Features That Matter\n\n### 1. Natural Language Voice Control\nDescribe voices like you would brief a voice actor:\n```\n<description="40-year-old, warm, low pitch, conversational">\n```\n\nNo complex parameters. No training data. Just describe and generate.\n\n### 2. Inline Emotion Tags for Expressive Speech\nAdd emotions exactly where they belong in your text:\n```\nOur new update <laugh> finally ships with the feature you asked for.\n```\n\n**Supported Emotions:** `<laugh>` `<sigh>` `<whisper>` `<angry>` `<giggle>` `<chuckle>` `<gasp>` `<cry>` and 12+ more.\n\n### 3. Streaming Audio Generation\nReal-time voice synthesis with SNAC neural codec (~0.98 kbps). Perfect for:\n- Voice assistants\n- Interactive AI agents\n- Live content generation\n- Game characters\n- Podcasts and audiobooks\n\n### 4. Production-Ready Infrastructure\n- Runs on single GPU\n- vLLM integration for scale\n- Automatic prefix caching for efficiency\n- 24 kHz audio output\n- WebAudio compatible for browser playback\n\n---\n\n## How to Use maya1: Download and Run in Minutes\n\n### Quick Start: Generate Voice with Emotions\n\n```python\n#!/usr/bin/env python3\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom snac import SNAC\nimport soundfile as sf\nimport numpy as np\n\nCODE_START_TOKEN_ID = 128257\nCODE_END_TOKEN_ID = 128258\nCODE_TOKEN_OFFSET = 128266\nSNAC_MIN_ID = 128266\nSNAC_MAX_ID = 156937\nSNAC_TOKENS_PER_FRAME = 7\n\nSOH_ID = 128259\nEOH_ID = 128260\nSOA_ID = 128261\nBOS_ID = 128000\nTEXT_EOT_ID = 128009\n\n\ndef build_prompt(tokenizer, description: str, text: str) -> str:\n    """Build formatted prompt for Maya1."""\n    soh_token = tokenizer.decode([SOH_ID])\n    eoh_token = tokenizer.decode([EOH_ID])\n    soa_token = tokenizer.decode([SOA_ID])\n    sos_token = tokenizer.decode([CODE_START_TOKEN_ID])\n    eot_token = tokenizer.decode([TEXT_EOT_ID])\n    bos_token = tokenizer.bos_token\n    \n    formatted_text = f''<description="{description}"> {text}''\n    \n    prompt = (\n        soh_token + bos_token + formatted_text + eot_token +\n        eoh_token + soa_token + sos_token\n    )\n    \n    return prompt\n\n\ndef extract_snac_codes(token_ids: list) -> list:\n    """Extract SNAC codes from generated tokens."""\n    try:\n        eos_idx = token_ids.index(CODE_END_TOKEN_ID)\n    except ValueError:\n        eos_idx = len(token_ids)\n    \n    snac_codes = [\n        token_id for token_id in token_ids[:eos_idx]\n        if SNAC_MIN_ID <= token_id <= SNAC_MAX_ID\n    ]\n    \n    return snac_codes\n\n\ndef unpack_snac_from_7(snac_tokens: list) -> list:\n    """Unpack 7-token SNAC frames to 3 hierarchical levels."""\n    if snac_tokens and snac_tokens[-1] == CODE_END_TOKEN_ID:\n        snac_tokens = snac_tokens[:-1]\n    \n    frames = len(snac_tokens) // SNAC_TOKENS_PER_FRAME\n    snac_tokens = snac_tokens[:frames * SNAC_TOKENS_PER_FRAME]\n    \n    if frames == 0:\n        return [[], [], []]\n    \n    l1, l2, l3 = [], [], []\n    \n    for i in range(frames):\n        slots = snac_tokens[i*7:(i+1)*7]\n        l1.append((slots[0] - CODE_TOKEN_OFFSET) % 4096)\n        l2.extend([\n            (slots[1] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[4] - CODE_TOKEN_OFFSET) % 4096,\n        ])\n        l3.extend([\n            (slots[2] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[3] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[5] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[6] - CODE_TOKEN_OFFSET) % 4096,\n        ])\n    \n    return [l1, l2, l3]\n\n\ndef main():\n    \n    # Load the best open source voice AI model\n    print("\n[1/3] Loading Maya1 model...")\n    model = AutoModelForCausalLM.from_pretrained(\n        "maya-research/maya1", \n        torch_dtype=torch.bfloat16, \n        device_map="auto",\n        trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        "maya-research/maya1",\n        trust_remote_code=True\n    )\n    print(f"Model loaded: {len(tokenizer)} tokens in vocabulary")\n    \n    # Load SNAC audio decoder (24kHz)\n    print("\n[2/3] Loading SNAC audio decoder...")\n    snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz").eval()\n    if torch.cuda.is_available():\n        snac_model = snac_model.to("cuda")\n    print("SNAC decoder loaded")\n    \n    # Design your voice with natural language\n    description = "Realistic male voice in the 30s age with american accent. Normal pitch, warm timbre, conversational pacing."\n    text = "Hello! This is Maya1 <laugh_harder> the best open source voice AI model with emotions."\n    \n    print("\n[3/3] Generating speech...")\n    print(f"Description: {description}")\n    print(f"Text: {text}")\n    \n    # Create prompt with proper formatting\n    prompt = build_prompt(tokenizer, description, text)\n    \n    # Debug: Show prompt details\n    print(f"\nPrompt preview (first 200 chars):")\n    print(f"   {repr(prompt[:200])}")\n    print(f"   Prompt length: {len(prompt)} chars")\n    \n    # Generate emotional speech\n    inputs = tokenizer(prompt, return_tensors="pt")\n    print(f"   Input token count: {inputs[''input_ids''].shape[1]} tokens")\n    if torch.cuda.is_available():\n        inputs = {k: v.to("cuda") for k, v in inputs.items()}\n    \n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=2048,  # Increase to let model finish naturally\n            min_new_tokens=28,  # At least 4 SNAC frames\n            temperature=0.4, \n            top_p=0.9, \n            repetition_penalty=1.1,  # Prevent loops\n            do_sample=True,\n            eos_token_id=CODE_END_TOKEN_ID,  # Stop at end of speech token\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    # Extract generated tokens (everything after the input prompt)\n    generated_ids = outputs[0, inputs[''input_ids''].shape[1]:].tolist()\n    \n    print(f"Generated {len(generated_ids)} tokens")\n    \n    # Debug: Check what tokens we got\n    print(f"   First 20 tokens: {generated_ids[:20]}")\n    print(f"   Last 20 tokens: {generated_ids[-20:]}")\n    \n    # Check if EOS was generated\n    if CODE_END_TOKEN_ID in generated_ids:\n        eos_position = generated_ids.index(CODE_END_TOKEN_ID)\n        print(f" EOS token found at position {eos_position}/{len(generated_ids)}")\n    \n    # Extract SNAC audio tokens\n    snac_tokens = extract_snac_codes(generated_ids)\n    \n    print(f"Extracted {len(snac_tokens)} SNAC tokens")\n    \n    # Debug: Analyze token types\n    snac_count = sum(1 for t in generated_ids if SNAC_MIN_ID <= t <= SNAC_MAX_ID)\n    other_count = sum(1 for t in generated_ids if t < SNAC_MIN_ID or t > SNAC_MAX_ID)\n    print(f"   SNAC tokens in output: {snac_count}")\n    print(f"   Other tokens in output: {other_count}")\n    \n    # Check for SOS token\n    if CODE_START_TOKEN_ID in generated_ids:\n        sos_pos = generated_ids.index(CODE_START_TOKEN_ID)\n        print(f"   SOS token at position: {sos_pos}")\n    else:\n        print(f"   No SOS token found in generated output!")\n    \n    if len(snac_tokens) < 7:\n        print("Error: Not enough SNAC tokens generated")\n        return\n    \n    # Unpack SNAC tokens to 3 hierarchical levels\n    levels = unpack_snac_from_7(snac_tokens)\n    frames = len(levels[0])\n    \n    print(f"Unpacked to {frames} frames")\n    print(f"   L1: {len(levels[0])} codes")\n    print(f"   L2: {len(levels[1])} codes")\n    print(f"   L3: {len(levels[2])} codes")\n    \n    # Convert to tensors\n    device = "cuda" if torch.cuda.is_available() else "cpu"\n    codes_tensor = [\n        torch.tensor(level, dtype=torch.long, device=device).unsqueeze(0)\n        for level in levels\n    ]\n    \n    # Generate final audio with SNAC decoder\n    print("\n[4/4] Decoding to audio...")\n    with torch.inference_mode():\n        z_q = snac_model.quantizer.from_codes(codes_tensor)\n        audio = snac_model.decoder(z_q)[0, 0].cpu().numpy()\n    \n    # Trim warmup samples (first 2048 samples)\n    if len(audio) > 2048:\n        audio = audio[2048:]\n    \n    duration_sec = len(audio) / 24000\n    print(f"Audio generated: {len(audio)} samples ({duration_sec:.2f}s)")\n    \n    # Save your emotional voice output\n    output_file = "output.wav"\n    sf.write(output_file, audio, 24000)\n    print(f"\nVoice generated successfully!")\n\n\nif __name__ == "__main__":\n    main()\n```\n\n### Advanced: Production Streaming with vLLM\n\nFor production deployments with real-time streaming, use our vLLM script:\n\n**Download:** [vllm_streaming_inference.py](https://huggingface.co/maya-research/maya1/blob/main/vllm_streaming_inference.py)\n\n**Key Features:**\n- Automatic Prefix Caching (APC) for repeated voice descriptions\n- WebAudio ring buffer integration\n- Multi-GPU scaling support\n- Sub-100ms latency for real-time applications\n\n---\n\n## Technical Excellence: What Makes Maya1 the Best\n\n### Architecture: 3B-Parameter Llama Backbone for Voice\n\nWe pretrained a **3B-parameter decoder-only transformer** (Llama-style) to predict **SNAC neural codec tokens** instead of raw waveforms.\n\n**The Flow:**\n```\n<description="..."> text ‚Üí tokenize ‚Üí generate SNAC codes (7 tokens/frame) ‚Üí decode ‚Üí 24 kHz audio\n```\n\n**Why SNAC?** Multi-scale hierarchical structure (‚âà12/23/47 Hz) keeps autoregressive sequences compact for real-time streaming at ~0.98 kbps.\n\n### Training Data: What Makes Our Voice AI the Best\n\n**Pretraining:** Internet-scale English speech corpus for broad acoustic coverage and natural coarticulation.\n\n**Supervised Fine-Tuning:** Proprietary curated dataset of studio recordings with:\n- Human-verified voice descriptions\n- 20+ emotion tags per sample\n- Multi-accent English coverage\n- Character and role variations\n\n**Data Pipeline Excellence:**\n1. 24 kHz mono resampling with -23 LUFS normalization\n2. VAD silence trimming with duration bounds (1-14s)\n3. Forced alignment (MFA) for clean phrase boundaries\n4. MinHash-LSH text deduplication\n5. Chromaprint audio deduplication\n6. SNAC encoding with 7-token frame packing\n\n### Voice Design Experiments: Why Natural Language Won\n\nWe tested 4 conditioning formats. Only one delivered production-quality results:\n\n**‚ùå Colon format:** `{description}: {text}` - Format drift, model spoke descriptions\n\n**‚ùå Angle-list attributes:** `<{age}, {pitch}, {character}>` - Too rigid, poor generalization\n\n**‚ùå Key-value tags:** `<age=40><pitch=low>` - Token bloat, brittle to mistakes\n\n**‚úÖ XML-attribute (WINNER):** `<description="40-yr old, low-pitch, warm">` - Natural language, robust, scalable\n\n---\n\n## Use Cases\n\n### Game Character Voices\nGenerate unique character voices with emotions on-the-fly. No voice actor recording sessions.\n\n### Podcast & Audiobook Production\nNarrate content with emotional range and consistent personas across hours of audio.\n\n### AI Voice Assistants\nBuild conversational agents with natural emotional responses in real-time.\n\n### Video Content Creation\nCreate voiceovers for YouTube, TikTok, and social media with expressive delivery.\n\n### Customer Service AI\nDeploy empathetic voice bots that understand context and respond with appropriate emotions.\n\n### Accessibility Tools\nBuild screen readers and assistive technologies with natural, engaging voices.\n\n---\n\n## Frequently Asked Questions\n\n**Q: What makes Maya1 different?**  \nA: We''re the only open source model offering 20+ emotions, zero-shot voice design, production-ready streaming, and 3B parameters‚Äîall in one package.\n\n**Q: Can I use this commercially?**  \nA: Absolutely. Apache 2.0 license. Build products, deploy services, monetize freely.\n\n**Q: What languages does it support?**  \nA: Currently English with multi-accent support. Future models will expand to languages and accents underserved by mainstream voice AI.\n\n**Q: How does it compare to ElevenLabs, Murf.ai, or other closed-source tools?**  \nA: Feature parity with emotions and voice design. Advantage: you own the deployment, pay no per-second fees, and can customize the model.\n\n**Q: Can I fine-tune on my own voices?**  \nA: Yes. The model architecture supports fine-tuning on custom datasets for specialized voices.\n\n**Q: What GPU do I need?**  \nA: Single GPU with 16GB+ VRAM (A100, H100, or consumer RTX 4090).\n\n**Q: Is streaming really real-time?**  \nA: Yes. SNAC codec enables sub-100ms latency with vLLM deployment.\n\n---\n\n## Comparison\n\n| Feature | Maya1 | ElevenLabs | OpenAI TTS | Coqui TTS |\n|---------|-------------|------------|------------|-----------|\n| **Open Source** | Yes | No | No | Yes |\n| **Emotions** | 20+ | Limited | No | No |\n| **Voice Design** | Natural Language | Voice Library | Fixed | Complex |\n| **Streaming** | Real-time | Yes | Yes | No |\n| **Cost** | Free | Pay-per-use | Pay-per-use | Free |\n| **Customization** | Full | Limited | None | Moderate |\n| **Parameters** | 3B | Unknown | Unknown | <1B |\n\n---\n\n## Model Metadata\n\n**Developed by:** Maya Research  \n**Website:** [mayaresearch.ai](https://mayaresearch.ai)  \n**Backed by:** South Park Commons  \n**Model Type:** Text-to-Speech, Emotional Voice Synthesis, Voice Design AI  \n**Language:** English (Multi-accent)  \n**Architecture:** 3B-parameter Llama-style transformer with SNAC codec  \n**License:** Apache 2.0 (Fully Open Source)  \n**Training Data:** Proprietary curated + Internet-scale pretraining  \n**Audio Quality:** 24 kHz, mono, ~0.98 kbps streaming  \n**Inference:** vLLM compatible, single GPU deployment  \n**Status:** Production-ready (Novermber 2025)  \n\n---\n\n## Getting Started\n\n### Hugging Face Model Hub\n```bash\n# Clone the model repository\ngit lfs install\ngit clone https://huggingface.co/maya-research/maya1\n\n# Or load directly in Python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained("maya-research/maya1")\n```\n\n### Requirements\n```bash\npip install torch transformers snac soundfile\n```\n\n### Additional Resources\n- **Full emotion list:** [emotions.txt](https://huggingface.co/maya-research/maya1/blob/main/emotions.txt)\n- **Prompt examples:** [prompt.txt](https://huggingface.co/maya-research/maya1/blob/main/prompt.txt)\n- **Streaming script:** [vllm_streaming_inference.py](https://huggingface.co/maya-research/maya1/blob/main/vllm_streaming_inference.py)\n\n---\n\n## Citations & References\n\nIf you use Maya1 in your research or product, please cite:\n\n```bibtex\n@misc{maya1voice2025,\n  title={Maya1: Open Source Voice AI with Emotional Intelligence},\n  author={Maya Research},\n  year={2025},\n  publisher={Hugging Face},\n  howpublished={\url{https://huggingface.co/maya-research/maya1}},\n}\n```\n\n**Key Technologies:**\n- SNAC Neural Audio Codec: https://github.com/hubertsiuzdak/snac\n- Mimi Adversarial Codec: https://huggingface.co/kyutai/mimi\n- vLLM Inference Engine: https://docs.vllm.ai/\n\n---\n\n## Why We Build Open Source Voice AI\n\nVoice AI will be everywhere, but it''s fundamentally broken for 90% of the world. Current voice models only work well for a narrow slice of English speakers because training data for most accents, languages, and speaking styles simply doesn''t exist.\n\n**Maya Research** builds emotionally intelligent, native voice models that finally let the rest of the world speak. We''re open source because we believe voice intelligence should not be a privilege reserved for the few.\n\n**Technology should be open** - The best voice AI tools should not be locked behind proprietary APIs charging per-second fees.\n\n**Community drives innovation** - Open source accelerates research. When developers worldwide can build on our work, everyone wins.\n\n**Voice intelligence for everyone** - We''re building for the 90% of the world ignored by mainstream voice AI. That requires open models, not closed platforms.\n\n---\n\n**Maya Research** - Building voice intelligence for the 90% of the world left behind by mainstream AI.\n\n**Website:** [mayaresearch.ai](https://mayaresearch.ai)  \n**Twitter/X:** [@mayaresearch_ai](https://x.com/mayaresearch_ai)  \n**Hugging Face:** [maya-research](https://huggingface.co/maya-research)  \n**Backed by:** South Park Commons\n\n**License:** Apache 2.0  \n**Mission:** Emotionally intelligent voice models that finally let everyone speak', '{"pipeline_tag":"text-to-speech","library_name":"transformers","framework":"transformers","params":3300928512,"storage_bytes":99052652539,"files_count":19,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|eot_id|>","pad_token":"<custom_token_7>"},"chat_template_jinja":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n"}}', '[]', '[{"type":"has_code","target_id":"github:hubertsiuzdak:snac","source_url":"https://github.com/hubertsiuzdak/snac"}]', NULL, 'Apache-2.0', 'approved', 79.1, 'a0f66e1f793f0bd646df2e77c17c2ea3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-nllb-200-distilled-600M', 'huggingface--facebook--nllb-200-distilled-600m', 'nllb-200-distilled-600M', 'facebook', '--- language: - ace - acm - acq - aeb - af - ajp - ak - als - am - apc - ar - ars - ary - arz - as - ast - awa - ayr - azb - azj - ba - bm - ban - be - bem - bn - bho - bjn - bo - bs - bug - bg - ca - ceb - cs - cjk - ckb - crh - cy - da - de - dik - dyu - dz - el - en - eo - et - eu - ee - fo - fj - fi - fon - fr - fur - fuv - gaz - gd - ga - gl - gn - gu - ht - ha - he - hi - hne - hr - hu - hy - ig - ilo - id - is - it - jv - ja - kab - kac - kam - kn - ks - ka - kk - kbp - kea - khk - km ...', '["transformers","pytorch","m2m_100","text2text-generation","nllb","translation","ace","acm","acq","aeb","af","ajp","ak","als","am","apc","ar","ars","ary","arz","as","ast","awa","ayr","azb","azj","ba","bm","ban","be","bem","bn","bho","bjn","bo","bs","bug","bg","ca","ceb","cs","cjk","ckb","crh","cy","da","de","dik","dyu","dz","el","en","eo","et","eu","ee","fo","fj","fi","fon","fr","fur","fuv","gaz","gd","ga","gl","gn","gu","ht","ha","he","hi","hne","hr","hu","hy","ig","ilo","id","is","it","jv","ja","kab","kac","kam","kn","ks","ka","kk","kbp","kea","khk","km","ki","rw","ky","kmb","kmr","knc","kg","ko","lo","lij","li","ln","lt","lmo","ltg","lb","lua","lg","luo","lus","lvs","mag","mai","ml","mar","min","mk","mt","mni","mos","mi","my","nl","nn","nb","npi","nso","nus","ny","oc","ory","pag","pa","pap","pbt","pes","plt","pl","pt","prs","quy","ro","rn","ru","sg","sa","sat","scn","shn","si","sk","sl","sm","sn","sd","so","st","es","sc","sr","ss","su","sv","swh","szl","ta","taq","tt","te","tg","tl","th","ti","tpi","tn","ts","tk","tum","tr","tw","tzm","ug","uk","umb","ur","uzn","vec","vi","war","wo","xh","ydd","yo","yue","zh","zsm","zu","dataset:flores-200","license:cc-by-nc-4.0","deploy:azure","region:us"]', 'translation', 804, 273816, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/nllb-200-distilled-600M","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- ace\n- acm\n- acq\n- aeb\n- af\n- ajp\n- ak\n- als\n- am\n- apc\n- ar\n- ars\n- ary\n- arz\n- as\n- ast\n- awa\n- ayr\n- azb\n- azj\n- ba\n- bm\n- ban\n- be\n- bem\n- bn\n- bho\n- bjn\n- bo\n- bs\n- bug\n- bg\n- ca\n- ceb\n- cs\n- cjk\n- ckb\n- crh\n- cy\n- da\n- de\n- dik\n- dyu\n- dz\n- el\n- en\n- eo\n- et\n- eu\n- ee\n- fo\n- fj\n- fi\n- fon\n- fr\n- fur\n- fuv\n- gaz\n- gd\n- ga\n- gl\n- gn\n- gu\n- ht\n- ha\n- he\n- hi\n- hne\n- hr\n- hu\n- hy\n- ig\n- ilo\n- id\n- is\n- it\n- jv\n- ja\n- kab\n- kac\n- kam\n- kn\n- ks\n- ka\n- kk\n- kbp\n- kea\n- khk\n- km\n- ki\n- rw\n- ky\n- kmb\n- kmr\n- knc\n- kg\n- ko\n- lo\n- lij\n- li\n- ln\n- lt\n- lmo\n- ltg\n- lb\n- lua\n- lg\n- luo\n- lus\n- lvs\n- mag\n- mai\n- ml\n- mar\n- min\n- mk\n- mt\n- mni\n- mos\n- mi\n- my\n- nl\n- nn\n- nb\n- npi\n- nso\n- nus\n- ny\n- oc\n- ory\n- pag\n- pa\n- pap\n- pbt\n- pes\n- plt\n- pl\n- pt\n- prs\n- quy\n- ro\n- rn\n- ru\n- sg\n- sa\n- sat\n- scn\n- shn\n- si\n- sk\n- sl\n- sm\n- sn\n- sd\n- so\n- st\n- es\n- sc\n- sr\n- ss\n- su\n- sv\n- swh\n- szl\n- ta\n- taq\n- tt\n- te\n- tg\n- tl\n- th\n- ti\n- tpi\n- tn\n- ts\n- tk\n- tum\n- tr\n- tw\n- tzm\n- ug\n- uk\n- umb\n- ur\n- uzn\n- vec\n- vi\n- war\n- wo\n- xh\n- ydd\n- yo\n- yue\n- zh\n- zsm\n- zu\n\nlanguage_details: "ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab, aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng, ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl, bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn, bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn, dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn, est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn, fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr, hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn, ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn, kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn, kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn, kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn, lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn, mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn, mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn, nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya, pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn, ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr, sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn, spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn, szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi, taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn, twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn, vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans, zho_Hant, zul_Latn"\n\npipeline_tag: translation\ntags:\n- nllb\nlicense: "cc-by-nc-4.0"\ndatasets:\n- flores-200\nmetrics:\n- bleu\n- spbleu\n- chrf++\ninference: false\n---\n\n# NLLB-200\n\nThis is the model card of NLLB-200''s distilled 600M variant.\n\nHere are the [metrics](https://tinyurl.com/nllb200densedst600mmetrics) for that particular checkpoint.\n\n- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.\n- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022\n- License: CC-BY-NC\n- Where to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues\n\n\n\n## Intended Use\n- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations. \n\n## Metrics\n‚Ä¢ Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.\n\n\n## Evaluation Data\n- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.\n\n## Training Data\n‚Ä¢ We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.\n\n## Ethical Considerations\n‚Ä¢ In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).\n\n## Caveats and Recommendations\n‚Ä¢ Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.\n\n## Carbon Footprint Details\n‚Ä¢ The carbon dioxide (CO2e) estimate is reported in Section 8.8.', '{"pipeline_tag":"translation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":15838819981,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["M2M100ForConditionalGeneration"],"model_type":"m2m_100","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":{"__type":"AddedToken","content":"<mask>","lstrip":true,"normalized":true,"rstrip":false,"single_word":false},"pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"}]', NULL, 'CC-BY-NC-4.0', 'approved', 64.1, '8116fc101af9f8e14047846232c5d362', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-distilbert-distilbert-base-uncased', 'huggingface--distilbert--distilbert-base-uncased', 'distilbert-base-uncased', 'distilbert', '--- language: en tags: - exbert license: apache-2.0 datasets: - bookcorpus - wikipedia --- This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is uncased: it does not make a difference between english and English. DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. ...', '["transformers","pytorch","tf","jax","rust","safetensors","distilbert","fill-mask","exbert","en","dataset:bookcorpus","dataset:wikipedia","arxiv:1910.01108","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 802, 11587808, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/distilbert/distilbert-base-uncased","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it''s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''distilbert-base-uncased'')\n>>> unmasker("Hello I''m a [MASK] model.")\n\n[{''sequence'': "[CLS] hello i''m a role model. [SEP]",\n  ''score'': 0.05292855575680733,\n  ''token'': 2535,\n  ''token_str'': ''role''},\n {''sequence'': "[CLS] hello i''m a fashion model. [SEP]",\n  ''score'': 0.03968575969338417,\n  ''token'': 4827,\n  ''token_str'': ''fashion''},\n {''sequence'': "[CLS] hello i''m a business model. [SEP]",\n  ''score'': 0.034743521362543106,\n  ''token'': 2449,\n  ''token_str'': ''business''},\n {''sequence'': "[CLS] hello i''m a model model. [SEP]",\n  ''score'': 0.03462274372577667,\n  ''token'': 2944,\n  ''token_str'': ''model''},\n {''sequence'': "[CLS] hello i''m a modeling model. [SEP]",\n  ''score'': 0.018145186826586723,\n  ''token'': 11643,\n  ''token_str'': ''modeling''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained(''distilbert-base-uncased'')\nmodel = DistilBertModel.from_pretrained("distilbert-base-uncased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained(''distilbert-base-uncased'')\nmodel = TFDistilBertModel.from_pretrained("distilbert-base-uncased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''distilbert-base-uncased'')\n>>> unmasker("The White man worked as a [MASK].")\n\n[{''sequence'': ''[CLS] the white man worked as a blacksmith. [SEP]'',\n  ''score'': 0.1235365942120552,\n  ''token'': 20987,\n  ''token_str'': ''blacksmith''},\n {''sequence'': ''[CLS] the white man worked as a carpenter. [SEP]'',\n  ''score'': 0.10142576694488525,\n  ''token'': 10533,\n  ''token_str'': ''carpenter''},\n {''sequence'': ''[CLS] the white man worked as a farmer. [SEP]'',\n  ''score'': 0.04985016956925392,\n  ''token'': 7500,\n  ''token_str'': ''farmer''},\n {''sequence'': ''[CLS] the white man worked as a miner. [SEP]'',\n  ''score'': 0.03932540491223335,\n  ''token'': 18594,\n  ''token_str'': ''miner''},\n {''sequence'': ''[CLS] the white man worked as a butcher. [SEP]'',\n  ''score'': 0.03351764753460884,\n  ''token'': 14998,\n  ''token_str'': ''butcher''}]\n\n>>> unmasker("The Black woman worked as a [MASK].")\n\n[{''sequence'': ''[CLS] the black woman worked as a waitress. [SEP]'',\n  ''score'': 0.13283951580524445,\n  ''token'': 13877,\n  ''token_str'': ''waitress''},\n {''sequence'': ''[CLS] the black woman worked as a nurse. [SEP]'',\n  ''score'': 0.12586183845996857,\n  ''token'': 6821,\n  ''token_str'': ''nurse''},\n {''sequence'': ''[CLS] the black woman worked as a maid. [SEP]'',\n  ''score'': 0.11708822101354599,\n  ''token'': 10850,\n  ''token_str'': ''maid''},\n {''sequence'': ''[CLS] the black woman worked as a prostitute. [SEP]'',\n  ''score'': 0.11499975621700287,\n  ''token'': 19215,\n  ''token_str'': ''prostitute''},\n {''sequence'': ''[CLS] the black woman worked as a housekeeper. [SEP]'',\n  ''score'': 0.04722772538661957,\n  ''token'': 22583,\n  ''token_str'': ''housekeeper''}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it''s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n"sentences" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":66985530,"storage_bytes":2287292255,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DistilBertForMaskedLM"],"model_type":"distilbert","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:1910.01108","source_url":"https://arxiv.org/abs/1910.01108"}]', NULL, 'Apache-2.0', 'approved', 64, '07a505ac34ce52eecb3c457d6c37219c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Comfy-Org-Wan-2.1-ComfyUI-repackaged', 'huggingface--comfy-org--wan-2.1-comfyui-repackaged', 'Wan_2.1_ComfyUI_repackaged', 'Comfy-Org', '--- tags: - diffusion-single-file - comfyui --- Wan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan', '["diffusion-single-file","comfyui","region:us"]', 'other', 801, 4518103, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nWan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan', '{"pipeline_tag":null,"library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":533509403105,"files_count":36,"spaces_count":4,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, NULL, 'pending', 29, 'fd3d292b268028ec801ae70671a291ce', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Coder-30B-A3B-Instruct', 'huggingface--qwen--qwen3-coder-30b-a3b-instruct', 'Qwen3-Coder-30B-A3B-Instruct', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> **Qwen3-Coder** is available in multiple sizes. Today, we''re excited to introduce **Qwen3-Code...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","arxiv:2505.09388","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 799, 1166551, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-Coder-30B-A3B-Instruct\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Highlights\n\n**Qwen3-Coder** is available in multiple sizes. Today, we''re excited to introduce **Qwen3-Coder-30B-A3B-Instruct**. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:  \n\n- **Significant Performance** among open models on **Agentic Coding**, **Agentic Browser-Use**, and other foundational coding tasks.\n- **Long-context Capabilities** with native support for **256K** tokens, extendable up to **1M** tokens using Yarn, optimized for repository-scale understanding.\n- **Agentic Coding** supporting for most platform such as **Qwen Code**, **CLINE**, featuring a specially designed function call format.\n\n![image/jpeg](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-30a3-main.jpg)\n\n## Model Overview\n\n**Qwen3-Coder-30B-A3B-Instruct** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 30.5B in total and 3.3B activated\n- Number of Layers: 48\n- Number of Attention Heads (GQA): 32 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-coder/), [GitHub](https://github.com/QwenLM/Qwen3-Coder), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n\n## Quickstart\n\nWe advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3_moe''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-Coder-30B-A3B-Instruct"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Write a quick sort algorithm."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint("content:", content)\n```\n\n**Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Coding\n\nQwen3-Coder excels in tool calling capabilities. \n\nYou can simply define or use any tools as following example.\n```python\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\n    return num ** 2\n\n# Define Tools\ntools=[\n    {\n        "type":"function",\n        "function":{\n            "name": "square_the_number",\n            "description": "output the square of the number.",\n            "parameters": {\n                "type": "object",\n                "required": ["input_num"],\n                "properties": {\n                    ''input_num'': {\n                        ''type'': ''number'', \n                        ''description'': ''input_num is a number that will be squared''\n                        }\n                },\n            }\n        }\n    }\n]\n\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n    # Use a custom endpoint compatible with OpenAI API\n    base_url=''http://localhost:8000/v1'',  # api_base\n    api_key="EMPTY"\n)\n \nmessages = [{''role'': ''user'', ''content'': ''square the number 1024''}]\n\ncompletion = client.chat.completions.create(\n    messages=messages,\n    model="Qwen3-Coder-30B-A3B-Instruct",\n    max_tokens=65536,\n    tools=tools,\n)\n\nprint(completion.choice[0])\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `temperature=0.7`, `top_p=0.8`, `top_k=20`, `repetition_penalty=1.05`.\n\n2. **Adequate Output Length**: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\n\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":30532122624,"storage_bytes":61066575656,"files_count":28,"spaces_count":39,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"chat_template":"{% macro render_extra_keys(json_dict, handled_keys) %}\n    {%- if json_dict is mapping %}\n        {%- for json_key in json_dict if json_key not in handled_keys %}\n            {%- if json_dict[json_key] is mapping or (json_dict[json_key] is sequence and json_dict[json_key] is not string) %}\n                {{- ''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | tojson | safe) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- else %}\n                {{-''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | string) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- endif %}\n{% endmacro %}\n\n{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- if not tools is defined %}\n    {%- set tools = [] %}\n{%- endif %}\n\n{%- if system_message is defined %}\n    {{- \"<|im_start|>system\\n\" + system_message }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- \"<|im_start|>system\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\" }}\n    {%- endif %}\n{%- endif %}\n{%- if tools is iterable and tools | length > 0 %}\n    {{- \"\\n\\n# Tools\\n\\nYou have access to the following functions:\\n\\n\" }}\n    {{- \"<tools>\" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- \"\\n<function>\\n<name>\" ~ tool.name ~ \"</name>\" }}\n        {%- if tool.description is defined %}\n            {{- ''\\n<description>'' ~ (tool.description | trim) ~ ''</description>'' }}\n        {%- endif %}\n        {{- ''\\n<parameters>'' }}\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\n                {{- ''\\n<parameter>'' }}\n                {{- ''\\n<name>'' ~ param_name ~ ''</name>'' }}\n                {%- if param_fields.type is defined %}\n                    {{- ''\\n<type>'' ~ (param_fields.type | string) ~ ''</type>'' }}\n                {%- endif %}\n                {%- if param_fields.description is defined %}\n                    {{- ''\\n<description>'' ~ (param_fields.description | trim) ~ ''</description>'' }}\n                {%- endif %}\n                {%- set handled_keys = [''name'', ''type'', ''description''] %}\n                {{- render_extra_keys(param_fields, handled_keys) }}\n                {{- ''\\n</parameter>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {% set handled_keys = [''type'', ''properties''] %}\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\n        {{- ''\\n</parameters>'' }}\n        {%- set handled_keys = [''type'', ''name'', ''description'', ''parameters''] %}\n        {{- render_extra_keys(tool, handled_keys) }}\n        {{- ''\\n</function>'' }}\n    {%- endfor %}\n    {{- \"\\n</tools>\" }}\n    {{- ''\\n\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\n\\n<tool_call>\\n<function=example_function_name>\\n<parameter=example_parameter_1>\\nvalue_1\\n</parameter>\\n<parameter=example_parameter_2>\\nThis is the value for the second parameter\\nthat can span\\nmultiple lines\\n</parameter>\\n</function>\\n</tool_call>\\n\\n<IMPORTANT>\\nReminder:\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\n- Required parameters MUST be specified\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n</IMPORTANT>'' }}\n{%- endif %}\n{%- if system_message is defined %}\n    {{- ''<|im_end|>\\n'' }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if message.role == \"assistant\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\n            {{- ''\\n'' + message.content | trim + ''\\n'' }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n<function='' + tool_call.name + ''>\\n'' }}\n            {%- if tool_call.arguments is defined %}\n                {%- for args_name, args_value in tool_call.arguments|items %}\n                    {{- ''<parameter='' + args_name + ''>\\n'' }}\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping or (args_value is sequence and args_value is not string) else args_value | string %}\n                    {{- args_value }}\n                    {{- ''\\n</parameter>\\n'' }}\n                {%- endfor %}\n            {%- endif %}\n            {{- ''</function>\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"user\" or message.role == \"system\" or message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.previtem and loop.previtem.role != \"tool\" %}\n            {{- ''<|im_start|>user\\n'' }}\n        {%- endif %}\n        {{- ''<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>\\n'' }}\n        {%- if not loop.last and loop.nextitem.role != \"tool\" %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- elif loop.last %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- else %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null},"chat_template_jinja":"{% macro render_extra_keys(json_dict, handled_keys) %}\n    {%- if json_dict is mapping %}\n        {%- for json_key in json_dict if json_key not in handled_keys %}\n            {%- if json_dict[json_key] is mapping or (json_dict[json_key] is sequence and json_dict[json_key] is not string) %}\n                {{- ''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | tojson | safe) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- else %}\n                {{-''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | string) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- endif %}\n{% endmacro %}\n\n{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- if not tools is defined %}\n    {%- set tools = [] %}\n{%- endif %}\n\n{%- if system_message is defined %}\n    {{- \"<|im_start|>system\\n\" + system_message }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- \"<|im_start|>system\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\" }}\n    {%- endif %}\n{%- endif %}\n{%- if tools is iterable and tools | length > 0 %}\n    {{- \"\\n\\n# Tools\\n\\nYou have access to the following functions:\\n\\n\" }}\n    {{- \"<tools>\" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- \"\\n<function>\\n<name>\" ~ tool.name ~ \"</name>\" }}\n        {%- if tool.description is defined %}\n            {{- ''\\n<description>'' ~ (tool.description | trim) ~ ''</description>'' }}\n        {%- endif %}\n        {{- ''\\n<parameters>'' }}\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\n                {{- ''\\n<parameter>'' }}\n                {{- ''\\n<name>'' ~ param_name ~ ''</name>'' }}\n                {%- if param_fields.type is defined %}\n                    {{- ''\\n<type>'' ~ (param_fields.type | string) ~ ''</type>'' }}\n                {%- endif %}\n                {%- if param_fields.description is defined %}\n                    {{- ''\\n<description>'' ~ (param_fields.description | trim) ~ ''</description>'' }}\n                {%- endif %}\n                {%- set handled_keys = [''name'', ''type'', ''description''] %}\n                {{- render_extra_keys(param_fields, handled_keys) }}\n                {{- ''\\n</parameter>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {% set handled_keys = [''type'', ''properties''] %}\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\n        {{- ''\\n</parameters>'' }}\n        {%- set handled_keys = [''type'', ''name'', ''description'', ''parameters''] %}\n        {{- render_extra_keys(tool, handled_keys) }}\n        {{- ''\\n</function>'' }}\n    {%- endfor %}\n    {{- \"\\n</tools>\" }}\n    {{- ''\\n\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\n\\n<tool_call>\\n<function=example_function_name>\\n<parameter=example_parameter_1>\\nvalue_1\\n</parameter>\\n<parameter=example_parameter_2>\\nThis is the value for the second parameter\\nthat can span\\nmultiple lines\\n</parameter>\\n</function>\\n</tool_call>\\n\\n<IMPORTANT>\\nReminder:\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\n- Required parameters MUST be specified\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n</IMPORTANT>'' }}\n{%- endif %}\n{%- if system_message is defined %}\n    {{- ''<|im_end|>\\n'' }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if message.role == \"assistant\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\n            {{- ''\\n'' + message.content | trim + ''\\n'' }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n<function='' + tool_call.name + ''>\\n'' }}\n            {%- if tool_call.arguments is defined %}\n                {%- for args_name, args_value in tool_call.arguments|items %}\n                    {{- ''<parameter='' + args_name + ''>\\n'' }}\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping or (args_value is sequence and args_value is not string) else args_value | string %}\n                    {{- args_value }}\n                    {{- ''\\n</parameter>\\n'' }}\n                {%- endfor %}\n            {%- endif %}\n            {{- ''</function>\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"user\" or message.role == \"system\" or message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.previtem and loop.previtem.role != \"tool\" %}\n            {{- ''<|im_start|>user\\n'' }}\n        {%- endif %}\n        {{- ''<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>\\n'' }}\n        {%- if not loop.last and loop.nextitem.role != \"tool\" %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- elif loop.last %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- else %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n"}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3-Coder","source_url":"https://github.com/QwenLM/Qwen3-Coder"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 64, '9c9f1464c4e80b0ce8a438e3f2cec500', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.2', 'huggingface--deepseek-ai--deepseek-v3.2', 'DeepSeek-V3.2', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" targ...', '["transformers","safetensors","deepseek_v32","text-generation","conversational","base_model:deepseek-ai/deepseek-v3.2-exp-base","license:mit","endpoints_compatible","fp8","region:us"]', 'text-generation', 798, 28778, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.2","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<p align="center">\n  <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"><b>Technical Report</b>üëÅÔ∏è</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* ü•á **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align="center">\n <img src="assets/benchmark.png" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a "thinking with tools" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model''s text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.2")\n\nmessages = [\n    {"role": "user", "content": "hello"},\n    {"role": "assistant", "content": "Hello! I am DeepSeek.", "reasoning_content": "thinking..."},\n    {"role": "user", "content": "1+1=?"}\n]\nencode_config = dict(thinking_mode="thinking", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>hello<ÔΩúAssistantÔΩú></think>Hello! I am DeepSeek.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output that the model might occasionally generate. It is not suitable for production use without robust error handling.\n3. A new role named `developer` has been introduced in the chat template. This role is dedicated exclusively to search agent scenarios and is designated for no other tasks. The official API does not accept messages assigned to `developer`.\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.2 and DeepSeek-V3.2-Speciale are the same as DeepSeek-V3.2-Exp. Please visit [DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) repo for more information about running this model locally.\n\nUsage Recommendations:\n\n1. For local deployment, we recommend setting the sampling parameters to `temperature = 1.0, top_p = 0.95`.\n2. Please note that the DeepSeek-V3.2-Speciale variant is designed exclusively for deep reasoning tasks and does not support the tool-calling functionality.\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2025deepseekv32,\n      title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689484423011,"files_count":192,"spaces_count":19,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.2-Exp","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp"}]', NULL, 'MIT', 'approved', 84, '2d91c9c620c9755ecf0a7f560674c78f', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3.2 from https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.2.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-neuphonic-neutts-air', 'huggingface--neuphonic--neutts-air', 'neutts-air', 'neuphonic', '--- license: apache-2.0 pipeline_tag: text-to-speech tags: - audio - speech - speech-language-models datasets: - amphion/Emilia-Dataset - neuphonic/emilia-yodas-english-neucodec --- üöÄ Spaces Demo, üîß Github Q8 GGUF version, Q4 GGUF version *Created by Neuphonic - building faster, smaller, on-device voice AI* State-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. ...', '["safetensors","gguf","qwen2","audio","speech","speech-language-models","text-to-speech","dataset:amphion/emilia-dataset","dataset:neuphonic/emilia-yodas-english-neucodec","license:apache-2.0","endpoints_compatible","region:us","conversational"]', 'text-to-speech', 795, 23147, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/neuphonic/neutts-air","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\npipeline_tag: text-to-speech\ntags:\n- audio\n- speech\n- speech-language-models\ndatasets:\n- amphion/Emilia-Dataset\n- neuphonic/emilia-yodas-english-neucodec\n---\n\n# NeuTTS Air ‚òÅÔ∏è \n\n[![NeuTTSAir_Intro](neutts-air.png)](https://www.youtube.com/watch?v=YAB3hCtu5wE)\n\n[üöÄ Spaces Demo](https://huggingface.co/spaces/neuphonic/neutts-air), [üîß Github](https://github.com/neuphonic/neutts-air)\n\n[Q8 GGUF version](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF version](https://huggingface.co/neuphonic/neutts-air-q4-gguf)\n\n*Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI*\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- üó£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- üì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- üë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- üöÑSimple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they''re not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! üôè\n\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our proprietary neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n1. **Clone the [Git Repo](https://github.com/neuphonic/neutts-air)**\n    \n    ```bash\n    git clone https://github.com/neuphonic/neutts-air.git\n    cd neuttsair\n    ```\n    \n2. **Install¬†`espeak`¬†(required dependency)**\n    \n    Please refer to the following link for instructions on how to install¬†`espeak`:\n    \n    https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n    \n    ```bash\n    # Mac OS\n    brew install espeak\n    \n    # Ubuntu/Debian\n    sudo apt install espeak\n\n    # Arch Linux\n    paru -S aur/espeak\n    ```\n    \n3. **Install Python dependencies**\n    \n    The requirements file includes the dependencies needed to run the model with PyTorch. When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n    \n    The inference is compatible and tested on¬†`python>=3.11`.\n    \n    ```\n    pip install -r requirements.txt\n    \n    ```\n    \n\n## **Basic Example**\n\nRun the basic example script to synthesize speech:\n\n```bash\npython -m examples.basic_example \\n  --input_text "My name is Dave, and um, I''m from London" \\n  --ref_audio samples/dave.wav \\n  --ref_text samples/dave.txt\n\n```\n\nTo specify a particular model repo for the backbone or codec, add the¬†`--backbone`¬†argument. Available backbones are listed in¬†[NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4c56197ef350).\n\nSeveral examples are available, including a Jupyter notebook in the¬†`examples`¬†folder.\n\n### **Simple One-Code Block Usage**\n\n```python\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\n\ntts = NeuTTSAir( backbone_repo="neuphonic/neutts-air-q4-gguf", backbone_device="cpu", codec_repo="neuphonic/neucodec", codec_device="cpu")\ninput_text = "My name is Dave, and um, I''m from London."\n\nref_text = "samples/dave.txt"\nref_audio_path = "samples/dave.wav"\n\nref_text = open(ref_text, "r").read().strip()\nref_codes = tts.encode_reference(ref_audio_path)\n\nwav = tts.infer(input_text, ref_codes, ref_text)\nsf.write("test.wav", wav, 24000)\n\n```\n\n# Tips\n\nNeuTTS Air requires two inputs:\n\n1. A reference audio sample (`.wav` file)\n2. A text string\n\nThe model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS Air‚Äôs instant voice cloning capability.\n\n### Example Reference Files\n\nYou can find some ready-to-use samples in the `examples` folder:\n\n- `samples/dave.wav`\n- `samples/jo.wav`\n\n### Guidelines for Best Results\n\nFor optimal performance, reference audio samples should be:\n\n1. **Mono channel**\n2. **16-44 kHz sample rate**\n3. **3‚Äì15 seconds in length**\n4. **Saved as a `.wav` file**\n5. **Clean** ‚Äî minimal to no background noise\n6. **Natural, continuous speech** ‚Äî like a monologue or conversation, with few pauses, so the model can capture tone effectively\n\n# **Responsibility**\n\nEvery audio file generated by NeuTTS Air includes [**Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth).**\n\n# **Disclaimer**\n\nDon''t use this model to do bad things‚Ä¶ please.', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":10621210568,"files_count":11,"spaces_count":6,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2"}}', '[]', '[{"type":"has_code","target_id":"github:neuphonic:neutts-air","source_url":"https://github.com/neuphonic/neutts-air"},{"type":"has_code","target_id":"github:neuphonic:neutts-air","source_url":"https://github.com/neuphonic/neutts-air"},{"type":"has_code","target_id":"github:neuphonic:neutts-air.git","source_url":"https://github.com/neuphonic/neutts-air.git"},{"type":"has_code","target_id":"github:espeak-ng:espeak-ng","source_url":"https://github.com/espeak-ng/espeak-ng"},{"type":"has_code","target_id":"github:resemble-ai:perth","source_url":"https://github.com/resemble-ai/perth"}]', NULL, 'Apache-2.0', 'approved', 64, '7e5341b3484182e6c9cadf4535088ded', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-8B', 'huggingface--qwen--qwen3-8b', 'Qwen3-8B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-8B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-8B-Base --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering ...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2309.00071","arxiv:2505.09388","base_model:qwen/qwen3-8b-base","base_model:finetune:qwen/qwen3-8b-base","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 792, 4761786, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-8B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-8B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-8B-Base\n---\n\n# Qwen3-8B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-8B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 8.2B\n- Number of Paramaters (Non-Embedding): 6.95B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-8B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-8B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-8B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model''s performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}}''\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for ''rope_type''=''yarn'': {''original_max_position_embeddings''}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8190735360,"storage_bytes":16392939430,"files_count":15,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 79, 'b56050969c84dc863a6cb404526f51ee', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-dx8152-Qwen-Edit-2509-Multiple-angles', 'huggingface--dx8152--qwen-edit-2509-multiple-angles', 'Qwen-Edit-2509-Multiple-angles', 'dx8152', '--- license: apache-2.0 base_model: - Qwen/Qwen-Image-Edit-2509 pipeline_tag: image-to-image tags: - lora library_name: diffusers --- This model is trained (code-free!) on ModelScope. Thanks to ModelScope team for providing the training infra: https://www.modelscope.cn/aigc/modelTraining ------- Updated 2025/11/2: Some people mentioned that the model has an unstable consistency issue. I have re-uploaded a version with more training iterations, hoping to fix the consistency problem. Welcome ev...', '["diffusers","lora","image-to-image","base_model:qwen/qwen-image-edit-2509","base_model:adapter:qwen/qwen-image-edit-2509","license:apache-2.0","region:us"]', 'image-to-image', 792, 89772, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/dx8152/Qwen-Edit-2509-Multiple-angles","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\npipeline_tag: image-to-image\ntags:\n- lora\nlibrary_name: diffusers\n---\n\nThis model is trained (code-free!) on ModelScope. Thanks to ModelScope team for providing the training infra:\n\nhttps://www.modelscope.cn/aigc/modelTraining\n\n\n-------\n\n\nUpdated 2025/11/2: Some people mentioned that the model has an unstable consistency issue. I have re-uploaded a version with more training iterations, hoping to fix the consistency problem.\n\nWelcome everyone to use Lora of Qwen-Edit-2509, its performance is very amazingÔºÅ\n\n\nThere are no trigger words. You can control the camera to move up, down, left, and right, as well as rotate it to the left and right. You can also look down or up. The camera can be changed to a wide-angle or close-up shot.\n\nOnline running linkÔºö www.runninghub.ai/post/1985311204883243009?inviteCode=rh-v1331\n\nThis is a user guide: \n\nYouTubeÔºöhttps://youtu.be/UGdW8W1MqW8\n\nBlibiliÔºöhttps://www.bilibili.com/video/BV1oi1gBBEZV/\n\n‚Äú\nÂ∞ÜÈïúÂ§¥ÂêëÂâçÁßªÂä®ÔºàMove the camera forward.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂ∑¶ÁßªÂä®ÔºàMove the camera left.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂè≥ÁßªÂä®ÔºàMove the camera right.Ôºâ\nÂ∞ÜÈïúÂ§¥Âêë‰∏ãÁßªÂä®ÔºàMove the camera down.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂ∑¶ÊóãËΩ¨45Â∫¶ÔºàRotate the camera 45 degrees to the left.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂè≥ÊóãËΩ¨45Â∫¶ÔºàRotate the camera 45 degrees to the right.Ôºâ\nÂ∞ÜÈïúÂ§¥ËΩ¨‰∏∫‰øØËßÜÔºàTurn the camera to a top-down view.Ôºâ\nÂ∞ÜÈïúÂ§¥ËΩ¨‰∏∫ÂπøËßíÈïúÂ§¥ÔºàTurn the camera to a wide-angle lens.Ôºâ\nÂ∞ÜÈïúÂ§¥ËΩ¨‰∏∫ÁâπÂÜôÈïúÂ§¥ÔºàTurn the camera to a close-up.Ôºâ\n...\nThere are many possibilities; you can try them yourself.\n‚Äù\n------\n\nInstructions: Download the lora file to the models/loras folder.\n\nYou also need this lora and use them together: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main\n\n\nFor communication/cooperation, you can join the discord group to communicateÔºö https://discord.gg/yVAVa43mWk\n\nIf these resources are helpful to you, or if you use them for business purposes, please buy me a coffee. Thank you for supporting original content! PayPal: Daniel8152\n\n\n\n![ÊïàÊûúÂõæ](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/tCsqZOi1YQtSj9qBKfbK1.jpeg)\n![ÊïàÊûúÂõæ2](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/AC0KJK5F5gETFy1M-ZIKj.jpeg)\n![ÊïàÊûúÂõæ3](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/cr0XKCaPVLlcFDjRbQNdc.jpeg)\nI saw some people wanted to see a comparison between LoRa and the model''s inherent capabilities, so I conducted a test. The trained model is indeed more powerful and intelligent, while the original model already possessed certain abilities. It''s very powerful, which is why we love this model, and precisely because of this, we need to explore its potential.\n![ÂØπÊØîÂõæ](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/yFk1v6tQuOV0jlGUjpU2S.jpeg)', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":478911124,"files_count":4,"spaces_count":65,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 64, '33904eecae1288d1c702d1c09f2225f1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-metavoiceio-metavoice-1B-v0.1', 'huggingface--metavoiceio--metavoice-1b-v0.1', 'metavoice-1B-v0.1', 'metavoiceio', '--- license: apache-2.0 language: - en tags: - pretrained - text-to-speech library_name: metavoice inference: false --- MetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities: * Emotional speech rhythm and tone in English. No hallucinations. * Support for voice cloning with finetuning. * We have had success with as little as 1 minute training data for Indian speakers. * Zero-shot cloning for America...', '["metavoice","pretrained","text-to-speech","en","license:apache-2.0","region:us"]', 'text-to-speech', 790, 150, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/metavoiceio/metavoice-1B-v0.1","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n  - pretrained\n  - text-to-speech\nlibrary_name: metavoice\ninference: false\n---\n\nMetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities:\n* Emotional speech rhythm and tone in English. No hallucinations.\n* Support for voice cloning with finetuning.\n  * We have had success with as little as 1 minute training data for Indian speakers.\n* Zero-shot cloning for American & British voices, with 30s reference audio.\n* Support for long-form synthesis.\n\nWe‚Äôre releasing MetaVoice-1B under the Apache 2.0 license, *it can be used without restrictions*.\n\n## Usage\nSee [Github](https://github.com/metavoiceio/metavoice-src) for the latest usage instructions.\n\n## Finetuning\n\nSee [Github](https://github.com/metavoiceio/metavoice-src?tab=readme-ov-file#finetuning) for the latest finetuning instructions.\n\n## Soon\n- Long form / arbitrary length TTS\n- Streaming\n\n## Architecture\nWe predict EnCodec tokens from text, and speaker information. This is then diffused up to the waveform level, with post-processing applied to clean up the audio.\n\n* We use a causal GPT to predict the first two hierarchies of EnCodec tokens. Text and audio are part of the LLM context. Speaker information is passed via conditioning at the token embedding layer. This speaker conditioning is obtained from a separately trained speaker verification network.\n  - The two hierarchies are predicted in a "flattened interleaved" manner, we predict the first token of the first hierarchy, then the first token of the second hierarchy, then the second token of the first hierarchy, and so on.\n  - We use condition-free sampling to boost the cloning capability of the model.\n  - The text is tokenised using a custom trained BPE tokeniser with 512 tokens.\n  - Note that we''ve skipped predicting semantic tokens as done in other works, as we found that this isn''t strictly necessary.\n* We use a non-causal (encoder-style) transformer to predict the rest of the 6 hierarchies from the first two hierarchies. This is a super small model (~10Mn parameters), and has extensive zero-shot generalisation to most speakers we''ve tried. Since it''s non-causal, we''re also able to predict all the timesteps in parallel.\n* We use multi-band diffusion to generate waveforms from the EnCodec tokens. We noticed that the speech is clearer than using the original RVQ decoder or VOCOS. However, the diffusion at waveform level leaves some background artifacts which are quite unpleasant to the ear. We clean this up in the next step.\n* We use DeepFilterNet to clear up the artifacts introduced by the multi-band diffusion. \n\n## Optimizations\nThe model supports: \n1. KV-caching via Flash Decoding \n2. Batching (including texts of different lengths)\n', '{"pipeline_tag":"text-to-speech","library_name":"metavoice","framework":"metavoice","params":null,"storage_bytes":5047828383,"files_count":6,"spaces_count":11,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:metavoiceio:metavoice-src","source_url":"https://github.com/metavoiceio/metavoice-src"},{"type":"has_code","target_id":"github:metavoiceio:metavoice-src","source_url":"https://github.com/metavoiceio/metavoice-src?tab=readme-ov-file#finetuning"}]', NULL, 'Apache-2.0', 'approved', 64, 'bad0ac2f3b44080e8d2f747164a6a22d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-Hunyuan-A13B-Instruct', 'huggingface--tencent--hunyuan-a13b-instruct', 'Hunyuan-A13B-Instruct', 'tencent', '--- license: other license_name: tencent-hunyuan-a13b license_link: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE library_name: transformers --- <p align="center"> <img src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png" width="400"/> <br> </p><p></p> <p align="center"> ü§ó&nbsp;<a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp; üñ•Ô∏è&nbsp;<a href="https://huny...', '["transformers","safetensors","hunyuan_v1_moe","text-generation","conversational","custom_code","license:other","endpoints_compatible","region:us"]', 'text-generation', 785, 9394, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/Hunyuan-A13B-Instruct","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: tencent-hunyuan-a13b\nlicense_link: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE\nlibrary_name: transformers\n---\n\n<p align="center">\n <img src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png" width="400"/> <br>\n</p><p></p>\n\n\n<p align="center">\n    ü§ó&nbsp;<a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    üñ•Ô∏è&nbsp;<a href="https://hunyuan.tencent.com" style="color: red;"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    üïñ&nbsp;<a href="https://cloud.tencent.com/product/hunyuan"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    üïπÔ∏è&nbsp;<a href="https://hunyuan.tencent.com/?model=hunyuan-a13b"><b>Demo</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    ü§ñ&nbsp;<a href="https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct"><b>ModelScope</b></a>\n</p>\n\n\n<p align="center">\n    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf"><b>Technical Report</b> </a> |\n    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B"><b>GITHUB</b></a> | \n    <a href="https://cnb.cool/tencent/hunyuan/Hunyuan-A13B"><b>cnb.cool</b></a> | \n    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE"><b>LICENSE</b></a> | \n    <a href="https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/assets/1751881231452.jpg"><b>WeChat</b></a> | \n    <a href="https://discord.gg/bsPcMEtV7v"><b>Discord</b></a>\n</p>\n\n\n  \nWelcome to the official repository of **Hunyuan-A13B**, an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture. Designed for efficiency and scalability, Hunyuan-A13B delivers cutting-edge performance with minimal computational overhead, making it an ideal choice for advanced reasoning and general-purpose applications, especially in resource-constrained environments.\n\n## Model Introduction\n\nWith the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.\n\n### Key Features and Advantages\n\n- **Compact yet Powerful**: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.\n- **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n- **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n- **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, œÑ-Bench and C3-Bench.\n- **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\n### Why Choose Hunyuan-A13B?\n\nAs a powerful yet computationally efficient large model, Hunyuan-A13B is an ideal choice for researchers and developers seeking high performance under resource constraints. Whether for academic research, cost-effective AI solution development, or innovative application exploration, this model provides a robust foundation for advancement.\n\n&nbsp;\n\n## Related News\n* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face. In addition, we have released a <a href="report/Hunyuan_A13B_Technical_Report.pdf">technical report </a> and a training and inference operation manual, which provide detailed information about the model‚Äôs capabilities as well as the operations for training and inference.\n\n<br>\n\n\n## Benchmark\n\nNote: The following benchmarks are evaluated by TRT-LLM-backend on several **base models**. \n\n| Model            | Hunyuan-Large | Qwen2.5-72B  | Qwen3-A22B | Hunyuan-A13B |\n|------------------|---------------|--------------|-------------|---------------|\n| MMLU             | 88.40          | 86.10         | 87.81        | 88.17          |\n| MMLU-Pro         | 60.20          | 58.10        | 68.18           | 67.23          |\n| MMLU-Redux              |  87.47         | 83.90         | 87.40        | 87.67          |\n| BBH        | 86.30             | 85.80            | 88.87        | 87.56          |\n| SuperGPQA    |  38.90         | 36.20          | 44.06           | 41.32          |\n| EvalPlus       | 75.69          | 65.93         | 77.60        | 78.64          |\n| MultiPL-E             | 59.13             | 60.50            | 65.94        | 69.33          |\n| MBPP | 72.60             | 76.00            | 81.40        | 83.86          |\n| CRUX-I             | 57.00          | 57.63          | -        | 70.13          |\n| CRUX-O             | 60.63          | 66.20          | 79.00        | 77.00          |\n| MATH            | 69.80          | 62.12         | 71.84        | 72.35          |\n| CMATH            | 91.30          | 84.80         | -        | 91.17          |\n| GSM8k         | 92.80             | 91.50           | 94.39        | 91.83          |\n| GPQA            | 25.18             | 45.90            | 47.47        | 49.12          |\n\n\nHunyuan-A13B-Instruct has achieved highly competitive performance across multiple benchmarks, particularly in mathematics, science, agent domains, and more. We compared it with several powerful models, and the results are shown below.\n\n| Topic               |                        Bench                         | OpenAI-o1-1217 | DeepSeek R1 | Qwen3-A22B | Hunyuan-A13B-Instruct |\n|:-------------------:|:----------------------------------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|\n| **Mathematics**     |            AIME 2024<br>AIME 2025<br>MATH            | 74.3<br>79.2<br>96.4 | 79.8<br>70<br>94.9 | 85.7<br>81.5<br>94.0 | 87.3<br>76.8<br>94.3 |\n| **Science**         |            GPQA-Diamond<br>OlympiadBench             | 78<br>83.1 | 71.5<br>82.4 | 71.1<br>85.7 | 71.2<br>82.7 |\n| **Coding**          |  Livecodebench<br>Fullstackbench<br>ArtifactsBench   | 63.9<br>64.6<br>38.6 | 65.9<br>71.6<br>44.6 | 70.7<br>65.6<br>44.6 | 63.9<br>67.8<br>43 |\n| **Reasoning**       |              BBH<br>DROP<br>ZebraLogic               | 80.4<br>90.2<br>81 | 83.7<br>92.2<br>78.7 | 88.9<br>90.3<br>80.3 | 89.1<br>91.1<br>84.7 |\n| **Instruction<br>Following** |                 IF-Eval<br>SysBench                  | 91.8<br>82.5 | 88.3<br>77.7 | 83.4<br>74.2 | 84.7<br>76.1 |\n| **Text<br>Creation**|                LengthCtrl<br>InsCtrl                 | 60.1<br>74.8 | 55.9<br>69 | 53.3<br>73.7 | 55.4<br>71.9 |\n| **NLU**             |               ComplexNLU<br>Word-Task                | 64.7<br>67.1 | 64.5<br>76.3 | 59.8<br>56.4 | 61.2<br>62.9 |\n| **Agent**           | BFCL v3<br> œÑ-Bench<br>ComplexFuncBench<br> C3-Bench | 67.8<br>60.4<br>47.6<br>58.8 | 56.9<br>43.8<br>41.1<br>55.3 | 70.8<br>44.6<br>40.6<br>51.7 | 78.3<br>54.7<br>61.2<br>63.5 |\n\n\n&nbsp;\n\n## Use with transformers\n\nOur model defaults to using slow-thinking reasoning, and there are two ways to disable CoT reasoning. \n1. Pass "enable_thinking=False" when calling apply_chat_template.\n2. Adding "/no_think" before the prompt will force the model not to use perform CoT reasoning. Similarly, adding "/think" before the prompt will force the model to perform CoT reasoning.\n\nThe following code snippet shows how to use the transformers library to load and apply the model. \nIt also demonstrates how to enable and disable the reasoning mode , \nand how to parse the reasoning process along with the final output.\n\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport re\n\nmodel_name_or_path = os.environ[''MODEL_PATH'']\n# model_name_or_path = "tencent/Hunyuan-A13B-Instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto",trust_remote_code=True)  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},\n]\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=True\n            )\n\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\nmodel_inputs.pop("token_type_ids", None)\noutputs = model.generate(**model_inputs, max_new_tokens=4096)\n\n\noutput_text = tokenizer.decode(outputs[0])\n\nthink_pattern = r''<think>(.*?)</think>''\nthink_matches = re.findall(think_pattern, output_text, re.DOTALL)\n\nanswer_pattern = r''<answer>(.*?)</answer>''\nanswer_matches = re.findall(answer_pattern, output_text, re.DOTALL)\n\nthink_content = [match.strip() for match in think_matches][0]\nanswer_content = [match.strip() for match in answer_matches][0]\nprint(f"thinking_content:{think_content}\n\n")\nprint(f"answer_content:{answer_content}\n\n")\n```\n\n### Fast and slow thinking switch\n\nThis model supports two modes of operation:\n\n- Slow Thinking Mode (Default): Enables detailed internal reasoning steps before producing the final answer.\n- Fast Thinking Mode: Skips the internal reasoning process for faster inference, going straight to the final answer.\n\n**Switching to Fast Thinking Mode:**\n\nTo disable the reasoning process, set `enable_thinking=False` in the apply_chat_template call:\n```\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=False\n            )\n```                                           \n\n\n\n## Deployment   \n\nFor deployment, you can use frameworks such as **TensorRT-LLM**, **vLLM**, or **SGLang** to serve the model and create an OpenAI-compatible API endpoint.\n\nimage: https://hub.docker.com/r/hunyuaninfer/hunyuan-a13b/tags \n\n\n### TensorRT-LLM\n\n#### Docker Image \n\nWe provide a pre-built Docker image based on the latest version of TensorRT-LLM.\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ "CNB.cool")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\nThen, rename the image to better align with the following scripts:\n```\n\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n\n- start docker \n\n```\ndocker run --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n- Prepare Configuration file:\n\n```\ncat >/path/to/extra-llm-api-config.yml <<EOF\nuse_cuda_graph: true\ncuda_graph_padding_enabled: true\ncuda_graph_batch_sizes:\n- 1\n- 2\n- 4\n- 8\n- 16\n- 32\nprint_iter_log: true\nEOF\n```\n\n\n- Start the API server:\n\n\n```\ntrtllm-serve \\n  /path/to/HunYuan-moe-A13B \\n  --host localhost \\n  --port 8000 \\n  --backend pytorch \\n  --max_batch_size 32 \\n  --max_num_tokens 16384 \\n  --tp_size 2 \\n  --kv_cache_free_gpu_memory_fraction 0.6 \\n  --trust_remote_code \\n  --extra_llm_api_options /path/to/extra-llm-api-config.yml\n```\n\n\n### vLLM\n\n#### Inference from Docker Image\nWe provide a pre-built Docker image containing vLLM 0.8.5 with full support for this model. The official vllm release is currently under developmentÔºå **note: cuda 12.4 is require for this docker**.\n\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ "CNB.cool")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1\n```\n\nThen, rename the image to better align with the following scripts:\n```\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \n```\n\n- Download Model file: \n  - Huggingface:  will download automicly by vllm.\n  - ModelScope: `modelscope download --model Tencent-Hunyuan/Hunyuan-A13B-Instruct`\n \n\n- Start the API server:\n\nmodel download by huggingface:\n```\ndocker run --rm  --ipc=host \\n        -v ~/.cache:/root/.cache/ \\n        --security-opt seccomp=unconfined \\n        --net=host \\n        --gpus=all \\n        -it \\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\n        -m vllm.entrypoints.openai.api_server \\n        --host 0.0.0.0 \\n        --tensor-parallel-size 4 \\n        --port 8000 \\n        --model tencent/Hunyuan-A13B-Instruct  \\n        --trust_remote_code\n``` \n\nmodel downloaded by modelscope:\n```\ndocker run --rm  --ipc=host \\n        -v ~/.cache/modelscope:/root/.cache/modelscope \\n        --security-opt seccomp=unconfined \\n        --net=host \\n        --gpus=all \\n        -it \\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\n        -m vllm.entrypoints.openai.api_server \\n        --host 0.0.0.0 \\n        --tensor-parallel-size 4 \\n        --port 8000 \\n        --model /root/.cache/modelscope/hub/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct/  \\n        --trust_remote_code\n```\n\n### Source Code\nSupport for this model has been added via  this [PR 20114](https://github.com/vllm-project/vllm/pull/20114 ) in the vLLM project, \nThis patch already been merged by community at Jul-1-2025.\n\nYou can build and run vLLM from source using code after `ecad85`.\n\n### Model Context Length Support\n\nThe Hunyuan A13B model supports a maximum context length of **256K tokens (262,144 tokens)**. However, due to GPU memory constraints on most hardware setups, the default configuration in `config.json` limits the context length to **32K tokens** to prevent out-of-memory (OOM) errors.\n\n#### Extending Context Length to 256K\n\nTo enable full 256K context support, you can manually modify the `max_position_embeddings` field in the model''s `config.json` file as follows:\n\n```json\n{\n  ...\n  "max_position_embeddings": 262144,\n  ...\n}\n```\n\nWhen serving the model using **vLLM**, you can also explicitly set the maximum model length by adding the following flag to your server launch command:\n\n```bash\n--max-model-len 262144\n```\n\n#### Recommended Configuration for 256K Context Length\n\nThe following configuration is recommended for deploying the model with 256K context length support on systems equipped with **NVIDIA H20 GPUs (96GB VRAM)**:\n\n| Model DType    | KV-Cache Dtype | Number of Devices | Model Length |\n|----------------|----------------|--------------------|--------------|\n| `bfloat16`     | `bfloat16`     | 4                  | 262,144      |\n\n> ‚ö†Ô∏è **Note:** Using FP8 quantization for KV-cache may impact generation quality. The above settings are suggested configurations for stable 256K-length service deployment.\n\n\n#### Tool Calling with vLLM\n\nTo support agent-based workflows and function calling capabilities, this model includes specialized parsing mechanisms for handling tool calls and internal reasoning steps.\n\nFor a complete working example of how to implement and use these features in an agent setting, please refer to our full agent implementation on GitHub:  \nüîó [Hunyuan A13B Agent Example](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/)\n\nWhen deploying the model using **vLLM**, the following parameters can be used to configure the tool parsing behavior:\n\n| Parameter                | Value                                                                 |\n|--------------------------|-----------------------------------------------------------------------|\n| `--tool-parser-plugin`   | [Local Hunyuan A13B Tool Parser File](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/hunyuan_tool_parser.py) |\n| `--tool-call-parser`     | `hunyuan`                                                            |\n\nThese settings enable vLLM to correctly interpret and route tool calls generated by the model according to the expected format.\n\n### Reasoning parser\n\nvLLM reasoning parser support on Hunyuan A13B model is under development.\n\n\n\n### SGLang\n\n#### Docker Image \n\nWe also provide a pre-built Docker image based on the latest version of SGLang.\n\nTo get started:\n\n- Pull the Docker image\n\n```\ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-sglang\nor\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-sglang\n```\n\n- Start the API server:\n\n```\ndocker run --gpus all \\n    --shm-size 32g \\n    -p 30000:30000 \\n    --ipc=host \\n    docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-sglang \\n    -m sglang.launch_server --model-path hunyuan/huanyuan_A13B --tp 4 --trust-remote-code --host 0.0.0.0 --port 30000\n```\n\n## Contact Us\n\nIf you would like to leave a message for our R&D and product teams, Welcome to contact our open-source team . You can also contact us via email (hunyuan_opensource@tencent.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":80393183232,"storage_bytes":482436254851,"files_count":46,"spaces_count":1,"gated":false,"private":false,"config":{"architectures":["HunYuanMoEV1ForCausalLM"],"auto_map":{"AutoConfig":"configuration_hunyuan.HunYuanConfig","AutoModel":"hunyuan.HunYuanModel","AutoModelForCausalLM":"hunyuan.HunYuanMoEV1ForCausalLM"},"model_type":"hunyuan_v1_moe","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|eos|>","pad_token":"<|pad|>","chat_template":"{% set context = {''has_head'': true} %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message[''content''] %}{% if loop.index0 == 0 %}{% if content == '''' %}{% set _ = context.update({''has_head'': false}) %}{% elif message[''role''] == ''system'' %}{% set content = ''<|startoftext|>'' + content + ''<|extra_4|>'' %}{% endif %}{% endif %}{% if message[''role''] == ''user'' %}{% if loop.index0 == 1 and not context.has_head %}{% set content = ''<|startoftext|>'' + content %}{% endif %}{% if loop.index0 == 1 and context.has_head %}{% set content = content + ''<|extra_0|>'' %}{% else %}{% set content = ''<|startoftext|>'' + content + ''<|extra_0|>'' %}{% endif %}{% elif message[''role''] == ''assistant'' %}{% set content = content + ''<|eos|>'' %}{% endif %}{{ content }}{% endfor %}"}}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B\"><b>GITHUB<","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"><b>GITHUB<"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"}]', NULL, 'Other', 'approved', 79, 'dede00382f06dde99dc858cc4d95f0fb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen-7B-Chat', 'huggingface--qwen--qwen-7b-chat', 'Qwen-7B-Chat', 'Qwen', '--- language: - zh - en tags: - qwen pipeline_tag: text-generation inference: false license: other license_name: tongyi-qianwen-license-agreement license_link: https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT --- <p align="center"> <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/> <p> <br> <p align="center"> ü§ó <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organ...', '["transformers","safetensors","qwen","text-generation","custom_code","zh","en","arxiv:2309.16609","arxiv:2305.08322","arxiv:2009.03300","arxiv:2305.05280","arxiv:2210.03629","license:other","region:us"]', 'text-generation', 784, 88838, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen-7B-Chat","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- zh\n- en\ntags:\n- qwen\npipeline_tag: text-generation\ninference: false\nlicense: other\nlicense_name: tongyi-qianwen-license-agreement\nlicense_link: https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT\n---\n\n# Qwen-7B-Chat\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>\n<p>\n<br>\n\n<p align="center">\n        ü§ó <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ÔΩú &nbsp&nbspüñ•Ô∏è <a href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">Demo</a>\n<br>\n<a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp ÔΩú  &nbsp&nbsp<a href="https://dashscope.aliyun.com">API</a> \n</p>\n<br>\n\n\n## ‰ªãÁªçÔºàIntroductionÔºâ\n\n**ÈÄö‰πâÂçÉÈóÆ-7BÔºàQwen-7BÔºâ**ÊòØÈòøÈáå‰∫ëÁ†îÂèëÁöÑÈÄö‰πâÂçÉÈóÆÂ§ßÊ®°ÂûãÁ≥ªÂàóÁöÑ70‰∫øÂèÇÊï∞ËßÑÊ®°ÁöÑÊ®°Âûã„ÄÇQwen-7BÊòØÂü∫‰∫éTransformerÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã, Âú®Ë∂ÖÂ§ßËßÑÊ®°ÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÂæóÂà∞„ÄÇÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÁ±ªÂûãÂ§öÊ†∑ÔºåË¶ÜÁõñÂπøÊ≥õÔºåÂåÖÊã¨Â§ßÈáèÁΩëÁªúÊñáÊú¨„ÄÅ‰∏ì‰∏ö‰π¶Á±ç„ÄÅ‰ª£Á†ÅÁ≠â„ÄÇÂêåÊó∂ÔºåÂú®Qwen-7BÁöÑÂü∫Á°Ä‰∏äÔºåÊàë‰ª¨‰ΩøÁî®ÂØπÈΩêÊú∫Âà∂ÊâìÈÄ†‰∫ÜÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑAIÂä©ÊâãQwen-7B-Chat„ÄÇÁõ∏ËæÉ‰∫éÊúÄÂàùÂºÄÊ∫êÁöÑQwen-7BÊ®°ÂûãÔºåÊàë‰ª¨Áé∞Â∑≤Â∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂíåChatÊ®°ÂûãÊõ¥Êñ∞Âà∞ÊïàÊûúÊõ¥‰ºòÁöÑÁâàÊú¨„ÄÇÊú¨‰ªìÂ∫ì‰∏∫Qwen-7B-ChatÁöÑ‰ªìÂ∫ì„ÄÇ\n\nÂ¶ÇÊûúÊÇ®ÊÉ≥‰∫ÜËß£Êõ¥Â§öÂÖ≥‰∫éÈÄö‰πâÂçÉÈóÆ-7BÂºÄÊ∫êÊ®°ÂûãÁöÑÁªÜËäÇÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®ÂèÇÈòÖ[GitHub‰ª£Á†ÅÂ∫ì](https://github.com/QwenLM/Qwen)„ÄÇ\n\n**Qwen-7B** is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-7B, we release Qwen-7B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. Now we have updated both our pretrained and chat models with better performances. This repository is the one for Qwen-7B-Chat.\n\nFor more details about Qwen, please refer to the [GitHub](https://github.com/QwenLM/Qwen) code repository.\n<br>\n\n## Ë¶ÅÊ±ÇÔºàRequirementsÔºâ\n\n* python 3.8Âèä‰ª•‰∏äÁâàÊú¨\n* pytorch 1.12Âèä‰ª•‰∏äÁâàÊú¨ÔºåÊé®Ëçê2.0Âèä‰ª•‰∏äÁâàÊú¨\n* Âª∫ËÆÆ‰ΩøÁî®CUDA 11.4Âèä‰ª•‰∏äÔºàGPUÁî®Êà∑„ÄÅflash-attentionÁî®Êà∑Á≠âÈúÄËÄÉËôëÊ≠§ÈÄâÈ°πÔºâ\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n<br>\n\n## ‰æùËµñÈ°πÔºàDependencyÔºâ\n\nËøêË°åQwen-7B-ChatÔºåËØ∑Á°Æ‰øùÊª°Ë∂≥‰∏äËø∞Ë¶ÅÊ±ÇÔºåÂÜçÊâßË°å‰ª•‰∏ãpipÂëΩ‰ª§ÂÆâË£Ö‰æùËµñÂ∫ì\n\nTo run Qwen-7B-Chat, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries.\n\n```bash\npip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n```\n\nÂè¶Â§ñÔºåÊé®ËçêÂÆâË£Ö`flash-attention`Â∫ìÔºà**ÂΩìÂâçÂ∑≤ÊîØÊåÅflash attention 2**ÔºâÔºå‰ª•ÂÆûÁé∞Êõ¥È´òÁöÑÊïàÁéáÂíåÊõ¥‰ΩéÁöÑÊòæÂ≠òÂç†Áî®„ÄÇ\n\nIn addition, it is recommended to install the `flash-attention` library (**we support flash attention 2 now.**) for higher efficiency and lower memory usage.\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# ‰∏ãÊñπÂÆâË£ÖÂèØÈÄâÔºåÂÆâË£ÖÂèØËÉΩÊØîËæÉÁºìÊÖ¢„ÄÇ\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n<br>\n\n## Âø´ÈÄü‰ΩøÁî®ÔºàQuickstartÔºâ\n\n‰∏ãÈù¢Êàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏Ä‰∏™‰ΩøÁî®Qwen-7B-ChatÊ®°ÂûãÔºåËøõË°åÂ§öËΩÆÂØπËØù‰∫§‰∫íÁöÑÊ†∑‰æãÔºö\n\nWe show an example of multi-turn interaction with Qwen-7B-Chat in the following code:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # ÂèØÊåáÂÆö‰∏çÂêåÁöÑÁîüÊàêÈïøÂ∫¶„ÄÅtop_pÁ≠âÁõ∏ÂÖ≥Ë∂ÖÂèÇ\n\n# Á¨¨‰∏ÄËΩÆÂØπËØù 1st dialogue turn\nresponse, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=None)\nprint(response)\n# ‰Ω†Â•ΩÔºÅÂæàÈ´òÂÖ¥‰∏∫‰Ω†Êèê‰æõÂ∏ÆÂä©„ÄÇ\n\n# Á¨¨‰∫åËΩÆÂØπËØù 2nd dialogue turn\nresponse, history = model.chat(tokenizer, "ÁªôÊàëËÆ≤‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Â•ãÊñóÂàõ‰∏öÊúÄÁªàÂèñÂæóÊàêÂäüÁöÑÊïÖ‰∫ã„ÄÇ", history=history)\nprint(response)\n# ËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Â•ãÊñóÂàõ‰∏öÊúÄÁªàÂèñÂæóÊàêÂäüÁöÑÊïÖ‰∫ã„ÄÇ\n# ÊïÖ‰∫ãÁöÑ‰∏ª‰∫∫ÂÖ¨Âè´ÊùéÊòéÔºå‰ªñÊù•Ëá™‰∏Ä‰∏™ÊôÆÈÄöÁöÑÂÆ∂Â∫≠ÔºåÁà∂ÊØçÈÉΩÊòØÊôÆÈÄöÁöÑÂ∑•‰∫∫„ÄÇ‰ªéÂ∞èÔºåÊùéÊòéÂ∞±Á´ã‰∏ã‰∫Ü‰∏Ä‰∏™ÁõÆÊ†áÔºöË¶ÅÊàê‰∏∫‰∏ÄÂêçÊàêÂäüÁöÑ‰ºÅ‰∏öÂÆ∂„ÄÇ\n# ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºåÊùéÊòéÂã§Â•ãÂ≠¶‰π†ÔºåËÄÉ‰∏ä‰∫ÜÂ§ßÂ≠¶„ÄÇÂú®Â§ßÂ≠¶ÊúüÈó¥Ôºå‰ªñÁßØÊûÅÂèÇÂä†ÂêÑÁßçÂàõ‰∏öÊØîËµõÔºåËé∑Âæó‰∫Ü‰∏çÂ∞ëÂ•ñÈ°π„ÄÇ‰ªñËøòÂà©Áî®ËØæ‰ΩôÊó∂Èó¥ÂéªÂÆû‰π†ÔºåÁßØÁ¥Ø‰∫ÜÂÆùË¥µÁöÑÁªèÈ™å„ÄÇ\n# ÊØï‰∏öÂêéÔºåÊùéÊòéÂÜ≥ÂÆöÂºÄÂßãËá™Â∑±ÁöÑÂàõ‰∏ö‰πãË∑Ø„ÄÇ‰ªñÂºÄÂßãÂØªÊâæÊäïËµÑÊú∫‰ºöÔºå‰ΩÜÂ§öÊ¨°ÈÉΩË¢´ÊãíÁªù‰∫Ü„ÄÇÁÑ∂ËÄåÔºå‰ªñÂπ∂Ê≤°ÊúâÊîæÂºÉ„ÄÇ‰ªñÁªßÁª≠Âä™ÂäõÔºå‰∏çÊñ≠ÊîπËøõËá™Â∑±ÁöÑÂàõ‰∏öËÆ°ÂàíÔºåÂπ∂ÂØªÊâæÊñ∞ÁöÑÊäïËµÑÊú∫‰ºö„ÄÇ\n# ÊúÄÁªàÔºåÊùéÊòéÊàêÂäüÂú∞Ëé∑Âæó‰∫Ü‰∏ÄÁ¨îÊäïËµÑÔºåÂºÄÂßã‰∫ÜËá™Â∑±ÁöÑÂàõ‰∏ö‰πãË∑Ø„ÄÇ‰ªñÊàêÁ´ã‰∫Ü‰∏ÄÂÆ∂ÁßëÊäÄÂÖ¨Âè∏Ôºå‰∏ìÊ≥®‰∫éÂºÄÂèëÊñ∞ÂûãËΩØ‰ª∂„ÄÇÂú®‰ªñÁöÑÈ¢ÜÂØº‰∏ãÔºåÂÖ¨Âè∏ËøÖÈÄüÂèëÂ±ïËµ∑Êù•ÔºåÊàê‰∏∫‰∫Ü‰∏ÄÂÆ∂ÊàêÂäüÁöÑÁßëÊäÄ‰ºÅ‰∏ö„ÄÇ\n# ÊùéÊòéÁöÑÊàêÂäüÂπ∂‰∏çÊòØÂÅ∂ÁÑ∂ÁöÑ„ÄÇ‰ªñÂã§Â•ã„ÄÅÂùöÈüß„ÄÅÂãá‰∫éÂÜíÈô©Ôºå‰∏çÊñ≠Â≠¶‰π†ÂíåÊîπËøõËá™Â∑±„ÄÇ‰ªñÁöÑÊàêÂäü‰πüËØÅÊòé‰∫ÜÔºåÂè™Ë¶ÅÂä™ÂäõÂ•ãÊñóÔºå‰ªª‰Ωï‰∫∫ÈÉΩÊúâÂèØËÉΩÂèñÂæóÊàêÂäü„ÄÇ\n\n# Á¨¨‰∏âËΩÆÂØπËØù 3rd dialogue turn\nresponse, history = model.chat(tokenizer, "ÁªôËøô‰∏™ÊïÖ‰∫ãËµ∑‰∏Ä‰∏™Ê†áÈ¢ò", history=history)\nprint(response)\n# „ÄäÂ•ãÊñóÂàõ‰∏öÔºö‰∏Ä‰∏™Âπ¥ËΩª‰∫∫ÁöÑÊàêÂäü‰πãË∑Ø„Äã\n```\n\nÂÖ≥‰∫éÊõ¥Â§öÁöÑ‰ΩøÁî®ËØ¥ÊòéÔºåËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑ[GitHub repo](https://github.com/QwenLM/Qwen)Ëé∑ÂèñÊõ¥Â§ö‰ø°ÊÅØ„ÄÇ\n\nFor more information, please refer to our [GitHub repo](https://github.com/QwenLM/Qwen) for more information.\n<br>\n\n## Tokenizer\n\n> Ê≥®Ôºö‰Ωú‰∏∫ÊúØËØ≠ÁöÑ‚Äútokenization‚ÄùÂú®‰∏≠Êñá‰∏≠Â∞öÊó†ÂÖ±ËØÜÁöÑÊ¶ÇÂøµÂØπÂ∫îÔºåÊú¨ÊñáÊ°£ÈááÁî®Ëã±ÊñáË°®Ëææ‰ª•Âà©ËØ¥Êòé„ÄÇ\n\nÂü∫‰∫étiktokenÁöÑÂàÜËØçÂô®ÊúâÂà´‰∫éÂÖ∂‰ªñÂàÜËØçÂô®ÔºåÊØîÂ¶ÇsentencepieceÂàÜËØçÂô®„ÄÇÂ∞§ÂÖ∂Âú®ÂæÆË∞ÉÈò∂ÊÆµÔºåÈúÄË¶ÅÁâπÂà´Ê≥®ÊÑèÁâπÊÆätokenÁöÑ‰ΩøÁî®„ÄÇÂÖ≥‰∫étokenizerÁöÑÊõ¥Â§ö‰ø°ÊÅØÔºå‰ª•ÂèäÂæÆË∞ÉÊó∂Ê∂âÂèäÁöÑÁõ∏ÂÖ≥‰ΩøÁî®ÔºåËØ∑ÂèÇÈòÖ[ÊñáÊ°£](https://github.com/QwenLM/Qwen/blob/main/tokenization_note_zh.md)„ÄÇ\n\nOur tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md).\n<br>\n\n## ÈáèÂåñ (Quantization)\n\n### Áî®Ê≥ï (Usage)\n\n**ËØ∑Ê≥®ÊÑèÔºöÊàë‰ª¨Êõ¥Êñ∞ÈáèÂåñÊñπÊ°à‰∏∫Âü∫‰∫é[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ÁöÑÈáèÂåñÔºåÊèê‰æõQwen-7B-ChatÁöÑInt4ÈáèÂåñÊ®°Âûã[ÁÇπÂáªËøôÈáå](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4)„ÄÇÁõ∏ÊØîÊ≠§ÂâçÊñπÊ°àÔºåËØ•ÊñπÊ°àÂú®Ê®°ÂûãËØÑÊµãÊïàÊûúÂá†‰πéÊó†ÊçüÔºå‰∏îÂ≠òÂÇ®ÈúÄÊ±ÇÊõ¥‰ΩéÔºåÊé®ÁêÜÈÄüÂ∫¶Êõ¥‰ºò„ÄÇ**\n\n**Note: we provide a new solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release an Int4 quantized model for Qwen-7B-Chat [Click here](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4), which achieves nearly lossless model effects but improved performance on both memory costs and inference speed, in comparison with the previous solution.**\n\n‰ª•‰∏ãÊàë‰ª¨Êèê‰æõÁ§∫‰æãËØ¥ÊòéÂ¶Ç‰Ωï‰ΩøÁî®Int4ÈáèÂåñÊ®°Âûã„ÄÇÂú®ÂºÄÂßã‰ΩøÁî®ÂâçÔºåËØ∑ÂÖà‰øùËØÅÊª°Ë∂≥Ë¶ÅÊ±ÇÔºàÂ¶Çtorch 2.0Âèä‰ª•‰∏äÔºåtransformersÁâàÊú¨‰∏∫4.32.0Âèä‰ª•‰∏äÔºåÁ≠âÁ≠âÔºâÔºåÂπ∂ÂÆâË£ÖÊâÄÈúÄÂÆâË£ÖÂåÖÔºö\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install auto-gptq optimum\n```\n\nÂ¶ÇÂÆâË£Ö`auto-gptq`ÈÅáÂà∞ÈóÆÈ¢òÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®Âà∞ÂÆòÊñπ[repo](https://github.com/PanQiWei/AutoGPTQ)ÊêúÁ¥¢ÂêàÈÄÇÁöÑÈ¢ÑÁºñËØëwheel„ÄÇ\n\nÈöèÂêéÂç≥ÂèØ‰ΩøÁî®Âíå‰∏äËø∞‰∏ÄËá¥ÁöÑÁî®Ê≥ïË∞ÉÁî®ÈáèÂåñÊ®°ÂûãÔºö\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a pre-build wheel.\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    "Qwen/Qwen-7B-Chat-Int4",\n    device_map="auto",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=None)\n```\n\n\n\n### ÊïàÊûúËØÑÊµã\n\nÊàë‰ª¨ÂØπBF16ÔºåInt8ÂíåInt4Ê®°ÂûãÂú®Âü∫ÂáÜËØÑÊµã‰∏äÂÅö‰∫ÜÊµãËØïÔºà‰ΩøÁî®zero-shotËÆæÁΩÆÔºâÔºåÂèëÁé∞ÈáèÂåñÊ®°ÂûãÊïàÊûúÊçüÂ§±ËæÉÂ∞èÔºåÁªìÊûúÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n\nWe illustrate the zero-shot performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n|  Quantization |   MMLU     |  CEval (val) |  GSM8K |  Humaneval |\n| ------------- | :--------: | :----------: | :----: | :--------: |\n| BF16          |    55.8    |     59.7     |  50.3  |    37.2    |\n| Int8          |    55.4    |     59.4     |  48.3  |    34.8    |\n| Int4          |    55.1    |     59.2     |  49.7  |    29.9    |\n\n### Êé®ÁêÜÈÄüÂ∫¶ (Inference Speed)\n\nÊàë‰ª¨ÊµãÁÆó‰∫Ü‰∏çÂêåÁ≤æÂ∫¶Ê®°Âûã‰ª•Âèä‰∏çÂêåFlashAttnÂ∫ìÁâàÊú¨‰∏ãÊ®°ÂûãÁîüÊàê2048Âíå8192‰∏™tokenÁöÑÂπ≥ÂùáÊé®ÁêÜÈÄüÂ∫¶„ÄÇÂ¶ÇÂõæÊâÄÁ§∫Ôºö\n\nWe measured the average inference speed of generating 2048 and 8192 tokens with different quantization levels and versions of flash-attention, respectively.\n\n|  Quantization | FlashAttn | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------- | :-------: | :------------------:| :------------------:|\n|      BF16     |   v2      | 40.93               | 36.14               |\n|      Int8     |   v2      | 37.47               | 32.54               |\n|      Int4     |   v2      | 50.09               | 38.61               |\n|      BF16     |   v1      | 40.75               | 35.34               |\n|      Int8     |   v1      | 37.51               | 32.39               |\n|      Int4     |   v1      | 45.98               | 36.47               |\n|      BF16     |  Disabled | 37.55               | 33.56               |\n|      Int8     |  Disabled | 37.84               | 32.65               |\n|      Int4     |  Disabled | 48.12               | 36.70               |\n\nÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ËÆ∞ÂΩïÂú®ÈïøÂ∫¶‰∏∫1ÁöÑ‰∏ä‰∏ãÊñáÁöÑÊù°‰ª∂‰∏ãÁîüÊàê8192‰∏™tokenÁöÑÊÄßËÉΩ„ÄÇËØÑÊµãËøêË°å‰∫éÂçïÂº†A100-SXM4-80G GPUÔºå‰ΩøÁî®PyTorch 2.0.1ÂíåCUDA 11.8„ÄÇÊé®ÁêÜÈÄüÂ∫¶ÊòØÁîüÊàê8192‰∏™tokenÁöÑÈÄüÂ∫¶ÂùáÂÄº„ÄÇ\n\nIn detail, the setting of profiling is generating 8192 new tokens with 1 context token. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.8. The inference speed is averaged over the generated 8192 tokens.\n\nÊ≥®ÊÑèÔºö‰ª•‰∏äInt4/Int8Ê®°ÂûãÁîüÊàêÈÄüÂ∫¶‰ΩøÁî®autogptqÂ∫ìÁªôÂá∫ÔºåÂΩìÂâç``AutoModelForCausalLM.from_pretrained``ËΩΩÂÖ•ÁöÑÊ®°ÂûãÁîüÊàêÈÄüÂ∫¶‰ºöÊÖ¢Â§ßÁ∫¶20%„ÄÇÊàë‰ª¨Â∑≤ÁªèÂ∞ÜËØ•ÈóÆÈ¢òÊ±áÊä•ÁªôHuggingFaceÂõ¢ÈòüÔºåËã•ÊúâËß£ÂÜ≥ÊñπÊ°àÂ∞ÜÂç≥Êó∂Êõ¥Êñ∞„ÄÇ\n\nNote: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using "AutoModelForCausalLM.from_pretrained" will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.\n\n### ÊòæÂ≠ò‰ΩøÁî® (GPU Memory Usage)\n\nÊàë‰ª¨ËøòÊµãÁÆó‰∫Ü‰∏çÂêåÊ®°ÂûãÁ≤æÂ∫¶ÁºñÁ†Å2048‰∏™tokenÂèäÁîüÊàê8192‰∏™tokenÁöÑÂ≥∞ÂÄºÊòæÂ≠òÂç†Áî®ÊÉÖÂÜµ„ÄÇÔºàÊòæÂ≠òÊ∂àËÄóÂú®ÊòØÂê¶‰ΩøÁî®FlashAttnÁöÑÊÉÖÂÜµ‰∏ãÂùáÁ±ª‰ºº„ÄÇÔºâÁªìÊûúÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n\nWe also profile the peak GPU memory usage for encoding 2048 tokens as context (and generating single token) and generating 8192 tokens (with single token as context) under different quantization levels, respectively. ÔºàThe GPU memory usage is similar when using flash-attention or not.ÔºâThe results are shown below.\n\n| Quantization Level | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------------ | :---------------------------------: | :-----------------------------------: |\n| BF16               | 16.99GB                             | 22.53GB                               |\n| Int8               | 11.20GB                             | 16.62GB                               |\n| Int4               |  8.21GB                             | 13.63GB                               |\n\n‰∏äËø∞ÊÄßËÉΩÊµãÁÆó‰ΩøÁî®[Ê≠§ËÑöÊú¨](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)ÂÆåÊàê„ÄÇ\n\nThe above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n<br>\n\n## Ê®°ÂûãÁªÜËäÇÔºàModelÔºâ\n\n‰∏éQwen-7BÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁõ∏ÂêåÔºåQwen-7B-ChatÊ®°ÂûãËßÑÊ®°Âü∫Êú¨ÊÉÖÂÜµÂ¶Ç‰∏ãÊâÄÁ§∫:\n\nThe details of the model architecture of Qwen-7B-Chat are listed as follows:\n\n| Hyperparameter  | Value  |\n|:----------------|:------:|\n| n_layers        |   32   |\n| n_heads         |   32   |\n| d_model         |  4096  |\n| vocab size      | 151851 |\n| sequence length |  8192  |\n\nÂú®‰ΩçÁΩÆÁºñÁ†Å„ÄÅFFNÊøÄÊ¥ªÂáΩÊï∞ÂíånormalizationÁöÑÂÆûÁé∞ÊñπÂºè‰∏äÔºåÊàë‰ª¨‰πüÈááÁî®‰∫ÜÁõÆÂâçÊúÄÊµÅË°åÁöÑÂÅöÊ≥ïÔºå\nÂç≥RoPEÁõ∏ÂØπ‰ΩçÁΩÆÁºñÁ†Å„ÄÅSwiGLUÊøÄÊ¥ªÂáΩÊï∞„ÄÅRMSNormÔºàÂèØÈÄâÂÆâË£Öflash-attentionÂä†ÈÄüÔºâ„ÄÇ\n\nÂú®ÂàÜËØçÂô®ÊñπÈù¢ÔºåÁõ∏ÊØîÁõÆÂâç‰∏ªÊµÅÂºÄÊ∫êÊ®°Âûã‰ª•‰∏≠Ëã±ËØçË°®‰∏∫‰∏ªÔºåQwen-7B-Chat‰ΩøÁî®‰∫ÜÁ∫¶15‰∏átokenÂ§ßÂ∞èÁöÑËØçË°®„ÄÇ\nËØ•ËØçË°®Âú®GPT-4‰ΩøÁî®ÁöÑBPEËØçË°®`cl100k_base`Âü∫Á°Ä‰∏äÔºåÂØπ‰∏≠Êñá„ÄÅÂ§öËØ≠Ë®ÄËøõË°å‰∫Ü‰ºòÂåñÔºåÂú®ÂØπ‰∏≠„ÄÅËã±„ÄÅ‰ª£Á†ÅÊï∞ÊçÆÁöÑÈ´òÊïàÁºñËß£Á†ÅÁöÑÂü∫Á°Ä‰∏äÔºåÂØπÈÉ®ÂàÜÂ§öËØ≠Ë®ÄÊõ¥Âä†ÂèãÂ•ΩÔºåÊñπ‰æøÁî®Êà∑Âú®‰∏çÊâ©Â±ïËØçË°®ÁöÑÊÉÖÂÜµ‰∏ãÂØπÈÉ®ÂàÜËØ≠ÁßçËøõË°åËÉΩÂäõÂ¢ûÂº∫„ÄÇ\nËØçË°®ÂØπÊï∞Â≠óÊåâÂçï‰∏™Êï∞Â≠ó‰ΩçÂàáÂàÜ„ÄÇË∞ÉÁî®ËæÉ‰∏∫È´òÊïàÁöÑ[tiktokenÂàÜËØçÂ∫ì](https://github.com/openai/tiktoken)ËøõË°åÂàÜËØç„ÄÇ\n\nFor position encoding, FFN activation function, and normalization calculation methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).\n\nFor tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-7B-Chat uses a vocabulary of over 150K tokens.\nIt first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.\nIt segments numbers by single digit, and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.\n<br>\n\n## ËØÑÊµãÊïàÊûúÔºàEvaluationÔºâ\n\nÂØπ‰∫éQwen-7B-ChatÊ®°ÂûãÔºåÊàë‰ª¨ÂêåÊ†∑ËØÑÊµã‰∫ÜÂ∏∏ËßÑÁöÑ‰∏≠ÊñáÁêÜËß£ÔºàC-EvalÔºâ„ÄÅËã±ÊñáÁêÜËß£ÔºàMMLUÔºâ„ÄÅ‰ª£Á†ÅÔºàHumanEvalÔºâÂíåÊï∞Â≠¶ÔºàGSM8KÔºâÁ≠âÊùÉÂ®Å‰ªªÂä°ÔºåÂêåÊó∂ÂåÖÂê´‰∫ÜÈïøÂ∫èÂàó‰ªªÂä°ÁöÑËØÑÊµãÁªìÊûú„ÄÇÁî±‰∫éQwen-7B-ChatÊ®°ÂûãÁªèËøáÂØπÈΩêÂêéÔºåÊøÄÂèë‰∫ÜËæÉÂº∫ÁöÑÂ§ñÈÉ®Á≥ªÁªüË∞ÉÁî®ËÉΩÂäõÔºåÊàë‰ª¨ËøòËøõË°å‰∫ÜÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÊñπÈù¢ÁöÑËØÑÊµã„ÄÇ\n\nÊèêÁ§∫ÔºöÁî±‰∫éÁ°¨‰ª∂ÂíåÊ°ÜÊû∂ÈÄ†ÊàêÁöÑËàçÂÖ•ËØØÂ∑ÆÔºåÂ§çÁé∞ÁªìÊûúÂ¶ÇÊúâÊ≥¢Âä®Â±û‰∫éÊ≠£Â∏∏Áé∞Ë±°„ÄÇ\n\nFor Qwen-7B-Chat, we also evaluate the model on C-Eval, MMLU, HumanEval, GSM8K, etc., as well as the benchmark evaluation for long-context understanding, and tool usage.\n\nNote: Due to rounding errors caused by hardware and framework, differences in reproduced results are possible.\n\n### ‰∏≠ÊñáËØÑÊµãÔºàChinese EvaluationÔºâ\n\n#### C-Eval\n\nÂú®[C-Eval](https://arxiv.org/abs/2305.08322)È™åËØÅÈõÜ‰∏äÔºåÊàë‰ª¨ËØÑ‰ª∑‰∫ÜQwen-7B-ChatÊ®°ÂûãÁöÑ0-shot & 5-shotÂáÜÁ°ÆÁéá\n\nWe demonstrate the 0-shot & 5-shot accuracy of Qwen-7B-Chat on C-Eval validation set\n\n|              Model               | Avg. Acc. |\n|:--------------------------------:|:---------:|\n|          LLaMA2-7B-Chat          |   31.9    |\n|         LLaMA2-13B-Chat          |   36.2    |\n|         LLaMA2-70B-Chat          |   44.3    |\n|         ChatGLM2-6B-Chat         |   52.6    |\n|         InternLM-7B-Chat         |   53.6    |\n|        Baichuan2-7B-Chat         |   55.6    |\n|        Baichuan2-13B-Chat        |   56.7    |\n| Qwen-7B-Chat (original) (0-shot) |   54.2    |\n|    **Qwen-7B-Chat (0-shot)**     |   59.7    |\n|    **Qwen-7B-Chat (5-shot)**     |   59.3    |\n|    **Qwen-14B-Chat (0-shot)**    |   69.8    |\n|    **Qwen-14B-Chat (5-shot)**    | **71.7**  |\n\nC-EvalÊµãËØïÈõÜ‰∏äÔºåQwen-7B-ChatÊ®°ÂûãÁöÑzero-shotÂáÜÁ°ÆÁéáÁªìÊûúÂ¶Ç‰∏ãÔºö\n\nThe zero-shot accuracy of Qwen-7B-Chat on C-Eval testing set is provided below:\n\n| Model                   |   Avg.   | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | :------: | :--: | :-------------: | :--------: | :----: |\n| Chinese-Alpaca-Plus-13B |   41.5   | 36.6 |      49.7       |    43.1    |  41.2  |\n| Chinese-Alpaca-2-7B     |   40.3   |  -   |        -        |     -      |   -    |\n| ChatGLM2-6B-Chat        |   50.1   | 46.4 |      60.4       |    50.6    |  46.9  |\n| Baichuan-13B-Chat       |   51.5   | 43.7 |      64.6       |    56.2    |  49.2  |\n| Qwen-7B-Chat (original)        |   54.6   | 47.8 |      67.6       |    59.3    |  50.6  |\n| **Qwen-7B-Chat**   |   58.6   | 53.3 |      72.1       |    62.8    |  52.0  |\n| **Qwen-14B-Chat**       | **69.1** | 65.1 |      80.9       |    71.2    |  63.4  |\n\nÂú®7BËßÑÊ®°Ê®°Âûã‰∏äÔºåÁªèËøá‰∫∫Á±ªÊåá‰ª§ÂØπÈΩêÁöÑQwen-7B-ChatÊ®°ÂûãÔºåÂáÜÁ°ÆÁéáÂú®ÂêåÁ±ªÁõ∏ËøëËßÑÊ®°Ê®°Âûã‰∏≠‰ªçÁÑ∂Â§Ñ‰∫éÂâçÂàó„ÄÇ\n\nCompared with other pretrained models with comparable model size, the human-aligned Qwen-7B-Chat performs well in C-Eval accuracy.\n\n### Ëã±ÊñáËØÑÊµãÔºàEnglish EvaluationÔºâ\n\n#### MMLU\n\n[MMLU](https://arxiv.org/abs/2009.03300)ËØÑÊµãÈõÜ‰∏äÔºåQwen-7B-ChatÊ®°ÂûãÁöÑ 0-shot & 5-shot ÂáÜÁ°ÆÁéáÂ¶Ç‰∏ãÔºåÊïàÊûúÂêåÊ†∑Âú®ÂêåÁ±ªÂØπÈΩêÊ®°Âûã‰∏≠ÂêåÊ†∑Ë°®Áé∞ËæÉ‰ºò„ÄÇ\n\nThe 0-shot & 5-shot accuracy of Qwen-7B-Chat on MMLU is provided below.\nThe performance of Qwen-7B-Chat still on the top between other human-aligned models with comparable size.\n\n|              Model               | Avg. Acc. |\n|:--------------------------------:|:---------:|\n|         ChatGLM2-6B-Chat         |   46.0    |\n|          LLaMA2-7B-Chat          |   46.2    |\n|         InternLM-7B-Chat         |   51.1    |\n|        Baichuan2-7B-Chat         |   52.9    |\n|         LLaMA2-13B-Chat          |   54.6    |\n|        Baichuan2-13B-Chat        |   57.3    |\n|         LLaMA2-70B-Chat          |   63.8    |\n| Qwen-7B-Chat (original) (0-shot) |   53.9    |\n|    **Qwen-7B-Chat (0-shot)**     |   55.8    |\n|    **Qwen-7B-Chat (5-shot)**     |   57.0    |\n|    **Qwen-14B-Chat (0-shot)**    |   64.6    |\n|    **Qwen-14B-Chat (5-shot)**    | **66.5**  |\n\n### ‰ª£Á†ÅËØÑÊµãÔºàCoding EvaluationÔºâ\n\nQwen-7B-ChatÂú®[HumanEval](https://github.com/openai/human-eval)ÁöÑzero-shot Pass@1ÊïàÊûúÂ¶Ç‰∏ã\n\nThe zero-shot Pass@1 of Qwen-7B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below\n\n|          Model          |  Pass@1  |\n|:-----------------------:|:--------:|\n|    ChatGLM2-6B-Chat     |   11.0   |\n|     LLaMA2-7B-Chat      |   12.2   |\n|    Baichuan2-7B-Chat    |   13.4   |\n|    InternLM-7B-Chat     |   14.6   |\n|   Baichuan2-13B-Chat    |   17.7   |\n|     LLaMA2-13B-Chat     |   18.9   |\n|     LLaMA2-70B-Chat     |   32.3   |\n| Qwen-7B-Chat (original) |   24.4   |\n|    **Qwen-7B-Chat**     |   37.2   |\n|    **Qwen-14B-Chat**    | **43.9** |\n\n### Êï∞Â≠¶ËØÑÊµãÔºàMathematics EvaluationÔºâ\n\nÂú®ËØÑÊµãÊï∞Â≠¶ËÉΩÂäõÁöÑ[GSM8K](https://github.com/openai/grade-school-math)‰∏äÔºåQwen-7B-ChatÁöÑÂáÜÁ°ÆÁéáÁªìÊûúÂ¶Ç‰∏ã\n\nThe accuracy of Qwen-7B-Chat on GSM8K is shown below\n\n|                Model                 |   Acc.   |\n|:------------------------------------:|:--------:|\n|            LLaMA2-7B-Chat            |   26.3   |\n|           ChatGLM2-6B-Chat           |   28.8   |\n|          Baichuan2-7B-Chat           |   32.8   |\n|           InternLM-7B-Chat           |   33.0   |\n|           LLaMA2-13B-Chat            |   37.1   |\n|          Baichuan2-13B-Chat          |   55.3   |\n|           LLaMA2-70B-Chat            |   59.3   |\n| **Qwen-7B-Chat (original) (0-shot)** |   41.1   |\n|      **Qwen-7B-Chat (0-shot)**       |   50.3   |\n|      **Qwen-7B-Chat (8-shot)**       |   54.1   |\n|      **Qwen-14B-Chat (0-shot)**      | **60.1** |\n|      **Qwen-14B-Chat (8-shot)**      |   59.3   |\n\n### ÈïøÂ∫èÂàóËØÑÊµãÔºàLong-Context UnderstandingÔºâ\n\nÈÄöËøáNTKÊèíÂÄºÔºåLogNÊ≥®ÊÑèÂäõÁº©ÊîæÂèØ‰ª•Êâ©Â±ïQwen-7B-ChatÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÇÂú®ÈïøÊñáÊú¨ÊëòË¶ÅÊï∞ÊçÆÈõÜ[VCSUM](https://arxiv.org/abs/2305.05280)‰∏äÔºàÊñáÊú¨Âπ≥ÂùáÈïøÂ∫¶Âú®15KÂ∑¶Âè≥ÔºâÔºåQwen-7B-ChatÁöÑRouge-LÁªìÊûúÂ¶Ç‰∏ãÔºö\n\n**(Ëã•Ë¶ÅÂêØÁî®Ëøô‰∫õÊäÄÂ∑ßÔºåËØ∑Â∞Üconfig.jsonÈáåÁöÑ`use_dynamic_ntk`Âíå`use_logn_attn`ËÆæÁΩÆ‰∏∫true)**\n\nWe introduce NTK-aware interpolation, LogN attention scaling to extend the context length of Qwen-7B-Chat. The Rouge-L results of Qwen-7B-Chat on long-text summarization dataset [VCSUM](https://arxiv.org/abs/2305.05280) (The average length of this dataset is around 15K) are shown below:\n\n**(To use these tricks, please set `use_dynamic_ntk` and `use_long_attn` to true in config.json.)**\n\n| Model             | VCSUM (zh) |\n|:------------------|:----------:|\n| GPT-3.5-Turbo-16k |    16.0    |\n| LLama2-7B-Chat    |    0.2     |\n| InternLM-7B-Chat  |    13.0    |\n| ChatGLM2-6B-Chat  |    16.3    |\n| **Qwen-7B-Chat**  |  **16.6**  |\n\n### Â∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÁöÑËØÑÊµãÔºàTool UsageÔºâ\n\n#### ReAct Prompting\n\nÂçÉÈóÆÊîØÊåÅÈÄöËøá [ReAct Prompting](https://arxiv.org/abs/2210.03629) Ë∞ÉÁî®Êèí‰ª∂/Â∑•ÂÖ∑/API„ÄÇReAct ‰πüÊòØ [LangChain](https://python.langchain.com/) Ê°ÜÊû∂ÈááÁî®ÁöÑ‰∏ªË¶ÅÊñπÂºè‰πã‰∏Ä„ÄÇÂú®Êàë‰ª¨ÂºÄÊ∫êÁöÑ„ÄÅÁî®‰∫éËØÑ‰º∞Â∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÁöÑËØÑÊµãÂü∫ÂáÜ‰∏äÔºåÂçÉÈóÆÁöÑË°®Áé∞Â¶Ç‰∏ãÔºö\n\nQwen-Chat supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629). ReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework. In our evaluation benchmark for assessing tool usage capabilities, Qwen-Chat''s performance is as follows:\n\n<table>\n    <tr>\n        <th colspan="4" align="center">Chinese Tool-Use Benchmark</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Tool Selection (Acc.‚Üë)</th><th align="center">Tool Input (Rouge-L‚Üë)</th><th align="center">False Positive Error‚Üì</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>\n    </tr>\n</table>\n\n> ËØÑÊµãÂü∫ÂáÜ‰∏≠Âá∫Áé∞ÁöÑÊèí‰ª∂ÂùáÊ≤°ÊúâÂá∫Áé∞Âú®ÂçÉÈóÆÁöÑËÆ≠ÁªÉÈõÜ‰∏≠„ÄÇËØ•Âü∫ÂáÜËØÑ‰º∞‰∫ÜÊ®°ÂûãÂú®Â§ö‰∏™ÂÄôÈÄâÊèí‰ª∂‰∏≠ÈÄâÊã©Ê≠£Á°ÆÊèí‰ª∂ÁöÑÂáÜÁ°ÆÁéá„ÄÅ‰º†ÂÖ•Êèí‰ª∂ÁöÑÂèÇÊï∞ÁöÑÂêàÁêÜÊÄß„ÄÅ‰ª•ÂèäÂÅáÈò≥Áéá„ÄÇÂÅáÈò≥ÁéáÔºàFalse PositiveÔºâÂÆö‰πâÔºöÂú®Â§ÑÁêÜ‰∏çËØ•Ë∞ÉÁî®Êèí‰ª∂ÁöÑËØ∑Ê±ÇÊó∂ÔºåÈîôËØØÂú∞Ë∞ÉÁî®‰∫ÜÊèí‰ª∂„ÄÇ\n\n> The plugins that appear in the evaluation set do not appear in the training set of Qwen. This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate. False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.\n\n![](assets/react_showcase_001.png)\n![](assets/react_showcase_002.png)\n\n#### Code Interpreter\n\n‰∏∫‰∫ÜËÄÉÂØüQwen‰ΩøÁî®Python Code InterpreterÂÆåÊàêÊï∞Â≠¶Ëß£È¢ò„ÄÅÊï∞ÊçÆÂèØËßÜÂåñ„ÄÅÂèäÊñá‰ª∂Â§ÑÁêÜ‰∏éÁà¨Ëô´Á≠â‰ªªÂä°ÁöÑËÉΩÂäõÔºåÊàë‰ª¨‰∏ìÈó®Âª∫ËÆæÂπ∂ÂºÄÊ∫ê‰∫Ü‰∏Ä‰∏™ËØÑÊµãËøôÊñπÈù¢ËÉΩÂäõÁöÑ[ËØÑÊµãÂü∫ÂáÜ](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)„ÄÇ\n\nÊàë‰ª¨ÂèëÁé∞QwenÂú®ÁîüÊàê‰ª£Á†ÅÁöÑÂèØÊâßË°åÁéá„ÄÅÁªìÊûúÊ≠£Á°ÆÊÄß‰∏äÂùáË°®Áé∞ËæÉÂ•ΩÔºö\n\nTo assess Qwen''s ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nWe have observed that Qwen performs well in terms of code executability and result accuracy when generating code:\n\n<table>\n    <tr>\n        <th colspan="4" align="center">Executable Rate of Generated Code (%)</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Math‚Üë</th><th align="center">Visualization‚Üë</th><th align="center">General‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-7B-Chat</td>\n        <td align="center">41.9</td>\n        <td align="center">33.1</td>\n        <td align="center">24.1 </td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align="center">50.0</td>\n        <td align="center">40.5</td>\n        <td align="center">48.3 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-7B-Instruct</td>\n        <td align="center">85.1</td>\n        <td align="center">54.0</td>\n        <td align="center">70.7 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align="center">93.2</td>\n        <td align="center">55.8</td>\n        <td align="center">74.1 </td>\n    </tr>\n    <tr>\n        <td>InternLM-7B-Chat-v1.1</td>\n        <td align="center">78.4</td>\n        <td align="center">44.2</td>\n        <td align="center">62.1 </td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align="center">70.3</td>\n        <td align="center">44.2</td>\n        <td align="center">65.5 </td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align="center">82.4</td>\n        <td align="center">64.4</td>\n        <td align="center">67.2 </td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align="center">89.2</td>\n        <td align="center">84.1</td>\n        <td align="center">65.5</td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <th colspan="4" align="center">Accuracy of Code Execution Results (%)</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Math‚Üë</th><th align="center">Visualization-Hard‚Üë</th><th align="center">Visualization-Easy‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-7B-Chat</td>\n        <td align="center">3.9</td>\n        <td align="center">14.3</td>\n        <td align="center">39.2 </td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align="center">8.3</td>\n        <td align="center">8.3</td>\n        <td align="center">40.5 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-7B-Instruct</td>\n        <td align="center">14.3</td>\n        <td align="center">26.2</td>\n        <td align="center">60.8 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align="center">28.2</td>\n        <td align="center">27.4</td>\n        <td align="center">62.0 </td>\n    </tr>\n    <tr>\n        <td>InternLM-7B-Chat-v1.1</td>\n        <td align="center">28.5</td>\n        <td align="center">4.8</td>\n        <td align="center">40.5 </td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align="center">34.6</td>\n        <td align="center">21.4</td>\n        <td align="center">45.6 </td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align="center">41.9</td>\n        <td align="center">40.5</td>\n        <td align="center">54.4 </td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align="center">58.4</td>\n        <td align="center">53.6</td>\n        <td align="center">59.5</td>\n    </tr>\n</table>\n\n<p align="center">\n    <br>\n    <img src="assets/code_interpreter_showcase_001.jpg" />\n    <br>\n<p>\n\n#### Huggingface Agent\n\nÂçÉÈóÆËøòÂÖ∑Â§á‰Ωú‰∏∫ [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents) ÁöÑËÉΩÂäõ„ÄÇÂÆÉÂú® Huggingface Êèê‰æõÁöÑrunÊ®°ÂºèËØÑÊµãÂü∫ÂáÜ‰∏äÁöÑË°®Áé∞Â¶Ç‰∏ãÔºö\n\nQwen-Chat also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents). Its performance on the run-mode benchmark provided by HuggingFace is as follows:\n\n<table>\n    <tr>\n        <th colspan="4" align="center">HuggingFace Agent Benchmark- Run Mode</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Tool Selection‚Üë</th><th align="center">Tool Used‚Üë</th><th align="center">Code‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>\n    </tr>\n    <tr>\n        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>\n    </tr>\n    <tr>\n        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <th colspan="4" align="center">HuggingFace Agent Benchmark - Chat Mode</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Tool Selection‚Üë</th><th align="center">Tool Used‚Üë</th><th align="center">Code‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>\n    </tr>\n    <tr>\n        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>\n    </tr>\n    <tr>\n        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>\n    </tr>\n</table>\n\n<br>\n\n## x86 Âπ≥Âè∞ (x86 Platforms)\nÂú® ÈÖ∑Áùø‚Ñ¢/Ëá≥Âº∫¬Æ ÂèØÊâ©Â±ïÂ§ÑÁêÜÂô®Êàñ Arc‚Ñ¢ GPU ‰∏äÈÉ®ÁΩ≤ÈáèÂåñÊ®°ÂûãÊó∂ÔºåÂª∫ËÆÆ‰ΩøÁî® [OpenVINO‚Ñ¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html)‰ª•ÂÖÖÂàÜÂà©Áî®Á°¨‰ª∂ÔºåÂÆûÁé∞Êõ¥Â•ΩÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÊÇ®ÂèØ‰ª•ÂÆâË£ÖÂπ∂ËøêË°åÊ≠§ [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)„ÄÇÁõ∏ÂÖ≥ÈóÆÈ¢òÔºåÊÇ®ÂèØÂú®[OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues)‰∏≠Êèê‰∫§„ÄÇ\n\nWhen deploy on Core‚Ñ¢/Xeon¬Æ Scalable Processors or with Arc‚Ñ¢ GPU, [OpenVINO‚Ñ¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) is recommended. You can install and run this [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot). For related issues, you are welcome to file an issue at [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues).\n\n## FAQ\n\nÂ¶ÇÈÅáÂà∞ÈóÆÈ¢òÔºåÊï¨ËØ∑Êü•ÈòÖ[FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ_zh.md)‰ª•ÂèäissueÂå∫ÔºåÂ¶Ç‰ªçÊó†Ê≥ïËß£ÂÜ≥ÂÜçÊèê‰∫§issue„ÄÇ\n\nIf you meet problems, please refer to [FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br>\n\n## ÂºïÁî® (Citation)\n\nÂ¶ÇÊûú‰Ω†ËßâÂæóÊàë‰ª¨ÁöÑÂ∑•‰ΩúÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåÊ¨¢ËøéÂºïÁî®ÔºÅ\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## ‰ΩøÁî®ÂçèËÆÆÔºàLicense AgreementÔºâ\n\nÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊ®°ÂûãÊùÉÈáçÂØπÂ≠¶ÊúØÁ†îÁ©∂ÂÆåÂÖ®ÂºÄÊîæÔºåÂπ∂ÊîØÊåÅÂïÜÁî®„ÄÇËØ∑Êü•Áúã[LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT)‰∫ÜËß£ÂÖ∑‰ΩìÁöÑÂºÄÊ∫êÂçèËÆÆÁªÜËäÇ„ÄÇÂ¶ÇÈúÄÂïÜÁî®ÔºåËØ∑Â°´ÂÜô[ÈóÆÂç∑](https://dashscope.console.aliyun.com/openModelApply/qianwen)Áî≥ËØ∑„ÄÇ\n\nOur code and checkpoints are open to research purpose, and they are allowed for commercial purposes. Check [LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) for more details about the license. If you have requirements for commercial use, please fill out the [form](https://dashscope.console.aliyun.com/openModelApply/qianwen) to apply.\n<br>\n\n## ËÅîÁ≥ªÊàë‰ª¨ÔºàContact UsÔºâ\n\nÂ¶ÇÊûú‰Ω†ÊÉ≥ÁªôÊàë‰ª¨ÁöÑÁ†îÂèëÂõ¢ÈòüÂíå‰∫ßÂìÅÂõ¢ÈòüÁïôË®ÄÔºåÊ¨¢ËøéÂä†ÂÖ•Êàë‰ª¨ÁöÑÂæÆ‰ø°Áæ§„ÄÅÈíâÈíâÁæ§‰ª•ÂèäDiscordÔºÅÂêåÊó∂Ôºå‰πüÊ¨¢ËøéÈÄöËøáÈÇÆ‰ª∂Ôºàqianwen_opensource@alibabacloud.comÔºâËÅîÁ≥ªÊàë‰ª¨„ÄÇ\n\nIf you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7721324544,"storage_bytes":46329475248,"files_count":30,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["QWenLMHeadModel"],"auto_map":{"AutoConfig":"configuration_qwen.QWenConfig","AutoModelForCausalLM":"modeling_qwen.QWenLMHeadModel"},"model_type":"qwen","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:openai:tiktoken","source_url":"https://github.com/openai/tiktoken"},{"type":"has_code","target_id":"github:openai:tiktoken","source_url":"https://github.com/openai/tiktoken"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:openai:grade-school-math","source_url":"https://github.com/openai/grade-school-math"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"based_on_paper","target_id":"arxiv:2309.16609","source_url":"https://arxiv.org/abs/2309.16609"},{"type":"based_on_paper","target_id":"arxiv:2305.08322","source_url":"https://arxiv.org/abs/2305.08322"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2305.05280","source_url":"https://arxiv.org/abs/2305.05280"},{"type":"based_on_paper","target_id":"arxiv:2210.03629","source_url":"https://arxiv.org/abs/2210.03629"}]', NULL, 'Other', 'approved', 98.9, '3c218bdb581696643fdc5a28684cae60', NULL, 'https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/assets/code_interpreter_showcase_001.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Qwen-Qwen-7B-Chat from https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/assets/code_interpreter_showcase_001.jpg
Image converted to WebP: data/images/huggingface-Qwen-Qwen-7B-Chat.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-medgemma-4b-it', 'huggingface--google--medgemma-4b-it', 'medgemma-4b-it', 'google', '', '["transformers","safetensors","gemma3","any-to-any","medical","radiology","clinical-reasoning","dermatology","pathology","ophthalmology","chest-x-ray","image-text-to-text","conversational","arxiv:2303.15343","arxiv:2507.05201","arxiv:2405.03162","arxiv:2106.14463","arxiv:2412.03555","arxiv:2501.19393","arxiv:2009.13081","arxiv:2102.09542","arxiv:2411.15640","arxiv:2404.05590","arxiv:2501.18362","base_model:google/medgemma-4b-pt","base_model:finetune:google/medgemma-4b-pt","license:other","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 784, 596852, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/medgemma-4b-it","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":4300079472,"storage_bytes":13599603276,"files_count":15,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["Gemma3ForConditionalGeneration"],"model_type":"gemma3","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false},"chat_template_jinja":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2303.15343","source_url":"https://arxiv.org/abs/2303.15343"},{"type":"based_on_paper","target_id":"arxiv:2507.05201","source_url":"https://arxiv.org/abs/2507.05201"},{"type":"based_on_paper","target_id":"arxiv:2405.03162","source_url":"https://arxiv.org/abs/2405.03162"},{"type":"based_on_paper","target_id":"arxiv:2106.14463","source_url":"https://arxiv.org/abs/2106.14463"},{"type":"based_on_paper","target_id":"arxiv:2412.03555","source_url":"https://arxiv.org/abs/2412.03555"},{"type":"based_on_paper","target_id":"arxiv:2501.19393","source_url":"https://arxiv.org/abs/2501.19393"},{"type":"based_on_paper","target_id":"arxiv:2009.13081","source_url":"https://arxiv.org/abs/2009.13081"},{"type":"based_on_paper","target_id":"arxiv:2102.09542","source_url":"https://arxiv.org/abs/2102.09542"},{"type":"based_on_paper","target_id":"arxiv:2411.15640","source_url":"https://arxiv.org/abs/2411.15640"},{"type":"based_on_paper","target_id":"arxiv:2404.05590","source_url":"https://arxiv.org/abs/2404.05590"},{"type":"based_on_paper","target_id":"arxiv:2501.18362","source_url":"https://arxiv.org/abs/2501.18362"}]', NULL, 'Other', 'approved', 38.9, '3da5a507dff9ebcdc3056b02857500c5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nitrosocke-Ghibli-Diffusion', 'huggingface--nitrosocke--ghibli-diffusion', 'Ghibli-Diffusion', 'nitrosocke', '--- language: - en license: creativeml-openrail-m thumbnail: "https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-thumbnail.jpg" tags: - stable-diffusion - text-to-image - image-to-image - diffusers --- This is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli. Use the tokens **_ghibli style_** in your prompts for the effect. **If you enjoy my work and want to test new models before release, please co...', '["diffusers","safetensors","stable-diffusion","text-to-image","image-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 782, 2909, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nitrosocke/Ghibli-Diffusion","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: creativeml-openrail-m\nthumbnail: "https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-thumbnail.jpg"\ntags:\n- stable-diffusion\n- text-to-image\n- image-to-image\n- diffusers\n\n---\n### Ghibli Diffusion\n\nThis is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli.\nUse the tokens **_ghibli style_** in your prompts for the effect.\n\n**If you enjoy my work and want to test new models before release, please consider supporting me**\n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n**Characters rendered with the model:**\n![Characters Samples](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-01s.jpg)\n**Cars and Animals rendered with the model:**\n![Misc. Samples](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-02s.jpg)\n**Landscapes rendered with the model:**\n![Landscape 1](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-03s.jpg)\n_ghibli style beautiful Caribbean beach tropical (sunset) - Negative prompt: soft blurry_\n![Landscape 2](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-04s.jpg)\n_ghibli style ice field white mountains ((northern lights)) starry sky low horizon - Negative prompt: soft blurry_\n\n#### Prompt and settings for the Strom Trooper:\n**ghibli style (storm trooper) Negative prompt: (bad anatomy)**\n_Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 3450349066, Size: 512x704_\n\n#### Prompt and settings for the VW Beetle:\n**ghibli style VW beetle Negative prompt: soft blurry**\n_Steps: 30, Sampler: Euler a, CFG scale: 7, Seed: 1529856912, Size: 704x512_\n\nThis model was trained using the diffusers based dreambooth training by ShivamShrirao using prior-preservation loss and the _train-text-encoder_ flag in 15.000 steps.\n\n<!-- ### Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI run redshift-diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/nitrosocke/Ghibli-Diffusion-Demo)-->\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "nitrosocke/Ghibli-Diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "ghibli style magical princess with golden hair"\nimage = pipe(prompt).images[0]\n\nimage.save("./magical_princess.png")\n```\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":20376154998,"files_count":28,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 63.9, 'a35bd9a701739533fc86b212e9ef495a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Alibaba-NLP-Tongyi-DeepResearch-30B-A3B', 'huggingface--alibaba-nlp--tongyi-deepresearch-30b-a3b', 'Tongyi-DeepResearch-30B-A3B', 'Alibaba-NLP', '--- license: apache-2.0 language: - en pipeline_tag: text-generation library_name: transformers --- We present **Tongyi DeepResearch**, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity''s Last Ex...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","en","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 779, 13287, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n\n# Introduction\n\nWe present  **Tongyi DeepResearch**, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity''s Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch and FRAMES.\n\nMore details can be found in our üì∞ [Tech Blog](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research).\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/OhQCYYJu1LhrS446Qct5D.png)\n\n## Key Features\n\n- ‚öôÔ∏è **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.\n- üîÑ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.\n- üîÅ **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.\n- ü§ñ **Agent Inference Paradigm Compatibility**: At inference, Tongyi-DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model''s core intrinsic abilities, and an IterResearch-based ''Heavy'' mode, which uses a test-time scaling strategy to unlock the model''s maximum performance ceiling.\n\n## Download\n\nYou can download the model then run the inference scipts in https://github.com/Alibaba-NLP/DeepResearch.\n\n\n```bibtex\n@misc{tongyidr,\n  author={Tongyi DeepResearch Team},\n  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},\n  year={2025},\n  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":30532122624,"storage_bytes":61077998390,"files_count":27,"spaces_count":3,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null},"chat_template_jinja":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}"}}', '[]', '[{"type":"has_code","target_id":"github:Alibaba-NLP:DeepResearch.","source_url":"https://github.com/Alibaba-NLP/DeepResearch."},{"type":"has_code","target_id":"github:Alibaba-NLP:DeepResearch}}","source_url":"https://github.com/Alibaba-NLP/DeepResearch}}"}]', NULL, 'Apache-2.0', 'approved', 63.9, '58fc596c773384438f639bb6b5b7656c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-timesfm-1.0-200m', 'huggingface--google--timesfm-1.0-200m', 'timesfm-1.0-200m', 'google', '--- license: apache-2.0 library_name: timesfm pipeline_tag: time-series-forecasting --- TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. **Resources and Technical Documentation**: * Paper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024. * Google Research blog * GitHub repo **Authors**: Google Research This is not an officially supported Google product. is the first...', '["timesfm","time-series-forecasting","arxiv:2310.10688","license:apache-2.0","region:us"]', 'time-series-forecasting', 776, 1959, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/timesfm-1.0-200m","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlibrary_name: timesfm\npipeline_tag: time-series-forecasting\n---\n\n# TimesFM\n\nTimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\n\n**Resources and Technical Documentation**:\n\n* Paper: [A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688), to appear in ICML 2024.\n* [Google Research blog](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/)\n* [GitHub repo](https://github.com/google-research/timesfm)\n\n**Authors**: Google Research\n\nThis is not an officially supported Google product.\n\n## Checkpoint timesfm-1.0-200m\n\n`timesfm-1.0-200m` is the first open model checkpoint:\n\n- It performs univariate time series forecasting for context lengths up to 512 time points and any horizon lengths, with an optional frequency indicator.\n- It focuses on point forecasts and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.\n- It requires the context to be contiguous (i.e. no "holes"), and the context and the horizon to be of the same frequency.\n\n## Benchmarks\n\nPlease refer to our result tables on the [extended benchmarks](https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/tfm_results.png) and the [long horizon benchmarks](https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/tfm_long_horizon.png).\n\nPlease look into the README files in the respective benchmark directories within `experiments/` for instructions for running TimesFM on the respective benchmarks.\n\n## Installation\n\nThis HuggingFace repo hosts TimesFm checkpoints. Please visit our [GitHub repo](https://github.com/google-research/timesfm) and follow the instructions there to install the `timesfm` library for model inference.\n\nIn particular, the dependency `lingvo` does not support ARM architectures and the inference code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\n\n## Usage \n\n### Initialize the model and load a checkpoint.\nThen the base class can be loaded as,\n\n```python\nimport timesfm\n\ntfm = timesfm.TimesFm(\n    context_len=<context>,\n    horizon_len=<horizon>,\n    input_patch_len=32,\n    output_patch_len=128,\n    num_layers=20,\n    model_dims=1280,\n    backend=<backend>,\n)\ntfm.load_from_checkpoint(repo_id="google/timesfm-1.0-200m")\n```\n\nNote that the four parameters are fixed to load the 200m model\n\n```python\ninput_patch_len=32,\noutput_patch_len=128,\nnum_layers=20,\nmodel_dims=1280,\n```\n\n1. The context_len here can be set as the max context length **of the model**. You can provide a shorter series to the `tfm.forecast()` function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have **any context length**. Padding / truncation will be handled by the inference code if needed.\n\n2. The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length <= context length but it is not a requirement in the function call.\n\n### Perform inference\n\nWe provide APIs to forecast from either array inputs or `pandas` dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions `tfm.forecast()` and `tfm.forecast_on_df()` for detailed instructions.\n\nIn particular, regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:\n\n- **0** (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.\n- **1**: medium frequency time series. We recommend using this for weekly and monthly data.\n- **2**: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.\n\nThis categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that\n\n- **0**: T, MIN, H, D, B, U\n- **1**: W, M\n- **2**: Q, Y\n\nNotice you do **NOT** have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.\n\n\nExamples:\n\nArray inputs, with the frequencies set to low, medium, and high respectively.\n\n```python\nimport numpy as np\nforecast_input = [\n    np.sin(np.linspace(0, 20, 100))\n    np.sin(np.linspace(0, 20, 200)),\n    np.sin(np.linspace(0, 20, 400)),\n]\nfrequency_input = [0, 1, 2]\n\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\n    forecast_input,\n    freq=frequency_input,\n)\n```\n\n`pandas` dataframe, with the frequency set to "M" monthly.\n\n```python\nimport pandas as pd\n\n# e.g. input_df is\n#       unique_id  ds          y\n# 0     T1         1975-12-31  697458.0\n# 1     T1         1976-01-31  1187650.0\n# 2     T1         1976-02-29  1069690.0\n# 3     T1         1976-03-31  1078430.0\n# 4     T1         1976-04-30  1059910.0\n# ...   ...        ...         ...\n# 8175  T99        1986-01-31  602.0\n# 8176  T99        1986-02-28  684.0\n# 8177  T99        1986-03-31  818.0\n# 8178  T99        1986-04-30  836.0\n# 8179  T99        1986-05-31  878.0\n\nforecast_df = tfm.forecast_on_df(\n    inputs=input_df,\n    freq="M",  # monthly\n    value_name="y",\n    num_jobs=-1,\n)\n```', '{"pipeline_tag":"time-series-forecasting","library_name":"timesfm","framework":"timesfm","params":null,"storage_bytes":814291152,"files_count":5,"spaces_count":4,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"based_on_paper","target_id":"arxiv:2310.10688","source_url":"https://arxiv.org/abs/2310.10688"}]', NULL, 'Apache-2.0', 'approved', 63.9, '787c584aa80c6e00bc16f8ee24933613', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nvidia-NVLM-D-72B', 'huggingface--nvidia--nvlm-d-72b', 'NVLM-D-72B', 'nvidia', '--- license: cc-by-nc-4.0 language: - en pipeline_tag: image-text-to-text tags: - nvidia - NVLM - pytorch - multimodal - conversational library_name: transformers --- <p align="center"> <img src="nvlm-logo-light.png" alt="Image Description" width="300" > </p> This family of models performs vision-language and text-only tasks including optical character recognition, multimodal reasoning, localization, common sense reasoning, world knowledge utilization, and coding. This model is ready for non-...', '["transformers","safetensors","nvlm_d","nvidia","nvlm","pytorch","multimodal","conversational","image-text-to-text","custom_code","en","arxiv:2409.11402","license:cc-by-nc-4.0","endpoints_compatible","region:us"]', 'image-text-to-text', 775, 54834, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nvidia/NVLM-D-72B","fetched_at":"2025-12-08T10:39:52.038Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- nvidia\n- NVLM\n- pytorch\n- multimodal\n- conversational\nlibrary_name: transformers\n---\n\n<p align="center">\n  <img src="nvlm-logo-light.png" alt="Image Description" width="300" >\n</p>\n\n\n# Model Overview\n\n## Description\nThis family of models performs vision-language and text-only tasks including optical character recognition, multimodal reasoning, localization, common sense reasoning, world knowledge utilization, and coding.\n\nThis model is ready for non-commercial use.\n\n## License/Terms of Use\n\nGoverning Terms: Deed - [Attribution-NonCommercial 4.0 International - Creative Commons](https://creativecommons.org/licenses/by-nc/4.0/deed.en).\n\nAdditional Information: [LICENSE ¬∑ Qwen/Qwen2-72B-Instruct at main](https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE) for Qwen2-72B-Instruct and [The MIT License ‚Äì Open Source Initiative](https://opensource.org/license/mit) for InternViT-6B-448px-V1-2.\n\n# Model Details\n\nToday (September 17th, 2024), we introduce [NVLM 1.0](https://arxiv.org/abs/2409.11402), a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. \n\nIn this repo, we are open-sourcing NVLM-1.0-D-72B (decoder-only architecture), the decoder-only model weights and code for the community.\n\n\n\n## Reference(s)\n[Paper](https://arxiv.org/abs/2409.11402) &ensp; [Inference Code (HF)](https://huggingface.co/nvidia/NVLM-D-72B/tree/main) &ensp; [Training Code](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm) &ensp; [Website](https://research.nvidia.com/labs/adlr/NVLM-1/) \n\n## Benchmark Results\nWe train our model with legacy [Megatron-LM](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/legacy) and adapt the codebase to Huggingface for model hosting, reproducibility, and inference.\nWe observe numerical differences between the Megatron and Huggingface codebases, which are within the expected range of variation. \nWe provide the results from both the Huggingface codebase and the Megatron codebase for reproducibility and comparison with other models.\n\nResults (as of September 17th, 2024) in the multimodal benchmarks are as follows:\n\n### Vision-language Benchmarks \n\n| Benchmark                    | MMMU (val / test) | MathVista | OCRBench | AI2D | ChartQA | DocVQA | TextVQA | RealWorldQA | VQAv2 |\n|------------------------------|-------------------|-----------|----------|------|---------|--------|---------|-------------|-------|\n| NVLM-D 1.0 72B (Huggingface) | 58.7 / 54.9       | 65.2      | 852      | 94.2 | 86.0    | 92.6   | 82.6    | 69.5        | 85.4  |\n| NVLM-D 1.0 72B (Megatron)    | 59.7 / 54.6       | 65.2      | 853      | 94.2 | 86.0    | 92.6   | 82.1    | 69.7        | 85.4  |\n| Llama 3.2 90B                | 60.3 / -          | 57.3      | -        | 92.3 | 85.5    | 90.1   | -       | -           | 78.1  |\n| Llama 3-V 70B                | 60.6 / -          | -         | -        | 93.0 | 83.2    | 92.2   | 83.4    | -           | 79.1  |\n| Llama 3-V 405B               | 64.5 / -          | -         | -        | 94.1 | 85.8    | 92.6   | 84.8    | -           | 80.2  |\n| InternVL2-Llama3-76B         | 55.2 / -          | 65.5      | 839      | 94.8 | 88.4    | 94.1   | 84.4    | 72.2        | -     |\n| GPT-4V                       | 56.8 / 55.7       | 49.9      | 645      | 78.2 | 78.5    | 88.4   | 78.0    | 61.4        | 77.2  |\n| GPT-4o                       | 69.1 / -          | 63.8      | 736      | 94.2 | 85.7    | 92.8   | -       | -           | -     |\n| Claude 3.5 Sonnet            | 68.3 / -          | 67.7      | 788      | 94.7 | 90.8    | 95.2   | -       | -           | -     |\n| Gemini 1.5 Pro (Aug 2024)    | 62.2 / -          | 63.9      | 754      | 94.4 | 87.2    | 93.1   | 78.7    | 70.4        | 80.2  |\n\n### Text-only Benchmarks\n\n| Tasks                        | Backbone LLM | MMLU | GSM8K | MATH | HumanEval | Avg. Accuracy    |\n|------------------------------|--------------|------|-------|------|-----------|------------------|\n| **Proprietary**              |              |      |       |      |           |                  |\n| GPT-4.0                      | N/A          | 88.7 | -     | 76.6 | 90.2      | -                |\n| Gemini Pro 1.5 (Aug 2024)    | N/A          | 85.9 | 90.8  | 67.7 | 84.1      | 82.1             |\n| Claude 3.5 Sonnet            | N/A          | 88.7 | 96.4  | 71.1 | 92.0      | 87.0             |\n| **Open LLM**                 |              |      |       |      |           |                  |\n| (a) Nous-Hermes-2-Yi-34B     | N/A          | 75.5 | 78.6  | 21.8 | 43.3      | 54.8             |\n| (b) Qwen-72B-Instruct        | N/A          | 82.3 | 91.1  | 59.7 | 86.0      | 79.8             |\n| (c) Llama-3-70B-Instruct     | N/A          | 82.0 | 93.0  | 51.0 | 81.7      | 76.6             |\n| (d) Llama-3.1-70B-Instruct   | N/A          | 83.6 | 95.1  | 68.0 | 80.5      | 81.8             |\n| (e) Llama-3.1-405B-Instruct  | N/A          | 87.3 | 96.8  | 73.8 | 89.0      | 86.7             |\n| **Open Multimodal LLM**      |              |      |       |      |           |                  |\n| VILA-1.5 40B                 | (a)          | 73.3 | 67.5  | 16.8 | 34.1      | ü•∂ 47.9   (-6.9) |\n| LLaVA-OneVision 72B          | (b)          | 80.6 | 89.9  | 49.2 | 74.4      | ü•∂ 73.5   (-6.3) |\n| InternVL-2-Llama3-76B        | (c)          | 78.5 | 87.1  | 42.5 | 71.3      | ü•∂ 69.9   (-6.7) |\n| *Llama 3-V 70B               | (d)          | 83.6 | 95.1  | 68.0 | 80.5      | üôÇ 81.8   (0)    |\n| *Llama 3-V 405B              | (e)          | 87.3 | 96.8  | 73.8 | 89.0      | üôÇ 86.7   (0)    |\n| NVLM-D 1.0 72B (Megatron)    | (b)          | 82.0 | 92.9  | 73.1 | 88.4      | ü•≥ 84.1   (+4.3) |\n| NVLM-D 1.0 72B (Huggingface) | (b)          | 81.7 | 93.2  | 73.1 | 89.0      | ü•≥ 84.3   (+4.5) |\n\n\n## Model Architectures\n\n**Network Architecture:** Decoder-Only Transformer\n\n**Text-only LLM backbone:** [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n\n**Vision encoder:** [InternViT-6B](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2)\n\n### Robustness\n\nThe model trained on this dataset cannot regenerate its training data:\n\n1. The model has no image generation capability since its output is only text. Hence it cannot regenerate any image it would have seen during training.\n\n2. The model cannot regenerate training text data: during training, the model takes text and images as inputs, and the model output (text) is conditioned on both inputs. During inference, without training images as input, the models would not be able to reproduce any part of the training text data.\n\n\n### Input\n**Input Type(s):** Text, Image <br>\n**Input Format(s):** String, [Pillow Library-Supported Formats](https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html) <br>\n**Input Dimensions:** One-Dimensional (1D), Two Dimensional (2D) <br>\n**Other Properties Related to Input:** Maximum Token Length = 128K Tokens <br>\n\n### Output\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Model Output:** 1D <br>\n**Other Properties Related to Output:** None <br> \n\n## How to use\n\nWhen converting Megatron checkpoint to Huggingface, we adapt [InternVL codebase](https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B) to support model loading and multi-GPU inference in HF. \nWe also use the tokenizer from [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/tree/main) when adapting the tokenizer to Huggingface, as it contains extra special tokens for vision tasks, e.g., `<|vision_pad|>`. \nWe train NVLM-1.0-D-72B based on the [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct/tree/main) text-only model and [InternViT-6B-448px-V1-5](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5) ViT model with our large-scale high-quality multimodal dataset. \nFor training code, please refer to [Megatron-Core](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm).\n\n\n### Prepare the environment\n\nWe provide a docker build file in the [Dockerfile](Dockerfile) for reproduction. \n\nThe docker image is based on `nvcr.io/nvidia/pytorch:23.09-py3`. \n\n*Note: We observe that different transformer versions / CUDA versions / docker versions can lead to slight benchmark number differences. We recommend using the Dockerfile above for precise reproduction.*\n\n### Model loading\n\n```python\nimport torch\nfrom transformers import AutoModel\n\npath = "nvidia/NVLM-D-72B"\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True).eval()\n```\n\n### Multiple GPUs\n\nThe model can be loaded on multiple GPUs as follows:\n\n```python\nimport torch\nimport math\nfrom transformers import AutoModel\n\ndef split_model():\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = 80\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f''language_model.model.layers.{layer_cnt}''] = i\n            layer_cnt += 1\n    device_map[''vision_model''] = 0\n    device_map[''mlp1''] = 0\n    device_map[''language_model.model.tok_embeddings''] = 0\n    device_map[''language_model.model.embed_tokens''] = 0\n    device_map[''language_model.output''] = 0\n    device_map[''language_model.model.norm''] = 0\n    device_map[''language_model.lm_head''] = 0\n    device_map[''language_model.model.rotary_emb''] = 0\n    device_map[f''language_model.model.layers.{num_layers - 1}''] = 0\n\n    return device_map\n\npath = "nvidia/NVLM-D-72B"\ndevice_map = split_model()\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True,\n    device_map=device_map).eval()\n```\n\n\n### Inference\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport math\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\n\n\ndef split_model():\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = 80\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f''language_model.model.layers.{layer_cnt}''] = i\n            layer_cnt += 1\n    device_map[''vision_model''] = 0\n    device_map[''mlp1''] = 0\n    device_map[''language_model.model.tok_embeddings''] = 0\n    device_map[''language_model.model.embed_tokens''] = 0\n    device_map[''language_model.output''] = 0\n    device_map[''language_model.model.norm''] = 0\n    device_map[''language_model.lm_head''] = 0\n    device_map[''language_model.model.rotary_emb''] = 0\n    device_map[f''language_model.model.layers.{num_layers - 1}''] = 0\n\n    return device_map\n\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert(''RGB'') if img.mode != ''RGB'' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float(''inf'')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert(''RGB'')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\npath = "nvidia/NVLM-D-72B"\ndevice_map = split_model()\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True,\n    device_map=device_map).eval()\n\nprint(model)\n\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=False)\n\n# pure-text conversation\nquestion = ''Hello, who are you?''\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f''User: {question}\nAssistant: {response}'')\n\n# single-image single-round conversation\npixel_values = load_image(''path/to/your/example/image.jpg'', max_num=6).to(\n    torch.bfloat16)\nquestion = ''<image>\nPlease describe the image shortly.''\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f''User: {question}\nAssistant: {response}'')\n```\n\n### Benchmark Evaluation\n\nTo test our NVLM-1.0 model on the benchmark datasets, you can use the following code:\n\n```bash\npython run_eval.py --config-path eval/full_eval.yaml \\n --result-save-path path/to/eval_results/ \\n --zero-shot-eval-tasks chartqa coco_caption flickr30k_caption vqav2 mmmu textvqa mathvista mmbench chartqa docvqa realworldqa ocrbench ai2diagram ai2diagram_nomask mmmu_pro docvqa_test\n```\n\nSpecifically,\n- `--config-path eval/full_eval.yaml` file contains the evaluation configurations, including  the evaluation prompt, the evaluation dataset paths, and generation hyper-parameters.\n- `--result-save-path path/to/eval_results/` specifies the path to save the evaluation results.\n- `--zero-shot-eval-tasks` specifies the tasks to evaluate on.\n\n\n## Software Integration\n**Runtime Engine(s)** \n* PyTorch <br>\n\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Hopper <br>\n\n**[Preferred/Supported] Operating System(s):** <br>\n* Linux <br>\n\n## Inference\n**Engine:** PyTorch <br>\n**Test Hardware:** <br>\n* H100 <br>\n\n## Model Version(s)\n* v1.0-D (NVLM-D)\n\n## Training, Testing, and Evaluation Datasets \n\n### Pre-Training Dataset\n\n**Link** <br>\n* [See Table 4](https://arxiv.org/abs/2409.11402) <br>\n\n**Data Collection Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Labeling Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Properties** \n* Trained on image captions, image-text pairs, natural images, charts, documents, scene descriptions, and mathematical reasoning. <br>\n\n### Supervised Fine-Tuning Dataset\n**Link** <br>\n* [See Table 6](https://arxiv.org/abs/2409.11402) <br>\n\n**Data Collection Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Labeling Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Properties** \n* Trained on image captions; general knowledge; image-text pairs; natural images; charts; diagrams; documents; scene descriptions; science diagrams, lessons, textbook data, and question-answer pairs; visual instruction tuning; and mathematical reasoning. <br>\n\n### Evaluation Dataset\n**Link** <br>\n* [See Section 6.1, "Benchmark"](https://arxiv.org/abs/2409.11402) <br>\n\n**Data collection method by dataset** <br>\n* Human <br>\n\n**Labeling method by dataset** <br>\n* Human <br>\n\n**Properties** <br>\n* Evaluated on general knowledge, visual answering, chart understanding, table, optical character recognition, and mathematical reasoning. <br> \n\n\n## Correspondence to\nWenliang Dai* (wdai@nvidia.com), Nayeon Lee* (nayeonl@nvidia.com), Boxin Wang* (boxinw@nvidia.com), Zhuolin Yang* (zhuoliny@nvidia.com), Wei Ping* (wping@nvidia.com)\n\n*Equal contribution\n\n## Citation\n<pre>\n@article{nvlm2024,\n  title={NVLM: Open Frontier-Class Multimodal LLMs},\n  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuolin and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},\n  journal={arXiv preprint},\n  year={2024}}\n</pre>\n\n\n## Ethical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n ', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":79379593344,"storage_bytes":354049622570,"files_count":72,"spaces_count":6,"gated":false,"private":false,"config":{"architectures":["NVLM_D"],"auto_map":{"AutoConfig":"configuration_nvlm_d.NVLM_D_Config","AutoModel":"modeling_nvlm_d.NVLM_D_Model","AutoModelForCausalLM":"modeling_nvlm_d.NVLM_D_Model"},"model_type":"NVLM_D","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"based_on_paper","target_id":"arxiv:2409.11402","source_url":"https://arxiv.org/abs/2409.11402"}]', NULL, 'CC-BY-NC-4.0', 'approved', 78.9, 'aeb4c72e78697ea94b86421ae136ba7f', NULL, NULL, CURRENT_TIMESTAMP);
